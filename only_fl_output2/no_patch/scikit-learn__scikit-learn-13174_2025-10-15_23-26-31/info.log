2025-10-15 23:26:31.377 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 23:26:35.069 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=198, output_tokens=12, cost=0.002340
2025-10-15 23:26:35.070 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 23:26:35.074 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 23:26:43.401 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=506, output_tokens=327, cost=0.014870
2025-10-15 23:26:43.405 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 23:26:46.376 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=681, output_tokens=76, cost=0.009090
2025-10-15 23:26:46.377 | DEBUG    | Extracted a valid json.
2025-10-15 23:26:46.377 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"AdaBoostClassifier\")",
        "search_class(\"AdaBoostRegressor\")",
        "search_method_in_class(\"fit\", \"AdaBoostClassifier\")",
        "search_method_in_class(\"fit\", \"AdaBoostRegressor\")",
        "search_method(\"check_X_y\")"
    ],
    "bug_locations": []
}
2025-10-15 23:26:46.415 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 23:26:46.415 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 23:27:01.151 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3493, output_tokens=556, cost=0.051610
2025-10-15 23:27:01.162 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 23:27:11.021 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4265, output_tokens=389, cost=0.054320
2025-10-15 23:27:11.026 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 23:27:19.306 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=743, output_tokens=314, cost=0.016850
2025-10-15 23:27:19.307 | DEBUG    | Extracted a valid json.
2025-10-15 23:27:19.307 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/ensemble/weight_boosting.py",
            "class": "AdaBoostClassifier",
            "method": "fit",
            "intended_behavior": "The `fit` method should be updated to flexibly validate the `X` input based on the capabilities of the base estimator. Instead of strictly requiring `X` to be a numeric array or sparse matrix, the method should validate `X` in a way that also allows it to accept other data types (e.g., lists of text blobs), provided the base estimator can handle such data types. This will likely involve conditionally calling a modified or different validation function that checks the type of base estimator and its data handling capabilities before applying specific validation rules on `X`."
        },
        {
            "file": "sklearn/ensemble/weight_boosting.py",
            "class": "AdaBoostRegressor",
            "method": "fit",
            "intended_behavior": "Similar to `AdaBoostClassifier`, the `fit` method in `AdaBoostRegressor` needs to be modified to allow for more flexibility in the type of `X` input it accepts. This modification means adjusting or bypassing certain validation steps that impose the current constraints on `X`. The method should ensure that the provided `X` is suitable for the base estimator being used, which may involve checking the base estimator's documentation or capabilities regarding accepted input types and adapting the input validation process accordingly."
        }
    ]
}
2025-10-15 23:27:19.312 | DEBUG    | Found override of fit in BaseWeightBoosting
2025-10-15 23:27:19.312 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x7fd54d722c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x7fd45b357080>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x7fd45d67e3e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7fd45b357080>, <app.raw_tasks.RawSweTask object at 0x7fd45b3570b0>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x7fd45d67e480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7fd45b28eae0>
    └ <function run_task_in_subprocess at 0x7fd45d67e660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7fd45b28eae0>
    │        │      └ <function run_raw_task at 0x7fd45d67e700>
    │        └ <function ProcessPoolExecutor.submit at 0x7fd54d336020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x7fd54d335da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x7fd54d335ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x7fd54d335f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x7fd54d4fcea0>
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x7fd54d56e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-338' parent=188003 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    └ <function Popen._launch at 0x7fd45b6737e0>
    └ <multiprocessing.popen_fork.Popen object at 0x7fd45c0ad4f0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x7fd54d4fd8a0>
           └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x7fd54d4fce00>
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x7fd45d89a150>, <multiprocessing.queues.SimpleQueue object at 0x7fd45bb801...
    │    │        └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    └ <function _process_worker at 0x7fd54d3351c0>
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x7fd45c961430>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>
        │         └ <function run_raw_task at 0x7fd45d67e700>
        └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output2/scikit-learn__scikit-learn-13174_2025-10-15_23-26-31'
             │            │    └ <function RawSweTask.to_task at 0x7fd45d67d6c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x7fd45c961430>
             └ <function do_inference at 0x7fd45d67e8e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7fd45d8e0400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7fd45dd084a0>
       │                 │         │            └ SweTask(task_id='scikit-learn__scikit-learn-13174', problem_statement='Minimize validation of X in ensembles with a base esti...
       │                 │         └ <app.manage.ProjectApiManager object at 0x7fd45d827050>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output2/scikit-learn__scikit-learn-13174_2025-10-15_23-26-31/output_0')
       └ <function _run_one_task at 0x7fd45d8e18a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x7fd45d995bc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x7fd54c9f17f0>
                                  └ <app.manage.ProjectApiManager object at 0x7fd45d827050>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'sklearn/ensemble/weight_boosting.py', 'class': 'AdaBoostClassifier', 'method': 'fit', 'intended_behavior': 'The `fi...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x7fd45d9958a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x7fd54c9f17f0>
    │                 └ <method 'extend' of 'list' objects>
    └ []

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'fit'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    │                      │   └ 'AdaBoostClassifier'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    └ <function SearchBackend._get_inherited_methods at 0x7fd45d995800>
    └ <app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseWeightBoosting', 'method_name': 'fit'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x7fd45d994f40>
                                  └ <app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>, {'class_name': 'BaseWeightBoosting', 'method_name': 'fit'})
           └ <function SearchBackend.search_method_in_class at 0x7fd45d994ea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-15 23:27:19.319 | DEBUG    | Found override of fit in BaseWeightBoosting
2025-10-15 23:27:19.319 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x7fd54d722c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x7fd45b357080>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x7fd45d67e3e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7fd45b357080>, <app.raw_tasks.RawSweTask object at 0x7fd45b3570b0>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x7fd45d67e480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7fd45b28eae0>
    └ <function run_task_in_subprocess at 0x7fd45d67e660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7fd45b28eae0>
    │        │      └ <function run_raw_task at 0x7fd45d67e700>
    │        └ <function ProcessPoolExecutor.submit at 0x7fd54d336020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x7fd54d335da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x7fd54d335ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x7fd54d335f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7fd45e31aa50>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x7fd54d4fcea0>
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x7fd54d56e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-338' parent=188003 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    └ <function Popen._launch at 0x7fd45b6737e0>
    └ <multiprocessing.popen_fork.Popen object at 0x7fd45c0ad4f0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x7fd54d4fd8a0>
           └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x7fd54d4fce00>
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x7fd45d89a150>, <multiprocessing.queues.SimpleQueue object at 0x7fd45bb801...
    │    │        └ <ForkProcess name='ForkProcess-338' parent=188003 started>
    │    └ <function _process_worker at 0x7fd54d3351c0>
    └ <ForkProcess name='ForkProcess-338' parent=188003 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x7fd45c961430>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>
        │         └ <function run_raw_task at 0x7fd45d67e700>
        └ <concurrent.futures.process._CallItem object at 0x7fd45bb815b0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output2/scikit-learn__scikit-learn-13174_2025-10-15_23-26-31'
             │            │    └ <function RawSweTask.to_task at 0x7fd45d67d6c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x7fd45c961430>
             └ <function do_inference at 0x7fd45d67e8e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7fd45d8e0400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7fd45dd084a0>
       │                 │         │            └ SweTask(task_id='scikit-learn__scikit-learn-13174', problem_statement='Minimize validation of X in ensembles with a base esti...
       │                 │         └ <app.manage.ProjectApiManager object at 0x7fd45d827050>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output2/scikit-learn__scikit-learn-13174_2025-10-15_23-26-31/output_0')
       └ <function _run_one_task at 0x7fd45d8e18a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x7fd45d995bc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x7fd54c9f17f0>
                                  └ <app.manage.ProjectApiManager object at 0x7fd45d827050>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'sklearn/ensemble/weight_boosting.py', 'class': 'AdaBoostRegressor', 'method': 'fit', 'intended_behavior': "Similar ...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x7fd45d9958a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x7fd54c9f17f0>
    │                 └ <method 'extend' of 'list' objects>
    └ [<file>sklearn/ensemble/weight_boosting.py</file>
      <class>AdaBoostClassifier</class>
      <method>fit</method>
      <code>
      387     def f...

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'fit'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    │                      │   └ 'AdaBoostRegressor'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    └ <function SearchBackend._get_inherited_methods at 0x7fd45d995800>
    └ <app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseWeightBoosting', 'method_name': 'fit'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x7fd45d994f40>
                                  └ <app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x7fd45bb83ce0>, {'class_name': 'BaseWeightBoosting', 'method_name': 'fit'})
           └ <function SearchBackend.search_method_in_class at 0x7fd45d994ea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-15 23:27:19.324 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>fit</method>
<code>
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)

</code><intended_behavior>The `fit` method should be updated to flexibly validate the `X` input based on the capabilities of the base estimator. Instead of strictly requiring `X` to be a numeric array or sparse matrix, the method should validate `X` in a way that also allows it to accept other data types (e.g., lists of text blobs), provided the base estimator can handle such data types. This will likely involve conditionally calling a modified or different validation function that checks the type of base estimator and its data handling capabilities before applying specific validation rules on `X`.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostRegressor</class>
<method>fit</method>
<code>
933     def fit(self, X, y, sample_weight=None):
934         """Build a boosted regressor from the training set (X, y).
935 
936         Parameters
937         ----------
938         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
939             The training input samples. Sparse matrix can be CSC, CSR, COO,
940             DOK, or LIL. DOK and LIL are converted to CSR.
941 
942         y : array-like of shape = [n_samples]
943             The target values (real numbers).
944 
945         sample_weight : array-like of shape = [n_samples], optional
946             Sample weights. If None, the sample weights are initialized to
947             1 / n_samples.
948 
949         Returns
950         -------
951         self : object
952         """
953         # Check loss
954         if self.loss not in ('linear', 'square', 'exponential'):
955             raise ValueError(
956                 "loss must be 'linear', 'square', or 'exponential'")
957 
958         # Fit
959         return super().fit(X, y, sample_weight)

</code><intended_behavior>Similar to `AdaBoostClassifier`, the `fit` method in `AdaBoostRegressor` needs to be modified to allow for more flexibility in the type of `X` input it accepts. This modification means adjusting or bypassing certain validation steps that impose the current constraints on `X`. The method should ensure that the provided `X` is suitable for the base estimator being used, which may involve checking the base estimator's documentation or capabilities regarding accepted input types and adapting the input validation process accordingly.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostRegressor</class>
<method>None</method>
<code>
851 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):
852     """An AdaBoost regressor.
853 
854     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
855     regressor on the original dataset and then fits additional copies of the
856     regressor on the same dataset but where the weights of instances are
857     adjusted according to the error of the current prediction. As such,
858     subsequent regressors focus more on difficult cases.
859 
860     This class implements the algorithm known as AdaBoost.R2 [2].
861 
862     Read more in the :ref:`User Guide <adaboost>`.
863 
864     Parameters
865     ----------
866     base_estimator : object, optional (default=None)
867         The base estimator from which the boosted ensemble is built.
868         Support for sample weighting is required. If ``None``, then
869         the base estimator is ``DecisionTreeRegressor(max_depth=3)``
870 
871     n_estimators : integer, optional (default=50)
872         The maximum number of estimators at which boosting is terminated.
873         In case of perfect fit, the learning procedure is stopped early.
874 
875     learning_rate : float, optional (default=1.)
876         Learning rate shrinks the contribution of each regressor by
877         ``learning_rate``. There is a trade-off between ``learning_rate`` and
878         ``n_estimators``.
879 
880     loss : {'linear', 'square', 'exponential'}, optional (default='linear')
881         The loss function to use when updating the weights after each
882         boosting iteration.
883 
884     random_state : int, RandomState instance or None, optional (default=None)
885         If int, random_state is the seed used by the random number generator;
886         If RandomState instance, random_state is the random number generator;
887         If None, the random number generator is the RandomState instance used
888         by `np.random`.
889 
890     Attributes
891     ----------
892     estimators_ : list of classifiers
893         The collection of fitted sub-estimators.
894 
895     estimator_weights_ : array of floats
896         Weights for each estimator in the boosted ensemble.
897 
898     estimator_errors_ : array of floats
899         Regression error for each estimator in the boosted ensemble.
900 
901     feature_importances_ : array of shape = [n_features]
902         The feature importances if supported by the ``base_estimator``.
903 
904     See also
905     --------
906     AdaBoostClassifier, GradientBoostingRegressor,
907     sklearn.tree.DecisionTreeRegressor
908 
909     References
910     ----------
911     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
912            on-Line Learning and an Application to Boosting", 1995.
913 
914     .. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.
915 
916     """
917     def __init__(self,
918                  base_estimator=None,
919                  n_estimators=50,
920                  learning_rate=1.,
921                  loss='linear',
922                  random_state=None):
923 
924         super().__init__(
925             base_estimator=base_estimator,
926             n_estimators=n_estimators,
927             learning_rate=learning_rate,
928             random_state=random_state)
929 
930         self.loss = loss
931         self.random_state = random_state
932 
933     def fit(self, X, y, sample_weight=None):
934         """Build a boosted regressor from the training set (X, y).
935 
936         Parameters
937         ----------
938         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
939             The training input samples. Sparse matrix can be CSC, CSR, COO,
940             DOK, or LIL. DOK and LIL are converted to CSR.
941 
942         y : array-like of shape = [n_samples]
943             The target values (real numbers).
944 
945         sample_weight : array-like of shape = [n_samples], optional
946             Sample weights. If None, the sample weights are initialized to
947             1 / n_samples.
948 
949         Returns
950         -------
951         self : object
952         """
953         # Check loss
954         if self.loss not in ('linear', 'square', 'exponential'):
955             raise ValueError(
956                 "loss must be 'linear', 'square', or 'exponential'")
957 
958         # Fit
959         return super().fit(X, y, sample_weight)
960 
961     def _validate_estimator(self):
962         """Check the estimator and set the base_estimator_ attribute."""
963         super()._validate_estimator(
964             default=DecisionTreeRegressor(max_depth=3))
965 
966     def _boost(self, iboost, X, y, sample_weight, random_state):
967         """Implement a single boost for regression
968 
969         Perform a single boost according to the AdaBoost.R2 algorithm and
970         return the updated sample weights.
971 
972         Parameters
973         ----------
974         iboost : int
975             The index of the current boost iteration.
976 
977         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
978             The training input samples. Sparse matrix can be CSC, CSR, COO,
979             DOK, or LIL. DOK and LIL are converted to CSR.
980 
981         y : array-like of shape = [n_samples]
982             The target values (class labels in classification, real numbers in
983             regression).
984 
985         sample_weight : array-like of shape = [n_samples]
986             The current sample weights.
987 
988         random_state : RandomState
989             The current random number generator
990 
991         Returns
992         -------
993         sample_weight : array-like of shape = [n_samples] or None
994             The reweighted sample weights.
995             If None then boosting has terminated early.
996 
997         estimator_weight : float
998             The weight for the current boost.
999             If None then boosting has terminated early.
1000 
1001         estimator_error : float
1002             The regression error for the current boost.
1003             If None then boosting has terminated early.
1004         """
1005         estimator = self._make_estimator(random_state=random_state)
1006 
1007         # Weighted sampling of the training set with replacement
1008         # For NumPy >= 1.7.0 use np.random.choice
1009         cdf = stable_cumsum(sample_weight)
1010         cdf /= cdf[-1]
1011         uniform_samples = random_state.random_sample(X.shape[0])
1012         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
1013         # searchsorted returns a scalar
1014         bootstrap_idx = np.array(bootstrap_idx, copy=False)
1015 
1016         # Fit on the bootstrapped sample and obtain a prediction
1017         # for all samples in the training set
1018         estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
1019         y_predict = estimator.predict(X)
1020 
1021         error_vect = np.abs(y_predict - y)
1022         error_max = error_vect.max()
1023 
1024         if error_max != 0.:
1025             error_vect /= error_max
1026 
1027         if self.loss == 'square':
1028             error_vect **= 2
1029         elif self.loss == 'exponential':
1030             error_vect = 1. - np.exp(- error_vect)
1031 
1032         # Calculate the average loss
1033         estimator_error = (sample_weight * error_vect).sum()
1034 
1035         if estimator_error <= 0:
1036             # Stop if fit is perfect
1037             return sample_weight, 1., 0.
1038 
1039         elif estimator_error >= 0.5:
1040             # Discard current estimator only if it isn't the only one
1041             if len(self.estimators_) > 1:
1042                 self.estimators_.pop(-1)
1043             return None, None, None
1044 
1045         beta = estimator_error / (1. - estimator_error)
1046 
1047         # Boost weight using AdaBoost.R2 alg
1048         estimator_weight = self.learning_rate * np.log(1. / beta)
1049 
1050         if not iboost == self.n_estimators - 1:
1051             sample_weight *= np.power(
1052                 beta,
1053                 (1. - error_vect) * self.learning_rate)
1054 
1055         return sample_weight, estimator_weight, estimator_error
1056 
1057     def _get_median_predict(self, X, limit):
1058         # Evaluate predictions of all estimators
1059         predictions = np.array([
1060             est.predict(X) for est in self.estimators_[:limit]]).T
1061 
1062         # Sort the predictions
1063         sorted_idx = np.argsort(predictions, axis=1)
1064 
1065         # Find index of median prediction for each sample
1066         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
1067         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]
1068         median_idx = median_or_above.argmax(axis=1)
1069 
1070         median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]
1071 
1072         # Return median predictions
1073         return predictions[np.arange(X.shape[0]), median_estimators]
1074 
1075     def predict(self, X):
1076         """Predict regression value for X.
1077 
1078         The predicted regression value of an input sample is computed
1079         as the weighted median prediction of the classifiers in the ensemble.
1080 
1081         Parameters
1082         ----------
1083         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1084             The training input samples. Sparse matrix can be CSC, CSR, COO,
1085             DOK, or LIL. DOK and LIL are converted to CSR.
1086 
1087         Returns
1088         -------
1089         y : array of shape = [n_samples]
1090             The predicted regression values.
1091         """
1092         check_is_fitted(self, "estimator_weights_")
1093         X = self._validate_X_predict(X)
1094 
1095         return self._get_median_predict(X, len(self.estimators_))
1096 
1097     def staged_predict(self, X):
1098         """Return staged predictions for X.
1099 
1100         The predicted regression value of an input sample is computed
1101         as the weighted median prediction of the classifiers in the ensemble.
1102 
1103         This generator method yields the ensemble prediction after each
1104         iteration of boosting and therefore allows monitoring, such as to
1105         determine the prediction on a test set after each boost.
1106 
1107         Parameters
1108         ----------
1109         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1110             The training input samples. Sparse matrix can be CSC, CSR, COO,
1111             DOK, or LIL. DOK and LIL are converted to CSR.
1112 
1113         Returns
1114         -------
1115         y : generator of array, shape = [n_samples]
1116             The predicted regression values.
1117         """
1118         check_is_fitted(self, "estimator_weights_")
1119         X = self._validate_X_predict(X)
1120 
1121         for i, _ in enumerate(self.estimators_, 1):
1122             yield self._get_median_predict(X, limit=i)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 23:27:19.324 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>fit</method>
<code>
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)

</code><intended_behavior>The `fit` method should be updated to flexibly validate the `X` input based on the capabilities of the base estimator. Instead of strictly requiring `X` to be a numeric array or sparse matrix, the method should validate `X` in a way that also allows it to accept other data types (e.g., lists of text blobs), provided the base estimator can handle such data types. This will likely involve conditionally calling a modified or different validation function that checks the type of base estimator and its data handling capabilities before applying specific validation rules on `X`.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostRegressor</class>
<method>fit</method>
<code>
933     def fit(self, X, y, sample_weight=None):
934         """Build a boosted regressor from the training set (X, y).
935 
936         Parameters
937         ----------
938         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
939             The training input samples. Sparse matrix can be CSC, CSR, COO,
940             DOK, or LIL. DOK and LIL are converted to CSR.
941 
942         y : array-like of shape = [n_samples]
943             The target values (real numbers).
944 
945         sample_weight : array-like of shape = [n_samples], optional
946             Sample weights. If None, the sample weights are initialized to
947             1 / n_samples.
948 
949         Returns
950         -------
951         self : object
952         """
953         # Check loss
954         if self.loss not in ('linear', 'square', 'exponential'):
955             raise ValueError(
956                 "loss must be 'linear', 'square', or 'exponential'")
957 
958         # Fit
959         return super().fit(X, y, sample_weight)

</code><intended_behavior>Similar to `AdaBoostClassifier`, the `fit` method in `AdaBoostRegressor` needs to be modified to allow for more flexibility in the type of `X` input it accepts. This modification means adjusting or bypassing certain validation steps that impose the current constraints on `X`. The method should ensure that the provided `X` is suitable for the base estimator being used, which may involve checking the base estimator's documentation or capabilities regarding accepted input types and adapting the input validation process accordingly.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostRegressor</class>
<method>None</method>
<code>
851 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):
852     """An AdaBoost regressor.
853 
854     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
855     regressor on the original dataset and then fits additional copies of the
856     regressor on the same dataset but where the weights of instances are
857     adjusted according to the error of the current prediction. As such,
858     subsequent regressors focus more on difficult cases.
859 
860     This class implements the algorithm known as AdaBoost.R2 [2].
861 
862     Read more in the :ref:`User Guide <adaboost>`.
863 
864     Parameters
865     ----------
866     base_estimator : object, optional (default=None)
867         The base estimator from which the boosted ensemble is built.
868         Support for sample weighting is required. If ``None``, then
869         the base estimator is ``DecisionTreeRegressor(max_depth=3)``
870 
871     n_estimators : integer, optional (default=50)
872         The maximum number of estimators at which boosting is terminated.
873         In case of perfect fit, the learning procedure is stopped early.
874 
875     learning_rate : float, optional (default=1.)
876         Learning rate shrinks the contribution of each regressor by
877         ``learning_rate``. There is a trade-off between ``learning_rate`` and
878         ``n_estimators``.
879 
880     loss : {'linear', 'square', 'exponential'}, optional (default='linear')
881         The loss function to use when updating the weights after each
882         boosting iteration.
883 
884     random_state : int, RandomState instance or None, optional (default=None)
885         If int, random_state is the seed used by the random number generator;
886         If RandomState instance, random_state is the random number generator;
887         If None, the random number generator is the RandomState instance used
888         by `np.random`.
889 
890     Attributes
891     ----------
892     estimators_ : list of classifiers
893         The collection of fitted sub-estimators.
894 
895     estimator_weights_ : array of floats
896         Weights for each estimator in the boosted ensemble.
897 
898     estimator_errors_ : array of floats
899         Regression error for each estimator in the boosted ensemble.
900 
901     feature_importances_ : array of shape = [n_features]
902         The feature importances if supported by the ``base_estimator``.
903 
904     See also
905     --------
906     AdaBoostClassifier, GradientBoostingRegressor,
907     sklearn.tree.DecisionTreeRegressor
908 
909     References
910     ----------
911     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
912            on-Line Learning and an Application to Boosting", 1995.
913 
914     .. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.
915 
916     """
917     def __init__(self,
918                  base_estimator=None,
919                  n_estimators=50,
920                  learning_rate=1.,
921                  loss='linear',
922                  random_state=None):
923 
924         super().__init__(
925             base_estimator=base_estimator,
926             n_estimators=n_estimators,
927             learning_rate=learning_rate,
928             random_state=random_state)
929 
930         self.loss = loss
931         self.random_state = random_state
932 
933     def fit(self, X, y, sample_weight=None):
934         """Build a boosted regressor from the training set (X, y).
935 
936         Parameters
937         ----------
938         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
939             The training input samples. Sparse matrix can be CSC, CSR, COO,
940             DOK, or LIL. DOK and LIL are converted to CSR.
941 
942         y : array-like of shape = [n_samples]
943             The target values (real numbers).
944 
945         sample_weight : array-like of shape = [n_samples], optional
946             Sample weights. If None, the sample weights are initialized to
947             1 / n_samples.
948 
949         Returns
950         -------
951         self : object
952         """
953         # Check loss
954         if self.loss not in ('linear', 'square', 'exponential'):
955             raise ValueError(
956                 "loss must be 'linear', 'square', or 'exponential'")
957 
958         # Fit
959         return super().fit(X, y, sample_weight)
960 
961     def _validate_estimator(self):
962         """Check the estimator and set the base_estimator_ attribute."""
963         super()._validate_estimator(
964             default=DecisionTreeRegressor(max_depth=3))
965 
966     def _boost(self, iboost, X, y, sample_weight, random_state):
967         """Implement a single boost for regression
968 
969         Perform a single boost according to the AdaBoost.R2 algorithm and
970         return the updated sample weights.
971 
972         Parameters
973         ----------
974         iboost : int
975             The index of the current boost iteration.
976 
977         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
978             The training input samples. Sparse matrix can be CSC, CSR, COO,
979             DOK, or LIL. DOK and LIL are converted to CSR.
980 
981         y : array-like of shape = [n_samples]
982             The target values (class labels in classification, real numbers in
983             regression).
984 
985         sample_weight : array-like of shape = [n_samples]
986             The current sample weights.
987 
988         random_state : RandomState
989             The current random number generator
990 
991         Returns
992         -------
993         sample_weight : array-like of shape = [n_samples] or None
994             The reweighted sample weights.
995             If None then boosting has terminated early.
996 
997         estimator_weight : float
998             The weight for the current boost.
999             If None then boosting has terminated early.
1000 
1001         estimator_error : float
1002             The regression error for the current boost.
1003             If None then boosting has terminated early.
1004         """
1005         estimator = self._make_estimator(random_state=random_state)
1006 
1007         # Weighted sampling of the training set with replacement
1008         # For NumPy >= 1.7.0 use np.random.choice
1009         cdf = stable_cumsum(sample_weight)
1010         cdf /= cdf[-1]
1011         uniform_samples = random_state.random_sample(X.shape[0])
1012         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
1013         # searchsorted returns a scalar
1014         bootstrap_idx = np.array(bootstrap_idx, copy=False)
1015 
1016         # Fit on the bootstrapped sample and obtain a prediction
1017         # for all samples in the training set
1018         estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
1019         y_predict = estimator.predict(X)
1020 
1021         error_vect = np.abs(y_predict - y)
1022         error_max = error_vect.max()
1023 
1024         if error_max != 0.:
1025             error_vect /= error_max
1026 
1027         if self.loss == 'square':
1028             error_vect **= 2
1029         elif self.loss == 'exponential':
1030             error_vect = 1. - np.exp(- error_vect)
1031 
1032         # Calculate the average loss
1033         estimator_error = (sample_weight * error_vect).sum()
1034 
1035         if estimator_error <= 0:
1036             # Stop if fit is perfect
1037             return sample_weight, 1., 0.
1038 
1039         elif estimator_error >= 0.5:
1040             # Discard current estimator only if it isn't the only one
1041             if len(self.estimators_) > 1:
1042                 self.estimators_.pop(-1)
1043             return None, None, None
1044 
1045         beta = estimator_error / (1. - estimator_error)
1046 
1047         # Boost weight using AdaBoost.R2 alg
1048         estimator_weight = self.learning_rate * np.log(1. / beta)
1049 
1050         if not iboost == self.n_estimators - 1:
1051             sample_weight *= np.power(
1052                 beta,
1053                 (1. - error_vect) * self.learning_rate)
1054 
1055         return sample_weight, estimator_weight, estimator_error
1056 
1057     def _get_median_predict(self, X, limit):
1058         # Evaluate predictions of all estimators
1059         predictions = np.array([
1060             est.predict(X) for est in self.estimators_[:limit]]).T
1061 
1062         # Sort the predictions
1063         sorted_idx = np.argsort(predictions, axis=1)
1064 
1065         # Find index of median prediction for each sample
1066         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
1067         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]
1068         median_idx = median_or_above.argmax(axis=1)
1069 
1070         median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]
1071 
1072         # Return median predictions
1073         return predictions[np.arange(X.shape[0]), median_estimators]
1074 
1075     def predict(self, X):
1076         """Predict regression value for X.
1077 
1078         The predicted regression value of an input sample is computed
1079         as the weighted median prediction of the classifiers in the ensemble.
1080 
1081         Parameters
1082         ----------
1083         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1084             The training input samples. Sparse matrix can be CSC, CSR, COO,
1085             DOK, or LIL. DOK and LIL are converted to CSR.
1086 
1087         Returns
1088         -------
1089         y : array of shape = [n_samples]
1090             The predicted regression values.
1091         """
1092         check_is_fitted(self, "estimator_weights_")
1093         X = self._validate_X_predict(X)
1094 
1095         return self._get_median_predict(X, len(self.estimators_))
1096 
1097     def staged_predict(self, X):
1098         """Return staged predictions for X.
1099 
1100         The predicted regression value of an input sample is computed
1101         as the weighted median prediction of the classifiers in the ensemble.
1102 
1103         This generator method yields the ensemble prediction after each
1104         iteration of boosting and therefore allows monitoring, such as to
1105         determine the prediction on a test set after each boost.
1106 
1107         Parameters
1108         ----------
1109         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1110             The training input samples. Sparse matrix can be CSC, CSR, COO,
1111             DOK, or LIL. DOK and LIL are converted to CSR.
1112 
1113         Returns
1114         -------
1115         y : generator of array, shape = [n_samples]
1116             The predicted regression values.
1117         """
1118         check_is_fitted(self, "estimator_weights_")
1119         X = self._validate_X_predict(X)
1120 
1121         for i, _ in enumerate(self.estimators_, 1):
1122             yield self._get_median_predict(X, limit=i)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 23:27:19.324 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 23:27:19.324 | INFO     | Search-only: skipping patch selection
2025-10-15 23:27:19.421 | INFO     | Task scikit-learn__scikit-learn-13174 completed successfully.
2025-10-15 23:27:19.422 | INFO     | No patch generated. You can try running ACR again.
