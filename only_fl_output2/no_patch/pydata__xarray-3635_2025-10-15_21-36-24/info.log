2025-10-15 21:36:24.784 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 21:36:26.357 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=743, output_tokens=13, cost=0.007820
2025-10-15 21:36:34.514 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=923, output_tokens=355, cost=0.019880
2025-10-15 21:36:35.822 | INFO     | registered non reproducing test 0
2025-10-15 21:36:49.768 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1907, output_tokens=614, cost=0.037490
2025-10-15 21:36:51.047 | INFO     | registered non reproducing test 1
2025-10-15 21:37:04.909 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2175, output_tokens=613, cost=0.040140
2025-10-15 21:37:06.188 | INFO     | registered non reproducing test 2
2025-10-15 21:37:06.188 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 21:37:06.195 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:37:10.082 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=993, output_tokens=145, cost=0.014280
2025-10-15 21:37:10.092 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:37:12.284 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=499, output_tokens=31, cost=0.005920
2025-10-15 21:37:12.286 | DEBUG    | Extracted a valid json.
2025-10-15 21:37:12.286 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method(\"quantile\")", "search_code(\"np.nanpercentile\")"],
    "bug_locations": []
}
2025-10-15 21:37:12.311 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 21:37:12.312 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 21:37:29.092 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1754, output_tokens=753, cost=0.040130
2025-10-15 21:37:29.101 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 21:37:38.694 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2723, output_tokens=368, cost=0.038270
2025-10-15 21:37:38.700 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 21:37:44.330 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=722, output_tokens=141, cost=0.011450
2025-10-15 21:37:44.332 | DEBUG    | Extracted a valid json.
2025-10-15 21:37:44.332 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/variable.py", 
            "class": "Variable", 
            "method": "quantile", 
            "intended_behavior": "The method should convert quantile inputs from the [0, 1] range to [0, 100] before passing them to np.nanpercentile to ensure compatibility with numpy's percentile requirements while maintaining xarray's API contract. Additionally, the error message for out-of-range values should reflect that 'Quantiles must be in the range [0, 1]' to align with xarray's expected input range."
        }
    ]
}
2025-10-15 21:37:44.345 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/variable.py</file>
<class>Variable</class>
<method>quantile</method>
<code>
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result

</code><intended_behavior>The method should convert quantile inputs from the [0, 1] range to [0, 100] before passing them to np.nanpercentile to ensure compatibility with numpy's percentile requirements while maintaining xarray's API contract. Additionally, the error message for out-of-range values should reflect that 'Quantiles must be in the range [0, 1]' to align with xarray's expected input range.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
263 class Variable(
264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
265 ):
266     """A netcdf-like variable consisting of dimensions, data and attributes
267     which describe a single Array. A single Variable object is not fully
268     described outside the context of its parent Dataset (if you want such a
269     fully described object, use a DataArray instead).
270 
271     The main functional difference between Variables and numpy arrays is that
272     numerical operations on Variables implement array broadcasting by dimension
273     name. For example, adding an Variable with dimensions `('time',)` to
274     another Variable with dimensions `('space',)` results in a new Variable
275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
276     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
277     instead of an "axis".
278 
279     Variables are light-weight objects used as the building block for datasets.
280     They are more primitive objects, so operations with them provide marginally
281     higher performance than using DataArrays. However, manipulating data in the
282     form of a Dataset or DataArray should almost always be preferred, because
283     they can use more complete metadata in context of coordinate labels.
284     """
285 
286     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
287 
288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
289         """
290         Parameters
291         ----------
292         dims : str or sequence of str
293             Name(s) of the the data dimension(s). Must be either a string (only
294             for 1D data) or a sequence of strings with length equal to the
295             number of dimensions.
296         data : array_like
297             Data array which supports numpy-like data access.
298         attrs : dict_like or None, optional
299             Attributes to assign to the new variable. If None (default), an
300             empty attribute dictionary is initialized.
301         encoding : dict_like or None, optional
302             Dictionary specifying how to encode this array's data into a
303             serialized format like netCDF4. Currently used keys (for netCDF)
304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
305             Well-behaved code to serialize a Variable should ignore
306             unrecognized encoding items.
307         """
308         self._data = as_compatible_data(data, fastpath=fastpath)
309         self._dims = self._parse_dimensions(dims)
310         self._attrs = None
311         self._encoding = None
312         if attrs is not None:
313             self.attrs = attrs
314         if encoding is not None:
315             self.encoding = encoding
316 
317     @property
318     def dtype(self):
319         return self._data.dtype
320 
321     @property
322     def shape(self):
323         return self._data.shape
324 
325     @property
326     def nbytes(self):
327         return self.size * self.dtype.itemsize
328 
329     @property
330     def _in_memory(self):
331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
332             isinstance(self._data, indexing.MemoryCachedArray)
333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
334         )
335 
336     @property
337     def data(self):
338         if hasattr(self._data, "__array_function__") or isinstance(
339             self._data, dask_array_type
340         ):
341             return self._data
342         else:
343             return self.values
344 
345     @data.setter
346     def data(self, data):
347         data = as_compatible_data(data)
348         if data.shape != self.shape:
349             raise ValueError(
350                 f"replacement data must match the Variable's shape. "
351                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
352             )
353         self._data = data
354 
355     def load(self, **kwargs):
356         """Manually trigger loading of this variable's data from disk or a
357         remote source into memory and return this variable.
358 
359         Normally, it should not be necessary to call this method in user code,
360         because all xarray functions should either work on deferred data or
361         load data automatically.
362 
363         Parameters
364         ----------
365         **kwargs : dict
366             Additional keyword arguments passed on to ``dask.array.compute``.
367 
368         See Also
369         --------
370         dask.array.compute
371         """
372         if isinstance(self._data, dask_array_type):
373             self._data = as_compatible_data(self._data.compute(**kwargs))
374         elif not hasattr(self._data, "__array_function__"):
375             self._data = np.asarray(self._data)
376         return self
377 
378     def compute(self, **kwargs):
379         """Manually trigger loading of this variable's data from disk or a
380         remote source into memory and return a new variable. The original is
381         left unaltered.
382 
383         Normally, it should not be necessary to call this method in user code,
384         because all xarray functions should either work on deferred data or
385         load data automatically.
386 
387         Parameters
388         ----------
389         **kwargs : dict
390             Additional keyword arguments passed on to ``dask.array.compute``.
391 
392         See Also
393         --------
394         dask.array.compute
395         """
396         new = self.copy(deep=False)
397         return new.load(**kwargs)
398 
399     def __dask_tokenize__(self):
400         # Use v.data, instead of v._data, in order to cope with the wrappers
401         # around NetCDF and the like
402         from dask.base import normalize_token
403 
404         return normalize_token((type(self), self._dims, self.data, self._attrs))
405 
406     def __dask_graph__(self):
407         if isinstance(self._data, dask_array_type):
408             return self._data.__dask_graph__()
409         else:
410             return None
411 
412     def __dask_keys__(self):
413         return self._data.__dask_keys__()
414 
415     def __dask_layers__(self):
416         return self._data.__dask_layers__()
417 
418     @property
419     def __dask_optimize__(self):
420         return self._data.__dask_optimize__
421 
422     @property
423     def __dask_scheduler__(self):
424         return self._data.__dask_scheduler__
425 
426     def __dask_postcompute__(self):
427         array_func, array_args = self._data.__dask_postcompute__()
428         return (
429             self._dask_finalize,
430             (array_func, array_args, self._dims, self._attrs, self._encoding),
431         )
432 
433     def __dask_postpersist__(self):
434         array_func, array_args = self._data.__dask_postpersist__()
435         return (
436             self._dask_finalize,
437             (array_func, array_args, self._dims, self._attrs, self._encoding),
438         )
439 
440     @staticmethod
441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
442         if isinstance(results, dict):  # persist case
443             name = array_args[0]
444             results = {k: v for k, v in results.items() if k[0] == name}
445         data = array_func(results, *array_args)
446         return Variable(dims, data, attrs=attrs, encoding=encoding)
447 
448     @property
449     def values(self):
450         """The variable's data as a numpy.ndarray"""
451         return _as_array_or_item(self._data)
452 
453     @values.setter
454     def values(self, values):
455         self.data = values
456 
457     def to_base_variable(self):
458         """Return this variable as a base xarray.Variable"""
459         return Variable(
460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
461         )
462 
463     to_variable = utils.alias(to_base_variable, "to_variable")
464 
465     def to_index_variable(self):
466         """Return this variable as an xarray.IndexVariable"""
467         return IndexVariable(
468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
469         )
470 
471     to_coord = utils.alias(to_index_variable, "to_coord")
472 
473     def to_index(self):
474         """Convert this variable to a pandas.Index"""
475         return self.to_index_variable().to_index()
476 
477     def to_dict(self, data=True):
478         """Dictionary representation of variable."""
479         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
480         if data:
481             item["data"] = ensure_us_time_resolution(self.values).tolist()
482         else:
483             item.update({"dtype": str(self.dtype), "shape": self.shape})
484         return item
485 
486     @property
487     def dims(self):
488         """Tuple of dimension names with which this variable is associated.
489         """
490         return self._dims
491 
492     @dims.setter
493     def dims(self, value):
494         self._dims = self._parse_dimensions(value)
495 
496     def _parse_dimensions(self, dims):
497         if isinstance(dims, str):
498             dims = (dims,)
499         dims = tuple(dims)
500         if len(dims) != self.ndim:
501             raise ValueError(
502                 "dimensions %s must have the same length as the "
503                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
504             )
505         return dims
506 
507     def _item_key_to_tuple(self, key):
508         if utils.is_dict_like(key):
509             return tuple(key.get(dim, slice(None)) for dim in self.dims)
510         else:
511             return key
512 
513     def _broadcast_indexes(self, key):
514         """Prepare an indexing key for an indexing operation.
515 
516         Parameters
517         -----------
518         key: int, slice, array, dict or tuple of integer, slices and arrays
519             Any valid input for indexing.
520 
521         Returns
522         -------
523         dims: tuple
524             Dimension of the resultant variable.
525         indexers: IndexingTuple subclass
526             Tuple of integer, array-like, or slices to use when indexing
527             self._data. The type of this argument indicates the type of
528             indexing to perform, either basic, outer or vectorized.
529         new_order : Optional[Sequence[int]]
530             Optional reordering to do on the result of indexing. If not None,
531             the first len(new_order) indexing should be moved to these
532             positions.
533         """
534         key = self._item_key_to_tuple(key)  # key is a tuple
535         # key is a tuple of full size
536         key = indexing.expanded_indexer(key, self.ndim)
537         # Convert a scalar Variable to an integer
538         key = tuple(
539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
540         )
541         # Convert a 0d-array to an integer
542         key = tuple(
543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
544         )
545 
546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
547             return self._broadcast_indexes_basic(key)
548 
549         self._validate_indexers(key)
550         # Detect it can be mapped as an outer indexer
551         # If all key is unlabeled, or
552         # key can be mapped as an OuterIndexer.
553         if all(not isinstance(k, Variable) for k in key):
554             return self._broadcast_indexes_outer(key)
555 
556         # If all key is 1-dimensional and there are no duplicate labels,
557         # key can be mapped as an OuterIndexer.
558         dims = []
559         for k, d in zip(key, self.dims):
560             if isinstance(k, Variable):
561                 if len(k.dims) > 1:
562                     return self._broadcast_indexes_vectorized(key)
563                 dims.append(k.dims[0])
564             elif not isinstance(k, integer_types):
565                 dims.append(d)
566         if len(set(dims)) == len(dims):
567             return self._broadcast_indexes_outer(key)
568 
569         return self._broadcast_indexes_vectorized(key)
570 
571     def _broadcast_indexes_basic(self, key):
572         dims = tuple(
573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
574         )
575         return dims, BasicIndexer(key), None
576 
577     def _validate_indexers(self, key):
578         """ Make sanity checks """
579         for dim, k in zip(self.dims, key):
580             if isinstance(k, BASIC_INDEXING_TYPES):
581                 pass
582             else:
583                 if not isinstance(k, Variable):
584                     k = np.asarray(k)
585                     if k.ndim > 1:
586                         raise IndexError(
587                             "Unlabeled multi-dimensional array cannot be "
588                             "used for indexing: {}".format(k)
589                         )
590                 if k.dtype.kind == "b":
591                     if self.shape[self.get_axis_num(dim)] != len(k):
592                         raise IndexError(
593                             "Boolean array size {:d} is used to index array "
594                             "with shape {:s}.".format(len(k), str(self.shape))
595                         )
596                     if k.ndim > 1:
597                         raise IndexError(
598                             "{}-dimensional boolean indexing is "
599                             "not supported. ".format(k.ndim)
600                         )
601                     if getattr(k, "dims", (dim,)) != (dim,):
602                         raise IndexError(
603                             "Boolean indexer should be unlabeled or on the "
604                             "same dimension to the indexed array. Indexer is "
605                             "on {:s} but the target dimension is {:s}.".format(
606                                 str(k.dims), dim
607                             )
608                         )
609 
610     def _broadcast_indexes_outer(self, key):
611         dims = tuple(
612             k.dims[0] if isinstance(k, Variable) else dim
613             for k, dim in zip(key, self.dims)
614             if not isinstance(k, integer_types)
615         )
616 
617         new_key = []
618         for k in key:
619             if isinstance(k, Variable):
620                 k = k.data
621             if not isinstance(k, BASIC_INDEXING_TYPES):
622                 k = np.asarray(k)
623                 if k.size == 0:
624                     # Slice by empty list; numpy could not infer the dtype
625                     k = k.astype(int)
626                 elif k.dtype.kind == "b":
627                     (k,) = np.nonzero(k)
628             new_key.append(k)
629 
630         return dims, OuterIndexer(tuple(new_key)), None
631 
632     def _nonzero(self):
633         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
634         # TODO we should replace dask's native nonzero
635         # after https://github.com/dask/dask/issues/1076 is implemented.
636         nonzeros = np.nonzero(self.data)
637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
638 
639     def _broadcast_indexes_vectorized(self, key):
640         variables = []
641         out_dims_set = OrderedSet()
642         for dim, value in zip(self.dims, key):
643             if isinstance(value, slice):
644                 out_dims_set.add(dim)
645             else:
646                 variable = (
647                     value
648                     if isinstance(value, Variable)
649                     else as_variable(value, name=dim)
650                 )
651                 if variable.dtype.kind == "b":  # boolean indexing case
652                     (variable,) = variable._nonzero()
653 
654                 variables.append(variable)
655                 out_dims_set.update(variable.dims)
656 
657         variable_dims = set()
658         for variable in variables:
659             variable_dims.update(variable.dims)
660 
661         slices = []
662         for i, (dim, value) in enumerate(zip(self.dims, key)):
663             if isinstance(value, slice):
664                 if dim in variable_dims:
665                     # We only convert slice objects to variables if they share
666                     # a dimension with at least one other variable. Otherwise,
667                     # we can equivalently leave them as slices aknd transpose
668                     # the result. This is significantly faster/more efficient
669                     # for most array backends.
670                     values = np.arange(*value.indices(self.sizes[dim]))
671                     variables.insert(i - len(slices), Variable((dim,), values))
672                 else:
673                     slices.append((i, value))
674 
675         try:
676             variables = _broadcast_compat_variables(*variables)
677         except ValueError:
678             raise IndexError(f"Dimensions of indexers mismatch: {key}")
679 
680         out_key = [variable.data for variable in variables]
681         out_dims = tuple(out_dims_set)
682         slice_positions = set()
683         for i, value in slices:
684             out_key.insert(i, value)
685             new_position = out_dims.index(self.dims[i])
686             slice_positions.add(new_position)
687 
688         if slice_positions:
689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
690         else:
691             new_order = None
692 
693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
694 
695     def __getitem__(self: VariableType, key) -> VariableType:
696         """Return a new Variable object whose contents are consistent with
697         getting the provided key from the underlying data.
698 
699         NB. __getitem__ and __setitem__ implement xarray-style indexing,
700         where if keys are unlabeled arrays, we index the array orthogonally
701         with them. If keys are labeled array (such as Variables), they are
702         broadcasted with our usual scheme and then the array is indexed with
703         the broadcasted key, like numpy's fancy indexing.
704 
705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
706         array `x.values` directly.
707         """
708         dims, indexer, new_order = self._broadcast_indexes(key)
709         data = as_indexable(self._data)[indexer]
710         if new_order:
711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
712         return self._finalize_indexing_result(dims, data)
713 
714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
715         """Used by IndexVariable to return IndexVariable objects when possible.
716         """
717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
718 
719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
720         """Index this Variable with -1 remapped to fill_value."""
721         # TODO(shoyer): expose this method in public API somewhere (isel?) and
722         # use it for reindex.
723         # TODO(shoyer): add a sanity check that all other integers are
724         # non-negative
725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
726         # that is actually indexed rather than mapping it to the last value
727         # along each axis.
728 
729         if fill_value is dtypes.NA:
730             fill_value = dtypes.get_fill_value(self.dtype)
731 
732         dims, indexer, new_order = self._broadcast_indexes(key)
733 
734         if self.size:
735             if isinstance(self._data, dask_array_type):
736                 # dask's indexing is faster this way; also vindex does not
737                 # support negative indices yet:
738                 # https://github.com/dask/dask/pull/2967
739                 actual_indexer = indexing.posify_mask_indexer(indexer)
740             else:
741                 actual_indexer = indexer
742 
743             data = as_indexable(self._data)[actual_indexer]
744             mask = indexing.create_mask(indexer, self.shape, data)
745             data = duck_array_ops.where(mask, fill_value, data)
746         else:
747             # array cannot be indexed along dimensions of size 0, so just
748             # build the mask directly instead.
749             mask = indexing.create_mask(indexer, self.shape)
750             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
751 
752         if new_order:
753             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
754         return self._finalize_indexing_result(dims, data)
755 
756     def __setitem__(self, key, value):
757         """__setitem__ is overloaded to access the underlying numpy values with
758         orthogonal indexing.
759 
760         See __getitem__ for more details.
761         """
762         dims, index_tuple, new_order = self._broadcast_indexes(key)
763 
764         if not isinstance(value, Variable):
765             value = as_compatible_data(value)
766             if value.ndim > len(dims):
767                 raise ValueError(
768                     "shape mismatch: value array of shape %s could not be "
769                     "broadcast to indexing result with %s dimensions"
770                     % (value.shape, len(dims))
771                 )
772             if value.ndim == 0:
773                 value = Variable((), value)
774             else:
775                 value = Variable(dims[-value.ndim :], value)
776         # broadcast to become assignable
777         value = value.set_dims(dims).data
778 
779         if new_order:
780             value = duck_array_ops.asarray(value)
781             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
782             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
783 
784         indexable = as_indexable(self._data)
785         indexable[index_tuple] = value
786 
787     @property
788     def attrs(self) -> Dict[Hashable, Any]:
789         """Dictionary of local attributes on this variable.
790         """
791         if self._attrs is None:
792             self._attrs = {}
793         return self._attrs
794 
795     @attrs.setter
796     def attrs(self, value: Mapping[Hashable, Any]) -> None:
797         self._attrs = dict(value)
798 
799     @property
800     def encoding(self):
801         """Dictionary of encodings on this variable.
802         """
803         if self._encoding is None:
804             self._encoding = {}
805         return self._encoding
806 
807     @encoding.setter
808     def encoding(self, value):
809         try:
810             self._encoding = dict(value)
811         except ValueError:
812             raise ValueError("encoding must be castable to a dictionary")
813 
814     def copy(self, deep=True, data=None):
815         """Returns a copy of this object.
816 
817         If `deep=True`, the data array is loaded into memory and copied onto
818         the new object. Dimensions, attributes and encodings are always copied.
819 
820         Use `data` to create a new object with the same structure as
821         original but entirely new data.
822 
823         Parameters
824         ----------
825         deep : bool, optional
826             Whether the data array is loaded into memory and copied onto
827             the new object. Default is True.
828         data : array_like, optional
829             Data to use in the new object. Must have same shape as original.
830             When `data` is used, `deep` is ignored.
831 
832         Returns
833         -------
834         object : Variable
835             New object with dimensions, attributes, encodings, and optionally
836             data copied from original.
837 
838         Examples
839         --------
840 
841         Shallow copy versus deep copy
842 
843         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
844         >>> var.copy()
845         <xarray.Variable (x: 3)>
846         array([1, 2, 3])
847         >>> var_0 = var.copy(deep=False)
848         >>> var_0[0] = 7
849         >>> var_0
850         <xarray.Variable (x: 3)>
851         array([7, 2, 3])
852         >>> var
853         <xarray.Variable (x: 3)>
854         array([7, 2, 3])
855 
856         Changing the data using the ``data`` argument maintains the
857         structure of the original object, but with the new data. Original
858         object is unaffected.
859 
860         >>> var.copy(data=[0.1, 0.2, 0.3])
861         <xarray.Variable (x: 3)>
862         array([ 0.1,  0.2,  0.3])
863         >>> var
864         <xarray.Variable (x: 3)>
865         array([7, 2, 3])
866 
867         See Also
868         --------
869         pandas.DataFrame.copy
870         """
871         if data is None:
872             data = self._data
873 
874             if isinstance(data, indexing.MemoryCachedArray):
875                 # don't share caching between copies
876                 data = indexing.MemoryCachedArray(data.array)
877 
878             if deep:
879                 if hasattr(data, "__array_function__") or isinstance(
880                     data, dask_array_type
881                 ):
882                     data = data.copy()
883                 elif not isinstance(data, PandasIndexAdapter):
884                     # pandas.Index is immutable
885                     data = np.array(data)
886         else:
887             data = as_compatible_data(data)
888             if self.shape != data.shape:
889                 raise ValueError(
890                     "Data shape {} must match shape of object {}".format(
891                         data.shape, self.shape
892                     )
893                 )
894 
895         # note:
896         # dims is already an immutable tuple
897         # attributes and encoding will be copied when the new Array is created
898         return self._replace(data=data)
899 
900     def _replace(
901         self, dims=_default, data=_default, attrs=_default, encoding=_default
902     ) -> "Variable":
903         if dims is _default:
904             dims = copy.copy(self._dims)
905         if data is _default:
906             data = copy.copy(self.data)
907         if attrs is _default:
908             attrs = copy.copy(self._attrs)
909         if encoding is _default:
910             encoding = copy.copy(self._encoding)
911         return type(self)(dims, data, attrs, encoding, fastpath=True)
912 
913     def __copy__(self):
914         return self.copy(deep=False)
915 
916     def __deepcopy__(self, memo=None):
917         # memo does nothing but is required for compatibility with
918         # copy.deepcopy
919         return self.copy(deep=True)
920 
921     # mutable objects should not be hashable
922     # https://github.com/python/mypy/issues/4266
923     __hash__ = None  # type: ignore
924 
925     @property
926     def chunks(self):
927         """Block dimensions for this array's data or None if it's not a dask
928         array.
929         """
930         return getattr(self._data, "chunks", None)
931 
932     _array_counter = itertools.count()
933 
934     def chunk(self, chunks=None, name=None, lock=False):
935         """Coerce this array's data into a dask arrays with the given chunks.
936 
937         If this variable is a non-dask array, it will be converted to dask
938         array. If it's a dask array, it will be rechunked to the given chunk
939         sizes.
940 
941         If neither chunks is not provided for one or more dimensions, chunk
942         sizes along that dimension will not be updated; non-dask arrays will be
943         converted into dask arrays with a single block.
944 
945         Parameters
946         ----------
947         chunks : int, tuple or dict, optional
948             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
949             ``{'x': 5, 'y': 5}``.
950         name : str, optional
951             Used to generate the name for this array in the internal dask
952             graph. Does not need not be unique.
953         lock : optional
954             Passed on to :py:func:`dask.array.from_array`, if the array is not
955             already as dask array.
956 
957         Returns
958         -------
959         chunked : xarray.Variable
960         """
961         import dask
962         import dask.array as da
963 
964         if utils.is_dict_like(chunks):
965             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
966 
967         if chunks is None:
968             chunks = self.chunks or self.shape
969 
970         data = self._data
971         if isinstance(data, da.Array):
972             data = data.rechunk(chunks)
973         else:
974             if isinstance(data, indexing.ExplicitlyIndexed):
975                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
976                 # that can't handle general array indexing. For example, in netCDF4 you
977                 # can do "outer" indexing along two dimensions independent, which works
978                 # differently from how NumPy handles it.
979                 # da.from_array works by using lazy indexing with a tuple of slices.
980                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
981                 # different indexing types in an explicit way:
982                 # https://github.com/dask/dask/issues/2883
983                 data = indexing.ImplicitToExplicitIndexingAdapter(
984                     data, indexing.OuterIndexer
985                 )
986                 if LooseVersion(dask.__version__) < "2.0.0":
987                     kwargs = {}
988                 else:
989                     # All of our lazily loaded backend array classes should use NumPy
990                     # array operations.
991                     kwargs = {"meta": np.ndarray}
992             else:
993                 kwargs = {}
994 
995             if utils.is_dict_like(chunks):
996                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
997 
998             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
999 
1000         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1001 
1002     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1003         """
1004         use sparse-array as backend.
1005         """
1006         import sparse
1007 
1008         # TODO  what to do if dask-backended?
1009         if fill_value is dtypes.NA:
1010             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1011         else:
1012             dtype = dtypes.result_type(self.dtype, fill_value)
1013 
1014         if sparse_format is _default:
1015             sparse_format = "coo"
1016         try:
1017             as_sparse = getattr(sparse, "as_{}".format(sparse_format.lower()))
1018         except AttributeError:
1019             raise ValueError("{} is not a valid sparse format".format(sparse_format))
1020 
1021         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1022         return self._replace(data=data)
1023 
1024     def _to_dense(self):
1025         """
1026         Change backend from sparse to np.array
1027         """
1028         if hasattr(self._data, "todense"):
1029             return self._replace(data=self._data.todense())
1030         return self.copy(deep=False)
1031 
1032     def isel(
1033         self: VariableType,
1034         indexers: Mapping[Hashable, Any] = None,
1035         **indexers_kwargs: Any,
1036     ) -> VariableType:
1037         """Return a new array indexed along the specified dimension(s).
1038 
1039         Parameters
1040         ----------
1041         **indexers : {dim: indexer, ...}
1042             Keyword arguments with names matching dimensions and values given
1043             by integers, slice objects or arrays.
1044 
1045         Returns
1046         -------
1047         obj : Array object
1048             A new Array with the selected data and dimensions. In general,
1049             the new variable's data will be a view of this variable's data,
1050             unless numpy fancy indexing was triggered by using an array
1051             indexer, in which case the data will be a copy.
1052         """
1053         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1054 
1055         invalid = indexers.keys() - set(self.dims)
1056         if invalid:
1057             raise ValueError("dimensions %r do not exist" % invalid)
1058 
1059         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1060         return self[key]
1061 
1062     def squeeze(self, dim=None):
1063         """Return a new object with squeezed data.
1064 
1065         Parameters
1066         ----------
1067         dim : None or str or tuple of str, optional
1068             Selects a subset of the length one dimensions. If a dimension is
1069             selected with length greater than one, an error is raised. If
1070             None, all length one dimensions are squeezed.
1071 
1072         Returns
1073         -------
1074         squeezed : same type as caller
1075             This object, but with with all or a subset of the dimensions of
1076             length 1 removed.
1077 
1078         See Also
1079         --------
1080         numpy.squeeze
1081         """
1082         dims = common.get_squeeze_dims(self, dim)
1083         return self.isel({d: 0 for d in dims})
1084 
1085     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1086         axis = self.get_axis_num(dim)
1087 
1088         if count > 0:
1089             keep = slice(None, -count)
1090         elif count < 0:
1091             keep = slice(-count, None)
1092         else:
1093             keep = slice(None)
1094 
1095         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1096 
1097         if fill_value is dtypes.NA:
1098             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1099         else:
1100             dtype = self.dtype
1101 
1102         shape = list(self.shape)
1103         shape[axis] = min(abs(count), shape[axis])
1104 
1105         if isinstance(trimmed_data, dask_array_type):
1106             chunks = list(trimmed_data.chunks)
1107             chunks[axis] = (shape[axis],)
1108             full = functools.partial(da.full, chunks=chunks)
1109         else:
1110             full = np.full
1111 
1112         filler = full(shape, fill_value, dtype=dtype)
1113 
1114         if count > 0:
1115             arrays = [filler, trimmed_data]
1116         else:
1117             arrays = [trimmed_data, filler]
1118 
1119         data = duck_array_ops.concatenate(arrays, axis)
1120 
1121         if isinstance(data, dask_array_type):
1122             # chunked data should come out with the same chunks; this makes
1123             # it feasible to combine shifted and unshifted data
1124             # TODO: remove this once dask.array automatically aligns chunks
1125             data = data.rechunk(self.data.chunks)
1126 
1127         return type(self)(self.dims, data, self._attrs, fastpath=True)
1128 
1129     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1130         """
1131         Return a new Variable with shifted data.
1132 
1133         Parameters
1134         ----------
1135         shifts : mapping of the form {dim: offset}
1136             Integer offset to shift along each of the given dimensions.
1137             Positive offsets shift to the right; negative offsets shift to the
1138             left.
1139         fill_value: scalar, optional
1140             Value to use for newly missing values
1141         **shifts_kwargs:
1142             The keyword arguments form of ``shifts``.
1143             One of shifts or shifts_kwargs must be provided.
1144 
1145         Returns
1146         -------
1147         shifted : Variable
1148             Variable with the same dimensions and attributes but shifted data.
1149         """
1150         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1151         result = self
1152         for dim, count in shifts.items():
1153             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1154         return result
1155 
1156     def pad_with_fill_value(
1157         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs
1158     ):
1159         """
1160         Return a new Variable with paddings.
1161 
1162         Parameters
1163         ----------
1164         pad_width: Mapping of the form {dim: (before, after)}
1165             Number of values padded to the edges of each dimension.
1166         **pad_widths_kwargs:
1167             Keyword argument for pad_widths
1168         """
1169         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, "pad")
1170 
1171         if fill_value is dtypes.NA:
1172             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1173         else:
1174             dtype = self.dtype
1175 
1176         if isinstance(self.data, dask_array_type):
1177             array = self.data
1178 
1179             # Dask does not yet support pad. We manually implement it.
1180             # https://github.com/dask/dask/issues/1926
1181             for d, pad in pad_widths.items():
1182                 axis = self.get_axis_num(d)
1183                 before_shape = list(array.shape)
1184                 before_shape[axis] = pad[0]
1185                 before_chunks = list(array.chunks)
1186                 before_chunks[axis] = (pad[0],)
1187                 after_shape = list(array.shape)
1188                 after_shape[axis] = pad[1]
1189                 after_chunks = list(array.chunks)
1190                 after_chunks[axis] = (pad[1],)
1191 
1192                 arrays = []
1193                 if pad[0] > 0:
1194                     arrays.append(
1195                         da.full(
1196                             before_shape, fill_value, dtype=dtype, chunks=before_chunks
1197                         )
1198                     )
1199                 arrays.append(array)
1200                 if pad[1] > 0:
1201                     arrays.append(
1202                         da.full(
1203                             after_shape, fill_value, dtype=dtype, chunks=after_chunks
1204                         )
1205                     )
1206                 if len(arrays) > 1:
1207                     array = da.concatenate(arrays, axis=axis)
1208         else:
1209             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]
1210             array = np.pad(
1211                 self.data.astype(dtype, copy=False),
1212                 pads,
1213                 mode="constant",
1214                 constant_values=fill_value,
1215             )
1216         return type(self)(self.dims, array)
1217 
1218     def _roll_one_dim(self, dim, count):
1219         axis = self.get_axis_num(dim)
1220 
1221         count %= self.shape[axis]
1222         if count != 0:
1223             indices = [slice(-count, None), slice(None, -count)]
1224         else:
1225             indices = [slice(None)]
1226 
1227         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1228 
1229         data = duck_array_ops.concatenate(arrays, axis)
1230 
1231         if isinstance(data, dask_array_type):
1232             # chunked data should come out with the same chunks; this makes
1233             # it feasible to combine shifted and unshifted data
1234             # TODO: remove this once dask.array automatically aligns chunks
1235             data = data.rechunk(self.data.chunks)
1236 
1237         return type(self)(self.dims, data, self._attrs, fastpath=True)
1238 
1239     def roll(self, shifts=None, **shifts_kwargs):
1240         """
1241         Return a new Variable with rolld data.
1242 
1243         Parameters
1244         ----------
1245         shifts : mapping of the form {dim: offset}
1246             Integer offset to roll along each of the given dimensions.
1247             Positive offsets roll to the right; negative offsets roll to the
1248             left.
1249         **shifts_kwargs:
1250             The keyword arguments form of ``shifts``.
1251             One of shifts or shifts_kwargs must be provided.
1252 
1253         Returns
1254         -------
1255         shifted : Variable
1256             Variable with the same dimensions and attributes but rolled data.
1257         """
1258         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1259 
1260         result = self
1261         for dim, count in shifts.items():
1262             result = result._roll_one_dim(dim, count)
1263         return result
1264 
1265     def transpose(self, *dims) -> "Variable":
1266         """Return a new Variable object with transposed dimensions.
1267 
1268         Parameters
1269         ----------
1270         *dims : str, optional
1271             By default, reverse the dimensions. Otherwise, reorder the
1272             dimensions to this order.
1273 
1274         Returns
1275         -------
1276         transposed : Variable
1277             The returned object has transposed data and dimensions with the
1278             same attributes as the original.
1279 
1280         Notes
1281         -----
1282         This operation returns a view of this variable's data. It is
1283         lazy for dask-backed Variables but not for numpy-backed Variables.
1284 
1285         See Also
1286         --------
1287         numpy.transpose
1288         """
1289         if len(dims) == 0:
1290             dims = self.dims[::-1]
1291         dims = tuple(infix_dims(dims, self.dims))
1292         axes = self.get_axis_num(dims)
1293         if len(dims) < 2 or dims == self.dims:
1294             # no need to transpose if only one dimension
1295             # or dims are in same order
1296             return self.copy(deep=False)
1297 
1298         data = as_indexable(self._data).transpose(axes)
1299         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1300 
1301     @property
1302     def T(self) -> "Variable":
1303         return self.transpose()
1304 
1305     def set_dims(self, dims, shape=None):
1306         """Return a new variable with given set of dimensions.
1307         This method might be used to attach new dimension(s) to variable.
1308 
1309         When possible, this operation does not copy this variable's data.
1310 
1311         Parameters
1312         ----------
1313         dims : str or sequence of str or dict
1314             Dimensions to include on the new variable. If a dict, values are
1315             used to provide the sizes of new dimensions; otherwise, new
1316             dimensions are inserted with length 1.
1317 
1318         Returns
1319         -------
1320         Variable
1321         """
1322         if isinstance(dims, str):
1323             dims = [dims]
1324 
1325         if shape is None and utils.is_dict_like(dims):
1326             shape = dims.values()
1327 
1328         missing_dims = set(self.dims) - set(dims)
1329         if missing_dims:
1330             raise ValueError(
1331                 "new dimensions %r must be a superset of "
1332                 "existing dimensions %r" % (dims, self.dims)
1333             )
1334 
1335         self_dims = set(self.dims)
1336         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1337 
1338         if self.dims == expanded_dims:
1339             # don't use broadcast_to unless necessary so the result remains
1340             # writeable if possible
1341             expanded_data = self.data
1342         elif shape is not None:
1343             dims_map = dict(zip(dims, shape))
1344             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1345             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1346         else:
1347             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1348 
1349         expanded_var = Variable(
1350             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1351         )
1352         return expanded_var.transpose(*dims)
1353 
1354     def _stack_once(self, dims, new_dim):
1355         if not set(dims) <= set(self.dims):
1356             raise ValueError("invalid existing dimensions: %s" % dims)
1357 
1358         if new_dim in self.dims:
1359             raise ValueError(
1360                 "cannot create a new dimension with the same "
1361                 "name as an existing dimension"
1362             )
1363 
1364         if len(dims) == 0:
1365             # don't stack
1366             return self.copy(deep=False)
1367 
1368         other_dims = [d for d in self.dims if d not in dims]
1369         dim_order = other_dims + list(dims)
1370         reordered = self.transpose(*dim_order)
1371 
1372         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1373         new_data = reordered.data.reshape(new_shape)
1374         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1375 
1376         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1377 
1378     def stack(self, dimensions=None, **dimensions_kwargs):
1379         """
1380         Stack any number of existing dimensions into a single new dimension.
1381 
1382         New dimensions will be added at the end, and the order of the data
1383         along each new dimension will be in contiguous (C) order.
1384 
1385         Parameters
1386         ----------
1387         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1388             Names of new dimensions, and the existing dimensions that they
1389             replace.
1390         **dimensions_kwargs:
1391             The keyword arguments form of ``dimensions``.
1392             One of dimensions or dimensions_kwargs must be provided.
1393 
1394         Returns
1395         -------
1396         stacked : Variable
1397             Variable with the same attributes but stacked data.
1398 
1399         See also
1400         --------
1401         Variable.unstack
1402         """
1403         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1404         result = self
1405         for new_dim, dims in dimensions.items():
1406             result = result._stack_once(dims, new_dim)
1407         return result
1408 
1409     def _unstack_once(self, dims, old_dim):
1410         new_dim_names = tuple(dims.keys())
1411         new_dim_sizes = tuple(dims.values())
1412 
1413         if old_dim not in self.dims:
1414             raise ValueError("invalid existing dimension: %s" % old_dim)
1415 
1416         if set(new_dim_names).intersection(self.dims):
1417             raise ValueError(
1418                 "cannot create a new dimension with the same "
1419                 "name as an existing dimension"
1420             )
1421 
1422         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1423             raise ValueError(
1424                 "the product of the new dimension sizes must "
1425                 "equal the size of the old dimension"
1426             )
1427 
1428         other_dims = [d for d in self.dims if d != old_dim]
1429         dim_order = other_dims + [old_dim]
1430         reordered = self.transpose(*dim_order)
1431 
1432         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1433         new_data = reordered.data.reshape(new_shape)
1434         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1435 
1436         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1437 
1438     def unstack(self, dimensions=None, **dimensions_kwargs):
1439         """
1440         Unstack an existing dimension into multiple new dimensions.
1441 
1442         New dimensions will be added at the end, and the order of the data
1443         along each new dimension will be in contiguous (C) order.
1444 
1445         Parameters
1446         ----------
1447         dimensions : mapping of the form old_dim={dim1: size1, ...}
1448             Names of existing dimensions, and the new dimensions and sizes
1449             that they map to.
1450         **dimensions_kwargs:
1451             The keyword arguments form of ``dimensions``.
1452             One of dimensions or dimensions_kwargs must be provided.
1453 
1454         Returns
1455         -------
1456         unstacked : Variable
1457             Variable with the same attributes but unstacked data.
1458 
1459         See also
1460         --------
1461         Variable.stack
1462         """
1463         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1464         result = self
1465         for old_dim, dims in dimensions.items():
1466             result = result._unstack_once(dims, old_dim)
1467         return result
1468 
1469     def fillna(self, value):
1470         return ops.fillna(self, value)
1471 
1472     def where(self, cond, other=dtypes.NA):
1473         return ops.where_method(self, cond, other)
1474 
1475     def reduce(
1476         self,
1477         func,
1478         dim=None,
1479         axis=None,
1480         keep_attrs=None,
1481         keepdims=False,
1482         allow_lazy=None,
1483         **kwargs,
1484     ):
1485         """Reduce this array by applying `func` along some dimension(s).
1486 
1487         Parameters
1488         ----------
1489         func : function
1490             Function which can be called in the form
1491             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1492             np.ndarray over an integer valued axis.
1493         dim : str or sequence of str, optional
1494             Dimension(s) over which to apply `func`.
1495         axis : int or sequence of int, optional
1496             Axis(es) over which to apply `func`. Only one of the 'dim'
1497             and 'axis' arguments can be supplied. If neither are supplied, then
1498             the reduction is calculated over the flattened array (by calling
1499             `func(x)` without an axis argument).
1500         keep_attrs : bool, optional
1501             If True, the variable's attributes (`attrs`) will be copied from
1502             the original object to the new one.  If False (default), the new
1503             object will be returned without attributes.
1504         keepdims : bool, default False
1505             If True, the dimensions which are reduced are left in the result
1506             as dimensions of size one
1507         **kwargs : dict
1508             Additional keyword arguments passed on to `func`.
1509 
1510         Returns
1511         -------
1512         reduced : Array
1513             Array with summarized data and the indicated dimension(s)
1514             removed.
1515         """
1516         if dim == ...:
1517             dim = None
1518         if dim is not None and axis is not None:
1519             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1520 
1521         if dim is not None:
1522             axis = self.get_axis_num(dim)
1523 
1524         if allow_lazy is not None:
1525             warnings.warn(
1526                 "allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.",
1527                 DeprecationWarning,
1528             )
1529         else:
1530             allow_lazy = True
1531 
1532         input_data = self.data if allow_lazy else self.values
1533 
1534         if axis is not None:
1535             data = func(input_data, axis=axis, **kwargs)
1536         else:
1537             data = func(input_data, **kwargs)
1538 
1539         if getattr(data, "shape", ()) == self.shape:
1540             dims = self.dims
1541         else:
1542             removed_axes = (
1543                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1544             )
1545             if keepdims:
1546                 # Insert np.newaxis for removed dims
1547                 slices = tuple(
1548                     np.newaxis if i in removed_axes else slice(None, None)
1549                     for i in range(self.ndim)
1550                 )
1551                 if getattr(data, "shape", None) is None:
1552                     # Reduce has produced a scalar value, not an array-like
1553                     data = np.asanyarray(data)[slices]
1554                 else:
1555                     data = data[slices]
1556                 dims = self.dims
1557             else:
1558                 dims = [
1559                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1560                 ]
1561 
1562         if keep_attrs is None:
1563             keep_attrs = _get_keep_attrs(default=False)
1564         attrs = self._attrs if keep_attrs else None
1565 
1566         return Variable(dims, data, attrs=attrs)
1567 
1568     @classmethod
1569     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1570         """Concatenate variables along a new or existing dimension.
1571 
1572         Parameters
1573         ----------
1574         variables : iterable of Array
1575             Arrays to stack together. Each variable is expected to have
1576             matching dimensions and shape except for along the stacked
1577             dimension.
1578         dim : str or DataArray, optional
1579             Name of the dimension to stack along. This can either be a new
1580             dimension name, in which case it is added along axis=0, or an
1581             existing dimension name, in which case the location of the
1582             dimension is unchanged. Where to insert the new dimension is
1583             determined by the first variable.
1584         positions : None or list of integer arrays, optional
1585             List of integer arrays which specifies the integer positions to
1586             which to assign each dataset along the concatenated dimension.
1587             If not supplied, objects are concatenated in the provided order.
1588         shortcut : bool, optional
1589             This option is used internally to speed-up groupby operations.
1590             If `shortcut` is True, some checks of internal consistency between
1591             arrays to concatenate are skipped.
1592 
1593         Returns
1594         -------
1595         stacked : Variable
1596             Concatenated Variable formed by stacking all the supplied variables
1597             along the given dimension.
1598         """
1599         if not isinstance(dim, str):
1600             (dim,) = dim.dims
1601 
1602         # can't do this lazily: we need to loop through variables at least
1603         # twice
1604         variables = list(variables)
1605         first_var = variables[0]
1606 
1607         arrays = [v.data for v in variables]
1608 
1609         if dim in first_var.dims:
1610             axis = first_var.get_axis_num(dim)
1611             dims = first_var.dims
1612             data = duck_array_ops.concatenate(arrays, axis=axis)
1613             if positions is not None:
1614                 # TODO: deprecate this option -- we don't need it for groupby
1615                 # any more.
1616                 indices = nputils.inverse_permutation(np.concatenate(positions))
1617                 data = duck_array_ops.take(data, indices, axis=axis)
1618         else:
1619             axis = 0
1620             dims = (dim,) + first_var.dims
1621             data = duck_array_ops.stack(arrays, axis=axis)
1622 
1623         attrs = dict(first_var.attrs)
1624         encoding = dict(first_var.encoding)
1625         if not shortcut:
1626             for var in variables:
1627                 if var.dims != first_var.dims:
1628                     raise ValueError("inconsistent dimensions")
1629                 utils.remove_incompatible_items(attrs, var.attrs)
1630 
1631         return cls(dims, data, attrs, encoding)
1632 
1633     def equals(self, other, equiv=duck_array_ops.array_equiv):
1634         """True if two Variables have the same dimensions and values;
1635         otherwise False.
1636 
1637         Variables can still be equal (like pandas objects) if they have NaN
1638         values in the same locations.
1639 
1640         This method is necessary because `v1 == v2` for Variables
1641         does element-wise comparisons (like numpy.ndarrays).
1642         """
1643         other = getattr(other, "variable", other)
1644         try:
1645             return self.dims == other.dims and (
1646                 self._data is other._data or equiv(self.data, other.data)
1647             )
1648         except (TypeError, AttributeError):
1649             return False
1650 
1651     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1652         """True if two Variables have the values after being broadcast against
1653         each other; otherwise False.
1654 
1655         Variables can still be equal (like pandas objects) if they have NaN
1656         values in the same locations.
1657         """
1658         try:
1659             self, other = broadcast_variables(self, other)
1660         except (ValueError, AttributeError):
1661             return False
1662         return self.equals(other, equiv=equiv)
1663 
1664     def identical(self, other, equiv=duck_array_ops.array_equiv):
1665         """Like equals, but also checks attributes.
1666         """
1667         try:
1668             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1669                 other, equiv=equiv
1670             )
1671         except (TypeError, AttributeError):
1672             return False
1673 
1674     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1675         """True if the intersection of two Variable's non-null data is
1676         equal; otherwise false.
1677 
1678         Variables can thus still be equal if there are locations where either,
1679         or both, contain NaN values.
1680         """
1681         return self.broadcast_equals(other, equiv=equiv)
1682 
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result
1764 
1765     def rank(self, dim, pct=False):
1766         """Ranks the data.
1767 
1768         Equal values are assigned a rank that is the average of the ranks that
1769         would have been otherwise assigned to all of the values within that
1770         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1771 
1772         NaNs in the input array are returned as NaNs.
1773 
1774         The `bottleneck` library is required.
1775 
1776         Parameters
1777         ----------
1778         dim : str
1779             Dimension over which to compute rank.
1780         pct : bool, optional
1781             If True, compute percentage ranks, otherwise compute integer ranks.
1782 
1783         Returns
1784         -------
1785         ranked : Variable
1786 
1787         See Also
1788         --------
1789         Dataset.rank, DataArray.rank
1790         """
1791         import bottleneck as bn
1792 
1793         data = self.data
1794 
1795         if isinstance(data, dask_array_type):
1796             raise TypeError(
1797                 "rank does not work for arrays stored as dask "
1798                 "arrays. Load the data via .compute() or .load() "
1799                 "prior to calling this method."
1800             )
1801         elif not isinstance(data, np.ndarray):
1802             raise TypeError(
1803                 "rank is not implemented for {} objects.".format(type(data))
1804             )
1805 
1806         axis = self.get_axis_num(dim)
1807         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1808         ranked = func(data, axis=axis)
1809         if pct:
1810             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1811             ranked /= count
1812         return Variable(self.dims, ranked)
1813 
1814     def rolling_window(
1815         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1816     ):
1817         """
1818         Make a rolling_window along dim and add a new_dim to the last place.
1819 
1820         Parameters
1821         ----------
1822         dim: str
1823             Dimension over which to compute rolling_window
1824         window: int
1825             Window size of the rolling
1826         window_dim: str
1827             New name of the window dimension.
1828         center: boolean. default False.
1829             If True, pad fill_value for both ends. Otherwise, pad in the head
1830             of the axis.
1831         fill_value:
1832             value to be filled.
1833 
1834         Returns
1835         -------
1836         Variable that is a view of the original array with a added dimension of
1837         size w.
1838         The return dim: self.dims + (window_dim, )
1839         The return shape: self.shape + (window, )
1840 
1841         Examples
1842         --------
1843         >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))
1844         >>> v.rolling_window(x, 'b', 3, 'window_dim')
1845         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1846         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1847                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1848 
1849         >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)
1850         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1851         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1852                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1853         """
1854         if fill_value is dtypes.NA:  # np.nan is passed
1855             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1856             array = self.astype(dtype, copy=False).data
1857         else:
1858             dtype = self.dtype
1859             array = self.data
1860 
1861         new_dims = self.dims + (window_dim,)
1862         return Variable(
1863             new_dims,
1864             duck_array_ops.rolling_window(
1865                 array,
1866                 axis=self.get_axis_num(dim),
1867                 window=window,
1868                 center=center,
1869                 fill_value=fill_value,
1870             ),
1871         )
1872 
1873     def coarsen(self, windows, func, boundary="exact", side="left", **kwargs):
1874         """
1875         Apply reduction function.
1876         """
1877         windows = {k: v for k, v in windows.items() if k in self.dims}
1878         if not windows:
1879             return self.copy()
1880 
1881         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1882         if isinstance(func, str):
1883             name = func
1884             func = getattr(duck_array_ops, name, None)
1885             if func is None:
1886                 raise NameError(f"{name} is not a valid method.")
1887         return self._replace(data=func(reshaped, axis=axes, **kwargs))
1888 
1889     def _coarsen_reshape(self, windows, boundary, side):
1890         """
1891         Construct a reshaped-array for coarsen
1892         """
1893         if not utils.is_dict_like(boundary):
1894             boundary = {d: boundary for d in windows.keys()}
1895 
1896         if not utils.is_dict_like(side):
1897             side = {d: side for d in windows.keys()}
1898 
1899         # remove unrelated dimensions
1900         boundary = {k: v for k, v in boundary.items() if k in windows}
1901         side = {k: v for k, v in side.items() if k in windows}
1902 
1903         for d, window in windows.items():
1904             if window <= 0:
1905                 raise ValueError(f"window must be > 0. Given {window}")
1906 
1907         variable = self
1908         for d, window in windows.items():
1909             # trim or pad the object
1910             size = variable.shape[self._get_axis_num(d)]
1911             n = int(size / window)
1912             if boundary[d] == "exact":
1913                 if n * window != size:
1914                     raise ValueError(
1915                         "Could not coarsen a dimension of size {} with "
1916                         "window {}".format(size, window)
1917                     )
1918             elif boundary[d] == "trim":
1919                 if side[d] == "left":
1920                     variable = variable.isel({d: slice(0, window * n)})
1921                 else:
1922                     excess = size - window * n
1923                     variable = variable.isel({d: slice(excess, None)})
1924             elif boundary[d] == "pad":  # pad
1925                 pad = window * n - size
1926                 if pad < 0:
1927                     pad += window
1928                 if side[d] == "left":
1929                     pad_widths = {d: (0, pad)}
1930                 else:
1931                     pad_widths = {d: (pad, 0)}
1932                 variable = variable.pad_with_fill_value(pad_widths)
1933             else:
1934                 raise TypeError(
1935                     "{} is invalid for boundary. Valid option is 'exact', "
1936                     "'trim' and 'pad'".format(boundary[d])
1937                 )
1938 
1939         shape = []
1940         axes = []
1941         axis_count = 0
1942         for i, d in enumerate(variable.dims):
1943             if d in windows:
1944                 size = variable.shape[i]
1945                 shape.append(int(size / windows[d]))
1946                 shape.append(windows[d])
1947                 axis_count += 1
1948                 axes.append(i + axis_count)
1949             else:
1950                 shape.append(variable.shape[i])
1951 
1952         return variable.data.reshape(shape), tuple(axes)
1953 
1954     @property
1955     def real(self):
1956         return type(self)(self.dims, self.data.real, self._attrs)
1957 
1958     @property
1959     def imag(self):
1960         return type(self)(self.dims, self.data.imag, self._attrs)
1961 
1962     def __array_wrap__(self, obj, context=None):
1963         return Variable(self.dims, obj)
1964 
1965     @staticmethod
1966     def _unary_op(f):
1967         @functools.wraps(f)
1968         def func(self, *args, **kwargs):
1969             with np.errstate(all="ignore"):
1970                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1971 
1972         return func
1973 
1974     @staticmethod
1975     def _binary_op(f, reflexive=False, **ignored_kwargs):
1976         @functools.wraps(f)
1977         def func(self, other):
1978             if isinstance(other, (xr.DataArray, xr.Dataset)):
1979                 return NotImplemented
1980             self_data, other_data, dims = _broadcast_compat_data(self, other)
1981             keep_attrs = _get_keep_attrs(default=False)
1982             attrs = self._attrs if keep_attrs else None
1983             with np.errstate(all="ignore"):
1984                 new_data = (
1985                     f(self_data, other_data)
1986                     if not reflexive
1987                     else f(other_data, self_data)
1988                 )
1989             result = Variable(dims, new_data, attrs=attrs)
1990             return result
1991 
1992         return func
1993 
1994     @staticmethod
1995     def _inplace_binary_op(f):
1996         @functools.wraps(f)
1997         def func(self, other):
1998             if isinstance(other, xr.Dataset):
1999                 raise TypeError("cannot add a Dataset to a Variable in-place")
2000             self_data, other_data, dims = _broadcast_compat_data(self, other)
2001             if dims != self.dims:
2002                 raise ValueError("dimensions cannot change for in-place " "operations")
2003             with np.errstate(all="ignore"):
2004                 self.values = f(self_data, other_data)
2005             return self
2006 
2007         return func
2008 
2009     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2010         """ A (private) method to convert datetime array to numeric dtype
2011         See duck_array_ops.datetime_to_numeric
2012         """
2013         numeric_array = duck_array_ops.datetime_to_numeric(
2014             self.data, offset, datetime_unit, dtype
2015         )
2016         return type(self)(self.dims, numeric_array, self._attrs)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 21:37:44.345 | INFO     | Search completed. Bug locations: [<file>xarray/core/variable.py</file>
<class>Variable</class>
<method>quantile</method>
<code>
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result

</code><intended_behavior>The method should convert quantile inputs from the [0, 1] range to [0, 100] before passing them to np.nanpercentile to ensure compatibility with numpy's percentile requirements while maintaining xarray's API contract. Additionally, the error message for out-of-range values should reflect that 'Quantiles must be in the range [0, 1]' to align with xarray's expected input range.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
263 class Variable(
264     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
265 ):
266     """A netcdf-like variable consisting of dimensions, data and attributes
267     which describe a single Array. A single Variable object is not fully
268     described outside the context of its parent Dataset (if you want such a
269     fully described object, use a DataArray instead).
270 
271     The main functional difference between Variables and numpy arrays is that
272     numerical operations on Variables implement array broadcasting by dimension
273     name. For example, adding an Variable with dimensions `('time',)` to
274     another Variable with dimensions `('space',)` results in a new Variable
275     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
276     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
277     instead of an "axis".
278 
279     Variables are light-weight objects used as the building block for datasets.
280     They are more primitive objects, so operations with them provide marginally
281     higher performance than using DataArrays. However, manipulating data in the
282     form of a Dataset or DataArray should almost always be preferred, because
283     they can use more complete metadata in context of coordinate labels.
284     """
285 
286     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
287 
288     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
289         """
290         Parameters
291         ----------
292         dims : str or sequence of str
293             Name(s) of the the data dimension(s). Must be either a string (only
294             for 1D data) or a sequence of strings with length equal to the
295             number of dimensions.
296         data : array_like
297             Data array which supports numpy-like data access.
298         attrs : dict_like or None, optional
299             Attributes to assign to the new variable. If None (default), an
300             empty attribute dictionary is initialized.
301         encoding : dict_like or None, optional
302             Dictionary specifying how to encode this array's data into a
303             serialized format like netCDF4. Currently used keys (for netCDF)
304             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
305             Well-behaved code to serialize a Variable should ignore
306             unrecognized encoding items.
307         """
308         self._data = as_compatible_data(data, fastpath=fastpath)
309         self._dims = self._parse_dimensions(dims)
310         self._attrs = None
311         self._encoding = None
312         if attrs is not None:
313             self.attrs = attrs
314         if encoding is not None:
315             self.encoding = encoding
316 
317     @property
318     def dtype(self):
319         return self._data.dtype
320 
321     @property
322     def shape(self):
323         return self._data.shape
324 
325     @property
326     def nbytes(self):
327         return self.size * self.dtype.itemsize
328 
329     @property
330     def _in_memory(self):
331         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
332             isinstance(self._data, indexing.MemoryCachedArray)
333             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
334         )
335 
336     @property
337     def data(self):
338         if hasattr(self._data, "__array_function__") or isinstance(
339             self._data, dask_array_type
340         ):
341             return self._data
342         else:
343             return self.values
344 
345     @data.setter
346     def data(self, data):
347         data = as_compatible_data(data)
348         if data.shape != self.shape:
349             raise ValueError(
350                 f"replacement data must match the Variable's shape. "
351                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
352             )
353         self._data = data
354 
355     def load(self, **kwargs):
356         """Manually trigger loading of this variable's data from disk or a
357         remote source into memory and return this variable.
358 
359         Normally, it should not be necessary to call this method in user code,
360         because all xarray functions should either work on deferred data or
361         load data automatically.
362 
363         Parameters
364         ----------
365         **kwargs : dict
366             Additional keyword arguments passed on to ``dask.array.compute``.
367 
368         See Also
369         --------
370         dask.array.compute
371         """
372         if isinstance(self._data, dask_array_type):
373             self._data = as_compatible_data(self._data.compute(**kwargs))
374         elif not hasattr(self._data, "__array_function__"):
375             self._data = np.asarray(self._data)
376         return self
377 
378     def compute(self, **kwargs):
379         """Manually trigger loading of this variable's data from disk or a
380         remote source into memory and return a new variable. The original is
381         left unaltered.
382 
383         Normally, it should not be necessary to call this method in user code,
384         because all xarray functions should either work on deferred data or
385         load data automatically.
386 
387         Parameters
388         ----------
389         **kwargs : dict
390             Additional keyword arguments passed on to ``dask.array.compute``.
391 
392         See Also
393         --------
394         dask.array.compute
395         """
396         new = self.copy(deep=False)
397         return new.load(**kwargs)
398 
399     def __dask_tokenize__(self):
400         # Use v.data, instead of v._data, in order to cope with the wrappers
401         # around NetCDF and the like
402         from dask.base import normalize_token
403 
404         return normalize_token((type(self), self._dims, self.data, self._attrs))
405 
406     def __dask_graph__(self):
407         if isinstance(self._data, dask_array_type):
408             return self._data.__dask_graph__()
409         else:
410             return None
411 
412     def __dask_keys__(self):
413         return self._data.__dask_keys__()
414 
415     def __dask_layers__(self):
416         return self._data.__dask_layers__()
417 
418     @property
419     def __dask_optimize__(self):
420         return self._data.__dask_optimize__
421 
422     @property
423     def __dask_scheduler__(self):
424         return self._data.__dask_scheduler__
425 
426     def __dask_postcompute__(self):
427         array_func, array_args = self._data.__dask_postcompute__()
428         return (
429             self._dask_finalize,
430             (array_func, array_args, self._dims, self._attrs, self._encoding),
431         )
432 
433     def __dask_postpersist__(self):
434         array_func, array_args = self._data.__dask_postpersist__()
435         return (
436             self._dask_finalize,
437             (array_func, array_args, self._dims, self._attrs, self._encoding),
438         )
439 
440     @staticmethod
441     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
442         if isinstance(results, dict):  # persist case
443             name = array_args[0]
444             results = {k: v for k, v in results.items() if k[0] == name}
445         data = array_func(results, *array_args)
446         return Variable(dims, data, attrs=attrs, encoding=encoding)
447 
448     @property
449     def values(self):
450         """The variable's data as a numpy.ndarray"""
451         return _as_array_or_item(self._data)
452 
453     @values.setter
454     def values(self, values):
455         self.data = values
456 
457     def to_base_variable(self):
458         """Return this variable as a base xarray.Variable"""
459         return Variable(
460             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
461         )
462 
463     to_variable = utils.alias(to_base_variable, "to_variable")
464 
465     def to_index_variable(self):
466         """Return this variable as an xarray.IndexVariable"""
467         return IndexVariable(
468             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
469         )
470 
471     to_coord = utils.alias(to_index_variable, "to_coord")
472 
473     def to_index(self):
474         """Convert this variable to a pandas.Index"""
475         return self.to_index_variable().to_index()
476 
477     def to_dict(self, data=True):
478         """Dictionary representation of variable."""
479         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
480         if data:
481             item["data"] = ensure_us_time_resolution(self.values).tolist()
482         else:
483             item.update({"dtype": str(self.dtype), "shape": self.shape})
484         return item
485 
486     @property
487     def dims(self):
488         """Tuple of dimension names with which this variable is associated.
489         """
490         return self._dims
491 
492     @dims.setter
493     def dims(self, value):
494         self._dims = self._parse_dimensions(value)
495 
496     def _parse_dimensions(self, dims):
497         if isinstance(dims, str):
498             dims = (dims,)
499         dims = tuple(dims)
500         if len(dims) != self.ndim:
501             raise ValueError(
502                 "dimensions %s must have the same length as the "
503                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
504             )
505         return dims
506 
507     def _item_key_to_tuple(self, key):
508         if utils.is_dict_like(key):
509             return tuple(key.get(dim, slice(None)) for dim in self.dims)
510         else:
511             return key
512 
513     def _broadcast_indexes(self, key):
514         """Prepare an indexing key for an indexing operation.
515 
516         Parameters
517         -----------
518         key: int, slice, array, dict or tuple of integer, slices and arrays
519             Any valid input for indexing.
520 
521         Returns
522         -------
523         dims: tuple
524             Dimension of the resultant variable.
525         indexers: IndexingTuple subclass
526             Tuple of integer, array-like, or slices to use when indexing
527             self._data. The type of this argument indicates the type of
528             indexing to perform, either basic, outer or vectorized.
529         new_order : Optional[Sequence[int]]
530             Optional reordering to do on the result of indexing. If not None,
531             the first len(new_order) indexing should be moved to these
532             positions.
533         """
534         key = self._item_key_to_tuple(key)  # key is a tuple
535         # key is a tuple of full size
536         key = indexing.expanded_indexer(key, self.ndim)
537         # Convert a scalar Variable to an integer
538         key = tuple(
539             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
540         )
541         # Convert a 0d-array to an integer
542         key = tuple(
543             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
544         )
545 
546         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
547             return self._broadcast_indexes_basic(key)
548 
549         self._validate_indexers(key)
550         # Detect it can be mapped as an outer indexer
551         # If all key is unlabeled, or
552         # key can be mapped as an OuterIndexer.
553         if all(not isinstance(k, Variable) for k in key):
554             return self._broadcast_indexes_outer(key)
555 
556         # If all key is 1-dimensional and there are no duplicate labels,
557         # key can be mapped as an OuterIndexer.
558         dims = []
559         for k, d in zip(key, self.dims):
560             if isinstance(k, Variable):
561                 if len(k.dims) > 1:
562                     return self._broadcast_indexes_vectorized(key)
563                 dims.append(k.dims[0])
564             elif not isinstance(k, integer_types):
565                 dims.append(d)
566         if len(set(dims)) == len(dims):
567             return self._broadcast_indexes_outer(key)
568 
569         return self._broadcast_indexes_vectorized(key)
570 
571     def _broadcast_indexes_basic(self, key):
572         dims = tuple(
573             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
574         )
575         return dims, BasicIndexer(key), None
576 
577     def _validate_indexers(self, key):
578         """ Make sanity checks """
579         for dim, k in zip(self.dims, key):
580             if isinstance(k, BASIC_INDEXING_TYPES):
581                 pass
582             else:
583                 if not isinstance(k, Variable):
584                     k = np.asarray(k)
585                     if k.ndim > 1:
586                         raise IndexError(
587                             "Unlabeled multi-dimensional array cannot be "
588                             "used for indexing: {}".format(k)
589                         )
590                 if k.dtype.kind == "b":
591                     if self.shape[self.get_axis_num(dim)] != len(k):
592                         raise IndexError(
593                             "Boolean array size {:d} is used to index array "
594                             "with shape {:s}.".format(len(k), str(self.shape))
595                         )
596                     if k.ndim > 1:
597                         raise IndexError(
598                             "{}-dimensional boolean indexing is "
599                             "not supported. ".format(k.ndim)
600                         )
601                     if getattr(k, "dims", (dim,)) != (dim,):
602                         raise IndexError(
603                             "Boolean indexer should be unlabeled or on the "
604                             "same dimension to the indexed array. Indexer is "
605                             "on {:s} but the target dimension is {:s}.".format(
606                                 str(k.dims), dim
607                             )
608                         )
609 
610     def _broadcast_indexes_outer(self, key):
611         dims = tuple(
612             k.dims[0] if isinstance(k, Variable) else dim
613             for k, dim in zip(key, self.dims)
614             if not isinstance(k, integer_types)
615         )
616 
617         new_key = []
618         for k in key:
619             if isinstance(k, Variable):
620                 k = k.data
621             if not isinstance(k, BASIC_INDEXING_TYPES):
622                 k = np.asarray(k)
623                 if k.size == 0:
624                     # Slice by empty list; numpy could not infer the dtype
625                     k = k.astype(int)
626                 elif k.dtype.kind == "b":
627                     (k,) = np.nonzero(k)
628             new_key.append(k)
629 
630         return dims, OuterIndexer(tuple(new_key)), None
631 
632     def _nonzero(self):
633         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
634         # TODO we should replace dask's native nonzero
635         # after https://github.com/dask/dask/issues/1076 is implemented.
636         nonzeros = np.nonzero(self.data)
637         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
638 
639     def _broadcast_indexes_vectorized(self, key):
640         variables = []
641         out_dims_set = OrderedSet()
642         for dim, value in zip(self.dims, key):
643             if isinstance(value, slice):
644                 out_dims_set.add(dim)
645             else:
646                 variable = (
647                     value
648                     if isinstance(value, Variable)
649                     else as_variable(value, name=dim)
650                 )
651                 if variable.dtype.kind == "b":  # boolean indexing case
652                     (variable,) = variable._nonzero()
653 
654                 variables.append(variable)
655                 out_dims_set.update(variable.dims)
656 
657         variable_dims = set()
658         for variable in variables:
659             variable_dims.update(variable.dims)
660 
661         slices = []
662         for i, (dim, value) in enumerate(zip(self.dims, key)):
663             if isinstance(value, slice):
664                 if dim in variable_dims:
665                     # We only convert slice objects to variables if they share
666                     # a dimension with at least one other variable. Otherwise,
667                     # we can equivalently leave them as slices aknd transpose
668                     # the result. This is significantly faster/more efficient
669                     # for most array backends.
670                     values = np.arange(*value.indices(self.sizes[dim]))
671                     variables.insert(i - len(slices), Variable((dim,), values))
672                 else:
673                     slices.append((i, value))
674 
675         try:
676             variables = _broadcast_compat_variables(*variables)
677         except ValueError:
678             raise IndexError(f"Dimensions of indexers mismatch: {key}")
679 
680         out_key = [variable.data for variable in variables]
681         out_dims = tuple(out_dims_set)
682         slice_positions = set()
683         for i, value in slices:
684             out_key.insert(i, value)
685             new_position = out_dims.index(self.dims[i])
686             slice_positions.add(new_position)
687 
688         if slice_positions:
689             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
690         else:
691             new_order = None
692 
693         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
694 
695     def __getitem__(self: VariableType, key) -> VariableType:
696         """Return a new Variable object whose contents are consistent with
697         getting the provided key from the underlying data.
698 
699         NB. __getitem__ and __setitem__ implement xarray-style indexing,
700         where if keys are unlabeled arrays, we index the array orthogonally
701         with them. If keys are labeled array (such as Variables), they are
702         broadcasted with our usual scheme and then the array is indexed with
703         the broadcasted key, like numpy's fancy indexing.
704 
705         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
706         array `x.values` directly.
707         """
708         dims, indexer, new_order = self._broadcast_indexes(key)
709         data = as_indexable(self._data)[indexer]
710         if new_order:
711             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
712         return self._finalize_indexing_result(dims, data)
713 
714     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
715         """Used by IndexVariable to return IndexVariable objects when possible.
716         """
717         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
718 
719     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
720         """Index this Variable with -1 remapped to fill_value."""
721         # TODO(shoyer): expose this method in public API somewhere (isel?) and
722         # use it for reindex.
723         # TODO(shoyer): add a sanity check that all other integers are
724         # non-negative
725         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
726         # that is actually indexed rather than mapping it to the last value
727         # along each axis.
728 
729         if fill_value is dtypes.NA:
730             fill_value = dtypes.get_fill_value(self.dtype)
731 
732         dims, indexer, new_order = self._broadcast_indexes(key)
733 
734         if self.size:
735             if isinstance(self._data, dask_array_type):
736                 # dask's indexing is faster this way; also vindex does not
737                 # support negative indices yet:
738                 # https://github.com/dask/dask/pull/2967
739                 actual_indexer = indexing.posify_mask_indexer(indexer)
740             else:
741                 actual_indexer = indexer
742 
743             data = as_indexable(self._data)[actual_indexer]
744             mask = indexing.create_mask(indexer, self.shape, data)
745             data = duck_array_ops.where(mask, fill_value, data)
746         else:
747             # array cannot be indexed along dimensions of size 0, so just
748             # build the mask directly instead.
749             mask = indexing.create_mask(indexer, self.shape)
750             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
751 
752         if new_order:
753             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
754         return self._finalize_indexing_result(dims, data)
755 
756     def __setitem__(self, key, value):
757         """__setitem__ is overloaded to access the underlying numpy values with
758         orthogonal indexing.
759 
760         See __getitem__ for more details.
761         """
762         dims, index_tuple, new_order = self._broadcast_indexes(key)
763 
764         if not isinstance(value, Variable):
765             value = as_compatible_data(value)
766             if value.ndim > len(dims):
767                 raise ValueError(
768                     "shape mismatch: value array of shape %s could not be "
769                     "broadcast to indexing result with %s dimensions"
770                     % (value.shape, len(dims))
771                 )
772             if value.ndim == 0:
773                 value = Variable((), value)
774             else:
775                 value = Variable(dims[-value.ndim :], value)
776         # broadcast to become assignable
777         value = value.set_dims(dims).data
778 
779         if new_order:
780             value = duck_array_ops.asarray(value)
781             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
782             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
783 
784         indexable = as_indexable(self._data)
785         indexable[index_tuple] = value
786 
787     @property
788     def attrs(self) -> Dict[Hashable, Any]:
789         """Dictionary of local attributes on this variable.
790         """
791         if self._attrs is None:
792             self._attrs = {}
793         return self._attrs
794 
795     @attrs.setter
796     def attrs(self, value: Mapping[Hashable, Any]) -> None:
797         self._attrs = dict(value)
798 
799     @property
800     def encoding(self):
801         """Dictionary of encodings on this variable.
802         """
803         if self._encoding is None:
804             self._encoding = {}
805         return self._encoding
806 
807     @encoding.setter
808     def encoding(self, value):
809         try:
810             self._encoding = dict(value)
811         except ValueError:
812             raise ValueError("encoding must be castable to a dictionary")
813 
814     def copy(self, deep=True, data=None):
815         """Returns a copy of this object.
816 
817         If `deep=True`, the data array is loaded into memory and copied onto
818         the new object. Dimensions, attributes and encodings are always copied.
819 
820         Use `data` to create a new object with the same structure as
821         original but entirely new data.
822 
823         Parameters
824         ----------
825         deep : bool, optional
826             Whether the data array is loaded into memory and copied onto
827             the new object. Default is True.
828         data : array_like, optional
829             Data to use in the new object. Must have same shape as original.
830             When `data` is used, `deep` is ignored.
831 
832         Returns
833         -------
834         object : Variable
835             New object with dimensions, attributes, encodings, and optionally
836             data copied from original.
837 
838         Examples
839         --------
840 
841         Shallow copy versus deep copy
842 
843         >>> var = xr.Variable(data=[1, 2, 3], dims='x')
844         >>> var.copy()
845         <xarray.Variable (x: 3)>
846         array([1, 2, 3])
847         >>> var_0 = var.copy(deep=False)
848         >>> var_0[0] = 7
849         >>> var_0
850         <xarray.Variable (x: 3)>
851         array([7, 2, 3])
852         >>> var
853         <xarray.Variable (x: 3)>
854         array([7, 2, 3])
855 
856         Changing the data using the ``data`` argument maintains the
857         structure of the original object, but with the new data. Original
858         object is unaffected.
859 
860         >>> var.copy(data=[0.1, 0.2, 0.3])
861         <xarray.Variable (x: 3)>
862         array([ 0.1,  0.2,  0.3])
863         >>> var
864         <xarray.Variable (x: 3)>
865         array([7, 2, 3])
866 
867         See Also
868         --------
869         pandas.DataFrame.copy
870         """
871         if data is None:
872             data = self._data
873 
874             if isinstance(data, indexing.MemoryCachedArray):
875                 # don't share caching between copies
876                 data = indexing.MemoryCachedArray(data.array)
877 
878             if deep:
879                 if hasattr(data, "__array_function__") or isinstance(
880                     data, dask_array_type
881                 ):
882                     data = data.copy()
883                 elif not isinstance(data, PandasIndexAdapter):
884                     # pandas.Index is immutable
885                     data = np.array(data)
886         else:
887             data = as_compatible_data(data)
888             if self.shape != data.shape:
889                 raise ValueError(
890                     "Data shape {} must match shape of object {}".format(
891                         data.shape, self.shape
892                     )
893                 )
894 
895         # note:
896         # dims is already an immutable tuple
897         # attributes and encoding will be copied when the new Array is created
898         return self._replace(data=data)
899 
900     def _replace(
901         self, dims=_default, data=_default, attrs=_default, encoding=_default
902     ) -> "Variable":
903         if dims is _default:
904             dims = copy.copy(self._dims)
905         if data is _default:
906             data = copy.copy(self.data)
907         if attrs is _default:
908             attrs = copy.copy(self._attrs)
909         if encoding is _default:
910             encoding = copy.copy(self._encoding)
911         return type(self)(dims, data, attrs, encoding, fastpath=True)
912 
913     def __copy__(self):
914         return self.copy(deep=False)
915 
916     def __deepcopy__(self, memo=None):
917         # memo does nothing but is required for compatibility with
918         # copy.deepcopy
919         return self.copy(deep=True)
920 
921     # mutable objects should not be hashable
922     # https://github.com/python/mypy/issues/4266
923     __hash__ = None  # type: ignore
924 
925     @property
926     def chunks(self):
927         """Block dimensions for this array's data or None if it's not a dask
928         array.
929         """
930         return getattr(self._data, "chunks", None)
931 
932     _array_counter = itertools.count()
933 
934     def chunk(self, chunks=None, name=None, lock=False):
935         """Coerce this array's data into a dask arrays with the given chunks.
936 
937         If this variable is a non-dask array, it will be converted to dask
938         array. If it's a dask array, it will be rechunked to the given chunk
939         sizes.
940 
941         If neither chunks is not provided for one or more dimensions, chunk
942         sizes along that dimension will not be updated; non-dask arrays will be
943         converted into dask arrays with a single block.
944 
945         Parameters
946         ----------
947         chunks : int, tuple or dict, optional
948             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
949             ``{'x': 5, 'y': 5}``.
950         name : str, optional
951             Used to generate the name for this array in the internal dask
952             graph. Does not need not be unique.
953         lock : optional
954             Passed on to :py:func:`dask.array.from_array`, if the array is not
955             already as dask array.
956 
957         Returns
958         -------
959         chunked : xarray.Variable
960         """
961         import dask
962         import dask.array as da
963 
964         if utils.is_dict_like(chunks):
965             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
966 
967         if chunks is None:
968             chunks = self.chunks or self.shape
969 
970         data = self._data
971         if isinstance(data, da.Array):
972             data = data.rechunk(chunks)
973         else:
974             if isinstance(data, indexing.ExplicitlyIndexed):
975                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
976                 # that can't handle general array indexing. For example, in netCDF4 you
977                 # can do "outer" indexing along two dimensions independent, which works
978                 # differently from how NumPy handles it.
979                 # da.from_array works by using lazy indexing with a tuple of slices.
980                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
981                 # different indexing types in an explicit way:
982                 # https://github.com/dask/dask/issues/2883
983                 data = indexing.ImplicitToExplicitIndexingAdapter(
984                     data, indexing.OuterIndexer
985                 )
986                 if LooseVersion(dask.__version__) < "2.0.0":
987                     kwargs = {}
988                 else:
989                     # All of our lazily loaded backend array classes should use NumPy
990                     # array operations.
991                     kwargs = {"meta": np.ndarray}
992             else:
993                 kwargs = {}
994 
995             if utils.is_dict_like(chunks):
996                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
997 
998             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
999 
1000         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1001 
1002     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1003         """
1004         use sparse-array as backend.
1005         """
1006         import sparse
1007 
1008         # TODO  what to do if dask-backended?
1009         if fill_value is dtypes.NA:
1010             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1011         else:
1012             dtype = dtypes.result_type(self.dtype, fill_value)
1013 
1014         if sparse_format is _default:
1015             sparse_format = "coo"
1016         try:
1017             as_sparse = getattr(sparse, "as_{}".format(sparse_format.lower()))
1018         except AttributeError:
1019             raise ValueError("{} is not a valid sparse format".format(sparse_format))
1020 
1021         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1022         return self._replace(data=data)
1023 
1024     def _to_dense(self):
1025         """
1026         Change backend from sparse to np.array
1027         """
1028         if hasattr(self._data, "todense"):
1029             return self._replace(data=self._data.todense())
1030         return self.copy(deep=False)
1031 
1032     def isel(
1033         self: VariableType,
1034         indexers: Mapping[Hashable, Any] = None,
1035         **indexers_kwargs: Any,
1036     ) -> VariableType:
1037         """Return a new array indexed along the specified dimension(s).
1038 
1039         Parameters
1040         ----------
1041         **indexers : {dim: indexer, ...}
1042             Keyword arguments with names matching dimensions and values given
1043             by integers, slice objects or arrays.
1044 
1045         Returns
1046         -------
1047         obj : Array object
1048             A new Array with the selected data and dimensions. In general,
1049             the new variable's data will be a view of this variable's data,
1050             unless numpy fancy indexing was triggered by using an array
1051             indexer, in which case the data will be a copy.
1052         """
1053         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1054 
1055         invalid = indexers.keys() - set(self.dims)
1056         if invalid:
1057             raise ValueError("dimensions %r do not exist" % invalid)
1058 
1059         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1060         return self[key]
1061 
1062     def squeeze(self, dim=None):
1063         """Return a new object with squeezed data.
1064 
1065         Parameters
1066         ----------
1067         dim : None or str or tuple of str, optional
1068             Selects a subset of the length one dimensions. If a dimension is
1069             selected with length greater than one, an error is raised. If
1070             None, all length one dimensions are squeezed.
1071 
1072         Returns
1073         -------
1074         squeezed : same type as caller
1075             This object, but with with all or a subset of the dimensions of
1076             length 1 removed.
1077 
1078         See Also
1079         --------
1080         numpy.squeeze
1081         """
1082         dims = common.get_squeeze_dims(self, dim)
1083         return self.isel({d: 0 for d in dims})
1084 
1085     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1086         axis = self.get_axis_num(dim)
1087 
1088         if count > 0:
1089             keep = slice(None, -count)
1090         elif count < 0:
1091             keep = slice(-count, None)
1092         else:
1093             keep = slice(None)
1094 
1095         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1096 
1097         if fill_value is dtypes.NA:
1098             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1099         else:
1100             dtype = self.dtype
1101 
1102         shape = list(self.shape)
1103         shape[axis] = min(abs(count), shape[axis])
1104 
1105         if isinstance(trimmed_data, dask_array_type):
1106             chunks = list(trimmed_data.chunks)
1107             chunks[axis] = (shape[axis],)
1108             full = functools.partial(da.full, chunks=chunks)
1109         else:
1110             full = np.full
1111 
1112         filler = full(shape, fill_value, dtype=dtype)
1113 
1114         if count > 0:
1115             arrays = [filler, trimmed_data]
1116         else:
1117             arrays = [trimmed_data, filler]
1118 
1119         data = duck_array_ops.concatenate(arrays, axis)
1120 
1121         if isinstance(data, dask_array_type):
1122             # chunked data should come out with the same chunks; this makes
1123             # it feasible to combine shifted and unshifted data
1124             # TODO: remove this once dask.array automatically aligns chunks
1125             data = data.rechunk(self.data.chunks)
1126 
1127         return type(self)(self.dims, data, self._attrs, fastpath=True)
1128 
1129     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1130         """
1131         Return a new Variable with shifted data.
1132 
1133         Parameters
1134         ----------
1135         shifts : mapping of the form {dim: offset}
1136             Integer offset to shift along each of the given dimensions.
1137             Positive offsets shift to the right; negative offsets shift to the
1138             left.
1139         fill_value: scalar, optional
1140             Value to use for newly missing values
1141         **shifts_kwargs:
1142             The keyword arguments form of ``shifts``.
1143             One of shifts or shifts_kwargs must be provided.
1144 
1145         Returns
1146         -------
1147         shifted : Variable
1148             Variable with the same dimensions and attributes but shifted data.
1149         """
1150         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1151         result = self
1152         for dim, count in shifts.items():
1153             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1154         return result
1155 
1156     def pad_with_fill_value(
1157         self, pad_widths=None, fill_value=dtypes.NA, **pad_widths_kwargs
1158     ):
1159         """
1160         Return a new Variable with paddings.
1161 
1162         Parameters
1163         ----------
1164         pad_width: Mapping of the form {dim: (before, after)}
1165             Number of values padded to the edges of each dimension.
1166         **pad_widths_kwargs:
1167             Keyword argument for pad_widths
1168         """
1169         pad_widths = either_dict_or_kwargs(pad_widths, pad_widths_kwargs, "pad")
1170 
1171         if fill_value is dtypes.NA:
1172             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1173         else:
1174             dtype = self.dtype
1175 
1176         if isinstance(self.data, dask_array_type):
1177             array = self.data
1178 
1179             # Dask does not yet support pad. We manually implement it.
1180             # https://github.com/dask/dask/issues/1926
1181             for d, pad in pad_widths.items():
1182                 axis = self.get_axis_num(d)
1183                 before_shape = list(array.shape)
1184                 before_shape[axis] = pad[0]
1185                 before_chunks = list(array.chunks)
1186                 before_chunks[axis] = (pad[0],)
1187                 after_shape = list(array.shape)
1188                 after_shape[axis] = pad[1]
1189                 after_chunks = list(array.chunks)
1190                 after_chunks[axis] = (pad[1],)
1191 
1192                 arrays = []
1193                 if pad[0] > 0:
1194                     arrays.append(
1195                         da.full(
1196                             before_shape, fill_value, dtype=dtype, chunks=before_chunks
1197                         )
1198                     )
1199                 arrays.append(array)
1200                 if pad[1] > 0:
1201                     arrays.append(
1202                         da.full(
1203                             after_shape, fill_value, dtype=dtype, chunks=after_chunks
1204                         )
1205                     )
1206                 if len(arrays) > 1:
1207                     array = da.concatenate(arrays, axis=axis)
1208         else:
1209             pads = [(0, 0) if d not in pad_widths else pad_widths[d] for d in self.dims]
1210             array = np.pad(
1211                 self.data.astype(dtype, copy=False),
1212                 pads,
1213                 mode="constant",
1214                 constant_values=fill_value,
1215             )
1216         return type(self)(self.dims, array)
1217 
1218     def _roll_one_dim(self, dim, count):
1219         axis = self.get_axis_num(dim)
1220 
1221         count %= self.shape[axis]
1222         if count != 0:
1223             indices = [slice(-count, None), slice(None, -count)]
1224         else:
1225             indices = [slice(None)]
1226 
1227         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1228 
1229         data = duck_array_ops.concatenate(arrays, axis)
1230 
1231         if isinstance(data, dask_array_type):
1232             # chunked data should come out with the same chunks; this makes
1233             # it feasible to combine shifted and unshifted data
1234             # TODO: remove this once dask.array automatically aligns chunks
1235             data = data.rechunk(self.data.chunks)
1236 
1237         return type(self)(self.dims, data, self._attrs, fastpath=True)
1238 
1239     def roll(self, shifts=None, **shifts_kwargs):
1240         """
1241         Return a new Variable with rolld data.
1242 
1243         Parameters
1244         ----------
1245         shifts : mapping of the form {dim: offset}
1246             Integer offset to roll along each of the given dimensions.
1247             Positive offsets roll to the right; negative offsets roll to the
1248             left.
1249         **shifts_kwargs:
1250             The keyword arguments form of ``shifts``.
1251             One of shifts or shifts_kwargs must be provided.
1252 
1253         Returns
1254         -------
1255         shifted : Variable
1256             Variable with the same dimensions and attributes but rolled data.
1257         """
1258         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1259 
1260         result = self
1261         for dim, count in shifts.items():
1262             result = result._roll_one_dim(dim, count)
1263         return result
1264 
1265     def transpose(self, *dims) -> "Variable":
1266         """Return a new Variable object with transposed dimensions.
1267 
1268         Parameters
1269         ----------
1270         *dims : str, optional
1271             By default, reverse the dimensions. Otherwise, reorder the
1272             dimensions to this order.
1273 
1274         Returns
1275         -------
1276         transposed : Variable
1277             The returned object has transposed data and dimensions with the
1278             same attributes as the original.
1279 
1280         Notes
1281         -----
1282         This operation returns a view of this variable's data. It is
1283         lazy for dask-backed Variables but not for numpy-backed Variables.
1284 
1285         See Also
1286         --------
1287         numpy.transpose
1288         """
1289         if len(dims) == 0:
1290             dims = self.dims[::-1]
1291         dims = tuple(infix_dims(dims, self.dims))
1292         axes = self.get_axis_num(dims)
1293         if len(dims) < 2 or dims == self.dims:
1294             # no need to transpose if only one dimension
1295             # or dims are in same order
1296             return self.copy(deep=False)
1297 
1298         data = as_indexable(self._data).transpose(axes)
1299         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1300 
1301     @property
1302     def T(self) -> "Variable":
1303         return self.transpose()
1304 
1305     def set_dims(self, dims, shape=None):
1306         """Return a new variable with given set of dimensions.
1307         This method might be used to attach new dimension(s) to variable.
1308 
1309         When possible, this operation does not copy this variable's data.
1310 
1311         Parameters
1312         ----------
1313         dims : str or sequence of str or dict
1314             Dimensions to include on the new variable. If a dict, values are
1315             used to provide the sizes of new dimensions; otherwise, new
1316             dimensions are inserted with length 1.
1317 
1318         Returns
1319         -------
1320         Variable
1321         """
1322         if isinstance(dims, str):
1323             dims = [dims]
1324 
1325         if shape is None and utils.is_dict_like(dims):
1326             shape = dims.values()
1327 
1328         missing_dims = set(self.dims) - set(dims)
1329         if missing_dims:
1330             raise ValueError(
1331                 "new dimensions %r must be a superset of "
1332                 "existing dimensions %r" % (dims, self.dims)
1333             )
1334 
1335         self_dims = set(self.dims)
1336         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1337 
1338         if self.dims == expanded_dims:
1339             # don't use broadcast_to unless necessary so the result remains
1340             # writeable if possible
1341             expanded_data = self.data
1342         elif shape is not None:
1343             dims_map = dict(zip(dims, shape))
1344             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1345             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1346         else:
1347             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1348 
1349         expanded_var = Variable(
1350             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1351         )
1352         return expanded_var.transpose(*dims)
1353 
1354     def _stack_once(self, dims, new_dim):
1355         if not set(dims) <= set(self.dims):
1356             raise ValueError("invalid existing dimensions: %s" % dims)
1357 
1358         if new_dim in self.dims:
1359             raise ValueError(
1360                 "cannot create a new dimension with the same "
1361                 "name as an existing dimension"
1362             )
1363 
1364         if len(dims) == 0:
1365             # don't stack
1366             return self.copy(deep=False)
1367 
1368         other_dims = [d for d in self.dims if d not in dims]
1369         dim_order = other_dims + list(dims)
1370         reordered = self.transpose(*dim_order)
1371 
1372         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1373         new_data = reordered.data.reshape(new_shape)
1374         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1375 
1376         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1377 
1378     def stack(self, dimensions=None, **dimensions_kwargs):
1379         """
1380         Stack any number of existing dimensions into a single new dimension.
1381 
1382         New dimensions will be added at the end, and the order of the data
1383         along each new dimension will be in contiguous (C) order.
1384 
1385         Parameters
1386         ----------
1387         dimensions : Mapping of form new_name=(dim1, dim2, ...)
1388             Names of new dimensions, and the existing dimensions that they
1389             replace.
1390         **dimensions_kwargs:
1391             The keyword arguments form of ``dimensions``.
1392             One of dimensions or dimensions_kwargs must be provided.
1393 
1394         Returns
1395         -------
1396         stacked : Variable
1397             Variable with the same attributes but stacked data.
1398 
1399         See also
1400         --------
1401         Variable.unstack
1402         """
1403         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1404         result = self
1405         for new_dim, dims in dimensions.items():
1406             result = result._stack_once(dims, new_dim)
1407         return result
1408 
1409     def _unstack_once(self, dims, old_dim):
1410         new_dim_names = tuple(dims.keys())
1411         new_dim_sizes = tuple(dims.values())
1412 
1413         if old_dim not in self.dims:
1414             raise ValueError("invalid existing dimension: %s" % old_dim)
1415 
1416         if set(new_dim_names).intersection(self.dims):
1417             raise ValueError(
1418                 "cannot create a new dimension with the same "
1419                 "name as an existing dimension"
1420             )
1421 
1422         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1423             raise ValueError(
1424                 "the product of the new dimension sizes must "
1425                 "equal the size of the old dimension"
1426             )
1427 
1428         other_dims = [d for d in self.dims if d != old_dim]
1429         dim_order = other_dims + [old_dim]
1430         reordered = self.transpose(*dim_order)
1431 
1432         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1433         new_data = reordered.data.reshape(new_shape)
1434         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1435 
1436         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1437 
1438     def unstack(self, dimensions=None, **dimensions_kwargs):
1439         """
1440         Unstack an existing dimension into multiple new dimensions.
1441 
1442         New dimensions will be added at the end, and the order of the data
1443         along each new dimension will be in contiguous (C) order.
1444 
1445         Parameters
1446         ----------
1447         dimensions : mapping of the form old_dim={dim1: size1, ...}
1448             Names of existing dimensions, and the new dimensions and sizes
1449             that they map to.
1450         **dimensions_kwargs:
1451             The keyword arguments form of ``dimensions``.
1452             One of dimensions or dimensions_kwargs must be provided.
1453 
1454         Returns
1455         -------
1456         unstacked : Variable
1457             Variable with the same attributes but unstacked data.
1458 
1459         See also
1460         --------
1461         Variable.stack
1462         """
1463         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1464         result = self
1465         for old_dim, dims in dimensions.items():
1466             result = result._unstack_once(dims, old_dim)
1467         return result
1468 
1469     def fillna(self, value):
1470         return ops.fillna(self, value)
1471 
1472     def where(self, cond, other=dtypes.NA):
1473         return ops.where_method(self, cond, other)
1474 
1475     def reduce(
1476         self,
1477         func,
1478         dim=None,
1479         axis=None,
1480         keep_attrs=None,
1481         keepdims=False,
1482         allow_lazy=None,
1483         **kwargs,
1484     ):
1485         """Reduce this array by applying `func` along some dimension(s).
1486 
1487         Parameters
1488         ----------
1489         func : function
1490             Function which can be called in the form
1491             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1492             np.ndarray over an integer valued axis.
1493         dim : str or sequence of str, optional
1494             Dimension(s) over which to apply `func`.
1495         axis : int or sequence of int, optional
1496             Axis(es) over which to apply `func`. Only one of the 'dim'
1497             and 'axis' arguments can be supplied. If neither are supplied, then
1498             the reduction is calculated over the flattened array (by calling
1499             `func(x)` without an axis argument).
1500         keep_attrs : bool, optional
1501             If True, the variable's attributes (`attrs`) will be copied from
1502             the original object to the new one.  If False (default), the new
1503             object will be returned without attributes.
1504         keepdims : bool, default False
1505             If True, the dimensions which are reduced are left in the result
1506             as dimensions of size one
1507         **kwargs : dict
1508             Additional keyword arguments passed on to `func`.
1509 
1510         Returns
1511         -------
1512         reduced : Array
1513             Array with summarized data and the indicated dimension(s)
1514             removed.
1515         """
1516         if dim == ...:
1517             dim = None
1518         if dim is not None and axis is not None:
1519             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1520 
1521         if dim is not None:
1522             axis = self.get_axis_num(dim)
1523 
1524         if allow_lazy is not None:
1525             warnings.warn(
1526                 "allow_lazy is deprecated and will be removed in version 0.16.0. It is now True by default.",
1527                 DeprecationWarning,
1528             )
1529         else:
1530             allow_lazy = True
1531 
1532         input_data = self.data if allow_lazy else self.values
1533 
1534         if axis is not None:
1535             data = func(input_data, axis=axis, **kwargs)
1536         else:
1537             data = func(input_data, **kwargs)
1538 
1539         if getattr(data, "shape", ()) == self.shape:
1540             dims = self.dims
1541         else:
1542             removed_axes = (
1543                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1544             )
1545             if keepdims:
1546                 # Insert np.newaxis for removed dims
1547                 slices = tuple(
1548                     np.newaxis if i in removed_axes else slice(None, None)
1549                     for i in range(self.ndim)
1550                 )
1551                 if getattr(data, "shape", None) is None:
1552                     # Reduce has produced a scalar value, not an array-like
1553                     data = np.asanyarray(data)[slices]
1554                 else:
1555                     data = data[slices]
1556                 dims = self.dims
1557             else:
1558                 dims = [
1559                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1560                 ]
1561 
1562         if keep_attrs is None:
1563             keep_attrs = _get_keep_attrs(default=False)
1564         attrs = self._attrs if keep_attrs else None
1565 
1566         return Variable(dims, data, attrs=attrs)
1567 
1568     @classmethod
1569     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1570         """Concatenate variables along a new or existing dimension.
1571 
1572         Parameters
1573         ----------
1574         variables : iterable of Array
1575             Arrays to stack together. Each variable is expected to have
1576             matching dimensions and shape except for along the stacked
1577             dimension.
1578         dim : str or DataArray, optional
1579             Name of the dimension to stack along. This can either be a new
1580             dimension name, in which case it is added along axis=0, or an
1581             existing dimension name, in which case the location of the
1582             dimension is unchanged. Where to insert the new dimension is
1583             determined by the first variable.
1584         positions : None or list of integer arrays, optional
1585             List of integer arrays which specifies the integer positions to
1586             which to assign each dataset along the concatenated dimension.
1587             If not supplied, objects are concatenated in the provided order.
1588         shortcut : bool, optional
1589             This option is used internally to speed-up groupby operations.
1590             If `shortcut` is True, some checks of internal consistency between
1591             arrays to concatenate are skipped.
1592 
1593         Returns
1594         -------
1595         stacked : Variable
1596             Concatenated Variable formed by stacking all the supplied variables
1597             along the given dimension.
1598         """
1599         if not isinstance(dim, str):
1600             (dim,) = dim.dims
1601 
1602         # can't do this lazily: we need to loop through variables at least
1603         # twice
1604         variables = list(variables)
1605         first_var = variables[0]
1606 
1607         arrays = [v.data for v in variables]
1608 
1609         if dim in first_var.dims:
1610             axis = first_var.get_axis_num(dim)
1611             dims = first_var.dims
1612             data = duck_array_ops.concatenate(arrays, axis=axis)
1613             if positions is not None:
1614                 # TODO: deprecate this option -- we don't need it for groupby
1615                 # any more.
1616                 indices = nputils.inverse_permutation(np.concatenate(positions))
1617                 data = duck_array_ops.take(data, indices, axis=axis)
1618         else:
1619             axis = 0
1620             dims = (dim,) + first_var.dims
1621             data = duck_array_ops.stack(arrays, axis=axis)
1622 
1623         attrs = dict(first_var.attrs)
1624         encoding = dict(first_var.encoding)
1625         if not shortcut:
1626             for var in variables:
1627                 if var.dims != first_var.dims:
1628                     raise ValueError("inconsistent dimensions")
1629                 utils.remove_incompatible_items(attrs, var.attrs)
1630 
1631         return cls(dims, data, attrs, encoding)
1632 
1633     def equals(self, other, equiv=duck_array_ops.array_equiv):
1634         """True if two Variables have the same dimensions and values;
1635         otherwise False.
1636 
1637         Variables can still be equal (like pandas objects) if they have NaN
1638         values in the same locations.
1639 
1640         This method is necessary because `v1 == v2` for Variables
1641         does element-wise comparisons (like numpy.ndarrays).
1642         """
1643         other = getattr(other, "variable", other)
1644         try:
1645             return self.dims == other.dims and (
1646                 self._data is other._data or equiv(self.data, other.data)
1647             )
1648         except (TypeError, AttributeError):
1649             return False
1650 
1651     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1652         """True if two Variables have the values after being broadcast against
1653         each other; otherwise False.
1654 
1655         Variables can still be equal (like pandas objects) if they have NaN
1656         values in the same locations.
1657         """
1658         try:
1659             self, other = broadcast_variables(self, other)
1660         except (ValueError, AttributeError):
1661             return False
1662         return self.equals(other, equiv=equiv)
1663 
1664     def identical(self, other, equiv=duck_array_ops.array_equiv):
1665         """Like equals, but also checks attributes.
1666         """
1667         try:
1668             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1669                 other, equiv=equiv
1670             )
1671         except (TypeError, AttributeError):
1672             return False
1673 
1674     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1675         """True if the intersection of two Variable's non-null data is
1676         equal; otherwise false.
1677 
1678         Variables can thus still be equal if there are locations where either,
1679         or both, contain NaN values.
1680         """
1681         return self.broadcast_equals(other, equiv=equiv)
1682 
1683     def quantile(self, q, dim=None, interpolation="linear", keep_attrs=None):
1684         """Compute the qth quantile of the data along the specified dimension.
1685 
1686         Returns the qth quantiles(s) of the array elements.
1687 
1688         Parameters
1689         ----------
1690         q : float in range of [0,1] (or sequence of floats)
1691             Quantile to compute, which must be between 0 and 1
1692             inclusive.
1693         dim : str or sequence of str, optional
1694             Dimension(s) over which to apply quantile.
1695         interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'}
1696             This optional parameter specifies the interpolation method to
1697             use when the desired quantile lies between two data points
1698             ``i < j``:
1699                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1700                   the fractional part of the index surrounded by ``i`` and
1701                   ``j``.
1702                 * lower: ``i``.
1703                 * higher: ``j``.
1704                 * nearest: ``i`` or ``j``, whichever is nearest.
1705                 * midpoint: ``(i + j) / 2``.
1706         keep_attrs : bool, optional
1707             If True, the variable's attributes (`attrs`) will be copied from
1708             the original object to the new one.  If False (default), the new
1709             object will be returned without attributes.
1710 
1711         Returns
1712         -------
1713         quantiles : Variable
1714             If `q` is a single quantile, then the result
1715             is a scalar. If multiple percentiles are given, first axis of
1716             the result corresponds to the quantile and a quantile dimension
1717             is added to the return array. The other dimensions are the
1718             dimensions that remain after the reduction of the array.
1719 
1720         See Also
1721         --------
1722         numpy.nanpercentile, pandas.Series.quantile, Dataset.quantile,
1723         DataArray.quantile
1724         """
1725 
1726         from .computation import apply_ufunc
1727 
1728         if keep_attrs is None:
1729             keep_attrs = _get_keep_attrs(default=False)
1730 
1731         scalar = utils.is_scalar(q)
1732         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1733 
1734         if dim is None:
1735             dim = self.dims
1736 
1737         if utils.is_scalar(dim):
1738             dim = [dim]
1739 
1740         def _wrapper(npa, **kwargs):
1741             # move quantile axis to end. required for apply_ufunc
1742             return np.moveaxis(np.nanpercentile(npa, **kwargs), 0, -1)
1743 
1744         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1745         result = apply_ufunc(
1746             _wrapper,
1747             self,
1748             input_core_dims=[dim],
1749             exclude_dims=set(dim),
1750             output_core_dims=[["quantile"]],
1751             output_dtypes=[np.float64],
1752             output_sizes={"quantile": len(q)},
1753             dask="parallelized",
1754             kwargs={"q": q * 100, "axis": axis, "interpolation": interpolation},
1755         )
1756 
1757         # for backward compatibility
1758         result = result.transpose("quantile", ...)
1759         if scalar:
1760             result = result.squeeze("quantile")
1761         if keep_attrs:
1762             result.attrs = self._attrs
1763         return result
1764 
1765     def rank(self, dim, pct=False):
1766         """Ranks the data.
1767 
1768         Equal values are assigned a rank that is the average of the ranks that
1769         would have been otherwise assigned to all of the values within that
1770         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1771 
1772         NaNs in the input array are returned as NaNs.
1773 
1774         The `bottleneck` library is required.
1775 
1776         Parameters
1777         ----------
1778         dim : str
1779             Dimension over which to compute rank.
1780         pct : bool, optional
1781             If True, compute percentage ranks, otherwise compute integer ranks.
1782 
1783         Returns
1784         -------
1785         ranked : Variable
1786 
1787         See Also
1788         --------
1789         Dataset.rank, DataArray.rank
1790         """
1791         import bottleneck as bn
1792 
1793         data = self.data
1794 
1795         if isinstance(data, dask_array_type):
1796             raise TypeError(
1797                 "rank does not work for arrays stored as dask "
1798                 "arrays. Load the data via .compute() or .load() "
1799                 "prior to calling this method."
1800             )
1801         elif not isinstance(data, np.ndarray):
1802             raise TypeError(
1803                 "rank is not implemented for {} objects.".format(type(data))
1804             )
1805 
1806         axis = self.get_axis_num(dim)
1807         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1808         ranked = func(data, axis=axis)
1809         if pct:
1810             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1811             ranked /= count
1812         return Variable(self.dims, ranked)
1813 
1814     def rolling_window(
1815         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1816     ):
1817         """
1818         Make a rolling_window along dim and add a new_dim to the last place.
1819 
1820         Parameters
1821         ----------
1822         dim: str
1823             Dimension over which to compute rolling_window
1824         window: int
1825             Window size of the rolling
1826         window_dim: str
1827             New name of the window dimension.
1828         center: boolean. default False.
1829             If True, pad fill_value for both ends. Otherwise, pad in the head
1830             of the axis.
1831         fill_value:
1832             value to be filled.
1833 
1834         Returns
1835         -------
1836         Variable that is a view of the original array with a added dimension of
1837         size w.
1838         The return dim: self.dims + (window_dim, )
1839         The return shape: self.shape + (window, )
1840 
1841         Examples
1842         --------
1843         >>> v=Variable(('a', 'b'), np.arange(8).reshape((2,4)))
1844         >>> v.rolling_window(x, 'b', 3, 'window_dim')
1845         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1846         array([[[nan, nan, 0], [nan, 0, 1], [0, 1, 2], [1, 2, 3]],
1847                [[nan, nan, 4], [nan, 4, 5], [4, 5, 6], [5, 6, 7]]])
1848 
1849         >>> v.rolling_window(x, 'b', 3, 'window_dim', center=True)
1850         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1851         array([[[nan, 0, 1], [0, 1, 2], [1, 2, 3], [2, 3, nan]],
1852                [[nan, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, nan]]])
1853         """
1854         if fill_value is dtypes.NA:  # np.nan is passed
1855             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1856             array = self.astype(dtype, copy=False).data
1857         else:
1858             dtype = self.dtype
1859             array = self.data
1860 
1861         new_dims = self.dims + (window_dim,)
1862         return Variable(
1863             new_dims,
1864             duck_array_ops.rolling_window(
1865                 array,
1866                 axis=self.get_axis_num(dim),
1867                 window=window,
1868                 center=center,
1869                 fill_value=fill_value,
1870             ),
1871         )
1872 
1873     def coarsen(self, windows, func, boundary="exact", side="left", **kwargs):
1874         """
1875         Apply reduction function.
1876         """
1877         windows = {k: v for k, v in windows.items() if k in self.dims}
1878         if not windows:
1879             return self.copy()
1880 
1881         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
1882         if isinstance(func, str):
1883             name = func
1884             func = getattr(duck_array_ops, name, None)
1885             if func is None:
1886                 raise NameError(f"{name} is not a valid method.")
1887         return self._replace(data=func(reshaped, axis=axes, **kwargs))
1888 
1889     def _coarsen_reshape(self, windows, boundary, side):
1890         """
1891         Construct a reshaped-array for coarsen
1892         """
1893         if not utils.is_dict_like(boundary):
1894             boundary = {d: boundary for d in windows.keys()}
1895 
1896         if not utils.is_dict_like(side):
1897             side = {d: side for d in windows.keys()}
1898 
1899         # remove unrelated dimensions
1900         boundary = {k: v for k, v in boundary.items() if k in windows}
1901         side = {k: v for k, v in side.items() if k in windows}
1902 
1903         for d, window in windows.items():
1904             if window <= 0:
1905                 raise ValueError(f"window must be > 0. Given {window}")
1906 
1907         variable = self
1908         for d, window in windows.items():
1909             # trim or pad the object
1910             size = variable.shape[self._get_axis_num(d)]
1911             n = int(size / window)
1912             if boundary[d] == "exact":
1913                 if n * window != size:
1914                     raise ValueError(
1915                         "Could not coarsen a dimension of size {} with "
1916                         "window {}".format(size, window)
1917                     )
1918             elif boundary[d] == "trim":
1919                 if side[d] == "left":
1920                     variable = variable.isel({d: slice(0, window * n)})
1921                 else:
1922                     excess = size - window * n
1923                     variable = variable.isel({d: slice(excess, None)})
1924             elif boundary[d] == "pad":  # pad
1925                 pad = window * n - size
1926                 if pad < 0:
1927                     pad += window
1928                 if side[d] == "left":
1929                     pad_widths = {d: (0, pad)}
1930                 else:
1931                     pad_widths = {d: (pad, 0)}
1932                 variable = variable.pad_with_fill_value(pad_widths)
1933             else:
1934                 raise TypeError(
1935                     "{} is invalid for boundary. Valid option is 'exact', "
1936                     "'trim' and 'pad'".format(boundary[d])
1937                 )
1938 
1939         shape = []
1940         axes = []
1941         axis_count = 0
1942         for i, d in enumerate(variable.dims):
1943             if d in windows:
1944                 size = variable.shape[i]
1945                 shape.append(int(size / windows[d]))
1946                 shape.append(windows[d])
1947                 axis_count += 1
1948                 axes.append(i + axis_count)
1949             else:
1950                 shape.append(variable.shape[i])
1951 
1952         return variable.data.reshape(shape), tuple(axes)
1953 
1954     @property
1955     def real(self):
1956         return type(self)(self.dims, self.data.real, self._attrs)
1957 
1958     @property
1959     def imag(self):
1960         return type(self)(self.dims, self.data.imag, self._attrs)
1961 
1962     def __array_wrap__(self, obj, context=None):
1963         return Variable(self.dims, obj)
1964 
1965     @staticmethod
1966     def _unary_op(f):
1967         @functools.wraps(f)
1968         def func(self, *args, **kwargs):
1969             with np.errstate(all="ignore"):
1970                 return self.__array_wrap__(f(self.data, *args, **kwargs))
1971 
1972         return func
1973 
1974     @staticmethod
1975     def _binary_op(f, reflexive=False, **ignored_kwargs):
1976         @functools.wraps(f)
1977         def func(self, other):
1978             if isinstance(other, (xr.DataArray, xr.Dataset)):
1979                 return NotImplemented
1980             self_data, other_data, dims = _broadcast_compat_data(self, other)
1981             keep_attrs = _get_keep_attrs(default=False)
1982             attrs = self._attrs if keep_attrs else None
1983             with np.errstate(all="ignore"):
1984                 new_data = (
1985                     f(self_data, other_data)
1986                     if not reflexive
1987                     else f(other_data, self_data)
1988                 )
1989             result = Variable(dims, new_data, attrs=attrs)
1990             return result
1991 
1992         return func
1993 
1994     @staticmethod
1995     def _inplace_binary_op(f):
1996         @functools.wraps(f)
1997         def func(self, other):
1998             if isinstance(other, xr.Dataset):
1999                 raise TypeError("cannot add a Dataset to a Variable in-place")
2000             self_data, other_data, dims = _broadcast_compat_data(self, other)
2001             if dims != self.dims:
2002                 raise ValueError("dimensions cannot change for in-place " "operations")
2003             with np.errstate(all="ignore"):
2004                 self.values = f(self_data, other_data)
2005             return self
2006 
2007         return func
2008 
2009     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2010         """ A (private) method to convert datetime array to numeric dtype
2011         See duck_array_ops.datetime_to_numeric
2012         """
2013         numeric_array = duck_array_ops.datetime_to_numeric(
2014             self.data, offset, datetime_unit, dtype
2015         )
2016         return type(self)(self.dims, numeric_array, self._attrs)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 21:37:44.346 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 21:37:44.346 | INFO     | Search-only: skipping patch selection
2025-10-15 21:37:44.411 | INFO     | Task pydata__xarray-3635 completed successfully.
2025-10-15 21:37:44.412 | INFO     | No patch generated. You can try running ACR again.
