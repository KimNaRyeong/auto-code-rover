2025-10-20 11:41:58.656 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-20 11:42:01.410 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=269, output_tokens=12, cost=0.003050
2025-10-20 11:42:14.629 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=449, output_tokens=604, cost=0.022610
2025-10-20 11:42:15.768 | INFO     | registered non reproducing test 0
2025-10-20 11:42:34.901 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1408, output_tokens=702, cost=0.035140
2025-10-20 11:42:36.072 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-20 11:42:46.481 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=567, output_tokens=462, cost=0.019530
2025-10-20 11:42:46.491 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-20 11:42:48.090 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=816, output_tokens=48, cost=0.009600
2025-10-20 11:42:48.092 | DEBUG    | Extracted a valid json.
2025-10-20 11:42:48.092 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Window\")",
        "search_class(\"Case\")",
        "search_class(\"When\")",
        "search_method(\"annotate\")"
    ],
    "bug_locations": []
}
2025-10-20 11:42:48.160 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-20 11:42:48.160 | DEBUG    | <Agent search> Analyzing search results.
2025-10-20 11:43:06.154 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2671, output_tokens=741, cost=0.048940
2025-10-20 11:43:06.169 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-20 11:43:16.908 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3628, output_tokens=473, cost=0.050470
2025-10-20 11:43:16.919 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-20 11:43:27.206 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=827, output_tokens=367, cost=0.019280
2025-10-20 11:43:27.207 | DEBUG    | Extracted a valid json.
2025-10-20 11:43:27.208 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/expressions.py",
            "class": "Window",
            "method": "as_sql",
            "intended_behavior": "The method should be reviewed and possibly adjusted to ensure that window expressions can be used within Case and When expressions without raising NotSupportedError when used in the SELECT clause. This adjustment must preserve the SQL standard's compliance that disallows window functions in the WHERE clause but permits them in the SELECT clause. The method might need to include logic to identify when a Window expression is properly used within a SELECT clause context and bypass restrictions that incorrectly flag this as an error."
        },
        {
            "file": "django/db/models/expressions.py",
            "class": "Case",
            "method": "resolve_expression or as_sql",
            "intended_behavior": "These methods might require adjustments to handle Window expressions without errors when constructing SQL queries. They should ensure that Window expressions encapsulated within Case statements are translated into SQL without triggering validation errors that are not applicable for their use in SELECT clauses. This includes properly handling the SQL generation to reflect the intended logic of window expressions used in Case and When."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "This method's validation logic might need refinement to allow Window expressions within conditional annotations. Specifically, the method should preserve its functionality while also supporting the inclusion of Window functions within Case expressions as part of annotations. The validation process should differentiate between the unsupported use of window functions (e.g., in WHERE clauses directly) and supported uses, such as within the SELECT clause inside conditional statements."
        }
    ]
}
2025-10-20 11:43:27.213 | DEBUG    | Found override of as_sql in BaseExpression
2025-10-20 11:43:27.213 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x741288726c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_django__django__3.1': [<app.raw_tasks.RawSweTask object at 0x7411960e75c0>]}
    └ <function run_task_groups at 0x7411984223e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x7411960e75c0>]
    └ <function run_tasks_serial at 0x741198422480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x7411960e75c0>
    └ <function run_task_in_subprocess at 0x741198422660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x7411960e75c0>
    │        │      └ <function run_raw_task at 0x741198422700>
    │        └ <function ProcessPoolExecutor.submit at 0x741288072020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7411994b9dc0>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x741288071da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7411994b9dc0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x741288071ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7411994b9dc0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x741288071f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x7411994b9dc0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x74128822cea0>
    └ <ForkProcess name='ForkProcess-1' parent=615326 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-1' parent=615326 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x74128829e0c0>)>
    │    │        └ <ForkProcess name='ForkProcess-1' parent=615326 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-1' parent=615326 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-1' parent=615326 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-1' parent=615326 started>
    │    └ <function Popen._launch at 0x7411984805e0>
    └ <multiprocessing.popen_fork.Popen object at 0x7411984426c0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x74128822d8a0>
           └ <ForkProcess name='ForkProcess-1' parent=615326 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x74128822ce00>
    └ <ForkProcess name='ForkProcess-1' parent=615326 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-1' parent=615326 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x741198442e40>, <multiprocessing.queues.SimpleQueue object at 0x741198442a...
    │    │        └ <ForkProcess name='ForkProcess-1' parent=615326 started>
    │    └ <function _process_worker at 0x7412880711c0>
    └ <ForkProcess name='ForkProcess-1' parent=615326 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x7412881fee70>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x741198442270>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x7412881fee70>
        │         └ <function run_raw_task at 0x741198422700>
        └ <concurrent.futures.process._CallItem object at 0x7412881fee70>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/try_execution/django__django-12185_2025-10-20_11-41-58'
             │            │    └ <function RawSweTask.to_task at 0x7411984216c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x741198442270>
             └ <function do_inference at 0x7411984228e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x7411986b4400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x741198ce04a0>
       │                 │         │            └ SweTask(task_id='django__django-12185', problem_statement="Window expression are not allowed in conditional statements used o...
       │                 │         └ <app.manage.ProjectApiManager object at 0x741198441b80>
       │                 └ Path('/home/kimnal0/auto-code-rover/try_execution/django__django-12185_2025-10-20_11-41-58/output_0')
       └ <function _run_one_task at 0x7411986b58a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x741198755c60>
                                  │           └ <app.search.search_manage.SearchManager object at 0x741198441d30>
                                  └ <app.manage.ProjectApiManager object at 0x741198441b80>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 126, in search_iterative

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'as_sql'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.1/django/db/mo...
    │    │                      │   └ 'Window'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.1/django/db/mo...
    │    └ <function SearchBackend._get_inherited_methods at 0x7411987558a0>
    └ <app.search.search_backend.SearchBackend object at 0x741198441730>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseExpression', 'method_name': 'as_sql'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x741198754fe0>
                                  └ <app.search.search_backend.SearchBackend object at 0x741198441730>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x741198441730>, {'class_name': 'BaseExpression', 'method_name': 'as_sql'})
           └ <function SearchBackend.search_method_in_class at 0x741198754f40>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-20 11:43:27.224 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/expressions.py</file>
<class>Window</class>
<method>as_sql</method>
<code>
1201     def as_sql(self, compiler, connection, template=None):
1202         connection.ops.check_expression_support(self)
1203         if not connection.features.supports_over_clause:
1204             raise NotSupportedError('This backend does not support window expressions.')
1205         expr_sql, params = compiler.compile(self.source_expression)
1206         window_sql, window_params = [], []
1207 
1208         if self.partition_by is not None:
1209             sql_expr, sql_params = self.partition_by.as_sql(
1210                 compiler=compiler, connection=connection,
1211                 template='PARTITION BY %(expressions)s',
1212             )
1213             window_sql.extend(sql_expr)
1214             window_params.extend(sql_params)
1215 
1216         if self.order_by is not None:
1217             window_sql.append(' ORDER BY ')
1218             order_sql, order_params = compiler.compile(self.order_by)
1219             window_sql.extend(order_sql)
1220             window_params.extend(order_params)
1221 
1222         if self.frame:
1223             frame_sql, frame_params = compiler.compile(self.frame)
1224             window_sql.append(' ' + frame_sql)
1225             window_params.extend(frame_params)
1226 
1227         params.extend(window_params)
1228         template = template or self.template
1229 
1230         return template % {
1231             'expression': expr_sql,
1232             'window': ''.join(window_sql).strip()
1233         }, params

</code><intended_behavior>The method should be reviewed and possibly adjusted to ensure that window expressions can be used within Case and When expressions without raising NotSupportedError when used in the SELECT clause. This adjustment must preserve the SQL standard's compliance that disallows window functions in the WHERE clause but permits them in the SELECT clause. The method might need to include logic to identify when a Window expression is properly used within a SELECT clause context and bypass restrictions that incorrectly flag this as an error.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Window</class>
<method>None</method>
<code>
1156 class Window(Expression):
1157     template = '%(expression)s OVER (%(window)s)'
1158     # Although the main expression may either be an aggregate or an
1159     # expression with an aggregate function, the GROUP BY that will
1160     # be introduced in the query as a result is not desired.
1161     contains_aggregate = False
1162     contains_over_clause = True
1163     filterable = False
1164 
1165     def __init__(self, expression, partition_by=None, order_by=None, frame=None, output_field=None):
1166         self.partition_by = partition_by
1167         self.order_by = order_by
1168         self.frame = frame
1169 
1170         if not getattr(expression, 'window_compatible', False):
1171             raise ValueError(
1172                 "Expression '%s' isn't compatible with OVER clauses." %
1173                 expression.__class__.__name__
1174             )
1175 
1176         if self.partition_by is not None:
1177             if not isinstance(self.partition_by, (tuple, list)):
1178                 self.partition_by = (self.partition_by,)
1179             self.partition_by = ExpressionList(*self.partition_by)
1180 
1181         if self.order_by is not None:
1182             if isinstance(self.order_by, (list, tuple)):
1183                 self.order_by = ExpressionList(*self.order_by)
1184             elif not isinstance(self.order_by, BaseExpression):
1185                 raise ValueError(
1186                     'order_by must be either an Expression or a sequence of '
1187                     'expressions.'
1188                 )
1189         super().__init__(output_field=output_field)
1190         self.source_expression = self._parse_expressions(expression)[0]
1191 
1192     def _resolve_output_field(self):
1193         return self.source_expression.output_field
1194 
1195     def get_source_expressions(self):
1196         return [self.source_expression, self.partition_by, self.order_by, self.frame]
1197 
1198     def set_source_expressions(self, exprs):
1199         self.source_expression, self.partition_by, self.order_by, self.frame = exprs
1200 
1201     def as_sql(self, compiler, connection, template=None):
1202         connection.ops.check_expression_support(self)
1203         if not connection.features.supports_over_clause:
1204             raise NotSupportedError('This backend does not support window expressions.')
1205         expr_sql, params = compiler.compile(self.source_expression)
1206         window_sql, window_params = [], []
1207 
1208         if self.partition_by is not None:
1209             sql_expr, sql_params = self.partition_by.as_sql(
1210                 compiler=compiler, connection=connection,
1211                 template='PARTITION BY %(expressions)s',
1212             )
1213             window_sql.extend(sql_expr)
1214             window_params.extend(sql_params)
1215 
1216         if self.order_by is not None:
1217             window_sql.append(' ORDER BY ')
1218             order_sql, order_params = compiler.compile(self.order_by)
1219             window_sql.extend(order_sql)
1220             window_params.extend(order_params)
1221 
1222         if self.frame:
1223             frame_sql, frame_params = compiler.compile(self.frame)
1224             window_sql.append(' ' + frame_sql)
1225             window_params.extend(frame_params)
1226 
1227         params.extend(window_params)
1228         template = template or self.template
1229 
1230         return template % {
1231             'expression': expr_sql,
1232             'window': ''.join(window_sql).strip()
1233         }, params
1234 
1235     def __str__(self):
1236         return '{} OVER ({}{}{})'.format(
1237             str(self.source_expression),
1238             'PARTITION BY ' + str(self.partition_by) if self.partition_by else '',
1239             'ORDER BY ' + str(self.order_by) if self.order_by else '',
1240             str(self.frame or ''),
1241         )
1242 
1243     def __repr__(self):
1244         return '<%s: %s>' % (self.__class__.__name__, self)
1245 
1246     def get_group_by_cols(self, alias=None):
1247         return []

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Case</class>
<method>None</method>
<code>
918 class Case(Expression):
919     """
920     An SQL searched CASE expression:
921 
922         CASE
923             WHEN n > 0
924                 THEN 'positive'
925             WHEN n < 0
926                 THEN 'negative'
927             ELSE 'zero'
928         END
929     """
930     template = 'CASE %(cases)s ELSE %(default)s END'
931     case_joiner = ' '
932 
933     def __init__(self, *cases, default=None, output_field=None, **extra):
934         if not all(isinstance(case, When) for case in cases):
935             raise TypeError("Positional arguments must all be When objects.")
936         super().__init__(output_field)
937         self.cases = list(cases)
938         self.default = self._parse_expressions(default)[0]
939         self.extra = extra
940 
941     def __str__(self):
942         return "CASE %s, ELSE %r" % (', '.join(str(c) for c in self.cases), self.default)
943 
944     def __repr__(self):
945         return "<%s: %s>" % (self.__class__.__name__, self)
946 
947     def get_source_expressions(self):
948         return self.cases + [self.default]
949 
950     def set_source_expressions(self, exprs):
951         *self.cases, self.default = exprs
952 
953     def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
954         c = self.copy()
955         c.is_summary = summarize
956         for pos, case in enumerate(c.cases):
957             c.cases[pos] = case.resolve_expression(query, allow_joins, reuse, summarize, for_save)
958         c.default = c.default.resolve_expression(query, allow_joins, reuse, summarize, for_save)
959         return c
960 
961     def copy(self):
962         c = super().copy()
963         c.cases = c.cases[:]
964         return c
965 
966     def as_sql(self, compiler, connection, template=None, case_joiner=None, **extra_context):
967         connection.ops.check_expression_support(self)
968         if not self.cases:
969             return compiler.compile(self.default)
970         template_params = {**self.extra, **extra_context}
971         case_parts = []
972         sql_params = []
973         for case in self.cases:
974             try:
975                 case_sql, case_params = compiler.compile(case)
976             except EmptyResultSet:
977                 continue
978             case_parts.append(case_sql)
979             sql_params.extend(case_params)
980         default_sql, default_params = compiler.compile(self.default)
981         if not case_parts:
982             return default_sql, default_params
983         case_joiner = case_joiner or self.case_joiner
984         template_params['cases'] = case_joiner.join(case_parts)
985         template_params['default'] = default_sql
986         sql_params.extend(default_params)
987         template = template or template_params.get('template', self.template)
988         sql = template % template_params
989         if self._output_field_or_none is not None:
990             sql = connection.ops.unification_cast_sql(self.output_field) % sql
991         return sql, sql_params

</code><intended_behavior>These methods might require adjustments to handle Window expressions without errors when constructing SQL queries. They should ensure that Window expressions encapsulated within Case statements are translated into SQL without triggering validation errors that are not applicable for their use in SELECT clauses. This includes properly handling the SQL generation to reflect the intended logic of window expressions used in Case and When.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1058     def annotate(self, *args, **kwargs):
1059         """
1060         Return a query set in which the returned objects have been annotated
1061         with extra data or aggregations.
1062         """
1063         self._not_support_combined_queries('annotate')
1064         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1065         annotations = {}
1066         for arg in args:
1067             # The default_alias property may raise a TypeError.
1068             try:
1069                 if arg.default_alias in kwargs:
1070                     raise ValueError("The named annotation '%s' conflicts with the "
1071                                      "default name for another annotation."
1072                                      % arg.default_alias)
1073             except TypeError:
1074                 raise TypeError("Complex annotations require an alias")
1075             annotations[arg.default_alias] = arg
1076         annotations.update(kwargs)
1077 
1078         clone = self._chain()
1079         names = self._fields
1080         if names is None:
1081             names = set(chain.from_iterable(
1082                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1083                 for field in self.model._meta.get_fields()
1084             ))
1085 
1086         for alias, annotation in annotations.items():
1087             if alias in names:
1088                 raise ValueError("The annotation '%s' conflicts with a field on "
1089                                  "the model." % alias)
1090             if isinstance(annotation, FilteredRelation):
1091                 clone.query.add_filtered_relation(annotation, alias)
1092             else:
1093                 clone.query.add_annotation(annotation, alias, is_summary=False)
1094 
1095         for alias, annotation in clone.query.annotations.items():
1096             if alias in annotations and annotation.contains_aggregate:
1097                 if clone._fields is None:
1098                     clone.query.group_by = True
1099                 else:
1100                     clone.query.set_group_by()
1101                 break
1102 
1103         return clone

</code><intended_behavior>This method's validation logic might need refinement to allow Window expressions within conditional annotations. Specifically, the method should preserve its functionality while also supporting the inclusion of Window functions within Case expressions as part of annotations. The validation process should differentiate between the unsupported use of window functions (e.g., in WHERE clauses directly) and supported uses, such as within the SELECT clause inside conditional statements.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
186 class QuerySet:
187     """Represent a lazy database lookup for a set of objects."""
188 
189     def __init__(self, model=None, query=None, using=None, hints=None):
190         self.model = model
191         self._db = using
192         self._hints = hints or {}
193         self._query = query or sql.Query(self.model)
194         self._result_cache = None
195         self._sticky_filter = False
196         self._for_write = False
197         self._prefetch_related_lookups = ()
198         self._prefetch_done = False
199         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
200         self._iterable_class = ModelIterable
201         self._fields = None
202         self._defer_next_filter = False
203         self._deferred_filter = None
204 
205     @property
206     def query(self):
207         if self._deferred_filter:
208             negate, args, kwargs = self._deferred_filter
209             self._filter_or_exclude_inplace(negate, *args, **kwargs)
210             self._deferred_filter = None
211         return self._query
212 
213     @query.setter
214     def query(self, value):
215         self._query = value
216 
217     def as_manager(cls):
218         # Address the circular dependency between `Queryset` and `Manager`.
219         from django.db.models.manager import Manager
220         manager = Manager.from_queryset(cls)()
221         manager._built_with_as_manager = True
222         return manager
223     as_manager.queryset_only = True
224     as_manager = classmethod(as_manager)
225 
226     ########################
227     # PYTHON MAGIC METHODS #
228     ########################
229 
230     def __deepcopy__(self, memo):
231         """Don't populate the QuerySet's cache."""
232         obj = self.__class__()
233         for k, v in self.__dict__.items():
234             if k == '_result_cache':
235                 obj.__dict__[k] = None
236             else:
237                 obj.__dict__[k] = copy.deepcopy(v, memo)
238         return obj
239 
240     def __getstate__(self):
241         # Force the cache to be fully populated.
242         self._fetch_all()
243         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
244 
245     def __setstate__(self, state):
246         msg = None
247         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
248         if pickled_version:
249             current_version = get_version()
250             if current_version != pickled_version:
251                 msg = (
252                     "Pickled queryset instance's Django version %s does not "
253                     "match the current version %s." % (pickled_version, current_version)
254                 )
255         else:
256             msg = "Pickled queryset instance's Django version is not specified."
257 
258         if msg:
259             warnings.warn(msg, RuntimeWarning, stacklevel=2)
260 
261         self.__dict__.update(state)
262 
263     def __repr__(self):
264         data = list(self[:REPR_OUTPUT_SIZE + 1])
265         if len(data) > REPR_OUTPUT_SIZE:
266             data[-1] = "...(remaining elements truncated)..."
267         return '<%s %r>' % (self.__class__.__name__, data)
268 
269     def __len__(self):
270         self._fetch_all()
271         return len(self._result_cache)
272 
273     def __iter__(self):
274         """
275         The queryset iterator protocol uses three nested iterators in the
276         default case:
277             1. sql.compiler.execute_sql()
278                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
279                  using cursor.fetchmany(). This part is responsible for
280                  doing some column masking, and returning the rows in chunks.
281             2. sql.compiler.results_iter()
282                - Returns one row at time. At this point the rows are still just
283                  tuples. In some cases the return values are converted to
284                  Python values at this location.
285             3. self.iterator()
286                - Responsible for turning the rows into model objects.
287         """
288         self._fetch_all()
289         return iter(self._result_cache)
290 
291     def __bool__(self):
292         self._fetch_all()
293         return bool(self._result_cache)
294 
295     def __getitem__(self, k):
296         """Retrieve an item or slice from the set of results."""
297         if not isinstance(k, (int, slice)):
298             raise TypeError(
299                 'QuerySet indices must be integers or slices, not %s.'
300                 % type(k).__name__
301             )
302         assert ((not isinstance(k, slice) and (k >= 0)) or
303                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
304                  (k.stop is None or k.stop >= 0))), \
305             "Negative indexing is not supported."
306 
307         if self._result_cache is not None:
308             return self._result_cache[k]
309 
310         if isinstance(k, slice):
311             qs = self._chain()
312             if k.start is not None:
313                 start = int(k.start)
314             else:
315                 start = None
316             if k.stop is not None:
317                 stop = int(k.stop)
318             else:
319                 stop = None
320             qs.query.set_limits(start, stop)
321             return list(qs)[::k.step] if k.step else qs
322 
323         qs = self._chain()
324         qs.query.set_limits(k, k + 1)
325         qs._fetch_all()
326         return qs._result_cache[0]
327 
328     def __and__(self, other):
329         self._merge_sanity_check(other)
330         if isinstance(other, EmptyQuerySet):
331             return other
332         if isinstance(self, EmptyQuerySet):
333             return self
334         combined = self._chain()
335         combined._merge_known_related_objects(other)
336         combined.query.combine(other.query, sql.AND)
337         return combined
338 
339     def __or__(self, other):
340         self._merge_sanity_check(other)
341         if isinstance(self, EmptyQuerySet):
342             return other
343         if isinstance(other, EmptyQuerySet):
344             return self
345         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
346         combined = query._chain()
347         combined._merge_known_related_objects(other)
348         if not other.query.can_filter():
349             other = other.model._base_manager.filter(pk__in=other.values('pk'))
350         combined.query.combine(other.query, sql.OR)
351         return combined
352 
353     ####################################
354     # METHODS THAT DO DATABASE QUERIES #
355     ####################################
356 
357     def _iterator(self, use_chunked_fetch, chunk_size):
358         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
359 
360     def iterator(self, chunk_size=2000):
361         """
362         An iterator over the results from applying this QuerySet to the
363         database.
364         """
365         if chunk_size <= 0:
366             raise ValueError('Chunk size must be strictly positive.')
367         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
368         return self._iterator(use_chunked_fetch, chunk_size)
369 
370     def aggregate(self, *args, **kwargs):
371         """
372         Return a dictionary containing the calculations (aggregation)
373         over the current queryset.
374 
375         If args is present the expression is passed as a kwarg using
376         the Aggregate object's default alias.
377         """
378         if self.query.distinct_fields:
379             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
380         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
381         for arg in args:
382             # The default_alias property raises TypeError if default_alias
383             # can't be set automatically or AttributeError if it isn't an
384             # attribute.
385             try:
386                 arg.default_alias
387             except (AttributeError, TypeError):
388                 raise TypeError("Complex aggregates require an alias")
389             kwargs[arg.default_alias] = arg
390 
391         query = self.query.chain()
392         for (alias, aggregate_expr) in kwargs.items():
393             query.add_annotation(aggregate_expr, alias, is_summary=True)
394             if not query.annotations[alias].contains_aggregate:
395                 raise TypeError("%s is not an aggregate expression" % alias)
396         return query.get_aggregation(self.db, kwargs)
397 
398     def count(self):
399         """
400         Perform a SELECT COUNT() and return the number of records as an
401         integer.
402 
403         If the QuerySet is already fully cached, return the length of the
404         cached results set to avoid multiple SELECT COUNT(*) calls.
405         """
406         if self._result_cache is not None:
407             return len(self._result_cache)
408 
409         return self.query.get_count(using=self.db)
410 
411     def get(self, *args, **kwargs):
412         """
413         Perform the query and return a single object matching the given
414         keyword arguments.
415         """
416         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
417         if self.query.can_filter() and not self.query.distinct_fields:
418             clone = clone.order_by()
419         limit = None
420         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
421             limit = MAX_GET_RESULTS
422             clone.query.set_limits(high=limit)
423         num = len(clone)
424         if num == 1:
425             return clone._result_cache[0]
426         if not num:
427             raise self.model.DoesNotExist(
428                 "%s matching query does not exist." %
429                 self.model._meta.object_name
430             )
431         raise self.model.MultipleObjectsReturned(
432             'get() returned more than one %s -- it returned %s!' % (
433                 self.model._meta.object_name,
434                 num if not limit or num < limit else 'more than %s' % (limit - 1),
435             )
436         )
437 
438     def create(self, **kwargs):
439         """
440         Create a new object with the given kwargs, saving it to the database
441         and returning the created object.
442         """
443         obj = self.model(**kwargs)
444         self._for_write = True
445         obj.save(force_insert=True, using=self.db)
446         return obj
447 
448     def _populate_pk_values(self, objs):
449         for obj in objs:
450             if obj.pk is None:
451                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
452 
453     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
454         """
455         Insert each of the instances into the database. Do *not* call
456         save() on each of the instances, do not send any pre/post_save
457         signals, and do not set the primary key attribute if it is an
458         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
459         Multi-table models are not supported.
460         """
461         # When you bulk insert you don't get the primary keys back (if it's an
462         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
463         # you can't insert into the child tables which references this. There
464         # are two workarounds:
465         # 1) This could be implemented if you didn't have an autoincrement pk
466         # 2) You could do it by doing O(n) normal inserts into the parent
467         #    tables to get the primary keys back and then doing a single bulk
468         #    insert into the childmost table.
469         # We currently set the primary keys on the objects when using
470         # PostgreSQL via the RETURNING ID clause. It should be possible for
471         # Oracle as well, but the semantics for extracting the primary keys is
472         # trickier so it's not done yet.
473         assert batch_size is None or batch_size > 0
474         # Check that the parents share the same concrete model with the our
475         # model to detect the inheritance pattern ConcreteGrandParent ->
476         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
477         # would not identify that case as involving multiple tables.
478         for parent in self.model._meta.get_parent_list():
479             if parent._meta.concrete_model is not self.model._meta.concrete_model:
480                 raise ValueError("Can't bulk create a multi-table inherited model")
481         if not objs:
482             return objs
483         self._for_write = True
484         connection = connections[self.db]
485         opts = self.model._meta
486         fields = opts.concrete_fields
487         objs = list(objs)
488         self._populate_pk_values(objs)
489         with transaction.atomic(using=self.db, savepoint=False):
490             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
491             if objs_with_pk:
492                 returned_columns = self._batched_insert(
493                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
494                 )
495                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
496                     for result, field in zip(results, opts.db_returning_fields):
497                         if field != opts.pk:
498                             setattr(obj_with_pk, field.attname, result)
499                 for obj_with_pk in objs_with_pk:
500                     obj_with_pk._state.adding = False
501                     obj_with_pk._state.db = self.db
502             if objs_without_pk:
503                 fields = [f for f in fields if not isinstance(f, AutoField)]
504                 returned_columns = self._batched_insert(
505                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
506                 )
507                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
508                     assert len(returned_columns) == len(objs_without_pk)
509                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
510                     for result, field in zip(results, opts.db_returning_fields):
511                         setattr(obj_without_pk, field.attname, result)
512                     obj_without_pk._state.adding = False
513                     obj_without_pk._state.db = self.db
514 
515         return objs
516 
517     def bulk_update(self, objs, fields, batch_size=None):
518         """
519         Update the given fields in each of the given objects in the database.
520         """
521         if batch_size is not None and batch_size < 0:
522             raise ValueError('Batch size must be a positive integer.')
523         if not fields:
524             raise ValueError('Field names must be given to bulk_update().')
525         objs = tuple(objs)
526         if any(obj.pk is None for obj in objs):
527             raise ValueError('All bulk_update() objects must have a primary key set.')
528         fields = [self.model._meta.get_field(name) for name in fields]
529         if any(not f.concrete or f.many_to_many for f in fields):
530             raise ValueError('bulk_update() can only be used with concrete fields.')
531         if any(f.primary_key for f in fields):
532             raise ValueError('bulk_update() cannot be used with primary key fields.')
533         if not objs:
534             return
535         # PK is used twice in the resulting update query, once in the filter
536         # and once in the WHEN. Each field will also have one CAST.
537         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
538         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
539         requires_casting = connections[self.db].features.requires_casted_case_in_updates
540         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
541         updates = []
542         for batch_objs in batches:
543             update_kwargs = {}
544             for field in fields:
545                 when_statements = []
546                 for obj in batch_objs:
547                     attr = getattr(obj, field.attname)
548                     if not isinstance(attr, Expression):
549                         attr = Value(attr, output_field=field)
550                     when_statements.append(When(pk=obj.pk, then=attr))
551                 case_statement = Case(*when_statements, output_field=field)
552                 if requires_casting:
553                     case_statement = Cast(case_statement, output_field=field)
554                 update_kwargs[field.attname] = case_statement
555             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
556         with transaction.atomic(using=self.db, savepoint=False):
557             for pks, update_kwargs in updates:
558                 self.filter(pk__in=pks).update(**update_kwargs)
559     bulk_update.alters_data = True
560 
561     def get_or_create(self, defaults=None, **kwargs):
562         """
563         Look up an object with the given kwargs, creating one if necessary.
564         Return a tuple of (object, created), where created is a boolean
565         specifying whether an object was created.
566         """
567         # The get() needs to be targeted at the write database in order
568         # to avoid potential transaction consistency problems.
569         self._for_write = True
570         try:
571             return self.get(**kwargs), False
572         except self.model.DoesNotExist:
573             params = self._extract_model_params(defaults, **kwargs)
574             return self._create_object_from_params(kwargs, params)
575 
576     def update_or_create(self, defaults=None, **kwargs):
577         """
578         Look up an object with the given kwargs, updating one with defaults
579         if it exists, otherwise create a new one.
580         Return a tuple (object, created), where created is a boolean
581         specifying whether an object was created.
582         """
583         defaults = defaults or {}
584         self._for_write = True
585         with transaction.atomic(using=self.db):
586             try:
587                 obj = self.select_for_update().get(**kwargs)
588             except self.model.DoesNotExist:
589                 params = self._extract_model_params(defaults, **kwargs)
590                 # Lock the row so that a concurrent update is blocked until
591                 # after update_or_create() has performed its save.
592                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
593                 if created:
594                     return obj, created
595             for k, v in resolve_callables(defaults):
596                 setattr(obj, k, v)
597             obj.save(using=self.db)
598         return obj, False
599 
600     def _create_object_from_params(self, lookup, params, lock=False):
601         """
602         Try to create an object using passed params. Used by get_or_create()
603         and update_or_create().
604         """
605         try:
606             with transaction.atomic(using=self.db):
607                 params = dict(resolve_callables(params))
608                 obj = self.create(**params)
609             return obj, True
610         except IntegrityError as e:
611             try:
612                 qs = self.select_for_update() if lock else self
613                 return qs.get(**lookup), False
614             except self.model.DoesNotExist:
615                 pass
616             raise e
617 
618     def _extract_model_params(self, defaults, **kwargs):
619         """
620         Prepare `params` for creating a model instance based on the given
621         kwargs; for use by get_or_create() and update_or_create().
622         """
623         defaults = defaults or {}
624         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
625         params.update(defaults)
626         property_names = self.model._meta._property_names
627         invalid_params = []
628         for param in params:
629             try:
630                 self.model._meta.get_field(param)
631             except exceptions.FieldDoesNotExist:
632                 # It's okay to use a model's property if it has a setter.
633                 if not (param in property_names and getattr(self.model, param).fset):
634                     invalid_params.append(param)
635         if invalid_params:
636             raise exceptions.FieldError(
637                 "Invalid field name(s) for model %s: '%s'." % (
638                     self.model._meta.object_name,
639                     "', '".join(sorted(invalid_params)),
640                 ))
641         return params
642 
643     def _earliest(self, *fields):
644         """
645         Return the earliest object according to fields (if given) or by the
646         model's Meta.get_latest_by.
647         """
648         if fields:
649             order_by = fields
650         else:
651             order_by = getattr(self.model._meta, 'get_latest_by')
652             if order_by and not isinstance(order_by, (tuple, list)):
653                 order_by = (order_by,)
654         if order_by is None:
655             raise ValueError(
656                 "earliest() and latest() require either fields as positional "
657                 "arguments or 'get_latest_by' in the model's Meta."
658             )
659 
660         assert not self.query.is_sliced, \
661             "Cannot change a query once a slice has been taken."
662         obj = self._chain()
663         obj.query.set_limits(high=1)
664         obj.query.clear_ordering(force_empty=True)
665         obj.query.add_ordering(*order_by)
666         return obj.get()
667 
668     def earliest(self, *fields):
669         return self._earliest(*fields)
670 
671     def latest(self, *fields):
672         return self.reverse()._earliest(*fields)
673 
674     def first(self):
675         """Return the first object of a query or None if no match is found."""
676         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
677             return obj
678 
679     def last(self):
680         """Return the last object of a query or None if no match is found."""
681         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
682             return obj
683 
684     def in_bulk(self, id_list=None, *, field_name='pk'):
685         """
686         Return a dictionary mapping each of the given IDs to the object with
687         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
688         """
689         assert not self.query.is_sliced, \
690             "Cannot use 'limit' or 'offset' with in_bulk"
691         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
692             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
693         if id_list is not None:
694             if not id_list:
695                 return {}
696             filter_key = '{}__in'.format(field_name)
697             batch_size = connections[self.db].features.max_query_params
698             id_list = tuple(id_list)
699             # If the database has a limit on the number of query parameters
700             # (e.g. SQLite), retrieve objects in batches if necessary.
701             if batch_size and batch_size < len(id_list):
702                 qs = ()
703                 for offset in range(0, len(id_list), batch_size):
704                     batch = id_list[offset:offset + batch_size]
705                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
706             else:
707                 qs = self.filter(**{filter_key: id_list}).order_by()
708         else:
709             qs = self._chain()
710         return {getattr(obj, field_name): obj for obj in qs}
711 
712     def delete(self):
713         """Delete the records in the current QuerySet."""
714         assert not self.query.is_sliced, \
715             "Cannot use 'limit' or 'offset' with delete."
716 
717         if self._fields is not None:
718             raise TypeError("Cannot call delete() after .values() or .values_list()")
719 
720         del_query = self._chain()
721 
722         # The delete is actually 2 queries - one to find related objects,
723         # and one to delete. Make sure that the discovery of related
724         # objects is performed on the same database as the deletion.
725         del_query._for_write = True
726 
727         # Disable non-supported fields.
728         del_query.query.select_for_update = False
729         del_query.query.select_related = False
730         del_query.query.clear_ordering(force_empty=True)
731 
732         collector = Collector(using=del_query.db)
733         collector.collect(del_query)
734         deleted, _rows_count = collector.delete()
735 
736         # Clear the result cache, in case this QuerySet gets reused.
737         self._result_cache = None
738         return deleted, _rows_count
739 
740     delete.alters_data = True
741     delete.queryset_only = True
742 
743     def _raw_delete(self, using):
744         """
745         Delete objects found from the given queryset in single direct SQL
746         query. No signals are sent and there is no protection for cascades.
747         """
748         query = self.query.clone()
749         query.__class__ = sql.DeleteQuery
750         cursor = query.get_compiler(using).execute_sql(CURSOR)
751         return cursor.rowcount if cursor else 0
752     _raw_delete.alters_data = True
753 
754     def update(self, **kwargs):
755         """
756         Update all elements in the current QuerySet, setting all the given
757         fields to the appropriate values.
758         """
759         assert not self.query.is_sliced, \
760             "Cannot update a query once a slice has been taken."
761         self._for_write = True
762         query = self.query.chain(sql.UpdateQuery)
763         query.add_update_values(kwargs)
764         # Clear any annotations so that they won't be present in subqueries.
765         query.annotations = {}
766         with transaction.mark_for_rollback_on_error(using=self.db):
767             rows = query.get_compiler(self.db).execute_sql(CURSOR)
768         self._result_cache = None
769         return rows
770     update.alters_data = True
771 
772     def _update(self, values):
773         """
774         A version of update() that accepts field objects instead of field names.
775         Used primarily for model saving and not intended for use by general
776         code (it requires too much poking around at model internals to be
777         useful at that level).
778         """
779         assert not self.query.is_sliced, \
780             "Cannot update a query once a slice has been taken."
781         query = self.query.chain(sql.UpdateQuery)
782         query.add_update_fields(values)
783         # Clear any annotations so that they won't be present in subqueries.
784         query.annotations = {}
785         self._result_cache = None
786         return query.get_compiler(self.db).execute_sql(CURSOR)
787     _update.alters_data = True
788     _update.queryset_only = False
789 
790     def exists(self):
791         if self._result_cache is None:
792             return self.query.has_results(using=self.db)
793         return bool(self._result_cache)
794 
795     def _prefetch_related_objects(self):
796         # This method can only be called once the result cache has been filled.
797         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
798         self._prefetch_done = True
799 
800     def explain(self, *, format=None, **options):
801         return self.query.explain(using=self.db, format=format, **options)
802 
803     ##################################################
804     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
805     ##################################################
806 
807     def raw(self, raw_query, params=None, translations=None, using=None):
808         if using is None:
809             using = self.db
810         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
811         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
812         return qs
813 
814     def _values(self, *fields, **expressions):
815         clone = self._chain()
816         if expressions:
817             clone = clone.annotate(**expressions)
818         clone._fields = fields
819         clone.query.set_values(fields)
820         return clone
821 
822     def values(self, *fields, **expressions):
823         fields += tuple(expressions)
824         clone = self._values(*fields, **expressions)
825         clone._iterable_class = ValuesIterable
826         return clone
827 
828     def values_list(self, *fields, flat=False, named=False):
829         if flat and named:
830             raise TypeError("'flat' and 'named' can't be used together.")
831         if flat and len(fields) > 1:
832             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
833 
834         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
835         _fields = []
836         expressions = {}
837         counter = 1
838         for field in fields:
839             if hasattr(field, 'resolve_expression'):
840                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
841                 while True:
842                     field_id = field_id_prefix + str(counter)
843                     counter += 1
844                     if field_id not in field_names:
845                         break
846                 expressions[field_id] = field
847                 _fields.append(field_id)
848             else:
849                 _fields.append(field)
850 
851         clone = self._values(*_fields, **expressions)
852         clone._iterable_class = (
853             NamedValuesListIterable if named
854             else FlatValuesListIterable if flat
855             else ValuesListIterable
856         )
857         return clone
858 
859     def dates(self, field_name, kind, order='ASC'):
860         """
861         Return a list of date objects representing all available dates for
862         the given field_name, scoped to 'kind'.
863         """
864         assert kind in ('year', 'month', 'week', 'day'), \
865             "'kind' must be one of 'year', 'month', 'week', or 'day'."
866         assert order in ('ASC', 'DESC'), \
867             "'order' must be either 'ASC' or 'DESC'."
868         return self.annotate(
869             datefield=Trunc(field_name, kind, output_field=DateField()),
870             plain_field=F(field_name)
871         ).values_list(
872             'datefield', flat=True
873         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
874 
875     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
876         """
877         Return a list of datetime objects representing all available
878         datetimes for the given field_name, scoped to 'kind'.
879         """
880         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
881             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
882         assert order in ('ASC', 'DESC'), \
883             "'order' must be either 'ASC' or 'DESC'."
884         if settings.USE_TZ:
885             if tzinfo is None:
886                 tzinfo = timezone.get_current_timezone()
887         else:
888             tzinfo = None
889         return self.annotate(
890             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
891             plain_field=F(field_name)
892         ).values_list(
893             'datetimefield', flat=True
894         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
895 
896     def none(self):
897         """Return an empty QuerySet."""
898         clone = self._chain()
899         clone.query.set_empty()
900         return clone
901 
902     ##################################################################
903     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
904     ##################################################################
905 
906     def all(self):
907         """
908         Return a new QuerySet that is a copy of the current one. This allows a
909         QuerySet to proxy for a model manager in some cases.
910         """
911         return self._chain()
912 
913     def filter(self, *args, **kwargs):
914         """
915         Return a new QuerySet instance with the args ANDed to the existing
916         set.
917         """
918         self._not_support_combined_queries('filter')
919         return self._filter_or_exclude(False, *args, **kwargs)
920 
921     def exclude(self, *args, **kwargs):
922         """
923         Return a new QuerySet instance with NOT (args) ANDed to the existing
924         set.
925         """
926         self._not_support_combined_queries('exclude')
927         return self._filter_or_exclude(True, *args, **kwargs)
928 
929     def _filter_or_exclude(self, negate, *args, **kwargs):
930         if args or kwargs:
931             assert not self.query.is_sliced, \
932                 "Cannot filter a query once a slice has been taken."
933 
934         clone = self._chain()
935         if self._defer_next_filter:
936             self._defer_next_filter = False
937             clone._deferred_filter = negate, args, kwargs
938         else:
939             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
940         return clone
941 
942     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
943         if negate:
944             self._query.add_q(~Q(*args, **kwargs))
945         else:
946             self._query.add_q(Q(*args, **kwargs))
947 
948     def complex_filter(self, filter_obj):
949         """
950         Return a new QuerySet instance with filter_obj added to the filters.
951 
952         filter_obj can be a Q object or a dictionary of keyword lookup
953         arguments.
954 
955         This exists to support framework features such as 'limit_choices_to',
956         and usually it will be more natural to use other methods.
957         """
958         if isinstance(filter_obj, Q):
959             clone = self._chain()
960             clone.query.add_q(filter_obj)
961             return clone
962         else:
963             return self._filter_or_exclude(False, **filter_obj)
964 
965     def _combinator_query(self, combinator, *other_qs, all=False):
966         # Clone the query to inherit the select list and everything
967         clone = self._chain()
968         # Clear limits and ordering so they can be reapplied
969         clone.query.clear_ordering(True)
970         clone.query.clear_limits()
971         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
972         clone.query.combinator = combinator
973         clone.query.combinator_all = all
974         return clone
975 
976     def union(self, *other_qs, all=False):
977         # If the query is an EmptyQuerySet, combine all nonempty querysets.
978         if isinstance(self, EmptyQuerySet):
979             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
980             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
981         return self._combinator_query('union', *other_qs, all=all)
982 
983     def intersection(self, *other_qs):
984         # If any query is an EmptyQuerySet, return it.
985         if isinstance(self, EmptyQuerySet):
986             return self
987         for other in other_qs:
988             if isinstance(other, EmptyQuerySet):
989                 return other
990         return self._combinator_query('intersection', *other_qs)
991 
992     def difference(self, *other_qs):
993         # If the query is an EmptyQuerySet, return it.
994         if isinstance(self, EmptyQuerySet):
995             return self
996         return self._combinator_query('difference', *other_qs)
997 
998     def select_for_update(self, nowait=False, skip_locked=False, of=()):
999         """
1000         Return a new QuerySet instance that will select objects with a
1001         FOR UPDATE lock.
1002         """
1003         if nowait and skip_locked:
1004             raise ValueError('The nowait option cannot be used with skip_locked.')
1005         obj = self._chain()
1006         obj._for_write = True
1007         obj.query.select_for_update = True
1008         obj.query.select_for_update_nowait = nowait
1009         obj.query.select_for_update_skip_locked = skip_locked
1010         obj.query.select_for_update_of = of
1011         return obj
1012 
1013     def select_related(self, *fields):
1014         """
1015         Return a new QuerySet instance that will select related objects.
1016 
1017         If fields are specified, they must be ForeignKey fields and only those
1018         related objects are included in the selection.
1019 
1020         If select_related(None) is called, clear the list.
1021         """
1022         self._not_support_combined_queries('select_related')
1023         if self._fields is not None:
1024             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1025 
1026         obj = self._chain()
1027         if fields == (None,):
1028             obj.query.select_related = False
1029         elif fields:
1030             obj.query.add_select_related(fields)
1031         else:
1032             obj.query.select_related = True
1033         return obj
1034 
1035     def prefetch_related(self, *lookups):
1036         """
1037         Return a new QuerySet instance that will prefetch the specified
1038         Many-To-One and Many-To-Many related objects when the QuerySet is
1039         evaluated.
1040 
1041         When prefetch_related() is called more than once, append to the list of
1042         prefetch lookups. If prefetch_related(None) is called, clear the list.
1043         """
1044         self._not_support_combined_queries('prefetch_related')
1045         clone = self._chain()
1046         if lookups == (None,):
1047             clone._prefetch_related_lookups = ()
1048         else:
1049             for lookup in lookups:
1050                 if isinstance(lookup, Prefetch):
1051                     lookup = lookup.prefetch_to
1052                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1053                 if lookup in self.query._filtered_relations:
1054                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1055             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1056         return clone
1057 
1058     def annotate(self, *args, **kwargs):
1059         """
1060         Return a query set in which the returned objects have been annotated
1061         with extra data or aggregations.
1062         """
1063         self._not_support_combined_queries('annotate')
1064         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1065         annotations = {}
1066         for arg in args:
1067             # The default_alias property may raise a TypeError.
1068             try:
1069                 if arg.default_alias in kwargs:
1070                     raise ValueError("The named annotation '%s' conflicts with the "
1071                                      "default name for another annotation."
1072                                      % arg.default_alias)
1073             except TypeError:
1074                 raise TypeError("Complex annotations require an alias")
1075             annotations[arg.default_alias] = arg
1076         annotations.update(kwargs)
1077 
1078         clone = self._chain()
1079         names = self._fields
1080         if names is None:
1081             names = set(chain.from_iterable(
1082                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1083                 for field in self.model._meta.get_fields()
1084             ))
1085 
1086         for alias, annotation in annotations.items():
1087             if alias in names:
1088                 raise ValueError("The annotation '%s' conflicts with a field on "
1089                                  "the model." % alias)
1090             if isinstance(annotation, FilteredRelation):
1091                 clone.query.add_filtered_relation(annotation, alias)
1092             else:
1093                 clone.query.add_annotation(annotation, alias, is_summary=False)
1094 
1095         for alias, annotation in clone.query.annotations.items():
1096             if alias in annotations and annotation.contains_aggregate:
1097                 if clone._fields is None:
1098                     clone.query.group_by = True
1099                 else:
1100                     clone.query.set_group_by()
1101                 break
1102 
1103         return clone
1104 
1105     def order_by(self, *field_names):
1106         """Return a new QuerySet instance with the ordering changed."""
1107         assert not self.query.is_sliced, \
1108             "Cannot reorder a query once a slice has been taken."
1109         obj = self._chain()
1110         obj.query.clear_ordering(force_empty=False)
1111         obj.query.add_ordering(*field_names)
1112         return obj
1113 
1114     def distinct(self, *field_names):
1115         """
1116         Return a new QuerySet instance that will select only distinct results.
1117         """
1118         assert not self.query.is_sliced, \
1119             "Cannot create distinct fields once a slice has been taken."
1120         obj = self._chain()
1121         obj.query.add_distinct_fields(*field_names)
1122         return obj
1123 
1124     def extra(self, select=None, where=None, params=None, tables=None,
1125               order_by=None, select_params=None):
1126         """Add extra SQL fragments to the query."""
1127         self._not_support_combined_queries('extra')
1128         assert not self.query.is_sliced, \
1129             "Cannot change a query once a slice has been taken"
1130         clone = self._chain()
1131         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1132         return clone
1133 
1134     def reverse(self):
1135         """Reverse the ordering of the QuerySet."""
1136         if self.query.is_sliced:
1137             raise TypeError('Cannot reverse a query once a slice has been taken.')
1138         clone = self._chain()
1139         clone.query.standard_ordering = not clone.query.standard_ordering
1140         return clone
1141 
1142     def defer(self, *fields):
1143         """
1144         Defer the loading of data for certain fields until they are accessed.
1145         Add the set of deferred fields to any existing set of deferred fields.
1146         The only exception to this is if None is passed in as the only
1147         parameter, in which case removal all deferrals.
1148         """
1149         self._not_support_combined_queries('defer')
1150         if self._fields is not None:
1151             raise TypeError("Cannot call defer() after .values() or .values_list()")
1152         clone = self._chain()
1153         if fields == (None,):
1154             clone.query.clear_deferred_loading()
1155         else:
1156             clone.query.add_deferred_loading(fields)
1157         return clone
1158 
1159     def only(self, *fields):
1160         """
1161         Essentially, the opposite of defer(). Only the fields passed into this
1162         method and that are not already specified as deferred are loaded
1163         immediately when the queryset is evaluated.
1164         """
1165         self._not_support_combined_queries('only')
1166         if self._fields is not None:
1167             raise TypeError("Cannot call only() after .values() or .values_list()")
1168         if fields == (None,):
1169             # Can only pass None to defer(), not only(), as the rest option.
1170             # That won't stop people trying to do this, so let's be explicit.
1171             raise TypeError("Cannot pass None as an argument to only().")
1172         for field in fields:
1173             field = field.split(LOOKUP_SEP, 1)[0]
1174             if field in self.query._filtered_relations:
1175                 raise ValueError('only() is not supported with FilteredRelation.')
1176         clone = self._chain()
1177         clone.query.add_immediate_loading(fields)
1178         return clone
1179 
1180     def using(self, alias):
1181         """Select which database this QuerySet should execute against."""
1182         clone = self._chain()
1183         clone._db = alias
1184         return clone
1185 
1186     ###################################
1187     # PUBLIC INTROSPECTION ATTRIBUTES #
1188     ###################################
1189 
1190     @property
1191     def ordered(self):
1192         """
1193         Return True if the QuerySet is ordered -- i.e. has an order_by()
1194         clause or a default ordering on the model (or is empty).
1195         """
1196         if isinstance(self, EmptyQuerySet):
1197             return True
1198         if self.query.extra_order_by or self.query.order_by:
1199             return True
1200         elif self.query.default_ordering and self.query.get_meta().ordering:
1201             return True
1202         else:
1203             return False
1204 
1205     @property
1206     def db(self):
1207         """Return the database used if this query is executed now."""
1208         if self._for_write:
1209             return self._db or router.db_for_write(self.model, **self._hints)
1210         return self._db or router.db_for_read(self.model, **self._hints)
1211 
1212     ###################
1213     # PRIVATE METHODS #
1214     ###################
1215 
1216     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1217         """
1218         Insert a new record for the given model. This provides an interface to
1219         the InsertQuery class and is how Model.save() is implemented.
1220         """
1221         self._for_write = True
1222         if using is None:
1223             using = self.db
1224         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1225         query.insert_values(fields, objs, raw=raw)
1226         return query.get_compiler(using=using).execute_sql(returning_fields)
1227     _insert.alters_data = True
1228     _insert.queryset_only = False
1229 
1230     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1231         """
1232         Helper method for bulk_create() to insert objs one batch at a time.
1233         """
1234         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1235             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1236         ops = connections[self.db].ops
1237         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1238         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1239         inserted_rows = []
1240         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1241         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1242             if bulk_return and not ignore_conflicts:
1243                 inserted_columns = self._insert(
1244                     item, fields=fields, using=self.db,
1245                     returning_fields=self.model._meta.db_returning_fields,
1246                     ignore_conflicts=ignore_conflicts,
1247                 )
1248                 if isinstance(inserted_columns, list):
1249                     inserted_rows.extend(inserted_columns)
1250                 else:
1251                     inserted_rows.append(inserted_columns)
1252             else:
1253                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1254         return inserted_rows
1255 
1256     def _chain(self, **kwargs):
1257         """
1258         Return a copy of the current QuerySet that's ready for another
1259         operation.
1260         """
1261         obj = self._clone()
1262         if obj._sticky_filter:
1263             obj.query.filter_is_sticky = True
1264             obj._sticky_filter = False
1265         obj.__dict__.update(kwargs)
1266         return obj
1267 
1268     def _clone(self):
1269         """
1270         Return a copy of the current QuerySet. A lightweight alternative
1271         to deepcopy().
1272         """
1273         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1274         c._sticky_filter = self._sticky_filter
1275         c._for_write = self._for_write
1276         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1277         c._known_related_objects = self._known_related_objects
1278         c._iterable_class = self._iterable_class
1279         c._fields = self._fields
1280         return c
1281 
1282     def _fetch_all(self):
1283         if self._result_cache is None:
1284             self._result_cache = list(self._iterable_class(self))
1285         if self._prefetch_related_lookups and not self._prefetch_done:
1286             self._prefetch_related_objects()
1287 
1288     def _next_is_sticky(self):
1289         """
1290         Indicate that the next filter call and the one following that should
1291         be treated as a single filter. This is only important when it comes to
1292         determining when to reuse tables for many-to-many filters. Required so
1293         that we can filter naturally on the results of related managers.
1294 
1295         This doesn't return a clone of the current QuerySet (it returns
1296         "self"). The method is only used internally and should be immediately
1297         followed by a filter() that does create a clone.
1298         """
1299         self._sticky_filter = True
1300         return self
1301 
1302     def _merge_sanity_check(self, other):
1303         """Check that two QuerySet classes may be merged."""
1304         if self._fields is not None and (
1305                 set(self.query.values_select) != set(other.query.values_select) or
1306                 set(self.query.extra_select) != set(other.query.extra_select) or
1307                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1308             raise TypeError(
1309                 "Merging '%s' classes must involve the same values in each case."
1310                 % self.__class__.__name__
1311             )
1312 
1313     def _merge_known_related_objects(self, other):
1314         """
1315         Keep track of all known related objects from either QuerySet instance.
1316         """
1317         for field, objects in other._known_related_objects.items():
1318             self._known_related_objects.setdefault(field, {}).update(objects)
1319 
1320     def resolve_expression(self, *args, **kwargs):
1321         if self._fields and len(self._fields) > 1:
1322             # values() queryset can only be used as nested queries
1323             # if they are set up to select only a single field.
1324             raise TypeError('Cannot use multi-field values as a filter value.')
1325         query = self.query.resolve_expression(*args, **kwargs)
1326         query._db = self._db
1327         return query
1328     resolve_expression.queryset_only = True
1329 
1330     def _add_hints(self, **hints):
1331         """
1332         Update hinting information for use by routers. Add new key/values or
1333         overwrite existing key/values.
1334         """
1335         self._hints.update(hints)
1336 
1337     def _has_filters(self):
1338         """
1339         Check if this QuerySet has any filtering going on. This isn't
1340         equivalent with checking if all objects are present in results, for
1341         example, qs[1:]._has_filters() -> False.
1342         """
1343         return self.query.has_filters()
1344 
1345     @staticmethod
1346     def _validate_values_are_expressions(values, method_name):
1347         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1348         if invalid_args:
1349             raise TypeError(
1350                 'QuerySet.%s() received non-expression(s): %s.' % (
1351                     method_name,
1352                     ', '.join(invalid_args),
1353                 )
1354             )
1355 
1356     def _not_support_combined_queries(self, operation_name):
1357         if self.query.combinator:
1358             raise NotSupportedError(
1359                 'Calling QuerySet.%s() after %s() is not supported.'
1360                 % (operation_name, self.query.combinator)
1361             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-20 11:43:27.224 | INFO     | Search completed. Bug locations: [<file>django/db/models/expressions.py</file>
<class>Window</class>
<method>as_sql</method>
<code>
1201     def as_sql(self, compiler, connection, template=None):
1202         connection.ops.check_expression_support(self)
1203         if not connection.features.supports_over_clause:
1204             raise NotSupportedError('This backend does not support window expressions.')
1205         expr_sql, params = compiler.compile(self.source_expression)
1206         window_sql, window_params = [], []
1207 
1208         if self.partition_by is not None:
1209             sql_expr, sql_params = self.partition_by.as_sql(
1210                 compiler=compiler, connection=connection,
1211                 template='PARTITION BY %(expressions)s',
1212             )
1213             window_sql.extend(sql_expr)
1214             window_params.extend(sql_params)
1215 
1216         if self.order_by is not None:
1217             window_sql.append(' ORDER BY ')
1218             order_sql, order_params = compiler.compile(self.order_by)
1219             window_sql.extend(order_sql)
1220             window_params.extend(order_params)
1221 
1222         if self.frame:
1223             frame_sql, frame_params = compiler.compile(self.frame)
1224             window_sql.append(' ' + frame_sql)
1225             window_params.extend(frame_params)
1226 
1227         params.extend(window_params)
1228         template = template or self.template
1229 
1230         return template % {
1231             'expression': expr_sql,
1232             'window': ''.join(window_sql).strip()
1233         }, params

</code><intended_behavior>The method should be reviewed and possibly adjusted to ensure that window expressions can be used within Case and When expressions without raising NotSupportedError when used in the SELECT clause. This adjustment must preserve the SQL standard's compliance that disallows window functions in the WHERE clause but permits them in the SELECT clause. The method might need to include logic to identify when a Window expression is properly used within a SELECT clause context and bypass restrictions that incorrectly flag this as an error.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Window</class>
<method>None</method>
<code>
1156 class Window(Expression):
1157     template = '%(expression)s OVER (%(window)s)'
1158     # Although the main expression may either be an aggregate or an
1159     # expression with an aggregate function, the GROUP BY that will
1160     # be introduced in the query as a result is not desired.
1161     contains_aggregate = False
1162     contains_over_clause = True
1163     filterable = False
1164 
1165     def __init__(self, expression, partition_by=None, order_by=None, frame=None, output_field=None):
1166         self.partition_by = partition_by
1167         self.order_by = order_by
1168         self.frame = frame
1169 
1170         if not getattr(expression, 'window_compatible', False):
1171             raise ValueError(
1172                 "Expression '%s' isn't compatible with OVER clauses." %
1173                 expression.__class__.__name__
1174             )
1175 
1176         if self.partition_by is not None:
1177             if not isinstance(self.partition_by, (tuple, list)):
1178                 self.partition_by = (self.partition_by,)
1179             self.partition_by = ExpressionList(*self.partition_by)
1180 
1181         if self.order_by is not None:
1182             if isinstance(self.order_by, (list, tuple)):
1183                 self.order_by = ExpressionList(*self.order_by)
1184             elif not isinstance(self.order_by, BaseExpression):
1185                 raise ValueError(
1186                     'order_by must be either an Expression or a sequence of '
1187                     'expressions.'
1188                 )
1189         super().__init__(output_field=output_field)
1190         self.source_expression = self._parse_expressions(expression)[0]
1191 
1192     def _resolve_output_field(self):
1193         return self.source_expression.output_field
1194 
1195     def get_source_expressions(self):
1196         return [self.source_expression, self.partition_by, self.order_by, self.frame]
1197 
1198     def set_source_expressions(self, exprs):
1199         self.source_expression, self.partition_by, self.order_by, self.frame = exprs
1200 
1201     def as_sql(self, compiler, connection, template=None):
1202         connection.ops.check_expression_support(self)
1203         if not connection.features.supports_over_clause:
1204             raise NotSupportedError('This backend does not support window expressions.')
1205         expr_sql, params = compiler.compile(self.source_expression)
1206         window_sql, window_params = [], []
1207 
1208         if self.partition_by is not None:
1209             sql_expr, sql_params = self.partition_by.as_sql(
1210                 compiler=compiler, connection=connection,
1211                 template='PARTITION BY %(expressions)s',
1212             )
1213             window_sql.extend(sql_expr)
1214             window_params.extend(sql_params)
1215 
1216         if self.order_by is not None:
1217             window_sql.append(' ORDER BY ')
1218             order_sql, order_params = compiler.compile(self.order_by)
1219             window_sql.extend(order_sql)
1220             window_params.extend(order_params)
1221 
1222         if self.frame:
1223             frame_sql, frame_params = compiler.compile(self.frame)
1224             window_sql.append(' ' + frame_sql)
1225             window_params.extend(frame_params)
1226 
1227         params.extend(window_params)
1228         template = template or self.template
1229 
1230         return template % {
1231             'expression': expr_sql,
1232             'window': ''.join(window_sql).strip()
1233         }, params
1234 
1235     def __str__(self):
1236         return '{} OVER ({}{}{})'.format(
1237             str(self.source_expression),
1238             'PARTITION BY ' + str(self.partition_by) if self.partition_by else '',
1239             'ORDER BY ' + str(self.order_by) if self.order_by else '',
1240             str(self.frame or ''),
1241         )
1242 
1243     def __repr__(self):
1244         return '<%s: %s>' % (self.__class__.__name__, self)
1245 
1246     def get_group_by_cols(self, alias=None):
1247         return []

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Case</class>
<method>None</method>
<code>
918 class Case(Expression):
919     """
920     An SQL searched CASE expression:
921 
922         CASE
923             WHEN n > 0
924                 THEN 'positive'
925             WHEN n < 0
926                 THEN 'negative'
927             ELSE 'zero'
928         END
929     """
930     template = 'CASE %(cases)s ELSE %(default)s END'
931     case_joiner = ' '
932 
933     def __init__(self, *cases, default=None, output_field=None, **extra):
934         if not all(isinstance(case, When) for case in cases):
935             raise TypeError("Positional arguments must all be When objects.")
936         super().__init__(output_field)
937         self.cases = list(cases)
938         self.default = self._parse_expressions(default)[0]
939         self.extra = extra
940 
941     def __str__(self):
942         return "CASE %s, ELSE %r" % (', '.join(str(c) for c in self.cases), self.default)
943 
944     def __repr__(self):
945         return "<%s: %s>" % (self.__class__.__name__, self)
946 
947     def get_source_expressions(self):
948         return self.cases + [self.default]
949 
950     def set_source_expressions(self, exprs):
951         *self.cases, self.default = exprs
952 
953     def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
954         c = self.copy()
955         c.is_summary = summarize
956         for pos, case in enumerate(c.cases):
957             c.cases[pos] = case.resolve_expression(query, allow_joins, reuse, summarize, for_save)
958         c.default = c.default.resolve_expression(query, allow_joins, reuse, summarize, for_save)
959         return c
960 
961     def copy(self):
962         c = super().copy()
963         c.cases = c.cases[:]
964         return c
965 
966     def as_sql(self, compiler, connection, template=None, case_joiner=None, **extra_context):
967         connection.ops.check_expression_support(self)
968         if not self.cases:
969             return compiler.compile(self.default)
970         template_params = {**self.extra, **extra_context}
971         case_parts = []
972         sql_params = []
973         for case in self.cases:
974             try:
975                 case_sql, case_params = compiler.compile(case)
976             except EmptyResultSet:
977                 continue
978             case_parts.append(case_sql)
979             sql_params.extend(case_params)
980         default_sql, default_params = compiler.compile(self.default)
981         if not case_parts:
982             return default_sql, default_params
983         case_joiner = case_joiner or self.case_joiner
984         template_params['cases'] = case_joiner.join(case_parts)
985         template_params['default'] = default_sql
986         sql_params.extend(default_params)
987         template = template or template_params.get('template', self.template)
988         sql = template % template_params
989         if self._output_field_or_none is not None:
990             sql = connection.ops.unification_cast_sql(self.output_field) % sql
991         return sql, sql_params

</code><intended_behavior>These methods might require adjustments to handle Window expressions without errors when constructing SQL queries. They should ensure that Window expressions encapsulated within Case statements are translated into SQL without triggering validation errors that are not applicable for their use in SELECT clauses. This includes properly handling the SQL generation to reflect the intended logic of window expressions used in Case and When.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1058     def annotate(self, *args, **kwargs):
1059         """
1060         Return a query set in which the returned objects have been annotated
1061         with extra data or aggregations.
1062         """
1063         self._not_support_combined_queries('annotate')
1064         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1065         annotations = {}
1066         for arg in args:
1067             # The default_alias property may raise a TypeError.
1068             try:
1069                 if arg.default_alias in kwargs:
1070                     raise ValueError("The named annotation '%s' conflicts with the "
1071                                      "default name for another annotation."
1072                                      % arg.default_alias)
1073             except TypeError:
1074                 raise TypeError("Complex annotations require an alias")
1075             annotations[arg.default_alias] = arg
1076         annotations.update(kwargs)
1077 
1078         clone = self._chain()
1079         names = self._fields
1080         if names is None:
1081             names = set(chain.from_iterable(
1082                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1083                 for field in self.model._meta.get_fields()
1084             ))
1085 
1086         for alias, annotation in annotations.items():
1087             if alias in names:
1088                 raise ValueError("The annotation '%s' conflicts with a field on "
1089                                  "the model." % alias)
1090             if isinstance(annotation, FilteredRelation):
1091                 clone.query.add_filtered_relation(annotation, alias)
1092             else:
1093                 clone.query.add_annotation(annotation, alias, is_summary=False)
1094 
1095         for alias, annotation in clone.query.annotations.items():
1096             if alias in annotations and annotation.contains_aggregate:
1097                 if clone._fields is None:
1098                     clone.query.group_by = True
1099                 else:
1100                     clone.query.set_group_by()
1101                 break
1102 
1103         return clone

</code><intended_behavior>This method's validation logic might need refinement to allow Window expressions within conditional annotations. Specifically, the method should preserve its functionality while also supporting the inclusion of Window functions within Case expressions as part of annotations. The validation process should differentiate between the unsupported use of window functions (e.g., in WHERE clauses directly) and supported uses, such as within the SELECT clause inside conditional statements.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
186 class QuerySet:
187     """Represent a lazy database lookup for a set of objects."""
188 
189     def __init__(self, model=None, query=None, using=None, hints=None):
190         self.model = model
191         self._db = using
192         self._hints = hints or {}
193         self._query = query or sql.Query(self.model)
194         self._result_cache = None
195         self._sticky_filter = False
196         self._for_write = False
197         self._prefetch_related_lookups = ()
198         self._prefetch_done = False
199         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
200         self._iterable_class = ModelIterable
201         self._fields = None
202         self._defer_next_filter = False
203         self._deferred_filter = None
204 
205     @property
206     def query(self):
207         if self._deferred_filter:
208             negate, args, kwargs = self._deferred_filter
209             self._filter_or_exclude_inplace(negate, *args, **kwargs)
210             self._deferred_filter = None
211         return self._query
212 
213     @query.setter
214     def query(self, value):
215         self._query = value
216 
217     def as_manager(cls):
218         # Address the circular dependency between `Queryset` and `Manager`.
219         from django.db.models.manager import Manager
220         manager = Manager.from_queryset(cls)()
221         manager._built_with_as_manager = True
222         return manager
223     as_manager.queryset_only = True
224     as_manager = classmethod(as_manager)
225 
226     ########################
227     # PYTHON MAGIC METHODS #
228     ########################
229 
230     def __deepcopy__(self, memo):
231         """Don't populate the QuerySet's cache."""
232         obj = self.__class__()
233         for k, v in self.__dict__.items():
234             if k == '_result_cache':
235                 obj.__dict__[k] = None
236             else:
237                 obj.__dict__[k] = copy.deepcopy(v, memo)
238         return obj
239 
240     def __getstate__(self):
241         # Force the cache to be fully populated.
242         self._fetch_all()
243         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
244 
245     def __setstate__(self, state):
246         msg = None
247         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
248         if pickled_version:
249             current_version = get_version()
250             if current_version != pickled_version:
251                 msg = (
252                     "Pickled queryset instance's Django version %s does not "
253                     "match the current version %s." % (pickled_version, current_version)
254                 )
255         else:
256             msg = "Pickled queryset instance's Django version is not specified."
257 
258         if msg:
259             warnings.warn(msg, RuntimeWarning, stacklevel=2)
260 
261         self.__dict__.update(state)
262 
263     def __repr__(self):
264         data = list(self[:REPR_OUTPUT_SIZE + 1])
265         if len(data) > REPR_OUTPUT_SIZE:
266             data[-1] = "...(remaining elements truncated)..."
267         return '<%s %r>' % (self.__class__.__name__, data)
268 
269     def __len__(self):
270         self._fetch_all()
271         return len(self._result_cache)
272 
273     def __iter__(self):
274         """
275         The queryset iterator protocol uses three nested iterators in the
276         default case:
277             1. sql.compiler.execute_sql()
278                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
279                  using cursor.fetchmany(). This part is responsible for
280                  doing some column masking, and returning the rows in chunks.
281             2. sql.compiler.results_iter()
282                - Returns one row at time. At this point the rows are still just
283                  tuples. In some cases the return values are converted to
284                  Python values at this location.
285             3. self.iterator()
286                - Responsible for turning the rows into model objects.
287         """
288         self._fetch_all()
289         return iter(self._result_cache)
290 
291     def __bool__(self):
292         self._fetch_all()
293         return bool(self._result_cache)
294 
295     def __getitem__(self, k):
296         """Retrieve an item or slice from the set of results."""
297         if not isinstance(k, (int, slice)):
298             raise TypeError(
299                 'QuerySet indices must be integers or slices, not %s.'
300                 % type(k).__name__
301             )
302         assert ((not isinstance(k, slice) and (k >= 0)) or
303                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
304                  (k.stop is None or k.stop >= 0))), \
305             "Negative indexing is not supported."
306 
307         if self._result_cache is not None:
308             return self._result_cache[k]
309 
310         if isinstance(k, slice):
311             qs = self._chain()
312             if k.start is not None:
313                 start = int(k.start)
314             else:
315                 start = None
316             if k.stop is not None:
317                 stop = int(k.stop)
318             else:
319                 stop = None
320             qs.query.set_limits(start, stop)
321             return list(qs)[::k.step] if k.step else qs
322 
323         qs = self._chain()
324         qs.query.set_limits(k, k + 1)
325         qs._fetch_all()
326         return qs._result_cache[0]
327 
328     def __and__(self, other):
329         self._merge_sanity_check(other)
330         if isinstance(other, EmptyQuerySet):
331             return other
332         if isinstance(self, EmptyQuerySet):
333             return self
334         combined = self._chain()
335         combined._merge_known_related_objects(other)
336         combined.query.combine(other.query, sql.AND)
337         return combined
338 
339     def __or__(self, other):
340         self._merge_sanity_check(other)
341         if isinstance(self, EmptyQuerySet):
342             return other
343         if isinstance(other, EmptyQuerySet):
344             return self
345         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
346         combined = query._chain()
347         combined._merge_known_related_objects(other)
348         if not other.query.can_filter():
349             other = other.model._base_manager.filter(pk__in=other.values('pk'))
350         combined.query.combine(other.query, sql.OR)
351         return combined
352 
353     ####################################
354     # METHODS THAT DO DATABASE QUERIES #
355     ####################################
356 
357     def _iterator(self, use_chunked_fetch, chunk_size):
358         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
359 
360     def iterator(self, chunk_size=2000):
361         """
362         An iterator over the results from applying this QuerySet to the
363         database.
364         """
365         if chunk_size <= 0:
366             raise ValueError('Chunk size must be strictly positive.')
367         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
368         return self._iterator(use_chunked_fetch, chunk_size)
369 
370     def aggregate(self, *args, **kwargs):
371         """
372         Return a dictionary containing the calculations (aggregation)
373         over the current queryset.
374 
375         If args is present the expression is passed as a kwarg using
376         the Aggregate object's default alias.
377         """
378         if self.query.distinct_fields:
379             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
380         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
381         for arg in args:
382             # The default_alias property raises TypeError if default_alias
383             # can't be set automatically or AttributeError if it isn't an
384             # attribute.
385             try:
386                 arg.default_alias
387             except (AttributeError, TypeError):
388                 raise TypeError("Complex aggregates require an alias")
389             kwargs[arg.default_alias] = arg
390 
391         query = self.query.chain()
392         for (alias, aggregate_expr) in kwargs.items():
393             query.add_annotation(aggregate_expr, alias, is_summary=True)
394             if not query.annotations[alias].contains_aggregate:
395                 raise TypeError("%s is not an aggregate expression" % alias)
396         return query.get_aggregation(self.db, kwargs)
397 
398     def count(self):
399         """
400         Perform a SELECT COUNT() and return the number of records as an
401         integer.
402 
403         If the QuerySet is already fully cached, return the length of the
404         cached results set to avoid multiple SELECT COUNT(*) calls.
405         """
406         if self._result_cache is not None:
407             return len(self._result_cache)
408 
409         return self.query.get_count(using=self.db)
410 
411     def get(self, *args, **kwargs):
412         """
413         Perform the query and return a single object matching the given
414         keyword arguments.
415         """
416         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
417         if self.query.can_filter() and not self.query.distinct_fields:
418             clone = clone.order_by()
419         limit = None
420         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
421             limit = MAX_GET_RESULTS
422             clone.query.set_limits(high=limit)
423         num = len(clone)
424         if num == 1:
425             return clone._result_cache[0]
426         if not num:
427             raise self.model.DoesNotExist(
428                 "%s matching query does not exist." %
429                 self.model._meta.object_name
430             )
431         raise self.model.MultipleObjectsReturned(
432             'get() returned more than one %s -- it returned %s!' % (
433                 self.model._meta.object_name,
434                 num if not limit or num < limit else 'more than %s' % (limit - 1),
435             )
436         )
437 
438     def create(self, **kwargs):
439         """
440         Create a new object with the given kwargs, saving it to the database
441         and returning the created object.
442         """
443         obj = self.model(**kwargs)
444         self._for_write = True
445         obj.save(force_insert=True, using=self.db)
446         return obj
447 
448     def _populate_pk_values(self, objs):
449         for obj in objs:
450             if obj.pk is None:
451                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
452 
453     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
454         """
455         Insert each of the instances into the database. Do *not* call
456         save() on each of the instances, do not send any pre/post_save
457         signals, and do not set the primary key attribute if it is an
458         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
459         Multi-table models are not supported.
460         """
461         # When you bulk insert you don't get the primary keys back (if it's an
462         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
463         # you can't insert into the child tables which references this. There
464         # are two workarounds:
465         # 1) This could be implemented if you didn't have an autoincrement pk
466         # 2) You could do it by doing O(n) normal inserts into the parent
467         #    tables to get the primary keys back and then doing a single bulk
468         #    insert into the childmost table.
469         # We currently set the primary keys on the objects when using
470         # PostgreSQL via the RETURNING ID clause. It should be possible for
471         # Oracle as well, but the semantics for extracting the primary keys is
472         # trickier so it's not done yet.
473         assert batch_size is None or batch_size > 0
474         # Check that the parents share the same concrete model with the our
475         # model to detect the inheritance pattern ConcreteGrandParent ->
476         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
477         # would not identify that case as involving multiple tables.
478         for parent in self.model._meta.get_parent_list():
479             if parent._meta.concrete_model is not self.model._meta.concrete_model:
480                 raise ValueError("Can't bulk create a multi-table inherited model")
481         if not objs:
482             return objs
483         self._for_write = True
484         connection = connections[self.db]
485         opts = self.model._meta
486         fields = opts.concrete_fields
487         objs = list(objs)
488         self._populate_pk_values(objs)
489         with transaction.atomic(using=self.db, savepoint=False):
490             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
491             if objs_with_pk:
492                 returned_columns = self._batched_insert(
493                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
494                 )
495                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
496                     for result, field in zip(results, opts.db_returning_fields):
497                         if field != opts.pk:
498                             setattr(obj_with_pk, field.attname, result)
499                 for obj_with_pk in objs_with_pk:
500                     obj_with_pk._state.adding = False
501                     obj_with_pk._state.db = self.db
502             if objs_without_pk:
503                 fields = [f for f in fields if not isinstance(f, AutoField)]
504                 returned_columns = self._batched_insert(
505                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
506                 )
507                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
508                     assert len(returned_columns) == len(objs_without_pk)
509                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
510                     for result, field in zip(results, opts.db_returning_fields):
511                         setattr(obj_without_pk, field.attname, result)
512                     obj_without_pk._state.adding = False
513                     obj_without_pk._state.db = self.db
514 
515         return objs
516 
517     def bulk_update(self, objs, fields, batch_size=None):
518         """
519         Update the given fields in each of the given objects in the database.
520         """
521         if batch_size is not None and batch_size < 0:
522             raise ValueError('Batch size must be a positive integer.')
523         if not fields:
524             raise ValueError('Field names must be given to bulk_update().')
525         objs = tuple(objs)
526         if any(obj.pk is None for obj in objs):
527             raise ValueError('All bulk_update() objects must have a primary key set.')
528         fields = [self.model._meta.get_field(name) for name in fields]
529         if any(not f.concrete or f.many_to_many for f in fields):
530             raise ValueError('bulk_update() can only be used with concrete fields.')
531         if any(f.primary_key for f in fields):
532             raise ValueError('bulk_update() cannot be used with primary key fields.')
533         if not objs:
534             return
535         # PK is used twice in the resulting update query, once in the filter
536         # and once in the WHEN. Each field will also have one CAST.
537         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
538         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
539         requires_casting = connections[self.db].features.requires_casted_case_in_updates
540         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
541         updates = []
542         for batch_objs in batches:
543             update_kwargs = {}
544             for field in fields:
545                 when_statements = []
546                 for obj in batch_objs:
547                     attr = getattr(obj, field.attname)
548                     if not isinstance(attr, Expression):
549                         attr = Value(attr, output_field=field)
550                     when_statements.append(When(pk=obj.pk, then=attr))
551                 case_statement = Case(*when_statements, output_field=field)
552                 if requires_casting:
553                     case_statement = Cast(case_statement, output_field=field)
554                 update_kwargs[field.attname] = case_statement
555             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
556         with transaction.atomic(using=self.db, savepoint=False):
557             for pks, update_kwargs in updates:
558                 self.filter(pk__in=pks).update(**update_kwargs)
559     bulk_update.alters_data = True
560 
561     def get_or_create(self, defaults=None, **kwargs):
562         """
563         Look up an object with the given kwargs, creating one if necessary.
564         Return a tuple of (object, created), where created is a boolean
565         specifying whether an object was created.
566         """
567         # The get() needs to be targeted at the write database in order
568         # to avoid potential transaction consistency problems.
569         self._for_write = True
570         try:
571             return self.get(**kwargs), False
572         except self.model.DoesNotExist:
573             params = self._extract_model_params(defaults, **kwargs)
574             return self._create_object_from_params(kwargs, params)
575 
576     def update_or_create(self, defaults=None, **kwargs):
577         """
578         Look up an object with the given kwargs, updating one with defaults
579         if it exists, otherwise create a new one.
580         Return a tuple (object, created), where created is a boolean
581         specifying whether an object was created.
582         """
583         defaults = defaults or {}
584         self._for_write = True
585         with transaction.atomic(using=self.db):
586             try:
587                 obj = self.select_for_update().get(**kwargs)
588             except self.model.DoesNotExist:
589                 params = self._extract_model_params(defaults, **kwargs)
590                 # Lock the row so that a concurrent update is blocked until
591                 # after update_or_create() has performed its save.
592                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
593                 if created:
594                     return obj, created
595             for k, v in resolve_callables(defaults):
596                 setattr(obj, k, v)
597             obj.save(using=self.db)
598         return obj, False
599 
600     def _create_object_from_params(self, lookup, params, lock=False):
601         """
602         Try to create an object using passed params. Used by get_or_create()
603         and update_or_create().
604         """
605         try:
606             with transaction.atomic(using=self.db):
607                 params = dict(resolve_callables(params))
608                 obj = self.create(**params)
609             return obj, True
610         except IntegrityError as e:
611             try:
612                 qs = self.select_for_update() if lock else self
613                 return qs.get(**lookup), False
614             except self.model.DoesNotExist:
615                 pass
616             raise e
617 
618     def _extract_model_params(self, defaults, **kwargs):
619         """
620         Prepare `params` for creating a model instance based on the given
621         kwargs; for use by get_or_create() and update_or_create().
622         """
623         defaults = defaults or {}
624         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
625         params.update(defaults)
626         property_names = self.model._meta._property_names
627         invalid_params = []
628         for param in params:
629             try:
630                 self.model._meta.get_field(param)
631             except exceptions.FieldDoesNotExist:
632                 # It's okay to use a model's property if it has a setter.
633                 if not (param in property_names and getattr(self.model, param).fset):
634                     invalid_params.append(param)
635         if invalid_params:
636             raise exceptions.FieldError(
637                 "Invalid field name(s) for model %s: '%s'." % (
638                     self.model._meta.object_name,
639                     "', '".join(sorted(invalid_params)),
640                 ))
641         return params
642 
643     def _earliest(self, *fields):
644         """
645         Return the earliest object according to fields (if given) or by the
646         model's Meta.get_latest_by.
647         """
648         if fields:
649             order_by = fields
650         else:
651             order_by = getattr(self.model._meta, 'get_latest_by')
652             if order_by and not isinstance(order_by, (tuple, list)):
653                 order_by = (order_by,)
654         if order_by is None:
655             raise ValueError(
656                 "earliest() and latest() require either fields as positional "
657                 "arguments or 'get_latest_by' in the model's Meta."
658             )
659 
660         assert not self.query.is_sliced, \
661             "Cannot change a query once a slice has been taken."
662         obj = self._chain()
663         obj.query.set_limits(high=1)
664         obj.query.clear_ordering(force_empty=True)
665         obj.query.add_ordering(*order_by)
666         return obj.get()
667 
668     def earliest(self, *fields):
669         return self._earliest(*fields)
670 
671     def latest(self, *fields):
672         return self.reverse()._earliest(*fields)
673 
674     def first(self):
675         """Return the first object of a query or None if no match is found."""
676         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
677             return obj
678 
679     def last(self):
680         """Return the last object of a query or None if no match is found."""
681         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
682             return obj
683 
684     def in_bulk(self, id_list=None, *, field_name='pk'):
685         """
686         Return a dictionary mapping each of the given IDs to the object with
687         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
688         """
689         assert not self.query.is_sliced, \
690             "Cannot use 'limit' or 'offset' with in_bulk"
691         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
692             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
693         if id_list is not None:
694             if not id_list:
695                 return {}
696             filter_key = '{}__in'.format(field_name)
697             batch_size = connections[self.db].features.max_query_params
698             id_list = tuple(id_list)
699             # If the database has a limit on the number of query parameters
700             # (e.g. SQLite), retrieve objects in batches if necessary.
701             if batch_size and batch_size < len(id_list):
702                 qs = ()
703                 for offset in range(0, len(id_list), batch_size):
704                     batch = id_list[offset:offset + batch_size]
705                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
706             else:
707                 qs = self.filter(**{filter_key: id_list}).order_by()
708         else:
709             qs = self._chain()
710         return {getattr(obj, field_name): obj for obj in qs}
711 
712     def delete(self):
713         """Delete the records in the current QuerySet."""
714         assert not self.query.is_sliced, \
715             "Cannot use 'limit' or 'offset' with delete."
716 
717         if self._fields is not None:
718             raise TypeError("Cannot call delete() after .values() or .values_list()")
719 
720         del_query = self._chain()
721 
722         # The delete is actually 2 queries - one to find related objects,
723         # and one to delete. Make sure that the discovery of related
724         # objects is performed on the same database as the deletion.
725         del_query._for_write = True
726 
727         # Disable non-supported fields.
728         del_query.query.select_for_update = False
729         del_query.query.select_related = False
730         del_query.query.clear_ordering(force_empty=True)
731 
732         collector = Collector(using=del_query.db)
733         collector.collect(del_query)
734         deleted, _rows_count = collector.delete()
735 
736         # Clear the result cache, in case this QuerySet gets reused.
737         self._result_cache = None
738         return deleted, _rows_count
739 
740     delete.alters_data = True
741     delete.queryset_only = True
742 
743     def _raw_delete(self, using):
744         """
745         Delete objects found from the given queryset in single direct SQL
746         query. No signals are sent and there is no protection for cascades.
747         """
748         query = self.query.clone()
749         query.__class__ = sql.DeleteQuery
750         cursor = query.get_compiler(using).execute_sql(CURSOR)
751         return cursor.rowcount if cursor else 0
752     _raw_delete.alters_data = True
753 
754     def update(self, **kwargs):
755         """
756         Update all elements in the current QuerySet, setting all the given
757         fields to the appropriate values.
758         """
759         assert not self.query.is_sliced, \
760             "Cannot update a query once a slice has been taken."
761         self._for_write = True
762         query = self.query.chain(sql.UpdateQuery)
763         query.add_update_values(kwargs)
764         # Clear any annotations so that they won't be present in subqueries.
765         query.annotations = {}
766         with transaction.mark_for_rollback_on_error(using=self.db):
767             rows = query.get_compiler(self.db).execute_sql(CURSOR)
768         self._result_cache = None
769         return rows
770     update.alters_data = True
771 
772     def _update(self, values):
773         """
774         A version of update() that accepts field objects instead of field names.
775         Used primarily for model saving and not intended for use by general
776         code (it requires too much poking around at model internals to be
777         useful at that level).
778         """
779         assert not self.query.is_sliced, \
780             "Cannot update a query once a slice has been taken."
781         query = self.query.chain(sql.UpdateQuery)
782         query.add_update_fields(values)
783         # Clear any annotations so that they won't be present in subqueries.
784         query.annotations = {}
785         self._result_cache = None
786         return query.get_compiler(self.db).execute_sql(CURSOR)
787     _update.alters_data = True
788     _update.queryset_only = False
789 
790     def exists(self):
791         if self._result_cache is None:
792             return self.query.has_results(using=self.db)
793         return bool(self._result_cache)
794 
795     def _prefetch_related_objects(self):
796         # This method can only be called once the result cache has been filled.
797         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
798         self._prefetch_done = True
799 
800     def explain(self, *, format=None, **options):
801         return self.query.explain(using=self.db, format=format, **options)
802 
803     ##################################################
804     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
805     ##################################################
806 
807     def raw(self, raw_query, params=None, translations=None, using=None):
808         if using is None:
809             using = self.db
810         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
811         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
812         return qs
813 
814     def _values(self, *fields, **expressions):
815         clone = self._chain()
816         if expressions:
817             clone = clone.annotate(**expressions)
818         clone._fields = fields
819         clone.query.set_values(fields)
820         return clone
821 
822     def values(self, *fields, **expressions):
823         fields += tuple(expressions)
824         clone = self._values(*fields, **expressions)
825         clone._iterable_class = ValuesIterable
826         return clone
827 
828     def values_list(self, *fields, flat=False, named=False):
829         if flat and named:
830             raise TypeError("'flat' and 'named' can't be used together.")
831         if flat and len(fields) > 1:
832             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
833 
834         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
835         _fields = []
836         expressions = {}
837         counter = 1
838         for field in fields:
839             if hasattr(field, 'resolve_expression'):
840                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
841                 while True:
842                     field_id = field_id_prefix + str(counter)
843                     counter += 1
844                     if field_id not in field_names:
845                         break
846                 expressions[field_id] = field
847                 _fields.append(field_id)
848             else:
849                 _fields.append(field)
850 
851         clone = self._values(*_fields, **expressions)
852         clone._iterable_class = (
853             NamedValuesListIterable if named
854             else FlatValuesListIterable if flat
855             else ValuesListIterable
856         )
857         return clone
858 
859     def dates(self, field_name, kind, order='ASC'):
860         """
861         Return a list of date objects representing all available dates for
862         the given field_name, scoped to 'kind'.
863         """
864         assert kind in ('year', 'month', 'week', 'day'), \
865             "'kind' must be one of 'year', 'month', 'week', or 'day'."
866         assert order in ('ASC', 'DESC'), \
867             "'order' must be either 'ASC' or 'DESC'."
868         return self.annotate(
869             datefield=Trunc(field_name, kind, output_field=DateField()),
870             plain_field=F(field_name)
871         ).values_list(
872             'datefield', flat=True
873         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
874 
875     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
876         """
877         Return a list of datetime objects representing all available
878         datetimes for the given field_name, scoped to 'kind'.
879         """
880         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
881             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
882         assert order in ('ASC', 'DESC'), \
883             "'order' must be either 'ASC' or 'DESC'."
884         if settings.USE_TZ:
885             if tzinfo is None:
886                 tzinfo = timezone.get_current_timezone()
887         else:
888             tzinfo = None
889         return self.annotate(
890             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
891             plain_field=F(field_name)
892         ).values_list(
893             'datetimefield', flat=True
894         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
895 
896     def none(self):
897         """Return an empty QuerySet."""
898         clone = self._chain()
899         clone.query.set_empty()
900         return clone
901 
902     ##################################################################
903     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
904     ##################################################################
905 
906     def all(self):
907         """
908         Return a new QuerySet that is a copy of the current one. This allows a
909         QuerySet to proxy for a model manager in some cases.
910         """
911         return self._chain()
912 
913     def filter(self, *args, **kwargs):
914         """
915         Return a new QuerySet instance with the args ANDed to the existing
916         set.
917         """
918         self._not_support_combined_queries('filter')
919         return self._filter_or_exclude(False, *args, **kwargs)
920 
921     def exclude(self, *args, **kwargs):
922         """
923         Return a new QuerySet instance with NOT (args) ANDed to the existing
924         set.
925         """
926         self._not_support_combined_queries('exclude')
927         return self._filter_or_exclude(True, *args, **kwargs)
928 
929     def _filter_or_exclude(self, negate, *args, **kwargs):
930         if args or kwargs:
931             assert not self.query.is_sliced, \
932                 "Cannot filter a query once a slice has been taken."
933 
934         clone = self._chain()
935         if self._defer_next_filter:
936             self._defer_next_filter = False
937             clone._deferred_filter = negate, args, kwargs
938         else:
939             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
940         return clone
941 
942     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
943         if negate:
944             self._query.add_q(~Q(*args, **kwargs))
945         else:
946             self._query.add_q(Q(*args, **kwargs))
947 
948     def complex_filter(self, filter_obj):
949         """
950         Return a new QuerySet instance with filter_obj added to the filters.
951 
952         filter_obj can be a Q object or a dictionary of keyword lookup
953         arguments.
954 
955         This exists to support framework features such as 'limit_choices_to',
956         and usually it will be more natural to use other methods.
957         """
958         if isinstance(filter_obj, Q):
959             clone = self._chain()
960             clone.query.add_q(filter_obj)
961             return clone
962         else:
963             return self._filter_or_exclude(False, **filter_obj)
964 
965     def _combinator_query(self, combinator, *other_qs, all=False):
966         # Clone the query to inherit the select list and everything
967         clone = self._chain()
968         # Clear limits and ordering so they can be reapplied
969         clone.query.clear_ordering(True)
970         clone.query.clear_limits()
971         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
972         clone.query.combinator = combinator
973         clone.query.combinator_all = all
974         return clone
975 
976     def union(self, *other_qs, all=False):
977         # If the query is an EmptyQuerySet, combine all nonempty querysets.
978         if isinstance(self, EmptyQuerySet):
979             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
980             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
981         return self._combinator_query('union', *other_qs, all=all)
982 
983     def intersection(self, *other_qs):
984         # If any query is an EmptyQuerySet, return it.
985         if isinstance(self, EmptyQuerySet):
986             return self
987         for other in other_qs:
988             if isinstance(other, EmptyQuerySet):
989                 return other
990         return self._combinator_query('intersection', *other_qs)
991 
992     def difference(self, *other_qs):
993         # If the query is an EmptyQuerySet, return it.
994         if isinstance(self, EmptyQuerySet):
995             return self
996         return self._combinator_query('difference', *other_qs)
997 
998     def select_for_update(self, nowait=False, skip_locked=False, of=()):
999         """
1000         Return a new QuerySet instance that will select objects with a
1001         FOR UPDATE lock.
1002         """
1003         if nowait and skip_locked:
1004             raise ValueError('The nowait option cannot be used with skip_locked.')
1005         obj = self._chain()
1006         obj._for_write = True
1007         obj.query.select_for_update = True
1008         obj.query.select_for_update_nowait = nowait
1009         obj.query.select_for_update_skip_locked = skip_locked
1010         obj.query.select_for_update_of = of
1011         return obj
1012 
1013     def select_related(self, *fields):
1014         """
1015         Return a new QuerySet instance that will select related objects.
1016 
1017         If fields are specified, they must be ForeignKey fields and only those
1018         related objects are included in the selection.
1019 
1020         If select_related(None) is called, clear the list.
1021         """
1022         self._not_support_combined_queries('select_related')
1023         if self._fields is not None:
1024             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1025 
1026         obj = self._chain()
1027         if fields == (None,):
1028             obj.query.select_related = False
1029         elif fields:
1030             obj.query.add_select_related(fields)
1031         else:
1032             obj.query.select_related = True
1033         return obj
1034 
1035     def prefetch_related(self, *lookups):
1036         """
1037         Return a new QuerySet instance that will prefetch the specified
1038         Many-To-One and Many-To-Many related objects when the QuerySet is
1039         evaluated.
1040 
1041         When prefetch_related() is called more than once, append to the list of
1042         prefetch lookups. If prefetch_related(None) is called, clear the list.
1043         """
1044         self._not_support_combined_queries('prefetch_related')
1045         clone = self._chain()
1046         if lookups == (None,):
1047             clone._prefetch_related_lookups = ()
1048         else:
1049             for lookup in lookups:
1050                 if isinstance(lookup, Prefetch):
1051                     lookup = lookup.prefetch_to
1052                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1053                 if lookup in self.query._filtered_relations:
1054                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1055             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1056         return clone
1057 
1058     def annotate(self, *args, **kwargs):
1059         """
1060         Return a query set in which the returned objects have been annotated
1061         with extra data or aggregations.
1062         """
1063         self._not_support_combined_queries('annotate')
1064         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1065         annotations = {}
1066         for arg in args:
1067             # The default_alias property may raise a TypeError.
1068             try:
1069                 if arg.default_alias in kwargs:
1070                     raise ValueError("The named annotation '%s' conflicts with the "
1071                                      "default name for another annotation."
1072                                      % arg.default_alias)
1073             except TypeError:
1074                 raise TypeError("Complex annotations require an alias")
1075             annotations[arg.default_alias] = arg
1076         annotations.update(kwargs)
1077 
1078         clone = self._chain()
1079         names = self._fields
1080         if names is None:
1081             names = set(chain.from_iterable(
1082                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1083                 for field in self.model._meta.get_fields()
1084             ))
1085 
1086         for alias, annotation in annotations.items():
1087             if alias in names:
1088                 raise ValueError("The annotation '%s' conflicts with a field on "
1089                                  "the model." % alias)
1090             if isinstance(annotation, FilteredRelation):
1091                 clone.query.add_filtered_relation(annotation, alias)
1092             else:
1093                 clone.query.add_annotation(annotation, alias, is_summary=False)
1094 
1095         for alias, annotation in clone.query.annotations.items():
1096             if alias in annotations and annotation.contains_aggregate:
1097                 if clone._fields is None:
1098                     clone.query.group_by = True
1099                 else:
1100                     clone.query.set_group_by()
1101                 break
1102 
1103         return clone
1104 
1105     def order_by(self, *field_names):
1106         """Return a new QuerySet instance with the ordering changed."""
1107         assert not self.query.is_sliced, \
1108             "Cannot reorder a query once a slice has been taken."
1109         obj = self._chain()
1110         obj.query.clear_ordering(force_empty=False)
1111         obj.query.add_ordering(*field_names)
1112         return obj
1113 
1114     def distinct(self, *field_names):
1115         """
1116         Return a new QuerySet instance that will select only distinct results.
1117         """
1118         assert not self.query.is_sliced, \
1119             "Cannot create distinct fields once a slice has been taken."
1120         obj = self._chain()
1121         obj.query.add_distinct_fields(*field_names)
1122         return obj
1123 
1124     def extra(self, select=None, where=None, params=None, tables=None,
1125               order_by=None, select_params=None):
1126         """Add extra SQL fragments to the query."""
1127         self._not_support_combined_queries('extra')
1128         assert not self.query.is_sliced, \
1129             "Cannot change a query once a slice has been taken"
1130         clone = self._chain()
1131         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1132         return clone
1133 
1134     def reverse(self):
1135         """Reverse the ordering of the QuerySet."""
1136         if self.query.is_sliced:
1137             raise TypeError('Cannot reverse a query once a slice has been taken.')
1138         clone = self._chain()
1139         clone.query.standard_ordering = not clone.query.standard_ordering
1140         return clone
1141 
1142     def defer(self, *fields):
1143         """
1144         Defer the loading of data for certain fields until they are accessed.
1145         Add the set of deferred fields to any existing set of deferred fields.
1146         The only exception to this is if None is passed in as the only
1147         parameter, in which case removal all deferrals.
1148         """
1149         self._not_support_combined_queries('defer')
1150         if self._fields is not None:
1151             raise TypeError("Cannot call defer() after .values() or .values_list()")
1152         clone = self._chain()
1153         if fields == (None,):
1154             clone.query.clear_deferred_loading()
1155         else:
1156             clone.query.add_deferred_loading(fields)
1157         return clone
1158 
1159     def only(self, *fields):
1160         """
1161         Essentially, the opposite of defer(). Only the fields passed into this
1162         method and that are not already specified as deferred are loaded
1163         immediately when the queryset is evaluated.
1164         """
1165         self._not_support_combined_queries('only')
1166         if self._fields is not None:
1167             raise TypeError("Cannot call only() after .values() or .values_list()")
1168         if fields == (None,):
1169             # Can only pass None to defer(), not only(), as the rest option.
1170             # That won't stop people trying to do this, so let's be explicit.
1171             raise TypeError("Cannot pass None as an argument to only().")
1172         for field in fields:
1173             field = field.split(LOOKUP_SEP, 1)[0]
1174             if field in self.query._filtered_relations:
1175                 raise ValueError('only() is not supported with FilteredRelation.')
1176         clone = self._chain()
1177         clone.query.add_immediate_loading(fields)
1178         return clone
1179 
1180     def using(self, alias):
1181         """Select which database this QuerySet should execute against."""
1182         clone = self._chain()
1183         clone._db = alias
1184         return clone
1185 
1186     ###################################
1187     # PUBLIC INTROSPECTION ATTRIBUTES #
1188     ###################################
1189 
1190     @property
1191     def ordered(self):
1192         """
1193         Return True if the QuerySet is ordered -- i.e. has an order_by()
1194         clause or a default ordering on the model (or is empty).
1195         """
1196         if isinstance(self, EmptyQuerySet):
1197             return True
1198         if self.query.extra_order_by or self.query.order_by:
1199             return True
1200         elif self.query.default_ordering and self.query.get_meta().ordering:
1201             return True
1202         else:
1203             return False
1204 
1205     @property
1206     def db(self):
1207         """Return the database used if this query is executed now."""
1208         if self._for_write:
1209             return self._db or router.db_for_write(self.model, **self._hints)
1210         return self._db or router.db_for_read(self.model, **self._hints)
1211 
1212     ###################
1213     # PRIVATE METHODS #
1214     ###################
1215 
1216     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1217         """
1218         Insert a new record for the given model. This provides an interface to
1219         the InsertQuery class and is how Model.save() is implemented.
1220         """
1221         self._for_write = True
1222         if using is None:
1223             using = self.db
1224         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1225         query.insert_values(fields, objs, raw=raw)
1226         return query.get_compiler(using=using).execute_sql(returning_fields)
1227     _insert.alters_data = True
1228     _insert.queryset_only = False
1229 
1230     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1231         """
1232         Helper method for bulk_create() to insert objs one batch at a time.
1233         """
1234         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1235             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1236         ops = connections[self.db].ops
1237         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1238         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1239         inserted_rows = []
1240         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1241         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1242             if bulk_return and not ignore_conflicts:
1243                 inserted_columns = self._insert(
1244                     item, fields=fields, using=self.db,
1245                     returning_fields=self.model._meta.db_returning_fields,
1246                     ignore_conflicts=ignore_conflicts,
1247                 )
1248                 if isinstance(inserted_columns, list):
1249                     inserted_rows.extend(inserted_columns)
1250                 else:
1251                     inserted_rows.append(inserted_columns)
1252             else:
1253                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1254         return inserted_rows
1255 
1256     def _chain(self, **kwargs):
1257         """
1258         Return a copy of the current QuerySet that's ready for another
1259         operation.
1260         """
1261         obj = self._clone()
1262         if obj._sticky_filter:
1263             obj.query.filter_is_sticky = True
1264             obj._sticky_filter = False
1265         obj.__dict__.update(kwargs)
1266         return obj
1267 
1268     def _clone(self):
1269         """
1270         Return a copy of the current QuerySet. A lightweight alternative
1271         to deepcopy().
1272         """
1273         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1274         c._sticky_filter = self._sticky_filter
1275         c._for_write = self._for_write
1276         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1277         c._known_related_objects = self._known_related_objects
1278         c._iterable_class = self._iterable_class
1279         c._fields = self._fields
1280         return c
1281 
1282     def _fetch_all(self):
1283         if self._result_cache is None:
1284             self._result_cache = list(self._iterable_class(self))
1285         if self._prefetch_related_lookups and not self._prefetch_done:
1286             self._prefetch_related_objects()
1287 
1288     def _next_is_sticky(self):
1289         """
1290         Indicate that the next filter call and the one following that should
1291         be treated as a single filter. This is only important when it comes to
1292         determining when to reuse tables for many-to-many filters. Required so
1293         that we can filter naturally on the results of related managers.
1294 
1295         This doesn't return a clone of the current QuerySet (it returns
1296         "self"). The method is only used internally and should be immediately
1297         followed by a filter() that does create a clone.
1298         """
1299         self._sticky_filter = True
1300         return self
1301 
1302     def _merge_sanity_check(self, other):
1303         """Check that two QuerySet classes may be merged."""
1304         if self._fields is not None and (
1305                 set(self.query.values_select) != set(other.query.values_select) or
1306                 set(self.query.extra_select) != set(other.query.extra_select) or
1307                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1308             raise TypeError(
1309                 "Merging '%s' classes must involve the same values in each case."
1310                 % self.__class__.__name__
1311             )
1312 
1313     def _merge_known_related_objects(self, other):
1314         """
1315         Keep track of all known related objects from either QuerySet instance.
1316         """
1317         for field, objects in other._known_related_objects.items():
1318             self._known_related_objects.setdefault(field, {}).update(objects)
1319 
1320     def resolve_expression(self, *args, **kwargs):
1321         if self._fields and len(self._fields) > 1:
1322             # values() queryset can only be used as nested queries
1323             # if they are set up to select only a single field.
1324             raise TypeError('Cannot use multi-field values as a filter value.')
1325         query = self.query.resolve_expression(*args, **kwargs)
1326         query._db = self._db
1327         return query
1328     resolve_expression.queryset_only = True
1329 
1330     def _add_hints(self, **hints):
1331         """
1332         Update hinting information for use by routers. Add new key/values or
1333         overwrite existing key/values.
1334         """
1335         self._hints.update(hints)
1336 
1337     def _has_filters(self):
1338         """
1339         Check if this QuerySet has any filtering going on. This isn't
1340         equivalent with checking if all objects are present in results, for
1341         example, qs[1:]._has_filters() -> False.
1342         """
1343         return self.query.has_filters()
1344 
1345     @staticmethod
1346     def _validate_values_are_expressions(values, method_name):
1347         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1348         if invalid_args:
1349             raise TypeError(
1350                 'QuerySet.%s() received non-expression(s): %s.' % (
1351                     method_name,
1352                     ', '.join(invalid_args),
1353                 )
1354             )
1355 
1356     def _not_support_combined_queries(self, operation_name):
1357         if self.query.combinator:
1358             raise NotSupportedError(
1359                 'Calling QuerySet.%s() after %s() is not supported.'
1360                 % (operation_name, self.query.combinator)
1361             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-20 11:43:27.224 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-20 11:43:27.225 | INFO     | Search-only: skipping patch selection
2025-10-20 11:43:27.356 | INFO     | Task django__django-12185 completed successfully.
2025-10-20 11:43:27.357 | INFO     | No patch generated. You can try running ACR again.
