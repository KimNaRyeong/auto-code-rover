[
    {
        "rel_file_path": "sklearn/linear_model/ridge.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/linear_model/ridge.py",
        "start": 746,
        "end": 764,
        "class_name": "Ridge",
        "method_name": "fit",
        "code": "746     def fit(self, X, y, sample_weight=None):\n747         \"\"\"Fit Ridge regression model\n748 \n749         Parameters\n750         ----------\n751         X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n752             Training data\n753 \n754         y : array-like, shape = [n_samples] or [n_samples, n_targets]\n755             Target values\n756 \n757         sample_weight : float or numpy array of shape [n_samples]\n758             Individual weights for each sample\n759 \n760         Returns\n761         -------\n762         self : returns an instance of self.\n763         \"\"\"\n764         return super().fit(X, y, sample_weight=sample_weight)\n",
        "intended_behavior": "In the `fit` method of the `Ridge` class, the data passed to the `fit` method (both `X` and `y`) should be checked for C-contiguity and possibly converted if necessary before being further processed or passed downstream where `ArrayData` or similar mechanisms expect C-contiguous arrays. This can be achieved by integrating a call to `check_array` with the appropriate arguments to enforce C-contiguity. This ensures that the existing functionality of fitting a Ridge regression model is preserved while preventing the `ValueError` caused by non-C-contiguous arrays being passed to functions or methods that require C-contiguity. The `check_array` function from `sklearn.utils.validation` should be leveraged for this purpose, as it already contains mechanisms for enforcing array checks and transformations based on specified criteria."
    },
    {
        "rel_file_path": "sklearn/linear_model/ridge.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/linear_model/ridge.py",
        "start": 605,
        "end": 764,
        "class_name": "Ridge",
        "method_name": null,
        "code": "605 class Ridge(_BaseRidge, RegressorMixin):\n606     \"\"\"Linear least squares with l2 regularization.\n607 \n608     Minimizes the objective function::\n609 \n610     ||y - Xw||^2_2 + alpha * ||w||^2_2\n611 \n612     This model solves a regression model where the loss function is\n613     the linear least squares function and regularization is given by\n614     the l2-norm. Also known as Ridge Regression or Tikhonov regularization.\n615     This estimator has built-in support for multi-variate regression\n616     (i.e., when y is a 2d-array of shape [n_samples, n_targets]).\n617 \n618     Read more in the :ref:`User Guide <ridge_regression>`.\n619 \n620     Parameters\n621     ----------\n622     alpha : {float, array-like}, shape (n_targets)\n623         Regularization strength; must be a positive float. Regularization\n624         improves the conditioning of the problem and reduces the variance of\n625         the estimates. Larger values specify stronger regularization.\n626         Alpha corresponds to ``C^-1`` in other linear models such as\n627         LogisticRegression or LinearSVC. If an array is passed, penalties are\n628         assumed to be specific to the targets. Hence they must correspond in\n629         number.\n630 \n631     fit_intercept : bool, default True\n632         Whether to calculate the intercept for this model. If set\n633         to false, no intercept will be used in calculations\n634         (e.g. data is expected to be already centered).\n635 \n636     normalize : boolean, optional, default False\n637         This parameter is ignored when ``fit_intercept`` is set to False.\n638         If True, the regressors X will be normalized before regression by\n639         subtracting the mean and dividing by the l2-norm.\n640         If you wish to standardize, please use\n641         :class:`sklearn.preprocessing.StandardScaler` before calling ``fit``\n642         on an estimator with ``normalize=False``.\n643 \n644     copy_X : boolean, optional, default True\n645         If True, X will be copied; else, it may be overwritten.\n646 \n647     max_iter : int, optional\n648         Maximum number of iterations for conjugate gradient solver.\n649         For 'sparse_cg' and 'lsqr' solvers, the default value is determined\n650         by scipy.sparse.linalg. For 'sag' solver, the default value is 1000.\n651 \n652     tol : float\n653         Precision of the solution.\n654 \n655     solver : {'auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga'}\n656         Solver to use in the computational routines:\n657 \n658         - 'auto' chooses the solver automatically based on the type of data.\n659 \n660         - 'svd' uses a Singular Value Decomposition of X to compute the Ridge\n661           coefficients. More stable for singular matrices than\n662           'cholesky'.\n663 \n664         - 'cholesky' uses the standard scipy.linalg.solve function to\n665           obtain a closed-form solution.\n666 \n667         - 'sparse_cg' uses the conjugate gradient solver as found in\n668           scipy.sparse.linalg.cg. As an iterative algorithm, this solver is\n669           more appropriate than 'cholesky' for large-scale data\n670           (possibility to set `tol` and `max_iter`).\n671 \n672         - 'lsqr' uses the dedicated regularized least-squares routine\n673           scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative\n674           procedure.\n675 \n676         - 'sag' uses a Stochastic Average Gradient descent, and 'saga' uses\n677           its improved, unbiased version named SAGA. Both methods also use an\n678           iterative procedure, and are often faster than other solvers when\n679           both n_samples and n_features are large. Note that 'sag' and\n680           'saga' fast convergence is only guaranteed on features with\n681           approximately the same scale. You can preprocess the data with a\n682           scaler from sklearn.preprocessing.\n683 \n684         All last five solvers support both dense and sparse data. However, only\n685         'sparse_cg' supports sparse input when `fit_intercept` is True.\n686 \n687         .. versionadded:: 0.17\n688            Stochastic Average Gradient descent solver.\n689         .. versionadded:: 0.19\n690            SAGA solver.\n691 \n692     random_state : int, RandomState instance or None, optional, default None\n693         The seed of the pseudo random number generator to use when shuffling\n694         the data.  If int, random_state is the seed used by the random number\n695         generator; If RandomState instance, random_state is the random number\n696         generator; If None, the random number generator is the RandomState\n697         instance used by `np.random`. Used when ``solver`` == 'sag'.\n698 \n699         .. versionadded:: 0.17\n700            *random_state* to support Stochastic Average Gradient.\n701 \n702     Attributes\n703     ----------\n704     coef_ : array, shape (n_features,) or (n_targets, n_features)\n705         Weight vector(s).\n706 \n707     intercept_ : float | array, shape = (n_targets,)\n708         Independent term in decision function. Set to 0.0 if\n709         ``fit_intercept = False``.\n710 \n711     n_iter_ : array or None, shape (n_targets,)\n712         Actual number of iterations for each target. Available only for\n713         sag and lsqr solvers. Other solvers will return None.\n714 \n715         .. versionadded:: 0.17\n716 \n717     See also\n718     --------\n719     RidgeClassifier : Ridge classifier\n720     RidgeCV : Ridge regression with built-in cross validation\n721     :class:`sklearn.kernel_ridge.KernelRidge` : Kernel ridge regression\n722         combines ridge regression with the kernel trick\n723 \n724     Examples\n725     --------\n726     >>> from sklearn.linear_model import Ridge\n727     >>> import numpy as np\n728     >>> n_samples, n_features = 10, 5\n729     >>> rng = np.random.RandomState(0)\n730     >>> y = rng.randn(n_samples)\n731     >>> X = rng.randn(n_samples, n_features)\n732     >>> clf = Ridge(alpha=1.0)\n733     >>> clf.fit(X, y)\n734     Ridge()\n735 \n736     \"\"\"\n737     def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n738                  copy_X=True, max_iter=None, tol=1e-3, solver=\"auto\",\n739                  random_state=None):\n740         super().__init__(\n741             alpha=alpha, fit_intercept=fit_intercept,\n742             normalize=normalize, copy_X=copy_X,\n743             max_iter=max_iter, tol=tol, solver=solver,\n744             random_state=random_state)\n745 \n746     def fit(self, X, y, sample_weight=None):\n747         \"\"\"Fit Ridge regression model\n748 \n749         Parameters\n750         ----------\n751         X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n752             Training data\n753 \n754         y : array-like, shape = [n_samples] or [n_samples, n_targets]\n755             Target values\n756 \n757         sample_weight : float or numpy array of shape [n_samples]\n758             Individual weights for each sample\n759 \n760         Returns\n761         -------\n762         self : returns an instance of self.\n763         \"\"\"\n764         return super().fit(X, y, sample_weight=sample_weight)\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]