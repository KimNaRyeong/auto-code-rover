2025-10-15 08:51:06.443 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 08:51:08.117 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=969, output_tokens=13, cost=0.010080
2025-10-15 08:51:28.746 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1149, output_tokens=653, cost=0.031080
2025-10-15 08:51:30.574 | INFO     | registered non reproducing test 0
2025-10-15 08:51:51.651 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2539, output_tokens=789, cost=0.049060
2025-10-15 08:51:53.452 | INFO     | registered non reproducing test 1
2025-10-15 08:52:10.003 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2675, output_tokens=570, cost=0.043850
2025-10-15 08:52:11.177 | INFO     | registered non reproducing test 2
2025-10-15 08:52:11.177 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 08:52:11.183 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 08:52:15.730 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1253, output_tokens=170, cost=0.017630
2025-10-15 08:52:15.735 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 08:52:19.714 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=524, output_tokens=48, cost=0.006680
2025-10-15 08:52:19.716 | DEBUG    | Extracted a valid json.
2025-10-15 08:52:19.716 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"dot\")",
        "search_code(\"apply_ufunc\")",
        "search_code(\"dask='parallelized'\")"
    ],
    "bug_locations": []
}
2025-10-15 08:52:19.820 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 08:52:19.821 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 08:52:44.293 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4462, output_tokens=757, cost=0.067330
2025-10-15 08:52:44.308 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 08:52:55.288 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5435, output_tokens=350, cost=0.064850
2025-10-15 08:52:55.296 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 08:53:04.199 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=704, output_tokens=344, cost=0.017360
2025-10-15 08:53:04.200 | DEBUG    | Extracted a valid json.
2025-10-15 08:53:04.200 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/computation.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file should include a new `cross` function that mimics the structure and approach used by the existing `dot` function but instead employs `numpy.cross` to compute the cross product of two `DataArray` objects. The function should handle input validation (e.g., ensuring that the spatial dimension has a size of 3 for 3D vectors or 2 for 2D vectors), work with dask arrays (using `dask='parallelized'` when applicable), and respect xarray's dimension handling conventions. This new function will extend xarray's computational capabilities to include vector cross products, filling a gap for users working on physical applications requiring such operations."
        },
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "cross",
            "intended_behavior": "Add a new method `cross` to the `DataArray` class. This method would provide a high-level interface to the `cross` function defined in `computation.py`, allowing users to call the cross product operation directly on `DataArray` instances, similar to how they currently use the `dot` method. This method should validate the input and dimensions, similarly ensuring that only suitable `DataArray` objects are passed (e.g., checking that the spatial dimension is correctly sized and exists within the arrays). This extension will make the feature easily accessible to users, integrating seamlessly with the existing API."
        }
    ]
}
2025-10-15 08:53:04.224 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/computation.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Functions for applying functions that act on arrays to xarray's labeled data.
3 """
4 from __future__ import annotations
5 
6 import functools
7 import itertools
8 import operator
9 import warnings
10 from collections import Counter
11 from typing import (
12     TYPE_CHECKING,
13     AbstractSet,
14     Any,
15     Callable,
16     Dict,
17     Hashable,
18     Iterable,
19     List,
20     Mapping,
21     Optional,
22     Sequence,
23     Tuple,
24     Union,
25 )
26 
27 import numpy as np
28 
29 from . import dtypes, duck_array_ops, utils
30 from .alignment import align, deep_align
31 from .merge import merge_attrs, merge_coordinates_without_align
32 from .options import OPTIONS, _get_keep_attrs
33 from .pycompat import is_duck_dask_array
34 from .utils import is_dict_like
35 from .variable import Variable
36 
37 if TYPE_CHECKING:
38     from .coordinates import Coordinates
39     from .dataset import Dataset
40     from .types import T_Xarray
41 
42 _NO_FILL_VALUE = utils.ReprObject("<no-fill-value>")
43 _DEFAULT_NAME = utils.ReprObject("<default-name>")
44 _JOINS_WITHOUT_FILL_VALUES = frozenset({"inner", "exact"})
45 
46 
47 def _first_of_type(args, kind):
48     """Return either first object of type 'kind' or raise if not found."""
49     for arg in args:
50         if isinstance(arg, kind):
51             return arg
52     raise ValueError("This should be unreachable.")
53 
54 
55 def _all_of_type(args, kind):
56     """Return all objects of type 'kind'"""
57     return [arg for arg in args if isinstance(arg, kind)]
58 
59 
60 class _UFuncSignature:
61     """Core dimensions signature for a given function.
62 
63     Based on the signature provided by generalized ufuncs in NumPy.
64 
65     Attributes
66     ----------
67     input_core_dims : tuple[tuple]
68         Core dimension names on each input variable.
69     output_core_dims : tuple[tuple]
70         Core dimension names on each output variable.
71     """
72 
73     __slots__ = (
74         "input_core_dims",
75         "output_core_dims",
76         "_all_input_core_dims",
77         "_all_output_core_dims",
78         "_all_core_dims",
79     )
80 
81     def __init__(self, input_core_dims, output_core_dims=((),)):
82         self.input_core_dims = tuple(tuple(a) for a in input_core_dims)
83         self.output_core_dims = tuple(tuple(a) for a in output_core_dims)
84         self._all_input_core_dims = None
85         self._all_output_core_dims = None
86         self._all_core_dims = None
87 
88     @property
89     def all_input_core_dims(self):
90         if self._all_input_core_dims is None:
91             self._all_input_core_dims = frozenset(
92                 dim for dims in self.input_core_dims for dim in dims
93             )
94         return self._all_input_core_dims
95 
96     @property
97     def all_output_core_dims(self):
98         if self._all_output_core_dims is None:
99             self._all_output_core_dims = frozenset(
100                 dim for dims in self.output_core_dims for dim in dims
101             )
102         return self._all_output_core_dims
103 
104     @property
105     def all_core_dims(self):
106         if self._all_core_dims is None:
107             self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims
108         return self._all_core_dims
109 
110     @property
111     def dims_map(self):
112         return {
113             core_dim: f"dim{n}" for n, core_dim in enumerate(sorted(self.all_core_dims))
114         }
115 
116     @property
117     def num_inputs(self):
118         return len(self.input_core_dims)
119 
120     @property
121     def num_outputs(self):
122         return len(self.output_core_dims)
123 
124     def __eq__(self, other):
125         try:
126             return (
127                 self.input_core_dims == other.input_core_dims
128                 and self.output_core_dims == other.output_core_dims
129             )
130         except AttributeError:
131             return False
132 
133     def __ne__(self, other):
134         return not self == other
135 
136     def __repr__(self):
137         return "{}({!r}, {!r})".format(
138             type(self).__name__,
139             list(self.input_core_dims),
140             list(self.output_core_dims),
141         )
142 
143     def __str__(self):
144         lhs = ",".join("({})".format(",".join(dims)) for dims in self.input_core_dims)
145         rhs = ",".join("({})".format(",".join(dims)) for dims in self.output_core_dims)
146         return f"{lhs}->{rhs}"
147 
148     def to_gufunc_string(self, exclude_dims=frozenset()):
149         """Create an equivalent signature string for a NumPy gufunc.
150 
151         Unlike __str__, handles dimensions that don't map to Python
152         identifiers.
153 
154         Also creates unique names for input_core_dims contained in exclude_dims.
155         """
156         input_core_dims = [
157             [self.dims_map[dim] for dim in core_dims]
158             for core_dims in self.input_core_dims
159         ]
160         output_core_dims = [
161             [self.dims_map[dim] for dim in core_dims]
162             for core_dims in self.output_core_dims
163         ]
164 
165         # enumerate input_core_dims contained in exclude_dims to make them unique
166         if exclude_dims:
167 
168             exclude_dims = [self.dims_map[dim] for dim in exclude_dims]
169 
170             counter = Counter()
171 
172             def _enumerate(dim):
173                 if dim in exclude_dims:
174                     n = counter[dim]
175                     counter.update([dim])
176                     dim = f"{dim}_{n}"
177                 return dim
178 
179             input_core_dims = [
180                 [_enumerate(dim) for dim in arg] for arg in input_core_dims
181             ]
182 
183         alt_signature = type(self)(input_core_dims, output_core_dims)
184         return str(alt_signature)
185 
186 
187 def result_name(objects: list) -> Any:
188     # use the same naming heuristics as pandas:
189     # https://github.com/blaze/blaze/issues/458#issuecomment-51936356
190     names = {getattr(obj, "name", _DEFAULT_NAME) for obj in objects}
191     names.discard(_DEFAULT_NAME)
192     if len(names) == 1:
193         (name,) = names
194     else:
195         name = None
196     return name
197 
198 
199 def _get_coords_list(args) -> List[Coordinates]:
200     coords_list = []
201     for arg in args:
202         try:
203             coords = arg.coords
204         except AttributeError:
205             pass  # skip this argument
206         else:
207             coords_list.append(coords)
208     return coords_list
209 
210 
211 def build_output_coords(
212     args: list,
213     signature: _UFuncSignature,
214     exclude_dims: AbstractSet = frozenset(),
215     combine_attrs: str = "override",
216 ) -> "List[Dict[Any, Variable]]":
217     """Build output coordinates for an operation.
218 
219     Parameters
220     ----------
221     args : list
222         List of raw operation arguments. Any valid types for xarray operations
223         are OK, e.g., scalars, Variable, DataArray, Dataset.
224     signature : _UfuncSignature
225         Core dimensions signature for the operation.
226     exclude_dims : set, optional
227         Dimensions excluded from the operation. Coordinates along these
228         dimensions are dropped.
229 
230     Returns
231     -------
232     Dictionary of Variable objects with merged coordinates.
233     """
234     coords_list = _get_coords_list(args)
235 
236     if len(coords_list) == 1 and not exclude_dims:
237         # we can skip the expensive merge
238         (unpacked_coords,) = coords_list
239         merged_vars = dict(unpacked_coords.variables)
240     else:
241         # TODO: save these merged indexes, instead of re-computing them later
242         merged_vars, unused_indexes = merge_coordinates_without_align(
243             coords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs
244         )
245 
246     output_coords = []
247     for output_dims in signature.output_core_dims:
248         dropped_dims = signature.all_input_core_dims - set(output_dims)
249         if dropped_dims:
250             filtered = {
251                 k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)
252             }
253         else:
254             filtered = merged_vars
255         output_coords.append(filtered)
256 
257     return output_coords
258 
259 
260 def apply_dataarray_vfunc(
261     func,
262     *args,
263     signature,
264     join="inner",
265     exclude_dims=frozenset(),
266     keep_attrs="override",
267 ):
268     """Apply a variable level function over DataArray, Variable and/or ndarray
269     objects.
270     """
271     from .dataarray import DataArray
272 
273     if len(args) > 1:
274         args = deep_align(
275             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
276         )
277 
278     objs = _all_of_type(args, DataArray)
279 
280     if keep_attrs == "drop":
281         name = result_name(args)
282     else:
283         first_obj = _first_of_type(args, DataArray)
284         name = first_obj.name
285     result_coords = build_output_coords(
286         args, signature, exclude_dims, combine_attrs=keep_attrs
287     )
288 
289     data_vars = [getattr(a, "variable", a) for a in args]
290     result_var = func(*data_vars)
291 
292     if signature.num_outputs > 1:
293         out = tuple(
294             DataArray(variable, coords, name=name, fastpath=True)
295             for variable, coords in zip(result_var, result_coords)
296         )
297     else:
298         (coords,) = result_coords
299         out = DataArray(result_var, coords, name=name, fastpath=True)
300 
301     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
302     if isinstance(out, tuple):
303         for da in out:
304             da.attrs = attrs
305     else:
306         out.attrs = attrs
307 
308     return out
309 
310 
311 def ordered_set_union(all_keys: List[Iterable]) -> Iterable:
312     return {key: None for keys in all_keys for key in keys}.keys()
313 
314 
315 def ordered_set_intersection(all_keys: List[Iterable]) -> Iterable:
316     intersection = set(all_keys[0])
317     for keys in all_keys[1:]:
318         intersection.intersection_update(keys)
319     return [key for key in all_keys[0] if key in intersection]
320 
321 
322 def assert_and_return_exact_match(all_keys):
323     first_keys = all_keys[0]
324     for keys in all_keys[1:]:
325         if keys != first_keys:
326             raise ValueError(
327                 "exact match required for all data variable names, "
328                 f"but {keys!r} != {first_keys!r}"
329             )
330     return first_keys
331 
332 
333 _JOINERS: Dict[str, Callable] = {
334     "inner": ordered_set_intersection,
335     "outer": ordered_set_union,
336     "left": operator.itemgetter(0),
337     "right": operator.itemgetter(-1),
338     "exact": assert_and_return_exact_match,
339 }
340 
341 
342 def join_dict_keys(
343     objects: Iterable[Union[Mapping, Any]], how: str = "inner"
344 ) -> Iterable:
345     joiner = _JOINERS[how]
346     all_keys = [obj.keys() for obj in objects if hasattr(obj, "keys")]
347     return joiner(all_keys)
348 
349 
350 def collect_dict_values(
351     objects: Iterable[Union[Mapping, Any]], keys: Iterable, fill_value: object = None
352 ) -> List[list]:
353     return [
354         [obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects]
355         for key in keys
356     ]
357 
358 
359 def _as_variables_or_variable(arg):
360     try:
361         return arg.variables
362     except AttributeError:
363         try:
364             return arg.variable
365         except AttributeError:
366             return arg
367 
368 
369 def _unpack_dict_tuples(
370     result_vars: Mapping[Any, Tuple[Variable, ...]], num_outputs: int
371 ) -> Tuple[Dict[Hashable, Variable], ...]:
372     out: Tuple[Dict[Hashable, Variable], ...] = tuple({} for _ in range(num_outputs))
373     for name, values in result_vars.items():
374         for value, results_dict in zip(values, out):
375             results_dict[name] = value
376     return out
377 
378 
379 def apply_dict_of_variables_vfunc(
380     func, *args, signature, join="inner", fill_value=None
381 ):
382     """Apply a variable level function over dicts of DataArray, DataArray,
383     Variable and ndarray objects.
384     """
385     args = [_as_variables_or_variable(arg) for arg in args]
386     names = join_dict_keys(args, how=join)
387     grouped_by_name = collect_dict_values(args, names, fill_value)
388 
389     result_vars = {}
390     for name, variable_args in zip(names, grouped_by_name):
391         result_vars[name] = func(*variable_args)
392 
393     if signature.num_outputs > 1:
394         return _unpack_dict_tuples(result_vars, signature.num_outputs)
395     else:
396         return result_vars
397 
398 
399 def _fast_dataset(
400     variables: Dict[Hashable, Variable], coord_variables: Mapping[Hashable, Variable]
401 ) -> Dataset:
402     """Create a dataset as quickly as possible.
403 
404     Beware: the `variables` dict is modified INPLACE.
405     """
406     from .dataset import Dataset
407 
408     variables.update(coord_variables)
409     coord_names = set(coord_variables)
410     return Dataset._construct_direct(variables, coord_names)
411 
412 
413 def apply_dataset_vfunc(
414     func,
415     *args,
416     signature,
417     join="inner",
418     dataset_join="exact",
419     fill_value=_NO_FILL_VALUE,
420     exclude_dims=frozenset(),
421     keep_attrs="override",
422 ):
423     """Apply a variable level function over Dataset, dict of DataArray,
424     DataArray, Variable and/or ndarray objects.
425     """
426     from .dataset import Dataset
427 
428     if dataset_join not in _JOINS_WITHOUT_FILL_VALUES and fill_value is _NO_FILL_VALUE:
429         raise TypeError(
430             "to apply an operation to datasets with different "
431             "data variables with apply_ufunc, you must supply the "
432             "dataset_fill_value argument."
433         )
434 
435     objs = _all_of_type(args, Dataset)
436 
437     if len(args) > 1:
438         args = deep_align(
439             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
440         )
441 
442     list_of_coords = build_output_coords(
443         args, signature, exclude_dims, combine_attrs=keep_attrs
444     )
445     args = [getattr(arg, "data_vars", arg) for arg in args]
446 
447     result_vars = apply_dict_of_variables_vfunc(
448         func, *args, signature=signature, join=dataset_join, fill_value=fill_value
449     )
450 
451     if signature.num_outputs > 1:
452         out = tuple(_fast_dataset(*args) for args in zip(result_vars, list_of_coords))
453     else:
454         (coord_vars,) = list_of_coords
455         out = _fast_dataset(result_vars, coord_vars)
456 
457     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
458     if isinstance(out, tuple):
459         for ds in out:
460             ds.attrs = attrs
461     else:
462         out.attrs = attrs
463 
464     return out
465 
466 
467 def _iter_over_selections(obj, dim, values):
468     """Iterate over selections of an xarray object in the provided order."""
469     from .groupby import _dummy_copy
470 
471     dummy = None
472     for value in values:
473         try:
474             obj_sel = obj.sel(**{dim: value})
475         except (KeyError, IndexError):
476             if dummy is None:
477                 dummy = _dummy_copy(obj)
478             obj_sel = dummy
479         yield obj_sel
480 
481 
482 def apply_groupby_func(func, *args):
483     """Apply a dataset or datarray level function over GroupBy, Dataset,
484     DataArray, Variable and/or ndarray objects.
485     """
486     from .groupby import GroupBy, peek_at
487     from .variable import Variable
488 
489     groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
490     assert groupbys, "must have at least one groupby to iterate over"
491     first_groupby = groupbys[0]
492     if any(not first_groupby._group.equals(gb._group) for gb in groupbys[1:]):
493         raise ValueError(
494             "apply_ufunc can only perform operations over "
495             "multiple GroupBy objects at once if they are all "
496             "grouped the same way"
497         )
498 
499     grouped_dim = first_groupby._group.name
500     unique_values = first_groupby._unique_coord.values
501 
502     iterators = []
503     for arg in args:
504         if isinstance(arg, GroupBy):
505             iterator = (value for _, value in arg)
506         elif hasattr(arg, "dims") and grouped_dim in arg.dims:
507             if isinstance(arg, Variable):
508                 raise ValueError(
509                     "groupby operations cannot be performed with "
510                     "xarray.Variable objects that share a dimension with "
511                     "the grouped dimension"
512                 )
513             iterator = _iter_over_selections(arg, grouped_dim, unique_values)
514         else:
515             iterator = itertools.repeat(arg)
516         iterators.append(iterator)
517 
518     applied = (func(*zipped_args) for zipped_args in zip(*iterators))
519     applied_example, applied = peek_at(applied)
520     combine = first_groupby._combine
521     if isinstance(applied_example, tuple):
522         combined = tuple(combine(output) for output in zip(*applied))
523     else:
524         combined = combine(applied)
525     return combined
526 
527 
528 def unified_dim_sizes(
529     variables: Iterable[Variable], exclude_dims: AbstractSet = frozenset()
530 ) -> Dict[Hashable, int]:
531 
532     dim_sizes: Dict[Hashable, int] = {}
533 
534     for var in variables:
535         if len(set(var.dims)) < len(var.dims):
536             raise ValueError(
537                 "broadcasting cannot handle duplicate "
538                 f"dimensions on a variable: {list(var.dims)}"
539             )
540         for dim, size in zip(var.dims, var.shape):
541             if dim not in exclude_dims:
542                 if dim not in dim_sizes:
543                     dim_sizes[dim] = size
544                 elif dim_sizes[dim] != size:
545                     raise ValueError(
546                         "operands cannot be broadcast together "
547                         "with mismatched lengths for dimension "
548                         f"{dim}: {dim_sizes[dim]} vs {size}"
549                     )
550     return dim_sizes
551 
552 
553 SLICE_NONE = slice(None)
554 
555 
556 def broadcast_compat_data(
557     variable: Variable,
558     broadcast_dims: Tuple[Hashable, ...],
559     core_dims: Tuple[Hashable, ...],
560 ) -> Any:
561     data = variable.data
562 
563     old_dims = variable.dims
564     new_dims = broadcast_dims + core_dims
565 
566     if new_dims == old_dims:
567         # optimize for the typical case
568         return data
569 
570     set_old_dims = set(old_dims)
571     missing_core_dims = [d for d in core_dims if d not in set_old_dims]
572     if missing_core_dims:
573         raise ValueError(
574             "operand to apply_ufunc has required core dimensions {}, but "
575             "some of these dimensions are absent on an input variable: {}".format(
576                 list(core_dims), missing_core_dims
577             )
578         )
579 
580     set_new_dims = set(new_dims)
581     unexpected_dims = [d for d in old_dims if d not in set_new_dims]
582     if unexpected_dims:
583         raise ValueError(
584             "operand to apply_ufunc encountered unexpected "
585             f"dimensions {unexpected_dims!r} on an input variable: these are core "
586             "dimensions on other input or output variables"
587         )
588 
589     # for consistency with numpy, keep broadcast dimensions to the left
590     old_broadcast_dims = tuple(d for d in broadcast_dims if d in set_old_dims)
591     reordered_dims = old_broadcast_dims + core_dims
592     if reordered_dims != old_dims:
593         order = tuple(old_dims.index(d) for d in reordered_dims)
594         data = duck_array_ops.transpose(data, order)
595 
596     if new_dims != reordered_dims:
597         key_parts: List[Optional[slice]] = []
598         for dim in new_dims:
599             if dim in set_old_dims:
600                 key_parts.append(SLICE_NONE)
601             elif key_parts:
602                 # no need to insert new axes at the beginning that are already
603                 # handled by broadcasting
604                 key_parts.append(np.newaxis)
605         data = data[tuple(key_parts)]
606 
607     return data
608 
609 
610 def _vectorize(func, signature, output_dtypes, exclude_dims):
611     if signature.all_core_dims:
612         func = np.vectorize(
613             func,
614             otypes=output_dtypes,
615             signature=signature.to_gufunc_string(exclude_dims),
616         )
617     else:
618         func = np.vectorize(func, otypes=output_dtypes)
619 
620     return func
621 
622 
623 def apply_variable_ufunc(
624     func,
625     *args,
626     signature,
627     exclude_dims=frozenset(),
628     dask="forbidden",
629     output_dtypes=None,
630     vectorize=False,
631     keep_attrs="override",
632     dask_gufunc_kwargs=None,
633 ):
634     """Apply a ndarray level function over Variable and/or ndarray objects."""
635     from .variable import Variable, as_compatible_data
636 
637     dim_sizes = unified_dim_sizes(
638         (a for a in args if hasattr(a, "dims")), exclude_dims=exclude_dims
639     )
640     broadcast_dims = tuple(
641         dim for dim in dim_sizes if dim not in signature.all_core_dims
642     )
643     output_dims = [broadcast_dims + out for out in signature.output_core_dims]
644 
645     input_data = [
646         broadcast_compat_data(arg, broadcast_dims, core_dims)
647         if isinstance(arg, Variable)
648         else arg
649         for arg, core_dims in zip(args, signature.input_core_dims)
650     ]
651 
652     if any(is_duck_dask_array(array) for array in input_data):
653         if dask == "forbidden":
654             raise ValueError(
655                 "apply_ufunc encountered a dask array on an "
656                 "argument, but handling for dask arrays has not "
657                 "been enabled. Either set the ``dask`` argument "
658                 "or load your data into memory first with "
659                 "``.load()`` or ``.compute()``"
660             )
661         elif dask == "parallelized":
662             numpy_func = func
663 
664             if dask_gufunc_kwargs is None:
665                 dask_gufunc_kwargs = {}
666             else:
667                 dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
668 
669             allow_rechunk = dask_gufunc_kwargs.get("allow_rechunk", None)
670             if allow_rechunk is None:
671                 for n, (data, core_dims) in enumerate(
672                     zip(input_data, signature.input_core_dims)
673                 ):
674                     if is_duck_dask_array(data):
675                         # core dimensions cannot span multiple chunks
676                         for axis, dim in enumerate(core_dims, start=-len(core_dims)):
677                             if len(data.chunks[axis]) != 1:
678                                 raise ValueError(
679                                     f"dimension {dim} on {n}th function argument to "
680                                     "apply_ufunc with dask='parallelized' consists of "
681                                     "multiple chunks, but is also a core dimension. To "
682                                     "fix, either rechunk into a single dask array chunk along "
683                                     f"this dimension, i.e., ``.chunk(dict({dim}=-1))``, or "
684                                     "pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` "
685                                     "but beware that this may significantly increase memory usage."
686                                 )
687                 dask_gufunc_kwargs["allow_rechunk"] = True
688 
689             output_sizes = dask_gufunc_kwargs.pop("output_sizes", {})
690             if output_sizes:
691                 output_sizes_renamed = {}
692                 for key, value in output_sizes.items():
693                     if key not in signature.all_output_core_dims:
694                         raise ValueError(
695                             f"dimension '{key}' in 'output_sizes' must correspond to output_core_dims"
696                         )
697                     output_sizes_renamed[signature.dims_map[key]] = value
698                 dask_gufunc_kwargs["output_sizes"] = output_sizes_renamed
699 
700             for key in signature.all_output_core_dims:
701                 if key not in signature.all_input_core_dims and key not in output_sizes:
702                     raise ValueError(
703                         f"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'"
704                     )
705 
706             def func(*arrays):
707                 import dask.array as da
708 
709                 res = da.apply_gufunc(
710                     numpy_func,
711                     signature.to_gufunc_string(exclude_dims),
712                     *arrays,
713                     vectorize=vectorize,
714                     output_dtypes=output_dtypes,
715                     **dask_gufunc_kwargs,
716                 )
717 
718                 return res
719 
720         elif dask == "allowed":
721             pass
722         else:
723             raise ValueError(
724                 "unknown setting for dask array handling in "
725                 "apply_ufunc: {}".format(dask)
726             )
727     else:
728         if vectorize:
729             func = _vectorize(
730                 func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims
731             )
732 
733     result_data = func(*input_data)
734 
735     if signature.num_outputs == 1:
736         result_data = (result_data,)
737     elif (
738         not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs
739     ):
740         raise ValueError(
741             "applied function does not have the number of "
742             "outputs specified in the ufunc signature. "
743             "Result is not a tuple of {} elements: {!r}".format(
744                 signature.num_outputs, result_data
745             )
746         )
747 
748     objs = _all_of_type(args, Variable)
749     attrs = merge_attrs(
750         [obj.attrs for obj in objs],
751         combine_attrs=keep_attrs,
752     )
753 
754     output = []
755     for dims, data in zip(output_dims, result_data):
756         data = as_compatible_data(data)
757         if data.ndim != len(dims):
758             raise ValueError(
759                 "applied function returned data with unexpected "
760                 f"number of dimensions. Received {data.ndim} dimension(s) but "
761                 f"expected {len(dims)} dimensions with names: {dims!r}"
762             )
763 
764         var = Variable(dims, data, fastpath=True)
765         for dim, new_size in var.sizes.items():
766             if dim in dim_sizes and new_size != dim_sizes[dim]:
767                 raise ValueError(
768                     "size of dimension {!r} on inputs was unexpectedly "
769                     "changed by applied function from {} to {}. Only "
770                     "dimensions specified in ``exclude_dims`` with "
771                     "xarray.apply_ufunc are allowed to change size.".format(
772                         dim, dim_sizes[dim], new_size
773                     )
774                 )
775 
776         var.attrs = attrs
777         output.append(var)
778 
779     if signature.num_outputs == 1:
780         return output[0]
781     else:
782         return tuple(output)
783 
784 
785 def apply_array_ufunc(func, *args, dask="forbidden"):
786     """Apply a ndarray level function over ndarray objects."""
787     if any(is_duck_dask_array(arg) for arg in args):
788         if dask == "forbidden":
789             raise ValueError(
790                 "apply_ufunc encountered a dask array on an "
791                 "argument, but handling for dask arrays has not "
792                 "been enabled. Either set the ``dask`` argument "
793                 "or load your data into memory first with "
794                 "``.load()`` or ``.compute()``"
795             )
796         elif dask == "parallelized":
797             raise ValueError(
798                 "cannot use dask='parallelized' for apply_ufunc "
799                 "unless at least one input is an xarray object"
800             )
801         elif dask == "allowed":
802             pass
803         else:
804             raise ValueError(f"unknown setting for dask array handling: {dask}")
805     return func(*args)
806 
807 
808 def apply_ufunc(
809     func: Callable,
810     *args: Any,
811     input_core_dims: Sequence[Sequence] = None,
812     output_core_dims: Optional[Sequence[Sequence]] = ((),),
813     exclude_dims: AbstractSet = frozenset(),
814     vectorize: bool = False,
815     join: str = "exact",
816     dataset_join: str = "exact",
817     dataset_fill_value: object = _NO_FILL_VALUE,
818     keep_attrs: Union[bool, str] = None,
819     kwargs: Mapping = None,
820     dask: str = "forbidden",
821     output_dtypes: Sequence = None,
822     output_sizes: Mapping[Any, int] = None,
823     meta: Any = None,
824     dask_gufunc_kwargs: Dict[str, Any] = None,
825 ) -> Any:
826     """Apply a vectorized function for unlabeled arrays on xarray objects.
827 
828     The function will be mapped over the data variable(s) of the input
829     arguments using xarray's standard rules for labeled computation, including
830     alignment, broadcasting, looping over GroupBy/Dataset variables, and
831     merging of coordinates.
832 
833     Parameters
834     ----------
835     func : callable
836         Function to call like ``func(*args, **kwargs)`` on unlabeled arrays
837         (``.data``) that returns an array or tuple of arrays. If multiple
838         arguments with non-matching dimensions are supplied, this function is
839         expected to vectorize (broadcast) over axes of positional arguments in
840         the style of NumPy universal functions [1]_ (if this is not the case,
841         set ``vectorize=True``). If this function returns multiple outputs, you
842         must set ``output_core_dims`` as well.
843     *args : Dataset, DataArray, DataArrayGroupBy, DatasetGroupBy, Variable, numpy.ndarray, dask.array.Array or scalar
844         Mix of labeled and/or unlabeled arrays to which to apply the function.
845     input_core_dims : sequence of sequence, optional
846         List of the same length as ``args`` giving the list of core dimensions
847         on each input argument that should not be broadcast. By default, we
848         assume there are no core dimensions on any input arguments.
849 
850         For example, ``input_core_dims=[[], ['time']]`` indicates that all
851         dimensions on the first argument and all dimensions other than 'time'
852         on the second argument should be broadcast.
853 
854         Core dimensions are automatically moved to the last axes of input
855         variables before applying ``func``, which facilitates using NumPy style
856         generalized ufuncs [2]_.
857     output_core_dims : list of tuple, optional
858         List of the same length as the number of output arguments from
859         ``func``, giving the list of core dimensions on each output that were
860         not broadcast on the inputs. By default, we assume that ``func``
861         outputs exactly one array, with axes corresponding to each broadcast
862         dimension.
863 
864         Core dimensions are assumed to appear as the last dimensions of each
865         output in the provided order.
866     exclude_dims : set, optional
867         Core dimensions on the inputs to exclude from alignment and
868         broadcasting entirely. Any input coordinates along these dimensions
869         will be dropped. Each excluded dimension must also appear in
870         ``input_core_dims`` for at least one argument. Only dimensions listed
871         here are allowed to change size between input and output objects.
872     vectorize : bool, optional
873         If True, then assume ``func`` only takes arrays defined over core
874         dimensions as input and vectorize it automatically with
875         :py:func:`numpy.vectorize`. This option exists for convenience, but is
876         almost always slower than supplying a pre-vectorized function.
877         Using this option requires NumPy version 1.12 or newer.
878     join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
879         Method for joining the indexes of the passed objects along each
880         dimension, and the variables of Dataset objects with mismatched
881         data variables:
882 
883         - 'outer': use the union of object indexes
884         - 'inner': use the intersection of object indexes
885         - 'left': use indexes from the first object with each dimension
886         - 'right': use indexes from the last object with each dimension
887         - 'exact': raise `ValueError` instead of aligning when indexes to be
888           aligned are not equal
889     dataset_join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
890         Method for joining variables of Dataset objects with mismatched
891         data variables.
892 
893         - 'outer': take variables from both Dataset objects
894         - 'inner': take only overlapped variables
895         - 'left': take only variables from the first object
896         - 'right': take only variables from the last object
897         - 'exact': data variables on all Dataset objects must match exactly
898     dataset_fill_value : optional
899         Value used in place of missing variables on Dataset inputs when the
900         datasets do not share the exact same ``data_vars``. Required if
901         ``dataset_join not in {'inner', 'exact'}``, otherwise ignored.
902     keep_attrs : bool, optional
903         Whether to copy attributes from the first argument to the output.
904     kwargs : dict, optional
905         Optional keyword arguments passed directly on to call ``func``.
906     dask : {"forbidden", "allowed", "parallelized"}, default: "forbidden"
907         How to handle applying to objects containing lazy data in the form of
908         dask arrays:
909 
910         - 'forbidden' (default): raise an error if a dask array is encountered.
911         - 'allowed': pass dask arrays directly on to ``func``. Prefer this option if
912           ``func`` natively supports dask arrays.
913         - 'parallelized': automatically parallelize ``func`` if any of the
914           inputs are a dask array by using :py:func:`dask.array.apply_gufunc`. Multiple output
915           arguments are supported. Only use this option if ``func`` does not natively
916           support dask arrays (e.g. converts them to numpy arrays).
917     dask_gufunc_kwargs : dict, optional
918         Optional keyword arguments passed to :py:func:`dask.array.apply_gufunc` if
919         dask='parallelized'. Possible keywords are ``output_sizes``, ``allow_rechunk``
920         and ``meta``.
921     output_dtypes : list of dtype, optional
922         Optional list of output dtypes. Only used if ``dask='parallelized'`` or
923         ``vectorize=True``.
924     output_sizes : dict, optional
925         Optional mapping from dimension names to sizes for outputs. Only used
926         if dask='parallelized' and new dimensions (not found on inputs) appear
927         on outputs. ``output_sizes`` should be given in the ``dask_gufunc_kwargs``
928         parameter. It will be removed as direct parameter in a future version.
929     meta : optional
930         Size-0 object representing the type of array wrapped by dask array. Passed on to
931         :py:func:`dask.array.apply_gufunc`. ``meta`` should be given in the
932         ``dask_gufunc_kwargs`` parameter . It will be removed as direct parameter
933         a future version.
934 
935     Returns
936     -------
937     Single value or tuple of Dataset, DataArray, Variable, dask.array.Array or
938     numpy.ndarray, the first type on that list to appear on an input.
939 
940     Notes
941     -----
942     This function is designed for the more common case where ``func`` can work on numpy
943     arrays. If ``func`` needs to manipulate a whole xarray object subset to each block
944     it is possible to use :py:func:`xarray.map_blocks`.
945 
946     Note that due to the overhead :py:func:`xarray.map_blocks` is considerably slower than ``apply_ufunc``.
947 
948     Examples
949     --------
950     Calculate the vector magnitude of two arguments:
951 
952     >>> def magnitude(a, b):
953     ...     func = lambda x, y: np.sqrt(x ** 2 + y ** 2)
954     ...     return xr.apply_ufunc(func, a, b)
955     ...
956 
957     You can now apply ``magnitude()`` to :py:class:`DataArray` and :py:class:`Dataset`
958     objects, with automatically preserved dimensions and coordinates, e.g.,
959 
960     >>> array = xr.DataArray([1, 2, 3], coords=[("x", [0.1, 0.2, 0.3])])
961     >>> magnitude(array, -array)
962     <xarray.DataArray (x: 3)>
963     array([1.41421356, 2.82842712, 4.24264069])
964     Coordinates:
965       * x        (x) float64 0.1 0.2 0.3
966 
967     Plain scalars, numpy arrays and a mix of these with xarray objects is also
968     supported:
969 
970     >>> magnitude(3, 4)
971     5.0
972     >>> magnitude(3, np.array([0, 4]))
973     array([3., 5.])
974     >>> magnitude(array, 0)
975     <xarray.DataArray (x: 3)>
976     array([1., 2., 3.])
977     Coordinates:
978       * x        (x) float64 0.1 0.2 0.3
979 
980     Other examples of how you could use ``apply_ufunc`` to write functions to
981     (very nearly) replicate existing xarray functionality:
982 
983     Compute the mean (``.mean``) over one dimension:
984 
985     >>> def mean(obj, dim):
986     ...     # note: apply always moves core dimensions to the end
987     ...     return apply_ufunc(
988     ...         np.mean, obj, input_core_dims=[[dim]], kwargs={"axis": -1}
989     ...     )
990     ...
991 
992     Inner product over a specific dimension (like :py:func:`dot`):
993 
994     >>> def _inner(x, y):
995     ...     result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])
996     ...     return result[..., 0, 0]
997     ...
998     >>> def inner_product(a, b, dim):
999     ...     return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])
1000     ...
1001 
1002     Stack objects along a new dimension (like :py:func:`concat`):
1003 
1004     >>> def stack(objects, dim, new_coord):
1005     ...     # note: this version does not stack coordinates
1006     ...     func = lambda *x: np.stack(x, axis=-1)
1007     ...     result = apply_ufunc(
1008     ...         func,
1009     ...         *objects,
1010     ...         output_core_dims=[[dim]],
1011     ...         join="outer",
1012     ...         dataset_fill_value=np.nan
1013     ...     )
1014     ...     result[dim] = new_coord
1015     ...     return result
1016     ...
1017 
1018     If your function is not vectorized but can be applied only to core
1019     dimensions, you can use ``vectorize=True`` to turn into a vectorized
1020     function. This wraps :py:func:`numpy.vectorize`, so the operation isn't
1021     terribly fast. Here we'll use it to calculate the distance between
1022     empirical samples from two probability distributions, using a scipy
1023     function that needs to be applied to vectors:
1024 
1025     >>> import scipy.stats
1026     >>> def earth_mover_distance(first_samples, second_samples, dim="ensemble"):
1027     ...     return apply_ufunc(
1028     ...         scipy.stats.wasserstein_distance,
1029     ...         first_samples,
1030     ...         second_samples,
1031     ...         input_core_dims=[[dim], [dim]],
1032     ...         vectorize=True,
1033     ...     )
1034     ...
1035 
1036     Most of NumPy's builtin functions already broadcast their inputs
1037     appropriately for use in ``apply_ufunc``. You may find helper functions such as
1038     :py:func:`numpy.broadcast_arrays` helpful in writing your function. ``apply_ufunc`` also
1039     works well with :py:func:`numba.vectorize` and :py:func:`numba.guvectorize`.
1040 
1041     See Also
1042     --------
1043     numpy.broadcast_arrays
1044     numba.vectorize
1045     numba.guvectorize
1046     dask.array.apply_gufunc
1047     xarray.map_blocks
1048     :ref:`dask.automatic-parallelization`
1049         User guide describing :py:func:`apply_ufunc` and :py:func:`map_blocks`.
1050 
1051     References
1052     ----------
1053     .. [1] http://docs.scipy.org/doc/numpy/reference/ufuncs.html
1054     .. [2] http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html
1055     """
1056     from .dataarray import DataArray
1057     from .groupby import GroupBy
1058     from .variable import Variable
1059 
1060     if input_core_dims is None:
1061         input_core_dims = ((),) * (len(args))
1062     elif len(input_core_dims) != len(args):
1063         raise ValueError(
1064             f"input_core_dims must be None or a tuple with the length same to "
1065             f"the number of arguments. "
1066             f"Given {len(input_core_dims)} input_core_dims: {input_core_dims}, "
1067             f" but number of args is {len(args)}."
1068         )
1069 
1070     if kwargs is None:
1071         kwargs = {}
1072 
1073     signature = _UFuncSignature(input_core_dims, output_core_dims)
1074 
1075     if exclude_dims:
1076         if not isinstance(exclude_dims, set):
1077             raise TypeError(
1078                 f"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead."
1079             )
1080         if not exclude_dims <= signature.all_core_dims:
1081             raise ValueError(
1082                 f"each dimension in `exclude_dims` must also be a "
1083                 f"core dimension in the function signature. "
1084                 f"Please make {(exclude_dims - signature.all_core_dims)} a core dimension"
1085             )
1086 
1087     # handle dask_gufunc_kwargs
1088     if dask == "parallelized":
1089         if dask_gufunc_kwargs is None:
1090             dask_gufunc_kwargs = {}
1091         else:
1092             dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
1093         # todo: remove warnings after deprecation cycle
1094         if meta is not None:
1095             warnings.warn(
1096                 "``meta`` should be given in the ``dask_gufunc_kwargs`` parameter."
1097                 " It will be removed as direct parameter in a future version.",
1098                 FutureWarning,
1099                 stacklevel=2,
1100             )
1101             dask_gufunc_kwargs.setdefault("meta", meta)
1102         if output_sizes is not None:
1103             warnings.warn(
1104                 "``output_sizes`` should be given in the ``dask_gufunc_kwargs`` "
1105                 "parameter. It will be removed as direct parameter in a future "
1106                 "version.",
1107                 FutureWarning,
1108                 stacklevel=2,
1109             )
1110             dask_gufunc_kwargs.setdefault("output_sizes", output_sizes)
1111 
1112     if kwargs:
1113         func = functools.partial(func, **kwargs)
1114 
1115     if keep_attrs is None:
1116         keep_attrs = _get_keep_attrs(default=False)
1117 
1118     if isinstance(keep_attrs, bool):
1119         keep_attrs = "override" if keep_attrs else "drop"
1120 
1121     variables_vfunc = functools.partial(
1122         apply_variable_ufunc,
1123         func,
1124         signature=signature,
1125         exclude_dims=exclude_dims,
1126         keep_attrs=keep_attrs,
1127         dask=dask,
1128         vectorize=vectorize,
1129         output_dtypes=output_dtypes,
1130         dask_gufunc_kwargs=dask_gufunc_kwargs,
1131     )
1132 
1133     # feed groupby-apply_ufunc through apply_groupby_func
1134     if any(isinstance(a, GroupBy) for a in args):
1135         this_apply = functools.partial(
1136             apply_ufunc,
1137             func,
1138             input_core_dims=input_core_dims,
1139             output_core_dims=output_core_dims,
1140             exclude_dims=exclude_dims,
1141             join=join,
1142             dataset_join=dataset_join,
1143             dataset_fill_value=dataset_fill_value,
1144             keep_attrs=keep_attrs,
1145             dask=dask,
1146             vectorize=vectorize,
1147             output_dtypes=output_dtypes,
1148             dask_gufunc_kwargs=dask_gufunc_kwargs,
1149         )
1150         return apply_groupby_func(this_apply, *args)
1151     # feed datasets apply_variable_ufunc through apply_dataset_vfunc
1152     elif any(is_dict_like(a) for a in args):
1153         return apply_dataset_vfunc(
1154             variables_vfunc,
1155             *args,
1156             signature=signature,
1157             join=join,
1158             exclude_dims=exclude_dims,
1159             dataset_join=dataset_join,
1160             fill_value=dataset_fill_value,
1161             keep_attrs=keep_attrs,
1162         )
1163     # feed DataArray apply_variable_ufunc through apply_dataarray_vfunc
1164     elif any(isinstance(a, DataArray) for a in args):
1165         return apply_dataarray_vfunc(
1166             variables_vfunc,
1167             *args,
1168             signature=signature,
1169             join=join,
1170             exclude_dims=exclude_dims,
1171             keep_attrs=keep_attrs,
1172         )
1173     # feed Variables directly through apply_variable_ufunc
1174     elif any(isinstance(a, Variable) for a in args):
1175         return variables_vfunc(*args)
1176     else:
1177         # feed anything else through apply_array_ufunc
1178         return apply_array_ufunc(func, *args, dask=dask)
1179 
1180 
1181 def cov(da_a, da_b, dim=None, ddof=1):
1182     """
1183     Compute covariance between two DataArray objects along a shared dimension.
1184 
1185     Parameters
1186     ----------
1187     da_a : DataArray
1188         Array to compute.
1189     da_b : DataArray
1190         Array to compute.
1191     dim : str, optional
1192         The dimension along which the covariance will be computed
1193     ddof : int, optional
1194         If ddof=1, covariance is normalized by N-1, giving an unbiased estimate,
1195         else normalization is by N.
1196 
1197     Returns
1198     -------
1199     covariance : DataArray
1200 
1201     See Also
1202     --------
1203     pandas.Series.cov : corresponding pandas function
1204     xarray.corr : respective function to calculate correlation
1205 
1206     Examples
1207     --------
1208     >>> from xarray import DataArray
1209     >>> da_a = DataArray(
1210     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1211     ...     dims=("space", "time"),
1212     ...     coords=[
1213     ...         ("space", ["IA", "IL", "IN"]),
1214     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1215     ...     ],
1216     ... )
1217     >>> da_a
1218     <xarray.DataArray (space: 3, time: 3)>
1219     array([[1. , 2. , 3. ],
1220            [0.1, 0.2, 0.3],
1221            [3.2, 0.6, 1.8]])
1222     Coordinates:
1223       * space    (space) <U2 'IA' 'IL' 'IN'
1224       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1225     >>> da_b = DataArray(
1226     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1227     ...     dims=("space", "time"),
1228     ...     coords=[
1229     ...         ("space", ["IA", "IL", "IN"]),
1230     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1231     ...     ],
1232     ... )
1233     >>> da_b
1234     <xarray.DataArray (space: 3, time: 3)>
1235     array([[ 0.2,  0.4,  0.6],
1236            [15. , 10. ,  5. ],
1237            [ 3.2,  0.6,  1.8]])
1238     Coordinates:
1239       * space    (space) <U2 'IA' 'IL' 'IN'
1240       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1241     >>> xr.cov(da_a, da_b)
1242     <xarray.DataArray ()>
1243     array(-3.53055556)
1244     >>> xr.cov(da_a, da_b, dim="time")
1245     <xarray.DataArray (space: 3)>
1246     array([ 0.2       , -0.5       ,  1.69333333])
1247     Coordinates:
1248       * space    (space) <U2 'IA' 'IL' 'IN'
1249     """
1250     from .dataarray import DataArray
1251 
1252     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1253         raise TypeError(
1254             "Only xr.DataArray is supported."
1255             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1256         )
1257 
1258     return _cov_corr(da_a, da_b, dim=dim, ddof=ddof, method="cov")
1259 
1260 
1261 def corr(da_a, da_b, dim=None):
1262     """
1263     Compute the Pearson correlation coefficient between
1264     two DataArray objects along a shared dimension.
1265 
1266     Parameters
1267     ----------
1268     da_a : DataArray
1269         Array to compute.
1270     da_b : DataArray
1271         Array to compute.
1272     dim : str, optional
1273         The dimension along which the correlation will be computed
1274 
1275     Returns
1276     -------
1277     correlation: DataArray
1278 
1279     See Also
1280     --------
1281     pandas.Series.corr : corresponding pandas function
1282     xarray.cov : underlying covariance function
1283 
1284     Examples
1285     --------
1286     >>> from xarray import DataArray
1287     >>> da_a = DataArray(
1288     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1289     ...     dims=("space", "time"),
1290     ...     coords=[
1291     ...         ("space", ["IA", "IL", "IN"]),
1292     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1293     ...     ],
1294     ... )
1295     >>> da_a
1296     <xarray.DataArray (space: 3, time: 3)>
1297     array([[1. , 2. , 3. ],
1298            [0.1, 0.2, 0.3],
1299            [3.2, 0.6, 1.8]])
1300     Coordinates:
1301       * space    (space) <U2 'IA' 'IL' 'IN'
1302       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1303     >>> da_b = DataArray(
1304     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1305     ...     dims=("space", "time"),
1306     ...     coords=[
1307     ...         ("space", ["IA", "IL", "IN"]),
1308     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1309     ...     ],
1310     ... )
1311     >>> da_b
1312     <xarray.DataArray (space: 3, time: 3)>
1313     array([[ 0.2,  0.4,  0.6],
1314            [15. , 10. ,  5. ],
1315            [ 3.2,  0.6,  1.8]])
1316     Coordinates:
1317       * space    (space) <U2 'IA' 'IL' 'IN'
1318       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1319     >>> xr.corr(da_a, da_b)
1320     <xarray.DataArray ()>
1321     array(-0.57087777)
1322     >>> xr.corr(da_a, da_b, dim="time")
1323     <xarray.DataArray (space: 3)>
1324     array([ 1., -1.,  1.])
1325     Coordinates:
1326       * space    (space) <U2 'IA' 'IL' 'IN'
1327     """
1328     from .dataarray import DataArray
1329 
1330     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1331         raise TypeError(
1332             "Only xr.DataArray is supported."
1333             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1334         )
1335 
1336     return _cov_corr(da_a, da_b, dim=dim, method="corr")
1337 
1338 
1339 def _cov_corr(da_a, da_b, dim=None, ddof=0, method=None):
1340     """
1341     Internal method for xr.cov() and xr.corr() so only have to
1342     sanitize the input arrays once and we don't repeat code.
1343     """
1344     # 1. Broadcast the two arrays
1345     da_a, da_b = align(da_a, da_b, join="inner", copy=False)
1346 
1347     # 2. Ignore the nans
1348     valid_values = da_a.notnull() & da_b.notnull()
1349     da_a = da_a.where(valid_values)
1350     da_b = da_b.where(valid_values)
1351     valid_count = valid_values.sum(dim) - ddof
1352 
1353     # 3. Detrend along the given dim
1354     demeaned_da_a = da_a - da_a.mean(dim=dim)
1355     demeaned_da_b = da_b - da_b.mean(dim=dim)
1356 
1357     # 4. Compute covariance along the given dim
1358     # N.B. `skipna=False` is required or there is a bug when computing
1359     # auto-covariance. E.g. Try xr.cov(da,da) for
1360     # da = xr.DataArray([[1, 2], [1, np.nan]], dims=["x", "time"])
1361     cov = (demeaned_da_a * demeaned_da_b).sum(dim=dim, skipna=True, min_count=1) / (
1362         valid_count
1363     )
1364 
1365     if method == "cov":
1366         return cov
1367 
1368     else:
1369         # compute std + corr
1370         da_a_std = da_a.std(dim=dim)
1371         da_b_std = da_b.std(dim=dim)
1372         corr = cov / (da_a_std * da_b_std)
1373         return corr
1374 
1375 
1376 def dot(*arrays, dims=None, **kwargs):
1377     """Generalized dot product for xarray objects. Like np.einsum, but
1378     provides a simpler interface based on array dimensions.
1379 
1380     Parameters
1381     ----------
1382     *arrays : DataArray or Variable
1383         Arrays to compute.
1384     dims : ..., str or tuple of str, optional
1385         Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
1386         If not specified, then all the common dimensions are summed over.
1387     **kwargs : dict
1388         Additional keyword arguments passed to numpy.einsum or
1389         dask.array.einsum
1390 
1391     Returns
1392     -------
1393     DataArray
1394 
1395     Examples
1396     --------
1397     >>> da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=["a", "b"])
1398     >>> da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=["a", "b", "c"])
1399     >>> da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=["c", "d"])
1400 
1401     >>> da_a
1402     <xarray.DataArray (a: 3, b: 2)>
1403     array([[0, 1],
1404            [2, 3],
1405            [4, 5]])
1406     Dimensions without coordinates: a, b
1407 
1408     >>> da_b
1409     <xarray.DataArray (a: 3, b: 2, c: 2)>
1410     array([[[ 0,  1],
1411             [ 2,  3]],
1412     <BLANKLINE>
1413            [[ 4,  5],
1414             [ 6,  7]],
1415     <BLANKLINE>
1416            [[ 8,  9],
1417             [10, 11]]])
1418     Dimensions without coordinates: a, b, c
1419 
1420     >>> da_c
1421     <xarray.DataArray (c: 2, d: 3)>
1422     array([[0, 1, 2],
1423            [3, 4, 5]])
1424     Dimensions without coordinates: c, d
1425 
1426     >>> xr.dot(da_a, da_b, dims=["a", "b"])
1427     <xarray.DataArray (c: 2)>
1428     array([110, 125])
1429     Dimensions without coordinates: c
1430 
1431     >>> xr.dot(da_a, da_b, dims=["a"])
1432     <xarray.DataArray (b: 2, c: 2)>
1433     array([[40, 46],
1434            [70, 79]])
1435     Dimensions without coordinates: b, c
1436 
1437     >>> xr.dot(da_a, da_b, da_c, dims=["b", "c"])
1438     <xarray.DataArray (a: 3, d: 3)>
1439     array([[  9,  14,  19],
1440            [ 93, 150, 207],
1441            [273, 446, 619]])
1442     Dimensions without coordinates: a, d
1443 
1444     >>> xr.dot(da_a, da_b)
1445     <xarray.DataArray (c: 2)>
1446     array([110, 125])
1447     Dimensions without coordinates: c
1448 
1449     >>> xr.dot(da_a, da_b, dims=...)
1450     <xarray.DataArray ()>
1451     array(235)
1452     """
1453     from .dataarray import DataArray
1454     from .variable import Variable
1455 
1456     if any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):
1457         raise TypeError(
1458             "Only xr.DataArray and xr.Variable are supported."
1459             "Given {}.".format([type(arr) for arr in arrays])
1460         )
1461 
1462     if len(arrays) == 0:
1463         raise TypeError("At least one array should be given.")
1464 
1465     if isinstance(dims, str):
1466         dims = (dims,)
1467 
1468     common_dims = set.intersection(*[set(arr.dims) for arr in arrays])
1469     all_dims = []
1470     for arr in arrays:
1471         all_dims += [d for d in arr.dims if d not in all_dims]
1472 
1473     einsum_axes = "abcdefghijklmnopqrstuvwxyz"
1474     dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}
1475 
1476     if dims is ...:
1477         dims = all_dims
1478     elif dims is None:
1479         # find dimensions that occur more than one times
1480         dim_counts = Counter()
1481         for arr in arrays:
1482             dim_counts.update(arr.dims)
1483         dims = tuple(d for d, c in dim_counts.items() if c > 1)
1484 
1485     dims = tuple(dims)  # make dims a tuple
1486 
1487     # dimensions to be parallelized
1488     broadcast_dims = tuple(d for d in all_dims if d in common_dims and d not in dims)
1489     input_core_dims = [
1490         [d for d in arr.dims if d not in broadcast_dims] for arr in arrays
1491     ]
1492     output_core_dims = [tuple(d for d in all_dims if d not in dims + broadcast_dims)]
1493 
1494     # construct einsum subscripts, such as '...abc,...ab->...c'
1495     # Note: input_core_dims are always moved to the last position
1496     subscripts_list = [
1497         "..." + "".join(dim_map[d] for d in ds) for ds in input_core_dims
1498     ]
1499     subscripts = ",".join(subscripts_list)
1500     subscripts += "->..." + "".join(dim_map[d] for d in output_core_dims[0])
1501 
1502     join = OPTIONS["arithmetic_join"]
1503     # using "inner" emulates `(a * b).sum()` for all joins (except "exact")
1504     if join != "exact":
1505         join = "inner"
1506 
1507     # subscripts should be passed to np.einsum as arg, not as kwargs. We need
1508     # to construct a partial function for apply_ufunc to work.
1509     func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)
1510     result = apply_ufunc(
1511         func,
1512         *arrays,
1513         input_core_dims=input_core_dims,
1514         output_core_dims=output_core_dims,
1515         join=join,
1516         dask="allowed",
1517     )
1518     return result.transpose(*all_dims, missing_dims="ignore")
1519 
1520 
1521 def where(cond, x, y):
1522     """Return elements from `x` or `y` depending on `cond`.
1523 
1524     Performs xarray-like broadcasting across input arguments.
1525 
1526     All dimension coordinates on `x` and `y`  must be aligned with each
1527     other and with `cond`.
1528 
1529     Parameters
1530     ----------
1531     cond : scalar, array, Variable, DataArray or Dataset
1532         When True, return values from `x`, otherwise returns values from `y`.
1533     x : scalar, array, Variable, DataArray or Dataset
1534         values to choose from where `cond` is True
1535     y : scalar, array, Variable, DataArray or Dataset
1536         values to choose from where `cond` is False
1537 
1538     Returns
1539     -------
1540     Dataset, DataArray, Variable or array
1541         In priority order: Dataset, DataArray, Variable or array, whichever
1542         type appears as an input argument.
1543 
1544     Examples
1545     --------
1546     >>> x = xr.DataArray(
1547     ...     0.1 * np.arange(10),
1548     ...     dims=["lat"],
1549     ...     coords={"lat": np.arange(10)},
1550     ...     name="sst",
1551     ... )
1552     >>> x
1553     <xarray.DataArray 'sst' (lat: 10)>
1554     array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
1555     Coordinates:
1556       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1557 
1558     >>> xr.where(x < 0.5, x, x * 100)
1559     <xarray.DataArray 'sst' (lat: 10)>
1560     array([ 0. ,  0.1,  0.2,  0.3,  0.4, 50. , 60. , 70. , 80. , 90. ])
1561     Coordinates:
1562       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1563 
1564     >>> y = xr.DataArray(
1565     ...     0.1 * np.arange(9).reshape(3, 3),
1566     ...     dims=["lat", "lon"],
1567     ...     coords={"lat": np.arange(3), "lon": 10 + np.arange(3)},
1568     ...     name="sst",
1569     ... )
1570     >>> y
1571     <xarray.DataArray 'sst' (lat: 3, lon: 3)>
1572     array([[0. , 0.1, 0.2],
1573            [0.3, 0.4, 0.5],
1574            [0.6, 0.7, 0.8]])
1575     Coordinates:
1576       * lat      (lat) int64 0 1 2
1577       * lon      (lon) int64 10 11 12
1578 
1579     >>> xr.where(y.lat < 1, y, -1)
1580     <xarray.DataArray (lat: 3, lon: 3)>
1581     array([[ 0. ,  0.1,  0.2],
1582            [-1. , -1. , -1. ],
1583            [-1. , -1. , -1. ]])
1584     Coordinates:
1585       * lat      (lat) int64 0 1 2
1586       * lon      (lon) int64 10 11 12
1587 
1588     >>> cond = xr.DataArray([True, False], dims=["x"])
1589     >>> x = xr.DataArray([1, 2], dims=["y"])
1590     >>> xr.where(cond, x, 0)
1591     <xarray.DataArray (x: 2, y: 2)>
1592     array([[1, 2],
1593            [0, 0]])
1594     Dimensions without coordinates: x, y
1595 
1596     See Also
1597     --------
1598     numpy.where : corresponding numpy function
1599     Dataset.where, DataArray.where :
1600         equivalent methods
1601     """
1602     # alignment for three arguments is complicated, so don't support it yet
1603     return apply_ufunc(
1604         duck_array_ops.where,
1605         cond,
1606         x,
1607         y,
1608         join="exact",
1609         dataset_join="exact",
1610         dask="allowed",
1611     )
1612 
1613 
1614 def polyval(coord, coeffs, degree_dim="degree"):
1615     """Evaluate a polynomial at specific values
1616 
1617     Parameters
1618     ----------
1619     coord : DataArray
1620         The 1D coordinate along which to evaluate the polynomial.
1621     coeffs : DataArray
1622         Coefficients of the polynomials.
1623     degree_dim : str, default: "degree"
1624         Name of the polynomial degree dimension in `coeffs`.
1625 
1626     See Also
1627     --------
1628     xarray.DataArray.polyfit
1629     numpy.polyval
1630     """
1631     from .dataarray import DataArray
1632     from .missing import get_clean_interp_index
1633 
1634     x = get_clean_interp_index(coord, coord.name, strict=False)
1635 
1636     deg_coord = coeffs[degree_dim]
1637 
1638     lhs = DataArray(
1639         np.vander(x, int(deg_coord.max()) + 1),
1640         dims=(coord.name, degree_dim),
1641         coords={coord.name: coord, degree_dim: np.arange(deg_coord.max() + 1)[::-1]},
1642     )
1643     return (lhs * coeffs).sum(degree_dim)
1644 
1645 
1646 def _calc_idxminmax(
1647     *,
1648     array,
1649     func: Callable,
1650     dim: Hashable = None,
1651     skipna: bool = None,
1652     fill_value: Any = dtypes.NA,
1653     keep_attrs: bool = None,
1654 ):
1655     """Apply common operations for idxmin and idxmax."""
1656     # This function doesn't make sense for scalars so don't try
1657     if not array.ndim:
1658         raise ValueError("This function does not apply for scalars")
1659 
1660     if dim is not None:
1661         pass  # Use the dim if available
1662     elif array.ndim == 1:
1663         # it is okay to guess the dim if there is only 1
1664         dim = array.dims[0]
1665     else:
1666         # The dim is not specified and ambiguous.  Don't guess.
1667         raise ValueError("Must supply 'dim' argument for multidimensional arrays")
1668 
1669     if dim not in array.dims:
1670         raise KeyError(f'Dimension "{dim}" not in dimension')
1671     if dim not in array.coords:
1672         raise KeyError(f'Dimension "{dim}" does not have coordinates')
1673 
1674     # These are dtypes with NaN values argmin and argmax can handle
1675     na_dtypes = "cfO"
1676 
1677     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
1678         # Need to skip NaN values since argmin and argmax can't handle them
1679         allna = array.isnull().all(dim)
1680         array = array.where(~allna, 0)
1681 
1682     # This will run argmin or argmax.
1683     indx = func(array, dim=dim, axis=None, keep_attrs=keep_attrs, skipna=skipna)
1684 
1685     # Handle dask arrays.
1686     if is_duck_dask_array(array.data):
1687         import dask.array
1688 
1689         chunks = dict(zip(array.dims, array.chunks))
1690         dask_coord = dask.array.from_array(array[dim].data, chunks=chunks[dim])
1691         res = indx.copy(data=dask_coord[indx.data.ravel()].reshape(indx.shape))
1692         # we need to attach back the dim name
1693         res.name = dim
1694     else:
1695         res = array[dim][(indx,)]
1696         # The dim is gone but we need to remove the corresponding coordinate.
1697         del res.coords[dim]
1698 
1699     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
1700         # Put the NaN values back in after removing them
1701         res = res.where(~allna, fill_value)
1702 
1703     # Copy attributes from argmin/argmax, if any
1704     res.attrs = indx.attrs
1705 
1706     return res
1707 
1708 
1709 def unify_chunks(*objects: T_Xarray) -> Tuple[T_Xarray, ...]:
1710     """
1711     Given any number of Dataset and/or DataArray objects, returns
1712     new objects with unified chunk size along all chunked dimensions.
1713 
1714     Returns
1715     -------
1716     unified (DataArray or Dataset) – Tuple of objects with the same type as
1717     *objects with consistent chunk sizes for all dask-array variables
1718 
1719     See Also
1720     --------
1721     dask.array.core.unify_chunks
1722     """
1723     from .dataarray import DataArray
1724 
1725     # Convert all objects to datasets
1726     datasets = [
1727         obj._to_temp_dataset() if isinstance(obj, DataArray) else obj.copy()
1728         for obj in objects
1729     ]
1730 
1731     # Get argumets to pass into dask.array.core.unify_chunks
1732     unify_chunks_args = []
1733     sizes: dict[Hashable, int] = {}
1734     for ds in datasets:
1735         for v in ds._variables.values():
1736             if v.chunks is not None:
1737                 # Check that sizes match across different datasets
1738                 for dim, size in v.sizes.items():
1739                     try:
1740                         if sizes[dim] != size:
1741                             raise ValueError(
1742                                 f"Dimension {dim!r} size mismatch: {sizes[dim]} != {size}"
1743                             )
1744                     except KeyError:
1745                         sizes[dim] = size
1746                 unify_chunks_args += [v._data, v._dims]
1747 
1748     # No dask arrays: Return inputs
1749     if not unify_chunks_args:
1750         return objects
1751 
1752     # Run dask.array.core.unify_chunks
1753     from dask.array.core import unify_chunks
1754 
1755     _, dask_data = unify_chunks(*unify_chunks_args)
1756     dask_data_iter = iter(dask_data)
1757     out = []
1758     for obj, ds in zip(objects, datasets):
1759         for k, v in ds._variables.items():
1760             if v.chunks is not None:
1761                 ds._variables[k] = v.copy(data=next(dask_data_iter))
1762         out.append(obj._from_temp_dataset(ds) if isinstance(obj, DataArray) else ds)
1763 
1764     return tuple(out)

</code><intended_behavior>This file should include a new `cross` function that mimics the structure and approach used by the existing `dot` function but instead employs `numpy.cross` to compute the cross product of two `DataArray` objects. The function should handle input validation (e.g., ensuring that the spatial dimension has a size of 3 for 3D vectors or 2 for 2D vectors), work with dask arrays (using `dask='parallelized'` when applicable), and respect xarray's dimension handling conventions. This new function will extend xarray's computational capabilities to include vector cross products, filling a gap for users working on physical applications requiring such operations.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
218 class DataArray(AbstractArray, DataWithCoords, DataArrayArithmetic):
219     """N-dimensional array with labeled coordinates and dimensions.
220 
221     DataArray provides a wrapper around numpy ndarrays that uses
222     labeled dimensions and coordinates to support metadata aware
223     operations. The API is similar to that for the pandas Series or
224     DataFrame, but DataArray objects can have any number of dimensions,
225     and their contents have fixed data types.
226 
227     Additional features over raw numpy arrays:
228 
229     - Apply operations over dimensions by name: ``x.sum('time')``.
230     - Select or assign values by integer location (like numpy):
231       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
232       ``x.sel(time='2014-01-01')``.
233     - Mathematical operations (e.g., ``x - y``) vectorize across
234       multiple dimensions (known in numpy as "broadcasting") based on
235       dimension names, regardless of their original order.
236     - Keep track of arbitrary metadata in the form of a Python
237       dictionary: ``x.attrs``
238     - Convert to a pandas Series: ``x.to_series()``.
239 
240     Getting items from or doing mathematical operations with a
241     DataArray always returns another DataArray.
242 
243     Parameters
244     ----------
245     data : array_like
246         Values for this array. Must be an ``numpy.ndarray``, ndarray
247         like, or castable to an ``ndarray``. If a self-described xarray
248         or pandas object, attempts are made to use this array's
249         metadata to fill in other unspecified arguments. A view of the
250         array's data is used instead of a copy if possible.
251     coords : sequence or dict of array_like, optional
252         Coordinates (tick labels) to use for indexing along each
253         dimension. The following notations are accepted:
254 
255         - mapping {dimension name: array-like}
256         - sequence of tuples that are valid arguments for
257           ``xarray.Variable()``
258           - (dims, data)
259           - (dims, data, attrs)
260           - (dims, data, attrs, encoding)
261 
262         Additionally, it is possible to define a coord whose name
263         does not match the dimension name, or a coord based on multiple
264         dimensions, with one of the following notations:
265 
266         - mapping {coord name: DataArray}
267         - mapping {coord name: Variable}
268         - mapping {coord name: (dimension name, array-like)}
269         - mapping {coord name: (tuple of dimension names, array-like)}
270 
271     dims : hashable or sequence of hashable, optional
272         Name(s) of the data dimension(s). Must be either a hashable
273         (only for 1D data) or a sequence of hashables with length equal
274         to the number of dimensions. If this argument is omitted,
275         dimension names are taken from ``coords`` (if possible) and
276         otherwise default to ``['dim_0', ... 'dim_n']``.
277     name : str or None, optional
278         Name of this array.
279     attrs : dict_like or None, optional
280         Attributes to assign to the new instance. By default, an empty
281         attribute dictionary is initialized.
282 
283     Examples
284     --------
285     Create data:
286 
287     >>> np.random.seed(0)
288     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
289     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
290     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
291     >>> time = pd.date_range("2014-09-06", periods=3)
292     >>> reference_time = pd.Timestamp("2014-09-05")
293 
294     Initialize a dataarray with multiple dimensions:
295 
296     >>> da = xr.DataArray(
297     ...     data=temperature,
298     ...     dims=["x", "y", "time"],
299     ...     coords=dict(
300     ...         lon=(["x", "y"], lon),
301     ...         lat=(["x", "y"], lat),
302     ...         time=time,
303     ...         reference_time=reference_time,
304     ...     ),
305     ...     attrs=dict(
306     ...         description="Ambient temperature.",
307     ...         units="degC",
308     ...     ),
309     ... )
310     >>> da
311     <xarray.DataArray (x: 2, y: 2, time: 3)>
312     array([[[29.11241877, 18.20125767, 22.82990387],
313             [32.92714559, 29.94046392,  7.18177696]],
314     <BLANKLINE>
315            [[22.60070734, 13.78914233, 14.17424919],
316             [18.28478802, 16.15234857, 26.63418806]]])
317     Coordinates:
318         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
319         lat             (x, y) float64 42.25 42.21 42.63 42.59
320       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
321         reference_time  datetime64[ns] 2014-09-05
322     Dimensions without coordinates: x, y
323     Attributes:
324         description:  Ambient temperature.
325         units:        degC
326 
327     Find out where the coldest temperature was:
328 
329     >>> da.isel(da.argmin(...))
330     <xarray.DataArray ()>
331     array(7.18177696)
332     Coordinates:
333         lon             float64 -99.32
334         lat             float64 42.21
335         time            datetime64[ns] 2014-09-08
336         reference_time  datetime64[ns] 2014-09-05
337     Attributes:
338         description:  Ambient temperature.
339         units:        degC
340     """
341 
342     _cache: Dict[str, Any]
343     _coords: Dict[Any, Variable]
344     _close: Optional[Callable[[], None]]
345     _indexes: Optional[Dict[Hashable, Index]]
346     _name: Optional[Hashable]
347     _variable: Variable
348 
349     __slots__ = (
350         "_cache",
351         "_coords",
352         "_close",
353         "_indexes",
354         "_name",
355         "_variable",
356         "__weakref__",
357     )
358 
359     _groupby_cls = groupby.DataArrayGroupBy
360     _rolling_cls = rolling.DataArrayRolling
361     _coarsen_cls = rolling.DataArrayCoarsen
362     _resample_cls = resample.DataArrayResample
363     _weighted_cls = weighted.DataArrayWeighted
364 
365     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
366 
367     def __init__(
368         self,
369         data: Any = dtypes.NA,
370         coords: Union[Sequence[Tuple], Mapping[Any, Any], None] = None,
371         dims: Union[Hashable, Sequence[Hashable], None] = None,
372         name: Hashable = None,
373         attrs: Mapping = None,
374         # internal parameters
375         indexes: Dict[Hashable, pd.Index] = None,
376         fastpath: bool = False,
377     ):
378         if fastpath:
379             variable = data
380             assert dims is None
381             assert attrs is None
382         else:
383             # try to fill in arguments from data if they weren't supplied
384             if coords is None:
385 
386                 if isinstance(data, DataArray):
387                     coords = data.coords
388                 elif isinstance(data, pd.Series):
389                     coords = [data.index]
390                 elif isinstance(data, pd.DataFrame):
391                     coords = [data.index, data.columns]
392                 elif isinstance(data, (pd.Index, IndexVariable)):
393                     coords = [data]
394                 elif isinstance(data, pdcompat.Panel):
395                     coords = [data.items, data.major_axis, data.minor_axis]
396 
397             if dims is None:
398                 dims = getattr(data, "dims", getattr(coords, "dims", None))
399             if name is None:
400                 name = getattr(data, "name", None)
401             if attrs is None and not isinstance(data, PANDAS_TYPES):
402                 attrs = getattr(data, "attrs", None)
403 
404             data = _check_data_shape(data, coords, dims)
405             data = as_compatible_data(data)
406             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
407             variable = Variable(dims, data, attrs, fastpath=True)
408             indexes = dict(
409                 _extract_indexes_from_coords(coords)
410             )  # needed for to_dataset
411 
412         # These fully describe a DataArray
413         self._variable = variable
414         assert isinstance(coords, dict)
415         self._coords = coords
416         self._name = name
417 
418         # TODO(shoyer): document this argument, once it becomes part of the
419         # public interface.
420         self._indexes = indexes
421 
422         self._close = None
423 
424     def _replace(
425         self: T_DataArray,
426         variable: Variable = None,
427         coords=None,
428         name: Union[Hashable, None, Default] = _default,
429         indexes=None,
430     ) -> T_DataArray:
431         if variable is None:
432             variable = self.variable
433         if coords is None:
434             coords = self._coords
435         if name is _default:
436             name = self.name
437         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
438 
439     def _replace_maybe_drop_dims(
440         self, variable: Variable, name: Union[Hashable, None, Default] = _default
441     ) -> "DataArray":
442         if variable.dims == self.dims and variable.shape == self.shape:
443             coords = self._coords.copy()
444             indexes = self._indexes
445         elif variable.dims == self.dims:
446             # Shape has changed (e.g. from reduce(..., keepdims=True)
447             new_sizes = dict(zip(self.dims, variable.shape))
448             coords = {
449                 k: v
450                 for k, v in self._coords.items()
451                 if v.shape == tuple(new_sizes[d] for d in v.dims)
452             }
453             changed_dims = [
454                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
455             ]
456             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
457         else:
458             allowed_dims = set(variable.dims)
459             coords = {
460                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
461             }
462             indexes = propagate_indexes(
463                 self._indexes, exclude=(set(self.dims) - allowed_dims)
464             )
465         return self._replace(variable, coords, name, indexes=indexes)
466 
467     def _overwrite_indexes(self, indexes: Mapping[Any, Any]) -> "DataArray":
468         if not len(indexes):
469             return self
470         coords = self._coords.copy()
471         for name, idx in indexes.items():
472             coords[name] = IndexVariable(name, idx.to_pandas_index())
473         obj = self._replace(coords=coords)
474 
475         # switch from dimension to level names, if necessary
476         dim_names: Dict[Any, str] = {}
477         for dim, idx in indexes.items():
478             pd_idx = idx.to_pandas_index()
479             if not isinstance(idx, pd.MultiIndex) and pd_idx.name != dim:
480                 dim_names[dim] = idx.name
481         if dim_names:
482             obj = obj.rename(dim_names)
483         return obj
484 
485     def _to_temp_dataset(self) -> Dataset:
486         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
487 
488     def _from_temp_dataset(
489         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
490     ) -> "DataArray":
491         variable = dataset._variables.pop(_THIS_ARRAY)
492         coords = dataset._variables
493         indexes = dataset._indexes
494         return self._replace(variable, coords, name, indexes=indexes)
495 
496     def _to_dataset_split(self, dim: Hashable) -> Dataset:
497         """splits dataarray along dimension 'dim'"""
498 
499         def subset(dim, label):
500             array = self.loc[{dim: label}]
501             array.attrs = {}
502             return as_variable(array)
503 
504         variables = {label: subset(dim, label) for label in self.get_index(dim)}
505         variables.update({k: v for k, v in self._coords.items() if k != dim})
506         indexes = propagate_indexes(self._indexes, exclude=dim)
507         coord_names = set(self._coords) - {dim}
508         dataset = Dataset._construct_direct(
509             variables, coord_names, indexes=indexes, attrs=self.attrs
510         )
511         return dataset
512 
513     def _to_dataset_whole(
514         self, name: Hashable = None, shallow_copy: bool = True
515     ) -> Dataset:
516         if name is None:
517             name = self.name
518         if name is None:
519             raise ValueError(
520                 "unable to convert unnamed DataArray to a "
521                 "Dataset without providing an explicit name"
522             )
523         if name in self.coords:
524             raise ValueError(
525                 "cannot create a Dataset from a DataArray with "
526                 "the same name as one of its coordinates"
527             )
528         # use private APIs for speed: this is called by _to_temp_dataset(),
529         # which is used in the guts of a lot of operations (e.g., reindex)
530         variables = self._coords.copy()
531         variables[name] = self.variable
532         if shallow_copy:
533             for k in variables:
534                 variables[k] = variables[k].copy(deep=False)
535         indexes = self._indexes
536 
537         coord_names = set(self._coords)
538         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
539 
540     def to_dataset(
541         self,
542         dim: Hashable = None,
543         *,
544         name: Hashable = None,
545         promote_attrs: bool = False,
546     ) -> Dataset:
547         """Convert a DataArray to a Dataset.
548 
549         Parameters
550         ----------
551         dim : hashable, optional
552             Name of the dimension on this array along which to split this array
553             into separate variables. If not provided, this array is converted
554             into a Dataset of one variable.
555         name : hashable, optional
556             Name to substitute for this array's name. Only valid if ``dim`` is
557             not provided.
558         promote_attrs : bool, default: False
559             Set to True to shallow copy attrs of DataArray to returned Dataset.
560 
561         Returns
562         -------
563         dataset : Dataset
564         """
565         if dim is not None and dim not in self.dims:
566             raise TypeError(
567                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
568             )
569 
570         if dim is not None:
571             if name is not None:
572                 raise TypeError("cannot supply both dim and name arguments")
573             result = self._to_dataset_split(dim)
574         else:
575             result = self._to_dataset_whole(name)
576 
577         if promote_attrs:
578             result.attrs = dict(self.attrs)
579 
580         return result
581 
582     @property
583     def name(self) -> Optional[Hashable]:
584         """The name of this array."""
585         return self._name
586 
587     @name.setter
588     def name(self, value: Optional[Hashable]) -> None:
589         self._name = value
590 
591     @property
592     def variable(self) -> Variable:
593         """Low level interface to the Variable object for this DataArray."""
594         return self._variable
595 
596     @property
597     def dtype(self) -> np.dtype:
598         return self.variable.dtype
599 
600     @property
601     def shape(self) -> Tuple[int, ...]:
602         return self.variable.shape
603 
604     @property
605     def size(self) -> int:
606         return self.variable.size
607 
608     @property
609     def nbytes(self) -> int:
610         return self.variable.nbytes
611 
612     @property
613     def ndim(self) -> int:
614         return self.variable.ndim
615 
616     def __len__(self) -> int:
617         return len(self.variable)
618 
619     @property
620     def data(self) -> Any:
621         """
622         The DataArray's data as an array. The underlying array type
623         (e.g. dask, sparse, pint) is preserved.
624 
625         See Also
626         --------
627         DataArray.to_numpy
628         DataArray.as_numpy
629         DataArray.values
630         """
631         return self.variable.data
632 
633     @data.setter
634     def data(self, value: Any) -> None:
635         self.variable.data = value
636 
637     @property
638     def values(self) -> np.ndarray:
639         """
640         The array's data as a numpy.ndarray.
641 
642         If the array's data is not a numpy.ndarray this will attempt to convert
643         it naively using np.array(), which will raise an error if the array
644         type does not support coercion like this (e.g. cupy).
645         """
646         return self.variable.values
647 
648     @values.setter
649     def values(self, value: Any) -> None:
650         self.variable.values = value
651 
652     def to_numpy(self) -> np.ndarray:
653         """
654         Coerces wrapped data to numpy and returns a numpy.ndarray.
655 
656         See Also
657         --------
658         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
659         Dataset.as_numpy
660         DataArray.values
661         DataArray.data
662         """
663         return self.variable.to_numpy()
664 
665     def as_numpy(self: T_DataArray) -> T_DataArray:
666         """
667         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
668 
669         See Also
670         --------
671         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
672         Dataset.as_numpy : Converts all variables in a Dataset.
673         DataArray.values
674         DataArray.data
675         """
676         coords = {k: v.as_numpy() for k, v in self._coords.items()}
677         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
678 
679     @property
680     def _in_memory(self) -> bool:
681         return self.variable._in_memory
682 
683     def to_index(self) -> pd.Index:
684         """Convert this variable to a pandas.Index. Only possible for 1D
685         arrays.
686         """
687         return self.variable.to_index()
688 
689     @property
690     def dims(self) -> Tuple[Hashable, ...]:
691         """Tuple of dimension names associated with this array.
692 
693         Note that the type of this property is inconsistent with
694         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
695         consistently named properties.
696         """
697         return self.variable.dims
698 
699     @dims.setter
700     def dims(self, value):
701         raise AttributeError(
702             "you cannot assign dims on a DataArray. Use "
703             ".rename() or .swap_dims() instead."
704         )
705 
706     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
707         if utils.is_dict_like(key):
708             return key
709         key = indexing.expanded_indexer(key, self.ndim)
710         return dict(zip(self.dims, key))
711 
712     @property
713     def _level_coords(self) -> Dict[Hashable, Hashable]:
714         """Return a mapping of all MultiIndex levels and their corresponding
715         coordinate name.
716         """
717         level_coords: Dict[Hashable, Hashable] = {}
718 
719         for cname, var in self._coords.items():
720             if var.ndim == 1 and isinstance(var, IndexVariable):
721                 level_names = var.level_names
722                 if level_names is not None:
723                     (dim,) = var.dims
724                     level_coords.update({lname: dim for lname in level_names})
725         return level_coords
726 
727     def _getitem_coord(self, key):
728         from .dataset import _get_virtual_variable
729 
730         try:
731             var = self._coords[key]
732         except KeyError:
733             dim_sizes = dict(zip(self.dims, self.shape))
734             _, key, var = _get_virtual_variable(
735                 self._coords, key, self._level_coords, dim_sizes
736             )
737 
738         return self._replace_maybe_drop_dims(var, name=key)
739 
740     def __getitem__(self, key: Any) -> "DataArray":
741         if isinstance(key, str):
742             return self._getitem_coord(key)
743         else:
744             # xarray-style array indexing
745             return self.isel(indexers=self._item_key_to_dict(key))
746 
747     def __setitem__(self, key: Any, value: Any) -> None:
748         if isinstance(key, str):
749             self.coords[key] = value
750         else:
751             # Coordinates in key, value and self[key] should be consistent.
752             # TODO Coordinate consistency in key is checked here, but it
753             # causes unnecessary indexing. It should be optimized.
754             obj = self[key]
755             if isinstance(value, DataArray):
756                 assert_coordinate_consistent(value, obj.coords.variables)
757             # DataArray key -> Variable key
758             key = {
759                 k: v.variable if isinstance(v, DataArray) else v
760                 for k, v in self._item_key_to_dict(key).items()
761             }
762             self.variable[key] = value
763 
764     def __delitem__(self, key: Any) -> None:
765         del self.coords[key]
766 
767     @property
768     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
769         """Places to look-up items for attribute-style access"""
770         yield from self._item_sources
771         yield self.attrs
772 
773     @property
774     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
775         """Places to look-up items for key-completion"""
776         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
777 
778         # virtual coordinates
779         # uses empty dict -- everything here can already be found in self.coords.
780         yield HybridMappingProxy(keys=self.dims, mapping={})
781         yield HybridMappingProxy(keys=self._level_coords, mapping={})
782 
783     def __contains__(self, key: Any) -> bool:
784         return key in self.data
785 
786     @property
787     def loc(self) -> _LocIndexer:
788         """Attribute for location based indexing like pandas."""
789         return _LocIndexer(self)
790 
791     @property
792     # Key type needs to be `Any` because of mypy#4167
793     def attrs(self) -> Dict[Any, Any]:
794         """Dictionary storing arbitrary metadata with this array."""
795         return self.variable.attrs
796 
797     @attrs.setter
798     def attrs(self, value: Mapping[Any, Any]) -> None:
799         # Disable type checking to work around mypy bug - see mypy#4167
800         self.variable.attrs = value  # type: ignore[assignment]
801 
802     @property
803     def encoding(self) -> Dict[Hashable, Any]:
804         """Dictionary of format-specific settings for how this array should be
805         serialized."""
806         return self.variable.encoding
807 
808     @encoding.setter
809     def encoding(self, value: Mapping[Any, Any]) -> None:
810         self.variable.encoding = value
811 
812     @property
813     def indexes(self) -> Indexes:
814         """Mapping of pandas.Index objects used for label based indexing.
815 
816         Raises an error if this Dataset has indexes that cannot be coerced
817         to pandas.Index objects.
818 
819         See Also
820         --------
821         DataArray.xindexes
822 
823         """
824         return Indexes({k: idx.to_pandas_index() for k, idx in self.xindexes.items()})
825 
826     @property
827     def xindexes(self) -> Indexes:
828         """Mapping of xarray Index objects used for label based indexing."""
829         if self._indexes is None:
830             self._indexes = default_indexes(self._coords, self.dims)
831         return Indexes(self._indexes)
832 
833     @property
834     def coords(self) -> DataArrayCoordinates:
835         """Dictionary-like container of coordinate arrays."""
836         return DataArrayCoordinates(self)
837 
838     def reset_coords(
839         self,
840         names: Union[Iterable[Hashable], Hashable, None] = None,
841         drop: bool = False,
842     ) -> Union[None, "DataArray", Dataset]:
843         """Given names of coordinates, reset them to become variables.
844 
845         Parameters
846         ----------
847         names : hashable or iterable of hashable, optional
848             Name(s) of non-index coordinates in this dataset to reset into
849             variables. By default, all non-index coordinates are reset.
850         drop : bool, optional
851             If True, remove coordinates instead of converting them into
852             variables.
853 
854         Returns
855         -------
856         Dataset, or DataArray if ``drop == True``
857         """
858         if names is None:
859             names = set(self.coords) - set(self.dims)
860         dataset = self.coords.to_dataset().reset_coords(names, drop)
861         if drop:
862             return self._replace(coords=dataset._variables)
863         if self.name is None:
864             raise ValueError(
865                 "cannot reset_coords with drop=False on an unnamed DataArrray"
866             )
867         dataset[self.name] = self.variable
868         return dataset
869 
870     def __dask_tokenize__(self):
871         from dask.base import normalize_token
872 
873         return normalize_token((type(self), self._variable, self._coords, self._name))
874 
875     def __dask_graph__(self):
876         return self._to_temp_dataset().__dask_graph__()
877 
878     def __dask_keys__(self):
879         return self._to_temp_dataset().__dask_keys__()
880 
881     def __dask_layers__(self):
882         return self._to_temp_dataset().__dask_layers__()
883 
884     @property
885     def __dask_optimize__(self):
886         return self._to_temp_dataset().__dask_optimize__
887 
888     @property
889     def __dask_scheduler__(self):
890         return self._to_temp_dataset().__dask_scheduler__
891 
892     def __dask_postcompute__(self):
893         func, args = self._to_temp_dataset().__dask_postcompute__()
894         return self._dask_finalize, (self.name, func) + args
895 
896     def __dask_postpersist__(self):
897         func, args = self._to_temp_dataset().__dask_postpersist__()
898         return self._dask_finalize, (self.name, func) + args
899 
900     @staticmethod
901     def _dask_finalize(results, name, func, *args, **kwargs):
902         ds = func(results, *args, **kwargs)
903         variable = ds._variables.pop(_THIS_ARRAY)
904         coords = ds._variables
905         return DataArray(variable, coords, name=name, fastpath=True)
906 
907     def load(self, **kwargs) -> "DataArray":
908         """Manually trigger loading of this array's data from disk or a
909         remote source into memory and return this array.
910 
911         Normally, it should not be necessary to call this method in user code,
912         because all xarray functions should either work on deferred data or
913         load data automatically. However, this method can be necessary when
914         working with many file objects on disk.
915 
916         Parameters
917         ----------
918         **kwargs : dict
919             Additional keyword arguments passed on to ``dask.compute``.
920 
921         See Also
922         --------
923         dask.compute
924         """
925         ds = self._to_temp_dataset().load(**kwargs)
926         new = self._from_temp_dataset(ds)
927         self._variable = new._variable
928         self._coords = new._coords
929         return self
930 
931     def compute(self, **kwargs) -> "DataArray":
932         """Manually trigger loading of this array's data from disk or a
933         remote source into memory and return a new array. The original is
934         left unaltered.
935 
936         Normally, it should not be necessary to call this method in user code,
937         because all xarray functions should either work on deferred data or
938         load data automatically. However, this method can be necessary when
939         working with many file objects on disk.
940 
941         Parameters
942         ----------
943         **kwargs : dict
944             Additional keyword arguments passed on to ``dask.compute``.
945 
946         See Also
947         --------
948         dask.compute
949         """
950         new = self.copy(deep=False)
951         return new.load(**kwargs)
952 
953     def persist(self, **kwargs) -> "DataArray":
954         """Trigger computation in constituent dask arrays
955 
956         This keeps them as dask arrays but encourages them to keep data in
957         memory.  This is particularly useful when on a distributed machine.
958         When on a single machine consider using ``.compute()`` instead.
959 
960         Parameters
961         ----------
962         **kwargs : dict
963             Additional keyword arguments passed on to ``dask.persist``.
964 
965         See Also
966         --------
967         dask.persist
968         """
969         ds = self._to_temp_dataset().persist(**kwargs)
970         return self._from_temp_dataset(ds)
971 
972     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
973         """Returns a copy of this array.
974 
975         If `deep=True`, a deep copy is made of the data array.
976         Otherwise, a shallow copy is made, and the returned data array's
977         values are a new view of this data array's values.
978 
979         Use `data` to create a new object with the same structure as
980         original but entirely new data.
981 
982         Parameters
983         ----------
984         deep : bool, optional
985             Whether the data array and its coordinates are loaded into memory
986             and copied onto the new object. Default is True.
987         data : array_like, optional
988             Data to use in the new object. Must have same shape as original.
989             When `data` is used, `deep` is ignored for all data variables,
990             and only used for coords.
991 
992         Returns
993         -------
994         object : DataArray
995             New object with dimensions, attributes, coordinates, name,
996             encoding, and optionally data copied from original.
997 
998         Examples
999         --------
1000         Shallow versus deep copy
1001 
1002         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1003         >>> array.copy()
1004         <xarray.DataArray (x: 3)>
1005         array([1, 2, 3])
1006         Coordinates:
1007           * x        (x) <U1 'a' 'b' 'c'
1008         >>> array_0 = array.copy(deep=False)
1009         >>> array_0[0] = 7
1010         >>> array_0
1011         <xarray.DataArray (x: 3)>
1012         array([7, 2, 3])
1013         Coordinates:
1014           * x        (x) <U1 'a' 'b' 'c'
1015         >>> array
1016         <xarray.DataArray (x: 3)>
1017         array([7, 2, 3])
1018         Coordinates:
1019           * x        (x) <U1 'a' 'b' 'c'
1020 
1021         Changing the data using the ``data`` argument maintains the
1022         structure of the original object, but with the new data. Original
1023         object is unaffected.
1024 
1025         >>> array.copy(data=[0.1, 0.2, 0.3])
1026         <xarray.DataArray (x: 3)>
1027         array([0.1, 0.2, 0.3])
1028         Coordinates:
1029           * x        (x) <U1 'a' 'b' 'c'
1030         >>> array
1031         <xarray.DataArray (x: 3)>
1032         array([7, 2, 3])
1033         Coordinates:
1034           * x        (x) <U1 'a' 'b' 'c'
1035 
1036         See Also
1037         --------
1038         pandas.DataFrame.copy
1039         """
1040         variable = self.variable.copy(deep=deep, data=data)
1041         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
1042         if self._indexes is None:
1043             indexes = self._indexes
1044         else:
1045             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
1046         return self._replace(variable, coords, indexes=indexes)
1047 
1048     def __copy__(self) -> "DataArray":
1049         return self.copy(deep=False)
1050 
1051     def __deepcopy__(self, memo=None) -> "DataArray":
1052         # memo does nothing but is required for compatibility with
1053         # copy.deepcopy
1054         return self.copy(deep=True)
1055 
1056     # mutable objects should not be hashable
1057     # https://github.com/python/mypy/issues/4266
1058     __hash__ = None  # type: ignore[assignment]
1059 
1060     @property
1061     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1062         """
1063         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1064         the underlying data is not a dask array.
1065 
1066         See Also
1067         --------
1068         DataArray.chunk
1069         DataArray.chunksizes
1070         xarray.unify_chunks
1071         """
1072         return self.variable.chunks
1073 
1074     @property
1075     def chunksizes(self) -> Mapping[Any, Tuple[int, ...]]:
1076         """
1077         Mapping from dimension names to block lengths for this dataarray's data, or None if
1078         the underlying data is not a dask array.
1079         Cannot be modified directly, but can be modified by calling .chunk().
1080 
1081         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1082         instead of a tuple of chunk shapes.
1083 
1084         See Also
1085         --------
1086         DataArray.chunk
1087         DataArray.chunks
1088         xarray.unify_chunks
1089         """
1090         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1091         return get_chunksizes(all_variables)
1092 
1093     def chunk(
1094         self,
1095         chunks: Union[
1096             int,
1097             Tuple[int, ...],
1098             Tuple[Tuple[int, ...], ...],
1099             Mapping[Any, Union[None, int, Tuple[int, ...]]],
1100         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1101         name_prefix: str = "xarray-",
1102         token: str = None,
1103         lock: bool = False,
1104     ) -> "DataArray":
1105         """Coerce this array's data into a dask arrays with the given chunks.
1106 
1107         If this variable is a non-dask array, it will be converted to dask
1108         array. If it's a dask array, it will be rechunked to the given chunk
1109         sizes.
1110 
1111         If neither chunks is not provided for one or more dimensions, chunk
1112         sizes along that dimension will not be updated; non-dask arrays will be
1113         converted into dask arrays with a single block.
1114 
1115         Parameters
1116         ----------
1117         chunks : int, tuple of int or mapping of hashable to int, optional
1118             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1119             ``{'x': 5, 'y': 5}``.
1120         name_prefix : str, optional
1121             Prefix for the name of the new dask array.
1122         token : str, optional
1123             Token uniquely identifying this array.
1124         lock : optional
1125             Passed on to :py:func:`dask.array.from_array`, if the array is not
1126             already as dask array.
1127 
1128         Returns
1129         -------
1130         chunked : xarray.DataArray
1131         """
1132         if isinstance(chunks, (tuple, list)):
1133             chunks = dict(zip(self.dims, chunks))
1134 
1135         ds = self._to_temp_dataset().chunk(
1136             chunks, name_prefix=name_prefix, token=token, lock=lock
1137         )
1138         return self._from_temp_dataset(ds)
1139 
1140     def isel(
1141         self,
1142         indexers: Mapping[Any, Any] = None,
1143         drop: bool = False,
1144         missing_dims: str = "raise",
1145         **indexers_kwargs: Any,
1146     ) -> "DataArray":
1147         """Return a new DataArray whose data is given by integer indexing
1148         along the specified dimension(s).
1149 
1150         Parameters
1151         ----------
1152         indexers : dict, optional
1153             A dict with keys matching dimensions and values given
1154             by integers, slice objects or arrays.
1155             indexer can be a integer, slice, array-like or DataArray.
1156             If DataArrays are passed as indexers, xarray-style indexing will be
1157             carried out. See :ref:`indexing` for the details.
1158             One of indexers or indexers_kwargs must be provided.
1159         drop : bool, optional
1160             If ``drop=True``, drop coordinates variables indexed by integers
1161             instead of making them scalar.
1162         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1163             What to do if dimensions that should be selected from are not present in the
1164             DataArray:
1165             - "raise": raise an exception
1166             - "warn": raise a warning, and ignore the missing dimensions
1167             - "ignore": ignore the missing dimensions
1168         **indexers_kwargs : {dim: indexer, ...}, optional
1169             The keyword arguments form of ``indexers``.
1170 
1171         See Also
1172         --------
1173         Dataset.isel
1174         DataArray.sel
1175 
1176         Examples
1177         --------
1178         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1179         >>> da
1180         <xarray.DataArray (x: 5, y: 5)>
1181         array([[ 0,  1,  2,  3,  4],
1182                [ 5,  6,  7,  8,  9],
1183                [10, 11, 12, 13, 14],
1184                [15, 16, 17, 18, 19],
1185                [20, 21, 22, 23, 24]])
1186         Dimensions without coordinates: x, y
1187 
1188         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1189         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1190         >>> da = da.isel(x=tgt_x, y=tgt_y)
1191         >>> da
1192         <xarray.DataArray (points: 5)>
1193         array([ 0,  6, 12, 18, 24])
1194         Dimensions without coordinates: points
1195         """
1196 
1197         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1198 
1199         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1200             ds = self._to_temp_dataset()._isel_fancy(
1201                 indexers, drop=drop, missing_dims=missing_dims
1202             )
1203             return self._from_temp_dataset(ds)
1204 
1205         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1206         # lists, or zero or one-dimensional np.ndarray's
1207 
1208         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1209 
1210         coords = {}
1211         for coord_name, coord_value in self._coords.items():
1212             coord_indexers = {
1213                 k: v for k, v in indexers.items() if k in coord_value.dims
1214             }
1215             if coord_indexers:
1216                 coord_value = coord_value.isel(coord_indexers)
1217                 if drop and coord_value.ndim == 0:
1218                     continue
1219             coords[coord_name] = coord_value
1220 
1221         return self._replace(variable=variable, coords=coords)
1222 
1223     def sel(
1224         self,
1225         indexers: Mapping[Any, Any] = None,
1226         method: str = None,
1227         tolerance=None,
1228         drop: bool = False,
1229         **indexers_kwargs: Any,
1230     ) -> "DataArray":
1231         """Return a new DataArray whose data is given by selecting index
1232         labels along the specified dimension(s).
1233 
1234         In contrast to `DataArray.isel`, indexers for this method should use
1235         labels instead of integers.
1236 
1237         Under the hood, this method is powered by using pandas's powerful Index
1238         objects. This makes label based indexing essentially just as fast as
1239         using integer indexing.
1240 
1241         It also means this method uses pandas's (well documented) logic for
1242         indexing. This means you can use string shortcuts for datetime indexes
1243         (e.g., '2000-01' to select all values in January 2000). It also means
1244         that slices are treated as inclusive of both the start and stop values,
1245         unlike normal Python indexing.
1246 
1247         .. warning::
1248 
1249           Do not try to assign values when using any of the indexing methods
1250           ``isel`` or ``sel``::
1251 
1252             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1253             # DO NOT do this
1254             da.isel(x=[0, 1, 2])[1] = -1
1255 
1256           Assigning values with the chained indexing using ``.sel`` or
1257           ``.isel`` fails silently.
1258 
1259         Parameters
1260         ----------
1261         indexers : dict, optional
1262             A dict with keys matching dimensions and values given
1263             by scalars, slices or arrays of tick labels. For dimensions with
1264             multi-index, the indexer may also be a dict-like object with keys
1265             matching index level names.
1266             If DataArrays are passed as indexers, xarray-style indexing will be
1267             carried out. See :ref:`indexing` for the details.
1268             One of indexers or indexers_kwargs must be provided.
1269         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1270             Method to use for inexact matches:
1271 
1272             * None (default): only exact matches
1273             * pad / ffill: propagate last valid index value forward
1274             * backfill / bfill: propagate next valid index value backward
1275             * nearest: use nearest valid index value
1276         tolerance : optional
1277             Maximum distance between original and new labels for inexact
1278             matches. The values of the index at the matching locations must
1279             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1280         drop : bool, optional
1281             If ``drop=True``, drop coordinates variables in `indexers` instead
1282             of making them scalar.
1283         **indexers_kwargs : {dim: indexer, ...}, optional
1284             The keyword arguments form of ``indexers``.
1285             One of indexers or indexers_kwargs must be provided.
1286 
1287         Returns
1288         -------
1289         obj : DataArray
1290             A new DataArray with the same contents as this DataArray, except the
1291             data and each dimension is indexed by the appropriate indexers.
1292             If indexer DataArrays have coordinates that do not conflict with
1293             this object, then these coordinates will be attached.
1294             In general, each array's data will be a view of the array's data
1295             in this DataArray, unless vectorized indexing was triggered by using
1296             an array indexer, in which case the data will be a copy.
1297 
1298         See Also
1299         --------
1300         Dataset.sel
1301         DataArray.isel
1302 
1303         Examples
1304         --------
1305         >>> da = xr.DataArray(
1306         ...     np.arange(25).reshape(5, 5),
1307         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1308         ...     dims=("x", "y"),
1309         ... )
1310         >>> da
1311         <xarray.DataArray (x: 5, y: 5)>
1312         array([[ 0,  1,  2,  3,  4],
1313                [ 5,  6,  7,  8,  9],
1314                [10, 11, 12, 13, 14],
1315                [15, 16, 17, 18, 19],
1316                [20, 21, 22, 23, 24]])
1317         Coordinates:
1318           * x        (x) int64 0 1 2 3 4
1319           * y        (y) int64 0 1 2 3 4
1320 
1321         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1322         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1323         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1324         >>> da
1325         <xarray.DataArray (points: 5)>
1326         array([ 0,  6, 12, 18, 24])
1327         Coordinates:
1328             x        (points) int64 0 1 2 3 4
1329             y        (points) int64 0 1 2 3 4
1330         Dimensions without coordinates: points
1331         """
1332         ds = self._to_temp_dataset().sel(
1333             indexers=indexers,
1334             drop=drop,
1335             method=method,
1336             tolerance=tolerance,
1337             **indexers_kwargs,
1338         )
1339         return self._from_temp_dataset(ds)
1340 
1341     def head(
1342         self,
1343         indexers: Union[Mapping[Any, int], int] = None,
1344         **indexers_kwargs: Any,
1345     ) -> "DataArray":
1346         """Return a new DataArray whose data is given by the the first `n`
1347         values along the specified dimension(s). Default `n` = 5
1348 
1349         See Also
1350         --------
1351         Dataset.head
1352         DataArray.tail
1353         DataArray.thin
1354         """
1355         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1356         return self._from_temp_dataset(ds)
1357 
1358     def tail(
1359         self,
1360         indexers: Union[Mapping[Any, int], int] = None,
1361         **indexers_kwargs: Any,
1362     ) -> "DataArray":
1363         """Return a new DataArray whose data is given by the the last `n`
1364         values along the specified dimension(s). Default `n` = 5
1365 
1366         See Also
1367         --------
1368         Dataset.tail
1369         DataArray.head
1370         DataArray.thin
1371         """
1372         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1373         return self._from_temp_dataset(ds)
1374 
1375     def thin(
1376         self,
1377         indexers: Union[Mapping[Any, int], int] = None,
1378         **indexers_kwargs: Any,
1379     ) -> "DataArray":
1380         """Return a new DataArray whose data is given by each `n` value
1381         along the specified dimension(s).
1382 
1383         See Also
1384         --------
1385         Dataset.thin
1386         DataArray.head
1387         DataArray.tail
1388         """
1389         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1390         return self._from_temp_dataset(ds)
1391 
1392     def broadcast_like(
1393         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1394     ) -> "DataArray":
1395         """Broadcast this DataArray against another Dataset or DataArray.
1396 
1397         This is equivalent to xr.broadcast(other, self)[1]
1398 
1399         xarray objects are broadcast against each other in arithmetic
1400         operations, so this method is not be necessary for most uses.
1401 
1402         If no change is needed, the input data is returned to the output
1403         without being copied.
1404 
1405         If new coords are added by the broadcast, their values are
1406         NaN filled.
1407 
1408         Parameters
1409         ----------
1410         other : Dataset or DataArray
1411             Object against which to broadcast this array.
1412         exclude : iterable of hashable, optional
1413             Dimensions that must not be broadcasted
1414 
1415         Returns
1416         -------
1417         new_da : DataArray
1418             The caller broadcasted against ``other``.
1419 
1420         Examples
1421         --------
1422         >>> arr1 = xr.DataArray(
1423         ...     np.random.randn(2, 3),
1424         ...     dims=("x", "y"),
1425         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1426         ... )
1427         >>> arr2 = xr.DataArray(
1428         ...     np.random.randn(3, 2),
1429         ...     dims=("x", "y"),
1430         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1431         ... )
1432         >>> arr1
1433         <xarray.DataArray (x: 2, y: 3)>
1434         array([[ 1.76405235,  0.40015721,  0.97873798],
1435                [ 2.2408932 ,  1.86755799, -0.97727788]])
1436         Coordinates:
1437           * x        (x) <U1 'a' 'b'
1438           * y        (y) <U1 'a' 'b' 'c'
1439         >>> arr2
1440         <xarray.DataArray (x: 3, y: 2)>
1441         array([[ 0.95008842, -0.15135721],
1442                [-0.10321885,  0.4105985 ],
1443                [ 0.14404357,  1.45427351]])
1444         Coordinates:
1445           * x        (x) <U1 'a' 'b' 'c'
1446           * y        (y) <U1 'a' 'b'
1447         >>> arr1.broadcast_like(arr2)
1448         <xarray.DataArray (x: 3, y: 3)>
1449         array([[ 1.76405235,  0.40015721,  0.97873798],
1450                [ 2.2408932 ,  1.86755799, -0.97727788],
1451                [        nan,         nan,         nan]])
1452         Coordinates:
1453           * x        (x) <U1 'a' 'b' 'c'
1454           * y        (y) <U1 'a' 'b' 'c'
1455         """
1456         if exclude is None:
1457             exclude = set()
1458         else:
1459             exclude = set(exclude)
1460         args = align(other, self, join="outer", copy=False, exclude=exclude)
1461 
1462         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1463 
1464         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1465 
1466     def reindex_like(
1467         self,
1468         other: Union["DataArray", Dataset],
1469         method: str = None,
1470         tolerance=None,
1471         copy: bool = True,
1472         fill_value=dtypes.NA,
1473     ) -> "DataArray":
1474         """Conform this object onto the indexes of another object, filling in
1475         missing values with ``fill_value``. The default fill value is NaN.
1476 
1477         Parameters
1478         ----------
1479         other : Dataset or DataArray
1480             Object with an 'indexes' attribute giving a mapping from dimension
1481             names to pandas.Index objects, which provides coordinates upon
1482             which to index the variables in this dataset. The indexes on this
1483             other object need not be the same as the indexes on this
1484             dataset. Any mis-matched index values will be filled in with
1485             NaN, and any mis-matched dimension names will simply be ignored.
1486         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1487             Method to use for filling index values from other not found on this
1488             data array:
1489 
1490             * None (default): don't fill gaps
1491             * pad / ffill: propagate last valid index value forward
1492             * backfill / bfill: propagate next valid index value backward
1493             * nearest: use nearest valid index value
1494         tolerance : optional
1495             Maximum distance between original and new labels for inexact
1496             matches. The values of the index at the matching locations must
1497             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1498         copy : bool, optional
1499             If ``copy=True``, data in the return value is always copied. If
1500             ``copy=False`` and reindexing is unnecessary, or can be performed
1501             with only slice operations, then the output may share memory with
1502             the input. In either case, a new xarray object is always returned.
1503         fill_value : scalar or dict-like, optional
1504             Value to use for newly missing values. If a dict-like, maps
1505             variable names (including coordinates) to fill values. Use this
1506             data array's name to refer to the data array's values.
1507 
1508         Returns
1509         -------
1510         reindexed : DataArray
1511             Another dataset array, with this array's data but coordinates from
1512             the other object.
1513 
1514         See Also
1515         --------
1516         DataArray.reindex
1517         align
1518         """
1519         indexers = reindex_like_indexers(self, other)
1520         return self.reindex(
1521             indexers=indexers,
1522             method=method,
1523             tolerance=tolerance,
1524             copy=copy,
1525             fill_value=fill_value,
1526         )
1527 
1528     def reindex(
1529         self,
1530         indexers: Mapping[Any, Any] = None,
1531         method: str = None,
1532         tolerance=None,
1533         copy: bool = True,
1534         fill_value=dtypes.NA,
1535         **indexers_kwargs: Any,
1536     ) -> "DataArray":
1537         """Conform this object onto the indexes of another object, filling in
1538         missing values with ``fill_value``. The default fill value is NaN.
1539 
1540         Parameters
1541         ----------
1542         indexers : dict, optional
1543             Dictionary with keys given by dimension names and values given by
1544             arrays of coordinates tick labels. Any mis-matched coordinate
1545             values will be filled in with NaN, and any mis-matched dimension
1546             names will simply be ignored.
1547             One of indexers or indexers_kwargs must be provided.
1548         copy : bool, optional
1549             If ``copy=True``, data in the return value is always copied. If
1550             ``copy=False`` and reindexing is unnecessary, or can be performed
1551             with only slice operations, then the output may share memory with
1552             the input. In either case, a new xarray object is always returned.
1553         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1554             Method to use for filling index values in ``indexers`` not found on
1555             this data array:
1556 
1557             * None (default): don't fill gaps
1558             * pad / ffill: propagate last valid index value forward
1559             * backfill / bfill: propagate next valid index value backward
1560             * nearest: use nearest valid index value
1561         tolerance : optional
1562             Maximum distance between original and new labels for inexact
1563             matches. The values of the index at the matching locations must
1564             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1565         fill_value : scalar or dict-like, optional
1566             Value to use for newly missing values. If a dict-like, maps
1567             variable names (including coordinates) to fill values. Use this
1568             data array's name to refer to the data array's values.
1569         **indexers_kwargs : {dim: indexer, ...}, optional
1570             The keyword arguments form of ``indexers``.
1571             One of indexers or indexers_kwargs must be provided.
1572 
1573         Returns
1574         -------
1575         reindexed : DataArray
1576             Another dataset array, with this array's data but replaced
1577             coordinates.
1578 
1579         Examples
1580         --------
1581         Reverse latitude:
1582 
1583         >>> da = xr.DataArray(
1584         ...     np.arange(4),
1585         ...     coords=[np.array([90, 89, 88, 87])],
1586         ...     dims="lat",
1587         ... )
1588         >>> da
1589         <xarray.DataArray (lat: 4)>
1590         array([0, 1, 2, 3])
1591         Coordinates:
1592           * lat      (lat) int64 90 89 88 87
1593         >>> da.reindex(lat=da.lat[::-1])
1594         <xarray.DataArray (lat: 4)>
1595         array([3, 2, 1, 0])
1596         Coordinates:
1597           * lat      (lat) int64 87 88 89 90
1598 
1599         See Also
1600         --------
1601         DataArray.reindex_like
1602         align
1603         """
1604         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1605         if isinstance(fill_value, dict):
1606             fill_value = fill_value.copy()
1607             sentinel = object()
1608             value = fill_value.pop(self.name, sentinel)
1609             if value is not sentinel:
1610                 fill_value[_THIS_ARRAY] = value
1611 
1612         ds = self._to_temp_dataset().reindex(
1613             indexers=indexers,
1614             method=method,
1615             tolerance=tolerance,
1616             copy=copy,
1617             fill_value=fill_value,
1618         )
1619         return self._from_temp_dataset(ds)
1620 
1621     def interp(
1622         self,
1623         coords: Mapping[Any, Any] = None,
1624         method: str = "linear",
1625         assume_sorted: bool = False,
1626         kwargs: Mapping[str, Any] = None,
1627         **coords_kwargs: Any,
1628     ) -> "DataArray":
1629         """Multidimensional interpolation of variables.
1630 
1631         Parameters
1632         ----------
1633         coords : dict, optional
1634             Mapping from dimension names to the new coordinates.
1635             New coordinate can be an scalar, array-like or DataArray.
1636             If DataArrays are passed as new coordinates, their dimensions are
1637             used for the broadcasting. Missing values are skipped.
1638         method : str, default: "linear"
1639             The method used to interpolate. Choose from
1640 
1641             - {"linear", "nearest"} for multidimensional array,
1642             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1643         assume_sorted : bool, optional
1644             If False, values of x can be in any order and they are sorted
1645             first. If True, x has to be an array of monotonically increasing
1646             values.
1647         kwargs : dict
1648             Additional keyword arguments passed to scipy's interpolator. Valid
1649             options and their behavior depend on if 1-dimensional or
1650             multi-dimensional interpolation is used.
1651         **coords_kwargs : {dim: coordinate, ...}, optional
1652             The keyword arguments form of ``coords``.
1653             One of coords or coords_kwargs must be provided.
1654 
1655         Returns
1656         -------
1657         interpolated : DataArray
1658             New dataarray on the new coordinates.
1659 
1660         Notes
1661         -----
1662         scipy is required.
1663 
1664         See Also
1665         --------
1666         scipy.interpolate.interp1d
1667         scipy.interpolate.interpn
1668 
1669         Examples
1670         --------
1671         >>> da = xr.DataArray(
1672         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1673         ...     dims=("x", "y"),
1674         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1675         ... )
1676         >>> da
1677         <xarray.DataArray (x: 3, y: 4)>
1678         array([[ 1.,  4.,  2.,  9.],
1679                [ 2.,  7.,  6., nan],
1680                [ 6., nan,  5.,  8.]])
1681         Coordinates:
1682           * x        (x) int64 0 1 2
1683           * y        (y) int64 10 12 14 16
1684 
1685         1D linear interpolation (the default):
1686 
1687         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1688         <xarray.DataArray (x: 4, y: 4)>
1689         array([[1.  , 4.  , 2.  ,  nan],
1690                [1.75, 6.25, 5.  ,  nan],
1691                [3.  ,  nan, 5.75,  nan],
1692                [5.  ,  nan, 5.25,  nan]])
1693         Coordinates:
1694           * y        (y) int64 10 12 14 16
1695           * x        (x) float64 0.0 0.75 1.25 1.75
1696 
1697         1D nearest interpolation:
1698 
1699         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1700         <xarray.DataArray (x: 4, y: 4)>
1701         array([[ 1.,  4.,  2.,  9.],
1702                [ 2.,  7.,  6., nan],
1703                [ 2.,  7.,  6., nan],
1704                [ 6., nan,  5.,  8.]])
1705         Coordinates:
1706           * y        (y) int64 10 12 14 16
1707           * x        (x) float64 0.0 0.75 1.25 1.75
1708 
1709         1D linear extrapolation:
1710 
1711         >>> da.interp(
1712         ...     x=[1, 1.5, 2.5, 3.5],
1713         ...     method="linear",
1714         ...     kwargs={"fill_value": "extrapolate"},
1715         ... )
1716         <xarray.DataArray (x: 4, y: 4)>
1717         array([[ 2. ,  7. ,  6. ,  nan],
1718                [ 4. ,  nan,  5.5,  nan],
1719                [ 8. ,  nan,  4.5,  nan],
1720                [12. ,  nan,  3.5,  nan]])
1721         Coordinates:
1722           * y        (y) int64 10 12 14 16
1723           * x        (x) float64 1.0 1.5 2.5 3.5
1724 
1725         2D linear interpolation:
1726 
1727         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1728         <xarray.DataArray (x: 4, y: 3)>
1729         array([[2.5  , 3.   ,   nan],
1730                [4.   , 5.625,   nan],
1731                [  nan,   nan,   nan],
1732                [  nan,   nan,   nan]])
1733         Coordinates:
1734           * x        (x) float64 0.0 0.75 1.25 1.75
1735           * y        (y) int64 11 13 15
1736         """
1737         if self.dtype.kind not in "uifc":
1738             raise TypeError(
1739                 "interp only works for a numeric type array. "
1740                 "Given {}.".format(self.dtype)
1741             )
1742         ds = self._to_temp_dataset().interp(
1743             coords,
1744             method=method,
1745             kwargs=kwargs,
1746             assume_sorted=assume_sorted,
1747             **coords_kwargs,
1748         )
1749         return self._from_temp_dataset(ds)
1750 
1751     def interp_like(
1752         self,
1753         other: Union["DataArray", Dataset],
1754         method: str = "linear",
1755         assume_sorted: bool = False,
1756         kwargs: Mapping[str, Any] = None,
1757     ) -> "DataArray":
1758         """Interpolate this object onto the coordinates of another object,
1759         filling out of range values with NaN.
1760 
1761         Parameters
1762         ----------
1763         other : Dataset or DataArray
1764             Object with an 'indexes' attribute giving a mapping from dimension
1765             names to an 1d array-like, which provides coordinates upon
1766             which to index the variables in this dataset. Missing values are skipped.
1767         method : str, default: "linear"
1768             The method used to interpolate. Choose from
1769 
1770             - {"linear", "nearest"} for multidimensional array,
1771             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1772         assume_sorted : bool, optional
1773             If False, values of coordinates that are interpolated over can be
1774             in any order and they are sorted first. If True, interpolated
1775             coordinates are assumed to be an array of monotonically increasing
1776             values.
1777         kwargs : dict, optional
1778             Additional keyword passed to scipy's interpolator.
1779 
1780         Returns
1781         -------
1782         interpolated : DataArray
1783             Another dataarray by interpolating this dataarray's data along the
1784             coordinates of the other object.
1785 
1786         Notes
1787         -----
1788         scipy is required.
1789         If the dataarray has object-type coordinates, reindex is used for these
1790         coordinates instead of the interpolation.
1791 
1792         See Also
1793         --------
1794         DataArray.interp
1795         DataArray.reindex_like
1796         """
1797         if self.dtype.kind not in "uifc":
1798             raise TypeError(
1799                 "interp only works for a numeric type array. "
1800                 "Given {}.".format(self.dtype)
1801             )
1802         ds = self._to_temp_dataset().interp_like(
1803             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1804         )
1805         return self._from_temp_dataset(ds)
1806 
1807     def rename(
1808         self,
1809         new_name_or_name_dict: Union[Hashable, Mapping[Any, Hashable]] = None,
1810         **names: Hashable,
1811     ) -> "DataArray":
1812         """Returns a new DataArray with renamed coordinates or a new name.
1813 
1814         Parameters
1815         ----------
1816         new_name_or_name_dict : str or dict-like, optional
1817             If the argument is dict-like, it used as a mapping from old
1818             names to new names for coordinates. Otherwise, use the argument
1819             as the new name for this array.
1820         **names : hashable, optional
1821             The keyword arguments form of a mapping from old names to
1822             new names for coordinates.
1823             One of new_name_or_name_dict or names must be provided.
1824 
1825         Returns
1826         -------
1827         renamed : DataArray
1828             Renamed array or array with renamed coordinates.
1829 
1830         See Also
1831         --------
1832         Dataset.rename
1833         DataArray.swap_dims
1834         """
1835         if names or utils.is_dict_like(new_name_or_name_dict):
1836             new_name_or_name_dict = cast(
1837                 Mapping[Hashable, Hashable], new_name_or_name_dict
1838             )
1839             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1840             dataset = self._to_temp_dataset().rename(name_dict)
1841             return self._from_temp_dataset(dataset)
1842         else:
1843             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1844             return self._replace(name=new_name_or_name_dict)
1845 
1846     def swap_dims(
1847         self, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
1848     ) -> "DataArray":
1849         """Returns a new DataArray with swapped dimensions.
1850 
1851         Parameters
1852         ----------
1853         dims_dict : dict-like
1854             Dictionary whose keys are current dimension names and whose values
1855             are new names.
1856         **dims_kwargs : {existing_dim: new_dim, ...}, optional
1857             The keyword arguments form of ``dims_dict``.
1858             One of dims_dict or dims_kwargs must be provided.
1859 
1860         Returns
1861         -------
1862         swapped : DataArray
1863             DataArray with swapped dimensions.
1864 
1865         Examples
1866         --------
1867         >>> arr = xr.DataArray(
1868         ...     data=[0, 1],
1869         ...     dims="x",
1870         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1871         ... )
1872         >>> arr
1873         <xarray.DataArray (x: 2)>
1874         array([0, 1])
1875         Coordinates:
1876           * x        (x) <U1 'a' 'b'
1877             y        (x) int64 0 1
1878 
1879         >>> arr.swap_dims({"x": "y"})
1880         <xarray.DataArray (y: 2)>
1881         array([0, 1])
1882         Coordinates:
1883             x        (y) <U1 'a' 'b'
1884           * y        (y) int64 0 1
1885 
1886         >>> arr.swap_dims({"x": "z"})
1887         <xarray.DataArray (z: 2)>
1888         array([0, 1])
1889         Coordinates:
1890             x        (z) <U1 'a' 'b'
1891             y        (z) int64 0 1
1892         Dimensions without coordinates: z
1893 
1894         See Also
1895         --------
1896         DataArray.rename
1897         Dataset.swap_dims
1898         """
1899         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
1900         ds = self._to_temp_dataset().swap_dims(dims_dict)
1901         return self._from_temp_dataset(ds)
1902 
1903     def expand_dims(
1904         self,
1905         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Any, Any]] = None,
1906         axis=None,
1907         **dim_kwargs: Any,
1908     ) -> "DataArray":
1909         """Return a new object with an additional axis (or axes) inserted at
1910         the corresponding position in the array shape. The new object is a
1911         view into the underlying array, not a copy.
1912 
1913         If dim is already a scalar coordinate, it will be promoted to a 1D
1914         coordinate consisting of a single value.
1915 
1916         Parameters
1917         ----------
1918         dim : hashable, sequence of hashable, dict, or None, optional
1919             Dimensions to include on the new variable.
1920             If provided as str or sequence of str, then dimensions are inserted
1921             with length 1. If provided as a dict, then the keys are the new
1922             dimensions and the values are either integers (giving the length of
1923             the new dimensions) or sequence/ndarray (giving the coordinates of
1924             the new dimensions).
1925         axis : int, list of int or tuple of int, or None, default: None
1926             Axis position(s) where new axis is to be inserted (position(s) on
1927             the result array). If a list (or tuple) of integers is passed,
1928             multiple axes are inserted. In this case, dim arguments should be
1929             same length list. If axis=None is passed, all the axes will be
1930             inserted to the start of the result array.
1931         **dim_kwargs : int or sequence or ndarray
1932             The keywords are arbitrary dimensions being inserted and the values
1933             are either the lengths of the new dims (if int is given), or their
1934             coordinates. Note, this is an alternative to passing a dict to the
1935             dim kwarg and will only be used if dim is None.
1936 
1937         Returns
1938         -------
1939         expanded : same type as caller
1940             This object, but with an additional dimension(s).
1941         """
1942         if isinstance(dim, int):
1943             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1944         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1945             if len(dim) != len(set(dim)):
1946                 raise ValueError("dims should not contain duplicate values.")
1947             dim = dict.fromkeys(dim, 1)
1948         elif dim is not None and not isinstance(dim, Mapping):
1949             dim = {cast(Hashable, dim): 1}
1950 
1951         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1952         ds = self._to_temp_dataset().expand_dims(dim, axis)
1953         return self._from_temp_dataset(ds)
1954 
1955     def set_index(
1956         self,
1957         indexes: Mapping[Any, Union[Hashable, Sequence[Hashable]]] = None,
1958         append: bool = False,
1959         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1960     ) -> "DataArray":
1961         """Set DataArray (multi-)indexes using one or more existing
1962         coordinates.
1963 
1964         Parameters
1965         ----------
1966         indexes : {dim: index, ...}
1967             Mapping from names matching dimensions and values given
1968             by (lists of) the names of existing coordinates or variables to set
1969             as new (multi-)index.
1970         append : bool, optional
1971             If True, append the supplied index(es) to the existing index(es).
1972             Otherwise replace the existing index(es) (default).
1973         **indexes_kwargs : optional
1974             The keyword arguments form of ``indexes``.
1975             One of indexes or indexes_kwargs must be provided.
1976 
1977         Returns
1978         -------
1979         obj : DataArray
1980             Another DataArray, with this data but replaced coordinates.
1981 
1982         Examples
1983         --------
1984         >>> arr = xr.DataArray(
1985         ...     data=np.ones((2, 3)),
1986         ...     dims=["x", "y"],
1987         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1988         ... )
1989         >>> arr
1990         <xarray.DataArray (x: 2, y: 3)>
1991         array([[1., 1., 1.],
1992                [1., 1., 1.]])
1993         Coordinates:
1994           * x        (x) int64 0 1
1995           * y        (y) int64 0 1 2
1996             a        (x) int64 3 4
1997         >>> arr.set_index(x="a")
1998         <xarray.DataArray (x: 2, y: 3)>
1999         array([[1., 1., 1.],
2000                [1., 1., 1.]])
2001         Coordinates:
2002           * x        (x) int64 3 4
2003           * y        (y) int64 0 1 2
2004 
2005         See Also
2006         --------
2007         DataArray.reset_index
2008         """
2009         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2010         return self._from_temp_dataset(ds)
2011 
2012     def reset_index(
2013         self,
2014         dims_or_levels: Union[Hashable, Sequence[Hashable]],
2015         drop: bool = False,
2016     ) -> "DataArray":
2017         """Reset the specified index(es) or multi-index level(s).
2018 
2019         Parameters
2020         ----------
2021         dims_or_levels : hashable or sequence of hashable
2022             Name(s) of the dimension(s) and/or multi-index level(s) that will
2023             be reset.
2024         drop : bool, optional
2025             If True, remove the specified indexes and/or multi-index levels
2026             instead of extracting them as new coordinates (default: False).
2027 
2028         Returns
2029         -------
2030         obj : DataArray
2031             Another dataarray, with this dataarray's data but replaced
2032             coordinates.
2033 
2034         See Also
2035         --------
2036         DataArray.set_index
2037         """
2038         coords, _ = split_indexes(
2039             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
2040         )
2041         return self._replace(coords=coords)
2042 
2043     def reorder_levels(
2044         self,
2045         dim_order: Mapping[Any, Sequence[int]] = None,
2046         **dim_order_kwargs: Sequence[int],
2047     ) -> "DataArray":
2048         """Rearrange index levels using input order.
2049 
2050         Parameters
2051         ----------
2052         dim_order : optional
2053             Mapping from names matching dimensions and values given
2054             by lists representing new level orders. Every given dimension
2055             must have a multi-index.
2056         **dim_order_kwargs : optional
2057             The keyword arguments form of ``dim_order``.
2058             One of dim_order or dim_order_kwargs must be provided.
2059 
2060         Returns
2061         -------
2062         obj : DataArray
2063             Another dataarray, with this dataarray's data but replaced
2064             coordinates.
2065         """
2066         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
2067         replace_coords = {}
2068         for dim, order in dim_order.items():
2069             coord = self._coords[dim]
2070             index = coord.to_index()
2071             if not isinstance(index, pd.MultiIndex):
2072                 raise ValueError(f"coordinate {dim!r} has no MultiIndex")
2073             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
2074         coords = self._coords.copy()
2075         coords.update(replace_coords)
2076         return self._replace(coords=coords)
2077 
2078     def stack(
2079         self,
2080         dimensions: Mapping[Any, Sequence[Hashable]] = None,
2081         **dimensions_kwargs: Sequence[Hashable],
2082     ) -> "DataArray":
2083         """
2084         Stack any number of existing dimensions into a single new dimension.
2085 
2086         New dimensions will be added at the end, and the corresponding
2087         coordinate variables will be combined into a MultiIndex.
2088 
2089         Parameters
2090         ----------
2091         dimensions : mapping of hashable to sequence of hashable
2092             Mapping of the form `new_name=(dim1, dim2, ...)`.
2093             Names of new dimensions, and the existing dimensions that they
2094             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2095             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2096             all dimensions.
2097         **dimensions_kwargs
2098             The keyword arguments form of ``dimensions``.
2099             One of dimensions or dimensions_kwargs must be provided.
2100 
2101         Returns
2102         -------
2103         stacked : DataArray
2104             DataArray with stacked data.
2105 
2106         Examples
2107         --------
2108         >>> arr = xr.DataArray(
2109         ...     np.arange(6).reshape(2, 3),
2110         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2111         ... )
2112         >>> arr
2113         <xarray.DataArray (x: 2, y: 3)>
2114         array([[0, 1, 2],
2115                [3, 4, 5]])
2116         Coordinates:
2117           * x        (x) <U1 'a' 'b'
2118           * y        (y) int64 0 1 2
2119         >>> stacked = arr.stack(z=("x", "y"))
2120         >>> stacked.indexes["z"]
2121         MultiIndex([('a', 0),
2122                     ('a', 1),
2123                     ('a', 2),
2124                     ('b', 0),
2125                     ('b', 1),
2126                     ('b', 2)],
2127                    names=['x', 'y'])
2128 
2129         See Also
2130         --------
2131         DataArray.unstack
2132         """
2133         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
2134         return self._from_temp_dataset(ds)
2135 
2136     def unstack(
2137         self,
2138         dim: Union[Hashable, Sequence[Hashable], None] = None,
2139         fill_value: Any = dtypes.NA,
2140         sparse: bool = False,
2141     ) -> "DataArray":
2142         """
2143         Unstack existing dimensions corresponding to MultiIndexes into
2144         multiple new dimensions.
2145 
2146         New dimensions will be added at the end.
2147 
2148         Parameters
2149         ----------
2150         dim : hashable or sequence of hashable, optional
2151             Dimension(s) over which to unstack. By default unstacks all
2152             MultiIndexes.
2153         fill_value : scalar or dict-like, default: nan
2154             value to be filled. If a dict-like, maps variable names to
2155             fill values. Use the data array's name to refer to its
2156             name. If not provided or if the dict-like does not contain
2157             all variables, the dtype's NA value will be used.
2158         sparse : bool, default: False
2159             use sparse-array if True
2160 
2161         Returns
2162         -------
2163         unstacked : DataArray
2164             Array with unstacked data.
2165 
2166         Examples
2167         --------
2168         >>> arr = xr.DataArray(
2169         ...     np.arange(6).reshape(2, 3),
2170         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2171         ... )
2172         >>> arr
2173         <xarray.DataArray (x: 2, y: 3)>
2174         array([[0, 1, 2],
2175                [3, 4, 5]])
2176         Coordinates:
2177           * x        (x) <U1 'a' 'b'
2178           * y        (y) int64 0 1 2
2179         >>> stacked = arr.stack(z=("x", "y"))
2180         >>> stacked.indexes["z"]
2181         MultiIndex([('a', 0),
2182                     ('a', 1),
2183                     ('a', 2),
2184                     ('b', 0),
2185                     ('b', 1),
2186                     ('b', 2)],
2187                    names=['x', 'y'])
2188         >>> roundtripped = stacked.unstack()
2189         >>> arr.identical(roundtripped)
2190         True
2191 
2192         See Also
2193         --------
2194         DataArray.stack
2195         """
2196         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2197         return self._from_temp_dataset(ds)
2198 
2199     def to_unstacked_dataset(self, dim, level=0):
2200         """Unstack DataArray expanding to Dataset along a given level of a
2201         stacked coordinate.
2202 
2203         This is the inverse operation of Dataset.to_stacked_array.
2204 
2205         Parameters
2206         ----------
2207         dim : str
2208             Name of existing dimension to unstack
2209         level : int or str
2210             The MultiIndex level to expand to a dataset along. Can either be
2211             the integer index of the level or its name.
2212 
2213         Returns
2214         -------
2215         unstacked: Dataset
2216 
2217         Examples
2218         --------
2219         >>> arr = xr.DataArray(
2220         ...     np.arange(6).reshape(2, 3),
2221         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2222         ... )
2223         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2224         >>> data
2225         <xarray.Dataset>
2226         Dimensions:  (x: 2, y: 3)
2227         Coordinates:
2228           * x        (x) <U1 'a' 'b'
2229           * y        (y) int64 0 1 2
2230         Data variables:
2231             a        (x, y) int64 0 1 2 3 4 5
2232             b        (x) int64 0 3
2233         >>> stacked = data.to_stacked_array("z", ["x"])
2234         >>> stacked.indexes["z"]
2235         MultiIndex([('a', 0.0),
2236                     ('a', 1.0),
2237                     ('a', 2.0),
2238                     ('b', nan)],
2239                    names=['variable', 'y'])
2240         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2241         >>> data.identical(roundtripped)
2242         True
2243 
2244         See Also
2245         --------
2246         Dataset.to_stacked_array
2247         """
2248 
2249         # TODO: benbovy - flexible indexes: update when MultIndex has its own
2250         # class inheriting from xarray.Index
2251         idx = self.xindexes[dim].to_pandas_index()
2252         if not isinstance(idx, pd.MultiIndex):
2253             raise ValueError(f"'{dim}' is not a stacked coordinate")
2254 
2255         level_number = idx._get_level_number(level)
2256         variables = idx.levels[level_number]
2257         variable_dim = idx.names[level_number]
2258 
2259         # pull variables out of datarray
2260         data_dict = {}
2261         for k in variables:
2262             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2263 
2264         # unstacked dataset
2265         return Dataset(data_dict)
2266 
2267     def transpose(
2268         self,
2269         *dims: Hashable,
2270         transpose_coords: bool = True,
2271         missing_dims: str = "raise",
2272     ) -> "DataArray":
2273         """Return a new DataArray object with transposed dimensions.
2274 
2275         Parameters
2276         ----------
2277         *dims : hashable, optional
2278             By default, reverse the dimensions. Otherwise, reorder the
2279             dimensions to this order.
2280         transpose_coords : bool, default: True
2281             If True, also transpose the coordinates of this DataArray.
2282         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2283             What to do if dimensions that should be selected from are not present in the
2284             DataArray:
2285             - "raise": raise an exception
2286             - "warn": raise a warning, and ignore the missing dimensions
2287             - "ignore": ignore the missing dimensions
2288 
2289         Returns
2290         -------
2291         transposed : DataArray
2292             The returned DataArray's array is transposed.
2293 
2294         Notes
2295         -----
2296         This operation returns a view of this array's data. It is
2297         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2298         -- the data will be fully loaded.
2299 
2300         See Also
2301         --------
2302         numpy.transpose
2303         Dataset.transpose
2304         """
2305         if dims:
2306             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2307         variable = self.variable.transpose(*dims)
2308         if transpose_coords:
2309             coords: Dict[Hashable, Variable] = {}
2310             for name, coord in self.coords.items():
2311                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2312                 coords[name] = coord.variable.transpose(*coord_dims)
2313             return self._replace(variable, coords)
2314         else:
2315             return self._replace(variable)
2316 
2317     @property
2318     def T(self) -> "DataArray":
2319         return self.transpose()
2320 
2321     def drop_vars(
2322         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2323     ) -> "DataArray":
2324         """Returns an array with dropped variables.
2325 
2326         Parameters
2327         ----------
2328         names : hashable or iterable of hashable
2329             Name(s) of variables to drop.
2330         errors : {"raise", "ignore"}, optional
2331             If 'raise' (default), raises a ValueError error if any of the variable
2332             passed are not in the dataset. If 'ignore', any given names that are in the
2333             DataArray are dropped and no error is raised.
2334 
2335         Returns
2336         -------
2337         dropped : Dataset
2338             New Dataset copied from `self` with variables removed.
2339         """
2340         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2341         return self._from_temp_dataset(ds)
2342 
2343     def drop(
2344         self,
2345         labels: Mapping = None,
2346         dim: Hashable = None,
2347         *,
2348         errors: str = "raise",
2349         **labels_kwargs,
2350     ) -> "DataArray":
2351         """Backward compatible method based on `drop_vars` and `drop_sel`
2352 
2353         Using either `drop_vars` or `drop_sel` is encouraged
2354 
2355         See Also
2356         --------
2357         DataArray.drop_vars
2358         DataArray.drop_sel
2359         """
2360         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2361         return self._from_temp_dataset(ds)
2362 
2363     def drop_sel(
2364         self,
2365         labels: Mapping[Any, Any] = None,
2366         *,
2367         errors: str = "raise",
2368         **labels_kwargs,
2369     ) -> "DataArray":
2370         """Drop index labels from this DataArray.
2371 
2372         Parameters
2373         ----------
2374         labels : mapping of hashable to Any
2375             Index labels to drop
2376         errors : {"raise", "ignore"}, optional
2377             If 'raise' (default), raises a ValueError error if
2378             any of the index labels passed are not
2379             in the dataset. If 'ignore', any given labels that are in the
2380             dataset are dropped and no error is raised.
2381         **labels_kwargs : {dim: label, ...}, optional
2382             The keyword arguments form of ``dim`` and ``labels``
2383 
2384         Returns
2385         -------
2386         dropped : DataArray
2387         """
2388         if labels_kwargs or isinstance(labels, dict):
2389             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2390 
2391         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2392         return self._from_temp_dataset(ds)
2393 
2394     def drop_isel(self, indexers=None, **indexers_kwargs):
2395         """Drop index positions from this DataArray.
2396 
2397         Parameters
2398         ----------
2399         indexers : mapping of hashable to Any
2400             Index locations to drop
2401         **indexers_kwargs : {dim: position, ...}, optional
2402             The keyword arguments form of ``dim`` and ``positions``
2403 
2404         Returns
2405         -------
2406         dropped : DataArray
2407 
2408         Raises
2409         ------
2410         IndexError
2411         """
2412         dataset = self._to_temp_dataset()
2413         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2414         return self._from_temp_dataset(dataset)
2415 
2416     def dropna(
2417         self, dim: Hashable, how: str = "any", thresh: int = None
2418     ) -> "DataArray":
2419         """Returns a new array with dropped labels for missing values along
2420         the provided dimension.
2421 
2422         Parameters
2423         ----------
2424         dim : hashable
2425             Dimension along which to drop missing values. Dropping along
2426             multiple dimensions simultaneously is not yet supported.
2427         how : {"any", "all"}, optional
2428             * any : if any NA values are present, drop that label
2429             * all : if all values are NA, drop that label
2430         thresh : int, default: None
2431             If supplied, require this many non-NA values.
2432 
2433         Returns
2434         -------
2435         DataArray
2436         """
2437         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2438         return self._from_temp_dataset(ds)
2439 
2440     def fillna(self, value: Any) -> "DataArray":
2441         """Fill missing values in this object.
2442 
2443         This operation follows the normal broadcasting and alignment rules that
2444         xarray uses for binary arithmetic, except the result is aligned to this
2445         object (``join='left'``) instead of aligned to the intersection of
2446         index coordinates (``join='inner'``).
2447 
2448         Parameters
2449         ----------
2450         value : scalar, ndarray or DataArray
2451             Used to fill all matching missing values in this array. If the
2452             argument is a DataArray, it is first aligned with (reindexed to)
2453             this array.
2454 
2455         Returns
2456         -------
2457         DataArray
2458         """
2459         if utils.is_dict_like(value):
2460             raise TypeError(
2461                 "cannot provide fill value as a dictionary with "
2462                 "fillna on a DataArray"
2463             )
2464         out = ops.fillna(self, value)
2465         return out
2466 
2467     def interpolate_na(
2468         self,
2469         dim: Hashable = None,
2470         method: str = "linear",
2471         limit: int = None,
2472         use_coordinate: Union[bool, str] = True,
2473         max_gap: Union[
2474             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2475         ] = None,
2476         keep_attrs: bool = None,
2477         **kwargs: Any,
2478     ) -> "DataArray":
2479         """Fill in NaNs by interpolating according to different methods.
2480 
2481         Parameters
2482         ----------
2483         dim : str
2484             Specifies the dimension along which to interpolate.
2485         method : str, optional
2486             String indicating which method to use for interpolation:
2487 
2488             - 'linear': linear interpolation (Default). Additional keyword
2489               arguments are passed to :py:func:`numpy.interp`
2490             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2491               are passed to :py:func:`scipy.interpolate.interp1d`. If
2492               ``method='polynomial'``, the ``order`` keyword argument must also be
2493               provided.
2494             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2495               respective :py:class:`scipy.interpolate` classes.
2496         use_coordinate : bool or str, default: True
2497             Specifies which index to use as the x values in the interpolation
2498             formulated as `y = f(x)`. If False, values are treated as if
2499             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2500             used. If ``use_coordinate`` is a string, it specifies the name of a
2501             coordinate variariable to use as the index.
2502         limit : int, default: None
2503             Maximum number of consecutive NaNs to fill. Must be greater than 0
2504             or None for no limit. This filling is done regardless of the size of
2505             the gap in the data. To only interpolate over gaps less than a given length,
2506             see ``max_gap``.
2507         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2508             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2509             Use None for no limit. When interpolating along a datetime64 dimension
2510             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2511 
2512             - a string that is valid input for pandas.to_timedelta
2513             - a :py:class:`numpy.timedelta64` object
2514             - a :py:class:`pandas.Timedelta` object
2515             - a :py:class:`datetime.timedelta` object
2516 
2517             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2518             dimensions has not been implemented yet. Gap length is defined as the difference
2519             between coordinate values at the first data point after a gap and the last value
2520             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2521             between coordinate values at the first (last) valid data point and the first (last) NaN.
2522             For example, consider::
2523 
2524                 <xarray.DataArray (x: 9)>
2525                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2526                 Coordinates:
2527                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2528 
2529             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2530         keep_attrs : bool, default: True
2531             If True, the dataarray's attributes (`attrs`) will be copied from
2532             the original object to the new one.  If False, the new
2533             object will be returned without attributes.
2534         **kwargs : dict, optional
2535             parameters passed verbatim to the underlying interpolation function
2536 
2537         Returns
2538         -------
2539         interpolated: DataArray
2540             Filled in DataArray.
2541 
2542         See Also
2543         --------
2544         numpy.interp
2545         scipy.interpolate
2546 
2547         Examples
2548         --------
2549         >>> da = xr.DataArray(
2550         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2551         ... )
2552         >>> da
2553         <xarray.DataArray (x: 5)>
2554         array([nan,  2.,  3., nan,  0.])
2555         Coordinates:
2556           * x        (x) int64 0 1 2 3 4
2557 
2558         >>> da.interpolate_na(dim="x", method="linear")
2559         <xarray.DataArray (x: 5)>
2560         array([nan, 2. , 3. , 1.5, 0. ])
2561         Coordinates:
2562           * x        (x) int64 0 1 2 3 4
2563 
2564         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2565         <xarray.DataArray (x: 5)>
2566         array([1. , 2. , 3. , 1.5, 0. ])
2567         Coordinates:
2568           * x        (x) int64 0 1 2 3 4
2569         """
2570         from .missing import interp_na
2571 
2572         return interp_na(
2573             self,
2574             dim=dim,
2575             method=method,
2576             limit=limit,
2577             use_coordinate=use_coordinate,
2578             max_gap=max_gap,
2579             keep_attrs=keep_attrs,
2580             **kwargs,
2581         )
2582 
2583     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2584         """Fill NaN values by propogating values forward
2585 
2586         *Requires bottleneck.*
2587 
2588         Parameters
2589         ----------
2590         dim : hashable
2591             Specifies the dimension along which to propagate values when
2592             filling.
2593         limit : int, default: None
2594             The maximum number of consecutive NaN values to forward fill. In
2595             other words, if there is a gap with more than this number of
2596             consecutive NaNs, it will only be partially filled. Must be greater
2597             than 0 or None for no limit. Must be None or greater than or equal
2598             to axis length if filling along chunked axes (dimensions).
2599 
2600         Returns
2601         -------
2602         DataArray
2603         """
2604         from .missing import ffill
2605 
2606         return ffill(self, dim, limit=limit)
2607 
2608     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2609         """Fill NaN values by propogating values backward
2610 
2611         *Requires bottleneck.*
2612 
2613         Parameters
2614         ----------
2615         dim : str
2616             Specifies the dimension along which to propagate values when
2617             filling.
2618         limit : int, default: None
2619             The maximum number of consecutive NaN values to backward fill. In
2620             other words, if there is a gap with more than this number of
2621             consecutive NaNs, it will only be partially filled. Must be greater
2622             than 0 or None for no limit. Must be None or greater than or equal
2623             to axis length if filling along chunked axes (dimensions).
2624 
2625         Returns
2626         -------
2627         DataArray
2628         """
2629         from .missing import bfill
2630 
2631         return bfill(self, dim, limit=limit)
2632 
2633     def combine_first(self, other: "DataArray") -> "DataArray":
2634         """Combine two DataArray objects, with union of coordinates.
2635 
2636         This operation follows the normal broadcasting and alignment rules of
2637         ``join='outer'``.  Default to non-null values of array calling the
2638         method.  Use np.nan to fill in vacant cells after alignment.
2639 
2640         Parameters
2641         ----------
2642         other : DataArray
2643             Used to fill all matching missing values in this array.
2644 
2645         Returns
2646         -------
2647         DataArray
2648         """
2649         return ops.fillna(self, other, join="outer")
2650 
2651     def reduce(
2652         self,
2653         func: Callable[..., Any],
2654         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2655         axis: Union[None, int, Sequence[int]] = None,
2656         keep_attrs: bool = None,
2657         keepdims: bool = False,
2658         **kwargs: Any,
2659     ) -> "DataArray":
2660         """Reduce this array by applying `func` along some dimension(s).
2661 
2662         Parameters
2663         ----------
2664         func : callable
2665             Function which can be called in the form
2666             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2667             np.ndarray over an integer valued axis.
2668         dim : hashable or sequence of hashable, optional
2669             Dimension(s) over which to apply `func`.
2670         axis : int or sequence of int, optional
2671             Axis(es) over which to repeatedly apply `func`. Only one of the
2672             'dim' and 'axis' arguments can be supplied. If neither are
2673             supplied, then the reduction is calculated over the flattened array
2674             (by calling `f(x)` without an axis argument).
2675         keep_attrs : bool, optional
2676             If True, the variable's attributes (`attrs`) will be copied from
2677             the original object to the new one.  If False (default), the new
2678             object will be returned without attributes.
2679         keepdims : bool, default: False
2680             If True, the dimensions which are reduced are left in the result
2681             as dimensions of size one. Coordinates that use these dimensions
2682             are removed.
2683         **kwargs : dict
2684             Additional keyword arguments passed on to `func`.
2685 
2686         Returns
2687         -------
2688         reduced : DataArray
2689             DataArray with this object's array replaced with an array with
2690             summarized data and the indicated dimension(s) removed.
2691         """
2692 
2693         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2694         return self._replace_maybe_drop_dims(var)
2695 
2696     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2697         """Convert this array into a pandas object with the same shape.
2698 
2699         The type of the returned object depends on the number of DataArray
2700         dimensions:
2701 
2702         * 0D -> `xarray.DataArray`
2703         * 1D -> `pandas.Series`
2704         * 2D -> `pandas.DataFrame`
2705 
2706         Only works for arrays with 2 or fewer dimensions.
2707 
2708         The DataArray constructor performs the inverse transformation.
2709         """
2710         # TODO: consolidate the info about pandas constructors and the
2711         # attributes that correspond to their indexes into a separate module?
2712         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2713         try:
2714             constructor = constructors[self.ndim]
2715         except KeyError:
2716             raise ValueError(
2717                 f"cannot convert arrays with {self.ndim} dimensions into "
2718                 "pandas objects"
2719             )
2720         indexes = [self.get_index(dim) for dim in self.dims]
2721         return constructor(self.values, *indexes)
2722 
2723     def to_dataframe(
2724         self, name: Hashable = None, dim_order: List[Hashable] = None
2725     ) -> pd.DataFrame:
2726         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2727 
2728         The DataFrame is indexed by the Cartesian product of index coordinates
2729         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
2730         included as columns in the DataFrame.
2731 
2732         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
2733         doesn't rely on a MultiIndex to build the DataFrame.
2734 
2735         Parameters
2736         ----------
2737         name
2738             Name to give to this array (required if unnamed).
2739         dim_order
2740             Hierarchical dimension order for the resulting dataframe.
2741             Array content is transposed to this order and then written out as flat
2742             vectors in contiguous order, so the last dimension in this list
2743             will be contiguous in the resulting DataFrame. This has a major
2744             influence on which operations are efficient on the resulting
2745             dataframe.
2746 
2747             If provided, must include all dimensions of this DataArray. By default,
2748             dimensions are sorted according to the DataArray dimensions order.
2749 
2750         Returns
2751         -------
2752         result
2753             DataArray as a pandas DataFrame.
2754 
2755         See also
2756         --------
2757         DataArray.to_pandas
2758         """
2759         if name is None:
2760             name = self.name
2761         if name is None:
2762             raise ValueError(
2763                 "cannot convert an unnamed DataArray to a "
2764                 "DataFrame: use the ``name`` parameter"
2765             )
2766         if self.ndim == 0:
2767             raise ValueError("cannot convert a scalar to a DataFrame")
2768 
2769         # By using a unique name, we can convert a DataArray into a DataFrame
2770         # even if it shares a name with one of its coordinates.
2771         # I would normally use unique_name = object() but that results in a
2772         # dataframe with columns in the wrong order, for reasons I have not
2773         # been able to debug (possibly a pandas bug?).
2774         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2775         ds = self._to_dataset_whole(name=unique_name)
2776 
2777         if dim_order is None:
2778             ordered_dims = dict(zip(self.dims, self.shape))
2779         else:
2780             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2781 
2782         df = ds._to_dataframe(ordered_dims)
2783         df.columns = [name if c == unique_name else c for c in df.columns]
2784         return df
2785 
2786     def to_series(self) -> pd.Series:
2787         """Convert this array into a pandas.Series.
2788 
2789         The Series is indexed by the Cartesian product of index coordinates
2790         (in the form of a :py:class:`pandas.MultiIndex`).
2791         """
2792         index = self.coords.to_index()
2793         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2794 
2795     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2796         """Convert this array into a numpy.ma.MaskedArray
2797 
2798         Parameters
2799         ----------
2800         copy : bool, default: True
2801             If True make a copy of the array in the result. If False,
2802             a MaskedArray view of DataArray.values is returned.
2803 
2804         Returns
2805         -------
2806         result : MaskedArray
2807             Masked where invalid values (nan or inf) occur.
2808         """
2809         values = self.to_numpy()  # only compute lazy arrays once
2810         isnull = pd.isnull(values)
2811         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2812 
2813     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2814         """Write DataArray contents to a netCDF file.
2815 
2816         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2817 
2818         Notes
2819         -----
2820         Only xarray.Dataset objects can be written to netCDF files, so
2821         the xarray.DataArray is converted to a xarray.Dataset object
2822         containing a single variable. If the DataArray has no name, or if the
2823         name is the same as a coordinate name, then it is given the name
2824         ``"__xarray_dataarray_variable__"``.
2825 
2826         See Also
2827         --------
2828         Dataset.to_netcdf
2829         """
2830         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2831 
2832         if self.name is None:
2833             # If no name is set then use a generic xarray name
2834             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2835         elif self.name in self.coords or self.name in self.dims:
2836             # The name is the same as one of the coords names, which netCDF
2837             # doesn't support, so rename it but keep track of the old name
2838             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2839             dataset.attrs[DATAARRAY_NAME] = self.name
2840         else:
2841             # No problems with the name - so we're fine!
2842             dataset = self.to_dataset()
2843 
2844         return dataset.to_netcdf(*args, **kwargs)
2845 
2846     def to_dict(self, data: bool = True) -> dict:
2847         """
2848         Convert this xarray.DataArray into a dictionary following xarray
2849         naming conventions.
2850 
2851         Converts all variables and attributes to native Python objects.
2852         Useful for converting to json. To avoid datetime incompatibility
2853         use decode_times=False kwarg in xarray.open_dataset.
2854 
2855         Parameters
2856         ----------
2857         data : bool, optional
2858             Whether to include the actual data in the dictionary. When set to
2859             False, returns just the schema.
2860 
2861         See Also
2862         --------
2863         DataArray.from_dict
2864         """
2865         d = self.variable.to_dict(data=data)
2866         d.update({"coords": {}, "name": self.name})
2867         for k in self.coords:
2868             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2869         return d
2870 
2871     @classmethod
2872     def from_dict(cls, d: dict) -> "DataArray":
2873         """
2874         Convert a dictionary into an xarray.DataArray
2875 
2876         Input dict can take several forms:
2877 
2878         .. code:: python
2879 
2880             d = {"dims": ("t"), "data": x}
2881 
2882             d = {
2883                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2884                 "attrs": {"title": "air temperature"},
2885                 "dims": "t",
2886                 "data": x,
2887                 "name": "a",
2888             }
2889 
2890         where "t" is the name of the dimension, "a" is the name of the array,
2891         and x and t are lists, numpy.arrays, or pandas objects.
2892 
2893         Parameters
2894         ----------
2895         d : dict
2896             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2897 
2898         Returns
2899         -------
2900         obj : xarray.DataArray
2901 
2902         See Also
2903         --------
2904         DataArray.to_dict
2905         Dataset.from_dict
2906         """
2907         coords = None
2908         if "coords" in d:
2909             try:
2910                 coords = {
2911                     k: (v["dims"], v["data"], v.get("attrs"))
2912                     for k, v in d["coords"].items()
2913                 }
2914             except KeyError as e:
2915                 raise ValueError(
2916                     "cannot convert dict when coords are missing the key "
2917                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2918                 )
2919         try:
2920             data = d["data"]
2921         except KeyError:
2922             raise ValueError("cannot convert dict without the key 'data''")
2923         else:
2924             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2925         return obj
2926 
2927     @classmethod
2928     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2929         """Convert a pandas.Series into an xarray.DataArray.
2930 
2931         If the series's index is a MultiIndex, it will be expanded into a
2932         tensor product of one-dimensional coordinates (filling in missing
2933         values with NaN). Thus this operation should be the inverse of the
2934         `to_series` method.
2935 
2936         If sparse=True, creates a sparse array instead of a dense NumPy array.
2937         Requires the pydata/sparse package.
2938 
2939         See Also
2940         --------
2941         xarray.Dataset.from_dataframe
2942         """
2943         temp_name = "__temporary_name"
2944         df = pd.DataFrame({temp_name: series})
2945         ds = Dataset.from_dataframe(df, sparse=sparse)
2946         result = cast(DataArray, ds[temp_name])
2947         result.name = series.name
2948         return result
2949 
2950     def to_cdms2(self) -> "cdms2_Variable":
2951         """Convert this array into a cdms2.Variable"""
2952         from ..convert import to_cdms2
2953 
2954         return to_cdms2(self)
2955 
2956     @classmethod
2957     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2958         """Convert a cdms2.Variable into an xarray.DataArray"""
2959         from ..convert import from_cdms2
2960 
2961         return from_cdms2(variable)
2962 
2963     def to_iris(self) -> "iris_Cube":
2964         """Convert this array into a iris.cube.Cube"""
2965         from ..convert import to_iris
2966 
2967         return to_iris(self)
2968 
2969     @classmethod
2970     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2971         """Convert a iris.cube.Cube into an xarray.DataArray"""
2972         from ..convert import from_iris
2973 
2974         return from_iris(cube)
2975 
2976     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2977         """Helper function for equals, broadcast_equals, and identical"""
2978 
2979         def compat(x, y):
2980             return getattr(x.variable, compat_str)(y.variable)
2981 
2982         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2983             self, other
2984         )
2985 
2986     def broadcast_equals(self, other: "DataArray") -> bool:
2987         """Two DataArrays are broadcast equal if they are equal after
2988         broadcasting them against each other such that they have the same
2989         dimensions.
2990 
2991         See Also
2992         --------
2993         DataArray.equals
2994         DataArray.identical
2995         """
2996         try:
2997             return self._all_compat(other, "broadcast_equals")
2998         except (TypeError, AttributeError):
2999             return False
3000 
3001     def equals(self, other: "DataArray") -> bool:
3002         """True if two DataArrays have the same dimensions, coordinates and
3003         values; otherwise False.
3004 
3005         DataArrays can still be equal (like pandas objects) if they have NaN
3006         values in the same locations.
3007 
3008         This method is necessary because `v1 == v2` for ``DataArray``
3009         does element-wise comparisons (like numpy.ndarrays).
3010 
3011         See Also
3012         --------
3013         DataArray.broadcast_equals
3014         DataArray.identical
3015         """
3016         try:
3017             return self._all_compat(other, "equals")
3018         except (TypeError, AttributeError):
3019             return False
3020 
3021     def identical(self, other: "DataArray") -> bool:
3022         """Like equals, but also checks the array name and attributes, and
3023         attributes on all coordinates.
3024 
3025         See Also
3026         --------
3027         DataArray.broadcast_equals
3028         DataArray.equals
3029         """
3030         try:
3031             return self.name == other.name and self._all_compat(other, "identical")
3032         except (TypeError, AttributeError):
3033             return False
3034 
3035     def _result_name(self, other: Any = None) -> Optional[Hashable]:
3036         # use the same naming heuristics as pandas:
3037         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
3038         other_name = getattr(other, "name", _default)
3039         if other_name is _default or other_name == self.name:
3040             return self.name
3041         else:
3042             return None
3043 
3044     def __array_wrap__(self, obj, context=None) -> "DataArray":
3045         new_var = self.variable.__array_wrap__(obj, context)
3046         return self._replace(new_var)
3047 
3048     def __matmul__(self, obj):
3049         return self.dot(obj)
3050 
3051     def __rmatmul__(self, other):
3052         # currently somewhat duplicative, as only other DataArrays are
3053         # compatible with matmul
3054         return computation.dot(other, self)
3055 
3056     def _unary_op(self, f: Callable, *args, **kwargs):
3057         keep_attrs = kwargs.pop("keep_attrs", None)
3058         if keep_attrs is None:
3059             keep_attrs = _get_keep_attrs(default=True)
3060         with warnings.catch_warnings():
3061             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
3062             warnings.filterwarnings(
3063                 "ignore", r"Mean of empty slice", category=RuntimeWarning
3064             )
3065             with np.errstate(all="ignore"):
3066                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
3067             if keep_attrs:
3068                 da.attrs = self.attrs
3069             return da
3070 
3071     def _binary_op(
3072         self,
3073         other,
3074         f: Callable,
3075         reflexive: bool = False,
3076     ):
3077         if isinstance(other, (Dataset, groupby.GroupBy)):
3078             return NotImplemented
3079         if isinstance(other, DataArray):
3080             align_type = OPTIONS["arithmetic_join"]
3081             self, other = align(self, other, join=align_type, copy=False)
3082         other_variable = getattr(other, "variable", other)
3083         other_coords = getattr(other, "coords", None)
3084 
3085         variable = (
3086             f(self.variable, other_variable)
3087             if not reflexive
3088             else f(other_variable, self.variable)
3089         )
3090         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
3091         name = self._result_name(other)
3092 
3093         return self._replace(variable, coords, name, indexes=indexes)
3094 
3095     def _inplace_binary_op(self, other, f: Callable):
3096         if isinstance(other, groupby.GroupBy):
3097             raise TypeError(
3098                 "in-place operations between a DataArray and "
3099                 "a grouped object are not permitted"
3100             )
3101         # n.b. we can't align other to self (with other.reindex_like(self))
3102         # because `other` may be converted into floats, which would cause
3103         # in-place arithmetic to fail unpredictably. Instead, we simply
3104         # don't support automatic alignment with in-place arithmetic.
3105         other_coords = getattr(other, "coords", None)
3106         other_variable = getattr(other, "variable", other)
3107         try:
3108             with self.coords._merge_inplace(other_coords):
3109                 f(self.variable, other_variable)
3110         except MergeError as exc:
3111             raise MergeError(
3112                 "Automatic alignment is not supported for in-place operations.\n"
3113                 "Consider aligning the indices manually or using a not-in-place operation.\n"
3114                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3115             ) from exc
3116         return self
3117 
3118     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
3119         self.attrs = other.attrs
3120 
3121     plot = utils.UncachedAccessor(_PlotMethods)
3122 
3123     def _title_for_slice(self, truncate: int = 50) -> str:
3124         """
3125         If the dataarray has 1 dimensional coordinates or comes from a slice
3126         we can show that info in the title
3127 
3128         Parameters
3129         ----------
3130         truncate : int, default: 50
3131             maximum number of characters for title
3132 
3133         Returns
3134         -------
3135         title : string
3136             Can be used for plot titles
3137 
3138         """
3139         one_dims = []
3140         for dim, coord in self.coords.items():
3141             if coord.size == 1:
3142                 one_dims.append(
3143                     "{dim} = {v}{unit}".format(
3144                         dim=dim,
3145                         v=format_item(coord.values),
3146                         unit=_get_units_from_attrs(coord),
3147                     )
3148                 )
3149 
3150         title = ", ".join(one_dims)
3151         if len(title) > truncate:
3152             title = title[: (truncate - 3)] + "..."
3153 
3154         return title
3155 
3156     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
3157         """Calculate the n-th order discrete difference along given axis.
3158 
3159         Parameters
3160         ----------
3161         dim : hashable
3162             Dimension over which to calculate the finite difference.
3163         n : int, optional
3164             The number of times values are differenced.
3165         label : hashable, optional
3166             The new coordinate in dimension ``dim`` will have the
3167             values of either the minuend's or subtrahend's coordinate
3168             for values 'upper' and 'lower', respectively.  Other
3169             values are not supported.
3170 
3171         Returns
3172         -------
3173         difference : same type as caller
3174             The n-th order finite difference of this object.
3175 
3176         Notes
3177         -----
3178         `n` matches numpy's behavior and is different from pandas' first argument named
3179         `periods`.
3180 
3181         Examples
3182         --------
3183         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3184         >>> arr.diff("x")
3185         <xarray.DataArray (x: 3)>
3186         array([0, 1, 0])
3187         Coordinates:
3188           * x        (x) int64 2 3 4
3189         >>> arr.diff("x", 2)
3190         <xarray.DataArray (x: 2)>
3191         array([ 1, -1])
3192         Coordinates:
3193           * x        (x) int64 3 4
3194 
3195         See Also
3196         --------
3197         DataArray.differentiate
3198         """
3199         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3200         return self._from_temp_dataset(ds)
3201 
3202     def shift(
3203         self,
3204         shifts: Mapping[Any, int] = None,
3205         fill_value: Any = dtypes.NA,
3206         **shifts_kwargs: int,
3207     ) -> "DataArray":
3208         """Shift this DataArray by an offset along one or more dimensions.
3209 
3210         Only the data is moved; coordinates stay in place. This is consistent
3211         with the behavior of ``shift`` in pandas.
3212 
3213         Values shifted from beyond array bounds will appear at one end of
3214         each dimension, which are filled according to `fill_value`. For periodic
3215         offsets instead see `roll`.
3216 
3217         Parameters
3218         ----------
3219         shifts : mapping of hashable to int, optional
3220             Integer offset to shift along each of the given dimensions.
3221             Positive offsets shift to the right; negative offsets shift to the
3222             left.
3223         fill_value : scalar, optional
3224             Value to use for newly missing values
3225         **shifts_kwargs
3226             The keyword arguments form of ``shifts``.
3227             One of shifts or shifts_kwargs must be provided.
3228 
3229         Returns
3230         -------
3231         shifted : DataArray
3232             DataArray with the same coordinates and attributes but shifted
3233             data.
3234 
3235         See Also
3236         --------
3237         roll
3238 
3239         Examples
3240         --------
3241         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3242         >>> arr.shift(x=1)
3243         <xarray.DataArray (x: 3)>
3244         array([nan,  5.,  6.])
3245         Dimensions without coordinates: x
3246         """
3247         variable = self.variable.shift(
3248             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3249         )
3250         return self._replace(variable=variable)
3251 
3252     def roll(
3253         self,
3254         shifts: Mapping[Hashable, int] = None,
3255         roll_coords: bool = False,
3256         **shifts_kwargs: int,
3257     ) -> "DataArray":
3258         """Roll this array by an offset along one or more dimensions.
3259 
3260         Unlike shift, roll treats the given dimensions as periodic, so will not
3261         create any missing values to be filled.
3262 
3263         Unlike shift, roll may rotate all variables, including coordinates
3264         if specified. The direction of rotation is consistent with
3265         :py:func:`numpy.roll`.
3266 
3267         Parameters
3268         ----------
3269         shifts : mapping of hashable to int, optional
3270             Integer offset to rotate each of the given dimensions.
3271             Positive offsets roll to the right; negative offsets roll to the
3272             left.
3273         roll_coords : bool, default: False
3274             Indicates whether to roll the coordinates by the offset too.
3275         **shifts_kwargs : {dim: offset, ...}, optional
3276             The keyword arguments form of ``shifts``.
3277             One of shifts or shifts_kwargs must be provided.
3278 
3279         Returns
3280         -------
3281         rolled : DataArray
3282             DataArray with the same attributes but rolled data and coordinates.
3283 
3284         See Also
3285         --------
3286         shift
3287 
3288         Examples
3289         --------
3290         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3291         >>> arr.roll(x=1)
3292         <xarray.DataArray (x: 3)>
3293         array([7, 5, 6])
3294         Dimensions without coordinates: x
3295         """
3296         ds = self._to_temp_dataset().roll(
3297             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3298         )
3299         return self._from_temp_dataset(ds)
3300 
3301     @property
3302     def real(self) -> "DataArray":
3303         return self._replace(self.variable.real)
3304 
3305     @property
3306     def imag(self) -> "DataArray":
3307         return self._replace(self.variable.imag)
3308 
3309     def dot(
3310         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3311     ) -> "DataArray":
3312         """Perform dot product of two DataArrays along their shared dims.
3313 
3314         Equivalent to taking taking tensordot over all shared dims.
3315 
3316         Parameters
3317         ----------
3318         other : DataArray
3319             The other array with which the dot product is performed.
3320         dims : ..., hashable or sequence of hashable, optional
3321             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3322             If not specified, then all the common dimensions are summed over.
3323 
3324         Returns
3325         -------
3326         result : DataArray
3327             Array resulting from the dot product over all shared dimensions.
3328 
3329         See Also
3330         --------
3331         dot
3332         numpy.tensordot
3333 
3334         Examples
3335         --------
3336         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3337         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3338         >>> dm_vals = np.arange(4)
3339         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3340 
3341         >>> dm.dims
3342         ('z',)
3343 
3344         >>> da.dims
3345         ('x', 'y', 'z')
3346 
3347         >>> dot_result = da.dot(dm)
3348         >>> dot_result.dims
3349         ('x', 'y')
3350 
3351         """
3352         if isinstance(other, Dataset):
3353             raise NotImplementedError(
3354                 "dot products are not yet supported with Dataset objects."
3355             )
3356         if not isinstance(other, DataArray):
3357             raise TypeError("dot only operates on DataArrays.")
3358 
3359         return computation.dot(self, other, dims=dims)
3360 
3361     def sortby(
3362         self,
3363         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3364         ascending: bool = True,
3365     ) -> "DataArray":
3366         """Sort object by labels or values (along an axis).
3367 
3368         Sorts the dataarray, either along specified dimensions,
3369         or according to values of 1-D dataarrays that share dimension
3370         with calling object.
3371 
3372         If the input variables are dataarrays, then the dataarrays are aligned
3373         (via left-join) to the calling object prior to sorting by cell values.
3374         NaNs are sorted to the end, following Numpy convention.
3375 
3376         If multiple sorts along the same dimension is
3377         given, numpy's lexsort is performed along that dimension:
3378         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3379         and the FIRST key in the sequence is used as the primary sort key,
3380         followed by the 2nd key, etc.
3381 
3382         Parameters
3383         ----------
3384         variables : hashable, DataArray, or sequence of hashable or DataArray
3385             1D DataArray objects or name(s) of 1D variable(s) in
3386             coords whose values are used to sort this array.
3387         ascending : bool, optional
3388             Whether to sort by ascending or descending order.
3389 
3390         Returns
3391         -------
3392         sorted : DataArray
3393             A new dataarray where all the specified dims are sorted by dim
3394             labels.
3395 
3396         See Also
3397         --------
3398         Dataset.sortby
3399         numpy.sort
3400         pandas.sort_values
3401         pandas.sort_index
3402 
3403         Examples
3404         --------
3405         >>> da = xr.DataArray(
3406         ...     np.random.rand(5),
3407         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3408         ...     dims="time",
3409         ... )
3410         >>> da
3411         <xarray.DataArray (time: 5)>
3412         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3413         Coordinates:
3414           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3415 
3416         >>> da.sortby(da)
3417         <xarray.DataArray (time: 5)>
3418         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3419         Coordinates:
3420           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3421         """
3422         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3423         return self._from_temp_dataset(ds)
3424 
3425     def quantile(
3426         self,
3427         q: Any,
3428         dim: Union[Hashable, Sequence[Hashable], None] = None,
3429         interpolation: str = "linear",
3430         keep_attrs: bool = None,
3431         skipna: bool = True,
3432     ) -> "DataArray":
3433         """Compute the qth quantile of the data along the specified dimension.
3434 
3435         Returns the qth quantiles(s) of the array elements.
3436 
3437         Parameters
3438         ----------
3439         q : float or array-like of float
3440             Quantile to compute, which must be between 0 and 1 inclusive.
3441         dim : hashable or sequence of hashable, optional
3442             Dimension(s) over which to apply quantile.
3443         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3444             This optional parameter specifies the interpolation method to
3445             use when the desired quantile lies between two data points
3446             ``i < j``:
3447 
3448                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3449                   the fractional part of the index surrounded by ``i`` and
3450                   ``j``.
3451                 - lower: ``i``.
3452                 - higher: ``j``.
3453                 - nearest: ``i`` or ``j``, whichever is nearest.
3454                 - midpoint: ``(i + j) / 2``.
3455         keep_attrs : bool, optional
3456             If True, the dataset's attributes (`attrs`) will be copied from
3457             the original object to the new one.  If False (default), the new
3458             object will be returned without attributes.
3459         skipna : bool, optional
3460             Whether to skip missing values when aggregating.
3461 
3462         Returns
3463         -------
3464         quantiles : DataArray
3465             If `q` is a single quantile, then the result
3466             is a scalar. If multiple percentiles are given, first axis of
3467             the result corresponds to the quantile and a quantile dimension
3468             is added to the return array. The other dimensions are the
3469             dimensions that remain after the reduction of the array.
3470 
3471         See Also
3472         --------
3473         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3474 
3475         Examples
3476         --------
3477         >>> da = xr.DataArray(
3478         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3479         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3480         ...     dims=("x", "y"),
3481         ... )
3482         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3483         <xarray.DataArray ()>
3484         array(0.7)
3485         Coordinates:
3486             quantile  float64 0.0
3487         >>> da.quantile(0, dim="x")
3488         <xarray.DataArray (y: 4)>
3489         array([0.7, 4.2, 2.6, 1.5])
3490         Coordinates:
3491           * y         (y) float64 1.0 1.5 2.0 2.5
3492             quantile  float64 0.0
3493         >>> da.quantile([0, 0.5, 1])
3494         <xarray.DataArray (quantile: 3)>
3495         array([0.7, 3.4, 9.4])
3496         Coordinates:
3497           * quantile  (quantile) float64 0.0 0.5 1.0
3498         >>> da.quantile([0, 0.5, 1], dim="x")
3499         <xarray.DataArray (quantile: 3, y: 4)>
3500         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3501                [3.6 , 5.75, 6.  , 1.7 ],
3502                [6.5 , 7.3 , 9.4 , 1.9 ]])
3503         Coordinates:
3504           * y         (y) float64 1.0 1.5 2.0 2.5
3505           * quantile  (quantile) float64 0.0 0.5 1.0
3506         """
3507 
3508         ds = self._to_temp_dataset().quantile(
3509             q,
3510             dim=dim,
3511             keep_attrs=keep_attrs,
3512             interpolation=interpolation,
3513             skipna=skipna,
3514         )
3515         return self._from_temp_dataset(ds)
3516 
3517     def rank(
3518         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3519     ) -> "DataArray":
3520         """Ranks the data.
3521 
3522         Equal values are assigned a rank that is the average of the ranks that
3523         would have been otherwise assigned to all of the values within that
3524         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3525 
3526         NaNs in the input array are returned as NaNs.
3527 
3528         The `bottleneck` library is required.
3529 
3530         Parameters
3531         ----------
3532         dim : hashable
3533             Dimension over which to compute rank.
3534         pct : bool, optional
3535             If True, compute percentage ranks, otherwise compute integer ranks.
3536         keep_attrs : bool, optional
3537             If True, the dataset's attributes (`attrs`) will be copied from
3538             the original object to the new one.  If False (default), the new
3539             object will be returned without attributes.
3540 
3541         Returns
3542         -------
3543         ranked : DataArray
3544             DataArray with the same coordinates and dtype 'float64'.
3545 
3546         Examples
3547         --------
3548         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3549         >>> arr.rank("x")
3550         <xarray.DataArray (x: 3)>
3551         array([1., 2., 3.])
3552         Dimensions without coordinates: x
3553         """
3554 
3555         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3556         return self._from_temp_dataset(ds)
3557 
3558     def differentiate(
3559         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3560     ) -> "DataArray":
3561         """ Differentiate the array with the second order accurate central
3562         differences.
3563 
3564         .. note::
3565             This feature is limited to simple cartesian geometry, i.e. coord
3566             must be one dimensional.
3567 
3568         Parameters
3569         ----------
3570         coord : hashable
3571             The coordinate to be used to compute the gradient.
3572         edge_order : {1, 2}, default: 1
3573             N-th order accurate differences at the boundaries.
3574         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3575                          "us", "ns", "ps", "fs", "as"} or None, optional
3576             Unit to compute gradient. Only valid for datetime coordinate.
3577 
3578         Returns
3579         -------
3580         differentiated: DataArray
3581 
3582         See also
3583         --------
3584         numpy.gradient: corresponding numpy function
3585 
3586         Examples
3587         --------
3588 
3589         >>> da = xr.DataArray(
3590         ...     np.arange(12).reshape(4, 3),
3591         ...     dims=["x", "y"],
3592         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3593         ... )
3594         >>> da
3595         <xarray.DataArray (x: 4, y: 3)>
3596         array([[ 0,  1,  2],
3597                [ 3,  4,  5],
3598                [ 6,  7,  8],
3599                [ 9, 10, 11]])
3600         Coordinates:
3601           * x        (x) float64 0.0 0.1 1.1 1.2
3602         Dimensions without coordinates: y
3603         >>>
3604         >>> da.differentiate("x")
3605         <xarray.DataArray (x: 4, y: 3)>
3606         array([[30.        , 30.        , 30.        ],
3607                [27.54545455, 27.54545455, 27.54545455],
3608                [27.54545455, 27.54545455, 27.54545455],
3609                [30.        , 30.        , 30.        ]])
3610         Coordinates:
3611           * x        (x) float64 0.0 0.1 1.1 1.2
3612         Dimensions without coordinates: y
3613         """
3614         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3615         return self._from_temp_dataset(ds)
3616 
3617     def integrate(
3618         self,
3619         coord: Union[Hashable, Sequence[Hashable]] = None,
3620         datetime_unit: str = None,
3621     ) -> "DataArray":
3622         """Integrate along the given coordinate using the trapezoidal rule.
3623 
3624         .. note::
3625             This feature is limited to simple cartesian geometry, i.e. coord
3626             must be one dimensional.
3627 
3628         Parameters
3629         ----------
3630         coord : hashable, or sequence of hashable
3631             Coordinate(s) used for the integration.
3632         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3633                         'ps', 'fs', 'as'}, optional
3634             Specify the unit if a datetime coordinate is used.
3635 
3636         Returns
3637         -------
3638         integrated : DataArray
3639 
3640         See also
3641         --------
3642         Dataset.integrate
3643         numpy.trapz : corresponding numpy function
3644 
3645         Examples
3646         --------
3647 
3648         >>> da = xr.DataArray(
3649         ...     np.arange(12).reshape(4, 3),
3650         ...     dims=["x", "y"],
3651         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3652         ... )
3653         >>> da
3654         <xarray.DataArray (x: 4, y: 3)>
3655         array([[ 0,  1,  2],
3656                [ 3,  4,  5],
3657                [ 6,  7,  8],
3658                [ 9, 10, 11]])
3659         Coordinates:
3660           * x        (x) float64 0.0 0.1 1.1 1.2
3661         Dimensions without coordinates: y
3662         >>>
3663         >>> da.integrate("x")
3664         <xarray.DataArray (y: 3)>
3665         array([5.4, 6.6, 7.8])
3666         Dimensions without coordinates: y
3667         """
3668         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
3669         return self._from_temp_dataset(ds)
3670 
3671     def cumulative_integrate(
3672         self,
3673         coord: Union[Hashable, Sequence[Hashable]] = None,
3674         datetime_unit: str = None,
3675     ) -> "DataArray":
3676         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
3677 
3678         .. note::
3679             This feature is limited to simple cartesian geometry, i.e. coord
3680             must be one dimensional.
3681 
3682             The first entry of the cumulative integral is always 0, in order to keep the
3683             length of the dimension unchanged between input and output.
3684 
3685         Parameters
3686         ----------
3687         coord : hashable, or sequence of hashable
3688             Coordinate(s) used for the integration.
3689         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3690                         'ps', 'fs', 'as'}, optional
3691             Specify the unit if a datetime coordinate is used.
3692 
3693         Returns
3694         -------
3695         integrated : DataArray
3696 
3697         See also
3698         --------
3699         Dataset.cumulative_integrate
3700         scipy.integrate.cumulative_trapezoid : corresponding scipy function
3701 
3702         Examples
3703         --------
3704 
3705         >>> da = xr.DataArray(
3706         ...     np.arange(12).reshape(4, 3),
3707         ...     dims=["x", "y"],
3708         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3709         ... )
3710         >>> da
3711         <xarray.DataArray (x: 4, y: 3)>
3712         array([[ 0,  1,  2],
3713                [ 3,  4,  5],
3714                [ 6,  7,  8],
3715                [ 9, 10, 11]])
3716         Coordinates:
3717           * x        (x) float64 0.0 0.1 1.1 1.2
3718         Dimensions without coordinates: y
3719         >>>
3720         >>> da.cumulative_integrate("x")
3721         <xarray.DataArray (x: 4, y: 3)>
3722         array([[0.  , 0.  , 0.  ],
3723                [0.15, 0.25, 0.35],
3724                [4.65, 5.75, 6.85],
3725                [5.4 , 6.6 , 7.8 ]])
3726         Coordinates:
3727           * x        (x) float64 0.0 0.1 1.1 1.2
3728         Dimensions without coordinates: y
3729         """
3730         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
3731         return self._from_temp_dataset(ds)
3732 
3733     def unify_chunks(self) -> "DataArray":
3734         """Unify chunk size along all chunked dimensions of this DataArray.
3735 
3736         Returns
3737         -------
3738         DataArray with consistent chunk sizes for all dask-array variables
3739 
3740         See Also
3741         --------
3742         dask.array.core.unify_chunks
3743         """
3744 
3745         return unify_chunks(self)[0]
3746 
3747     def map_blocks(
3748         self,
3749         func: Callable[..., T_Xarray],
3750         args: Sequence[Any] = (),
3751         kwargs: Mapping[str, Any] = None,
3752         template: Union["DataArray", "Dataset"] = None,
3753     ) -> T_Xarray:
3754         """
3755         Apply a function to each block of this DataArray.
3756 
3757         .. warning::
3758             This method is experimental and its signature may change.
3759 
3760         Parameters
3761         ----------
3762         func : callable
3763             User-provided function that accepts a DataArray as its first
3764             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3765             corresponding to one chunk along each chunked dimension. ``func`` will be
3766             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3767 
3768             This function must return either a single DataArray or a single Dataset.
3769 
3770             This function cannot add a new chunked dimension.
3771         args : sequence
3772             Passed to func after unpacking and subsetting any xarray objects by blocks.
3773             xarray objects in args must be aligned with this object, otherwise an error is raised.
3774         kwargs : mapping
3775             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3776             subset to blocks. Passing dask collections in kwargs is not allowed.
3777         template : DataArray or Dataset, optional
3778             xarray object representing the final result after compute is called. If not provided,
3779             the function will be first run on mocked-up data, that looks like this object but
3780             has sizes 0, to determine properties of the returned object such as dtype,
3781             variable names, attributes, new dimensions and new indexes (if any).
3782             ``template`` must be provided if the function changes the size of existing dimensions.
3783             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3784             ``attrs`` set by ``func`` will be ignored.
3785 
3786         Returns
3787         -------
3788         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3789         function.
3790 
3791         Notes
3792         -----
3793         This function is designed for when ``func`` needs to manipulate a whole xarray object
3794         subset to each block. Each block is loaded into memory. In the more common case where
3795         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
3796 
3797         If none of the variables in this object is backed by dask arrays, calling this function is
3798         equivalent to calling ``func(obj, *args, **kwargs)``.
3799 
3800         See Also
3801         --------
3802         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
3803         xarray.DataArray.map_blocks
3804 
3805         Examples
3806         --------
3807         Calculate an anomaly from climatology using ``.groupby()``. Using
3808         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3809         its indices, and its methods like ``.groupby()``.
3810 
3811         >>> def calculate_anomaly(da, groupby_type="time.month"):
3812         ...     gb = da.groupby(groupby_type)
3813         ...     clim = gb.mean(dim="time")
3814         ...     return gb - clim
3815         ...
3816         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3817         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3818         >>> np.random.seed(123)
3819         >>> array = xr.DataArray(
3820         ...     np.random.rand(len(time)),
3821         ...     dims=["time"],
3822         ...     coords={"time": time, "month": month},
3823         ... ).chunk()
3824         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3825         <xarray.DataArray (time: 24)>
3826         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3827                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3828                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3829                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3830                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3831         Coordinates:
3832           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3833             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3834 
3835         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3836         to the function being applied in ``xr.map_blocks()``:
3837 
3838         >>> array.map_blocks(
3839         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3840         ... )  # doctest: +ELLIPSIS
3841         <xarray.DataArray (time: 24)>
3842         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3843         Coordinates:
3844           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3845             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3846         """
3847         from .parallel import map_blocks
3848 
3849         return map_blocks(func, self, args, kwargs, template)
3850 
3851     def polyfit(
3852         self,
3853         dim: Hashable,
3854         deg: int,
3855         skipna: bool = None,
3856         rcond: float = None,
3857         w: Union[Hashable, Any] = None,
3858         full: bool = False,
3859         cov: bool = False,
3860     ):
3861         """
3862         Least squares polynomial fit.
3863 
3864         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3865         invalid values when `skipna = True`.
3866 
3867         Parameters
3868         ----------
3869         dim : hashable
3870             Coordinate along which to fit the polynomials.
3871         deg : int
3872             Degree of the fitting polynomial.
3873         skipna : bool, optional
3874             If True, removes all invalid values before fitting each 1D slices of the array.
3875             Default is True if data is stored in a dask.array or if there is any
3876             invalid values, False otherwise.
3877         rcond : float, optional
3878             Relative condition number to the fit.
3879         w : hashable or array-like, optional
3880             Weights to apply to the y-coordinate of the sample points.
3881             Can be an array-like object or the name of a coordinate in the dataset.
3882         full : bool, optional
3883             Whether to return the residuals, matrix rank and singular values in addition
3884             to the coefficients.
3885         cov : bool or str, optional
3886             Whether to return to the covariance matrix in addition to the coefficients.
3887             The matrix is not scaled if `cov='unscaled'`.
3888 
3889         Returns
3890         -------
3891         polyfit_results : Dataset
3892             A single dataset which contains:
3893 
3894             polyfit_coefficients
3895                 The coefficients of the best fit.
3896             polyfit_residuals
3897                 The residuals of the least-square computation (only included if `full=True`).
3898                 When the matrix rank is deficient, np.nan is returned.
3899             [dim]_matrix_rank
3900                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3901             [dim]_singular_value
3902                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3903             polyfit_covariance
3904                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3905 
3906         See Also
3907         --------
3908         numpy.polyfit
3909         numpy.polyval
3910         xarray.polyval
3911         """
3912         return self._to_temp_dataset().polyfit(
3913             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3914         )
3915 
3916     def pad(
3917         self,
3918         pad_width: Mapping[Any, Union[int, Tuple[int, int]]] = None,
3919         mode: str = "constant",
3920         stat_length: Union[int, Tuple[int, int], Mapping[Any, Tuple[int, int]]] = None,
3921         constant_values: Union[
3922             int, Tuple[int, int], Mapping[Any, Tuple[int, int]]
3923         ] = None,
3924         end_values: Union[int, Tuple[int, int], Mapping[Any, Tuple[int, int]]] = None,
3925         reflect_type: str = None,
3926         **pad_width_kwargs: Any,
3927     ) -> "DataArray":
3928         """Pad this array along one or more dimensions.
3929 
3930         .. warning::
3931             This function is experimental and its behaviour is likely to change
3932             especially regarding padding of dimension coordinates (or IndexVariables).
3933 
3934         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3935         coordinates will be padded with the same mode, otherwise coordinates
3936         are padded using the "constant" mode with fill_value dtypes.NA.
3937 
3938         Parameters
3939         ----------
3940         pad_width : mapping of hashable to tuple of int
3941             Mapping with the form of {dim: (pad_before, pad_after)}
3942             describing the number of values padded along each dimension.
3943             {dim: pad} is a shortcut for pad_before = pad_after = pad
3944         mode : str, default: "constant"
3945             One of the following string values (taken from numpy docs)
3946 
3947             'constant' (default)
3948                 Pads with a constant value.
3949             'edge'
3950                 Pads with the edge values of array.
3951             'linear_ramp'
3952                 Pads with the linear ramp between end_value and the
3953                 array edge value.
3954             'maximum'
3955                 Pads with the maximum value of all or part of the
3956                 vector along each axis.
3957             'mean'
3958                 Pads with the mean value of all or part of the
3959                 vector along each axis.
3960             'median'
3961                 Pads with the median value of all or part of the
3962                 vector along each axis.
3963             'minimum'
3964                 Pads with the minimum value of all or part of the
3965                 vector along each axis.
3966             'reflect'
3967                 Pads with the reflection of the vector mirrored on
3968                 the first and last values of the vector along each
3969                 axis.
3970             'symmetric'
3971                 Pads with the reflection of the vector mirrored
3972                 along the edge of the array.
3973             'wrap'
3974                 Pads with the wrap of the vector along the axis.
3975                 The first values are used to pad the end and the
3976                 end values are used to pad the beginning.
3977         stat_length : int, tuple or mapping of hashable to tuple, default: None
3978             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3979             values at edge of each axis used to calculate the statistic value.
3980             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3981             statistic lengths along each dimension.
3982             ((before, after),) yields same before and after statistic lengths
3983             for each dimension.
3984             (stat_length,) or int is a shortcut for before = after = statistic
3985             length for all axes.
3986             Default is ``None``, to use the entire axis.
3987         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3988             Used in 'constant'.  The values to set the padded values for each
3989             axis.
3990             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3991             pad constants along each dimension.
3992             ``((before, after),)`` yields same before and after constants for each
3993             dimension.
3994             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3995             all dimensions.
3996             Default is 0.
3997         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3998             Used in 'linear_ramp'.  The values used for the ending value of the
3999             linear_ramp and that will form the edge of the padded array.
4000             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4001             end values along each dimension.
4002             ``((before, after),)`` yields same before and after end values for each
4003             axis.
4004             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4005             all axes.
4006             Default is 0.
4007         reflect_type : {"even", "odd"}, optional
4008             Used in "reflect", and "symmetric".  The "even" style is the
4009             default with an unaltered reflection around the edge value.  For
4010             the "odd" style, the extended part of the array is created by
4011             subtracting the reflected values from two times the edge value.
4012         **pad_width_kwargs
4013             The keyword arguments form of ``pad_width``.
4014             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
4015 
4016         Returns
4017         -------
4018         padded : DataArray
4019             DataArray with the padded coordinates and data.
4020 
4021         See Also
4022         --------
4023         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
4024 
4025         Notes
4026         -----
4027         For ``mode="constant"`` and ``constant_values=None``, integer types will be
4028         promoted to ``float`` and padded with ``np.nan``.
4029 
4030         Examples
4031         --------
4032         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
4033         >>> arr.pad(x=(1, 2), constant_values=0)
4034         <xarray.DataArray (x: 6)>
4035         array([0, 5, 6, 7, 0, 0])
4036         Coordinates:
4037           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
4038 
4039         >>> da = xr.DataArray(
4040         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
4041         ...     dims=["x", "y"],
4042         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
4043         ... )
4044         >>> da.pad(x=1)
4045         <xarray.DataArray (x: 4, y: 4)>
4046         array([[nan, nan, nan, nan],
4047                [ 0.,  1.,  2.,  3.],
4048                [10., 11., 12., 13.],
4049                [nan, nan, nan, nan]])
4050         Coordinates:
4051           * x        (x) float64 nan 0.0 1.0 nan
4052           * y        (y) int64 10 20 30 40
4053             z        (x) float64 nan 100.0 200.0 nan
4054 
4055         Careful, ``constant_values`` are coerced to the data type of the array which may
4056         lead to a loss of precision:
4057 
4058         >>> da.pad(x=1, constant_values=1.23456789)
4059         <xarray.DataArray (x: 4, y: 4)>
4060         array([[ 1,  1,  1,  1],
4061                [ 0,  1,  2,  3],
4062                [10, 11, 12, 13],
4063                [ 1,  1,  1,  1]])
4064         Coordinates:
4065           * x        (x) float64 nan 0.0 1.0 nan
4066           * y        (y) int64 10 20 30 40
4067             z        (x) float64 nan 100.0 200.0 nan
4068         """
4069         ds = self._to_temp_dataset().pad(
4070             pad_width=pad_width,
4071             mode=mode,
4072             stat_length=stat_length,
4073             constant_values=constant_values,
4074             end_values=end_values,
4075             reflect_type=reflect_type,
4076             **pad_width_kwargs,
4077         )
4078         return self._from_temp_dataset(ds)
4079 
4080     def idxmin(
4081         self,
4082         dim: Hashable = None,
4083         skipna: bool = None,
4084         fill_value: Any = dtypes.NA,
4085         keep_attrs: bool = None,
4086     ) -> "DataArray":
4087         """Return the coordinate label of the minimum value along a dimension.
4088 
4089         Returns a new `DataArray` named after the dimension with the values of
4090         the coordinate labels along that dimension corresponding to minimum
4091         values along that dimension.
4092 
4093         In comparison to :py:meth:`~DataArray.argmin`, this returns the
4094         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
4095 
4096         Parameters
4097         ----------
4098         dim : str, optional
4099             Dimension over which to apply `idxmin`.  This is optional for 1D
4100             arrays, but required for arrays with 2 or more dimensions.
4101         skipna : bool or None, default: None
4102             If True, skip missing values (as marked by NaN). By default, only
4103             skips missing values for ``float``, ``complex``, and ``object``
4104             dtypes; other dtypes either do not have a sentinel missing value
4105             (``int``) or ``skipna=True`` has not been implemented
4106             (``datetime64`` or ``timedelta64``).
4107         fill_value : Any, default: NaN
4108             Value to be filled in case all of the values along a dimension are
4109             null.  By default this is NaN.  The fill value and result are
4110             automatically converted to a compatible dtype if possible.
4111             Ignored if ``skipna`` is False.
4112         keep_attrs : bool, default: False
4113             If True, the attributes (``attrs``) will be copied from the
4114             original object to the new one.  If False (default), the new object
4115             will be returned without attributes.
4116 
4117         Returns
4118         -------
4119         reduced : DataArray
4120             New `DataArray` object with `idxmin` applied to its data and the
4121             indicated dimension removed.
4122 
4123         See Also
4124         --------
4125         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
4126 
4127         Examples
4128         --------
4129         >>> array = xr.DataArray(
4130         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4131         ... )
4132         >>> array.min()
4133         <xarray.DataArray ()>
4134         array(-2)
4135         >>> array.argmin()
4136         <xarray.DataArray ()>
4137         array(4)
4138         >>> array.idxmin()
4139         <xarray.DataArray 'x' ()>
4140         array('e', dtype='<U1')
4141 
4142         >>> array = xr.DataArray(
4143         ...     [
4144         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4145         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4146         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4147         ...     ],
4148         ...     dims=["y", "x"],
4149         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4150         ... )
4151         >>> array.min(dim="x")
4152         <xarray.DataArray (y: 3)>
4153         array([-2., -4.,  1.])
4154         Coordinates:
4155           * y        (y) int64 -1 0 1
4156         >>> array.argmin(dim="x")
4157         <xarray.DataArray (y: 3)>
4158         array([4, 0, 2])
4159         Coordinates:
4160           * y        (y) int64 -1 0 1
4161         >>> array.idxmin(dim="x")
4162         <xarray.DataArray 'x' (y: 3)>
4163         array([16.,  0.,  4.])
4164         Coordinates:
4165           * y        (y) int64 -1 0 1
4166         """
4167         return computation._calc_idxminmax(
4168             array=self,
4169             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4170             dim=dim,
4171             skipna=skipna,
4172             fill_value=fill_value,
4173             keep_attrs=keep_attrs,
4174         )
4175 
4176     def idxmax(
4177         self,
4178         dim: Hashable = None,
4179         skipna: bool = None,
4180         fill_value: Any = dtypes.NA,
4181         keep_attrs: bool = None,
4182     ) -> "DataArray":
4183         """Return the coordinate label of the maximum value along a dimension.
4184 
4185         Returns a new `DataArray` named after the dimension with the values of
4186         the coordinate labels along that dimension corresponding to maximum
4187         values along that dimension.
4188 
4189         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4190         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4191 
4192         Parameters
4193         ----------
4194         dim : hashable, optional
4195             Dimension over which to apply `idxmax`.  This is optional for 1D
4196             arrays, but required for arrays with 2 or more dimensions.
4197         skipna : bool or None, default: None
4198             If True, skip missing values (as marked by NaN). By default, only
4199             skips missing values for ``float``, ``complex``, and ``object``
4200             dtypes; other dtypes either do not have a sentinel missing value
4201             (``int``) or ``skipna=True`` has not been implemented
4202             (``datetime64`` or ``timedelta64``).
4203         fill_value : Any, default: NaN
4204             Value to be filled in case all of the values along a dimension are
4205             null.  By default this is NaN.  The fill value and result are
4206             automatically converted to a compatible dtype if possible.
4207             Ignored if ``skipna`` is False.
4208         keep_attrs : bool, default: False
4209             If True, the attributes (``attrs``) will be copied from the
4210             original object to the new one.  If False (default), the new object
4211             will be returned without attributes.
4212 
4213         Returns
4214         -------
4215         reduced : DataArray
4216             New `DataArray` object with `idxmax` applied to its data and the
4217             indicated dimension removed.
4218 
4219         See Also
4220         --------
4221         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4222 
4223         Examples
4224         --------
4225         >>> array = xr.DataArray(
4226         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4227         ... )
4228         >>> array.max()
4229         <xarray.DataArray ()>
4230         array(2)
4231         >>> array.argmax()
4232         <xarray.DataArray ()>
4233         array(1)
4234         >>> array.idxmax()
4235         <xarray.DataArray 'x' ()>
4236         array('b', dtype='<U1')
4237 
4238         >>> array = xr.DataArray(
4239         ...     [
4240         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4241         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4242         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4243         ...     ],
4244         ...     dims=["y", "x"],
4245         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4246         ... )
4247         >>> array.max(dim="x")
4248         <xarray.DataArray (y: 3)>
4249         array([2., 2., 1.])
4250         Coordinates:
4251           * y        (y) int64 -1 0 1
4252         >>> array.argmax(dim="x")
4253         <xarray.DataArray (y: 3)>
4254         array([0, 2, 2])
4255         Coordinates:
4256           * y        (y) int64 -1 0 1
4257         >>> array.idxmax(dim="x")
4258         <xarray.DataArray 'x' (y: 3)>
4259         array([0., 4., 4.])
4260         Coordinates:
4261           * y        (y) int64 -1 0 1
4262         """
4263         return computation._calc_idxminmax(
4264             array=self,
4265             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4266             dim=dim,
4267             skipna=skipna,
4268             fill_value=fill_value,
4269             keep_attrs=keep_attrs,
4270         )
4271 
4272     def argmin(
4273         self,
4274         dim: Union[Hashable, Sequence[Hashable]] = None,
4275         axis: int = None,
4276         keep_attrs: bool = None,
4277         skipna: bool = None,
4278     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4279         """Index or indices of the minimum of the DataArray over one or more dimensions.
4280 
4281         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4282         which can be passed directly to isel(). If a single str is passed to 'dim' then
4283         returns a DataArray with dtype int.
4284 
4285         If there are multiple minima, the indices of the first one found will be
4286         returned.
4287 
4288         Parameters
4289         ----------
4290         dim : hashable, sequence of hashable or ..., optional
4291             The dimensions over which to find the minimum. By default, finds minimum over
4292             all dimensions - for now returning an int for backward compatibility, but
4293             this is deprecated, in future will return a dict with indices for all
4294             dimensions; to return a dict with all dimensions now, pass '...'.
4295         axis : int, optional
4296             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4297             can be supplied.
4298         keep_attrs : bool, optional
4299             If True, the attributes (`attrs`) will be copied from the original
4300             object to the new one.  If False (default), the new object will be
4301             returned without attributes.
4302         skipna : bool, optional
4303             If True, skip missing values (as marked by NaN). By default, only
4304             skips missing values for float dtypes; other dtypes either do not
4305             have a sentinel missing value (int) or skipna=True has not been
4306             implemented (object, datetime64 or timedelta64).
4307 
4308         Returns
4309         -------
4310         result : DataArray or dict of DataArray
4311 
4312         See Also
4313         --------
4314         Variable.argmin, DataArray.idxmin
4315 
4316         Examples
4317         --------
4318         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4319         >>> array.min()
4320         <xarray.DataArray ()>
4321         array(-1)
4322         >>> array.argmin()
4323         <xarray.DataArray ()>
4324         array(2)
4325         >>> array.argmin(...)
4326         {'x': <xarray.DataArray ()>
4327         array(2)}
4328         >>> array.isel(array.argmin(...))
4329         <xarray.DataArray ()>
4330         array(-1)
4331 
4332         >>> array = xr.DataArray(
4333         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4334         ...     dims=("x", "y", "z"),
4335         ... )
4336         >>> array.min(dim="x")
4337         <xarray.DataArray (y: 3, z: 3)>
4338         array([[ 1,  2,  1],
4339                [ 2, -5,  1],
4340                [ 2,  1,  1]])
4341         Dimensions without coordinates: y, z
4342         >>> array.argmin(dim="x")
4343         <xarray.DataArray (y: 3, z: 3)>
4344         array([[1, 0, 0],
4345                [1, 1, 1],
4346                [0, 0, 1]])
4347         Dimensions without coordinates: y, z
4348         >>> array.argmin(dim=["x"])
4349         {'x': <xarray.DataArray (y: 3, z: 3)>
4350         array([[1, 0, 0],
4351                [1, 1, 1],
4352                [0, 0, 1]])
4353         Dimensions without coordinates: y, z}
4354         >>> array.min(dim=("x", "z"))
4355         <xarray.DataArray (y: 3)>
4356         array([ 1, -5,  1])
4357         Dimensions without coordinates: y
4358         >>> array.argmin(dim=["x", "z"])
4359         {'x': <xarray.DataArray (y: 3)>
4360         array([0, 1, 0])
4361         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4362         array([2, 1, 1])
4363         Dimensions without coordinates: y}
4364         >>> array.isel(array.argmin(dim=["x", "z"]))
4365         <xarray.DataArray (y: 3)>
4366         array([ 1, -5,  1])
4367         Dimensions without coordinates: y
4368         """
4369         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4370         if isinstance(result, dict):
4371             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4372         else:
4373             return self._replace_maybe_drop_dims(result)
4374 
4375     def argmax(
4376         self,
4377         dim: Union[Hashable, Sequence[Hashable]] = None,
4378         axis: int = None,
4379         keep_attrs: bool = None,
4380         skipna: bool = None,
4381     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4382         """Index or indices of the maximum of the DataArray over one or more dimensions.
4383 
4384         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4385         which can be passed directly to isel(). If a single str is passed to 'dim' then
4386         returns a DataArray with dtype int.
4387 
4388         If there are multiple maxima, the indices of the first one found will be
4389         returned.
4390 
4391         Parameters
4392         ----------
4393         dim : hashable, sequence of hashable or ..., optional
4394             The dimensions over which to find the maximum. By default, finds maximum over
4395             all dimensions - for now returning an int for backward compatibility, but
4396             this is deprecated, in future will return a dict with indices for all
4397             dimensions; to return a dict with all dimensions now, pass '...'.
4398         axis : int, optional
4399             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4400             can be supplied.
4401         keep_attrs : bool, optional
4402             If True, the attributes (`attrs`) will be copied from the original
4403             object to the new one.  If False (default), the new object will be
4404             returned without attributes.
4405         skipna : bool, optional
4406             If True, skip missing values (as marked by NaN). By default, only
4407             skips missing values for float dtypes; other dtypes either do not
4408             have a sentinel missing value (int) or skipna=True has not been
4409             implemented (object, datetime64 or timedelta64).
4410 
4411         Returns
4412         -------
4413         result : DataArray or dict of DataArray
4414 
4415         See Also
4416         --------
4417         Variable.argmax, DataArray.idxmax
4418 
4419         Examples
4420         --------
4421         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4422         >>> array.max()
4423         <xarray.DataArray ()>
4424         array(3)
4425         >>> array.argmax()
4426         <xarray.DataArray ()>
4427         array(3)
4428         >>> array.argmax(...)
4429         {'x': <xarray.DataArray ()>
4430         array(3)}
4431         >>> array.isel(array.argmax(...))
4432         <xarray.DataArray ()>
4433         array(3)
4434 
4435         >>> array = xr.DataArray(
4436         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4437         ...     dims=("x", "y", "z"),
4438         ... )
4439         >>> array.max(dim="x")
4440         <xarray.DataArray (y: 3, z: 3)>
4441         array([[3, 3, 2],
4442                [3, 5, 2],
4443                [2, 3, 3]])
4444         Dimensions without coordinates: y, z
4445         >>> array.argmax(dim="x")
4446         <xarray.DataArray (y: 3, z: 3)>
4447         array([[0, 1, 1],
4448                [0, 1, 0],
4449                [0, 1, 0]])
4450         Dimensions without coordinates: y, z
4451         >>> array.argmax(dim=["x"])
4452         {'x': <xarray.DataArray (y: 3, z: 3)>
4453         array([[0, 1, 1],
4454                [0, 1, 0],
4455                [0, 1, 0]])
4456         Dimensions without coordinates: y, z}
4457         >>> array.max(dim=("x", "z"))
4458         <xarray.DataArray (y: 3)>
4459         array([3, 5, 3])
4460         Dimensions without coordinates: y
4461         >>> array.argmax(dim=["x", "z"])
4462         {'x': <xarray.DataArray (y: 3)>
4463         array([0, 1, 0])
4464         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4465         array([0, 1, 2])
4466         Dimensions without coordinates: y}
4467         >>> array.isel(array.argmax(dim=["x", "z"]))
4468         <xarray.DataArray (y: 3)>
4469         array([3, 5, 3])
4470         Dimensions without coordinates: y
4471         """
4472         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4473         if isinstance(result, dict):
4474             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4475         else:
4476             return self._replace_maybe_drop_dims(result)
4477 
4478     def query(
4479         self,
4480         queries: Mapping[Any, Any] = None,
4481         parser: str = "pandas",
4482         engine: str = None,
4483         missing_dims: str = "raise",
4484         **queries_kwargs: Any,
4485     ) -> "DataArray":
4486         """Return a new data array indexed along the specified
4487         dimension(s), where the indexers are given as strings containing
4488         Python expressions to be evaluated against the values in the array.
4489 
4490         Parameters
4491         ----------
4492         queries : dict, optional
4493             A dict with keys matching dimensions and values given by strings
4494             containing Python expressions to be evaluated against the data variables
4495             in the dataset. The expressions will be evaluated using the pandas
4496             eval() function, and can contain any valid Python expressions but cannot
4497             contain any Python statements.
4498         parser : {"pandas", "python"}, default: "pandas"
4499             The parser to use to construct the syntax tree from the expression.
4500             The default of 'pandas' parses code slightly different than standard
4501             Python. Alternatively, you can parse an expression using the 'python'
4502             parser to retain strict Python semantics.
4503         engine : {"python", "numexpr", None}, default: None
4504             The engine used to evaluate the expression. Supported engines are:
4505             - None: tries to use numexpr, falls back to python
4506             - "numexpr": evaluates expressions using numexpr
4507             - "python": performs operations as if you had eval’d in top level python
4508         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
4509             What to do if dimensions that should be selected from are not present in the
4510             Dataset:
4511             - "raise": raise an exception
4512             - "warn": raise a warning, and ignore the missing dimensions
4513             - "ignore": ignore the missing dimensions
4514         **queries_kwargs : {dim: query, ...}, optional
4515             The keyword arguments form of ``queries``.
4516             One of queries or queries_kwargs must be provided.
4517 
4518         Returns
4519         -------
4520         obj : DataArray
4521             A new DataArray with the same contents as this dataset, indexed by
4522             the results of the appropriate queries.
4523 
4524         See Also
4525         --------
4526         DataArray.isel
4527         Dataset.query
4528         pandas.eval
4529 
4530         Examples
4531         --------
4532         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
4533         >>> da
4534         <xarray.DataArray 'a' (x: 5)>
4535         array([0, 1, 2, 3, 4])
4536         Dimensions without coordinates: x
4537         >>> da.query(x="a > 2")
4538         <xarray.DataArray 'a' (x: 2)>
4539         array([3, 4])
4540         Dimensions without coordinates: x
4541         """
4542 
4543         ds = self._to_dataset_whole(shallow_copy=True)
4544         ds = ds.query(
4545             queries=queries,
4546             parser=parser,
4547             engine=engine,
4548             missing_dims=missing_dims,
4549             **queries_kwargs,
4550         )
4551         return ds[self.name]
4552 
4553     def curvefit(
4554         self,
4555         coords: Union[Union[str, "DataArray"], Iterable[Union[str, "DataArray"]]],
4556         func: Callable[..., Any],
4557         reduce_dims: Union[Hashable, Iterable[Hashable]] = None,
4558         skipna: bool = True,
4559         p0: Dict[str, Any] = None,
4560         bounds: Dict[str, Any] = None,
4561         param_names: Sequence[str] = None,
4562         kwargs: Dict[str, Any] = None,
4563     ):
4564         """
4565         Curve fitting optimization for arbitrary functions.
4566 
4567         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
4568 
4569         Parameters
4570         ----------
4571         coords : hashable, DataArray, or sequence of DataArray or hashable
4572             Independent coordinate(s) over which to perform the curve fitting. Must share
4573             at least one dimension with the calling object. When fitting multi-dimensional
4574             functions, supply `coords` as a sequence in the same order as arguments in
4575             `func`. To fit along existing dimensions of the calling object, `coords` can
4576             also be specified as a str or sequence of strs.
4577         func : callable
4578             User specified function in the form `f(x, *params)` which returns a numpy
4579             array of length `len(x)`. `params` are the fittable parameters which are optimized
4580             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
4581             coordinates, e.g. `f((x0, x1), *params)`.
4582         reduce_dims : hashable or sequence of hashable
4583             Additional dimension(s) over which to aggregate while fitting. For example,
4584             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
4585             aggregate all lat and lon points and fit the specified function along the
4586             time dimension.
4587         skipna : bool, optional
4588             Whether to skip missing values when fitting. Default is True.
4589         p0 : dict-like, optional
4590             Optional dictionary of parameter names to initial guesses passed to the
4591             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
4592             be assigned initial values following the default scipy behavior.
4593         bounds : dict-like, optional
4594             Optional dictionary of parameter names to bounding values passed to the
4595             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
4596             will be unbounded following the default scipy behavior.
4597         param_names : sequence of hashable, optional
4598             Sequence of names for the fittable parameters of `func`. If not supplied,
4599             this will be automatically determined by arguments of `func`. `param_names`
4600             should be manually supplied when fitting a function that takes a variable
4601             number of parameters.
4602         **kwargs : optional
4603             Additional keyword arguments to passed to scipy curve_fit.
4604 
4605         Returns
4606         -------
4607         curvefit_results : Dataset
4608             A single dataset which contains:
4609 
4610             [var]_curvefit_coefficients
4611                 The coefficients of the best fit.
4612             [var]_curvefit_covariance
4613                 The covariance matrix of the coefficient estimates.
4614 
4615         See Also
4616         --------
4617         DataArray.polyfit
4618         scipy.optimize.curve_fit
4619         """
4620         return self._to_temp_dataset().curvefit(
4621             coords,
4622             func,
4623             reduce_dims=reduce_dims,
4624             skipna=skipna,
4625             p0=p0,
4626             bounds=bounds,
4627             param_names=param_names,
4628             kwargs=kwargs,
4629         )
4630 
4631     def drop_duplicates(
4632         self,
4633         dim: Hashable,
4634         keep: Union[
4635             str,
4636             bool,
4637         ] = "first",
4638     ):
4639         """Returns a new DataArray with duplicate dimension values removed.
4640 
4641         Parameters
4642         ----------
4643         dim : dimension label, optional
4644         keep : {"first", "last", False}, default: "first"
4645             Determines which duplicates (if any) to keep.
4646             - ``"first"`` : Drop duplicates except for the first occurrence.
4647             - ``"last"`` : Drop duplicates except for the last occurrence.
4648             - False : Drop all duplicates.
4649 
4650         Returns
4651         -------
4652         DataArray
4653         """
4654         if dim not in self.dims:
4655             raise ValueError(f"'{dim}' not found in dimensions")
4656         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep)}
4657         return self.isel(indexes)
4658 
4659     # this needs to be at the end, or mypy will confuse with `str`
4660     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4661     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>Add a new method `cross` to the `DataArray` class. This method would provide a high-level interface to the `cross` function defined in `computation.py`, allowing users to call the cross product operation directly on `DataArray` instances, similar to how they currently use the `dot` method. This method should validate the input and dimensions, similarly ensuring that only suitable `DataArray` objects are passed (e.g., checking that the spatial dimension is correctly sized and exists within the arrays). This extension will make the feature easily accessible to users, integrating seamlessly with the existing API.</intended_behavior>]
2025-10-15 08:53:04.224 | INFO     | Search completed. Bug locations: [<file>xarray/core/computation.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Functions for applying functions that act on arrays to xarray's labeled data.
3 """
4 from __future__ import annotations
5 
6 import functools
7 import itertools
8 import operator
9 import warnings
10 from collections import Counter
11 from typing import (
12     TYPE_CHECKING,
13     AbstractSet,
14     Any,
15     Callable,
16     Dict,
17     Hashable,
18     Iterable,
19     List,
20     Mapping,
21     Optional,
22     Sequence,
23     Tuple,
24     Union,
25 )
26 
27 import numpy as np
28 
29 from . import dtypes, duck_array_ops, utils
30 from .alignment import align, deep_align
31 from .merge import merge_attrs, merge_coordinates_without_align
32 from .options import OPTIONS, _get_keep_attrs
33 from .pycompat import is_duck_dask_array
34 from .utils import is_dict_like
35 from .variable import Variable
36 
37 if TYPE_CHECKING:
38     from .coordinates import Coordinates
39     from .dataset import Dataset
40     from .types import T_Xarray
41 
42 _NO_FILL_VALUE = utils.ReprObject("<no-fill-value>")
43 _DEFAULT_NAME = utils.ReprObject("<default-name>")
44 _JOINS_WITHOUT_FILL_VALUES = frozenset({"inner", "exact"})
45 
46 
47 def _first_of_type(args, kind):
48     """Return either first object of type 'kind' or raise if not found."""
49     for arg in args:
50         if isinstance(arg, kind):
51             return arg
52     raise ValueError("This should be unreachable.")
53 
54 
55 def _all_of_type(args, kind):
56     """Return all objects of type 'kind'"""
57     return [arg for arg in args if isinstance(arg, kind)]
58 
59 
60 class _UFuncSignature:
61     """Core dimensions signature for a given function.
62 
63     Based on the signature provided by generalized ufuncs in NumPy.
64 
65     Attributes
66     ----------
67     input_core_dims : tuple[tuple]
68         Core dimension names on each input variable.
69     output_core_dims : tuple[tuple]
70         Core dimension names on each output variable.
71     """
72 
73     __slots__ = (
74         "input_core_dims",
75         "output_core_dims",
76         "_all_input_core_dims",
77         "_all_output_core_dims",
78         "_all_core_dims",
79     )
80 
81     def __init__(self, input_core_dims, output_core_dims=((),)):
82         self.input_core_dims = tuple(tuple(a) for a in input_core_dims)
83         self.output_core_dims = tuple(tuple(a) for a in output_core_dims)
84         self._all_input_core_dims = None
85         self._all_output_core_dims = None
86         self._all_core_dims = None
87 
88     @property
89     def all_input_core_dims(self):
90         if self._all_input_core_dims is None:
91             self._all_input_core_dims = frozenset(
92                 dim for dims in self.input_core_dims for dim in dims
93             )
94         return self._all_input_core_dims
95 
96     @property
97     def all_output_core_dims(self):
98         if self._all_output_core_dims is None:
99             self._all_output_core_dims = frozenset(
100                 dim for dims in self.output_core_dims for dim in dims
101             )
102         return self._all_output_core_dims
103 
104     @property
105     def all_core_dims(self):
106         if self._all_core_dims is None:
107             self._all_core_dims = self.all_input_core_dims | self.all_output_core_dims
108         return self._all_core_dims
109 
110     @property
111     def dims_map(self):
112         return {
113             core_dim: f"dim{n}" for n, core_dim in enumerate(sorted(self.all_core_dims))
114         }
115 
116     @property
117     def num_inputs(self):
118         return len(self.input_core_dims)
119 
120     @property
121     def num_outputs(self):
122         return len(self.output_core_dims)
123 
124     def __eq__(self, other):
125         try:
126             return (
127                 self.input_core_dims == other.input_core_dims
128                 and self.output_core_dims == other.output_core_dims
129             )
130         except AttributeError:
131             return False
132 
133     def __ne__(self, other):
134         return not self == other
135 
136     def __repr__(self):
137         return "{}({!r}, {!r})".format(
138             type(self).__name__,
139             list(self.input_core_dims),
140             list(self.output_core_dims),
141         )
142 
143     def __str__(self):
144         lhs = ",".join("({})".format(",".join(dims)) for dims in self.input_core_dims)
145         rhs = ",".join("({})".format(",".join(dims)) for dims in self.output_core_dims)
146         return f"{lhs}->{rhs}"
147 
148     def to_gufunc_string(self, exclude_dims=frozenset()):
149         """Create an equivalent signature string for a NumPy gufunc.
150 
151         Unlike __str__, handles dimensions that don't map to Python
152         identifiers.
153 
154         Also creates unique names for input_core_dims contained in exclude_dims.
155         """
156         input_core_dims = [
157             [self.dims_map[dim] for dim in core_dims]
158             for core_dims in self.input_core_dims
159         ]
160         output_core_dims = [
161             [self.dims_map[dim] for dim in core_dims]
162             for core_dims in self.output_core_dims
163         ]
164 
165         # enumerate input_core_dims contained in exclude_dims to make them unique
166         if exclude_dims:
167 
168             exclude_dims = [self.dims_map[dim] for dim in exclude_dims]
169 
170             counter = Counter()
171 
172             def _enumerate(dim):
173                 if dim in exclude_dims:
174                     n = counter[dim]
175                     counter.update([dim])
176                     dim = f"{dim}_{n}"
177                 return dim
178 
179             input_core_dims = [
180                 [_enumerate(dim) for dim in arg] for arg in input_core_dims
181             ]
182 
183         alt_signature = type(self)(input_core_dims, output_core_dims)
184         return str(alt_signature)
185 
186 
187 def result_name(objects: list) -> Any:
188     # use the same naming heuristics as pandas:
189     # https://github.com/blaze/blaze/issues/458#issuecomment-51936356
190     names = {getattr(obj, "name", _DEFAULT_NAME) for obj in objects}
191     names.discard(_DEFAULT_NAME)
192     if len(names) == 1:
193         (name,) = names
194     else:
195         name = None
196     return name
197 
198 
199 def _get_coords_list(args) -> List[Coordinates]:
200     coords_list = []
201     for arg in args:
202         try:
203             coords = arg.coords
204         except AttributeError:
205             pass  # skip this argument
206         else:
207             coords_list.append(coords)
208     return coords_list
209 
210 
211 def build_output_coords(
212     args: list,
213     signature: _UFuncSignature,
214     exclude_dims: AbstractSet = frozenset(),
215     combine_attrs: str = "override",
216 ) -> "List[Dict[Any, Variable]]":
217     """Build output coordinates for an operation.
218 
219     Parameters
220     ----------
221     args : list
222         List of raw operation arguments. Any valid types for xarray operations
223         are OK, e.g., scalars, Variable, DataArray, Dataset.
224     signature : _UfuncSignature
225         Core dimensions signature for the operation.
226     exclude_dims : set, optional
227         Dimensions excluded from the operation. Coordinates along these
228         dimensions are dropped.
229 
230     Returns
231     -------
232     Dictionary of Variable objects with merged coordinates.
233     """
234     coords_list = _get_coords_list(args)
235 
236     if len(coords_list) == 1 and not exclude_dims:
237         # we can skip the expensive merge
238         (unpacked_coords,) = coords_list
239         merged_vars = dict(unpacked_coords.variables)
240     else:
241         # TODO: save these merged indexes, instead of re-computing them later
242         merged_vars, unused_indexes = merge_coordinates_without_align(
243             coords_list, exclude_dims=exclude_dims, combine_attrs=combine_attrs
244         )
245 
246     output_coords = []
247     for output_dims in signature.output_core_dims:
248         dropped_dims = signature.all_input_core_dims - set(output_dims)
249         if dropped_dims:
250             filtered = {
251                 k: v for k, v in merged_vars.items() if dropped_dims.isdisjoint(v.dims)
252             }
253         else:
254             filtered = merged_vars
255         output_coords.append(filtered)
256 
257     return output_coords
258 
259 
260 def apply_dataarray_vfunc(
261     func,
262     *args,
263     signature,
264     join="inner",
265     exclude_dims=frozenset(),
266     keep_attrs="override",
267 ):
268     """Apply a variable level function over DataArray, Variable and/or ndarray
269     objects.
270     """
271     from .dataarray import DataArray
272 
273     if len(args) > 1:
274         args = deep_align(
275             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
276         )
277 
278     objs = _all_of_type(args, DataArray)
279 
280     if keep_attrs == "drop":
281         name = result_name(args)
282     else:
283         first_obj = _first_of_type(args, DataArray)
284         name = first_obj.name
285     result_coords = build_output_coords(
286         args, signature, exclude_dims, combine_attrs=keep_attrs
287     )
288 
289     data_vars = [getattr(a, "variable", a) for a in args]
290     result_var = func(*data_vars)
291 
292     if signature.num_outputs > 1:
293         out = tuple(
294             DataArray(variable, coords, name=name, fastpath=True)
295             for variable, coords in zip(result_var, result_coords)
296         )
297     else:
298         (coords,) = result_coords
299         out = DataArray(result_var, coords, name=name, fastpath=True)
300 
301     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
302     if isinstance(out, tuple):
303         for da in out:
304             da.attrs = attrs
305     else:
306         out.attrs = attrs
307 
308     return out
309 
310 
311 def ordered_set_union(all_keys: List[Iterable]) -> Iterable:
312     return {key: None for keys in all_keys for key in keys}.keys()
313 
314 
315 def ordered_set_intersection(all_keys: List[Iterable]) -> Iterable:
316     intersection = set(all_keys[0])
317     for keys in all_keys[1:]:
318         intersection.intersection_update(keys)
319     return [key for key in all_keys[0] if key in intersection]
320 
321 
322 def assert_and_return_exact_match(all_keys):
323     first_keys = all_keys[0]
324     for keys in all_keys[1:]:
325         if keys != first_keys:
326             raise ValueError(
327                 "exact match required for all data variable names, "
328                 f"but {keys!r} != {first_keys!r}"
329             )
330     return first_keys
331 
332 
333 _JOINERS: Dict[str, Callable] = {
334     "inner": ordered_set_intersection,
335     "outer": ordered_set_union,
336     "left": operator.itemgetter(0),
337     "right": operator.itemgetter(-1),
338     "exact": assert_and_return_exact_match,
339 }
340 
341 
342 def join_dict_keys(
343     objects: Iterable[Union[Mapping, Any]], how: str = "inner"
344 ) -> Iterable:
345     joiner = _JOINERS[how]
346     all_keys = [obj.keys() for obj in objects if hasattr(obj, "keys")]
347     return joiner(all_keys)
348 
349 
350 def collect_dict_values(
351     objects: Iterable[Union[Mapping, Any]], keys: Iterable, fill_value: object = None
352 ) -> List[list]:
353     return [
354         [obj.get(key, fill_value) if is_dict_like(obj) else obj for obj in objects]
355         for key in keys
356     ]
357 
358 
359 def _as_variables_or_variable(arg):
360     try:
361         return arg.variables
362     except AttributeError:
363         try:
364             return arg.variable
365         except AttributeError:
366             return arg
367 
368 
369 def _unpack_dict_tuples(
370     result_vars: Mapping[Any, Tuple[Variable, ...]], num_outputs: int
371 ) -> Tuple[Dict[Hashable, Variable], ...]:
372     out: Tuple[Dict[Hashable, Variable], ...] = tuple({} for _ in range(num_outputs))
373     for name, values in result_vars.items():
374         for value, results_dict in zip(values, out):
375             results_dict[name] = value
376     return out
377 
378 
379 def apply_dict_of_variables_vfunc(
380     func, *args, signature, join="inner", fill_value=None
381 ):
382     """Apply a variable level function over dicts of DataArray, DataArray,
383     Variable and ndarray objects.
384     """
385     args = [_as_variables_or_variable(arg) for arg in args]
386     names = join_dict_keys(args, how=join)
387     grouped_by_name = collect_dict_values(args, names, fill_value)
388 
389     result_vars = {}
390     for name, variable_args in zip(names, grouped_by_name):
391         result_vars[name] = func(*variable_args)
392 
393     if signature.num_outputs > 1:
394         return _unpack_dict_tuples(result_vars, signature.num_outputs)
395     else:
396         return result_vars
397 
398 
399 def _fast_dataset(
400     variables: Dict[Hashable, Variable], coord_variables: Mapping[Hashable, Variable]
401 ) -> Dataset:
402     """Create a dataset as quickly as possible.
403 
404     Beware: the `variables` dict is modified INPLACE.
405     """
406     from .dataset import Dataset
407 
408     variables.update(coord_variables)
409     coord_names = set(coord_variables)
410     return Dataset._construct_direct(variables, coord_names)
411 
412 
413 def apply_dataset_vfunc(
414     func,
415     *args,
416     signature,
417     join="inner",
418     dataset_join="exact",
419     fill_value=_NO_FILL_VALUE,
420     exclude_dims=frozenset(),
421     keep_attrs="override",
422 ):
423     """Apply a variable level function over Dataset, dict of DataArray,
424     DataArray, Variable and/or ndarray objects.
425     """
426     from .dataset import Dataset
427 
428     if dataset_join not in _JOINS_WITHOUT_FILL_VALUES and fill_value is _NO_FILL_VALUE:
429         raise TypeError(
430             "to apply an operation to datasets with different "
431             "data variables with apply_ufunc, you must supply the "
432             "dataset_fill_value argument."
433         )
434 
435     objs = _all_of_type(args, Dataset)
436 
437     if len(args) > 1:
438         args = deep_align(
439             args, join=join, copy=False, exclude=exclude_dims, raise_on_invalid=False
440         )
441 
442     list_of_coords = build_output_coords(
443         args, signature, exclude_dims, combine_attrs=keep_attrs
444     )
445     args = [getattr(arg, "data_vars", arg) for arg in args]
446 
447     result_vars = apply_dict_of_variables_vfunc(
448         func, *args, signature=signature, join=dataset_join, fill_value=fill_value
449     )
450 
451     if signature.num_outputs > 1:
452         out = tuple(_fast_dataset(*args) for args in zip(result_vars, list_of_coords))
453     else:
454         (coord_vars,) = list_of_coords
455         out = _fast_dataset(result_vars, coord_vars)
456 
457     attrs = merge_attrs([x.attrs for x in objs], combine_attrs=keep_attrs)
458     if isinstance(out, tuple):
459         for ds in out:
460             ds.attrs = attrs
461     else:
462         out.attrs = attrs
463 
464     return out
465 
466 
467 def _iter_over_selections(obj, dim, values):
468     """Iterate over selections of an xarray object in the provided order."""
469     from .groupby import _dummy_copy
470 
471     dummy = None
472     for value in values:
473         try:
474             obj_sel = obj.sel(**{dim: value})
475         except (KeyError, IndexError):
476             if dummy is None:
477                 dummy = _dummy_copy(obj)
478             obj_sel = dummy
479         yield obj_sel
480 
481 
482 def apply_groupby_func(func, *args):
483     """Apply a dataset or datarray level function over GroupBy, Dataset,
484     DataArray, Variable and/or ndarray objects.
485     """
486     from .groupby import GroupBy, peek_at
487     from .variable import Variable
488 
489     groupbys = [arg for arg in args if isinstance(arg, GroupBy)]
490     assert groupbys, "must have at least one groupby to iterate over"
491     first_groupby = groupbys[0]
492     if any(not first_groupby._group.equals(gb._group) for gb in groupbys[1:]):
493         raise ValueError(
494             "apply_ufunc can only perform operations over "
495             "multiple GroupBy objects at once if they are all "
496             "grouped the same way"
497         )
498 
499     grouped_dim = first_groupby._group.name
500     unique_values = first_groupby._unique_coord.values
501 
502     iterators = []
503     for arg in args:
504         if isinstance(arg, GroupBy):
505             iterator = (value for _, value in arg)
506         elif hasattr(arg, "dims") and grouped_dim in arg.dims:
507             if isinstance(arg, Variable):
508                 raise ValueError(
509                     "groupby operations cannot be performed with "
510                     "xarray.Variable objects that share a dimension with "
511                     "the grouped dimension"
512                 )
513             iterator = _iter_over_selections(arg, grouped_dim, unique_values)
514         else:
515             iterator = itertools.repeat(arg)
516         iterators.append(iterator)
517 
518     applied = (func(*zipped_args) for zipped_args in zip(*iterators))
519     applied_example, applied = peek_at(applied)
520     combine = first_groupby._combine
521     if isinstance(applied_example, tuple):
522         combined = tuple(combine(output) for output in zip(*applied))
523     else:
524         combined = combine(applied)
525     return combined
526 
527 
528 def unified_dim_sizes(
529     variables: Iterable[Variable], exclude_dims: AbstractSet = frozenset()
530 ) -> Dict[Hashable, int]:
531 
532     dim_sizes: Dict[Hashable, int] = {}
533 
534     for var in variables:
535         if len(set(var.dims)) < len(var.dims):
536             raise ValueError(
537                 "broadcasting cannot handle duplicate "
538                 f"dimensions on a variable: {list(var.dims)}"
539             )
540         for dim, size in zip(var.dims, var.shape):
541             if dim not in exclude_dims:
542                 if dim not in dim_sizes:
543                     dim_sizes[dim] = size
544                 elif dim_sizes[dim] != size:
545                     raise ValueError(
546                         "operands cannot be broadcast together "
547                         "with mismatched lengths for dimension "
548                         f"{dim}: {dim_sizes[dim]} vs {size}"
549                     )
550     return dim_sizes
551 
552 
553 SLICE_NONE = slice(None)
554 
555 
556 def broadcast_compat_data(
557     variable: Variable,
558     broadcast_dims: Tuple[Hashable, ...],
559     core_dims: Tuple[Hashable, ...],
560 ) -> Any:
561     data = variable.data
562 
563     old_dims = variable.dims
564     new_dims = broadcast_dims + core_dims
565 
566     if new_dims == old_dims:
567         # optimize for the typical case
568         return data
569 
570     set_old_dims = set(old_dims)
571     missing_core_dims = [d for d in core_dims if d not in set_old_dims]
572     if missing_core_dims:
573         raise ValueError(
574             "operand to apply_ufunc has required core dimensions {}, but "
575             "some of these dimensions are absent on an input variable: {}".format(
576                 list(core_dims), missing_core_dims
577             )
578         )
579 
580     set_new_dims = set(new_dims)
581     unexpected_dims = [d for d in old_dims if d not in set_new_dims]
582     if unexpected_dims:
583         raise ValueError(
584             "operand to apply_ufunc encountered unexpected "
585             f"dimensions {unexpected_dims!r} on an input variable: these are core "
586             "dimensions on other input or output variables"
587         )
588 
589     # for consistency with numpy, keep broadcast dimensions to the left
590     old_broadcast_dims = tuple(d for d in broadcast_dims if d in set_old_dims)
591     reordered_dims = old_broadcast_dims + core_dims
592     if reordered_dims != old_dims:
593         order = tuple(old_dims.index(d) for d in reordered_dims)
594         data = duck_array_ops.transpose(data, order)
595 
596     if new_dims != reordered_dims:
597         key_parts: List[Optional[slice]] = []
598         for dim in new_dims:
599             if dim in set_old_dims:
600                 key_parts.append(SLICE_NONE)
601             elif key_parts:
602                 # no need to insert new axes at the beginning that are already
603                 # handled by broadcasting
604                 key_parts.append(np.newaxis)
605         data = data[tuple(key_parts)]
606 
607     return data
608 
609 
610 def _vectorize(func, signature, output_dtypes, exclude_dims):
611     if signature.all_core_dims:
612         func = np.vectorize(
613             func,
614             otypes=output_dtypes,
615             signature=signature.to_gufunc_string(exclude_dims),
616         )
617     else:
618         func = np.vectorize(func, otypes=output_dtypes)
619 
620     return func
621 
622 
623 def apply_variable_ufunc(
624     func,
625     *args,
626     signature,
627     exclude_dims=frozenset(),
628     dask="forbidden",
629     output_dtypes=None,
630     vectorize=False,
631     keep_attrs="override",
632     dask_gufunc_kwargs=None,
633 ):
634     """Apply a ndarray level function over Variable and/or ndarray objects."""
635     from .variable import Variable, as_compatible_data
636 
637     dim_sizes = unified_dim_sizes(
638         (a for a in args if hasattr(a, "dims")), exclude_dims=exclude_dims
639     )
640     broadcast_dims = tuple(
641         dim for dim in dim_sizes if dim not in signature.all_core_dims
642     )
643     output_dims = [broadcast_dims + out for out in signature.output_core_dims]
644 
645     input_data = [
646         broadcast_compat_data(arg, broadcast_dims, core_dims)
647         if isinstance(arg, Variable)
648         else arg
649         for arg, core_dims in zip(args, signature.input_core_dims)
650     ]
651 
652     if any(is_duck_dask_array(array) for array in input_data):
653         if dask == "forbidden":
654             raise ValueError(
655                 "apply_ufunc encountered a dask array on an "
656                 "argument, but handling for dask arrays has not "
657                 "been enabled. Either set the ``dask`` argument "
658                 "or load your data into memory first with "
659                 "``.load()`` or ``.compute()``"
660             )
661         elif dask == "parallelized":
662             numpy_func = func
663 
664             if dask_gufunc_kwargs is None:
665                 dask_gufunc_kwargs = {}
666             else:
667                 dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
668 
669             allow_rechunk = dask_gufunc_kwargs.get("allow_rechunk", None)
670             if allow_rechunk is None:
671                 for n, (data, core_dims) in enumerate(
672                     zip(input_data, signature.input_core_dims)
673                 ):
674                     if is_duck_dask_array(data):
675                         # core dimensions cannot span multiple chunks
676                         for axis, dim in enumerate(core_dims, start=-len(core_dims)):
677                             if len(data.chunks[axis]) != 1:
678                                 raise ValueError(
679                                     f"dimension {dim} on {n}th function argument to "
680                                     "apply_ufunc with dask='parallelized' consists of "
681                                     "multiple chunks, but is also a core dimension. To "
682                                     "fix, either rechunk into a single dask array chunk along "
683                                     f"this dimension, i.e., ``.chunk(dict({dim}=-1))``, or "
684                                     "pass ``allow_rechunk=True`` in ``dask_gufunc_kwargs`` "
685                                     "but beware that this may significantly increase memory usage."
686                                 )
687                 dask_gufunc_kwargs["allow_rechunk"] = True
688 
689             output_sizes = dask_gufunc_kwargs.pop("output_sizes", {})
690             if output_sizes:
691                 output_sizes_renamed = {}
692                 for key, value in output_sizes.items():
693                     if key not in signature.all_output_core_dims:
694                         raise ValueError(
695                             f"dimension '{key}' in 'output_sizes' must correspond to output_core_dims"
696                         )
697                     output_sizes_renamed[signature.dims_map[key]] = value
698                 dask_gufunc_kwargs["output_sizes"] = output_sizes_renamed
699 
700             for key in signature.all_output_core_dims:
701                 if key not in signature.all_input_core_dims and key not in output_sizes:
702                     raise ValueError(
703                         f"dimension '{key}' in 'output_core_dims' needs corresponding (dim, size) in 'output_sizes'"
704                     )
705 
706             def func(*arrays):
707                 import dask.array as da
708 
709                 res = da.apply_gufunc(
710                     numpy_func,
711                     signature.to_gufunc_string(exclude_dims),
712                     *arrays,
713                     vectorize=vectorize,
714                     output_dtypes=output_dtypes,
715                     **dask_gufunc_kwargs,
716                 )
717 
718                 return res
719 
720         elif dask == "allowed":
721             pass
722         else:
723             raise ValueError(
724                 "unknown setting for dask array handling in "
725                 "apply_ufunc: {}".format(dask)
726             )
727     else:
728         if vectorize:
729             func = _vectorize(
730                 func, signature, output_dtypes=output_dtypes, exclude_dims=exclude_dims
731             )
732 
733     result_data = func(*input_data)
734 
735     if signature.num_outputs == 1:
736         result_data = (result_data,)
737     elif (
738         not isinstance(result_data, tuple) or len(result_data) != signature.num_outputs
739     ):
740         raise ValueError(
741             "applied function does not have the number of "
742             "outputs specified in the ufunc signature. "
743             "Result is not a tuple of {} elements: {!r}".format(
744                 signature.num_outputs, result_data
745             )
746         )
747 
748     objs = _all_of_type(args, Variable)
749     attrs = merge_attrs(
750         [obj.attrs for obj in objs],
751         combine_attrs=keep_attrs,
752     )
753 
754     output = []
755     for dims, data in zip(output_dims, result_data):
756         data = as_compatible_data(data)
757         if data.ndim != len(dims):
758             raise ValueError(
759                 "applied function returned data with unexpected "
760                 f"number of dimensions. Received {data.ndim} dimension(s) but "
761                 f"expected {len(dims)} dimensions with names: {dims!r}"
762             )
763 
764         var = Variable(dims, data, fastpath=True)
765         for dim, new_size in var.sizes.items():
766             if dim in dim_sizes and new_size != dim_sizes[dim]:
767                 raise ValueError(
768                     "size of dimension {!r} on inputs was unexpectedly "
769                     "changed by applied function from {} to {}. Only "
770                     "dimensions specified in ``exclude_dims`` with "
771                     "xarray.apply_ufunc are allowed to change size.".format(
772                         dim, dim_sizes[dim], new_size
773                     )
774                 )
775 
776         var.attrs = attrs
777         output.append(var)
778 
779     if signature.num_outputs == 1:
780         return output[0]
781     else:
782         return tuple(output)
783 
784 
785 def apply_array_ufunc(func, *args, dask="forbidden"):
786     """Apply a ndarray level function over ndarray objects."""
787     if any(is_duck_dask_array(arg) for arg in args):
788         if dask == "forbidden":
789             raise ValueError(
790                 "apply_ufunc encountered a dask array on an "
791                 "argument, but handling for dask arrays has not "
792                 "been enabled. Either set the ``dask`` argument "
793                 "or load your data into memory first with "
794                 "``.load()`` or ``.compute()``"
795             )
796         elif dask == "parallelized":
797             raise ValueError(
798                 "cannot use dask='parallelized' for apply_ufunc "
799                 "unless at least one input is an xarray object"
800             )
801         elif dask == "allowed":
802             pass
803         else:
804             raise ValueError(f"unknown setting for dask array handling: {dask}")
805     return func(*args)
806 
807 
808 def apply_ufunc(
809     func: Callable,
810     *args: Any,
811     input_core_dims: Sequence[Sequence] = None,
812     output_core_dims: Optional[Sequence[Sequence]] = ((),),
813     exclude_dims: AbstractSet = frozenset(),
814     vectorize: bool = False,
815     join: str = "exact",
816     dataset_join: str = "exact",
817     dataset_fill_value: object = _NO_FILL_VALUE,
818     keep_attrs: Union[bool, str] = None,
819     kwargs: Mapping = None,
820     dask: str = "forbidden",
821     output_dtypes: Sequence = None,
822     output_sizes: Mapping[Any, int] = None,
823     meta: Any = None,
824     dask_gufunc_kwargs: Dict[str, Any] = None,
825 ) -> Any:
826     """Apply a vectorized function for unlabeled arrays on xarray objects.
827 
828     The function will be mapped over the data variable(s) of the input
829     arguments using xarray's standard rules for labeled computation, including
830     alignment, broadcasting, looping over GroupBy/Dataset variables, and
831     merging of coordinates.
832 
833     Parameters
834     ----------
835     func : callable
836         Function to call like ``func(*args, **kwargs)`` on unlabeled arrays
837         (``.data``) that returns an array or tuple of arrays. If multiple
838         arguments with non-matching dimensions are supplied, this function is
839         expected to vectorize (broadcast) over axes of positional arguments in
840         the style of NumPy universal functions [1]_ (if this is not the case,
841         set ``vectorize=True``). If this function returns multiple outputs, you
842         must set ``output_core_dims`` as well.
843     *args : Dataset, DataArray, DataArrayGroupBy, DatasetGroupBy, Variable, numpy.ndarray, dask.array.Array or scalar
844         Mix of labeled and/or unlabeled arrays to which to apply the function.
845     input_core_dims : sequence of sequence, optional
846         List of the same length as ``args`` giving the list of core dimensions
847         on each input argument that should not be broadcast. By default, we
848         assume there are no core dimensions on any input arguments.
849 
850         For example, ``input_core_dims=[[], ['time']]`` indicates that all
851         dimensions on the first argument and all dimensions other than 'time'
852         on the second argument should be broadcast.
853 
854         Core dimensions are automatically moved to the last axes of input
855         variables before applying ``func``, which facilitates using NumPy style
856         generalized ufuncs [2]_.
857     output_core_dims : list of tuple, optional
858         List of the same length as the number of output arguments from
859         ``func``, giving the list of core dimensions on each output that were
860         not broadcast on the inputs. By default, we assume that ``func``
861         outputs exactly one array, with axes corresponding to each broadcast
862         dimension.
863 
864         Core dimensions are assumed to appear as the last dimensions of each
865         output in the provided order.
866     exclude_dims : set, optional
867         Core dimensions on the inputs to exclude from alignment and
868         broadcasting entirely. Any input coordinates along these dimensions
869         will be dropped. Each excluded dimension must also appear in
870         ``input_core_dims`` for at least one argument. Only dimensions listed
871         here are allowed to change size between input and output objects.
872     vectorize : bool, optional
873         If True, then assume ``func`` only takes arrays defined over core
874         dimensions as input and vectorize it automatically with
875         :py:func:`numpy.vectorize`. This option exists for convenience, but is
876         almost always slower than supplying a pre-vectorized function.
877         Using this option requires NumPy version 1.12 or newer.
878     join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
879         Method for joining the indexes of the passed objects along each
880         dimension, and the variables of Dataset objects with mismatched
881         data variables:
882 
883         - 'outer': use the union of object indexes
884         - 'inner': use the intersection of object indexes
885         - 'left': use indexes from the first object with each dimension
886         - 'right': use indexes from the last object with each dimension
887         - 'exact': raise `ValueError` instead of aligning when indexes to be
888           aligned are not equal
889     dataset_join : {"outer", "inner", "left", "right", "exact"}, default: "exact"
890         Method for joining variables of Dataset objects with mismatched
891         data variables.
892 
893         - 'outer': take variables from both Dataset objects
894         - 'inner': take only overlapped variables
895         - 'left': take only variables from the first object
896         - 'right': take only variables from the last object
897         - 'exact': data variables on all Dataset objects must match exactly
898     dataset_fill_value : optional
899         Value used in place of missing variables on Dataset inputs when the
900         datasets do not share the exact same ``data_vars``. Required if
901         ``dataset_join not in {'inner', 'exact'}``, otherwise ignored.
902     keep_attrs : bool, optional
903         Whether to copy attributes from the first argument to the output.
904     kwargs : dict, optional
905         Optional keyword arguments passed directly on to call ``func``.
906     dask : {"forbidden", "allowed", "parallelized"}, default: "forbidden"
907         How to handle applying to objects containing lazy data in the form of
908         dask arrays:
909 
910         - 'forbidden' (default): raise an error if a dask array is encountered.
911         - 'allowed': pass dask arrays directly on to ``func``. Prefer this option if
912           ``func`` natively supports dask arrays.
913         - 'parallelized': automatically parallelize ``func`` if any of the
914           inputs are a dask array by using :py:func:`dask.array.apply_gufunc`. Multiple output
915           arguments are supported. Only use this option if ``func`` does not natively
916           support dask arrays (e.g. converts them to numpy arrays).
917     dask_gufunc_kwargs : dict, optional
918         Optional keyword arguments passed to :py:func:`dask.array.apply_gufunc` if
919         dask='parallelized'. Possible keywords are ``output_sizes``, ``allow_rechunk``
920         and ``meta``.
921     output_dtypes : list of dtype, optional
922         Optional list of output dtypes. Only used if ``dask='parallelized'`` or
923         ``vectorize=True``.
924     output_sizes : dict, optional
925         Optional mapping from dimension names to sizes for outputs. Only used
926         if dask='parallelized' and new dimensions (not found on inputs) appear
927         on outputs. ``output_sizes`` should be given in the ``dask_gufunc_kwargs``
928         parameter. It will be removed as direct parameter in a future version.
929     meta : optional
930         Size-0 object representing the type of array wrapped by dask array. Passed on to
931         :py:func:`dask.array.apply_gufunc`. ``meta`` should be given in the
932         ``dask_gufunc_kwargs`` parameter . It will be removed as direct parameter
933         a future version.
934 
935     Returns
936     -------
937     Single value or tuple of Dataset, DataArray, Variable, dask.array.Array or
938     numpy.ndarray, the first type on that list to appear on an input.
939 
940     Notes
941     -----
942     This function is designed for the more common case where ``func`` can work on numpy
943     arrays. If ``func`` needs to manipulate a whole xarray object subset to each block
944     it is possible to use :py:func:`xarray.map_blocks`.
945 
946     Note that due to the overhead :py:func:`xarray.map_blocks` is considerably slower than ``apply_ufunc``.
947 
948     Examples
949     --------
950     Calculate the vector magnitude of two arguments:
951 
952     >>> def magnitude(a, b):
953     ...     func = lambda x, y: np.sqrt(x ** 2 + y ** 2)
954     ...     return xr.apply_ufunc(func, a, b)
955     ...
956 
957     You can now apply ``magnitude()`` to :py:class:`DataArray` and :py:class:`Dataset`
958     objects, with automatically preserved dimensions and coordinates, e.g.,
959 
960     >>> array = xr.DataArray([1, 2, 3], coords=[("x", [0.1, 0.2, 0.3])])
961     >>> magnitude(array, -array)
962     <xarray.DataArray (x: 3)>
963     array([1.41421356, 2.82842712, 4.24264069])
964     Coordinates:
965       * x        (x) float64 0.1 0.2 0.3
966 
967     Plain scalars, numpy arrays and a mix of these with xarray objects is also
968     supported:
969 
970     >>> magnitude(3, 4)
971     5.0
972     >>> magnitude(3, np.array([0, 4]))
973     array([3., 5.])
974     >>> magnitude(array, 0)
975     <xarray.DataArray (x: 3)>
976     array([1., 2., 3.])
977     Coordinates:
978       * x        (x) float64 0.1 0.2 0.3
979 
980     Other examples of how you could use ``apply_ufunc`` to write functions to
981     (very nearly) replicate existing xarray functionality:
982 
983     Compute the mean (``.mean``) over one dimension:
984 
985     >>> def mean(obj, dim):
986     ...     # note: apply always moves core dimensions to the end
987     ...     return apply_ufunc(
988     ...         np.mean, obj, input_core_dims=[[dim]], kwargs={"axis": -1}
989     ...     )
990     ...
991 
992     Inner product over a specific dimension (like :py:func:`dot`):
993 
994     >>> def _inner(x, y):
995     ...     result = np.matmul(x[..., np.newaxis, :], y[..., :, np.newaxis])
996     ...     return result[..., 0, 0]
997     ...
998     >>> def inner_product(a, b, dim):
999     ...     return apply_ufunc(_inner, a, b, input_core_dims=[[dim], [dim]])
1000     ...
1001 
1002     Stack objects along a new dimension (like :py:func:`concat`):
1003 
1004     >>> def stack(objects, dim, new_coord):
1005     ...     # note: this version does not stack coordinates
1006     ...     func = lambda *x: np.stack(x, axis=-1)
1007     ...     result = apply_ufunc(
1008     ...         func,
1009     ...         *objects,
1010     ...         output_core_dims=[[dim]],
1011     ...         join="outer",
1012     ...         dataset_fill_value=np.nan
1013     ...     )
1014     ...     result[dim] = new_coord
1015     ...     return result
1016     ...
1017 
1018     If your function is not vectorized but can be applied only to core
1019     dimensions, you can use ``vectorize=True`` to turn into a vectorized
1020     function. This wraps :py:func:`numpy.vectorize`, so the operation isn't
1021     terribly fast. Here we'll use it to calculate the distance between
1022     empirical samples from two probability distributions, using a scipy
1023     function that needs to be applied to vectors:
1024 
1025     >>> import scipy.stats
1026     >>> def earth_mover_distance(first_samples, second_samples, dim="ensemble"):
1027     ...     return apply_ufunc(
1028     ...         scipy.stats.wasserstein_distance,
1029     ...         first_samples,
1030     ...         second_samples,
1031     ...         input_core_dims=[[dim], [dim]],
1032     ...         vectorize=True,
1033     ...     )
1034     ...
1035 
1036     Most of NumPy's builtin functions already broadcast their inputs
1037     appropriately for use in ``apply_ufunc``. You may find helper functions such as
1038     :py:func:`numpy.broadcast_arrays` helpful in writing your function. ``apply_ufunc`` also
1039     works well with :py:func:`numba.vectorize` and :py:func:`numba.guvectorize`.
1040 
1041     See Also
1042     --------
1043     numpy.broadcast_arrays
1044     numba.vectorize
1045     numba.guvectorize
1046     dask.array.apply_gufunc
1047     xarray.map_blocks
1048     :ref:`dask.automatic-parallelization`
1049         User guide describing :py:func:`apply_ufunc` and :py:func:`map_blocks`.
1050 
1051     References
1052     ----------
1053     .. [1] http://docs.scipy.org/doc/numpy/reference/ufuncs.html
1054     .. [2] http://docs.scipy.org/doc/numpy/reference/c-api.generalized-ufuncs.html
1055     """
1056     from .dataarray import DataArray
1057     from .groupby import GroupBy
1058     from .variable import Variable
1059 
1060     if input_core_dims is None:
1061         input_core_dims = ((),) * (len(args))
1062     elif len(input_core_dims) != len(args):
1063         raise ValueError(
1064             f"input_core_dims must be None or a tuple with the length same to "
1065             f"the number of arguments. "
1066             f"Given {len(input_core_dims)} input_core_dims: {input_core_dims}, "
1067             f" but number of args is {len(args)}."
1068         )
1069 
1070     if kwargs is None:
1071         kwargs = {}
1072 
1073     signature = _UFuncSignature(input_core_dims, output_core_dims)
1074 
1075     if exclude_dims:
1076         if not isinstance(exclude_dims, set):
1077             raise TypeError(
1078                 f"Expected exclude_dims to be a 'set'. Received '{type(exclude_dims).__name__}' instead."
1079             )
1080         if not exclude_dims <= signature.all_core_dims:
1081             raise ValueError(
1082                 f"each dimension in `exclude_dims` must also be a "
1083                 f"core dimension in the function signature. "
1084                 f"Please make {(exclude_dims - signature.all_core_dims)} a core dimension"
1085             )
1086 
1087     # handle dask_gufunc_kwargs
1088     if dask == "parallelized":
1089         if dask_gufunc_kwargs is None:
1090             dask_gufunc_kwargs = {}
1091         else:
1092             dask_gufunc_kwargs = dask_gufunc_kwargs.copy()
1093         # todo: remove warnings after deprecation cycle
1094         if meta is not None:
1095             warnings.warn(
1096                 "``meta`` should be given in the ``dask_gufunc_kwargs`` parameter."
1097                 " It will be removed as direct parameter in a future version.",
1098                 FutureWarning,
1099                 stacklevel=2,
1100             )
1101             dask_gufunc_kwargs.setdefault("meta", meta)
1102         if output_sizes is not None:
1103             warnings.warn(
1104                 "``output_sizes`` should be given in the ``dask_gufunc_kwargs`` "
1105                 "parameter. It will be removed as direct parameter in a future "
1106                 "version.",
1107                 FutureWarning,
1108                 stacklevel=2,
1109             )
1110             dask_gufunc_kwargs.setdefault("output_sizes", output_sizes)
1111 
1112     if kwargs:
1113         func = functools.partial(func, **kwargs)
1114 
1115     if keep_attrs is None:
1116         keep_attrs = _get_keep_attrs(default=False)
1117 
1118     if isinstance(keep_attrs, bool):
1119         keep_attrs = "override" if keep_attrs else "drop"
1120 
1121     variables_vfunc = functools.partial(
1122         apply_variable_ufunc,
1123         func,
1124         signature=signature,
1125         exclude_dims=exclude_dims,
1126         keep_attrs=keep_attrs,
1127         dask=dask,
1128         vectorize=vectorize,
1129         output_dtypes=output_dtypes,
1130         dask_gufunc_kwargs=dask_gufunc_kwargs,
1131     )
1132 
1133     # feed groupby-apply_ufunc through apply_groupby_func
1134     if any(isinstance(a, GroupBy) for a in args):
1135         this_apply = functools.partial(
1136             apply_ufunc,
1137             func,
1138             input_core_dims=input_core_dims,
1139             output_core_dims=output_core_dims,
1140             exclude_dims=exclude_dims,
1141             join=join,
1142             dataset_join=dataset_join,
1143             dataset_fill_value=dataset_fill_value,
1144             keep_attrs=keep_attrs,
1145             dask=dask,
1146             vectorize=vectorize,
1147             output_dtypes=output_dtypes,
1148             dask_gufunc_kwargs=dask_gufunc_kwargs,
1149         )
1150         return apply_groupby_func(this_apply, *args)
1151     # feed datasets apply_variable_ufunc through apply_dataset_vfunc
1152     elif any(is_dict_like(a) for a in args):
1153         return apply_dataset_vfunc(
1154             variables_vfunc,
1155             *args,
1156             signature=signature,
1157             join=join,
1158             exclude_dims=exclude_dims,
1159             dataset_join=dataset_join,
1160             fill_value=dataset_fill_value,
1161             keep_attrs=keep_attrs,
1162         )
1163     # feed DataArray apply_variable_ufunc through apply_dataarray_vfunc
1164     elif any(isinstance(a, DataArray) for a in args):
1165         return apply_dataarray_vfunc(
1166             variables_vfunc,
1167             *args,
1168             signature=signature,
1169             join=join,
1170             exclude_dims=exclude_dims,
1171             keep_attrs=keep_attrs,
1172         )
1173     # feed Variables directly through apply_variable_ufunc
1174     elif any(isinstance(a, Variable) for a in args):
1175         return variables_vfunc(*args)
1176     else:
1177         # feed anything else through apply_array_ufunc
1178         return apply_array_ufunc(func, *args, dask=dask)
1179 
1180 
1181 def cov(da_a, da_b, dim=None, ddof=1):
1182     """
1183     Compute covariance between two DataArray objects along a shared dimension.
1184 
1185     Parameters
1186     ----------
1187     da_a : DataArray
1188         Array to compute.
1189     da_b : DataArray
1190         Array to compute.
1191     dim : str, optional
1192         The dimension along which the covariance will be computed
1193     ddof : int, optional
1194         If ddof=1, covariance is normalized by N-1, giving an unbiased estimate,
1195         else normalization is by N.
1196 
1197     Returns
1198     -------
1199     covariance : DataArray
1200 
1201     See Also
1202     --------
1203     pandas.Series.cov : corresponding pandas function
1204     xarray.corr : respective function to calculate correlation
1205 
1206     Examples
1207     --------
1208     >>> from xarray import DataArray
1209     >>> da_a = DataArray(
1210     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1211     ...     dims=("space", "time"),
1212     ...     coords=[
1213     ...         ("space", ["IA", "IL", "IN"]),
1214     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1215     ...     ],
1216     ... )
1217     >>> da_a
1218     <xarray.DataArray (space: 3, time: 3)>
1219     array([[1. , 2. , 3. ],
1220            [0.1, 0.2, 0.3],
1221            [3.2, 0.6, 1.8]])
1222     Coordinates:
1223       * space    (space) <U2 'IA' 'IL' 'IN'
1224       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1225     >>> da_b = DataArray(
1226     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1227     ...     dims=("space", "time"),
1228     ...     coords=[
1229     ...         ("space", ["IA", "IL", "IN"]),
1230     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1231     ...     ],
1232     ... )
1233     >>> da_b
1234     <xarray.DataArray (space: 3, time: 3)>
1235     array([[ 0.2,  0.4,  0.6],
1236            [15. , 10. ,  5. ],
1237            [ 3.2,  0.6,  1.8]])
1238     Coordinates:
1239       * space    (space) <U2 'IA' 'IL' 'IN'
1240       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1241     >>> xr.cov(da_a, da_b)
1242     <xarray.DataArray ()>
1243     array(-3.53055556)
1244     >>> xr.cov(da_a, da_b, dim="time")
1245     <xarray.DataArray (space: 3)>
1246     array([ 0.2       , -0.5       ,  1.69333333])
1247     Coordinates:
1248       * space    (space) <U2 'IA' 'IL' 'IN'
1249     """
1250     from .dataarray import DataArray
1251 
1252     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1253         raise TypeError(
1254             "Only xr.DataArray is supported."
1255             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1256         )
1257 
1258     return _cov_corr(da_a, da_b, dim=dim, ddof=ddof, method="cov")
1259 
1260 
1261 def corr(da_a, da_b, dim=None):
1262     """
1263     Compute the Pearson correlation coefficient between
1264     two DataArray objects along a shared dimension.
1265 
1266     Parameters
1267     ----------
1268     da_a : DataArray
1269         Array to compute.
1270     da_b : DataArray
1271         Array to compute.
1272     dim : str, optional
1273         The dimension along which the correlation will be computed
1274 
1275     Returns
1276     -------
1277     correlation: DataArray
1278 
1279     See Also
1280     --------
1281     pandas.Series.corr : corresponding pandas function
1282     xarray.cov : underlying covariance function
1283 
1284     Examples
1285     --------
1286     >>> from xarray import DataArray
1287     >>> da_a = DataArray(
1288     ...     np.array([[1, 2, 3], [0.1, 0.2, 0.3], [3.2, 0.6, 1.8]]),
1289     ...     dims=("space", "time"),
1290     ...     coords=[
1291     ...         ("space", ["IA", "IL", "IN"]),
1292     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1293     ...     ],
1294     ... )
1295     >>> da_a
1296     <xarray.DataArray (space: 3, time: 3)>
1297     array([[1. , 2. , 3. ],
1298            [0.1, 0.2, 0.3],
1299            [3.2, 0.6, 1.8]])
1300     Coordinates:
1301       * space    (space) <U2 'IA' 'IL' 'IN'
1302       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1303     >>> da_b = DataArray(
1304     ...     np.array([[0.2, 0.4, 0.6], [15, 10, 5], [3.2, 0.6, 1.8]]),
1305     ...     dims=("space", "time"),
1306     ...     coords=[
1307     ...         ("space", ["IA", "IL", "IN"]),
1308     ...         ("time", pd.date_range("2000-01-01", freq="1D", periods=3)),
1309     ...     ],
1310     ... )
1311     >>> da_b
1312     <xarray.DataArray (space: 3, time: 3)>
1313     array([[ 0.2,  0.4,  0.6],
1314            [15. , 10. ,  5. ],
1315            [ 3.2,  0.6,  1.8]])
1316     Coordinates:
1317       * space    (space) <U2 'IA' 'IL' 'IN'
1318       * time     (time) datetime64[ns] 2000-01-01 2000-01-02 2000-01-03
1319     >>> xr.corr(da_a, da_b)
1320     <xarray.DataArray ()>
1321     array(-0.57087777)
1322     >>> xr.corr(da_a, da_b, dim="time")
1323     <xarray.DataArray (space: 3)>
1324     array([ 1., -1.,  1.])
1325     Coordinates:
1326       * space    (space) <U2 'IA' 'IL' 'IN'
1327     """
1328     from .dataarray import DataArray
1329 
1330     if any(not isinstance(arr, DataArray) for arr in [da_a, da_b]):
1331         raise TypeError(
1332             "Only xr.DataArray is supported."
1333             "Given {}.".format([type(arr) for arr in [da_a, da_b]])
1334         )
1335 
1336     return _cov_corr(da_a, da_b, dim=dim, method="corr")
1337 
1338 
1339 def _cov_corr(da_a, da_b, dim=None, ddof=0, method=None):
1340     """
1341     Internal method for xr.cov() and xr.corr() so only have to
1342     sanitize the input arrays once and we don't repeat code.
1343     """
1344     # 1. Broadcast the two arrays
1345     da_a, da_b = align(da_a, da_b, join="inner", copy=False)
1346 
1347     # 2. Ignore the nans
1348     valid_values = da_a.notnull() & da_b.notnull()
1349     da_a = da_a.where(valid_values)
1350     da_b = da_b.where(valid_values)
1351     valid_count = valid_values.sum(dim) - ddof
1352 
1353     # 3. Detrend along the given dim
1354     demeaned_da_a = da_a - da_a.mean(dim=dim)
1355     demeaned_da_b = da_b - da_b.mean(dim=dim)
1356 
1357     # 4. Compute covariance along the given dim
1358     # N.B. `skipna=False` is required or there is a bug when computing
1359     # auto-covariance. E.g. Try xr.cov(da,da) for
1360     # da = xr.DataArray([[1, 2], [1, np.nan]], dims=["x", "time"])
1361     cov = (demeaned_da_a * demeaned_da_b).sum(dim=dim, skipna=True, min_count=1) / (
1362         valid_count
1363     )
1364 
1365     if method == "cov":
1366         return cov
1367 
1368     else:
1369         # compute std + corr
1370         da_a_std = da_a.std(dim=dim)
1371         da_b_std = da_b.std(dim=dim)
1372         corr = cov / (da_a_std * da_b_std)
1373         return corr
1374 
1375 
1376 def dot(*arrays, dims=None, **kwargs):
1377     """Generalized dot product for xarray objects. Like np.einsum, but
1378     provides a simpler interface based on array dimensions.
1379 
1380     Parameters
1381     ----------
1382     *arrays : DataArray or Variable
1383         Arrays to compute.
1384     dims : ..., str or tuple of str, optional
1385         Which dimensions to sum over. Ellipsis ('...') sums over all dimensions.
1386         If not specified, then all the common dimensions are summed over.
1387     **kwargs : dict
1388         Additional keyword arguments passed to numpy.einsum or
1389         dask.array.einsum
1390 
1391     Returns
1392     -------
1393     DataArray
1394 
1395     Examples
1396     --------
1397     >>> da_a = xr.DataArray(np.arange(3 * 2).reshape(3, 2), dims=["a", "b"])
1398     >>> da_b = xr.DataArray(np.arange(3 * 2 * 2).reshape(3, 2, 2), dims=["a", "b", "c"])
1399     >>> da_c = xr.DataArray(np.arange(2 * 3).reshape(2, 3), dims=["c", "d"])
1400 
1401     >>> da_a
1402     <xarray.DataArray (a: 3, b: 2)>
1403     array([[0, 1],
1404            [2, 3],
1405            [4, 5]])
1406     Dimensions without coordinates: a, b
1407 
1408     >>> da_b
1409     <xarray.DataArray (a: 3, b: 2, c: 2)>
1410     array([[[ 0,  1],
1411             [ 2,  3]],
1412     <BLANKLINE>
1413            [[ 4,  5],
1414             [ 6,  7]],
1415     <BLANKLINE>
1416            [[ 8,  9],
1417             [10, 11]]])
1418     Dimensions without coordinates: a, b, c
1419 
1420     >>> da_c
1421     <xarray.DataArray (c: 2, d: 3)>
1422     array([[0, 1, 2],
1423            [3, 4, 5]])
1424     Dimensions without coordinates: c, d
1425 
1426     >>> xr.dot(da_a, da_b, dims=["a", "b"])
1427     <xarray.DataArray (c: 2)>
1428     array([110, 125])
1429     Dimensions without coordinates: c
1430 
1431     >>> xr.dot(da_a, da_b, dims=["a"])
1432     <xarray.DataArray (b: 2, c: 2)>
1433     array([[40, 46],
1434            [70, 79]])
1435     Dimensions without coordinates: b, c
1436 
1437     >>> xr.dot(da_a, da_b, da_c, dims=["b", "c"])
1438     <xarray.DataArray (a: 3, d: 3)>
1439     array([[  9,  14,  19],
1440            [ 93, 150, 207],
1441            [273, 446, 619]])
1442     Dimensions without coordinates: a, d
1443 
1444     >>> xr.dot(da_a, da_b)
1445     <xarray.DataArray (c: 2)>
1446     array([110, 125])
1447     Dimensions without coordinates: c
1448 
1449     >>> xr.dot(da_a, da_b, dims=...)
1450     <xarray.DataArray ()>
1451     array(235)
1452     """
1453     from .dataarray import DataArray
1454     from .variable import Variable
1455 
1456     if any(not isinstance(arr, (Variable, DataArray)) for arr in arrays):
1457         raise TypeError(
1458             "Only xr.DataArray and xr.Variable are supported."
1459             "Given {}.".format([type(arr) for arr in arrays])
1460         )
1461 
1462     if len(arrays) == 0:
1463         raise TypeError("At least one array should be given.")
1464 
1465     if isinstance(dims, str):
1466         dims = (dims,)
1467 
1468     common_dims = set.intersection(*[set(arr.dims) for arr in arrays])
1469     all_dims = []
1470     for arr in arrays:
1471         all_dims += [d for d in arr.dims if d not in all_dims]
1472 
1473     einsum_axes = "abcdefghijklmnopqrstuvwxyz"
1474     dim_map = {d: einsum_axes[i] for i, d in enumerate(all_dims)}
1475 
1476     if dims is ...:
1477         dims = all_dims
1478     elif dims is None:
1479         # find dimensions that occur more than one times
1480         dim_counts = Counter()
1481         for arr in arrays:
1482             dim_counts.update(arr.dims)
1483         dims = tuple(d for d, c in dim_counts.items() if c > 1)
1484 
1485     dims = tuple(dims)  # make dims a tuple
1486 
1487     # dimensions to be parallelized
1488     broadcast_dims = tuple(d for d in all_dims if d in common_dims and d not in dims)
1489     input_core_dims = [
1490         [d for d in arr.dims if d not in broadcast_dims] for arr in arrays
1491     ]
1492     output_core_dims = [tuple(d for d in all_dims if d not in dims + broadcast_dims)]
1493 
1494     # construct einsum subscripts, such as '...abc,...ab->...c'
1495     # Note: input_core_dims are always moved to the last position
1496     subscripts_list = [
1497         "..." + "".join(dim_map[d] for d in ds) for ds in input_core_dims
1498     ]
1499     subscripts = ",".join(subscripts_list)
1500     subscripts += "->..." + "".join(dim_map[d] for d in output_core_dims[0])
1501 
1502     join = OPTIONS["arithmetic_join"]
1503     # using "inner" emulates `(a * b).sum()` for all joins (except "exact")
1504     if join != "exact":
1505         join = "inner"
1506 
1507     # subscripts should be passed to np.einsum as arg, not as kwargs. We need
1508     # to construct a partial function for apply_ufunc to work.
1509     func = functools.partial(duck_array_ops.einsum, subscripts, **kwargs)
1510     result = apply_ufunc(
1511         func,
1512         *arrays,
1513         input_core_dims=input_core_dims,
1514         output_core_dims=output_core_dims,
1515         join=join,
1516         dask="allowed",
1517     )
1518     return result.transpose(*all_dims, missing_dims="ignore")
1519 
1520 
1521 def where(cond, x, y):
1522     """Return elements from `x` or `y` depending on `cond`.
1523 
1524     Performs xarray-like broadcasting across input arguments.
1525 
1526     All dimension coordinates on `x` and `y`  must be aligned with each
1527     other and with `cond`.
1528 
1529     Parameters
1530     ----------
1531     cond : scalar, array, Variable, DataArray or Dataset
1532         When True, return values from `x`, otherwise returns values from `y`.
1533     x : scalar, array, Variable, DataArray or Dataset
1534         values to choose from where `cond` is True
1535     y : scalar, array, Variable, DataArray or Dataset
1536         values to choose from where `cond` is False
1537 
1538     Returns
1539     -------
1540     Dataset, DataArray, Variable or array
1541         In priority order: Dataset, DataArray, Variable or array, whichever
1542         type appears as an input argument.
1543 
1544     Examples
1545     --------
1546     >>> x = xr.DataArray(
1547     ...     0.1 * np.arange(10),
1548     ...     dims=["lat"],
1549     ...     coords={"lat": np.arange(10)},
1550     ...     name="sst",
1551     ... )
1552     >>> x
1553     <xarray.DataArray 'sst' (lat: 10)>
1554     array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])
1555     Coordinates:
1556       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1557 
1558     >>> xr.where(x < 0.5, x, x * 100)
1559     <xarray.DataArray 'sst' (lat: 10)>
1560     array([ 0. ,  0.1,  0.2,  0.3,  0.4, 50. , 60. , 70. , 80. , 90. ])
1561     Coordinates:
1562       * lat      (lat) int64 0 1 2 3 4 5 6 7 8 9
1563 
1564     >>> y = xr.DataArray(
1565     ...     0.1 * np.arange(9).reshape(3, 3),
1566     ...     dims=["lat", "lon"],
1567     ...     coords={"lat": np.arange(3), "lon": 10 + np.arange(3)},
1568     ...     name="sst",
1569     ... )
1570     >>> y
1571     <xarray.DataArray 'sst' (lat: 3, lon: 3)>
1572     array([[0. , 0.1, 0.2],
1573            [0.3, 0.4, 0.5],
1574            [0.6, 0.7, 0.8]])
1575     Coordinates:
1576       * lat      (lat) int64 0 1 2
1577       * lon      (lon) int64 10 11 12
1578 
1579     >>> xr.where(y.lat < 1, y, -1)
1580     <xarray.DataArray (lat: 3, lon: 3)>
1581     array([[ 0. ,  0.1,  0.2],
1582            [-1. , -1. , -1. ],
1583            [-1. , -1. , -1. ]])
1584     Coordinates:
1585       * lat      (lat) int64 0 1 2
1586       * lon      (lon) int64 10 11 12
1587 
1588     >>> cond = xr.DataArray([True, False], dims=["x"])
1589     >>> x = xr.DataArray([1, 2], dims=["y"])
1590     >>> xr.where(cond, x, 0)
1591     <xarray.DataArray (x: 2, y: 2)>
1592     array([[1, 2],
1593            [0, 0]])
1594     Dimensions without coordinates: x, y
1595 
1596     See Also
1597     --------
1598     numpy.where : corresponding numpy function
1599     Dataset.where, DataArray.where :
1600         equivalent methods
1601     """
1602     # alignment for three arguments is complicated, so don't support it yet
1603     return apply_ufunc(
1604         duck_array_ops.where,
1605         cond,
1606         x,
1607         y,
1608         join="exact",
1609         dataset_join="exact",
1610         dask="allowed",
1611     )
1612 
1613 
1614 def polyval(coord, coeffs, degree_dim="degree"):
1615     """Evaluate a polynomial at specific values
1616 
1617     Parameters
1618     ----------
1619     coord : DataArray
1620         The 1D coordinate along which to evaluate the polynomial.
1621     coeffs : DataArray
1622         Coefficients of the polynomials.
1623     degree_dim : str, default: "degree"
1624         Name of the polynomial degree dimension in `coeffs`.
1625 
1626     See Also
1627     --------
1628     xarray.DataArray.polyfit
1629     numpy.polyval
1630     """
1631     from .dataarray import DataArray
1632     from .missing import get_clean_interp_index
1633 
1634     x = get_clean_interp_index(coord, coord.name, strict=False)
1635 
1636     deg_coord = coeffs[degree_dim]
1637 
1638     lhs = DataArray(
1639         np.vander(x, int(deg_coord.max()) + 1),
1640         dims=(coord.name, degree_dim),
1641         coords={coord.name: coord, degree_dim: np.arange(deg_coord.max() + 1)[::-1]},
1642     )
1643     return (lhs * coeffs).sum(degree_dim)
1644 
1645 
1646 def _calc_idxminmax(
1647     *,
1648     array,
1649     func: Callable,
1650     dim: Hashable = None,
1651     skipna: bool = None,
1652     fill_value: Any = dtypes.NA,
1653     keep_attrs: bool = None,
1654 ):
1655     """Apply common operations for idxmin and idxmax."""
1656     # This function doesn't make sense for scalars so don't try
1657     if not array.ndim:
1658         raise ValueError("This function does not apply for scalars")
1659 
1660     if dim is not None:
1661         pass  # Use the dim if available
1662     elif array.ndim == 1:
1663         # it is okay to guess the dim if there is only 1
1664         dim = array.dims[0]
1665     else:
1666         # The dim is not specified and ambiguous.  Don't guess.
1667         raise ValueError("Must supply 'dim' argument for multidimensional arrays")
1668 
1669     if dim not in array.dims:
1670         raise KeyError(f'Dimension "{dim}" not in dimension')
1671     if dim not in array.coords:
1672         raise KeyError(f'Dimension "{dim}" does not have coordinates')
1673 
1674     # These are dtypes with NaN values argmin and argmax can handle
1675     na_dtypes = "cfO"
1676 
1677     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
1678         # Need to skip NaN values since argmin and argmax can't handle them
1679         allna = array.isnull().all(dim)
1680         array = array.where(~allna, 0)
1681 
1682     # This will run argmin or argmax.
1683     indx = func(array, dim=dim, axis=None, keep_attrs=keep_attrs, skipna=skipna)
1684 
1685     # Handle dask arrays.
1686     if is_duck_dask_array(array.data):
1687         import dask.array
1688 
1689         chunks = dict(zip(array.dims, array.chunks))
1690         dask_coord = dask.array.from_array(array[dim].data, chunks=chunks[dim])
1691         res = indx.copy(data=dask_coord[indx.data.ravel()].reshape(indx.shape))
1692         # we need to attach back the dim name
1693         res.name = dim
1694     else:
1695         res = array[dim][(indx,)]
1696         # The dim is gone but we need to remove the corresponding coordinate.
1697         del res.coords[dim]
1698 
1699     if skipna or (skipna is None and array.dtype.kind in na_dtypes):
1700         # Put the NaN values back in after removing them
1701         res = res.where(~allna, fill_value)
1702 
1703     # Copy attributes from argmin/argmax, if any
1704     res.attrs = indx.attrs
1705 
1706     return res
1707 
1708 
1709 def unify_chunks(*objects: T_Xarray) -> Tuple[T_Xarray, ...]:
1710     """
1711     Given any number of Dataset and/or DataArray objects, returns
1712     new objects with unified chunk size along all chunked dimensions.
1713 
1714     Returns
1715     -------
1716     unified (DataArray or Dataset) – Tuple of objects with the same type as
1717     *objects with consistent chunk sizes for all dask-array variables
1718 
1719     See Also
1720     --------
1721     dask.array.core.unify_chunks
1722     """
1723     from .dataarray import DataArray
1724 
1725     # Convert all objects to datasets
1726     datasets = [
1727         obj._to_temp_dataset() if isinstance(obj, DataArray) else obj.copy()
1728         for obj in objects
1729     ]
1730 
1731     # Get argumets to pass into dask.array.core.unify_chunks
1732     unify_chunks_args = []
1733     sizes: dict[Hashable, int] = {}
1734     for ds in datasets:
1735         for v in ds._variables.values():
1736             if v.chunks is not None:
1737                 # Check that sizes match across different datasets
1738                 for dim, size in v.sizes.items():
1739                     try:
1740                         if sizes[dim] != size:
1741                             raise ValueError(
1742                                 f"Dimension {dim!r} size mismatch: {sizes[dim]} != {size}"
1743                             )
1744                     except KeyError:
1745                         sizes[dim] = size
1746                 unify_chunks_args += [v._data, v._dims]
1747 
1748     # No dask arrays: Return inputs
1749     if not unify_chunks_args:
1750         return objects
1751 
1752     # Run dask.array.core.unify_chunks
1753     from dask.array.core import unify_chunks
1754 
1755     _, dask_data = unify_chunks(*unify_chunks_args)
1756     dask_data_iter = iter(dask_data)
1757     out = []
1758     for obj, ds in zip(objects, datasets):
1759         for k, v in ds._variables.items():
1760             if v.chunks is not None:
1761                 ds._variables[k] = v.copy(data=next(dask_data_iter))
1762         out.append(obj._from_temp_dataset(ds) if isinstance(obj, DataArray) else ds)
1763 
1764     return tuple(out)

</code><intended_behavior>This file should include a new `cross` function that mimics the structure and approach used by the existing `dot` function but instead employs `numpy.cross` to compute the cross product of two `DataArray` objects. The function should handle input validation (e.g., ensuring that the spatial dimension has a size of 3 for 3D vectors or 2 for 2D vectors), work with dask arrays (using `dask='parallelized'` when applicable), and respect xarray's dimension handling conventions. This new function will extend xarray's computational capabilities to include vector cross products, filling a gap for users working on physical applications requiring such operations.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
218 class DataArray(AbstractArray, DataWithCoords, DataArrayArithmetic):
219     """N-dimensional array with labeled coordinates and dimensions.
220 
221     DataArray provides a wrapper around numpy ndarrays that uses
222     labeled dimensions and coordinates to support metadata aware
223     operations. The API is similar to that for the pandas Series or
224     DataFrame, but DataArray objects can have any number of dimensions,
225     and their contents have fixed data types.
226 
227     Additional features over raw numpy arrays:
228 
229     - Apply operations over dimensions by name: ``x.sum('time')``.
230     - Select or assign values by integer location (like numpy):
231       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
232       ``x.sel(time='2014-01-01')``.
233     - Mathematical operations (e.g., ``x - y``) vectorize across
234       multiple dimensions (known in numpy as "broadcasting") based on
235       dimension names, regardless of their original order.
236     - Keep track of arbitrary metadata in the form of a Python
237       dictionary: ``x.attrs``
238     - Convert to a pandas Series: ``x.to_series()``.
239 
240     Getting items from or doing mathematical operations with a
241     DataArray always returns another DataArray.
242 
243     Parameters
244     ----------
245     data : array_like
246         Values for this array. Must be an ``numpy.ndarray``, ndarray
247         like, or castable to an ``ndarray``. If a self-described xarray
248         or pandas object, attempts are made to use this array's
249         metadata to fill in other unspecified arguments. A view of the
250         array's data is used instead of a copy if possible.
251     coords : sequence or dict of array_like, optional
252         Coordinates (tick labels) to use for indexing along each
253         dimension. The following notations are accepted:
254 
255         - mapping {dimension name: array-like}
256         - sequence of tuples that are valid arguments for
257           ``xarray.Variable()``
258           - (dims, data)
259           - (dims, data, attrs)
260           - (dims, data, attrs, encoding)
261 
262         Additionally, it is possible to define a coord whose name
263         does not match the dimension name, or a coord based on multiple
264         dimensions, with one of the following notations:
265 
266         - mapping {coord name: DataArray}
267         - mapping {coord name: Variable}
268         - mapping {coord name: (dimension name, array-like)}
269         - mapping {coord name: (tuple of dimension names, array-like)}
270 
271     dims : hashable or sequence of hashable, optional
272         Name(s) of the data dimension(s). Must be either a hashable
273         (only for 1D data) or a sequence of hashables with length equal
274         to the number of dimensions. If this argument is omitted,
275         dimension names are taken from ``coords`` (if possible) and
276         otherwise default to ``['dim_0', ... 'dim_n']``.
277     name : str or None, optional
278         Name of this array.
279     attrs : dict_like or None, optional
280         Attributes to assign to the new instance. By default, an empty
281         attribute dictionary is initialized.
282 
283     Examples
284     --------
285     Create data:
286 
287     >>> np.random.seed(0)
288     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
289     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
290     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
291     >>> time = pd.date_range("2014-09-06", periods=3)
292     >>> reference_time = pd.Timestamp("2014-09-05")
293 
294     Initialize a dataarray with multiple dimensions:
295 
296     >>> da = xr.DataArray(
297     ...     data=temperature,
298     ...     dims=["x", "y", "time"],
299     ...     coords=dict(
300     ...         lon=(["x", "y"], lon),
301     ...         lat=(["x", "y"], lat),
302     ...         time=time,
303     ...         reference_time=reference_time,
304     ...     ),
305     ...     attrs=dict(
306     ...         description="Ambient temperature.",
307     ...         units="degC",
308     ...     ),
309     ... )
310     >>> da
311     <xarray.DataArray (x: 2, y: 2, time: 3)>
312     array([[[29.11241877, 18.20125767, 22.82990387],
313             [32.92714559, 29.94046392,  7.18177696]],
314     <BLANKLINE>
315            [[22.60070734, 13.78914233, 14.17424919],
316             [18.28478802, 16.15234857, 26.63418806]]])
317     Coordinates:
318         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
319         lat             (x, y) float64 42.25 42.21 42.63 42.59
320       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
321         reference_time  datetime64[ns] 2014-09-05
322     Dimensions without coordinates: x, y
323     Attributes:
324         description:  Ambient temperature.
325         units:        degC
326 
327     Find out where the coldest temperature was:
328 
329     >>> da.isel(da.argmin(...))
330     <xarray.DataArray ()>
331     array(7.18177696)
332     Coordinates:
333         lon             float64 -99.32
334         lat             float64 42.21
335         time            datetime64[ns] 2014-09-08
336         reference_time  datetime64[ns] 2014-09-05
337     Attributes:
338         description:  Ambient temperature.
339         units:        degC
340     """
341 
342     _cache: Dict[str, Any]
343     _coords: Dict[Any, Variable]
344     _close: Optional[Callable[[], None]]
345     _indexes: Optional[Dict[Hashable, Index]]
346     _name: Optional[Hashable]
347     _variable: Variable
348 
349     __slots__ = (
350         "_cache",
351         "_coords",
352         "_close",
353         "_indexes",
354         "_name",
355         "_variable",
356         "__weakref__",
357     )
358 
359     _groupby_cls = groupby.DataArrayGroupBy
360     _rolling_cls = rolling.DataArrayRolling
361     _coarsen_cls = rolling.DataArrayCoarsen
362     _resample_cls = resample.DataArrayResample
363     _weighted_cls = weighted.DataArrayWeighted
364 
365     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor)
366 
367     def __init__(
368         self,
369         data: Any = dtypes.NA,
370         coords: Union[Sequence[Tuple], Mapping[Any, Any], None] = None,
371         dims: Union[Hashable, Sequence[Hashable], None] = None,
372         name: Hashable = None,
373         attrs: Mapping = None,
374         # internal parameters
375         indexes: Dict[Hashable, pd.Index] = None,
376         fastpath: bool = False,
377     ):
378         if fastpath:
379             variable = data
380             assert dims is None
381             assert attrs is None
382         else:
383             # try to fill in arguments from data if they weren't supplied
384             if coords is None:
385 
386                 if isinstance(data, DataArray):
387                     coords = data.coords
388                 elif isinstance(data, pd.Series):
389                     coords = [data.index]
390                 elif isinstance(data, pd.DataFrame):
391                     coords = [data.index, data.columns]
392                 elif isinstance(data, (pd.Index, IndexVariable)):
393                     coords = [data]
394                 elif isinstance(data, pdcompat.Panel):
395                     coords = [data.items, data.major_axis, data.minor_axis]
396 
397             if dims is None:
398                 dims = getattr(data, "dims", getattr(coords, "dims", None))
399             if name is None:
400                 name = getattr(data, "name", None)
401             if attrs is None and not isinstance(data, PANDAS_TYPES):
402                 attrs = getattr(data, "attrs", None)
403 
404             data = _check_data_shape(data, coords, dims)
405             data = as_compatible_data(data)
406             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
407             variable = Variable(dims, data, attrs, fastpath=True)
408             indexes = dict(
409                 _extract_indexes_from_coords(coords)
410             )  # needed for to_dataset
411 
412         # These fully describe a DataArray
413         self._variable = variable
414         assert isinstance(coords, dict)
415         self._coords = coords
416         self._name = name
417 
418         # TODO(shoyer): document this argument, once it becomes part of the
419         # public interface.
420         self._indexes = indexes
421 
422         self._close = None
423 
424     def _replace(
425         self: T_DataArray,
426         variable: Variable = None,
427         coords=None,
428         name: Union[Hashable, None, Default] = _default,
429         indexes=None,
430     ) -> T_DataArray:
431         if variable is None:
432             variable = self.variable
433         if coords is None:
434             coords = self._coords
435         if name is _default:
436             name = self.name
437         return type(self)(variable, coords, name=name, fastpath=True, indexes=indexes)
438 
439     def _replace_maybe_drop_dims(
440         self, variable: Variable, name: Union[Hashable, None, Default] = _default
441     ) -> "DataArray":
442         if variable.dims == self.dims and variable.shape == self.shape:
443             coords = self._coords.copy()
444             indexes = self._indexes
445         elif variable.dims == self.dims:
446             # Shape has changed (e.g. from reduce(..., keepdims=True)
447             new_sizes = dict(zip(self.dims, variable.shape))
448             coords = {
449                 k: v
450                 for k, v in self._coords.items()
451                 if v.shape == tuple(new_sizes[d] for d in v.dims)
452             }
453             changed_dims = [
454                 k for k in variable.dims if variable.sizes[k] != self.sizes[k]
455             ]
456             indexes = propagate_indexes(self._indexes, exclude=changed_dims)
457         else:
458             allowed_dims = set(variable.dims)
459             coords = {
460                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
461             }
462             indexes = propagate_indexes(
463                 self._indexes, exclude=(set(self.dims) - allowed_dims)
464             )
465         return self._replace(variable, coords, name, indexes=indexes)
466 
467     def _overwrite_indexes(self, indexes: Mapping[Any, Any]) -> "DataArray":
468         if not len(indexes):
469             return self
470         coords = self._coords.copy()
471         for name, idx in indexes.items():
472             coords[name] = IndexVariable(name, idx.to_pandas_index())
473         obj = self._replace(coords=coords)
474 
475         # switch from dimension to level names, if necessary
476         dim_names: Dict[Any, str] = {}
477         for dim, idx in indexes.items():
478             pd_idx = idx.to_pandas_index()
479             if not isinstance(idx, pd.MultiIndex) and pd_idx.name != dim:
480                 dim_names[dim] = idx.name
481         if dim_names:
482             obj = obj.rename(dim_names)
483         return obj
484 
485     def _to_temp_dataset(self) -> Dataset:
486         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
487 
488     def _from_temp_dataset(
489         self, dataset: Dataset, name: Union[Hashable, None, Default] = _default
490     ) -> "DataArray":
491         variable = dataset._variables.pop(_THIS_ARRAY)
492         coords = dataset._variables
493         indexes = dataset._indexes
494         return self._replace(variable, coords, name, indexes=indexes)
495 
496     def _to_dataset_split(self, dim: Hashable) -> Dataset:
497         """splits dataarray along dimension 'dim'"""
498 
499         def subset(dim, label):
500             array = self.loc[{dim: label}]
501             array.attrs = {}
502             return as_variable(array)
503 
504         variables = {label: subset(dim, label) for label in self.get_index(dim)}
505         variables.update({k: v for k, v in self._coords.items() if k != dim})
506         indexes = propagate_indexes(self._indexes, exclude=dim)
507         coord_names = set(self._coords) - {dim}
508         dataset = Dataset._construct_direct(
509             variables, coord_names, indexes=indexes, attrs=self.attrs
510         )
511         return dataset
512 
513     def _to_dataset_whole(
514         self, name: Hashable = None, shallow_copy: bool = True
515     ) -> Dataset:
516         if name is None:
517             name = self.name
518         if name is None:
519             raise ValueError(
520                 "unable to convert unnamed DataArray to a "
521                 "Dataset without providing an explicit name"
522             )
523         if name in self.coords:
524             raise ValueError(
525                 "cannot create a Dataset from a DataArray with "
526                 "the same name as one of its coordinates"
527             )
528         # use private APIs for speed: this is called by _to_temp_dataset(),
529         # which is used in the guts of a lot of operations (e.g., reindex)
530         variables = self._coords.copy()
531         variables[name] = self.variable
532         if shallow_copy:
533             for k in variables:
534                 variables[k] = variables[k].copy(deep=False)
535         indexes = self._indexes
536 
537         coord_names = set(self._coords)
538         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
539 
540     def to_dataset(
541         self,
542         dim: Hashable = None,
543         *,
544         name: Hashable = None,
545         promote_attrs: bool = False,
546     ) -> Dataset:
547         """Convert a DataArray to a Dataset.
548 
549         Parameters
550         ----------
551         dim : hashable, optional
552             Name of the dimension on this array along which to split this array
553             into separate variables. If not provided, this array is converted
554             into a Dataset of one variable.
555         name : hashable, optional
556             Name to substitute for this array's name. Only valid if ``dim`` is
557             not provided.
558         promote_attrs : bool, default: False
559             Set to True to shallow copy attrs of DataArray to returned Dataset.
560 
561         Returns
562         -------
563         dataset : Dataset
564         """
565         if dim is not None and dim not in self.dims:
566             raise TypeError(
567                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
568             )
569 
570         if dim is not None:
571             if name is not None:
572                 raise TypeError("cannot supply both dim and name arguments")
573             result = self._to_dataset_split(dim)
574         else:
575             result = self._to_dataset_whole(name)
576 
577         if promote_attrs:
578             result.attrs = dict(self.attrs)
579 
580         return result
581 
582     @property
583     def name(self) -> Optional[Hashable]:
584         """The name of this array."""
585         return self._name
586 
587     @name.setter
588     def name(self, value: Optional[Hashable]) -> None:
589         self._name = value
590 
591     @property
592     def variable(self) -> Variable:
593         """Low level interface to the Variable object for this DataArray."""
594         return self._variable
595 
596     @property
597     def dtype(self) -> np.dtype:
598         return self.variable.dtype
599 
600     @property
601     def shape(self) -> Tuple[int, ...]:
602         return self.variable.shape
603 
604     @property
605     def size(self) -> int:
606         return self.variable.size
607 
608     @property
609     def nbytes(self) -> int:
610         return self.variable.nbytes
611 
612     @property
613     def ndim(self) -> int:
614         return self.variable.ndim
615 
616     def __len__(self) -> int:
617         return len(self.variable)
618 
619     @property
620     def data(self) -> Any:
621         """
622         The DataArray's data as an array. The underlying array type
623         (e.g. dask, sparse, pint) is preserved.
624 
625         See Also
626         --------
627         DataArray.to_numpy
628         DataArray.as_numpy
629         DataArray.values
630         """
631         return self.variable.data
632 
633     @data.setter
634     def data(self, value: Any) -> None:
635         self.variable.data = value
636 
637     @property
638     def values(self) -> np.ndarray:
639         """
640         The array's data as a numpy.ndarray.
641 
642         If the array's data is not a numpy.ndarray this will attempt to convert
643         it naively using np.array(), which will raise an error if the array
644         type does not support coercion like this (e.g. cupy).
645         """
646         return self.variable.values
647 
648     @values.setter
649     def values(self, value: Any) -> None:
650         self.variable.values = value
651 
652     def to_numpy(self) -> np.ndarray:
653         """
654         Coerces wrapped data to numpy and returns a numpy.ndarray.
655 
656         See Also
657         --------
658         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
659         Dataset.as_numpy
660         DataArray.values
661         DataArray.data
662         """
663         return self.variable.to_numpy()
664 
665     def as_numpy(self: T_DataArray) -> T_DataArray:
666         """
667         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
668 
669         See Also
670         --------
671         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
672         Dataset.as_numpy : Converts all variables in a Dataset.
673         DataArray.values
674         DataArray.data
675         """
676         coords = {k: v.as_numpy() for k, v in self._coords.items()}
677         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
678 
679     @property
680     def _in_memory(self) -> bool:
681         return self.variable._in_memory
682 
683     def to_index(self) -> pd.Index:
684         """Convert this variable to a pandas.Index. Only possible for 1D
685         arrays.
686         """
687         return self.variable.to_index()
688 
689     @property
690     def dims(self) -> Tuple[Hashable, ...]:
691         """Tuple of dimension names associated with this array.
692 
693         Note that the type of this property is inconsistent with
694         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
695         consistently named properties.
696         """
697         return self.variable.dims
698 
699     @dims.setter
700     def dims(self, value):
701         raise AttributeError(
702             "you cannot assign dims on a DataArray. Use "
703             ".rename() or .swap_dims() instead."
704         )
705 
706     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
707         if utils.is_dict_like(key):
708             return key
709         key = indexing.expanded_indexer(key, self.ndim)
710         return dict(zip(self.dims, key))
711 
712     @property
713     def _level_coords(self) -> Dict[Hashable, Hashable]:
714         """Return a mapping of all MultiIndex levels and their corresponding
715         coordinate name.
716         """
717         level_coords: Dict[Hashable, Hashable] = {}
718 
719         for cname, var in self._coords.items():
720             if var.ndim == 1 and isinstance(var, IndexVariable):
721                 level_names = var.level_names
722                 if level_names is not None:
723                     (dim,) = var.dims
724                     level_coords.update({lname: dim for lname in level_names})
725         return level_coords
726 
727     def _getitem_coord(self, key):
728         from .dataset import _get_virtual_variable
729 
730         try:
731             var = self._coords[key]
732         except KeyError:
733             dim_sizes = dict(zip(self.dims, self.shape))
734             _, key, var = _get_virtual_variable(
735                 self._coords, key, self._level_coords, dim_sizes
736             )
737 
738         return self._replace_maybe_drop_dims(var, name=key)
739 
740     def __getitem__(self, key: Any) -> "DataArray":
741         if isinstance(key, str):
742             return self._getitem_coord(key)
743         else:
744             # xarray-style array indexing
745             return self.isel(indexers=self._item_key_to_dict(key))
746 
747     def __setitem__(self, key: Any, value: Any) -> None:
748         if isinstance(key, str):
749             self.coords[key] = value
750         else:
751             # Coordinates in key, value and self[key] should be consistent.
752             # TODO Coordinate consistency in key is checked here, but it
753             # causes unnecessary indexing. It should be optimized.
754             obj = self[key]
755             if isinstance(value, DataArray):
756                 assert_coordinate_consistent(value, obj.coords.variables)
757             # DataArray key -> Variable key
758             key = {
759                 k: v.variable if isinstance(v, DataArray) else v
760                 for k, v in self._item_key_to_dict(key).items()
761             }
762             self.variable[key] = value
763 
764     def __delitem__(self, key: Any) -> None:
765         del self.coords[key]
766 
767     @property
768     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
769         """Places to look-up items for attribute-style access"""
770         yield from self._item_sources
771         yield self.attrs
772 
773     @property
774     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
775         """Places to look-up items for key-completion"""
776         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
777 
778         # virtual coordinates
779         # uses empty dict -- everything here can already be found in self.coords.
780         yield HybridMappingProxy(keys=self.dims, mapping={})
781         yield HybridMappingProxy(keys=self._level_coords, mapping={})
782 
783     def __contains__(self, key: Any) -> bool:
784         return key in self.data
785 
786     @property
787     def loc(self) -> _LocIndexer:
788         """Attribute for location based indexing like pandas."""
789         return _LocIndexer(self)
790 
791     @property
792     # Key type needs to be `Any` because of mypy#4167
793     def attrs(self) -> Dict[Any, Any]:
794         """Dictionary storing arbitrary metadata with this array."""
795         return self.variable.attrs
796 
797     @attrs.setter
798     def attrs(self, value: Mapping[Any, Any]) -> None:
799         # Disable type checking to work around mypy bug - see mypy#4167
800         self.variable.attrs = value  # type: ignore[assignment]
801 
802     @property
803     def encoding(self) -> Dict[Hashable, Any]:
804         """Dictionary of format-specific settings for how this array should be
805         serialized."""
806         return self.variable.encoding
807 
808     @encoding.setter
809     def encoding(self, value: Mapping[Any, Any]) -> None:
810         self.variable.encoding = value
811 
812     @property
813     def indexes(self) -> Indexes:
814         """Mapping of pandas.Index objects used for label based indexing.
815 
816         Raises an error if this Dataset has indexes that cannot be coerced
817         to pandas.Index objects.
818 
819         See Also
820         --------
821         DataArray.xindexes
822 
823         """
824         return Indexes({k: idx.to_pandas_index() for k, idx in self.xindexes.items()})
825 
826     @property
827     def xindexes(self) -> Indexes:
828         """Mapping of xarray Index objects used for label based indexing."""
829         if self._indexes is None:
830             self._indexes = default_indexes(self._coords, self.dims)
831         return Indexes(self._indexes)
832 
833     @property
834     def coords(self) -> DataArrayCoordinates:
835         """Dictionary-like container of coordinate arrays."""
836         return DataArrayCoordinates(self)
837 
838     def reset_coords(
839         self,
840         names: Union[Iterable[Hashable], Hashable, None] = None,
841         drop: bool = False,
842     ) -> Union[None, "DataArray", Dataset]:
843         """Given names of coordinates, reset them to become variables.
844 
845         Parameters
846         ----------
847         names : hashable or iterable of hashable, optional
848             Name(s) of non-index coordinates in this dataset to reset into
849             variables. By default, all non-index coordinates are reset.
850         drop : bool, optional
851             If True, remove coordinates instead of converting them into
852             variables.
853 
854         Returns
855         -------
856         Dataset, or DataArray if ``drop == True``
857         """
858         if names is None:
859             names = set(self.coords) - set(self.dims)
860         dataset = self.coords.to_dataset().reset_coords(names, drop)
861         if drop:
862             return self._replace(coords=dataset._variables)
863         if self.name is None:
864             raise ValueError(
865                 "cannot reset_coords with drop=False on an unnamed DataArrray"
866             )
867         dataset[self.name] = self.variable
868         return dataset
869 
870     def __dask_tokenize__(self):
871         from dask.base import normalize_token
872 
873         return normalize_token((type(self), self._variable, self._coords, self._name))
874 
875     def __dask_graph__(self):
876         return self._to_temp_dataset().__dask_graph__()
877 
878     def __dask_keys__(self):
879         return self._to_temp_dataset().__dask_keys__()
880 
881     def __dask_layers__(self):
882         return self._to_temp_dataset().__dask_layers__()
883 
884     @property
885     def __dask_optimize__(self):
886         return self._to_temp_dataset().__dask_optimize__
887 
888     @property
889     def __dask_scheduler__(self):
890         return self._to_temp_dataset().__dask_scheduler__
891 
892     def __dask_postcompute__(self):
893         func, args = self._to_temp_dataset().__dask_postcompute__()
894         return self._dask_finalize, (self.name, func) + args
895 
896     def __dask_postpersist__(self):
897         func, args = self._to_temp_dataset().__dask_postpersist__()
898         return self._dask_finalize, (self.name, func) + args
899 
900     @staticmethod
901     def _dask_finalize(results, name, func, *args, **kwargs):
902         ds = func(results, *args, **kwargs)
903         variable = ds._variables.pop(_THIS_ARRAY)
904         coords = ds._variables
905         return DataArray(variable, coords, name=name, fastpath=True)
906 
907     def load(self, **kwargs) -> "DataArray":
908         """Manually trigger loading of this array's data from disk or a
909         remote source into memory and return this array.
910 
911         Normally, it should not be necessary to call this method in user code,
912         because all xarray functions should either work on deferred data or
913         load data automatically. However, this method can be necessary when
914         working with many file objects on disk.
915 
916         Parameters
917         ----------
918         **kwargs : dict
919             Additional keyword arguments passed on to ``dask.compute``.
920 
921         See Also
922         --------
923         dask.compute
924         """
925         ds = self._to_temp_dataset().load(**kwargs)
926         new = self._from_temp_dataset(ds)
927         self._variable = new._variable
928         self._coords = new._coords
929         return self
930 
931     def compute(self, **kwargs) -> "DataArray":
932         """Manually trigger loading of this array's data from disk or a
933         remote source into memory and return a new array. The original is
934         left unaltered.
935 
936         Normally, it should not be necessary to call this method in user code,
937         because all xarray functions should either work on deferred data or
938         load data automatically. However, this method can be necessary when
939         working with many file objects on disk.
940 
941         Parameters
942         ----------
943         **kwargs : dict
944             Additional keyword arguments passed on to ``dask.compute``.
945 
946         See Also
947         --------
948         dask.compute
949         """
950         new = self.copy(deep=False)
951         return new.load(**kwargs)
952 
953     def persist(self, **kwargs) -> "DataArray":
954         """Trigger computation in constituent dask arrays
955 
956         This keeps them as dask arrays but encourages them to keep data in
957         memory.  This is particularly useful when on a distributed machine.
958         When on a single machine consider using ``.compute()`` instead.
959 
960         Parameters
961         ----------
962         **kwargs : dict
963             Additional keyword arguments passed on to ``dask.persist``.
964 
965         See Also
966         --------
967         dask.persist
968         """
969         ds = self._to_temp_dataset().persist(**kwargs)
970         return self._from_temp_dataset(ds)
971 
972     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
973         """Returns a copy of this array.
974 
975         If `deep=True`, a deep copy is made of the data array.
976         Otherwise, a shallow copy is made, and the returned data array's
977         values are a new view of this data array's values.
978 
979         Use `data` to create a new object with the same structure as
980         original but entirely new data.
981 
982         Parameters
983         ----------
984         deep : bool, optional
985             Whether the data array and its coordinates are loaded into memory
986             and copied onto the new object. Default is True.
987         data : array_like, optional
988             Data to use in the new object. Must have same shape as original.
989             When `data` is used, `deep` is ignored for all data variables,
990             and only used for coords.
991 
992         Returns
993         -------
994         object : DataArray
995             New object with dimensions, attributes, coordinates, name,
996             encoding, and optionally data copied from original.
997 
998         Examples
999         --------
1000         Shallow versus deep copy
1001 
1002         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1003         >>> array.copy()
1004         <xarray.DataArray (x: 3)>
1005         array([1, 2, 3])
1006         Coordinates:
1007           * x        (x) <U1 'a' 'b' 'c'
1008         >>> array_0 = array.copy(deep=False)
1009         >>> array_0[0] = 7
1010         >>> array_0
1011         <xarray.DataArray (x: 3)>
1012         array([7, 2, 3])
1013         Coordinates:
1014           * x        (x) <U1 'a' 'b' 'c'
1015         >>> array
1016         <xarray.DataArray (x: 3)>
1017         array([7, 2, 3])
1018         Coordinates:
1019           * x        (x) <U1 'a' 'b' 'c'
1020 
1021         Changing the data using the ``data`` argument maintains the
1022         structure of the original object, but with the new data. Original
1023         object is unaffected.
1024 
1025         >>> array.copy(data=[0.1, 0.2, 0.3])
1026         <xarray.DataArray (x: 3)>
1027         array([0.1, 0.2, 0.3])
1028         Coordinates:
1029           * x        (x) <U1 'a' 'b' 'c'
1030         >>> array
1031         <xarray.DataArray (x: 3)>
1032         array([7, 2, 3])
1033         Coordinates:
1034           * x        (x) <U1 'a' 'b' 'c'
1035 
1036         See Also
1037         --------
1038         pandas.DataFrame.copy
1039         """
1040         variable = self.variable.copy(deep=deep, data=data)
1041         coords = {k: v.copy(deep=deep) for k, v in self._coords.items()}
1042         if self._indexes is None:
1043             indexes = self._indexes
1044         else:
1045             indexes = {k: v.copy(deep=deep) for k, v in self._indexes.items()}
1046         return self._replace(variable, coords, indexes=indexes)
1047 
1048     def __copy__(self) -> "DataArray":
1049         return self.copy(deep=False)
1050 
1051     def __deepcopy__(self, memo=None) -> "DataArray":
1052         # memo does nothing but is required for compatibility with
1053         # copy.deepcopy
1054         return self.copy(deep=True)
1055 
1056     # mutable objects should not be hashable
1057     # https://github.com/python/mypy/issues/4266
1058     __hash__ = None  # type: ignore[assignment]
1059 
1060     @property
1061     def chunks(self) -> Optional[Tuple[Tuple[int, ...], ...]]:
1062         """
1063         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1064         the underlying data is not a dask array.
1065 
1066         See Also
1067         --------
1068         DataArray.chunk
1069         DataArray.chunksizes
1070         xarray.unify_chunks
1071         """
1072         return self.variable.chunks
1073 
1074     @property
1075     def chunksizes(self) -> Mapping[Any, Tuple[int, ...]]:
1076         """
1077         Mapping from dimension names to block lengths for this dataarray's data, or None if
1078         the underlying data is not a dask array.
1079         Cannot be modified directly, but can be modified by calling .chunk().
1080 
1081         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1082         instead of a tuple of chunk shapes.
1083 
1084         See Also
1085         --------
1086         DataArray.chunk
1087         DataArray.chunks
1088         xarray.unify_chunks
1089         """
1090         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1091         return get_chunksizes(all_variables)
1092 
1093     def chunk(
1094         self,
1095         chunks: Union[
1096             int,
1097             Tuple[int, ...],
1098             Tuple[Tuple[int, ...], ...],
1099             Mapping[Any, Union[None, int, Tuple[int, ...]]],
1100         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1101         name_prefix: str = "xarray-",
1102         token: str = None,
1103         lock: bool = False,
1104     ) -> "DataArray":
1105         """Coerce this array's data into a dask arrays with the given chunks.
1106 
1107         If this variable is a non-dask array, it will be converted to dask
1108         array. If it's a dask array, it will be rechunked to the given chunk
1109         sizes.
1110 
1111         If neither chunks is not provided for one or more dimensions, chunk
1112         sizes along that dimension will not be updated; non-dask arrays will be
1113         converted into dask arrays with a single block.
1114 
1115         Parameters
1116         ----------
1117         chunks : int, tuple of int or mapping of hashable to int, optional
1118             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1119             ``{'x': 5, 'y': 5}``.
1120         name_prefix : str, optional
1121             Prefix for the name of the new dask array.
1122         token : str, optional
1123             Token uniquely identifying this array.
1124         lock : optional
1125             Passed on to :py:func:`dask.array.from_array`, if the array is not
1126             already as dask array.
1127 
1128         Returns
1129         -------
1130         chunked : xarray.DataArray
1131         """
1132         if isinstance(chunks, (tuple, list)):
1133             chunks = dict(zip(self.dims, chunks))
1134 
1135         ds = self._to_temp_dataset().chunk(
1136             chunks, name_prefix=name_prefix, token=token, lock=lock
1137         )
1138         return self._from_temp_dataset(ds)
1139 
1140     def isel(
1141         self,
1142         indexers: Mapping[Any, Any] = None,
1143         drop: bool = False,
1144         missing_dims: str = "raise",
1145         **indexers_kwargs: Any,
1146     ) -> "DataArray":
1147         """Return a new DataArray whose data is given by integer indexing
1148         along the specified dimension(s).
1149 
1150         Parameters
1151         ----------
1152         indexers : dict, optional
1153             A dict with keys matching dimensions and values given
1154             by integers, slice objects or arrays.
1155             indexer can be a integer, slice, array-like or DataArray.
1156             If DataArrays are passed as indexers, xarray-style indexing will be
1157             carried out. See :ref:`indexing` for the details.
1158             One of indexers or indexers_kwargs must be provided.
1159         drop : bool, optional
1160             If ``drop=True``, drop coordinates variables indexed by integers
1161             instead of making them scalar.
1162         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1163             What to do if dimensions that should be selected from are not present in the
1164             DataArray:
1165             - "raise": raise an exception
1166             - "warn": raise a warning, and ignore the missing dimensions
1167             - "ignore": ignore the missing dimensions
1168         **indexers_kwargs : {dim: indexer, ...}, optional
1169             The keyword arguments form of ``indexers``.
1170 
1171         See Also
1172         --------
1173         Dataset.isel
1174         DataArray.sel
1175 
1176         Examples
1177         --------
1178         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1179         >>> da
1180         <xarray.DataArray (x: 5, y: 5)>
1181         array([[ 0,  1,  2,  3,  4],
1182                [ 5,  6,  7,  8,  9],
1183                [10, 11, 12, 13, 14],
1184                [15, 16, 17, 18, 19],
1185                [20, 21, 22, 23, 24]])
1186         Dimensions without coordinates: x, y
1187 
1188         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1189         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1190         >>> da = da.isel(x=tgt_x, y=tgt_y)
1191         >>> da
1192         <xarray.DataArray (points: 5)>
1193         array([ 0,  6, 12, 18, 24])
1194         Dimensions without coordinates: points
1195         """
1196 
1197         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1198 
1199         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1200             ds = self._to_temp_dataset()._isel_fancy(
1201                 indexers, drop=drop, missing_dims=missing_dims
1202             )
1203             return self._from_temp_dataset(ds)
1204 
1205         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1206         # lists, or zero or one-dimensional np.ndarray's
1207 
1208         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1209 
1210         coords = {}
1211         for coord_name, coord_value in self._coords.items():
1212             coord_indexers = {
1213                 k: v for k, v in indexers.items() if k in coord_value.dims
1214             }
1215             if coord_indexers:
1216                 coord_value = coord_value.isel(coord_indexers)
1217                 if drop and coord_value.ndim == 0:
1218                     continue
1219             coords[coord_name] = coord_value
1220 
1221         return self._replace(variable=variable, coords=coords)
1222 
1223     def sel(
1224         self,
1225         indexers: Mapping[Any, Any] = None,
1226         method: str = None,
1227         tolerance=None,
1228         drop: bool = False,
1229         **indexers_kwargs: Any,
1230     ) -> "DataArray":
1231         """Return a new DataArray whose data is given by selecting index
1232         labels along the specified dimension(s).
1233 
1234         In contrast to `DataArray.isel`, indexers for this method should use
1235         labels instead of integers.
1236 
1237         Under the hood, this method is powered by using pandas's powerful Index
1238         objects. This makes label based indexing essentially just as fast as
1239         using integer indexing.
1240 
1241         It also means this method uses pandas's (well documented) logic for
1242         indexing. This means you can use string shortcuts for datetime indexes
1243         (e.g., '2000-01' to select all values in January 2000). It also means
1244         that slices are treated as inclusive of both the start and stop values,
1245         unlike normal Python indexing.
1246 
1247         .. warning::
1248 
1249           Do not try to assign values when using any of the indexing methods
1250           ``isel`` or ``sel``::
1251 
1252             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1253             # DO NOT do this
1254             da.isel(x=[0, 1, 2])[1] = -1
1255 
1256           Assigning values with the chained indexing using ``.sel`` or
1257           ``.isel`` fails silently.
1258 
1259         Parameters
1260         ----------
1261         indexers : dict, optional
1262             A dict with keys matching dimensions and values given
1263             by scalars, slices or arrays of tick labels. For dimensions with
1264             multi-index, the indexer may also be a dict-like object with keys
1265             matching index level names.
1266             If DataArrays are passed as indexers, xarray-style indexing will be
1267             carried out. See :ref:`indexing` for the details.
1268             One of indexers or indexers_kwargs must be provided.
1269         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1270             Method to use for inexact matches:
1271 
1272             * None (default): only exact matches
1273             * pad / ffill: propagate last valid index value forward
1274             * backfill / bfill: propagate next valid index value backward
1275             * nearest: use nearest valid index value
1276         tolerance : optional
1277             Maximum distance between original and new labels for inexact
1278             matches. The values of the index at the matching locations must
1279             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1280         drop : bool, optional
1281             If ``drop=True``, drop coordinates variables in `indexers` instead
1282             of making them scalar.
1283         **indexers_kwargs : {dim: indexer, ...}, optional
1284             The keyword arguments form of ``indexers``.
1285             One of indexers or indexers_kwargs must be provided.
1286 
1287         Returns
1288         -------
1289         obj : DataArray
1290             A new DataArray with the same contents as this DataArray, except the
1291             data and each dimension is indexed by the appropriate indexers.
1292             If indexer DataArrays have coordinates that do not conflict with
1293             this object, then these coordinates will be attached.
1294             In general, each array's data will be a view of the array's data
1295             in this DataArray, unless vectorized indexing was triggered by using
1296             an array indexer, in which case the data will be a copy.
1297 
1298         See Also
1299         --------
1300         Dataset.sel
1301         DataArray.isel
1302 
1303         Examples
1304         --------
1305         >>> da = xr.DataArray(
1306         ...     np.arange(25).reshape(5, 5),
1307         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1308         ...     dims=("x", "y"),
1309         ... )
1310         >>> da
1311         <xarray.DataArray (x: 5, y: 5)>
1312         array([[ 0,  1,  2,  3,  4],
1313                [ 5,  6,  7,  8,  9],
1314                [10, 11, 12, 13, 14],
1315                [15, 16, 17, 18, 19],
1316                [20, 21, 22, 23, 24]])
1317         Coordinates:
1318           * x        (x) int64 0 1 2 3 4
1319           * y        (y) int64 0 1 2 3 4
1320 
1321         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1322         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1323         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1324         >>> da
1325         <xarray.DataArray (points: 5)>
1326         array([ 0,  6, 12, 18, 24])
1327         Coordinates:
1328             x        (points) int64 0 1 2 3 4
1329             y        (points) int64 0 1 2 3 4
1330         Dimensions without coordinates: points
1331         """
1332         ds = self._to_temp_dataset().sel(
1333             indexers=indexers,
1334             drop=drop,
1335             method=method,
1336             tolerance=tolerance,
1337             **indexers_kwargs,
1338         )
1339         return self._from_temp_dataset(ds)
1340 
1341     def head(
1342         self,
1343         indexers: Union[Mapping[Any, int], int] = None,
1344         **indexers_kwargs: Any,
1345     ) -> "DataArray":
1346         """Return a new DataArray whose data is given by the the first `n`
1347         values along the specified dimension(s). Default `n` = 5
1348 
1349         See Also
1350         --------
1351         Dataset.head
1352         DataArray.tail
1353         DataArray.thin
1354         """
1355         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1356         return self._from_temp_dataset(ds)
1357 
1358     def tail(
1359         self,
1360         indexers: Union[Mapping[Any, int], int] = None,
1361         **indexers_kwargs: Any,
1362     ) -> "DataArray":
1363         """Return a new DataArray whose data is given by the the last `n`
1364         values along the specified dimension(s). Default `n` = 5
1365 
1366         See Also
1367         --------
1368         Dataset.tail
1369         DataArray.head
1370         DataArray.thin
1371         """
1372         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1373         return self._from_temp_dataset(ds)
1374 
1375     def thin(
1376         self,
1377         indexers: Union[Mapping[Any, int], int] = None,
1378         **indexers_kwargs: Any,
1379     ) -> "DataArray":
1380         """Return a new DataArray whose data is given by each `n` value
1381         along the specified dimension(s).
1382 
1383         See Also
1384         --------
1385         Dataset.thin
1386         DataArray.head
1387         DataArray.tail
1388         """
1389         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1390         return self._from_temp_dataset(ds)
1391 
1392     def broadcast_like(
1393         self, other: Union["DataArray", Dataset], exclude: Iterable[Hashable] = None
1394     ) -> "DataArray":
1395         """Broadcast this DataArray against another Dataset or DataArray.
1396 
1397         This is equivalent to xr.broadcast(other, self)[1]
1398 
1399         xarray objects are broadcast against each other in arithmetic
1400         operations, so this method is not be necessary for most uses.
1401 
1402         If no change is needed, the input data is returned to the output
1403         without being copied.
1404 
1405         If new coords are added by the broadcast, their values are
1406         NaN filled.
1407 
1408         Parameters
1409         ----------
1410         other : Dataset or DataArray
1411             Object against which to broadcast this array.
1412         exclude : iterable of hashable, optional
1413             Dimensions that must not be broadcasted
1414 
1415         Returns
1416         -------
1417         new_da : DataArray
1418             The caller broadcasted against ``other``.
1419 
1420         Examples
1421         --------
1422         >>> arr1 = xr.DataArray(
1423         ...     np.random.randn(2, 3),
1424         ...     dims=("x", "y"),
1425         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1426         ... )
1427         >>> arr2 = xr.DataArray(
1428         ...     np.random.randn(3, 2),
1429         ...     dims=("x", "y"),
1430         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1431         ... )
1432         >>> arr1
1433         <xarray.DataArray (x: 2, y: 3)>
1434         array([[ 1.76405235,  0.40015721,  0.97873798],
1435                [ 2.2408932 ,  1.86755799, -0.97727788]])
1436         Coordinates:
1437           * x        (x) <U1 'a' 'b'
1438           * y        (y) <U1 'a' 'b' 'c'
1439         >>> arr2
1440         <xarray.DataArray (x: 3, y: 2)>
1441         array([[ 0.95008842, -0.15135721],
1442                [-0.10321885,  0.4105985 ],
1443                [ 0.14404357,  1.45427351]])
1444         Coordinates:
1445           * x        (x) <U1 'a' 'b' 'c'
1446           * y        (y) <U1 'a' 'b'
1447         >>> arr1.broadcast_like(arr2)
1448         <xarray.DataArray (x: 3, y: 3)>
1449         array([[ 1.76405235,  0.40015721,  0.97873798],
1450                [ 2.2408932 ,  1.86755799, -0.97727788],
1451                [        nan,         nan,         nan]])
1452         Coordinates:
1453           * x        (x) <U1 'a' 'b' 'c'
1454           * y        (y) <U1 'a' 'b' 'c'
1455         """
1456         if exclude is None:
1457             exclude = set()
1458         else:
1459             exclude = set(exclude)
1460         args = align(other, self, join="outer", copy=False, exclude=exclude)
1461 
1462         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1463 
1464         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
1465 
1466     def reindex_like(
1467         self,
1468         other: Union["DataArray", Dataset],
1469         method: str = None,
1470         tolerance=None,
1471         copy: bool = True,
1472         fill_value=dtypes.NA,
1473     ) -> "DataArray":
1474         """Conform this object onto the indexes of another object, filling in
1475         missing values with ``fill_value``. The default fill value is NaN.
1476 
1477         Parameters
1478         ----------
1479         other : Dataset or DataArray
1480             Object with an 'indexes' attribute giving a mapping from dimension
1481             names to pandas.Index objects, which provides coordinates upon
1482             which to index the variables in this dataset. The indexes on this
1483             other object need not be the same as the indexes on this
1484             dataset. Any mis-matched index values will be filled in with
1485             NaN, and any mis-matched dimension names will simply be ignored.
1486         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1487             Method to use for filling index values from other not found on this
1488             data array:
1489 
1490             * None (default): don't fill gaps
1491             * pad / ffill: propagate last valid index value forward
1492             * backfill / bfill: propagate next valid index value backward
1493             * nearest: use nearest valid index value
1494         tolerance : optional
1495             Maximum distance between original and new labels for inexact
1496             matches. The values of the index at the matching locations must
1497             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1498         copy : bool, optional
1499             If ``copy=True``, data in the return value is always copied. If
1500             ``copy=False`` and reindexing is unnecessary, or can be performed
1501             with only slice operations, then the output may share memory with
1502             the input. In either case, a new xarray object is always returned.
1503         fill_value : scalar or dict-like, optional
1504             Value to use for newly missing values. If a dict-like, maps
1505             variable names (including coordinates) to fill values. Use this
1506             data array's name to refer to the data array's values.
1507 
1508         Returns
1509         -------
1510         reindexed : DataArray
1511             Another dataset array, with this array's data but coordinates from
1512             the other object.
1513 
1514         See Also
1515         --------
1516         DataArray.reindex
1517         align
1518         """
1519         indexers = reindex_like_indexers(self, other)
1520         return self.reindex(
1521             indexers=indexers,
1522             method=method,
1523             tolerance=tolerance,
1524             copy=copy,
1525             fill_value=fill_value,
1526         )
1527 
1528     def reindex(
1529         self,
1530         indexers: Mapping[Any, Any] = None,
1531         method: str = None,
1532         tolerance=None,
1533         copy: bool = True,
1534         fill_value=dtypes.NA,
1535         **indexers_kwargs: Any,
1536     ) -> "DataArray":
1537         """Conform this object onto the indexes of another object, filling in
1538         missing values with ``fill_value``. The default fill value is NaN.
1539 
1540         Parameters
1541         ----------
1542         indexers : dict, optional
1543             Dictionary with keys given by dimension names and values given by
1544             arrays of coordinates tick labels. Any mis-matched coordinate
1545             values will be filled in with NaN, and any mis-matched dimension
1546             names will simply be ignored.
1547             One of indexers or indexers_kwargs must be provided.
1548         copy : bool, optional
1549             If ``copy=True``, data in the return value is always copied. If
1550             ``copy=False`` and reindexing is unnecessary, or can be performed
1551             with only slice operations, then the output may share memory with
1552             the input. In either case, a new xarray object is always returned.
1553         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1554             Method to use for filling index values in ``indexers`` not found on
1555             this data array:
1556 
1557             * None (default): don't fill gaps
1558             * pad / ffill: propagate last valid index value forward
1559             * backfill / bfill: propagate next valid index value backward
1560             * nearest: use nearest valid index value
1561         tolerance : optional
1562             Maximum distance between original and new labels for inexact
1563             matches. The values of the index at the matching locations must
1564             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1565         fill_value : scalar or dict-like, optional
1566             Value to use for newly missing values. If a dict-like, maps
1567             variable names (including coordinates) to fill values. Use this
1568             data array's name to refer to the data array's values.
1569         **indexers_kwargs : {dim: indexer, ...}, optional
1570             The keyword arguments form of ``indexers``.
1571             One of indexers or indexers_kwargs must be provided.
1572 
1573         Returns
1574         -------
1575         reindexed : DataArray
1576             Another dataset array, with this array's data but replaced
1577             coordinates.
1578 
1579         Examples
1580         --------
1581         Reverse latitude:
1582 
1583         >>> da = xr.DataArray(
1584         ...     np.arange(4),
1585         ...     coords=[np.array([90, 89, 88, 87])],
1586         ...     dims="lat",
1587         ... )
1588         >>> da
1589         <xarray.DataArray (lat: 4)>
1590         array([0, 1, 2, 3])
1591         Coordinates:
1592           * lat      (lat) int64 90 89 88 87
1593         >>> da.reindex(lat=da.lat[::-1])
1594         <xarray.DataArray (lat: 4)>
1595         array([3, 2, 1, 0])
1596         Coordinates:
1597           * lat      (lat) int64 87 88 89 90
1598 
1599         See Also
1600         --------
1601         DataArray.reindex_like
1602         align
1603         """
1604         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1605         if isinstance(fill_value, dict):
1606             fill_value = fill_value.copy()
1607             sentinel = object()
1608             value = fill_value.pop(self.name, sentinel)
1609             if value is not sentinel:
1610                 fill_value[_THIS_ARRAY] = value
1611 
1612         ds = self._to_temp_dataset().reindex(
1613             indexers=indexers,
1614             method=method,
1615             tolerance=tolerance,
1616             copy=copy,
1617             fill_value=fill_value,
1618         )
1619         return self._from_temp_dataset(ds)
1620 
1621     def interp(
1622         self,
1623         coords: Mapping[Any, Any] = None,
1624         method: str = "linear",
1625         assume_sorted: bool = False,
1626         kwargs: Mapping[str, Any] = None,
1627         **coords_kwargs: Any,
1628     ) -> "DataArray":
1629         """Multidimensional interpolation of variables.
1630 
1631         Parameters
1632         ----------
1633         coords : dict, optional
1634             Mapping from dimension names to the new coordinates.
1635             New coordinate can be an scalar, array-like or DataArray.
1636             If DataArrays are passed as new coordinates, their dimensions are
1637             used for the broadcasting. Missing values are skipped.
1638         method : str, default: "linear"
1639             The method used to interpolate. Choose from
1640 
1641             - {"linear", "nearest"} for multidimensional array,
1642             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1643         assume_sorted : bool, optional
1644             If False, values of x can be in any order and they are sorted
1645             first. If True, x has to be an array of monotonically increasing
1646             values.
1647         kwargs : dict
1648             Additional keyword arguments passed to scipy's interpolator. Valid
1649             options and their behavior depend on if 1-dimensional or
1650             multi-dimensional interpolation is used.
1651         **coords_kwargs : {dim: coordinate, ...}, optional
1652             The keyword arguments form of ``coords``.
1653             One of coords or coords_kwargs must be provided.
1654 
1655         Returns
1656         -------
1657         interpolated : DataArray
1658             New dataarray on the new coordinates.
1659 
1660         Notes
1661         -----
1662         scipy is required.
1663 
1664         See Also
1665         --------
1666         scipy.interpolate.interp1d
1667         scipy.interpolate.interpn
1668 
1669         Examples
1670         --------
1671         >>> da = xr.DataArray(
1672         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1673         ...     dims=("x", "y"),
1674         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1675         ... )
1676         >>> da
1677         <xarray.DataArray (x: 3, y: 4)>
1678         array([[ 1.,  4.,  2.,  9.],
1679                [ 2.,  7.,  6., nan],
1680                [ 6., nan,  5.,  8.]])
1681         Coordinates:
1682           * x        (x) int64 0 1 2
1683           * y        (y) int64 10 12 14 16
1684 
1685         1D linear interpolation (the default):
1686 
1687         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1688         <xarray.DataArray (x: 4, y: 4)>
1689         array([[1.  , 4.  , 2.  ,  nan],
1690                [1.75, 6.25, 5.  ,  nan],
1691                [3.  ,  nan, 5.75,  nan],
1692                [5.  ,  nan, 5.25,  nan]])
1693         Coordinates:
1694           * y        (y) int64 10 12 14 16
1695           * x        (x) float64 0.0 0.75 1.25 1.75
1696 
1697         1D nearest interpolation:
1698 
1699         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1700         <xarray.DataArray (x: 4, y: 4)>
1701         array([[ 1.,  4.,  2.,  9.],
1702                [ 2.,  7.,  6., nan],
1703                [ 2.,  7.,  6., nan],
1704                [ 6., nan,  5.,  8.]])
1705         Coordinates:
1706           * y        (y) int64 10 12 14 16
1707           * x        (x) float64 0.0 0.75 1.25 1.75
1708 
1709         1D linear extrapolation:
1710 
1711         >>> da.interp(
1712         ...     x=[1, 1.5, 2.5, 3.5],
1713         ...     method="linear",
1714         ...     kwargs={"fill_value": "extrapolate"},
1715         ... )
1716         <xarray.DataArray (x: 4, y: 4)>
1717         array([[ 2. ,  7. ,  6. ,  nan],
1718                [ 4. ,  nan,  5.5,  nan],
1719                [ 8. ,  nan,  4.5,  nan],
1720                [12. ,  nan,  3.5,  nan]])
1721         Coordinates:
1722           * y        (y) int64 10 12 14 16
1723           * x        (x) float64 1.0 1.5 2.5 3.5
1724 
1725         2D linear interpolation:
1726 
1727         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1728         <xarray.DataArray (x: 4, y: 3)>
1729         array([[2.5  , 3.   ,   nan],
1730                [4.   , 5.625,   nan],
1731                [  nan,   nan,   nan],
1732                [  nan,   nan,   nan]])
1733         Coordinates:
1734           * x        (x) float64 0.0 0.75 1.25 1.75
1735           * y        (y) int64 11 13 15
1736         """
1737         if self.dtype.kind not in "uifc":
1738             raise TypeError(
1739                 "interp only works for a numeric type array. "
1740                 "Given {}.".format(self.dtype)
1741             )
1742         ds = self._to_temp_dataset().interp(
1743             coords,
1744             method=method,
1745             kwargs=kwargs,
1746             assume_sorted=assume_sorted,
1747             **coords_kwargs,
1748         )
1749         return self._from_temp_dataset(ds)
1750 
1751     def interp_like(
1752         self,
1753         other: Union["DataArray", Dataset],
1754         method: str = "linear",
1755         assume_sorted: bool = False,
1756         kwargs: Mapping[str, Any] = None,
1757     ) -> "DataArray":
1758         """Interpolate this object onto the coordinates of another object,
1759         filling out of range values with NaN.
1760 
1761         Parameters
1762         ----------
1763         other : Dataset or DataArray
1764             Object with an 'indexes' attribute giving a mapping from dimension
1765             names to an 1d array-like, which provides coordinates upon
1766             which to index the variables in this dataset. Missing values are skipped.
1767         method : str, default: "linear"
1768             The method used to interpolate. Choose from
1769 
1770             - {"linear", "nearest"} for multidimensional array,
1771             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"} for 1-dimensional array.
1772         assume_sorted : bool, optional
1773             If False, values of coordinates that are interpolated over can be
1774             in any order and they are sorted first. If True, interpolated
1775             coordinates are assumed to be an array of monotonically increasing
1776             values.
1777         kwargs : dict, optional
1778             Additional keyword passed to scipy's interpolator.
1779 
1780         Returns
1781         -------
1782         interpolated : DataArray
1783             Another dataarray by interpolating this dataarray's data along the
1784             coordinates of the other object.
1785 
1786         Notes
1787         -----
1788         scipy is required.
1789         If the dataarray has object-type coordinates, reindex is used for these
1790         coordinates instead of the interpolation.
1791 
1792         See Also
1793         --------
1794         DataArray.interp
1795         DataArray.reindex_like
1796         """
1797         if self.dtype.kind not in "uifc":
1798             raise TypeError(
1799                 "interp only works for a numeric type array. "
1800                 "Given {}.".format(self.dtype)
1801             )
1802         ds = self._to_temp_dataset().interp_like(
1803             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1804         )
1805         return self._from_temp_dataset(ds)
1806 
1807     def rename(
1808         self,
1809         new_name_or_name_dict: Union[Hashable, Mapping[Any, Hashable]] = None,
1810         **names: Hashable,
1811     ) -> "DataArray":
1812         """Returns a new DataArray with renamed coordinates or a new name.
1813 
1814         Parameters
1815         ----------
1816         new_name_or_name_dict : str or dict-like, optional
1817             If the argument is dict-like, it used as a mapping from old
1818             names to new names for coordinates. Otherwise, use the argument
1819             as the new name for this array.
1820         **names : hashable, optional
1821             The keyword arguments form of a mapping from old names to
1822             new names for coordinates.
1823             One of new_name_or_name_dict or names must be provided.
1824 
1825         Returns
1826         -------
1827         renamed : DataArray
1828             Renamed array or array with renamed coordinates.
1829 
1830         See Also
1831         --------
1832         Dataset.rename
1833         DataArray.swap_dims
1834         """
1835         if names or utils.is_dict_like(new_name_or_name_dict):
1836             new_name_or_name_dict = cast(
1837                 Mapping[Hashable, Hashable], new_name_or_name_dict
1838             )
1839             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
1840             dataset = self._to_temp_dataset().rename(name_dict)
1841             return self._from_temp_dataset(dataset)
1842         else:
1843             new_name_or_name_dict = cast(Hashable, new_name_or_name_dict)
1844             return self._replace(name=new_name_or_name_dict)
1845 
1846     def swap_dims(
1847         self, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
1848     ) -> "DataArray":
1849         """Returns a new DataArray with swapped dimensions.
1850 
1851         Parameters
1852         ----------
1853         dims_dict : dict-like
1854             Dictionary whose keys are current dimension names and whose values
1855             are new names.
1856         **dims_kwargs : {existing_dim: new_dim, ...}, optional
1857             The keyword arguments form of ``dims_dict``.
1858             One of dims_dict or dims_kwargs must be provided.
1859 
1860         Returns
1861         -------
1862         swapped : DataArray
1863             DataArray with swapped dimensions.
1864 
1865         Examples
1866         --------
1867         >>> arr = xr.DataArray(
1868         ...     data=[0, 1],
1869         ...     dims="x",
1870         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
1871         ... )
1872         >>> arr
1873         <xarray.DataArray (x: 2)>
1874         array([0, 1])
1875         Coordinates:
1876           * x        (x) <U1 'a' 'b'
1877             y        (x) int64 0 1
1878 
1879         >>> arr.swap_dims({"x": "y"})
1880         <xarray.DataArray (y: 2)>
1881         array([0, 1])
1882         Coordinates:
1883             x        (y) <U1 'a' 'b'
1884           * y        (y) int64 0 1
1885 
1886         >>> arr.swap_dims({"x": "z"})
1887         <xarray.DataArray (z: 2)>
1888         array([0, 1])
1889         Coordinates:
1890             x        (z) <U1 'a' 'b'
1891             y        (z) int64 0 1
1892         Dimensions without coordinates: z
1893 
1894         See Also
1895         --------
1896         DataArray.rename
1897         Dataset.swap_dims
1898         """
1899         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
1900         ds = self._to_temp_dataset().swap_dims(dims_dict)
1901         return self._from_temp_dataset(ds)
1902 
1903     def expand_dims(
1904         self,
1905         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Any, Any]] = None,
1906         axis=None,
1907         **dim_kwargs: Any,
1908     ) -> "DataArray":
1909         """Return a new object with an additional axis (or axes) inserted at
1910         the corresponding position in the array shape. The new object is a
1911         view into the underlying array, not a copy.
1912 
1913         If dim is already a scalar coordinate, it will be promoted to a 1D
1914         coordinate consisting of a single value.
1915 
1916         Parameters
1917         ----------
1918         dim : hashable, sequence of hashable, dict, or None, optional
1919             Dimensions to include on the new variable.
1920             If provided as str or sequence of str, then dimensions are inserted
1921             with length 1. If provided as a dict, then the keys are the new
1922             dimensions and the values are either integers (giving the length of
1923             the new dimensions) or sequence/ndarray (giving the coordinates of
1924             the new dimensions).
1925         axis : int, list of int or tuple of int, or None, default: None
1926             Axis position(s) where new axis is to be inserted (position(s) on
1927             the result array). If a list (or tuple) of integers is passed,
1928             multiple axes are inserted. In this case, dim arguments should be
1929             same length list. If axis=None is passed, all the axes will be
1930             inserted to the start of the result array.
1931         **dim_kwargs : int or sequence or ndarray
1932             The keywords are arbitrary dimensions being inserted and the values
1933             are either the lengths of the new dims (if int is given), or their
1934             coordinates. Note, this is an alternative to passing a dict to the
1935             dim kwarg and will only be used if dim is None.
1936 
1937         Returns
1938         -------
1939         expanded : same type as caller
1940             This object, but with an additional dimension(s).
1941         """
1942         if isinstance(dim, int):
1943             raise TypeError("dim should be hashable or sequence/mapping of hashables")
1944         elif isinstance(dim, Sequence) and not isinstance(dim, str):
1945             if len(dim) != len(set(dim)):
1946                 raise ValueError("dims should not contain duplicate values.")
1947             dim = dict.fromkeys(dim, 1)
1948         elif dim is not None and not isinstance(dim, Mapping):
1949             dim = {cast(Hashable, dim): 1}
1950 
1951         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
1952         ds = self._to_temp_dataset().expand_dims(dim, axis)
1953         return self._from_temp_dataset(ds)
1954 
1955     def set_index(
1956         self,
1957         indexes: Mapping[Any, Union[Hashable, Sequence[Hashable]]] = None,
1958         append: bool = False,
1959         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
1960     ) -> "DataArray":
1961         """Set DataArray (multi-)indexes using one or more existing
1962         coordinates.
1963 
1964         Parameters
1965         ----------
1966         indexes : {dim: index, ...}
1967             Mapping from names matching dimensions and values given
1968             by (lists of) the names of existing coordinates or variables to set
1969             as new (multi-)index.
1970         append : bool, optional
1971             If True, append the supplied index(es) to the existing index(es).
1972             Otherwise replace the existing index(es) (default).
1973         **indexes_kwargs : optional
1974             The keyword arguments form of ``indexes``.
1975             One of indexes or indexes_kwargs must be provided.
1976 
1977         Returns
1978         -------
1979         obj : DataArray
1980             Another DataArray, with this data but replaced coordinates.
1981 
1982         Examples
1983         --------
1984         >>> arr = xr.DataArray(
1985         ...     data=np.ones((2, 3)),
1986         ...     dims=["x", "y"],
1987         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
1988         ... )
1989         >>> arr
1990         <xarray.DataArray (x: 2, y: 3)>
1991         array([[1., 1., 1.],
1992                [1., 1., 1.]])
1993         Coordinates:
1994           * x        (x) int64 0 1
1995           * y        (y) int64 0 1 2
1996             a        (x) int64 3 4
1997         >>> arr.set_index(x="a")
1998         <xarray.DataArray (x: 2, y: 3)>
1999         array([[1., 1., 1.],
2000                [1., 1., 1.]])
2001         Coordinates:
2002           * x        (x) int64 3 4
2003           * y        (y) int64 0 1 2
2004 
2005         See Also
2006         --------
2007         DataArray.reset_index
2008         """
2009         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2010         return self._from_temp_dataset(ds)
2011 
2012     def reset_index(
2013         self,
2014         dims_or_levels: Union[Hashable, Sequence[Hashable]],
2015         drop: bool = False,
2016     ) -> "DataArray":
2017         """Reset the specified index(es) or multi-index level(s).
2018 
2019         Parameters
2020         ----------
2021         dims_or_levels : hashable or sequence of hashable
2022             Name(s) of the dimension(s) and/or multi-index level(s) that will
2023             be reset.
2024         drop : bool, optional
2025             If True, remove the specified indexes and/or multi-index levels
2026             instead of extracting them as new coordinates (default: False).
2027 
2028         Returns
2029         -------
2030         obj : DataArray
2031             Another dataarray, with this dataarray's data but replaced
2032             coordinates.
2033 
2034         See Also
2035         --------
2036         DataArray.set_index
2037         """
2038         coords, _ = split_indexes(
2039             dims_or_levels, self._coords, set(), self._level_coords, drop=drop
2040         )
2041         return self._replace(coords=coords)
2042 
2043     def reorder_levels(
2044         self,
2045         dim_order: Mapping[Any, Sequence[int]] = None,
2046         **dim_order_kwargs: Sequence[int],
2047     ) -> "DataArray":
2048         """Rearrange index levels using input order.
2049 
2050         Parameters
2051         ----------
2052         dim_order : optional
2053             Mapping from names matching dimensions and values given
2054             by lists representing new level orders. Every given dimension
2055             must have a multi-index.
2056         **dim_order_kwargs : optional
2057             The keyword arguments form of ``dim_order``.
2058             One of dim_order or dim_order_kwargs must be provided.
2059 
2060         Returns
2061         -------
2062         obj : DataArray
2063             Another dataarray, with this dataarray's data but replaced
2064             coordinates.
2065         """
2066         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
2067         replace_coords = {}
2068         for dim, order in dim_order.items():
2069             coord = self._coords[dim]
2070             index = coord.to_index()
2071             if not isinstance(index, pd.MultiIndex):
2072                 raise ValueError(f"coordinate {dim!r} has no MultiIndex")
2073             replace_coords[dim] = IndexVariable(coord.dims, index.reorder_levels(order))
2074         coords = self._coords.copy()
2075         coords.update(replace_coords)
2076         return self._replace(coords=coords)
2077 
2078     def stack(
2079         self,
2080         dimensions: Mapping[Any, Sequence[Hashable]] = None,
2081         **dimensions_kwargs: Sequence[Hashable],
2082     ) -> "DataArray":
2083         """
2084         Stack any number of existing dimensions into a single new dimension.
2085 
2086         New dimensions will be added at the end, and the corresponding
2087         coordinate variables will be combined into a MultiIndex.
2088 
2089         Parameters
2090         ----------
2091         dimensions : mapping of hashable to sequence of hashable
2092             Mapping of the form `new_name=(dim1, dim2, ...)`.
2093             Names of new dimensions, and the existing dimensions that they
2094             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2095             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2096             all dimensions.
2097         **dimensions_kwargs
2098             The keyword arguments form of ``dimensions``.
2099             One of dimensions or dimensions_kwargs must be provided.
2100 
2101         Returns
2102         -------
2103         stacked : DataArray
2104             DataArray with stacked data.
2105 
2106         Examples
2107         --------
2108         >>> arr = xr.DataArray(
2109         ...     np.arange(6).reshape(2, 3),
2110         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2111         ... )
2112         >>> arr
2113         <xarray.DataArray (x: 2, y: 3)>
2114         array([[0, 1, 2],
2115                [3, 4, 5]])
2116         Coordinates:
2117           * x        (x) <U1 'a' 'b'
2118           * y        (y) int64 0 1 2
2119         >>> stacked = arr.stack(z=("x", "y"))
2120         >>> stacked.indexes["z"]
2121         MultiIndex([('a', 0),
2122                     ('a', 1),
2123                     ('a', 2),
2124                     ('b', 0),
2125                     ('b', 1),
2126                     ('b', 2)],
2127                    names=['x', 'y'])
2128 
2129         See Also
2130         --------
2131         DataArray.unstack
2132         """
2133         ds = self._to_temp_dataset().stack(dimensions, **dimensions_kwargs)
2134         return self._from_temp_dataset(ds)
2135 
2136     def unstack(
2137         self,
2138         dim: Union[Hashable, Sequence[Hashable], None] = None,
2139         fill_value: Any = dtypes.NA,
2140         sparse: bool = False,
2141     ) -> "DataArray":
2142         """
2143         Unstack existing dimensions corresponding to MultiIndexes into
2144         multiple new dimensions.
2145 
2146         New dimensions will be added at the end.
2147 
2148         Parameters
2149         ----------
2150         dim : hashable or sequence of hashable, optional
2151             Dimension(s) over which to unstack. By default unstacks all
2152             MultiIndexes.
2153         fill_value : scalar or dict-like, default: nan
2154             value to be filled. If a dict-like, maps variable names to
2155             fill values. Use the data array's name to refer to its
2156             name. If not provided or if the dict-like does not contain
2157             all variables, the dtype's NA value will be used.
2158         sparse : bool, default: False
2159             use sparse-array if True
2160 
2161         Returns
2162         -------
2163         unstacked : DataArray
2164             Array with unstacked data.
2165 
2166         Examples
2167         --------
2168         >>> arr = xr.DataArray(
2169         ...     np.arange(6).reshape(2, 3),
2170         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2171         ... )
2172         >>> arr
2173         <xarray.DataArray (x: 2, y: 3)>
2174         array([[0, 1, 2],
2175                [3, 4, 5]])
2176         Coordinates:
2177           * x        (x) <U1 'a' 'b'
2178           * y        (y) int64 0 1 2
2179         >>> stacked = arr.stack(z=("x", "y"))
2180         >>> stacked.indexes["z"]
2181         MultiIndex([('a', 0),
2182                     ('a', 1),
2183                     ('a', 2),
2184                     ('b', 0),
2185                     ('b', 1),
2186                     ('b', 2)],
2187                    names=['x', 'y'])
2188         >>> roundtripped = stacked.unstack()
2189         >>> arr.identical(roundtripped)
2190         True
2191 
2192         See Also
2193         --------
2194         DataArray.stack
2195         """
2196         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2197         return self._from_temp_dataset(ds)
2198 
2199     def to_unstacked_dataset(self, dim, level=0):
2200         """Unstack DataArray expanding to Dataset along a given level of a
2201         stacked coordinate.
2202 
2203         This is the inverse operation of Dataset.to_stacked_array.
2204 
2205         Parameters
2206         ----------
2207         dim : str
2208             Name of existing dimension to unstack
2209         level : int or str
2210             The MultiIndex level to expand to a dataset along. Can either be
2211             the integer index of the level or its name.
2212 
2213         Returns
2214         -------
2215         unstacked: Dataset
2216 
2217         Examples
2218         --------
2219         >>> arr = xr.DataArray(
2220         ...     np.arange(6).reshape(2, 3),
2221         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2222         ... )
2223         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2224         >>> data
2225         <xarray.Dataset>
2226         Dimensions:  (x: 2, y: 3)
2227         Coordinates:
2228           * x        (x) <U1 'a' 'b'
2229           * y        (y) int64 0 1 2
2230         Data variables:
2231             a        (x, y) int64 0 1 2 3 4 5
2232             b        (x) int64 0 3
2233         >>> stacked = data.to_stacked_array("z", ["x"])
2234         >>> stacked.indexes["z"]
2235         MultiIndex([('a', 0.0),
2236                     ('a', 1.0),
2237                     ('a', 2.0),
2238                     ('b', nan)],
2239                    names=['variable', 'y'])
2240         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2241         >>> data.identical(roundtripped)
2242         True
2243 
2244         See Also
2245         --------
2246         Dataset.to_stacked_array
2247         """
2248 
2249         # TODO: benbovy - flexible indexes: update when MultIndex has its own
2250         # class inheriting from xarray.Index
2251         idx = self.xindexes[dim].to_pandas_index()
2252         if not isinstance(idx, pd.MultiIndex):
2253             raise ValueError(f"'{dim}' is not a stacked coordinate")
2254 
2255         level_number = idx._get_level_number(level)
2256         variables = idx.levels[level_number]
2257         variable_dim = idx.names[level_number]
2258 
2259         # pull variables out of datarray
2260         data_dict = {}
2261         for k in variables:
2262             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2263 
2264         # unstacked dataset
2265         return Dataset(data_dict)
2266 
2267     def transpose(
2268         self,
2269         *dims: Hashable,
2270         transpose_coords: bool = True,
2271         missing_dims: str = "raise",
2272     ) -> "DataArray":
2273         """Return a new DataArray object with transposed dimensions.
2274 
2275         Parameters
2276         ----------
2277         *dims : hashable, optional
2278             By default, reverse the dimensions. Otherwise, reorder the
2279             dimensions to this order.
2280         transpose_coords : bool, default: True
2281             If True, also transpose the coordinates of this DataArray.
2282         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2283             What to do if dimensions that should be selected from are not present in the
2284             DataArray:
2285             - "raise": raise an exception
2286             - "warn": raise a warning, and ignore the missing dimensions
2287             - "ignore": ignore the missing dimensions
2288 
2289         Returns
2290         -------
2291         transposed : DataArray
2292             The returned DataArray's array is transposed.
2293 
2294         Notes
2295         -----
2296         This operation returns a view of this array's data. It is
2297         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2298         -- the data will be fully loaded.
2299 
2300         See Also
2301         --------
2302         numpy.transpose
2303         Dataset.transpose
2304         """
2305         if dims:
2306             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2307         variable = self.variable.transpose(*dims)
2308         if transpose_coords:
2309             coords: Dict[Hashable, Variable] = {}
2310             for name, coord in self.coords.items():
2311                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2312                 coords[name] = coord.variable.transpose(*coord_dims)
2313             return self._replace(variable, coords)
2314         else:
2315             return self._replace(variable)
2316 
2317     @property
2318     def T(self) -> "DataArray":
2319         return self.transpose()
2320 
2321     def drop_vars(
2322         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
2323     ) -> "DataArray":
2324         """Returns an array with dropped variables.
2325 
2326         Parameters
2327         ----------
2328         names : hashable or iterable of hashable
2329             Name(s) of variables to drop.
2330         errors : {"raise", "ignore"}, optional
2331             If 'raise' (default), raises a ValueError error if any of the variable
2332             passed are not in the dataset. If 'ignore', any given names that are in the
2333             DataArray are dropped and no error is raised.
2334 
2335         Returns
2336         -------
2337         dropped : Dataset
2338             New Dataset copied from `self` with variables removed.
2339         """
2340         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2341         return self._from_temp_dataset(ds)
2342 
2343     def drop(
2344         self,
2345         labels: Mapping = None,
2346         dim: Hashable = None,
2347         *,
2348         errors: str = "raise",
2349         **labels_kwargs,
2350     ) -> "DataArray":
2351         """Backward compatible method based on `drop_vars` and `drop_sel`
2352 
2353         Using either `drop_vars` or `drop_sel` is encouraged
2354 
2355         See Also
2356         --------
2357         DataArray.drop_vars
2358         DataArray.drop_sel
2359         """
2360         ds = self._to_temp_dataset().drop(labels, dim, errors=errors)
2361         return self._from_temp_dataset(ds)
2362 
2363     def drop_sel(
2364         self,
2365         labels: Mapping[Any, Any] = None,
2366         *,
2367         errors: str = "raise",
2368         **labels_kwargs,
2369     ) -> "DataArray":
2370         """Drop index labels from this DataArray.
2371 
2372         Parameters
2373         ----------
2374         labels : mapping of hashable to Any
2375             Index labels to drop
2376         errors : {"raise", "ignore"}, optional
2377             If 'raise' (default), raises a ValueError error if
2378             any of the index labels passed are not
2379             in the dataset. If 'ignore', any given labels that are in the
2380             dataset are dropped and no error is raised.
2381         **labels_kwargs : {dim: label, ...}, optional
2382             The keyword arguments form of ``dim`` and ``labels``
2383 
2384         Returns
2385         -------
2386         dropped : DataArray
2387         """
2388         if labels_kwargs or isinstance(labels, dict):
2389             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2390 
2391         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2392         return self._from_temp_dataset(ds)
2393 
2394     def drop_isel(self, indexers=None, **indexers_kwargs):
2395         """Drop index positions from this DataArray.
2396 
2397         Parameters
2398         ----------
2399         indexers : mapping of hashable to Any
2400             Index locations to drop
2401         **indexers_kwargs : {dim: position, ...}, optional
2402             The keyword arguments form of ``dim`` and ``positions``
2403 
2404         Returns
2405         -------
2406         dropped : DataArray
2407 
2408         Raises
2409         ------
2410         IndexError
2411         """
2412         dataset = self._to_temp_dataset()
2413         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2414         return self._from_temp_dataset(dataset)
2415 
2416     def dropna(
2417         self, dim: Hashable, how: str = "any", thresh: int = None
2418     ) -> "DataArray":
2419         """Returns a new array with dropped labels for missing values along
2420         the provided dimension.
2421 
2422         Parameters
2423         ----------
2424         dim : hashable
2425             Dimension along which to drop missing values. Dropping along
2426             multiple dimensions simultaneously is not yet supported.
2427         how : {"any", "all"}, optional
2428             * any : if any NA values are present, drop that label
2429             * all : if all values are NA, drop that label
2430         thresh : int, default: None
2431             If supplied, require this many non-NA values.
2432 
2433         Returns
2434         -------
2435         DataArray
2436         """
2437         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2438         return self._from_temp_dataset(ds)
2439 
2440     def fillna(self, value: Any) -> "DataArray":
2441         """Fill missing values in this object.
2442 
2443         This operation follows the normal broadcasting and alignment rules that
2444         xarray uses for binary arithmetic, except the result is aligned to this
2445         object (``join='left'``) instead of aligned to the intersection of
2446         index coordinates (``join='inner'``).
2447 
2448         Parameters
2449         ----------
2450         value : scalar, ndarray or DataArray
2451             Used to fill all matching missing values in this array. If the
2452             argument is a DataArray, it is first aligned with (reindexed to)
2453             this array.
2454 
2455         Returns
2456         -------
2457         DataArray
2458         """
2459         if utils.is_dict_like(value):
2460             raise TypeError(
2461                 "cannot provide fill value as a dictionary with "
2462                 "fillna on a DataArray"
2463             )
2464         out = ops.fillna(self, value)
2465         return out
2466 
2467     def interpolate_na(
2468         self,
2469         dim: Hashable = None,
2470         method: str = "linear",
2471         limit: int = None,
2472         use_coordinate: Union[bool, str] = True,
2473         max_gap: Union[
2474             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
2475         ] = None,
2476         keep_attrs: bool = None,
2477         **kwargs: Any,
2478     ) -> "DataArray":
2479         """Fill in NaNs by interpolating according to different methods.
2480 
2481         Parameters
2482         ----------
2483         dim : str
2484             Specifies the dimension along which to interpolate.
2485         method : str, optional
2486             String indicating which method to use for interpolation:
2487 
2488             - 'linear': linear interpolation (Default). Additional keyword
2489               arguments are passed to :py:func:`numpy.interp`
2490             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2491               are passed to :py:func:`scipy.interpolate.interp1d`. If
2492               ``method='polynomial'``, the ``order`` keyword argument must also be
2493               provided.
2494             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2495               respective :py:class:`scipy.interpolate` classes.
2496         use_coordinate : bool or str, default: True
2497             Specifies which index to use as the x values in the interpolation
2498             formulated as `y = f(x)`. If False, values are treated as if
2499             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2500             used. If ``use_coordinate`` is a string, it specifies the name of a
2501             coordinate variariable to use as the index.
2502         limit : int, default: None
2503             Maximum number of consecutive NaNs to fill. Must be greater than 0
2504             or None for no limit. This filling is done regardless of the size of
2505             the gap in the data. To only interpolate over gaps less than a given length,
2506             see ``max_gap``.
2507         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2508             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2509             Use None for no limit. When interpolating along a datetime64 dimension
2510             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2511 
2512             - a string that is valid input for pandas.to_timedelta
2513             - a :py:class:`numpy.timedelta64` object
2514             - a :py:class:`pandas.Timedelta` object
2515             - a :py:class:`datetime.timedelta` object
2516 
2517             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2518             dimensions has not been implemented yet. Gap length is defined as the difference
2519             between coordinate values at the first data point after a gap and the last value
2520             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2521             between coordinate values at the first (last) valid data point and the first (last) NaN.
2522             For example, consider::
2523 
2524                 <xarray.DataArray (x: 9)>
2525                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2526                 Coordinates:
2527                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2528 
2529             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2530         keep_attrs : bool, default: True
2531             If True, the dataarray's attributes (`attrs`) will be copied from
2532             the original object to the new one.  If False, the new
2533             object will be returned without attributes.
2534         **kwargs : dict, optional
2535             parameters passed verbatim to the underlying interpolation function
2536 
2537         Returns
2538         -------
2539         interpolated: DataArray
2540             Filled in DataArray.
2541 
2542         See Also
2543         --------
2544         numpy.interp
2545         scipy.interpolate
2546 
2547         Examples
2548         --------
2549         >>> da = xr.DataArray(
2550         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2551         ... )
2552         >>> da
2553         <xarray.DataArray (x: 5)>
2554         array([nan,  2.,  3., nan,  0.])
2555         Coordinates:
2556           * x        (x) int64 0 1 2 3 4
2557 
2558         >>> da.interpolate_na(dim="x", method="linear")
2559         <xarray.DataArray (x: 5)>
2560         array([nan, 2. , 3. , 1.5, 0. ])
2561         Coordinates:
2562           * x        (x) int64 0 1 2 3 4
2563 
2564         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2565         <xarray.DataArray (x: 5)>
2566         array([1. , 2. , 3. , 1.5, 0. ])
2567         Coordinates:
2568           * x        (x) int64 0 1 2 3 4
2569         """
2570         from .missing import interp_na
2571 
2572         return interp_na(
2573             self,
2574             dim=dim,
2575             method=method,
2576             limit=limit,
2577             use_coordinate=use_coordinate,
2578             max_gap=max_gap,
2579             keep_attrs=keep_attrs,
2580             **kwargs,
2581         )
2582 
2583     def ffill(self, dim: Hashable, limit: int = None) -> "DataArray":
2584         """Fill NaN values by propogating values forward
2585 
2586         *Requires bottleneck.*
2587 
2588         Parameters
2589         ----------
2590         dim : hashable
2591             Specifies the dimension along which to propagate values when
2592             filling.
2593         limit : int, default: None
2594             The maximum number of consecutive NaN values to forward fill. In
2595             other words, if there is a gap with more than this number of
2596             consecutive NaNs, it will only be partially filled. Must be greater
2597             than 0 or None for no limit. Must be None or greater than or equal
2598             to axis length if filling along chunked axes (dimensions).
2599 
2600         Returns
2601         -------
2602         DataArray
2603         """
2604         from .missing import ffill
2605 
2606         return ffill(self, dim, limit=limit)
2607 
2608     def bfill(self, dim: Hashable, limit: int = None) -> "DataArray":
2609         """Fill NaN values by propogating values backward
2610 
2611         *Requires bottleneck.*
2612 
2613         Parameters
2614         ----------
2615         dim : str
2616             Specifies the dimension along which to propagate values when
2617             filling.
2618         limit : int, default: None
2619             The maximum number of consecutive NaN values to backward fill. In
2620             other words, if there is a gap with more than this number of
2621             consecutive NaNs, it will only be partially filled. Must be greater
2622             than 0 or None for no limit. Must be None or greater than or equal
2623             to axis length if filling along chunked axes (dimensions).
2624 
2625         Returns
2626         -------
2627         DataArray
2628         """
2629         from .missing import bfill
2630 
2631         return bfill(self, dim, limit=limit)
2632 
2633     def combine_first(self, other: "DataArray") -> "DataArray":
2634         """Combine two DataArray objects, with union of coordinates.
2635 
2636         This operation follows the normal broadcasting and alignment rules of
2637         ``join='outer'``.  Default to non-null values of array calling the
2638         method.  Use np.nan to fill in vacant cells after alignment.
2639 
2640         Parameters
2641         ----------
2642         other : DataArray
2643             Used to fill all matching missing values in this array.
2644 
2645         Returns
2646         -------
2647         DataArray
2648         """
2649         return ops.fillna(self, other, join="outer")
2650 
2651     def reduce(
2652         self,
2653         func: Callable[..., Any],
2654         dim: Union[None, Hashable, Sequence[Hashable]] = None,
2655         axis: Union[None, int, Sequence[int]] = None,
2656         keep_attrs: bool = None,
2657         keepdims: bool = False,
2658         **kwargs: Any,
2659     ) -> "DataArray":
2660         """Reduce this array by applying `func` along some dimension(s).
2661 
2662         Parameters
2663         ----------
2664         func : callable
2665             Function which can be called in the form
2666             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2667             np.ndarray over an integer valued axis.
2668         dim : hashable or sequence of hashable, optional
2669             Dimension(s) over which to apply `func`.
2670         axis : int or sequence of int, optional
2671             Axis(es) over which to repeatedly apply `func`. Only one of the
2672             'dim' and 'axis' arguments can be supplied. If neither are
2673             supplied, then the reduction is calculated over the flattened array
2674             (by calling `f(x)` without an axis argument).
2675         keep_attrs : bool, optional
2676             If True, the variable's attributes (`attrs`) will be copied from
2677             the original object to the new one.  If False (default), the new
2678             object will be returned without attributes.
2679         keepdims : bool, default: False
2680             If True, the dimensions which are reduced are left in the result
2681             as dimensions of size one. Coordinates that use these dimensions
2682             are removed.
2683         **kwargs : dict
2684             Additional keyword arguments passed on to `func`.
2685 
2686         Returns
2687         -------
2688         reduced : DataArray
2689             DataArray with this object's array replaced with an array with
2690             summarized data and the indicated dimension(s) removed.
2691         """
2692 
2693         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2694         return self._replace_maybe_drop_dims(var)
2695 
2696     def to_pandas(self) -> Union["DataArray", pd.Series, pd.DataFrame]:
2697         """Convert this array into a pandas object with the same shape.
2698 
2699         The type of the returned object depends on the number of DataArray
2700         dimensions:
2701 
2702         * 0D -> `xarray.DataArray`
2703         * 1D -> `pandas.Series`
2704         * 2D -> `pandas.DataFrame`
2705 
2706         Only works for arrays with 2 or fewer dimensions.
2707 
2708         The DataArray constructor performs the inverse transformation.
2709         """
2710         # TODO: consolidate the info about pandas constructors and the
2711         # attributes that correspond to their indexes into a separate module?
2712         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2713         try:
2714             constructor = constructors[self.ndim]
2715         except KeyError:
2716             raise ValueError(
2717                 f"cannot convert arrays with {self.ndim} dimensions into "
2718                 "pandas objects"
2719             )
2720         indexes = [self.get_index(dim) for dim in self.dims]
2721         return constructor(self.values, *indexes)
2722 
2723     def to_dataframe(
2724         self, name: Hashable = None, dim_order: List[Hashable] = None
2725     ) -> pd.DataFrame:
2726         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2727 
2728         The DataFrame is indexed by the Cartesian product of index coordinates
2729         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
2730         included as columns in the DataFrame.
2731 
2732         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
2733         doesn't rely on a MultiIndex to build the DataFrame.
2734 
2735         Parameters
2736         ----------
2737         name
2738             Name to give to this array (required if unnamed).
2739         dim_order
2740             Hierarchical dimension order for the resulting dataframe.
2741             Array content is transposed to this order and then written out as flat
2742             vectors in contiguous order, so the last dimension in this list
2743             will be contiguous in the resulting DataFrame. This has a major
2744             influence on which operations are efficient on the resulting
2745             dataframe.
2746 
2747             If provided, must include all dimensions of this DataArray. By default,
2748             dimensions are sorted according to the DataArray dimensions order.
2749 
2750         Returns
2751         -------
2752         result
2753             DataArray as a pandas DataFrame.
2754 
2755         See also
2756         --------
2757         DataArray.to_pandas
2758         """
2759         if name is None:
2760             name = self.name
2761         if name is None:
2762             raise ValueError(
2763                 "cannot convert an unnamed DataArray to a "
2764                 "DataFrame: use the ``name`` parameter"
2765             )
2766         if self.ndim == 0:
2767             raise ValueError("cannot convert a scalar to a DataFrame")
2768 
2769         # By using a unique name, we can convert a DataArray into a DataFrame
2770         # even if it shares a name with one of its coordinates.
2771         # I would normally use unique_name = object() but that results in a
2772         # dataframe with columns in the wrong order, for reasons I have not
2773         # been able to debug (possibly a pandas bug?).
2774         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
2775         ds = self._to_dataset_whole(name=unique_name)
2776 
2777         if dim_order is None:
2778             ordered_dims = dict(zip(self.dims, self.shape))
2779         else:
2780             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
2781 
2782         df = ds._to_dataframe(ordered_dims)
2783         df.columns = [name if c == unique_name else c for c in df.columns]
2784         return df
2785 
2786     def to_series(self) -> pd.Series:
2787         """Convert this array into a pandas.Series.
2788 
2789         The Series is indexed by the Cartesian product of index coordinates
2790         (in the form of a :py:class:`pandas.MultiIndex`).
2791         """
2792         index = self.coords.to_index()
2793         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
2794 
2795     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
2796         """Convert this array into a numpy.ma.MaskedArray
2797 
2798         Parameters
2799         ----------
2800         copy : bool, default: True
2801             If True make a copy of the array in the result. If False,
2802             a MaskedArray view of DataArray.values is returned.
2803 
2804         Returns
2805         -------
2806         result : MaskedArray
2807             Masked where invalid values (nan or inf) occur.
2808         """
2809         values = self.to_numpy()  # only compute lazy arrays once
2810         isnull = pd.isnull(values)
2811         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
2812 
2813     def to_netcdf(self, *args, **kwargs) -> Union[bytes, "Delayed", None]:
2814         """Write DataArray contents to a netCDF file.
2815 
2816         All parameters are passed directly to :py:meth:`xarray.Dataset.to_netcdf`.
2817 
2818         Notes
2819         -----
2820         Only xarray.Dataset objects can be written to netCDF files, so
2821         the xarray.DataArray is converted to a xarray.Dataset object
2822         containing a single variable. If the DataArray has no name, or if the
2823         name is the same as a coordinate name, then it is given the name
2824         ``"__xarray_dataarray_variable__"``.
2825 
2826         See Also
2827         --------
2828         Dataset.to_netcdf
2829         """
2830         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE
2831 
2832         if self.name is None:
2833             # If no name is set then use a generic xarray name
2834             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2835         elif self.name in self.coords or self.name in self.dims:
2836             # The name is the same as one of the coords names, which netCDF
2837             # doesn't support, so rename it but keep track of the old name
2838             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
2839             dataset.attrs[DATAARRAY_NAME] = self.name
2840         else:
2841             # No problems with the name - so we're fine!
2842             dataset = self.to_dataset()
2843 
2844         return dataset.to_netcdf(*args, **kwargs)
2845 
2846     def to_dict(self, data: bool = True) -> dict:
2847         """
2848         Convert this xarray.DataArray into a dictionary following xarray
2849         naming conventions.
2850 
2851         Converts all variables and attributes to native Python objects.
2852         Useful for converting to json. To avoid datetime incompatibility
2853         use decode_times=False kwarg in xarray.open_dataset.
2854 
2855         Parameters
2856         ----------
2857         data : bool, optional
2858             Whether to include the actual data in the dictionary. When set to
2859             False, returns just the schema.
2860 
2861         See Also
2862         --------
2863         DataArray.from_dict
2864         """
2865         d = self.variable.to_dict(data=data)
2866         d.update({"coords": {}, "name": self.name})
2867         for k in self.coords:
2868             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
2869         return d
2870 
2871     @classmethod
2872     def from_dict(cls, d: dict) -> "DataArray":
2873         """
2874         Convert a dictionary into an xarray.DataArray
2875 
2876         Input dict can take several forms:
2877 
2878         .. code:: python
2879 
2880             d = {"dims": ("t"), "data": x}
2881 
2882             d = {
2883                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
2884                 "attrs": {"title": "air temperature"},
2885                 "dims": "t",
2886                 "data": x,
2887                 "name": "a",
2888             }
2889 
2890         where "t" is the name of the dimension, "a" is the name of the array,
2891         and x and t are lists, numpy.arrays, or pandas objects.
2892 
2893         Parameters
2894         ----------
2895         d : dict
2896             Mapping with a minimum structure of {"dims": [...], "data": [...]}
2897 
2898         Returns
2899         -------
2900         obj : xarray.DataArray
2901 
2902         See Also
2903         --------
2904         DataArray.to_dict
2905         Dataset.from_dict
2906         """
2907         coords = None
2908         if "coords" in d:
2909             try:
2910                 coords = {
2911                     k: (v["dims"], v["data"], v.get("attrs"))
2912                     for k, v in d["coords"].items()
2913                 }
2914             except KeyError as e:
2915                 raise ValueError(
2916                     "cannot convert dict when coords are missing the key "
2917                     "'{dims_data}'".format(dims_data=str(e.args[0]))
2918                 )
2919         try:
2920             data = d["data"]
2921         except KeyError:
2922             raise ValueError("cannot convert dict without the key 'data''")
2923         else:
2924             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
2925         return obj
2926 
2927     @classmethod
2928     def from_series(cls, series: pd.Series, sparse: bool = False) -> "DataArray":
2929         """Convert a pandas.Series into an xarray.DataArray.
2930 
2931         If the series's index is a MultiIndex, it will be expanded into a
2932         tensor product of one-dimensional coordinates (filling in missing
2933         values with NaN). Thus this operation should be the inverse of the
2934         `to_series` method.
2935 
2936         If sparse=True, creates a sparse array instead of a dense NumPy array.
2937         Requires the pydata/sparse package.
2938 
2939         See Also
2940         --------
2941         xarray.Dataset.from_dataframe
2942         """
2943         temp_name = "__temporary_name"
2944         df = pd.DataFrame({temp_name: series})
2945         ds = Dataset.from_dataframe(df, sparse=sparse)
2946         result = cast(DataArray, ds[temp_name])
2947         result.name = series.name
2948         return result
2949 
2950     def to_cdms2(self) -> "cdms2_Variable":
2951         """Convert this array into a cdms2.Variable"""
2952         from ..convert import to_cdms2
2953 
2954         return to_cdms2(self)
2955 
2956     @classmethod
2957     def from_cdms2(cls, variable: "cdms2_Variable") -> "DataArray":
2958         """Convert a cdms2.Variable into an xarray.DataArray"""
2959         from ..convert import from_cdms2
2960 
2961         return from_cdms2(variable)
2962 
2963     def to_iris(self) -> "iris_Cube":
2964         """Convert this array into a iris.cube.Cube"""
2965         from ..convert import to_iris
2966 
2967         return to_iris(self)
2968 
2969     @classmethod
2970     def from_iris(cls, cube: "iris_Cube") -> "DataArray":
2971         """Convert a iris.cube.Cube into an xarray.DataArray"""
2972         from ..convert import from_iris
2973 
2974         return from_iris(cube)
2975 
2976     def _all_compat(self, other: "DataArray", compat_str: str) -> bool:
2977         """Helper function for equals, broadcast_equals, and identical"""
2978 
2979         def compat(x, y):
2980             return getattr(x.variable, compat_str)(y.variable)
2981 
2982         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
2983             self, other
2984         )
2985 
2986     def broadcast_equals(self, other: "DataArray") -> bool:
2987         """Two DataArrays are broadcast equal if they are equal after
2988         broadcasting them against each other such that they have the same
2989         dimensions.
2990 
2991         See Also
2992         --------
2993         DataArray.equals
2994         DataArray.identical
2995         """
2996         try:
2997             return self._all_compat(other, "broadcast_equals")
2998         except (TypeError, AttributeError):
2999             return False
3000 
3001     def equals(self, other: "DataArray") -> bool:
3002         """True if two DataArrays have the same dimensions, coordinates and
3003         values; otherwise False.
3004 
3005         DataArrays can still be equal (like pandas objects) if they have NaN
3006         values in the same locations.
3007 
3008         This method is necessary because `v1 == v2` for ``DataArray``
3009         does element-wise comparisons (like numpy.ndarrays).
3010 
3011         See Also
3012         --------
3013         DataArray.broadcast_equals
3014         DataArray.identical
3015         """
3016         try:
3017             return self._all_compat(other, "equals")
3018         except (TypeError, AttributeError):
3019             return False
3020 
3021     def identical(self, other: "DataArray") -> bool:
3022         """Like equals, but also checks the array name and attributes, and
3023         attributes on all coordinates.
3024 
3025         See Also
3026         --------
3027         DataArray.broadcast_equals
3028         DataArray.equals
3029         """
3030         try:
3031             return self.name == other.name and self._all_compat(other, "identical")
3032         except (TypeError, AttributeError):
3033             return False
3034 
3035     def _result_name(self, other: Any = None) -> Optional[Hashable]:
3036         # use the same naming heuristics as pandas:
3037         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
3038         other_name = getattr(other, "name", _default)
3039         if other_name is _default or other_name == self.name:
3040             return self.name
3041         else:
3042             return None
3043 
3044     def __array_wrap__(self, obj, context=None) -> "DataArray":
3045         new_var = self.variable.__array_wrap__(obj, context)
3046         return self._replace(new_var)
3047 
3048     def __matmul__(self, obj):
3049         return self.dot(obj)
3050 
3051     def __rmatmul__(self, other):
3052         # currently somewhat duplicative, as only other DataArrays are
3053         # compatible with matmul
3054         return computation.dot(other, self)
3055 
3056     def _unary_op(self, f: Callable, *args, **kwargs):
3057         keep_attrs = kwargs.pop("keep_attrs", None)
3058         if keep_attrs is None:
3059             keep_attrs = _get_keep_attrs(default=True)
3060         with warnings.catch_warnings():
3061             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
3062             warnings.filterwarnings(
3063                 "ignore", r"Mean of empty slice", category=RuntimeWarning
3064             )
3065             with np.errstate(all="ignore"):
3066                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
3067             if keep_attrs:
3068                 da.attrs = self.attrs
3069             return da
3070 
3071     def _binary_op(
3072         self,
3073         other,
3074         f: Callable,
3075         reflexive: bool = False,
3076     ):
3077         if isinstance(other, (Dataset, groupby.GroupBy)):
3078             return NotImplemented
3079         if isinstance(other, DataArray):
3080             align_type = OPTIONS["arithmetic_join"]
3081             self, other = align(self, other, join=align_type, copy=False)
3082         other_variable = getattr(other, "variable", other)
3083         other_coords = getattr(other, "coords", None)
3084 
3085         variable = (
3086             f(self.variable, other_variable)
3087             if not reflexive
3088             else f(other_variable, self.variable)
3089         )
3090         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
3091         name = self._result_name(other)
3092 
3093         return self._replace(variable, coords, name, indexes=indexes)
3094 
3095     def _inplace_binary_op(self, other, f: Callable):
3096         if isinstance(other, groupby.GroupBy):
3097             raise TypeError(
3098                 "in-place operations between a DataArray and "
3099                 "a grouped object are not permitted"
3100             )
3101         # n.b. we can't align other to self (with other.reindex_like(self))
3102         # because `other` may be converted into floats, which would cause
3103         # in-place arithmetic to fail unpredictably. Instead, we simply
3104         # don't support automatic alignment with in-place arithmetic.
3105         other_coords = getattr(other, "coords", None)
3106         other_variable = getattr(other, "variable", other)
3107         try:
3108             with self.coords._merge_inplace(other_coords):
3109                 f(self.variable, other_variable)
3110         except MergeError as exc:
3111             raise MergeError(
3112                 "Automatic alignment is not supported for in-place operations.\n"
3113                 "Consider aligning the indices manually or using a not-in-place operation.\n"
3114                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3115             ) from exc
3116         return self
3117 
3118     def _copy_attrs_from(self, other: Union["DataArray", Dataset, Variable]) -> None:
3119         self.attrs = other.attrs
3120 
3121     plot = utils.UncachedAccessor(_PlotMethods)
3122 
3123     def _title_for_slice(self, truncate: int = 50) -> str:
3124         """
3125         If the dataarray has 1 dimensional coordinates or comes from a slice
3126         we can show that info in the title
3127 
3128         Parameters
3129         ----------
3130         truncate : int, default: 50
3131             maximum number of characters for title
3132 
3133         Returns
3134         -------
3135         title : string
3136             Can be used for plot titles
3137 
3138         """
3139         one_dims = []
3140         for dim, coord in self.coords.items():
3141             if coord.size == 1:
3142                 one_dims.append(
3143                     "{dim} = {v}{unit}".format(
3144                         dim=dim,
3145                         v=format_item(coord.values),
3146                         unit=_get_units_from_attrs(coord),
3147                     )
3148                 )
3149 
3150         title = ", ".join(one_dims)
3151         if len(title) > truncate:
3152             title = title[: (truncate - 3)] + "..."
3153 
3154         return title
3155 
3156     def diff(self, dim: Hashable, n: int = 1, label: Hashable = "upper") -> "DataArray":
3157         """Calculate the n-th order discrete difference along given axis.
3158 
3159         Parameters
3160         ----------
3161         dim : hashable
3162             Dimension over which to calculate the finite difference.
3163         n : int, optional
3164             The number of times values are differenced.
3165         label : hashable, optional
3166             The new coordinate in dimension ``dim`` will have the
3167             values of either the minuend's or subtrahend's coordinate
3168             for values 'upper' and 'lower', respectively.  Other
3169             values are not supported.
3170 
3171         Returns
3172         -------
3173         difference : same type as caller
3174             The n-th order finite difference of this object.
3175 
3176         Notes
3177         -----
3178         `n` matches numpy's behavior and is different from pandas' first argument named
3179         `periods`.
3180 
3181         Examples
3182         --------
3183         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3184         >>> arr.diff("x")
3185         <xarray.DataArray (x: 3)>
3186         array([0, 1, 0])
3187         Coordinates:
3188           * x        (x) int64 2 3 4
3189         >>> arr.diff("x", 2)
3190         <xarray.DataArray (x: 2)>
3191         array([ 1, -1])
3192         Coordinates:
3193           * x        (x) int64 3 4
3194 
3195         See Also
3196         --------
3197         DataArray.differentiate
3198         """
3199         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3200         return self._from_temp_dataset(ds)
3201 
3202     def shift(
3203         self,
3204         shifts: Mapping[Any, int] = None,
3205         fill_value: Any = dtypes.NA,
3206         **shifts_kwargs: int,
3207     ) -> "DataArray":
3208         """Shift this DataArray by an offset along one or more dimensions.
3209 
3210         Only the data is moved; coordinates stay in place. This is consistent
3211         with the behavior of ``shift`` in pandas.
3212 
3213         Values shifted from beyond array bounds will appear at one end of
3214         each dimension, which are filled according to `fill_value`. For periodic
3215         offsets instead see `roll`.
3216 
3217         Parameters
3218         ----------
3219         shifts : mapping of hashable to int, optional
3220             Integer offset to shift along each of the given dimensions.
3221             Positive offsets shift to the right; negative offsets shift to the
3222             left.
3223         fill_value : scalar, optional
3224             Value to use for newly missing values
3225         **shifts_kwargs
3226             The keyword arguments form of ``shifts``.
3227             One of shifts or shifts_kwargs must be provided.
3228 
3229         Returns
3230         -------
3231         shifted : DataArray
3232             DataArray with the same coordinates and attributes but shifted
3233             data.
3234 
3235         See Also
3236         --------
3237         roll
3238 
3239         Examples
3240         --------
3241         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3242         >>> arr.shift(x=1)
3243         <xarray.DataArray (x: 3)>
3244         array([nan,  5.,  6.])
3245         Dimensions without coordinates: x
3246         """
3247         variable = self.variable.shift(
3248             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3249         )
3250         return self._replace(variable=variable)
3251 
3252     def roll(
3253         self,
3254         shifts: Mapping[Hashable, int] = None,
3255         roll_coords: bool = False,
3256         **shifts_kwargs: int,
3257     ) -> "DataArray":
3258         """Roll this array by an offset along one or more dimensions.
3259 
3260         Unlike shift, roll treats the given dimensions as periodic, so will not
3261         create any missing values to be filled.
3262 
3263         Unlike shift, roll may rotate all variables, including coordinates
3264         if specified. The direction of rotation is consistent with
3265         :py:func:`numpy.roll`.
3266 
3267         Parameters
3268         ----------
3269         shifts : mapping of hashable to int, optional
3270             Integer offset to rotate each of the given dimensions.
3271             Positive offsets roll to the right; negative offsets roll to the
3272             left.
3273         roll_coords : bool, default: False
3274             Indicates whether to roll the coordinates by the offset too.
3275         **shifts_kwargs : {dim: offset, ...}, optional
3276             The keyword arguments form of ``shifts``.
3277             One of shifts or shifts_kwargs must be provided.
3278 
3279         Returns
3280         -------
3281         rolled : DataArray
3282             DataArray with the same attributes but rolled data and coordinates.
3283 
3284         See Also
3285         --------
3286         shift
3287 
3288         Examples
3289         --------
3290         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3291         >>> arr.roll(x=1)
3292         <xarray.DataArray (x: 3)>
3293         array([7, 5, 6])
3294         Dimensions without coordinates: x
3295         """
3296         ds = self._to_temp_dataset().roll(
3297             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3298         )
3299         return self._from_temp_dataset(ds)
3300 
3301     @property
3302     def real(self) -> "DataArray":
3303         return self._replace(self.variable.real)
3304 
3305     @property
3306     def imag(self) -> "DataArray":
3307         return self._replace(self.variable.imag)
3308 
3309     def dot(
3310         self, other: "DataArray", dims: Union[Hashable, Sequence[Hashable], None] = None
3311     ) -> "DataArray":
3312         """Perform dot product of two DataArrays along their shared dims.
3313 
3314         Equivalent to taking taking tensordot over all shared dims.
3315 
3316         Parameters
3317         ----------
3318         other : DataArray
3319             The other array with which the dot product is performed.
3320         dims : ..., hashable or sequence of hashable, optional
3321             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3322             If not specified, then all the common dimensions are summed over.
3323 
3324         Returns
3325         -------
3326         result : DataArray
3327             Array resulting from the dot product over all shared dimensions.
3328 
3329         See Also
3330         --------
3331         dot
3332         numpy.tensordot
3333 
3334         Examples
3335         --------
3336         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3337         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3338         >>> dm_vals = np.arange(4)
3339         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3340 
3341         >>> dm.dims
3342         ('z',)
3343 
3344         >>> da.dims
3345         ('x', 'y', 'z')
3346 
3347         >>> dot_result = da.dot(dm)
3348         >>> dot_result.dims
3349         ('x', 'y')
3350 
3351         """
3352         if isinstance(other, Dataset):
3353             raise NotImplementedError(
3354                 "dot products are not yet supported with Dataset objects."
3355             )
3356         if not isinstance(other, DataArray):
3357             raise TypeError("dot only operates on DataArrays.")
3358 
3359         return computation.dot(self, other, dims=dims)
3360 
3361     def sortby(
3362         self,
3363         variables: Union[Hashable, "DataArray", Sequence[Union[Hashable, "DataArray"]]],
3364         ascending: bool = True,
3365     ) -> "DataArray":
3366         """Sort object by labels or values (along an axis).
3367 
3368         Sorts the dataarray, either along specified dimensions,
3369         or according to values of 1-D dataarrays that share dimension
3370         with calling object.
3371 
3372         If the input variables are dataarrays, then the dataarrays are aligned
3373         (via left-join) to the calling object prior to sorting by cell values.
3374         NaNs are sorted to the end, following Numpy convention.
3375 
3376         If multiple sorts along the same dimension is
3377         given, numpy's lexsort is performed along that dimension:
3378         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
3379         and the FIRST key in the sequence is used as the primary sort key,
3380         followed by the 2nd key, etc.
3381 
3382         Parameters
3383         ----------
3384         variables : hashable, DataArray, or sequence of hashable or DataArray
3385             1D DataArray objects or name(s) of 1D variable(s) in
3386             coords whose values are used to sort this array.
3387         ascending : bool, optional
3388             Whether to sort by ascending or descending order.
3389 
3390         Returns
3391         -------
3392         sorted : DataArray
3393             A new dataarray where all the specified dims are sorted by dim
3394             labels.
3395 
3396         See Also
3397         --------
3398         Dataset.sortby
3399         numpy.sort
3400         pandas.sort_values
3401         pandas.sort_index
3402 
3403         Examples
3404         --------
3405         >>> da = xr.DataArray(
3406         ...     np.random.rand(5),
3407         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3408         ...     dims="time",
3409         ... )
3410         >>> da
3411         <xarray.DataArray (time: 5)>
3412         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3413         Coordinates:
3414           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3415 
3416         >>> da.sortby(da)
3417         <xarray.DataArray (time: 5)>
3418         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3419         Coordinates:
3420           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3421         """
3422         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3423         return self._from_temp_dataset(ds)
3424 
3425     def quantile(
3426         self,
3427         q: Any,
3428         dim: Union[Hashable, Sequence[Hashable], None] = None,
3429         interpolation: str = "linear",
3430         keep_attrs: bool = None,
3431         skipna: bool = True,
3432     ) -> "DataArray":
3433         """Compute the qth quantile of the data along the specified dimension.
3434 
3435         Returns the qth quantiles(s) of the array elements.
3436 
3437         Parameters
3438         ----------
3439         q : float or array-like of float
3440             Quantile to compute, which must be between 0 and 1 inclusive.
3441         dim : hashable or sequence of hashable, optional
3442             Dimension(s) over which to apply quantile.
3443         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
3444             This optional parameter specifies the interpolation method to
3445             use when the desired quantile lies between two data points
3446             ``i < j``:
3447 
3448                 - linear: ``i + (j - i) * fraction``, where ``fraction`` is
3449                   the fractional part of the index surrounded by ``i`` and
3450                   ``j``.
3451                 - lower: ``i``.
3452                 - higher: ``j``.
3453                 - nearest: ``i`` or ``j``, whichever is nearest.
3454                 - midpoint: ``(i + j) / 2``.
3455         keep_attrs : bool, optional
3456             If True, the dataset's attributes (`attrs`) will be copied from
3457             the original object to the new one.  If False (default), the new
3458             object will be returned without attributes.
3459         skipna : bool, optional
3460             Whether to skip missing values when aggregating.
3461 
3462         Returns
3463         -------
3464         quantiles : DataArray
3465             If `q` is a single quantile, then the result
3466             is a scalar. If multiple percentiles are given, first axis of
3467             the result corresponds to the quantile and a quantile dimension
3468             is added to the return array. The other dimensions are the
3469             dimensions that remain after the reduction of the array.
3470 
3471         See Also
3472         --------
3473         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3474 
3475         Examples
3476         --------
3477         >>> da = xr.DataArray(
3478         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3479         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3480         ...     dims=("x", "y"),
3481         ... )
3482         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3483         <xarray.DataArray ()>
3484         array(0.7)
3485         Coordinates:
3486             quantile  float64 0.0
3487         >>> da.quantile(0, dim="x")
3488         <xarray.DataArray (y: 4)>
3489         array([0.7, 4.2, 2.6, 1.5])
3490         Coordinates:
3491           * y         (y) float64 1.0 1.5 2.0 2.5
3492             quantile  float64 0.0
3493         >>> da.quantile([0, 0.5, 1])
3494         <xarray.DataArray (quantile: 3)>
3495         array([0.7, 3.4, 9.4])
3496         Coordinates:
3497           * quantile  (quantile) float64 0.0 0.5 1.0
3498         >>> da.quantile([0, 0.5, 1], dim="x")
3499         <xarray.DataArray (quantile: 3, y: 4)>
3500         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3501                [3.6 , 5.75, 6.  , 1.7 ],
3502                [6.5 , 7.3 , 9.4 , 1.9 ]])
3503         Coordinates:
3504           * y         (y) float64 1.0 1.5 2.0 2.5
3505           * quantile  (quantile) float64 0.0 0.5 1.0
3506         """
3507 
3508         ds = self._to_temp_dataset().quantile(
3509             q,
3510             dim=dim,
3511             keep_attrs=keep_attrs,
3512             interpolation=interpolation,
3513             skipna=skipna,
3514         )
3515         return self._from_temp_dataset(ds)
3516 
3517     def rank(
3518         self, dim: Hashable, pct: bool = False, keep_attrs: bool = None
3519     ) -> "DataArray":
3520         """Ranks the data.
3521 
3522         Equal values are assigned a rank that is the average of the ranks that
3523         would have been otherwise assigned to all of the values within that
3524         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
3525 
3526         NaNs in the input array are returned as NaNs.
3527 
3528         The `bottleneck` library is required.
3529 
3530         Parameters
3531         ----------
3532         dim : hashable
3533             Dimension over which to compute rank.
3534         pct : bool, optional
3535             If True, compute percentage ranks, otherwise compute integer ranks.
3536         keep_attrs : bool, optional
3537             If True, the dataset's attributes (`attrs`) will be copied from
3538             the original object to the new one.  If False (default), the new
3539             object will be returned without attributes.
3540 
3541         Returns
3542         -------
3543         ranked : DataArray
3544             DataArray with the same coordinates and dtype 'float64'.
3545 
3546         Examples
3547         --------
3548         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3549         >>> arr.rank("x")
3550         <xarray.DataArray (x: 3)>
3551         array([1., 2., 3.])
3552         Dimensions without coordinates: x
3553         """
3554 
3555         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
3556         return self._from_temp_dataset(ds)
3557 
3558     def differentiate(
3559         self, coord: Hashable, edge_order: int = 1, datetime_unit: str = None
3560     ) -> "DataArray":
3561         """ Differentiate the array with the second order accurate central
3562         differences.
3563 
3564         .. note::
3565             This feature is limited to simple cartesian geometry, i.e. coord
3566             must be one dimensional.
3567 
3568         Parameters
3569         ----------
3570         coord : hashable
3571             The coordinate to be used to compute the gradient.
3572         edge_order : {1, 2}, default: 1
3573             N-th order accurate differences at the boundaries.
3574         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
3575                          "us", "ns", "ps", "fs", "as"} or None, optional
3576             Unit to compute gradient. Only valid for datetime coordinate.
3577 
3578         Returns
3579         -------
3580         differentiated: DataArray
3581 
3582         See also
3583         --------
3584         numpy.gradient: corresponding numpy function
3585 
3586         Examples
3587         --------
3588 
3589         >>> da = xr.DataArray(
3590         ...     np.arange(12).reshape(4, 3),
3591         ...     dims=["x", "y"],
3592         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3593         ... )
3594         >>> da
3595         <xarray.DataArray (x: 4, y: 3)>
3596         array([[ 0,  1,  2],
3597                [ 3,  4,  5],
3598                [ 6,  7,  8],
3599                [ 9, 10, 11]])
3600         Coordinates:
3601           * x        (x) float64 0.0 0.1 1.1 1.2
3602         Dimensions without coordinates: y
3603         >>>
3604         >>> da.differentiate("x")
3605         <xarray.DataArray (x: 4, y: 3)>
3606         array([[30.        , 30.        , 30.        ],
3607                [27.54545455, 27.54545455, 27.54545455],
3608                [27.54545455, 27.54545455, 27.54545455],
3609                [30.        , 30.        , 30.        ]])
3610         Coordinates:
3611           * x        (x) float64 0.0 0.1 1.1 1.2
3612         Dimensions without coordinates: y
3613         """
3614         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
3615         return self._from_temp_dataset(ds)
3616 
3617     def integrate(
3618         self,
3619         coord: Union[Hashable, Sequence[Hashable]] = None,
3620         datetime_unit: str = None,
3621     ) -> "DataArray":
3622         """Integrate along the given coordinate using the trapezoidal rule.
3623 
3624         .. note::
3625             This feature is limited to simple cartesian geometry, i.e. coord
3626             must be one dimensional.
3627 
3628         Parameters
3629         ----------
3630         coord : hashable, or sequence of hashable
3631             Coordinate(s) used for the integration.
3632         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3633                         'ps', 'fs', 'as'}, optional
3634             Specify the unit if a datetime coordinate is used.
3635 
3636         Returns
3637         -------
3638         integrated : DataArray
3639 
3640         See also
3641         --------
3642         Dataset.integrate
3643         numpy.trapz : corresponding numpy function
3644 
3645         Examples
3646         --------
3647 
3648         >>> da = xr.DataArray(
3649         ...     np.arange(12).reshape(4, 3),
3650         ...     dims=["x", "y"],
3651         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3652         ... )
3653         >>> da
3654         <xarray.DataArray (x: 4, y: 3)>
3655         array([[ 0,  1,  2],
3656                [ 3,  4,  5],
3657                [ 6,  7,  8],
3658                [ 9, 10, 11]])
3659         Coordinates:
3660           * x        (x) float64 0.0 0.1 1.1 1.2
3661         Dimensions without coordinates: y
3662         >>>
3663         >>> da.integrate("x")
3664         <xarray.DataArray (y: 3)>
3665         array([5.4, 6.6, 7.8])
3666         Dimensions without coordinates: y
3667         """
3668         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
3669         return self._from_temp_dataset(ds)
3670 
3671     def cumulative_integrate(
3672         self,
3673         coord: Union[Hashable, Sequence[Hashable]] = None,
3674         datetime_unit: str = None,
3675     ) -> "DataArray":
3676         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
3677 
3678         .. note::
3679             This feature is limited to simple cartesian geometry, i.e. coord
3680             must be one dimensional.
3681 
3682             The first entry of the cumulative integral is always 0, in order to keep the
3683             length of the dimension unchanged between input and output.
3684 
3685         Parameters
3686         ----------
3687         coord : hashable, or sequence of hashable
3688             Coordinate(s) used for the integration.
3689         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
3690                         'ps', 'fs', 'as'}, optional
3691             Specify the unit if a datetime coordinate is used.
3692 
3693         Returns
3694         -------
3695         integrated : DataArray
3696 
3697         See also
3698         --------
3699         Dataset.cumulative_integrate
3700         scipy.integrate.cumulative_trapezoid : corresponding scipy function
3701 
3702         Examples
3703         --------
3704 
3705         >>> da = xr.DataArray(
3706         ...     np.arange(12).reshape(4, 3),
3707         ...     dims=["x", "y"],
3708         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
3709         ... )
3710         >>> da
3711         <xarray.DataArray (x: 4, y: 3)>
3712         array([[ 0,  1,  2],
3713                [ 3,  4,  5],
3714                [ 6,  7,  8],
3715                [ 9, 10, 11]])
3716         Coordinates:
3717           * x        (x) float64 0.0 0.1 1.1 1.2
3718         Dimensions without coordinates: y
3719         >>>
3720         >>> da.cumulative_integrate("x")
3721         <xarray.DataArray (x: 4, y: 3)>
3722         array([[0.  , 0.  , 0.  ],
3723                [0.15, 0.25, 0.35],
3724                [4.65, 5.75, 6.85],
3725                [5.4 , 6.6 , 7.8 ]])
3726         Coordinates:
3727           * x        (x) float64 0.0 0.1 1.1 1.2
3728         Dimensions without coordinates: y
3729         """
3730         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
3731         return self._from_temp_dataset(ds)
3732 
3733     def unify_chunks(self) -> "DataArray":
3734         """Unify chunk size along all chunked dimensions of this DataArray.
3735 
3736         Returns
3737         -------
3738         DataArray with consistent chunk sizes for all dask-array variables
3739 
3740         See Also
3741         --------
3742         dask.array.core.unify_chunks
3743         """
3744 
3745         return unify_chunks(self)[0]
3746 
3747     def map_blocks(
3748         self,
3749         func: Callable[..., T_Xarray],
3750         args: Sequence[Any] = (),
3751         kwargs: Mapping[str, Any] = None,
3752         template: Union["DataArray", "Dataset"] = None,
3753     ) -> T_Xarray:
3754         """
3755         Apply a function to each block of this DataArray.
3756 
3757         .. warning::
3758             This method is experimental and its signature may change.
3759 
3760         Parameters
3761         ----------
3762         func : callable
3763             User-provided function that accepts a DataArray as its first
3764             parameter. The function will receive a subset or 'block' of this DataArray (see below),
3765             corresponding to one chunk along each chunked dimension. ``func`` will be
3766             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
3767 
3768             This function must return either a single DataArray or a single Dataset.
3769 
3770             This function cannot add a new chunked dimension.
3771         args : sequence
3772             Passed to func after unpacking and subsetting any xarray objects by blocks.
3773             xarray objects in args must be aligned with this object, otherwise an error is raised.
3774         kwargs : mapping
3775             Passed verbatim to func after unpacking. xarray objects, if any, will not be
3776             subset to blocks. Passing dask collections in kwargs is not allowed.
3777         template : DataArray or Dataset, optional
3778             xarray object representing the final result after compute is called. If not provided,
3779             the function will be first run on mocked-up data, that looks like this object but
3780             has sizes 0, to determine properties of the returned object such as dtype,
3781             variable names, attributes, new dimensions and new indexes (if any).
3782             ``template`` must be provided if the function changes the size of existing dimensions.
3783             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
3784             ``attrs`` set by ``func`` will be ignored.
3785 
3786         Returns
3787         -------
3788         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
3789         function.
3790 
3791         Notes
3792         -----
3793         This function is designed for when ``func`` needs to manipulate a whole xarray object
3794         subset to each block. Each block is loaded into memory. In the more common case where
3795         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
3796 
3797         If none of the variables in this object is backed by dask arrays, calling this function is
3798         equivalent to calling ``func(obj, *args, **kwargs)``.
3799 
3800         See Also
3801         --------
3802         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
3803         xarray.DataArray.map_blocks
3804 
3805         Examples
3806         --------
3807         Calculate an anomaly from climatology using ``.groupby()``. Using
3808         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
3809         its indices, and its methods like ``.groupby()``.
3810 
3811         >>> def calculate_anomaly(da, groupby_type="time.month"):
3812         ...     gb = da.groupby(groupby_type)
3813         ...     clim = gb.mean(dim="time")
3814         ...     return gb - clim
3815         ...
3816         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
3817         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
3818         >>> np.random.seed(123)
3819         >>> array = xr.DataArray(
3820         ...     np.random.rand(len(time)),
3821         ...     dims=["time"],
3822         ...     coords={"time": time, "month": month},
3823         ... ).chunk()
3824         >>> array.map_blocks(calculate_anomaly, template=array).compute()
3825         <xarray.DataArray (time: 24)>
3826         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
3827                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
3828                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
3829                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
3830                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
3831         Coordinates:
3832           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3833             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
3834 
3835         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
3836         to the function being applied in ``xr.map_blocks()``:
3837 
3838         >>> array.map_blocks(
3839         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
3840         ... )  # doctest: +ELLIPSIS
3841         <xarray.DataArray (time: 24)>
3842         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
3843         Coordinates:
3844           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
3845             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
3846         """
3847         from .parallel import map_blocks
3848 
3849         return map_blocks(func, self, args, kwargs, template)
3850 
3851     def polyfit(
3852         self,
3853         dim: Hashable,
3854         deg: int,
3855         skipna: bool = None,
3856         rcond: float = None,
3857         w: Union[Hashable, Any] = None,
3858         full: bool = False,
3859         cov: bool = False,
3860     ):
3861         """
3862         Least squares polynomial fit.
3863 
3864         This replicates the behaviour of `numpy.polyfit` but differs by skipping
3865         invalid values when `skipna = True`.
3866 
3867         Parameters
3868         ----------
3869         dim : hashable
3870             Coordinate along which to fit the polynomials.
3871         deg : int
3872             Degree of the fitting polynomial.
3873         skipna : bool, optional
3874             If True, removes all invalid values before fitting each 1D slices of the array.
3875             Default is True if data is stored in a dask.array or if there is any
3876             invalid values, False otherwise.
3877         rcond : float, optional
3878             Relative condition number to the fit.
3879         w : hashable or array-like, optional
3880             Weights to apply to the y-coordinate of the sample points.
3881             Can be an array-like object or the name of a coordinate in the dataset.
3882         full : bool, optional
3883             Whether to return the residuals, matrix rank and singular values in addition
3884             to the coefficients.
3885         cov : bool or str, optional
3886             Whether to return to the covariance matrix in addition to the coefficients.
3887             The matrix is not scaled if `cov='unscaled'`.
3888 
3889         Returns
3890         -------
3891         polyfit_results : Dataset
3892             A single dataset which contains:
3893 
3894             polyfit_coefficients
3895                 The coefficients of the best fit.
3896             polyfit_residuals
3897                 The residuals of the least-square computation (only included if `full=True`).
3898                 When the matrix rank is deficient, np.nan is returned.
3899             [dim]_matrix_rank
3900                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3901             [dim]_singular_value
3902                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
3903             polyfit_covariance
3904                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
3905 
3906         See Also
3907         --------
3908         numpy.polyfit
3909         numpy.polyval
3910         xarray.polyval
3911         """
3912         return self._to_temp_dataset().polyfit(
3913             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
3914         )
3915 
3916     def pad(
3917         self,
3918         pad_width: Mapping[Any, Union[int, Tuple[int, int]]] = None,
3919         mode: str = "constant",
3920         stat_length: Union[int, Tuple[int, int], Mapping[Any, Tuple[int, int]]] = None,
3921         constant_values: Union[
3922             int, Tuple[int, int], Mapping[Any, Tuple[int, int]]
3923         ] = None,
3924         end_values: Union[int, Tuple[int, int], Mapping[Any, Tuple[int, int]]] = None,
3925         reflect_type: str = None,
3926         **pad_width_kwargs: Any,
3927     ) -> "DataArray":
3928         """Pad this array along one or more dimensions.
3929 
3930         .. warning::
3931             This function is experimental and its behaviour is likely to change
3932             especially regarding padding of dimension coordinates (or IndexVariables).
3933 
3934         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
3935         coordinates will be padded with the same mode, otherwise coordinates
3936         are padded using the "constant" mode with fill_value dtypes.NA.
3937 
3938         Parameters
3939         ----------
3940         pad_width : mapping of hashable to tuple of int
3941             Mapping with the form of {dim: (pad_before, pad_after)}
3942             describing the number of values padded along each dimension.
3943             {dim: pad} is a shortcut for pad_before = pad_after = pad
3944         mode : str, default: "constant"
3945             One of the following string values (taken from numpy docs)
3946 
3947             'constant' (default)
3948                 Pads with a constant value.
3949             'edge'
3950                 Pads with the edge values of array.
3951             'linear_ramp'
3952                 Pads with the linear ramp between end_value and the
3953                 array edge value.
3954             'maximum'
3955                 Pads with the maximum value of all or part of the
3956                 vector along each axis.
3957             'mean'
3958                 Pads with the mean value of all or part of the
3959                 vector along each axis.
3960             'median'
3961                 Pads with the median value of all or part of the
3962                 vector along each axis.
3963             'minimum'
3964                 Pads with the minimum value of all or part of the
3965                 vector along each axis.
3966             'reflect'
3967                 Pads with the reflection of the vector mirrored on
3968                 the first and last values of the vector along each
3969                 axis.
3970             'symmetric'
3971                 Pads with the reflection of the vector mirrored
3972                 along the edge of the array.
3973             'wrap'
3974                 Pads with the wrap of the vector along the axis.
3975                 The first values are used to pad the end and the
3976                 end values are used to pad the beginning.
3977         stat_length : int, tuple or mapping of hashable to tuple, default: None
3978             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
3979             values at edge of each axis used to calculate the statistic value.
3980             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
3981             statistic lengths along each dimension.
3982             ((before, after),) yields same before and after statistic lengths
3983             for each dimension.
3984             (stat_length,) or int is a shortcut for before = after = statistic
3985             length for all axes.
3986             Default is ``None``, to use the entire axis.
3987         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
3988             Used in 'constant'.  The values to set the padded values for each
3989             axis.
3990             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
3991             pad constants along each dimension.
3992             ``((before, after),)`` yields same before and after constants for each
3993             dimension.
3994             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
3995             all dimensions.
3996             Default is 0.
3997         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
3998             Used in 'linear_ramp'.  The values used for the ending value of the
3999             linear_ramp and that will form the edge of the padded array.
4000             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4001             end values along each dimension.
4002             ``((before, after),)`` yields same before and after end values for each
4003             axis.
4004             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4005             all axes.
4006             Default is 0.
4007         reflect_type : {"even", "odd"}, optional
4008             Used in "reflect", and "symmetric".  The "even" style is the
4009             default with an unaltered reflection around the edge value.  For
4010             the "odd" style, the extended part of the array is created by
4011             subtracting the reflected values from two times the edge value.
4012         **pad_width_kwargs
4013             The keyword arguments form of ``pad_width``.
4014             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
4015 
4016         Returns
4017         -------
4018         padded : DataArray
4019             DataArray with the padded coordinates and data.
4020 
4021         See Also
4022         --------
4023         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
4024 
4025         Notes
4026         -----
4027         For ``mode="constant"`` and ``constant_values=None``, integer types will be
4028         promoted to ``float`` and padded with ``np.nan``.
4029 
4030         Examples
4031         --------
4032         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
4033         >>> arr.pad(x=(1, 2), constant_values=0)
4034         <xarray.DataArray (x: 6)>
4035         array([0, 5, 6, 7, 0, 0])
4036         Coordinates:
4037           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
4038 
4039         >>> da = xr.DataArray(
4040         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
4041         ...     dims=["x", "y"],
4042         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
4043         ... )
4044         >>> da.pad(x=1)
4045         <xarray.DataArray (x: 4, y: 4)>
4046         array([[nan, nan, nan, nan],
4047                [ 0.,  1.,  2.,  3.],
4048                [10., 11., 12., 13.],
4049                [nan, nan, nan, nan]])
4050         Coordinates:
4051           * x        (x) float64 nan 0.0 1.0 nan
4052           * y        (y) int64 10 20 30 40
4053             z        (x) float64 nan 100.0 200.0 nan
4054 
4055         Careful, ``constant_values`` are coerced to the data type of the array which may
4056         lead to a loss of precision:
4057 
4058         >>> da.pad(x=1, constant_values=1.23456789)
4059         <xarray.DataArray (x: 4, y: 4)>
4060         array([[ 1,  1,  1,  1],
4061                [ 0,  1,  2,  3],
4062                [10, 11, 12, 13],
4063                [ 1,  1,  1,  1]])
4064         Coordinates:
4065           * x        (x) float64 nan 0.0 1.0 nan
4066           * y        (y) int64 10 20 30 40
4067             z        (x) float64 nan 100.0 200.0 nan
4068         """
4069         ds = self._to_temp_dataset().pad(
4070             pad_width=pad_width,
4071             mode=mode,
4072             stat_length=stat_length,
4073             constant_values=constant_values,
4074             end_values=end_values,
4075             reflect_type=reflect_type,
4076             **pad_width_kwargs,
4077         )
4078         return self._from_temp_dataset(ds)
4079 
4080     def idxmin(
4081         self,
4082         dim: Hashable = None,
4083         skipna: bool = None,
4084         fill_value: Any = dtypes.NA,
4085         keep_attrs: bool = None,
4086     ) -> "DataArray":
4087         """Return the coordinate label of the minimum value along a dimension.
4088 
4089         Returns a new `DataArray` named after the dimension with the values of
4090         the coordinate labels along that dimension corresponding to minimum
4091         values along that dimension.
4092 
4093         In comparison to :py:meth:`~DataArray.argmin`, this returns the
4094         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
4095 
4096         Parameters
4097         ----------
4098         dim : str, optional
4099             Dimension over which to apply `idxmin`.  This is optional for 1D
4100             arrays, but required for arrays with 2 or more dimensions.
4101         skipna : bool or None, default: None
4102             If True, skip missing values (as marked by NaN). By default, only
4103             skips missing values for ``float``, ``complex``, and ``object``
4104             dtypes; other dtypes either do not have a sentinel missing value
4105             (``int``) or ``skipna=True`` has not been implemented
4106             (``datetime64`` or ``timedelta64``).
4107         fill_value : Any, default: NaN
4108             Value to be filled in case all of the values along a dimension are
4109             null.  By default this is NaN.  The fill value and result are
4110             automatically converted to a compatible dtype if possible.
4111             Ignored if ``skipna`` is False.
4112         keep_attrs : bool, default: False
4113             If True, the attributes (``attrs``) will be copied from the
4114             original object to the new one.  If False (default), the new object
4115             will be returned without attributes.
4116 
4117         Returns
4118         -------
4119         reduced : DataArray
4120             New `DataArray` object with `idxmin` applied to its data and the
4121             indicated dimension removed.
4122 
4123         See Also
4124         --------
4125         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
4126 
4127         Examples
4128         --------
4129         >>> array = xr.DataArray(
4130         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4131         ... )
4132         >>> array.min()
4133         <xarray.DataArray ()>
4134         array(-2)
4135         >>> array.argmin()
4136         <xarray.DataArray ()>
4137         array(4)
4138         >>> array.idxmin()
4139         <xarray.DataArray 'x' ()>
4140         array('e', dtype='<U1')
4141 
4142         >>> array = xr.DataArray(
4143         ...     [
4144         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4145         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4146         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4147         ...     ],
4148         ...     dims=["y", "x"],
4149         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4150         ... )
4151         >>> array.min(dim="x")
4152         <xarray.DataArray (y: 3)>
4153         array([-2., -4.,  1.])
4154         Coordinates:
4155           * y        (y) int64 -1 0 1
4156         >>> array.argmin(dim="x")
4157         <xarray.DataArray (y: 3)>
4158         array([4, 0, 2])
4159         Coordinates:
4160           * y        (y) int64 -1 0 1
4161         >>> array.idxmin(dim="x")
4162         <xarray.DataArray 'x' (y: 3)>
4163         array([16.,  0.,  4.])
4164         Coordinates:
4165           * y        (y) int64 -1 0 1
4166         """
4167         return computation._calc_idxminmax(
4168             array=self,
4169             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4170             dim=dim,
4171             skipna=skipna,
4172             fill_value=fill_value,
4173             keep_attrs=keep_attrs,
4174         )
4175 
4176     def idxmax(
4177         self,
4178         dim: Hashable = None,
4179         skipna: bool = None,
4180         fill_value: Any = dtypes.NA,
4181         keep_attrs: bool = None,
4182     ) -> "DataArray":
4183         """Return the coordinate label of the maximum value along a dimension.
4184 
4185         Returns a new `DataArray` named after the dimension with the values of
4186         the coordinate labels along that dimension corresponding to maximum
4187         values along that dimension.
4188 
4189         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4190         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4191 
4192         Parameters
4193         ----------
4194         dim : hashable, optional
4195             Dimension over which to apply `idxmax`.  This is optional for 1D
4196             arrays, but required for arrays with 2 or more dimensions.
4197         skipna : bool or None, default: None
4198             If True, skip missing values (as marked by NaN). By default, only
4199             skips missing values for ``float``, ``complex``, and ``object``
4200             dtypes; other dtypes either do not have a sentinel missing value
4201             (``int``) or ``skipna=True`` has not been implemented
4202             (``datetime64`` or ``timedelta64``).
4203         fill_value : Any, default: NaN
4204             Value to be filled in case all of the values along a dimension are
4205             null.  By default this is NaN.  The fill value and result are
4206             automatically converted to a compatible dtype if possible.
4207             Ignored if ``skipna`` is False.
4208         keep_attrs : bool, default: False
4209             If True, the attributes (``attrs``) will be copied from the
4210             original object to the new one.  If False (default), the new object
4211             will be returned without attributes.
4212 
4213         Returns
4214         -------
4215         reduced : DataArray
4216             New `DataArray` object with `idxmax` applied to its data and the
4217             indicated dimension removed.
4218 
4219         See Also
4220         --------
4221         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4222 
4223         Examples
4224         --------
4225         >>> array = xr.DataArray(
4226         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4227         ... )
4228         >>> array.max()
4229         <xarray.DataArray ()>
4230         array(2)
4231         >>> array.argmax()
4232         <xarray.DataArray ()>
4233         array(1)
4234         >>> array.idxmax()
4235         <xarray.DataArray 'x' ()>
4236         array('b', dtype='<U1')
4237 
4238         >>> array = xr.DataArray(
4239         ...     [
4240         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4241         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4242         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4243         ...     ],
4244         ...     dims=["y", "x"],
4245         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4246         ... )
4247         >>> array.max(dim="x")
4248         <xarray.DataArray (y: 3)>
4249         array([2., 2., 1.])
4250         Coordinates:
4251           * y        (y) int64 -1 0 1
4252         >>> array.argmax(dim="x")
4253         <xarray.DataArray (y: 3)>
4254         array([0, 2, 2])
4255         Coordinates:
4256           * y        (y) int64 -1 0 1
4257         >>> array.idxmax(dim="x")
4258         <xarray.DataArray 'x' (y: 3)>
4259         array([0., 4., 4.])
4260         Coordinates:
4261           * y        (y) int64 -1 0 1
4262         """
4263         return computation._calc_idxminmax(
4264             array=self,
4265             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4266             dim=dim,
4267             skipna=skipna,
4268             fill_value=fill_value,
4269             keep_attrs=keep_attrs,
4270         )
4271 
4272     def argmin(
4273         self,
4274         dim: Union[Hashable, Sequence[Hashable]] = None,
4275         axis: int = None,
4276         keep_attrs: bool = None,
4277         skipna: bool = None,
4278     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4279         """Index or indices of the minimum of the DataArray over one or more dimensions.
4280 
4281         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4282         which can be passed directly to isel(). If a single str is passed to 'dim' then
4283         returns a DataArray with dtype int.
4284 
4285         If there are multiple minima, the indices of the first one found will be
4286         returned.
4287 
4288         Parameters
4289         ----------
4290         dim : hashable, sequence of hashable or ..., optional
4291             The dimensions over which to find the minimum. By default, finds minimum over
4292             all dimensions - for now returning an int for backward compatibility, but
4293             this is deprecated, in future will return a dict with indices for all
4294             dimensions; to return a dict with all dimensions now, pass '...'.
4295         axis : int, optional
4296             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4297             can be supplied.
4298         keep_attrs : bool, optional
4299             If True, the attributes (`attrs`) will be copied from the original
4300             object to the new one.  If False (default), the new object will be
4301             returned without attributes.
4302         skipna : bool, optional
4303             If True, skip missing values (as marked by NaN). By default, only
4304             skips missing values for float dtypes; other dtypes either do not
4305             have a sentinel missing value (int) or skipna=True has not been
4306             implemented (object, datetime64 or timedelta64).
4307 
4308         Returns
4309         -------
4310         result : DataArray or dict of DataArray
4311 
4312         See Also
4313         --------
4314         Variable.argmin, DataArray.idxmin
4315 
4316         Examples
4317         --------
4318         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4319         >>> array.min()
4320         <xarray.DataArray ()>
4321         array(-1)
4322         >>> array.argmin()
4323         <xarray.DataArray ()>
4324         array(2)
4325         >>> array.argmin(...)
4326         {'x': <xarray.DataArray ()>
4327         array(2)}
4328         >>> array.isel(array.argmin(...))
4329         <xarray.DataArray ()>
4330         array(-1)
4331 
4332         >>> array = xr.DataArray(
4333         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4334         ...     dims=("x", "y", "z"),
4335         ... )
4336         >>> array.min(dim="x")
4337         <xarray.DataArray (y: 3, z: 3)>
4338         array([[ 1,  2,  1],
4339                [ 2, -5,  1],
4340                [ 2,  1,  1]])
4341         Dimensions without coordinates: y, z
4342         >>> array.argmin(dim="x")
4343         <xarray.DataArray (y: 3, z: 3)>
4344         array([[1, 0, 0],
4345                [1, 1, 1],
4346                [0, 0, 1]])
4347         Dimensions without coordinates: y, z
4348         >>> array.argmin(dim=["x"])
4349         {'x': <xarray.DataArray (y: 3, z: 3)>
4350         array([[1, 0, 0],
4351                [1, 1, 1],
4352                [0, 0, 1]])
4353         Dimensions without coordinates: y, z}
4354         >>> array.min(dim=("x", "z"))
4355         <xarray.DataArray (y: 3)>
4356         array([ 1, -5,  1])
4357         Dimensions without coordinates: y
4358         >>> array.argmin(dim=["x", "z"])
4359         {'x': <xarray.DataArray (y: 3)>
4360         array([0, 1, 0])
4361         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4362         array([2, 1, 1])
4363         Dimensions without coordinates: y}
4364         >>> array.isel(array.argmin(dim=["x", "z"]))
4365         <xarray.DataArray (y: 3)>
4366         array([ 1, -5,  1])
4367         Dimensions without coordinates: y
4368         """
4369         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4370         if isinstance(result, dict):
4371             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4372         else:
4373             return self._replace_maybe_drop_dims(result)
4374 
4375     def argmax(
4376         self,
4377         dim: Union[Hashable, Sequence[Hashable]] = None,
4378         axis: int = None,
4379         keep_attrs: bool = None,
4380         skipna: bool = None,
4381     ) -> Union["DataArray", Dict[Hashable, "DataArray"]]:
4382         """Index or indices of the maximum of the DataArray over one or more dimensions.
4383 
4384         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4385         which can be passed directly to isel(). If a single str is passed to 'dim' then
4386         returns a DataArray with dtype int.
4387 
4388         If there are multiple maxima, the indices of the first one found will be
4389         returned.
4390 
4391         Parameters
4392         ----------
4393         dim : hashable, sequence of hashable or ..., optional
4394             The dimensions over which to find the maximum. By default, finds maximum over
4395             all dimensions - for now returning an int for backward compatibility, but
4396             this is deprecated, in future will return a dict with indices for all
4397             dimensions; to return a dict with all dimensions now, pass '...'.
4398         axis : int, optional
4399             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4400             can be supplied.
4401         keep_attrs : bool, optional
4402             If True, the attributes (`attrs`) will be copied from the original
4403             object to the new one.  If False (default), the new object will be
4404             returned without attributes.
4405         skipna : bool, optional
4406             If True, skip missing values (as marked by NaN). By default, only
4407             skips missing values for float dtypes; other dtypes either do not
4408             have a sentinel missing value (int) or skipna=True has not been
4409             implemented (object, datetime64 or timedelta64).
4410 
4411         Returns
4412         -------
4413         result : DataArray or dict of DataArray
4414 
4415         See Also
4416         --------
4417         Variable.argmax, DataArray.idxmax
4418 
4419         Examples
4420         --------
4421         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4422         >>> array.max()
4423         <xarray.DataArray ()>
4424         array(3)
4425         >>> array.argmax()
4426         <xarray.DataArray ()>
4427         array(3)
4428         >>> array.argmax(...)
4429         {'x': <xarray.DataArray ()>
4430         array(3)}
4431         >>> array.isel(array.argmax(...))
4432         <xarray.DataArray ()>
4433         array(3)
4434 
4435         >>> array = xr.DataArray(
4436         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4437         ...     dims=("x", "y", "z"),
4438         ... )
4439         >>> array.max(dim="x")
4440         <xarray.DataArray (y: 3, z: 3)>
4441         array([[3, 3, 2],
4442                [3, 5, 2],
4443                [2, 3, 3]])
4444         Dimensions without coordinates: y, z
4445         >>> array.argmax(dim="x")
4446         <xarray.DataArray (y: 3, z: 3)>
4447         array([[0, 1, 1],
4448                [0, 1, 0],
4449                [0, 1, 0]])
4450         Dimensions without coordinates: y, z
4451         >>> array.argmax(dim=["x"])
4452         {'x': <xarray.DataArray (y: 3, z: 3)>
4453         array([[0, 1, 1],
4454                [0, 1, 0],
4455                [0, 1, 0]])
4456         Dimensions without coordinates: y, z}
4457         >>> array.max(dim=("x", "z"))
4458         <xarray.DataArray (y: 3)>
4459         array([3, 5, 3])
4460         Dimensions without coordinates: y
4461         >>> array.argmax(dim=["x", "z"])
4462         {'x': <xarray.DataArray (y: 3)>
4463         array([0, 1, 0])
4464         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4465         array([0, 1, 2])
4466         Dimensions without coordinates: y}
4467         >>> array.isel(array.argmax(dim=["x", "z"]))
4468         <xarray.DataArray (y: 3)>
4469         array([3, 5, 3])
4470         Dimensions without coordinates: y
4471         """
4472         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4473         if isinstance(result, dict):
4474             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4475         else:
4476             return self._replace_maybe_drop_dims(result)
4477 
4478     def query(
4479         self,
4480         queries: Mapping[Any, Any] = None,
4481         parser: str = "pandas",
4482         engine: str = None,
4483         missing_dims: str = "raise",
4484         **queries_kwargs: Any,
4485     ) -> "DataArray":
4486         """Return a new data array indexed along the specified
4487         dimension(s), where the indexers are given as strings containing
4488         Python expressions to be evaluated against the values in the array.
4489 
4490         Parameters
4491         ----------
4492         queries : dict, optional
4493             A dict with keys matching dimensions and values given by strings
4494             containing Python expressions to be evaluated against the data variables
4495             in the dataset. The expressions will be evaluated using the pandas
4496             eval() function, and can contain any valid Python expressions but cannot
4497             contain any Python statements.
4498         parser : {"pandas", "python"}, default: "pandas"
4499             The parser to use to construct the syntax tree from the expression.
4500             The default of 'pandas' parses code slightly different than standard
4501             Python. Alternatively, you can parse an expression using the 'python'
4502             parser to retain strict Python semantics.
4503         engine : {"python", "numexpr", None}, default: None
4504             The engine used to evaluate the expression. Supported engines are:
4505             - None: tries to use numexpr, falls back to python
4506             - "numexpr": evaluates expressions using numexpr
4507             - "python": performs operations as if you had eval’d in top level python
4508         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
4509             What to do if dimensions that should be selected from are not present in the
4510             Dataset:
4511             - "raise": raise an exception
4512             - "warn": raise a warning, and ignore the missing dimensions
4513             - "ignore": ignore the missing dimensions
4514         **queries_kwargs : {dim: query, ...}, optional
4515             The keyword arguments form of ``queries``.
4516             One of queries or queries_kwargs must be provided.
4517 
4518         Returns
4519         -------
4520         obj : DataArray
4521             A new DataArray with the same contents as this dataset, indexed by
4522             the results of the appropriate queries.
4523 
4524         See Also
4525         --------
4526         DataArray.isel
4527         Dataset.query
4528         pandas.eval
4529 
4530         Examples
4531         --------
4532         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
4533         >>> da
4534         <xarray.DataArray 'a' (x: 5)>
4535         array([0, 1, 2, 3, 4])
4536         Dimensions without coordinates: x
4537         >>> da.query(x="a > 2")
4538         <xarray.DataArray 'a' (x: 2)>
4539         array([3, 4])
4540         Dimensions without coordinates: x
4541         """
4542 
4543         ds = self._to_dataset_whole(shallow_copy=True)
4544         ds = ds.query(
4545             queries=queries,
4546             parser=parser,
4547             engine=engine,
4548             missing_dims=missing_dims,
4549             **queries_kwargs,
4550         )
4551         return ds[self.name]
4552 
4553     def curvefit(
4554         self,
4555         coords: Union[Union[str, "DataArray"], Iterable[Union[str, "DataArray"]]],
4556         func: Callable[..., Any],
4557         reduce_dims: Union[Hashable, Iterable[Hashable]] = None,
4558         skipna: bool = True,
4559         p0: Dict[str, Any] = None,
4560         bounds: Dict[str, Any] = None,
4561         param_names: Sequence[str] = None,
4562         kwargs: Dict[str, Any] = None,
4563     ):
4564         """
4565         Curve fitting optimization for arbitrary functions.
4566 
4567         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
4568 
4569         Parameters
4570         ----------
4571         coords : hashable, DataArray, or sequence of DataArray or hashable
4572             Independent coordinate(s) over which to perform the curve fitting. Must share
4573             at least one dimension with the calling object. When fitting multi-dimensional
4574             functions, supply `coords` as a sequence in the same order as arguments in
4575             `func`. To fit along existing dimensions of the calling object, `coords` can
4576             also be specified as a str or sequence of strs.
4577         func : callable
4578             User specified function in the form `f(x, *params)` which returns a numpy
4579             array of length `len(x)`. `params` are the fittable parameters which are optimized
4580             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
4581             coordinates, e.g. `f((x0, x1), *params)`.
4582         reduce_dims : hashable or sequence of hashable
4583             Additional dimension(s) over which to aggregate while fitting. For example,
4584             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
4585             aggregate all lat and lon points and fit the specified function along the
4586             time dimension.
4587         skipna : bool, optional
4588             Whether to skip missing values when fitting. Default is True.
4589         p0 : dict-like, optional
4590             Optional dictionary of parameter names to initial guesses passed to the
4591             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
4592             be assigned initial values following the default scipy behavior.
4593         bounds : dict-like, optional
4594             Optional dictionary of parameter names to bounding values passed to the
4595             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
4596             will be unbounded following the default scipy behavior.
4597         param_names : sequence of hashable, optional
4598             Sequence of names for the fittable parameters of `func`. If not supplied,
4599             this will be automatically determined by arguments of `func`. `param_names`
4600             should be manually supplied when fitting a function that takes a variable
4601             number of parameters.
4602         **kwargs : optional
4603             Additional keyword arguments to passed to scipy curve_fit.
4604 
4605         Returns
4606         -------
4607         curvefit_results : Dataset
4608             A single dataset which contains:
4609 
4610             [var]_curvefit_coefficients
4611                 The coefficients of the best fit.
4612             [var]_curvefit_covariance
4613                 The covariance matrix of the coefficient estimates.
4614 
4615         See Also
4616         --------
4617         DataArray.polyfit
4618         scipy.optimize.curve_fit
4619         """
4620         return self._to_temp_dataset().curvefit(
4621             coords,
4622             func,
4623             reduce_dims=reduce_dims,
4624             skipna=skipna,
4625             p0=p0,
4626             bounds=bounds,
4627             param_names=param_names,
4628             kwargs=kwargs,
4629         )
4630 
4631     def drop_duplicates(
4632         self,
4633         dim: Hashable,
4634         keep: Union[
4635             str,
4636             bool,
4637         ] = "first",
4638     ):
4639         """Returns a new DataArray with duplicate dimension values removed.
4640 
4641         Parameters
4642         ----------
4643         dim : dimension label, optional
4644         keep : {"first", "last", False}, default: "first"
4645             Determines which duplicates (if any) to keep.
4646             - ``"first"`` : Drop duplicates except for the first occurrence.
4647             - ``"last"`` : Drop duplicates except for the last occurrence.
4648             - False : Drop all duplicates.
4649 
4650         Returns
4651         -------
4652         DataArray
4653         """
4654         if dim not in self.dims:
4655             raise ValueError(f"'{dim}' not found in dimensions")
4656         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep)}
4657         return self.isel(indexes)
4658 
4659     # this needs to be at the end, or mypy will confuse with `str`
4660     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
4661     str = utils.UncachedAccessor(StringAccessor)

</code><intended_behavior>Add a new method `cross` to the `DataArray` class. This method would provide a high-level interface to the `cross` function defined in `computation.py`, allowing users to call the cross product operation directly on `DataArray` instances, similar to how they currently use the `dot` method. This method should validate the input and dimensions, similarly ensuring that only suitable `DataArray` objects are passed (e.g., checking that the spatial dimension is correctly sized and exists within the arrays). This extension will make the feature easily accessible to users, integrating seamlessly with the existing API.</intended_behavior>]
2025-10-15 08:53:04.225 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 08:53:04.226 | INFO     | Search-only: skipping patch selection
2025-10-15 08:53:04.312 | INFO     | Task pydata__xarray-5365 completed successfully.
2025-10-15 08:53:04.313 | INFO     | No patch generated. You can try running ACR again.
