2025-10-15 09:00:45.856 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 09:00:50.323 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=946, output_tokens=13, cost=0.009850
2025-10-15 09:00:59.772 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1126, output_tokens=390, cost=0.022960
2025-10-15 09:01:01.091 | INFO     | registered non reproducing test 0
2025-10-15 09:01:16.396 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2253, output_tokens=482, cost=0.036990
2025-10-15 09:01:17.664 | INFO     | registered non reproducing test 1
2025-10-15 09:01:34.484 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2343, output_tokens=476, cost=0.037710
2025-10-15 09:01:35.770 | INFO     | registered non reproducing test 2
2025-10-15 09:01:35.770 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 09:01:35.773 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 09:01:48.462 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1244, output_tokens=338, cost=0.022580
2025-10-15 09:01:48.469 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 09:01:51.898 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=692, output_tokens=63, cost=0.008810
2025-10-15 09:01:51.899 | DEBUG    | Extracted a valid json.
2025-10-15 09:01:51.899 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"DataArray\")",
        "search_method_in_class(\"rename\", \"DataArray\")",
        "search_method_in_class(\"set_index\", \"DataArray\")",
        "search_method(\"create_index\")"
    ],
    "bug_locations": []
}
2025-10-15 09:01:52.031 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 09:01:52.032 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 09:02:19.043 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=9070, output_tokens=728, cost=0.112540
2025-10-15 09:02:19.058 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 09:02:26.550 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=10014, output_tokens=187, cost=0.105750
2025-10-15 09:02:26.555 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 09:02:33.515 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=541, output_tokens=192, cost=0.011170
2025-10-15 09:02:33.516 | DEBUG    | Extracted a valid json.
2025-10-15 09:02:33.516 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "rename",
            "intended_behavior": "The `rename` method, when used to rename a coordinate to a dimension name, needs to ensure that an index is created or updated for the newly dimension coordinate. This behavior would align it more closely with the explicit indexing functionalities seen in `set_index`. The method should check if the renaming operation converts a non-dimension coordinate into a dimension coordinate and, in such cases, either automatically handle the indexing similar to `set_index` or raise a user-friendly error message advising the use of `set_index` for such cases. This modification preserves the existing functionality by still allowing simple renaming operations while ensuring that any operation turning coordinates into dimension coordinates maintains the integrity and expected behavior of DataArray indexing."
        }
    ]
}
2025-10-15 09:02:33.533 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>rename</method>
<code>
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)

</code><intended_behavior>The `rename` method, when used to rename a coordinate to a dimension name, needs to ensure that an index is created or updated for the newly dimension coordinate. This behavior would align it more closely with the explicit indexing functionalities seen in `set_index`. The method should check if the renaming operation converts a non-dimension coordinate into a dimension coordinate and, in such cases, either automatically handle the indexing similar to `set_index` or raise a user-friendly error message advising the use of `set_index` for such cases. This modification preserves the existing functionality by still allowing simple renaming operations while ensuring that any operation turning coordinates into dimension coordinates maintains the integrity and expected behavior of DataArray indexing.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
223 class DataArray(
224     AbstractArray, DataWithCoords, DataArrayArithmetic, DataArrayReductions
225 ):
226     """N-dimensional array with labeled coordinates and dimensions.
227 
228     DataArray provides a wrapper around numpy ndarrays that uses
229     labeled dimensions and coordinates to support metadata aware
230     operations. The API is similar to that for the pandas Series or
231     DataFrame, but DataArray objects can have any number of dimensions,
232     and their contents have fixed data types.
233 
234     Additional features over raw numpy arrays:
235 
236     - Apply operations over dimensions by name: ``x.sum('time')``.
237     - Select or assign values by integer location (like numpy):
238       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
239       ``x.sel(time='2014-01-01')``.
240     - Mathematical operations (e.g., ``x - y``) vectorize across
241       multiple dimensions (known in numpy as "broadcasting") based on
242       dimension names, regardless of their original order.
243     - Keep track of arbitrary metadata in the form of a Python
244       dictionary: ``x.attrs``
245     - Convert to a pandas Series: ``x.to_series()``.
246 
247     Getting items from or doing mathematical operations with a
248     DataArray always returns another DataArray.
249 
250     Parameters
251     ----------
252     data : array_like
253         Values for this array. Must be an ``numpy.ndarray``, ndarray
254         like, or castable to an ``ndarray``. If a self-described xarray
255         or pandas object, attempts are made to use this array's
256         metadata to fill in other unspecified arguments. A view of the
257         array's data is used instead of a copy if possible.
258     coords : sequence or dict of array_like, optional
259         Coordinates (tick labels) to use for indexing along each
260         dimension. The following notations are accepted:
261 
262         - mapping {dimension name: array-like}
263         - sequence of tuples that are valid arguments for
264           ``xarray.Variable()``
265           - (dims, data)
266           - (dims, data, attrs)
267           - (dims, data, attrs, encoding)
268 
269         Additionally, it is possible to define a coord whose name
270         does not match the dimension name, or a coord based on multiple
271         dimensions, with one of the following notations:
272 
273         - mapping {coord name: DataArray}
274         - mapping {coord name: Variable}
275         - mapping {coord name: (dimension name, array-like)}
276         - mapping {coord name: (tuple of dimension names, array-like)}
277 
278     dims : Hashable or sequence of Hashable, optional
279         Name(s) of the data dimension(s). Must be either a Hashable
280         (only for 1D data) or a sequence of Hashables with length equal
281         to the number of dimensions. If this argument is omitted,
282         dimension names are taken from ``coords`` (if possible) and
283         otherwise default to ``['dim_0', ... 'dim_n']``.
284     name : str or None, optional
285         Name of this array.
286     attrs : dict_like or None, optional
287         Attributes to assign to the new instance. By default, an empty
288         attribute dictionary is initialized.
289 
290     Examples
291     --------
292     Create data:
293 
294     >>> np.random.seed(0)
295     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
296     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
297     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
298     >>> time = pd.date_range("2014-09-06", periods=3)
299     >>> reference_time = pd.Timestamp("2014-09-05")
300 
301     Initialize a dataarray with multiple dimensions:
302 
303     >>> da = xr.DataArray(
304     ...     data=temperature,
305     ...     dims=["x", "y", "time"],
306     ...     coords=dict(
307     ...         lon=(["x", "y"], lon),
308     ...         lat=(["x", "y"], lat),
309     ...         time=time,
310     ...         reference_time=reference_time,
311     ...     ),
312     ...     attrs=dict(
313     ...         description="Ambient temperature.",
314     ...         units="degC",
315     ...     ),
316     ... )
317     >>> da
318     <xarray.DataArray (x: 2, y: 2, time: 3)>
319     array([[[29.11241877, 18.20125767, 22.82990387],
320             [32.92714559, 29.94046392,  7.18177696]],
321     <BLANKLINE>
322            [[22.60070734, 13.78914233, 14.17424919],
323             [18.28478802, 16.15234857, 26.63418806]]])
324     Coordinates:
325         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
326         lat             (x, y) float64 42.25 42.21 42.63 42.59
327       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
328         reference_time  datetime64[ns] 2014-09-05
329     Dimensions without coordinates: x, y
330     Attributes:
331         description:  Ambient temperature.
332         units:        degC
333 
334     Find out where the coldest temperature was:
335 
336     >>> da.isel(da.argmin(...))
337     <xarray.DataArray ()>
338     array(7.18177696)
339     Coordinates:
340         lon             float64 -99.32
341         lat             float64 42.21
342         time            datetime64[ns] 2014-09-08
343         reference_time  datetime64[ns] 2014-09-05
344     Attributes:
345         description:  Ambient temperature.
346         units:        degC
347     """
348 
349     _cache: dict[str, Any]
350     _coords: dict[Any, Variable]
351     _close: Callable[[], None] | None
352     _indexes: dict[Hashable, Index]
353     _name: Hashable | None
354     _variable: Variable
355 
356     __slots__ = (
357         "_cache",
358         "_coords",
359         "_close",
360         "_indexes",
361         "_name",
362         "_variable",
363         "__weakref__",
364     )
365 
366     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
367 
368     def __init__(
369         self,
370         data: Any = dtypes.NA,
371         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
372         | Mapping[Any, Any]
373         | None = None,
374         dims: Hashable | Sequence[Hashable] | None = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: dict[Hashable, Index] = None,
379         fastpath: bool = False,
380     ) -> None:
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385             assert indexes is not None
386         else:
387             # TODO: (benbovy - explicit indexes) remove
388             # once it becomes part of the public interface
389             if indexes is not None:
390                 raise ValueError("Providing explicit indexes is not supported yet")
391 
392             # try to fill in arguments from data if they weren't supplied
393             if coords is None:
394 
395                 if isinstance(data, DataArray):
396                     coords = data.coords
397                 elif isinstance(data, pd.Series):
398                     coords = [data.index]
399                 elif isinstance(data, pd.DataFrame):
400                     coords = [data.index, data.columns]
401                 elif isinstance(data, (pd.Index, IndexVariable)):
402                     coords = [data]
403 
404             if dims is None:
405                 dims = getattr(data, "dims", getattr(coords, "dims", None))
406             if name is None:
407                 name = getattr(data, "name", None)
408             if attrs is None and not isinstance(data, PANDAS_TYPES):
409                 attrs = getattr(data, "attrs", None)
410 
411             data = _check_data_shape(data, coords, dims)
412             data = as_compatible_data(data)
413             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
414             variable = Variable(dims, data, attrs, fastpath=True)
415             indexes, coords = _create_indexes_from_coords(coords)
416 
417         # These fully describe a DataArray
418         self._variable = variable
419         assert isinstance(coords, dict)
420         self._coords = coords
421         self._name = name
422 
423         # TODO(shoyer): document this argument, once it becomes part of the
424         # public interface.
425         self._indexes = indexes  # type: ignore[assignment]
426 
427         self._close = None
428 
429     @classmethod
430     def _construct_direct(
431         cls: type[T_DataArray],
432         variable: Variable,
433         coords: dict[Any, Variable],
434         name: Hashable,
435         indexes: dict[Hashable, Index],
436     ) -> T_DataArray:
437         """Shortcut around __init__ for internal use when we want to skip
438         costly validation
439         """
440         obj = object.__new__(cls)
441         obj._variable = variable
442         obj._coords = coords
443         obj._name = name
444         obj._indexes = indexes
445         obj._close = None
446         return obj
447 
448     def _replace(
449         self: T_DataArray,
450         variable: Variable = None,
451         coords=None,
452         name: Hashable | None | Default = _default,
453         indexes=None,
454     ) -> T_DataArray:
455         if variable is None:
456             variable = self.variable
457         if coords is None:
458             coords = self._coords
459         if indexes is None:
460             indexes = self._indexes
461         if name is _default:
462             name = self.name
463         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
464 
465     def _replace_maybe_drop_dims(
466         self: T_DataArray,
467         variable: Variable,
468         name: Hashable | None | Default = _default,
469     ) -> T_DataArray:
470         if variable.dims == self.dims and variable.shape == self.shape:
471             coords = self._coords.copy()
472             indexes = self._indexes
473         elif variable.dims == self.dims:
474             # Shape has changed (e.g. from reduce(..., keepdims=True)
475             new_sizes = dict(zip(self.dims, variable.shape))
476             coords = {
477                 k: v
478                 for k, v in self._coords.items()
479                 if v.shape == tuple(new_sizes[d] for d in v.dims)
480             }
481             indexes = filter_indexes_from_coords(self._indexes, set(coords))
482         else:
483             allowed_dims = set(variable.dims)
484             coords = {
485                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
486             }
487             indexes = filter_indexes_from_coords(self._indexes, set(coords))
488         return self._replace(variable, coords, name, indexes=indexes)
489 
490     def _overwrite_indexes(
491         self: T_DataArray,
492         indexes: Mapping[Any, Index],
493         coords: Mapping[Any, Variable] = None,
494         drop_coords: list[Hashable] = None,
495         rename_dims: Mapping[Any, Any] = None,
496     ) -> T_DataArray:
497         """Maybe replace indexes and their corresponding coordinates."""
498         if not indexes:
499             return self
500 
501         if coords is None:
502             coords = {}
503         if drop_coords is None:
504             drop_coords = []
505 
506         new_variable = self.variable.copy()
507         new_coords = self._coords.copy()
508         new_indexes = dict(self._indexes)
509 
510         for name in indexes:
511             new_coords[name] = coords[name]
512             new_indexes[name] = indexes[name]
513 
514         for name in drop_coords:
515             new_coords.pop(name)
516             new_indexes.pop(name)
517 
518         if rename_dims:
519             new_variable.dims = [rename_dims.get(d, d) for d in new_variable.dims]
520 
521         return self._replace(
522             variable=new_variable, coords=new_coords, indexes=new_indexes
523         )
524 
525     def _to_temp_dataset(self) -> Dataset:
526         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
527 
528     def _from_temp_dataset(
529         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
530     ) -> T_DataArray:
531         variable = dataset._variables.pop(_THIS_ARRAY)
532         coords = dataset._variables
533         indexes = dataset._indexes
534         return self._replace(variable, coords, name, indexes=indexes)
535 
536     def _to_dataset_split(self, dim: Hashable) -> Dataset:
537         """splits dataarray along dimension 'dim'"""
538 
539         def subset(dim, label):
540             array = self.loc[{dim: label}]
541             array.attrs = {}
542             return as_variable(array)
543 
544         variables = {label: subset(dim, label) for label in self.get_index(dim)}
545         variables.update({k: v for k, v in self._coords.items() if k != dim})
546         coord_names = set(self._coords) - {dim}
547         indexes = filter_indexes_from_coords(self._indexes, coord_names)
548         dataset = Dataset._construct_direct(
549             variables, coord_names, indexes=indexes, attrs=self.attrs
550         )
551         return dataset
552 
553     def _to_dataset_whole(
554         self, name: Hashable = None, shallow_copy: bool = True
555     ) -> Dataset:
556         if name is None:
557             name = self.name
558         if name is None:
559             raise ValueError(
560                 "unable to convert unnamed DataArray to a "
561                 "Dataset without providing an explicit name"
562             )
563         if name in self.coords:
564             raise ValueError(
565                 "cannot create a Dataset from a DataArray with "
566                 "the same name as one of its coordinates"
567             )
568         # use private APIs for speed: this is called by _to_temp_dataset(),
569         # which is used in the guts of a lot of operations (e.g., reindex)
570         variables = self._coords.copy()
571         variables[name] = self.variable
572         if shallow_copy:
573             for k in variables:
574                 variables[k] = variables[k].copy(deep=False)
575         indexes = self._indexes
576 
577         coord_names = set(self._coords)
578         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
579 
580     def to_dataset(
581         self,
582         dim: Hashable = None,
583         *,
584         name: Hashable = None,
585         promote_attrs: bool = False,
586     ) -> Dataset:
587         """Convert a DataArray to a Dataset.
588 
589         Parameters
590         ----------
591         dim : Hashable, optional
592             Name of the dimension on this array along which to split this array
593             into separate variables. If not provided, this array is converted
594             into a Dataset of one variable.
595         name : Hashable, optional
596             Name to substitute for this array's name. Only valid if ``dim`` is
597             not provided.
598         promote_attrs : bool, default: False
599             Set to True to shallow copy attrs of DataArray to returned Dataset.
600 
601         Returns
602         -------
603         dataset : Dataset
604         """
605         if dim is not None and dim not in self.dims:
606             raise TypeError(
607                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
608             )
609 
610         if dim is not None:
611             if name is not None:
612                 raise TypeError("cannot supply both dim and name arguments")
613             result = self._to_dataset_split(dim)
614         else:
615             result = self._to_dataset_whole(name)
616 
617         if promote_attrs:
618             result.attrs = dict(self.attrs)
619 
620         return result
621 
622     @property
623     def name(self) -> Hashable | None:
624         """The name of this array."""
625         return self._name
626 
627     @name.setter
628     def name(self, value: Hashable | None) -> None:
629         self._name = value
630 
631     @property
632     def variable(self) -> Variable:
633         """Low level interface to the Variable object for this DataArray."""
634         return self._variable
635 
636     @property
637     def dtype(self) -> np.dtype:
638         return self.variable.dtype
639 
640     @property
641     def shape(self) -> tuple[int, ...]:
642         return self.variable.shape
643 
644     @property
645     def size(self) -> int:
646         return self.variable.size
647 
648     @property
649     def nbytes(self) -> int:
650         """
651         Total bytes consumed by the elements of this DataArray's data.
652 
653         If the backend data array does not include ``nbytes``, estimates
654         the bytes consumed based on the ``size`` and ``dtype``.
655         """
656         return self.variable.nbytes
657 
658     @property
659     def ndim(self) -> int:
660         return self.variable.ndim
661 
662     def __len__(self) -> int:
663         return len(self.variable)
664 
665     @property
666     def data(self) -> Any:
667         """
668         The DataArray's data as an array. The underlying array type
669         (e.g. dask, sparse, pint) is preserved.
670 
671         See Also
672         --------
673         DataArray.to_numpy
674         DataArray.as_numpy
675         DataArray.values
676         """
677         return self.variable.data
678 
679     @data.setter
680     def data(self, value: Any) -> None:
681         self.variable.data = value
682 
683     @property
684     def values(self) -> np.ndarray:
685         """
686         The array's data as a numpy.ndarray.
687 
688         If the array's data is not a numpy.ndarray this will attempt to convert
689         it naively using np.array(), which will raise an error if the array
690         type does not support coercion like this (e.g. cupy).
691         """
692         return self.variable.values
693 
694     @values.setter
695     def values(self, value: Any) -> None:
696         self.variable.values = value
697 
698     def to_numpy(self) -> np.ndarray:
699         """
700         Coerces wrapped data to numpy and returns a numpy.ndarray.
701 
702         See Also
703         --------
704         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
705         Dataset.as_numpy
706         DataArray.values
707         DataArray.data
708         """
709         return self.variable.to_numpy()
710 
711     def as_numpy(self: T_DataArray) -> T_DataArray:
712         """
713         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
714 
715         See Also
716         --------
717         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
718         Dataset.as_numpy : Converts all variables in a Dataset.
719         DataArray.values
720         DataArray.data
721         """
722         coords = {k: v.as_numpy() for k, v in self._coords.items()}
723         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
724 
725     @property
726     def _in_memory(self) -> bool:
727         return self.variable._in_memory
728 
729     def to_index(self) -> pd.Index:
730         """Convert this variable to a pandas.Index. Only possible for 1D
731         arrays.
732         """
733         return self.variable.to_index()
734 
735     @property
736     def dims(self) -> tuple[Hashable, ...]:
737         """Tuple of dimension names associated with this array.
738 
739         Note that the type of this property is inconsistent with
740         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
741         consistently named properties.
742 
743         See Also
744         --------
745         DataArray.sizes
746         Dataset.dims
747         """
748         return self.variable.dims
749 
750     @dims.setter
751     def dims(self, value: Any) -> NoReturn:
752         raise AttributeError(
753             "you cannot assign dims on a DataArray. Use "
754             ".rename() or .swap_dims() instead."
755         )
756 
757     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
758         if utils.is_dict_like(key):
759             return key
760         key = indexing.expanded_indexer(key, self.ndim)
761         return dict(zip(self.dims, key))
762 
763     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
764         from .dataset import _get_virtual_variable
765 
766         try:
767             var = self._coords[key]
768         except KeyError:
769             dim_sizes = dict(zip(self.dims, self.shape))
770             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
771 
772         return self._replace_maybe_drop_dims(var, name=key)
773 
774     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
775         if isinstance(key, str):
776             return self._getitem_coord(key)
777         else:
778             # xarray-style array indexing
779             return self.isel(indexers=self._item_key_to_dict(key))
780 
781     def __setitem__(self, key: Any, value: Any) -> None:
782         if isinstance(key, str):
783             self.coords[key] = value
784         else:
785             # Coordinates in key, value and self[key] should be consistent.
786             # TODO Coordinate consistency in key is checked here, but it
787             # causes unnecessary indexing. It should be optimized.
788             obj = self[key]
789             if isinstance(value, DataArray):
790                 assert_coordinate_consistent(value, obj.coords.variables)
791             # DataArray key -> Variable key
792             key = {
793                 k: v.variable if isinstance(v, DataArray) else v
794                 for k, v in self._item_key_to_dict(key).items()
795             }
796             self.variable[key] = value
797 
798     def __delitem__(self, key: Any) -> None:
799         del self.coords[key]
800 
801     @property
802     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
803         """Places to look-up items for attribute-style access"""
804         yield from self._item_sources
805         yield self.attrs
806 
807     @property
808     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
809         """Places to look-up items for key-completion"""
810         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
811 
812         # virtual coordinates
813         # uses empty dict -- everything here can already be found in self.coords.
814         yield HybridMappingProxy(keys=self.dims, mapping={})
815 
816     def __contains__(self, key: Any) -> bool:
817         return key in self.data
818 
819     @property
820     def loc(self) -> _LocIndexer:
821         """Attribute for location based indexing like pandas."""
822         return _LocIndexer(self)
823 
824     @property
825     # Key type needs to be `Any` because of mypy#4167
826     def attrs(self) -> dict[Any, Any]:
827         """Dictionary storing arbitrary metadata with this array."""
828         return self.variable.attrs
829 
830     @attrs.setter
831     def attrs(self, value: Mapping[Any, Any]) -> None:
832         # Disable type checking to work around mypy bug - see mypy#4167
833         self.variable.attrs = value  # type: ignore[assignment]
834 
835     @property
836     def encoding(self) -> dict[Hashable, Any]:
837         """Dictionary of format-specific settings for how this array should be
838         serialized."""
839         return self.variable.encoding
840 
841     @encoding.setter
842     def encoding(self, value: Mapping[Any, Any]) -> None:
843         self.variable.encoding = value
844 
845     @property
846     def indexes(self) -> Indexes:
847         """Mapping of pandas.Index objects used for label based indexing.
848 
849         Raises an error if this Dataset has indexes that cannot be coerced
850         to pandas.Index objects.
851 
852         See Also
853         --------
854         DataArray.xindexes
855 
856         """
857         return self.xindexes.to_pandas_indexes()
858 
859     @property
860     def xindexes(self) -> Indexes:
861         """Mapping of xarray Index objects used for label based indexing."""
862         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
863 
864     @property
865     def coords(self) -> DataArrayCoordinates:
866         """Dictionary-like container of coordinate arrays."""
867         return DataArrayCoordinates(self)
868 
869     @overload
870     def reset_coords(
871         self: T_DataArray,
872         names: Hashable | Iterable[Hashable] | None = None,
873         drop: Literal[False] = False,
874     ) -> Dataset:
875         ...
876 
877     @overload
878     def reset_coords(
879         self: T_DataArray,
880         names: Hashable | Iterable[Hashable] | None = None,
881         *,
882         drop: Literal[True],
883     ) -> T_DataArray:
884         ...
885 
886     def reset_coords(
887         self: T_DataArray,
888         names: Hashable | Iterable[Hashable] | None = None,
889         drop: bool = False,
890     ) -> T_DataArray | Dataset:
891         """Given names of coordinates, reset them to become variables.
892 
893         Parameters
894         ----------
895         names : Hashable or iterable of Hashable, optional
896             Name(s) of non-index coordinates in this dataset to reset into
897             variables. By default, all non-index coordinates are reset.
898         drop : bool, default: False
899             If True, remove coordinates instead of converting them into
900             variables.
901 
902         Returns
903         -------
904         Dataset, or DataArray if ``drop == True``
905         """
906         if names is None:
907             names = set(self.coords) - set(self._indexes)
908         dataset = self.coords.to_dataset().reset_coords(names, drop)
909         if drop:
910             return self._replace(coords=dataset._variables)
911         if self.name is None:
912             raise ValueError(
913                 "cannot reset_coords with drop=False on an unnamed DataArrray"
914             )
915         dataset[self.name] = self.variable
916         return dataset
917 
918     def __dask_tokenize__(self):
919         from dask.base import normalize_token
920 
921         return normalize_token((type(self), self._variable, self._coords, self._name))
922 
923     def __dask_graph__(self):
924         return self._to_temp_dataset().__dask_graph__()
925 
926     def __dask_keys__(self):
927         return self._to_temp_dataset().__dask_keys__()
928 
929     def __dask_layers__(self):
930         return self._to_temp_dataset().__dask_layers__()
931 
932     @property
933     def __dask_optimize__(self):
934         return self._to_temp_dataset().__dask_optimize__
935 
936     @property
937     def __dask_scheduler__(self):
938         return self._to_temp_dataset().__dask_scheduler__
939 
940     def __dask_postcompute__(self):
941         func, args = self._to_temp_dataset().__dask_postcompute__()
942         return self._dask_finalize, (self.name, func) + args
943 
944     def __dask_postpersist__(self):
945         func, args = self._to_temp_dataset().__dask_postpersist__()
946         return self._dask_finalize, (self.name, func) + args
947 
948     @staticmethod
949     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
950         ds = func(results, *args, **kwargs)
951         variable = ds._variables.pop(_THIS_ARRAY)
952         coords = ds._variables
953         indexes = ds._indexes
954         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
955 
956     def load(self: T_DataArray, **kwargs) -> T_DataArray:
957         """Manually trigger loading of this array's data from disk or a
958         remote source into memory and return this array.
959 
960         Normally, it should not be necessary to call this method in user code,
961         because all xarray functions should either work on deferred data or
962         load data automatically. However, this method can be necessary when
963         working with many file objects on disk.
964 
965         Parameters
966         ----------
967         **kwargs : dict
968             Additional keyword arguments passed on to ``dask.compute``.
969 
970         See Also
971         --------
972         dask.compute
973         """
974         ds = self._to_temp_dataset().load(**kwargs)
975         new = self._from_temp_dataset(ds)
976         self._variable = new._variable
977         self._coords = new._coords
978         return self
979 
980     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
981         """Manually trigger loading of this array's data from disk or a
982         remote source into memory and return a new array. The original is
983         left unaltered.
984 
985         Normally, it should not be necessary to call this method in user code,
986         because all xarray functions should either work on deferred data or
987         load data automatically. However, this method can be necessary when
988         working with many file objects on disk.
989 
990         Parameters
991         ----------
992         **kwargs : dict
993             Additional keyword arguments passed on to ``dask.compute``.
994 
995         See Also
996         --------
997         dask.compute
998         """
999         new = self.copy(deep=False)
1000         return new.load(**kwargs)
1001 
1002     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1003         """Trigger computation in constituent dask arrays
1004 
1005         This keeps them as dask arrays but encourages them to keep data in
1006         memory.  This is particularly useful when on a distributed machine.
1007         When on a single machine consider using ``.compute()`` instead.
1008 
1009         Parameters
1010         ----------
1011         **kwargs : dict
1012             Additional keyword arguments passed on to ``dask.persist``.
1013 
1014         See Also
1015         --------
1016         dask.persist
1017         """
1018         ds = self._to_temp_dataset().persist(**kwargs)
1019         return self._from_temp_dataset(ds)
1020 
1021     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1022         """Returns a copy of this array.
1023 
1024         If `deep=True`, a deep copy is made of the data array.
1025         Otherwise, a shallow copy is made, and the returned data array's
1026         values are a new view of this data array's values.
1027 
1028         Use `data` to create a new object with the same structure as
1029         original but entirely new data.
1030 
1031         Parameters
1032         ----------
1033         deep : bool, optional
1034             Whether the data array and its coordinates are loaded into memory
1035             and copied onto the new object. Default is True.
1036         data : array_like, optional
1037             Data to use in the new object. Must have same shape as original.
1038             When `data` is used, `deep` is ignored for all data variables,
1039             and only used for coords.
1040 
1041         Returns
1042         -------
1043         copy : DataArray
1044             New object with dimensions, attributes, coordinates, name,
1045             encoding, and optionally data copied from original.
1046 
1047         Examples
1048         --------
1049         Shallow versus deep copy
1050 
1051         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1052         >>> array.copy()
1053         <xarray.DataArray (x: 3)>
1054         array([1, 2, 3])
1055         Coordinates:
1056           * x        (x) <U1 'a' 'b' 'c'
1057         >>> array_0 = array.copy(deep=False)
1058         >>> array_0[0] = 7
1059         >>> array_0
1060         <xarray.DataArray (x: 3)>
1061         array([7, 2, 3])
1062         Coordinates:
1063           * x        (x) <U1 'a' 'b' 'c'
1064         >>> array
1065         <xarray.DataArray (x: 3)>
1066         array([7, 2, 3])
1067         Coordinates:
1068           * x        (x) <U1 'a' 'b' 'c'
1069 
1070         Changing the data using the ``data`` argument maintains the
1071         structure of the original object, but with the new data. Original
1072         object is unaffected.
1073 
1074         >>> array.copy(data=[0.1, 0.2, 0.3])
1075         <xarray.DataArray (x: 3)>
1076         array([0.1, 0.2, 0.3])
1077         Coordinates:
1078           * x        (x) <U1 'a' 'b' 'c'
1079         >>> array
1080         <xarray.DataArray (x: 3)>
1081         array([7, 2, 3])
1082         Coordinates:
1083           * x        (x) <U1 'a' 'b' 'c'
1084 
1085         See Also
1086         --------
1087         pandas.DataFrame.copy
1088         """
1089         variable = self.variable.copy(deep=deep, data=data)
1090         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1091 
1092         coords = {}
1093         for k, v in self._coords.items():
1094             if k in index_vars:
1095                 coords[k] = index_vars[k]
1096             else:
1097                 coords[k] = v.copy(deep=deep)
1098 
1099         return self._replace(variable, coords, indexes=indexes)
1100 
1101     def __copy__(self: T_DataArray) -> T_DataArray:
1102         return self.copy(deep=False)
1103 
1104     def __deepcopy__(self: T_DataArray, memo=None) -> T_DataArray:
1105         # memo does nothing but is required for compatibility with
1106         # copy.deepcopy
1107         return self.copy(deep=True)
1108 
1109     # mutable objects should not be Hashable
1110     # https://github.com/python/mypy/issues/4266
1111     __hash__ = None  # type: ignore[assignment]
1112 
1113     @property
1114     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1115         """
1116         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1117         the underlying data is not a dask array.
1118 
1119         See Also
1120         --------
1121         DataArray.chunk
1122         DataArray.chunksizes
1123         xarray.unify_chunks
1124         """
1125         return self.variable.chunks
1126 
1127     @property
1128     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1129         """
1130         Mapping from dimension names to block lengths for this dataarray's data, or None if
1131         the underlying data is not a dask array.
1132         Cannot be modified directly, but can be modified by calling .chunk().
1133 
1134         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1135         instead of a tuple of chunk shapes.
1136 
1137         See Also
1138         --------
1139         DataArray.chunk
1140         DataArray.chunks
1141         xarray.unify_chunks
1142         """
1143         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1144         return get_chunksizes(all_variables)
1145 
1146     def chunk(
1147         self: T_DataArray,
1148         chunks: (
1149             int
1150             | Literal["auto"]
1151             | tuple[int, ...]
1152             | tuple[tuple[int, ...], ...]
1153             | Mapping[Any, None | int | tuple[int, ...]]
1154         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1155         name_prefix: str = "xarray-",
1156         token: str | None = None,
1157         lock: bool = False,
1158         inline_array: bool = False,
1159         **chunks_kwargs: Any,
1160     ) -> T_DataArray:
1161         """Coerce this array's data into a dask arrays with the given chunks.
1162 
1163         If this variable is a non-dask array, it will be converted to dask
1164         array. If it's a dask array, it will be rechunked to the given chunk
1165         sizes.
1166 
1167         If neither chunks is not provided for one or more dimensions, chunk
1168         sizes along that dimension will not be updated; non-dask arrays will be
1169         converted into dask arrays with a single block.
1170 
1171         Parameters
1172         ----------
1173         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1174             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1175             ``{"x": 5, "y": 5}``.
1176         name_prefix : str, optional
1177             Prefix for the name of the new dask array.
1178         token : str, optional
1179             Token uniquely identifying this array.
1180         lock : optional
1181             Passed on to :py:func:`dask.array.from_array`, if the array is not
1182             already as dask array.
1183         inline_array: optional
1184             Passed on to :py:func:`dask.array.from_array`, if the array is not
1185             already as dask array.
1186         **chunks_kwargs : {dim: chunks, ...}, optional
1187             The keyword arguments form of ``chunks``.
1188             One of chunks or chunks_kwargs must be provided.
1189 
1190         Returns
1191         -------
1192         chunked : xarray.DataArray
1193 
1194         See Also
1195         --------
1196         DataArray.chunks
1197         DataArray.chunksizes
1198         xarray.unify_chunks
1199         dask.array.from_array
1200         """
1201         if chunks is None:
1202             warnings.warn(
1203                 "None value for 'chunks' is deprecated. "
1204                 "It will raise an error in the future. Use instead '{}'",
1205                 category=FutureWarning,
1206             )
1207             chunks = {}
1208 
1209         if isinstance(chunks, (float, str, int)):
1210             # ignoring type; unclear why it won't accept a Literal into the value.
1211             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1212         elif isinstance(chunks, (tuple, list)):
1213             chunks = dict(zip(self.dims, chunks))
1214         else:
1215             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1216 
1217         ds = self._to_temp_dataset().chunk(
1218             chunks,
1219             name_prefix=name_prefix,
1220             token=token,
1221             lock=lock,
1222             inline_array=inline_array,
1223         )
1224         return self._from_temp_dataset(ds)
1225 
1226     def isel(
1227         self: T_DataArray,
1228         indexers: Mapping[Any, Any] | None = None,
1229         drop: bool = False,
1230         missing_dims: ErrorOptionsWithWarn = "raise",
1231         **indexers_kwargs: Any,
1232     ) -> T_DataArray:
1233         """Return a new DataArray whose data is given by selecting indexes
1234         along the specified dimension(s).
1235 
1236         Parameters
1237         ----------
1238         indexers : dict, optional
1239             A dict with keys matching dimensions and values given
1240             by integers, slice objects or arrays.
1241             indexer can be a integer, slice, array-like or DataArray.
1242             If DataArrays are passed as indexers, xarray-style indexing will be
1243             carried out. See :ref:`indexing` for the details.
1244             One of indexers or indexers_kwargs must be provided.
1245         drop : bool, default: False
1246             If ``drop=True``, drop coordinates variables indexed by integers
1247             instead of making them scalar.
1248         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1249             What to do if dimensions that should be selected from are not present in the
1250             DataArray:
1251             - "raise": raise an exception
1252             - "warn": raise a warning, and ignore the missing dimensions
1253             - "ignore": ignore the missing dimensions
1254         **indexers_kwargs : {dim: indexer, ...}, optional
1255             The keyword arguments form of ``indexers``.
1256 
1257         Returns
1258         -------
1259         indexed : xarray.DataArray
1260 
1261         See Also
1262         --------
1263         Dataset.isel
1264         DataArray.sel
1265 
1266         Examples
1267         --------
1268         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1269         >>> da
1270         <xarray.DataArray (x: 5, y: 5)>
1271         array([[ 0,  1,  2,  3,  4],
1272                [ 5,  6,  7,  8,  9],
1273                [10, 11, 12, 13, 14],
1274                [15, 16, 17, 18, 19],
1275                [20, 21, 22, 23, 24]])
1276         Dimensions without coordinates: x, y
1277 
1278         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1279         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1280         >>> da = da.isel(x=tgt_x, y=tgt_y)
1281         >>> da
1282         <xarray.DataArray (points: 5)>
1283         array([ 0,  6, 12, 18, 24])
1284         Dimensions without coordinates: points
1285         """
1286 
1287         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1288 
1289         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1290             ds = self._to_temp_dataset()._isel_fancy(
1291                 indexers, drop=drop, missing_dims=missing_dims
1292             )
1293             return self._from_temp_dataset(ds)
1294 
1295         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1296         # lists, or zero or one-dimensional np.ndarray's
1297 
1298         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1299         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1300 
1301         coords = {}
1302         for coord_name, coord_value in self._coords.items():
1303             if coord_name in index_variables:
1304                 coord_value = index_variables[coord_name]
1305             else:
1306                 coord_indexers = {
1307                     k: v for k, v in indexers.items() if k in coord_value.dims
1308                 }
1309                 if coord_indexers:
1310                     coord_value = coord_value.isel(coord_indexers)
1311                     if drop and coord_value.ndim == 0:
1312                         continue
1313             coords[coord_name] = coord_value
1314 
1315         return self._replace(variable=variable, coords=coords, indexes=indexes)
1316 
1317     def sel(
1318         self: T_DataArray,
1319         indexers: Mapping[Any, Any] = None,
1320         method: str = None,
1321         tolerance=None,
1322         drop: bool = False,
1323         **indexers_kwargs: Any,
1324     ) -> T_DataArray:
1325         """Return a new DataArray whose data is given by selecting index
1326         labels along the specified dimension(s).
1327 
1328         In contrast to `DataArray.isel`, indexers for this method should use
1329         labels instead of integers.
1330 
1331         Under the hood, this method is powered by using pandas's powerful Index
1332         objects. This makes label based indexing essentially just as fast as
1333         using integer indexing.
1334 
1335         It also means this method uses pandas's (well documented) logic for
1336         indexing. This means you can use string shortcuts for datetime indexes
1337         (e.g., '2000-01' to select all values in January 2000). It also means
1338         that slices are treated as inclusive of both the start and stop values,
1339         unlike normal Python indexing.
1340 
1341         .. warning::
1342 
1343           Do not try to assign values when using any of the indexing methods
1344           ``isel`` or ``sel``::
1345 
1346             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1347             # DO NOT do this
1348             da.isel(x=[0, 1, 2])[1] = -1
1349 
1350           Assigning values with the chained indexing using ``.sel`` or
1351           ``.isel`` fails silently.
1352 
1353         Parameters
1354         ----------
1355         indexers : dict, optional
1356             A dict with keys matching dimensions and values given
1357             by scalars, slices or arrays of tick labels. For dimensions with
1358             multi-index, the indexer may also be a dict-like object with keys
1359             matching index level names.
1360             If DataArrays are passed as indexers, xarray-style indexing will be
1361             carried out. See :ref:`indexing` for the details.
1362             One of indexers or indexers_kwargs must be provided.
1363         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1364             Method to use for inexact matches:
1365 
1366             - None (default): only exact matches
1367             - pad / ffill: propagate last valid index value forward
1368             - backfill / bfill: propagate next valid index value backward
1369             - nearest: use nearest valid index value
1370 
1371         tolerance : optional
1372             Maximum distance between original and new labels for inexact
1373             matches. The values of the index at the matching locations must
1374             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1375         drop : bool, optional
1376             If ``drop=True``, drop coordinates variables in `indexers` instead
1377             of making them scalar.
1378         **indexers_kwargs : {dim: indexer, ...}, optional
1379             The keyword arguments form of ``indexers``.
1380             One of indexers or indexers_kwargs must be provided.
1381 
1382         Returns
1383         -------
1384         obj : DataArray
1385             A new DataArray with the same contents as this DataArray, except the
1386             data and each dimension is indexed by the appropriate indexers.
1387             If indexer DataArrays have coordinates that do not conflict with
1388             this object, then these coordinates will be attached.
1389             In general, each array's data will be a view of the array's data
1390             in this DataArray, unless vectorized indexing was triggered by using
1391             an array indexer, in which case the data will be a copy.
1392 
1393         See Also
1394         --------
1395         Dataset.sel
1396         DataArray.isel
1397 
1398         Examples
1399         --------
1400         >>> da = xr.DataArray(
1401         ...     np.arange(25).reshape(5, 5),
1402         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1403         ...     dims=("x", "y"),
1404         ... )
1405         >>> da
1406         <xarray.DataArray (x: 5, y: 5)>
1407         array([[ 0,  1,  2,  3,  4],
1408                [ 5,  6,  7,  8,  9],
1409                [10, 11, 12, 13, 14],
1410                [15, 16, 17, 18, 19],
1411                [20, 21, 22, 23, 24]])
1412         Coordinates:
1413           * x        (x) int64 0 1 2 3 4
1414           * y        (y) int64 0 1 2 3 4
1415 
1416         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1417         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1418         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1419         >>> da
1420         <xarray.DataArray (points: 5)>
1421         array([ 0,  6, 12, 18, 24])
1422         Coordinates:
1423             x        (points) int64 0 1 2 3 4
1424             y        (points) int64 0 1 2 3 4
1425         Dimensions without coordinates: points
1426         """
1427         ds = self._to_temp_dataset().sel(
1428             indexers=indexers,
1429             drop=drop,
1430             method=method,
1431             tolerance=tolerance,
1432             **indexers_kwargs,
1433         )
1434         return self._from_temp_dataset(ds)
1435 
1436     def head(
1437         self: T_DataArray,
1438         indexers: Mapping[Any, int] | int | None = None,
1439         **indexers_kwargs: Any,
1440     ) -> T_DataArray:
1441         """Return a new DataArray whose data is given by the the first `n`
1442         values along the specified dimension(s). Default `n` = 5
1443 
1444         See Also
1445         --------
1446         Dataset.head
1447         DataArray.tail
1448         DataArray.thin
1449         """
1450         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1451         return self._from_temp_dataset(ds)
1452 
1453     def tail(
1454         self: T_DataArray,
1455         indexers: Mapping[Any, int] | int | None = None,
1456         **indexers_kwargs: Any,
1457     ) -> T_DataArray:
1458         """Return a new DataArray whose data is given by the the last `n`
1459         values along the specified dimension(s). Default `n` = 5
1460 
1461         See Also
1462         --------
1463         Dataset.tail
1464         DataArray.head
1465         DataArray.thin
1466         """
1467         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1468         return self._from_temp_dataset(ds)
1469 
1470     def thin(
1471         self: T_DataArray,
1472         indexers: Mapping[Any, int] | int | None = None,
1473         **indexers_kwargs: Any,
1474     ) -> T_DataArray:
1475         """Return a new DataArray whose data is given by each `n` value
1476         along the specified dimension(s).
1477 
1478         Examples
1479         --------
1480         >>> x_arr = np.arange(0, 26)
1481         >>> x_arr
1482         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1483                17, 18, 19, 20, 21, 22, 23, 24, 25])
1484         >>> x = xr.DataArray(
1485         ...     np.reshape(x_arr, (2, 13)),
1486         ...     dims=("x", "y"),
1487         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1488         ... )
1489         >>> x
1490         <xarray.DataArray (x: 2, y: 13)>
1491         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1492                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1493         Coordinates:
1494           * x        (x) int64 0 1
1495           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1496 
1497         >>>
1498         >>> x.thin(3)
1499         <xarray.DataArray (x: 1, y: 5)>
1500         array([[ 0,  3,  6,  9, 12]])
1501         Coordinates:
1502           * x        (x) int64 0
1503           * y        (y) int64 0 3 6 9 12
1504         >>> x.thin({"x": 2, "y": 5})
1505         <xarray.DataArray (x: 1, y: 3)>
1506         array([[ 0,  5, 10]])
1507         Coordinates:
1508           * x        (x) int64 0
1509           * y        (y) int64 0 5 10
1510 
1511         See Also
1512         --------
1513         Dataset.thin
1514         DataArray.head
1515         DataArray.tail
1516         """
1517         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1518         return self._from_temp_dataset(ds)
1519 
1520     def broadcast_like(
1521         self: T_DataArray,
1522         other: DataArray | Dataset,
1523         exclude: Iterable[Hashable] | None = None,
1524     ) -> T_DataArray:
1525         """Broadcast this DataArray against another Dataset or DataArray.
1526 
1527         This is equivalent to xr.broadcast(other, self)[1]
1528 
1529         xarray objects are broadcast against each other in arithmetic
1530         operations, so this method is not be necessary for most uses.
1531 
1532         If no change is needed, the input data is returned to the output
1533         without being copied.
1534 
1535         If new coords are added by the broadcast, their values are
1536         NaN filled.
1537 
1538         Parameters
1539         ----------
1540         other : Dataset or DataArray
1541             Object against which to broadcast this array.
1542         exclude : iterable of Hashable, optional
1543             Dimensions that must not be broadcasted
1544 
1545         Returns
1546         -------
1547         new_da : DataArray
1548             The caller broadcasted against ``other``.
1549 
1550         Examples
1551         --------
1552         >>> arr1 = xr.DataArray(
1553         ...     np.random.randn(2, 3),
1554         ...     dims=("x", "y"),
1555         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1556         ... )
1557         >>> arr2 = xr.DataArray(
1558         ...     np.random.randn(3, 2),
1559         ...     dims=("x", "y"),
1560         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1561         ... )
1562         >>> arr1
1563         <xarray.DataArray (x: 2, y: 3)>
1564         array([[ 1.76405235,  0.40015721,  0.97873798],
1565                [ 2.2408932 ,  1.86755799, -0.97727788]])
1566         Coordinates:
1567           * x        (x) <U1 'a' 'b'
1568           * y        (y) <U1 'a' 'b' 'c'
1569         >>> arr2
1570         <xarray.DataArray (x: 3, y: 2)>
1571         array([[ 0.95008842, -0.15135721],
1572                [-0.10321885,  0.4105985 ],
1573                [ 0.14404357,  1.45427351]])
1574         Coordinates:
1575           * x        (x) <U1 'a' 'b' 'c'
1576           * y        (y) <U1 'a' 'b'
1577         >>> arr1.broadcast_like(arr2)
1578         <xarray.DataArray (x: 3, y: 3)>
1579         array([[ 1.76405235,  0.40015721,  0.97873798],
1580                [ 2.2408932 ,  1.86755799, -0.97727788],
1581                [        nan,         nan,         nan]])
1582         Coordinates:
1583           * x        (x) <U1 'a' 'b' 'c'
1584           * y        (y) <U1 'a' 'b' 'c'
1585         """
1586         if exclude is None:
1587             exclude = set()
1588         else:
1589             exclude = set(exclude)
1590         args = align(other, self, join="outer", copy=False, exclude=exclude)
1591 
1592         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1593 
1594         return _broadcast_helper(
1595             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1596         )
1597 
1598     def _reindex_callback(
1599         self: T_DataArray,
1600         aligner: alignment.Aligner,
1601         dim_pos_indexers: dict[Hashable, Any],
1602         variables: dict[Hashable, Variable],
1603         indexes: dict[Hashable, Index],
1604         fill_value: Any,
1605         exclude_dims: frozenset[Hashable],
1606         exclude_vars: frozenset[Hashable],
1607     ) -> T_DataArray:
1608         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1609 
1610         if isinstance(fill_value, dict):
1611             fill_value = fill_value.copy()
1612             sentinel = object()
1613             value = fill_value.pop(self.name, sentinel)
1614             if value is not sentinel:
1615                 fill_value[_THIS_ARRAY] = value
1616 
1617         ds = self._to_temp_dataset()
1618         reindexed = ds._reindex_callback(
1619             aligner,
1620             dim_pos_indexers,
1621             variables,
1622             indexes,
1623             fill_value,
1624             exclude_dims,
1625             exclude_vars,
1626         )
1627         return self._from_temp_dataset(reindexed)
1628 
1629     def reindex_like(
1630         self: T_DataArray,
1631         other: DataArray | Dataset,
1632         method: ReindexMethodOptions = None,
1633         tolerance: int | float | Iterable[int | float] | None = None,
1634         copy: bool = True,
1635         fill_value=dtypes.NA,
1636     ) -> T_DataArray:
1637         """Conform this object onto the indexes of another object, filling in
1638         missing values with ``fill_value``. The default fill value is NaN.
1639 
1640         Parameters
1641         ----------
1642         other : Dataset or DataArray
1643             Object with an 'indexes' attribute giving a mapping from dimension
1644             names to pandas.Index objects, which provides coordinates upon
1645             which to index the variables in this dataset. The indexes on this
1646             other object need not be the same as the indexes on this
1647             dataset. Any mis-matched index values will be filled in with
1648             NaN, and any mis-matched dimension names will simply be ignored.
1649         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1650             Method to use for filling index values from other not found on this
1651             data array:
1652 
1653             - None (default): don't fill gaps
1654             - pad / ffill: propagate last valid index value forward
1655             - backfill / bfill: propagate next valid index value backward
1656             - nearest: use nearest valid index value
1657 
1658         tolerance : optional
1659             Maximum distance between original and new labels for inexact
1660             matches. The values of the index at the matching locations must
1661             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1662             Tolerance may be a scalar value, which applies the same tolerance
1663             to all values, or list-like, which applies variable tolerance per
1664             element. List-like must be the same size as the index and its dtype
1665             must exactly match the index’s type.
1666         copy : bool, default: True
1667             If ``copy=True``, data in the return value is always copied. If
1668             ``copy=False`` and reindexing is unnecessary, or can be performed
1669             with only slice operations, then the output may share memory with
1670             the input. In either case, a new xarray object is always returned.
1671         fill_value : scalar or dict-like, optional
1672             Value to use for newly missing values. If a dict-like, maps
1673             variable names (including coordinates) to fill values. Use this
1674             data array's name to refer to the data array's values.
1675 
1676         Returns
1677         -------
1678         reindexed : DataArray
1679             Another dataset array, with this array's data but coordinates from
1680             the other object.
1681 
1682         See Also
1683         --------
1684         DataArray.reindex
1685         align
1686         """
1687         return alignment.reindex_like(
1688             self,
1689             other=other,
1690             method=method,
1691             tolerance=tolerance,
1692             copy=copy,
1693             fill_value=fill_value,
1694         )
1695 
1696     def reindex(
1697         self: T_DataArray,
1698         indexers: Mapping[Any, Any] = None,
1699         method: ReindexMethodOptions = None,
1700         tolerance: float | Iterable[float] | None = None,
1701         copy: bool = True,
1702         fill_value=dtypes.NA,
1703         **indexers_kwargs: Any,
1704     ) -> T_DataArray:
1705         """Conform this object onto the indexes of another object, filling in
1706         missing values with ``fill_value``. The default fill value is NaN.
1707 
1708         Parameters
1709         ----------
1710         indexers : dict, optional
1711             Dictionary with keys given by dimension names and values given by
1712             arrays of coordinates tick labels. Any mis-matched coordinate
1713             values will be filled in with NaN, and any mis-matched dimension
1714             names will simply be ignored.
1715             One of indexers or indexers_kwargs must be provided.
1716         copy : bool, optional
1717             If ``copy=True``, data in the return value is always copied. If
1718             ``copy=False`` and reindexing is unnecessary, or can be performed
1719             with only slice operations, then the output may share memory with
1720             the input. In either case, a new xarray object is always returned.
1721         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1722             Method to use for filling index values in ``indexers`` not found on
1723             this data array:
1724 
1725             - None (default): don't fill gaps
1726             - pad / ffill: propagate last valid index value forward
1727             - backfill / bfill: propagate next valid index value backward
1728             - nearest: use nearest valid index value
1729 
1730         tolerance : float | Iterable[float] | None, default: None
1731             Maximum distance between original and new labels for inexact
1732             matches. The values of the index at the matching locations must
1733             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1734             Tolerance may be a scalar value, which applies the same tolerance
1735             to all values, or list-like, which applies variable tolerance per
1736             element. List-like must be the same size as the index and its dtype
1737             must exactly match the index’s type.
1738         fill_value : scalar or dict-like, optional
1739             Value to use for newly missing values. If a dict-like, maps
1740             variable names (including coordinates) to fill values. Use this
1741             data array's name to refer to the data array's values.
1742         **indexers_kwargs : {dim: indexer, ...}, optional
1743             The keyword arguments form of ``indexers``.
1744             One of indexers or indexers_kwargs must be provided.
1745 
1746         Returns
1747         -------
1748         reindexed : DataArray
1749             Another dataset array, with this array's data but replaced
1750             coordinates.
1751 
1752         Examples
1753         --------
1754         Reverse latitude:
1755 
1756         >>> da = xr.DataArray(
1757         ...     np.arange(4),
1758         ...     coords=[np.array([90, 89, 88, 87])],
1759         ...     dims="lat",
1760         ... )
1761         >>> da
1762         <xarray.DataArray (lat: 4)>
1763         array([0, 1, 2, 3])
1764         Coordinates:
1765           * lat      (lat) int64 90 89 88 87
1766         >>> da.reindex(lat=da.lat[::-1])
1767         <xarray.DataArray (lat: 4)>
1768         array([3, 2, 1, 0])
1769         Coordinates:
1770           * lat      (lat) int64 87 88 89 90
1771 
1772         See Also
1773         --------
1774         DataArray.reindex_like
1775         align
1776         """
1777         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1778         return alignment.reindex(
1779             self,
1780             indexers=indexers,
1781             method=method,
1782             tolerance=tolerance,
1783             copy=copy,
1784             fill_value=fill_value,
1785         )
1786 
1787     def interp(
1788         self: T_DataArray,
1789         coords: Mapping[Any, Any] | None = None,
1790         method: InterpOptions = "linear",
1791         assume_sorted: bool = False,
1792         kwargs: Mapping[str, Any] | None = None,
1793         **coords_kwargs: Any,
1794     ) -> T_DataArray:
1795         """Interpolate a DataArray onto new coordinates
1796 
1797         Performs univariate or multivariate interpolation of a DataArray onto
1798         new coordinates using scipy's interpolation routines. If interpolating
1799         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
1800         called. When interpolating along multiple existing dimensions, an
1801         attempt is made to decompose the interpolation into multiple
1802         1-dimensional interpolations. If this is possible,
1803         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1804         :py:func:`scipy.interpolate.interpn` is called.
1805 
1806         Parameters
1807         ----------
1808         coords : dict, optional
1809             Mapping from dimension names to the new coordinates.
1810             New coordinate can be a scalar, array-like or DataArray.
1811             If DataArrays are passed as new coordinates, their dimensions are
1812             used for the broadcasting. Missing values are skipped.
1813         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1814             The method used to interpolate. The method should be supported by
1815             the scipy interpolator:
1816 
1817             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
1818               "quadratic", "cubic", "polynomial"}
1819             - ``interpn``: {"linear", "nearest"}
1820 
1821             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1822             also be provided.
1823         assume_sorted : bool, default: False
1824             If False, values of x can be in any order and they are sorted
1825             first. If True, x has to be an array of monotonically increasing
1826             values.
1827         kwargs : dict-like or None, default: None
1828             Additional keyword arguments passed to scipy's interpolator. Valid
1829             options and their behavior depend whether ``interp1d`` or
1830             ``interpn`` is used.
1831         **coords_kwargs : {dim: coordinate, ...}, optional
1832             The keyword arguments form of ``coords``.
1833             One of coords or coords_kwargs must be provided.
1834 
1835         Returns
1836         -------
1837         interpolated : DataArray
1838             New dataarray on the new coordinates.
1839 
1840         Notes
1841         -----
1842         scipy is required.
1843 
1844         See Also
1845         --------
1846         scipy.interpolate.interp1d
1847         scipy.interpolate.interpn
1848 
1849         Examples
1850         --------
1851         >>> da = xr.DataArray(
1852         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1853         ...     dims=("x", "y"),
1854         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1855         ... )
1856         >>> da
1857         <xarray.DataArray (x: 3, y: 4)>
1858         array([[ 1.,  4.,  2.,  9.],
1859                [ 2.,  7.,  6., nan],
1860                [ 6., nan,  5.,  8.]])
1861         Coordinates:
1862           * x        (x) int64 0 1 2
1863           * y        (y) int64 10 12 14 16
1864 
1865         1D linear interpolation (the default):
1866 
1867         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1868         <xarray.DataArray (x: 4, y: 4)>
1869         array([[1.  , 4.  , 2.  ,  nan],
1870                [1.75, 6.25, 5.  ,  nan],
1871                [3.  ,  nan, 5.75,  nan],
1872                [5.  ,  nan, 5.25,  nan]])
1873         Coordinates:
1874           * y        (y) int64 10 12 14 16
1875           * x        (x) float64 0.0 0.75 1.25 1.75
1876 
1877         1D nearest interpolation:
1878 
1879         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1880         <xarray.DataArray (x: 4, y: 4)>
1881         array([[ 1.,  4.,  2.,  9.],
1882                [ 2.,  7.,  6., nan],
1883                [ 2.,  7.,  6., nan],
1884                [ 6., nan,  5.,  8.]])
1885         Coordinates:
1886           * y        (y) int64 10 12 14 16
1887           * x        (x) float64 0.0 0.75 1.25 1.75
1888 
1889         1D linear extrapolation:
1890 
1891         >>> da.interp(
1892         ...     x=[1, 1.5, 2.5, 3.5],
1893         ...     method="linear",
1894         ...     kwargs={"fill_value": "extrapolate"},
1895         ... )
1896         <xarray.DataArray (x: 4, y: 4)>
1897         array([[ 2. ,  7. ,  6. ,  nan],
1898                [ 4. ,  nan,  5.5,  nan],
1899                [ 8. ,  nan,  4.5,  nan],
1900                [12. ,  nan,  3.5,  nan]])
1901         Coordinates:
1902           * y        (y) int64 10 12 14 16
1903           * x        (x) float64 1.0 1.5 2.5 3.5
1904 
1905         2D linear interpolation:
1906 
1907         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1908         <xarray.DataArray (x: 4, y: 3)>
1909         array([[2.5  , 3.   ,   nan],
1910                [4.   , 5.625,   nan],
1911                [  nan,   nan,   nan],
1912                [  nan,   nan,   nan]])
1913         Coordinates:
1914           * x        (x) float64 0.0 0.75 1.25 1.75
1915           * y        (y) int64 11 13 15
1916         """
1917         if self.dtype.kind not in "uifc":
1918             raise TypeError(
1919                 "interp only works for a numeric type array. "
1920                 "Given {}.".format(self.dtype)
1921             )
1922         ds = self._to_temp_dataset().interp(
1923             coords,
1924             method=method,
1925             kwargs=kwargs,
1926             assume_sorted=assume_sorted,
1927             **coords_kwargs,
1928         )
1929         return self._from_temp_dataset(ds)
1930 
1931     def interp_like(
1932         self: T_DataArray,
1933         other: DataArray | Dataset,
1934         method: InterpOptions = "linear",
1935         assume_sorted: bool = False,
1936         kwargs: Mapping[str, Any] | None = None,
1937     ) -> T_DataArray:
1938         """Interpolate this object onto the coordinates of another object,
1939         filling out of range values with NaN.
1940 
1941         If interpolating along a single existing dimension,
1942         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
1943         along multiple existing dimensions, an attempt is made to decompose the
1944         interpolation into multiple 1-dimensional interpolations. If this is
1945         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1946         :py:func:`scipy.interpolate.interpn` is called.
1947 
1948         Parameters
1949         ----------
1950         other : Dataset or DataArray
1951             Object with an 'indexes' attribute giving a mapping from dimension
1952             names to an 1d array-like, which provides coordinates upon
1953             which to index the variables in this dataset. Missing values are skipped.
1954         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1955             The method used to interpolate. The method should be supported by
1956             the scipy interpolator:
1957 
1958             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
1959               "polynomial"} when ``interp1d`` is called.
1960             - {"linear", "nearest"} when ``interpn`` is called.
1961 
1962             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1963             also be provided.
1964         assume_sorted : bool, default: False
1965             If False, values of coordinates that are interpolated over can be
1966             in any order and they are sorted first. If True, interpolated
1967             coordinates are assumed to be an array of monotonically increasing
1968             values.
1969         kwargs : dict, optional
1970             Additional keyword passed to scipy's interpolator.
1971 
1972         Returns
1973         -------
1974         interpolated : DataArray
1975             Another dataarray by interpolating this dataarray's data along the
1976             coordinates of the other object.
1977 
1978         Notes
1979         -----
1980         scipy is required.
1981         If the dataarray has object-type coordinates, reindex is used for these
1982         coordinates instead of the interpolation.
1983 
1984         See Also
1985         --------
1986         DataArray.interp
1987         DataArray.reindex_like
1988         """
1989         if self.dtype.kind not in "uifc":
1990             raise TypeError(
1991                 "interp only works for a numeric type array. "
1992                 "Given {}.".format(self.dtype)
1993             )
1994         ds = self._to_temp_dataset().interp_like(
1995             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1996         )
1997         return self._from_temp_dataset(ds)
1998 
1999     # change type of self and return to T_DataArray once
2000     # https://github.com/python/mypy/issues/12846 is resolved
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)
2044 
2045     def swap_dims(
2046         self: T_DataArray,
2047         dims_dict: Mapping[Any, Hashable] | None = None,
2048         **dims_kwargs,
2049     ) -> T_DataArray:
2050         """Returns a new DataArray with swapped dimensions.
2051 
2052         Parameters
2053         ----------
2054         dims_dict : dict-like
2055             Dictionary whose keys are current dimension names and whose values
2056             are new names.
2057         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2058             The keyword arguments form of ``dims_dict``.
2059             One of dims_dict or dims_kwargs must be provided.
2060 
2061         Returns
2062         -------
2063         swapped : DataArray
2064             DataArray with swapped dimensions.
2065 
2066         Examples
2067         --------
2068         >>> arr = xr.DataArray(
2069         ...     data=[0, 1],
2070         ...     dims="x",
2071         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2072         ... )
2073         >>> arr
2074         <xarray.DataArray (x: 2)>
2075         array([0, 1])
2076         Coordinates:
2077           * x        (x) <U1 'a' 'b'
2078             y        (x) int64 0 1
2079 
2080         >>> arr.swap_dims({"x": "y"})
2081         <xarray.DataArray (y: 2)>
2082         array([0, 1])
2083         Coordinates:
2084             x        (y) <U1 'a' 'b'
2085           * y        (y) int64 0 1
2086 
2087         >>> arr.swap_dims({"x": "z"})
2088         <xarray.DataArray (z: 2)>
2089         array([0, 1])
2090         Coordinates:
2091             x        (z) <U1 'a' 'b'
2092             y        (z) int64 0 1
2093         Dimensions without coordinates: z
2094 
2095         See Also
2096         --------
2097         DataArray.rename
2098         Dataset.swap_dims
2099         """
2100         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2101         ds = self._to_temp_dataset().swap_dims(dims_dict)
2102         return self._from_temp_dataset(ds)
2103 
2104     # change type of self and return to T_DataArray once
2105     # https://github.com/python/mypy/issues/12846 is resolved
2106     def expand_dims(
2107         self,
2108         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2109         axis: None | int | Sequence[int] = None,
2110         **dim_kwargs: Any,
2111     ) -> DataArray:
2112         """Return a new object with an additional axis (or axes) inserted at
2113         the corresponding position in the array shape. The new object is a
2114         view into the underlying array, not a copy.
2115 
2116         If dim is already a scalar coordinate, it will be promoted to a 1D
2117         coordinate consisting of a single value.
2118 
2119         Parameters
2120         ----------
2121         dim : Hashable, sequence of Hashable, dict, or None, optional
2122             Dimensions to include on the new variable.
2123             If provided as str or sequence of str, then dimensions are inserted
2124             with length 1. If provided as a dict, then the keys are the new
2125             dimensions and the values are either integers (giving the length of
2126             the new dimensions) or sequence/ndarray (giving the coordinates of
2127             the new dimensions).
2128         axis : int, sequence of int, or None, default: None
2129             Axis position(s) where new axis is to be inserted (position(s) on
2130             the result array). If a sequence of integers is passed,
2131             multiple axes are inserted. In this case, dim arguments should be
2132             same length list. If axis=None is passed, all the axes will be
2133             inserted to the start of the result array.
2134         **dim_kwargs : int or sequence or ndarray
2135             The keywords are arbitrary dimensions being inserted and the values
2136             are either the lengths of the new dims (if int is given), or their
2137             coordinates. Note, this is an alternative to passing a dict to the
2138             dim kwarg and will only be used if dim is None.
2139 
2140         Returns
2141         -------
2142         expanded : DataArray
2143             This object, but with additional dimension(s).
2144 
2145         See Also
2146         --------
2147         Dataset.expand_dims
2148         """
2149         if isinstance(dim, int):
2150             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2151         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2152             if len(dim) != len(set(dim)):
2153                 raise ValueError("dims should not contain duplicate values.")
2154             dim = dict.fromkeys(dim, 1)
2155         elif dim is not None and not isinstance(dim, Mapping):
2156             dim = {cast(Hashable, dim): 1}
2157 
2158         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2159         ds = self._to_temp_dataset().expand_dims(dim, axis)
2160         return self._from_temp_dataset(ds)
2161 
2162     # change type of self and return to T_DataArray once
2163     # https://github.com/python/mypy/issues/12846 is resolved
2164     def set_index(
2165         self,
2166         indexes: Mapping[Any, Hashable | Sequence[Hashable]] = None,
2167         append: bool = False,
2168         **indexes_kwargs: Hashable | Sequence[Hashable],
2169     ) -> DataArray:
2170         """Set DataArray (multi-)indexes using one or more existing
2171         coordinates.
2172 
2173         Parameters
2174         ----------
2175         indexes : {dim: index, ...}
2176             Mapping from names matching dimensions and values given
2177             by (lists of) the names of existing coordinates or variables to set
2178             as new (multi-)index.
2179         append : bool, default: False
2180             If True, append the supplied index(es) to the existing index(es).
2181             Otherwise replace the existing index(es).
2182         **indexes_kwargs : optional
2183             The keyword arguments form of ``indexes``.
2184             One of indexes or indexes_kwargs must be provided.
2185 
2186         Returns
2187         -------
2188         obj : DataArray
2189             Another DataArray, with this data but replaced coordinates.
2190 
2191         Examples
2192         --------
2193         >>> arr = xr.DataArray(
2194         ...     data=np.ones((2, 3)),
2195         ...     dims=["x", "y"],
2196         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2197         ... )
2198         >>> arr
2199         <xarray.DataArray (x: 2, y: 3)>
2200         array([[1., 1., 1.],
2201                [1., 1., 1.]])
2202         Coordinates:
2203           * x        (x) int64 0 1
2204           * y        (y) int64 0 1 2
2205             a        (x) int64 3 4
2206         >>> arr.set_index(x="a")
2207         <xarray.DataArray (x: 2, y: 3)>
2208         array([[1., 1., 1.],
2209                [1., 1., 1.]])
2210         Coordinates:
2211           * x        (x) int64 3 4
2212           * y        (y) int64 0 1 2
2213 
2214         See Also
2215         --------
2216         DataArray.reset_index
2217         """
2218         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2219         return self._from_temp_dataset(ds)
2220 
2221     # change type of self and return to T_DataArray once
2222     # https://github.com/python/mypy/issues/12846 is resolved
2223     def reset_index(
2224         self,
2225         dims_or_levels: Hashable | Sequence[Hashable],
2226         drop: bool = False,
2227     ) -> DataArray:
2228         """Reset the specified index(es) or multi-index level(s).
2229 
2230         Parameters
2231         ----------
2232         dims_or_levels : Hashable or sequence of Hashable
2233             Name(s) of the dimension(s) and/or multi-index level(s) that will
2234             be reset.
2235         drop : bool, default: False
2236             If True, remove the specified indexes and/or multi-index levels
2237             instead of extracting them as new coordinates (default: False).
2238 
2239         Returns
2240         -------
2241         obj : DataArray
2242             Another dataarray, with this dataarray's data but replaced
2243             coordinates.
2244 
2245         See Also
2246         --------
2247         DataArray.set_index
2248         """
2249         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2250         return self._from_temp_dataset(ds)
2251 
2252     def reorder_levels(
2253         self: T_DataArray,
2254         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2255         **dim_order_kwargs: Sequence[int | Hashable],
2256     ) -> T_DataArray:
2257         """Rearrange index levels using input order.
2258 
2259         Parameters
2260         ----------
2261         dim_order dict-like of Hashable to int or Hashable: optional
2262             Mapping from names matching dimensions and values given
2263             by lists representing new level orders. Every given dimension
2264             must have a multi-index.
2265         **dim_order_kwargs : optional
2266             The keyword arguments form of ``dim_order``.
2267             One of dim_order or dim_order_kwargs must be provided.
2268 
2269         Returns
2270         -------
2271         obj : DataArray
2272             Another dataarray, with this dataarray's data but replaced
2273             coordinates.
2274         """
2275         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2276         return self._from_temp_dataset(ds)
2277 
2278     def stack(
2279         self: T_DataArray,
2280         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2281         create_index: bool | None = True,
2282         index_cls: type[Index] = PandasMultiIndex,
2283         **dimensions_kwargs: Sequence[Hashable],
2284     ) -> T_DataArray:
2285         """
2286         Stack any number of existing dimensions into a single new dimension.
2287 
2288         New dimensions will be added at the end, and the corresponding
2289         coordinate variables will be combined into a MultiIndex.
2290 
2291         Parameters
2292         ----------
2293         dimensions : mapping of Hashable to sequence of Hashable
2294             Mapping of the form `new_name=(dim1, dim2, ...)`.
2295             Names of new dimensions, and the existing dimensions that they
2296             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2297             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2298             all dimensions.
2299         create_index : bool or None, default: True
2300             If True, create a multi-index for each of the stacked dimensions.
2301             If False, don't create any index.
2302             If None, create a multi-index only if exactly one single (1-d) coordinate
2303             index is found for every dimension to stack.
2304         index_cls: class, optional
2305             Can be used to pass a custom multi-index type. Must be an Xarray index that
2306             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2307         **dimensions_kwargs
2308             The keyword arguments form of ``dimensions``.
2309             One of dimensions or dimensions_kwargs must be provided.
2310 
2311         Returns
2312         -------
2313         stacked : DataArray
2314             DataArray with stacked data.
2315 
2316         Examples
2317         --------
2318         >>> arr = xr.DataArray(
2319         ...     np.arange(6).reshape(2, 3),
2320         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2321         ... )
2322         >>> arr
2323         <xarray.DataArray (x: 2, y: 3)>
2324         array([[0, 1, 2],
2325                [3, 4, 5]])
2326         Coordinates:
2327           * x        (x) <U1 'a' 'b'
2328           * y        (y) int64 0 1 2
2329         >>> stacked = arr.stack(z=("x", "y"))
2330         >>> stacked.indexes["z"]
2331         MultiIndex([('a', 0),
2332                     ('a', 1),
2333                     ('a', 2),
2334                     ('b', 0),
2335                     ('b', 1),
2336                     ('b', 2)],
2337                    name='z')
2338 
2339         See Also
2340         --------
2341         DataArray.unstack
2342         """
2343         ds = self._to_temp_dataset().stack(
2344             dimensions,
2345             create_index=create_index,
2346             index_cls=index_cls,
2347             **dimensions_kwargs,
2348         )
2349         return self._from_temp_dataset(ds)
2350 
2351     # change type of self and return to T_DataArray once
2352     # https://github.com/python/mypy/issues/12846 is resolved
2353     def unstack(
2354         self,
2355         dim: Hashable | Sequence[Hashable] | None = None,
2356         fill_value: Any = dtypes.NA,
2357         sparse: bool = False,
2358     ) -> DataArray:
2359         """
2360         Unstack existing dimensions corresponding to MultiIndexes into
2361         multiple new dimensions.
2362 
2363         New dimensions will be added at the end.
2364 
2365         Parameters
2366         ----------
2367         dim : Hashable or sequence of Hashable, optional
2368             Dimension(s) over which to unstack. By default unstacks all
2369             MultiIndexes.
2370         fill_value : scalar or dict-like, default: nan
2371             Value to be filled. If a dict-like, maps variable names to
2372             fill values. Use the data array's name to refer to its
2373             name. If not provided or if the dict-like does not contain
2374             all variables, the dtype's NA value will be used.
2375         sparse : bool, default: False
2376             Use sparse-array if True
2377 
2378         Returns
2379         -------
2380         unstacked : DataArray
2381             Array with unstacked data.
2382 
2383         Examples
2384         --------
2385         >>> arr = xr.DataArray(
2386         ...     np.arange(6).reshape(2, 3),
2387         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2388         ... )
2389         >>> arr
2390         <xarray.DataArray (x: 2, y: 3)>
2391         array([[0, 1, 2],
2392                [3, 4, 5]])
2393         Coordinates:
2394           * x        (x) <U1 'a' 'b'
2395           * y        (y) int64 0 1 2
2396         >>> stacked = arr.stack(z=("x", "y"))
2397         >>> stacked.indexes["z"]
2398         MultiIndex([('a', 0),
2399                     ('a', 1),
2400                     ('a', 2),
2401                     ('b', 0),
2402                     ('b', 1),
2403                     ('b', 2)],
2404                    name='z')
2405         >>> roundtripped = stacked.unstack()
2406         >>> arr.identical(roundtripped)
2407         True
2408 
2409         See Also
2410         --------
2411         DataArray.stack
2412         """
2413         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2414         return self._from_temp_dataset(ds)
2415 
2416     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2417         """Unstack DataArray expanding to Dataset along a given level of a
2418         stacked coordinate.
2419 
2420         This is the inverse operation of Dataset.to_stacked_array.
2421 
2422         Parameters
2423         ----------
2424         dim : Hashable
2425             Name of existing dimension to unstack
2426         level : int or Hashable, default: 0
2427             The MultiIndex level to expand to a dataset along. Can either be
2428             the integer index of the level or its name.
2429 
2430         Returns
2431         -------
2432         unstacked: Dataset
2433 
2434         Examples
2435         --------
2436         >>> arr = xr.DataArray(
2437         ...     np.arange(6).reshape(2, 3),
2438         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2439         ... )
2440         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2441         >>> data
2442         <xarray.Dataset>
2443         Dimensions:  (x: 2, y: 3)
2444         Coordinates:
2445           * x        (x) <U1 'a' 'b'
2446           * y        (y) int64 0 1 2
2447         Data variables:
2448             a        (x, y) int64 0 1 2 3 4 5
2449             b        (x) int64 0 3
2450         >>> stacked = data.to_stacked_array("z", ["x"])
2451         >>> stacked.indexes["z"]
2452         MultiIndex([('a', 0.0),
2453                     ('a', 1.0),
2454                     ('a', 2.0),
2455                     ('b', nan)],
2456                    name='z')
2457         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2458         >>> data.identical(roundtripped)
2459         True
2460 
2461         See Also
2462         --------
2463         Dataset.to_stacked_array
2464         """
2465         idx = self._indexes[dim].to_pandas_index()
2466         if not isinstance(idx, pd.MultiIndex):
2467             raise ValueError(f"'{dim}' is not a stacked coordinate")
2468 
2469         level_number = idx._get_level_number(level)
2470         variables = idx.levels[level_number]
2471         variable_dim = idx.names[level_number]
2472 
2473         # pull variables out of datarray
2474         data_dict = {}
2475         for k in variables:
2476             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2477 
2478         # unstacked dataset
2479         return Dataset(data_dict)
2480 
2481     def transpose(
2482         self: T_DataArray,
2483         *dims: Hashable,
2484         transpose_coords: bool = True,
2485         missing_dims: ErrorOptionsWithWarn = "raise",
2486     ) -> T_DataArray:
2487         """Return a new DataArray object with transposed dimensions.
2488 
2489         Parameters
2490         ----------
2491         *dims : Hashable, optional
2492             By default, reverse the dimensions. Otherwise, reorder the
2493             dimensions to this order.
2494         transpose_coords : bool, default: True
2495             If True, also transpose the coordinates of this DataArray.
2496         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2497             What to do if dimensions that should be selected from are not present in the
2498             DataArray:
2499             - "raise": raise an exception
2500             - "warn": raise a warning, and ignore the missing dimensions
2501             - "ignore": ignore the missing dimensions
2502 
2503         Returns
2504         -------
2505         transposed : DataArray
2506             The returned DataArray's array is transposed.
2507 
2508         Notes
2509         -----
2510         This operation returns a view of this array's data. It is
2511         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2512         -- the data will be fully loaded.
2513 
2514         See Also
2515         --------
2516         numpy.transpose
2517         Dataset.transpose
2518         """
2519         if dims:
2520             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2521         variable = self.variable.transpose(*dims)
2522         if transpose_coords:
2523             coords: dict[Hashable, Variable] = {}
2524             for name, coord in self.coords.items():
2525                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2526                 coords[name] = coord.variable.transpose(*coord_dims)
2527             return self._replace(variable, coords)
2528         else:
2529             return self._replace(variable)
2530 
2531     @property
2532     def T(self: T_DataArray) -> T_DataArray:
2533         return self.transpose()
2534 
2535     # change type of self and return to T_DataArray once
2536     # https://github.com/python/mypy/issues/12846 is resolved
2537     def drop_vars(
2538         self,
2539         names: Hashable | Iterable[Hashable],
2540         *,
2541         errors: ErrorOptions = "raise",
2542     ) -> DataArray:
2543         """Returns an array with dropped variables.
2544 
2545         Parameters
2546         ----------
2547         names : Hashable or iterable of Hashable
2548             Name(s) of variables to drop.
2549         errors : {"raise", "ignore"}, default: "raise"
2550             If 'raise', raises a ValueError error if any of the variable
2551             passed are not in the dataset. If 'ignore', any given names that are in the
2552             DataArray are dropped and no error is raised.
2553 
2554         Returns
2555         -------
2556         dropped : Dataset
2557             New Dataset copied from `self` with variables removed.
2558         """
2559         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2560         return self._from_temp_dataset(ds)
2561 
2562     def drop(
2563         self: T_DataArray,
2564         labels: Mapping[Any, Any] | None = None,
2565         dim: Hashable | None = None,
2566         *,
2567         errors: ErrorOptions = "raise",
2568         **labels_kwargs,
2569     ) -> T_DataArray:
2570         """Backward compatible method based on `drop_vars` and `drop_sel`
2571 
2572         Using either `drop_vars` or `drop_sel` is encouraged
2573 
2574         See Also
2575         --------
2576         DataArray.drop_vars
2577         DataArray.drop_sel
2578         """
2579         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
2580         return self._from_temp_dataset(ds)
2581 
2582     def drop_sel(
2583         self: T_DataArray,
2584         labels: Mapping[Any, Any] | None = None,
2585         *,
2586         errors: ErrorOptions = "raise",
2587         **labels_kwargs,
2588     ) -> T_DataArray:
2589         """Drop index labels from this DataArray.
2590 
2591         Parameters
2592         ----------
2593         labels : mapping of Hashable to Any
2594             Index labels to drop
2595         errors : {"raise", "ignore"}, default: "raise"
2596             If 'raise', raises a ValueError error if
2597             any of the index labels passed are not
2598             in the dataset. If 'ignore', any given labels that are in the
2599             dataset are dropped and no error is raised.
2600         **labels_kwargs : {dim: label, ...}, optional
2601             The keyword arguments form of ``dim`` and ``labels``
2602 
2603         Returns
2604         -------
2605         dropped : DataArray
2606         """
2607         if labels_kwargs or isinstance(labels, dict):
2608             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2609 
2610         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2611         return self._from_temp_dataset(ds)
2612 
2613     def drop_isel(
2614         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
2615     ) -> T_DataArray:
2616         """Drop index positions from this DataArray.
2617 
2618         Parameters
2619         ----------
2620         indexers : mapping of Hashable to Any or None, default: None
2621             Index locations to drop
2622         **indexers_kwargs : {dim: position, ...}, optional
2623             The keyword arguments form of ``dim`` and ``positions``
2624 
2625         Returns
2626         -------
2627         dropped : DataArray
2628 
2629         Raises
2630         ------
2631         IndexError
2632         """
2633         dataset = self._to_temp_dataset()
2634         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2635         return self._from_temp_dataset(dataset)
2636 
2637     def dropna(
2638         self: T_DataArray,
2639         dim: Hashable,
2640         how: Literal["any", "all"] = "any",
2641         thresh: int | None = None,
2642     ) -> T_DataArray:
2643         """Returns a new array with dropped labels for missing values along
2644         the provided dimension.
2645 
2646         Parameters
2647         ----------
2648         dim : Hashable
2649             Dimension along which to drop missing values. Dropping along
2650             multiple dimensions simultaneously is not yet supported.
2651         how : {"any", "all"}, default: "any"
2652             - any : if any NA values are present, drop that label
2653             - all : if all values are NA, drop that label
2654 
2655         thresh : int or None, default: None
2656             If supplied, require this many non-NA values.
2657 
2658         Returns
2659         -------
2660         dropped : DataArray
2661         """
2662         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2663         return self._from_temp_dataset(ds)
2664 
2665     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
2666         """Fill missing values in this object.
2667 
2668         This operation follows the normal broadcasting and alignment rules that
2669         xarray uses for binary arithmetic, except the result is aligned to this
2670         object (``join='left'``) instead of aligned to the intersection of
2671         index coordinates (``join='inner'``).
2672 
2673         Parameters
2674         ----------
2675         value : scalar, ndarray or DataArray
2676             Used to fill all matching missing values in this array. If the
2677             argument is a DataArray, it is first aligned with (reindexed to)
2678             this array.
2679 
2680         Returns
2681         -------
2682         filled : DataArray
2683         """
2684         if utils.is_dict_like(value):
2685             raise TypeError(
2686                 "cannot provide fill value as a dictionary with "
2687                 "fillna on a DataArray"
2688             )
2689         out = ops.fillna(self, value)
2690         return out
2691 
2692     def interpolate_na(
2693         self: T_DataArray,
2694         dim: Hashable | None = None,
2695         method: InterpOptions = "linear",
2696         limit: int | None = None,
2697         use_coordinate: bool | str = True,
2698         max_gap: (
2699             None
2700             | int
2701             | float
2702             | str
2703             | pd.Timedelta
2704             | np.timedelta64
2705             | datetime.timedelta
2706         ) = None,
2707         keep_attrs: bool | None = None,
2708         **kwargs: Any,
2709     ) -> T_DataArray:
2710         """Fill in NaNs by interpolating according to different methods.
2711 
2712         Parameters
2713         ----------
2714         dim : Hashable or None, optional
2715             Specifies the dimension along which to interpolate.
2716         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
2717             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
2718             String indicating which method to use for interpolation:
2719 
2720             - 'linear': linear interpolation. Additional keyword
2721               arguments are passed to :py:func:`numpy.interp`
2722             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2723               are passed to :py:func:`scipy.interpolate.interp1d`. If
2724               ``method='polynomial'``, the ``order`` keyword argument must also be
2725               provided.
2726             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2727               respective :py:class:`scipy.interpolate` classes.
2728 
2729         use_coordinate : bool or str, default: True
2730             Specifies which index to use as the x values in the interpolation
2731             formulated as `y = f(x)`. If False, values are treated as if
2732             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2733             used. If ``use_coordinate`` is a string, it specifies the name of a
2734             coordinate variariable to use as the index.
2735         limit : int or None, default: None
2736             Maximum number of consecutive NaNs to fill. Must be greater than 0
2737             or None for no limit. This filling is done regardless of the size of
2738             the gap in the data. To only interpolate over gaps less than a given length,
2739             see ``max_gap``.
2740         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2741             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2742             Use None for no limit. When interpolating along a datetime64 dimension
2743             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2744 
2745             - a string that is valid input for pandas.to_timedelta
2746             - a :py:class:`numpy.timedelta64` object
2747             - a :py:class:`pandas.Timedelta` object
2748             - a :py:class:`datetime.timedelta` object
2749 
2750             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2751             dimensions has not been implemented yet. Gap length is defined as the difference
2752             between coordinate values at the first data point after a gap and the last value
2753             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2754             between coordinate values at the first (last) valid data point and the first (last) NaN.
2755             For example, consider::
2756 
2757                 <xarray.DataArray (x: 9)>
2758                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2759                 Coordinates:
2760                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2761 
2762             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2763         keep_attrs : bool or None, default: None
2764             If True, the dataarray's attributes (`attrs`) will be copied from
2765             the original object to the new one.  If False, the new
2766             object will be returned without attributes.
2767         **kwargs : dict, optional
2768             parameters passed verbatim to the underlying interpolation function
2769 
2770         Returns
2771         -------
2772         interpolated: DataArray
2773             Filled in DataArray.
2774 
2775         See Also
2776         --------
2777         numpy.interp
2778         scipy.interpolate
2779 
2780         Examples
2781         --------
2782         >>> da = xr.DataArray(
2783         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2784         ... )
2785         >>> da
2786         <xarray.DataArray (x: 5)>
2787         array([nan,  2.,  3., nan,  0.])
2788         Coordinates:
2789           * x        (x) int64 0 1 2 3 4
2790 
2791         >>> da.interpolate_na(dim="x", method="linear")
2792         <xarray.DataArray (x: 5)>
2793         array([nan, 2. , 3. , 1.5, 0. ])
2794         Coordinates:
2795           * x        (x) int64 0 1 2 3 4
2796 
2797         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2798         <xarray.DataArray (x: 5)>
2799         array([1. , 2. , 3. , 1.5, 0. ])
2800         Coordinates:
2801           * x        (x) int64 0 1 2 3 4
2802         """
2803         from .missing import interp_na
2804 
2805         return interp_na(
2806             self,
2807             dim=dim,
2808             method=method,
2809             limit=limit,
2810             use_coordinate=use_coordinate,
2811             max_gap=max_gap,
2812             keep_attrs=keep_attrs,
2813             **kwargs,
2814         )
2815 
2816     def ffill(
2817         self: T_DataArray, dim: Hashable, limit: int | None = None
2818     ) -> T_DataArray:
2819         """Fill NaN values by propagating values forward
2820 
2821         *Requires bottleneck.*
2822 
2823         Parameters
2824         ----------
2825         dim : Hashable
2826             Specifies the dimension along which to propagate values when
2827             filling.
2828         limit : int or None, default: None
2829             The maximum number of consecutive NaN values to forward fill. In
2830             other words, if there is a gap with more than this number of
2831             consecutive NaNs, it will only be partially filled. Must be greater
2832             than 0 or None for no limit. Must be None or greater than or equal
2833             to axis length if filling along chunked axes (dimensions).
2834 
2835         Returns
2836         -------
2837         filled : DataArray
2838         """
2839         from .missing import ffill
2840 
2841         return ffill(self, dim, limit=limit)
2842 
2843     def bfill(
2844         self: T_DataArray, dim: Hashable, limit: int | None = None
2845     ) -> T_DataArray:
2846         """Fill NaN values by propagating values backward
2847 
2848         *Requires bottleneck.*
2849 
2850         Parameters
2851         ----------
2852         dim : str
2853             Specifies the dimension along which to propagate values when
2854             filling.
2855         limit : int or None, default: None
2856             The maximum number of consecutive NaN values to backward fill. In
2857             other words, if there is a gap with more than this number of
2858             consecutive NaNs, it will only be partially filled. Must be greater
2859             than 0 or None for no limit. Must be None or greater than or equal
2860             to axis length if filling along chunked axes (dimensions).
2861 
2862         Returns
2863         -------
2864         filled : DataArray
2865         """
2866         from .missing import bfill
2867 
2868         return bfill(self, dim, limit=limit)
2869 
2870     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
2871         """Combine two DataArray objects, with union of coordinates.
2872 
2873         This operation follows the normal broadcasting and alignment rules of
2874         ``join='outer'``.  Default to non-null values of array calling the
2875         method.  Use np.nan to fill in vacant cells after alignment.
2876 
2877         Parameters
2878         ----------
2879         other : DataArray
2880             Used to fill all matching missing values in this array.
2881 
2882         Returns
2883         -------
2884         DataArray
2885         """
2886         return ops.fillna(self, other, join="outer")
2887 
2888     def reduce(
2889         self: T_DataArray,
2890         func: Callable[..., Any],
2891         dim: None | Hashable | Iterable[Hashable] = None,
2892         *,
2893         axis: None | int | Sequence[int] = None,
2894         keep_attrs: bool | None = None,
2895         keepdims: bool = False,
2896         **kwargs: Any,
2897     ) -> T_DataArray:
2898         """Reduce this array by applying `func` along some dimension(s).
2899 
2900         Parameters
2901         ----------
2902         func : callable
2903             Function which can be called in the form
2904             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2905             np.ndarray over an integer valued axis.
2906         dim : Hashable or Iterable of Hashable, optional
2907             Dimension(s) over which to apply `func`.
2908         axis : int or sequence of int, optional
2909             Axis(es) over which to repeatedly apply `func`. Only one of the
2910             'dim' and 'axis' arguments can be supplied. If neither are
2911             supplied, then the reduction is calculated over the flattened array
2912             (by calling `f(x)` without an axis argument).
2913         keep_attrs : bool or None, optional
2914             If True, the variable's attributes (`attrs`) will be copied from
2915             the original object to the new one.  If False (default), the new
2916             object will be returned without attributes.
2917         keepdims : bool, default: False
2918             If True, the dimensions which are reduced are left in the result
2919             as dimensions of size one. Coordinates that use these dimensions
2920             are removed.
2921         **kwargs : dict
2922             Additional keyword arguments passed on to `func`.
2923 
2924         Returns
2925         -------
2926         reduced : DataArray
2927             DataArray with this object's array replaced with an array with
2928             summarized data and the indicated dimension(s) removed.
2929         """
2930 
2931         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2932         return self._replace_maybe_drop_dims(var)
2933 
2934     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
2935         """Convert this array into a pandas object with the same shape.
2936 
2937         The type of the returned object depends on the number of DataArray
2938         dimensions:
2939 
2940         * 0D -> `xarray.DataArray`
2941         * 1D -> `pandas.Series`
2942         * 2D -> `pandas.DataFrame`
2943 
2944         Only works for arrays with 2 or fewer dimensions.
2945 
2946         The DataArray constructor performs the inverse transformation.
2947 
2948         Returns
2949         -------
2950         result : DataArray | Series | DataFrame
2951             DataArray, pandas Series or pandas DataFrame.
2952         """
2953         # TODO: consolidate the info about pandas constructors and the
2954         # attributes that correspond to their indexes into a separate module?
2955         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2956         try:
2957             constructor = constructors[self.ndim]
2958         except KeyError:
2959             raise ValueError(
2960                 f"Cannot convert arrays with {self.ndim} dimensions into "
2961                 "pandas objects. Requires 2 or fewer dimensions."
2962             )
2963         indexes = [self.get_index(dim) for dim in self.dims]
2964         return constructor(self.values, *indexes)
2965 
2966     def to_dataframe(
2967         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
2968     ) -> pd.DataFrame:
2969         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2970 
2971         The DataFrame is indexed by the Cartesian product of index coordinates
2972         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
2973         included as columns in the DataFrame.
2974 
2975         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
2976         doesn't rely on a MultiIndex to build the DataFrame.
2977 
2978         Parameters
2979         ----------
2980         name: Hashable or None, optional
2981             Name to give to this array (required if unnamed).
2982         dim_order: Sequence of Hashable or None, optional
2983             Hierarchical dimension order for the resulting dataframe.
2984             Array content is transposed to this order and then written out as flat
2985             vectors in contiguous order, so the last dimension in this list
2986             will be contiguous in the resulting DataFrame. This has a major
2987             influence on which operations are efficient on the resulting
2988             dataframe.
2989 
2990             If provided, must include all dimensions of this DataArray. By default,
2991             dimensions are sorted according to the DataArray dimensions order.
2992 
2993         Returns
2994         -------
2995         result: DataFrame
2996             DataArray as a pandas DataFrame.
2997 
2998         See also
2999         --------
3000         DataArray.to_pandas
3001         DataArray.to_series
3002         """
3003         if name is None:
3004             name = self.name
3005         if name is None:
3006             raise ValueError(
3007                 "cannot convert an unnamed DataArray to a "
3008                 "DataFrame: use the ``name`` parameter"
3009             )
3010         if self.ndim == 0:
3011             raise ValueError("cannot convert a scalar to a DataFrame")
3012 
3013         # By using a unique name, we can convert a DataArray into a DataFrame
3014         # even if it shares a name with one of its coordinates.
3015         # I would normally use unique_name = object() but that results in a
3016         # dataframe with columns in the wrong order, for reasons I have not
3017         # been able to debug (possibly a pandas bug?).
3018         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3019         ds = self._to_dataset_whole(name=unique_name)
3020 
3021         if dim_order is None:
3022             ordered_dims = dict(zip(self.dims, self.shape))
3023         else:
3024             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3025 
3026         df = ds._to_dataframe(ordered_dims)
3027         df.columns = [name if c == unique_name else c for c in df.columns]
3028         return df
3029 
3030     def to_series(self) -> pd.Series:
3031         """Convert this array into a pandas.Series.
3032 
3033         The Series is indexed by the Cartesian product of index coordinates
3034         (in the form of a :py:class:`pandas.MultiIndex`).
3035 
3036         Returns
3037         -------
3038         result : Series
3039             DataArray as a pandas Series.
3040 
3041         See also
3042         --------
3043         DataArray.to_pandas
3044         DataArray.to_dataframe
3045         """
3046         index = self.coords.to_index()
3047         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3048 
3049     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3050         """Convert this array into a numpy.ma.MaskedArray
3051 
3052         Parameters
3053         ----------
3054         copy : bool, default: True
3055             If True make a copy of the array in the result. If False,
3056             a MaskedArray view of DataArray.values is returned.
3057 
3058         Returns
3059         -------
3060         result : MaskedArray
3061             Masked where invalid values (nan or inf) occur.
3062         """
3063         values = self.to_numpy()  # only compute lazy arrays once
3064         isnull = pd.isnull(values)
3065         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3066 
3067     # path=None writes to bytes
3068     @overload
3069     def to_netcdf(
3070         self,
3071         path: None = None,
3072         mode: Literal["w", "a"] = "w",
3073         format: T_NetcdfTypes | None = None,
3074         group: str | None = None,
3075         engine: T_NetcdfEngine | None = None,
3076         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3077         unlimited_dims: Iterable[Hashable] | None = None,
3078         compute: bool = True,
3079         invalid_netcdf: bool = False,
3080     ) -> bytes:
3081         ...
3082 
3083     # default return None
3084     @overload
3085     def to_netcdf(
3086         self,
3087         path: str | PathLike,
3088         mode: Literal["w", "a"] = "w",
3089         format: T_NetcdfTypes | None = None,
3090         group: str | None = None,
3091         engine: T_NetcdfEngine | None = None,
3092         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3093         unlimited_dims: Iterable[Hashable] | None = None,
3094         compute: Literal[True] = True,
3095         invalid_netcdf: bool = False,
3096     ) -> None:
3097         ...
3098 
3099     # compute=False returns dask.Delayed
3100     @overload
3101     def to_netcdf(
3102         self,
3103         path: str | PathLike,
3104         mode: Literal["w", "a"] = "w",
3105         format: T_NetcdfTypes | None = None,
3106         group: str | None = None,
3107         engine: T_NetcdfEngine | None = None,
3108         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3109         unlimited_dims: Iterable[Hashable] | None = None,
3110         *,
3111         compute: Literal[False],
3112         invalid_netcdf: bool = False,
3113     ) -> Delayed:
3114         ...
3115 
3116     def to_netcdf(
3117         self,
3118         path: str | PathLike | None = None,
3119         mode: Literal["w", "a"] = "w",
3120         format: T_NetcdfTypes | None = None,
3121         group: str | None = None,
3122         engine: T_NetcdfEngine | None = None,
3123         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3124         unlimited_dims: Iterable[Hashable] | None = None,
3125         compute: bool = True,
3126         invalid_netcdf: bool = False,
3127     ) -> bytes | Delayed | None:
3128         """Write dataset contents to a netCDF file.
3129 
3130         Parameters
3131         ----------
3132         path : str, path-like or None, optional
3133             Path to which to save this dataset. File-like objects are only
3134             supported by the scipy engine. If no path is provided, this
3135             function returns the resulting netCDF file as bytes; in this case,
3136             we need to use scipy, which does not support netCDF version 4 (the
3137             default format becomes NETCDF3_64BIT).
3138         mode : {"w", "a"}, default: "w"
3139             Write ('w') or append ('a') mode. If mode='w', any existing file at
3140             this location will be overwritten. If mode='a', existing variables
3141             will be overwritten.
3142         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3143                   "NETCDF3_CLASSIC"}, optional
3144             File format for the resulting netCDF file:
3145 
3146             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3147               features.
3148             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3149               netCDF 3 compatible API features.
3150             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3151               which fully supports 2+ GB files, but is only compatible with
3152               clients linked against netCDF version 3.6.0 or later.
3153             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3154               handle 2+ GB files very well.
3155 
3156             All formats are supported by the netCDF4-python library.
3157             scipy.io.netcdf only supports the last two formats.
3158 
3159             The default format is NETCDF4 if you are saving a file to disk and
3160             have the netCDF4-python library available. Otherwise, xarray falls
3161             back to using scipy to write netCDF files and defaults to the
3162             NETCDF3_64BIT format (scipy does not support netCDF4).
3163         group : str, optional
3164             Path to the netCDF4 group in the given file to open (only works for
3165             format='NETCDF4'). The group(s) will be created if necessary.
3166         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3167             Engine to use when writing netCDF files. If not provided, the
3168             default engine is chosen based on available dependencies, with a
3169             preference for 'netcdf4' if writing to a file on disk.
3170         encoding : dict, optional
3171             Nested dictionary with variable names as keys and dictionaries of
3172             variable specific encodings as values, e.g.,
3173             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3174             "zlib": True}, ...}``
3175 
3176             The `h5netcdf` engine supports both the NetCDF4-style compression
3177             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3178             ones ``{"compression": "gzip", "compression_opts": 9}``.
3179             This allows using any compression plugin installed in the HDF5
3180             library, e.g. LZF.
3181 
3182         unlimited_dims : iterable of Hashable, optional
3183             Dimension(s) that should be serialized as unlimited dimensions.
3184             By default, no dimensions are treated as unlimited dimensions.
3185             Note that unlimited_dims may also be set via
3186             ``dataset.encoding["unlimited_dims"]``.
3187         compute: bool, default: True
3188             If true compute immediately, otherwise return a
3189             ``dask.delayed.Delayed`` object that can be computed later.
3190         invalid_netcdf: bool, default: False
3191             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3192             hdf5 files which are invalid netcdf as described in
3193             https://github.com/h5netcdf/h5netcdf.
3194 
3195         Returns
3196         -------
3197         store: bytes or Delayed or None
3198             * ``bytes`` if path is None
3199             * ``dask.delayed.Delayed`` if compute is False
3200             * None otherwise
3201 
3202         Notes
3203         -----
3204         Only xarray.Dataset objects can be written to netCDF files, so
3205         the xarray.DataArray is converted to a xarray.Dataset object
3206         containing a single variable. If the DataArray has no name, or if the
3207         name is the same as a coordinate name, then it is given the name
3208         ``"__xarray_dataarray_variable__"``.
3209 
3210         See Also
3211         --------
3212         Dataset.to_netcdf
3213         """
3214         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3215 
3216         if self.name is None:
3217             # If no name is set then use a generic xarray name
3218             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3219         elif self.name in self.coords or self.name in self.dims:
3220             # The name is the same as one of the coords names, which netCDF
3221             # doesn't support, so rename it but keep track of the old name
3222             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3223             dataset.attrs[DATAARRAY_NAME] = self.name
3224         else:
3225             # No problems with the name - so we're fine!
3226             dataset = self.to_dataset()
3227 
3228         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3229             dataset,
3230             path,
3231             mode=mode,
3232             format=format,
3233             group=group,
3234             engine=engine,
3235             encoding=encoding,
3236             unlimited_dims=unlimited_dims,
3237             compute=compute,
3238             multifile=False,
3239             invalid_netcdf=invalid_netcdf,
3240         )
3241 
3242     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
3243         """
3244         Convert this xarray.DataArray into a dictionary following xarray
3245         naming conventions.
3246 
3247         Converts all variables and attributes to native Python objects.
3248         Useful for converting to json. To avoid datetime incompatibility
3249         use decode_times=False kwarg in xarray.open_dataset.
3250 
3251         Parameters
3252         ----------
3253         data : bool, default: True
3254             Whether to include the actual data in the dictionary. When set to
3255             False, returns just the schema.
3256         encoding : bool, default: False
3257             Whether to include the Dataset's encoding in the dictionary.
3258 
3259         Returns
3260         -------
3261         dict: dict
3262 
3263         See Also
3264         --------
3265         DataArray.from_dict
3266         Dataset.to_dict
3267         """
3268         d = self.variable.to_dict(data=data)
3269         d.update({"coords": {}, "name": self.name})
3270         for k in self.coords:
3271             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
3272         if encoding:
3273             d["encoding"] = dict(self.encoding)
3274         return d
3275 
3276     @classmethod
3277     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
3278         """Convert a dictionary into an xarray.DataArray
3279 
3280         Parameters
3281         ----------
3282         d : dict
3283             Mapping with a minimum structure of {"dims": [...], "data": [...]}
3284 
3285         Returns
3286         -------
3287         obj : xarray.DataArray
3288 
3289         See Also
3290         --------
3291         DataArray.to_dict
3292         Dataset.from_dict
3293 
3294         Examples
3295         --------
3296         >>> d = {"dims": "t", "data": [1, 2, 3]}
3297         >>> da = xr.DataArray.from_dict(d)
3298         >>> da
3299         <xarray.DataArray (t: 3)>
3300         array([1, 2, 3])
3301         Dimensions without coordinates: t
3302 
3303         >>> d = {
3304         ...     "coords": {
3305         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
3306         ...     },
3307         ...     "attrs": {"title": "air temperature"},
3308         ...     "dims": "t",
3309         ...     "data": [10, 20, 30],
3310         ...     "name": "a",
3311         ... }
3312         >>> da = xr.DataArray.from_dict(d)
3313         >>> da
3314         <xarray.DataArray 'a' (t: 3)>
3315         array([10, 20, 30])
3316         Coordinates:
3317           * t        (t) int64 0 1 2
3318         Attributes:
3319             title:    air temperature
3320         """
3321         coords = None
3322         if "coords" in d:
3323             try:
3324                 coords = {
3325                     k: (v["dims"], v["data"], v.get("attrs"))
3326                     for k, v in d["coords"].items()
3327                 }
3328             except KeyError as e:
3329                 raise ValueError(
3330                     "cannot convert dict when coords are missing the key "
3331                     "'{dims_data}'".format(dims_data=str(e.args[0]))
3332                 )
3333         try:
3334             data = d["data"]
3335         except KeyError:
3336             raise ValueError("cannot convert dict without the key 'data''")
3337         else:
3338             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
3339 
3340         obj.encoding.update(d.get("encoding", {}))
3341 
3342         return obj
3343 
3344     @classmethod
3345     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
3346         """Convert a pandas.Series into an xarray.DataArray.
3347 
3348         If the series's index is a MultiIndex, it will be expanded into a
3349         tensor product of one-dimensional coordinates (filling in missing
3350         values with NaN). Thus this operation should be the inverse of the
3351         `to_series` method.
3352 
3353         Parameters
3354         ----------
3355         series : Series
3356             Pandas Series object to convert.
3357         sparse : bool, default: False
3358             If sparse=True, creates a sparse array instead of a dense NumPy array.
3359             Requires the pydata/sparse package.
3360 
3361         See Also
3362         --------
3363         DataArray.to_series
3364         Dataset.from_dataframe
3365         """
3366         temp_name = "__temporary_name"
3367         df = pd.DataFrame({temp_name: series})
3368         ds = Dataset.from_dataframe(df, sparse=sparse)
3369         result = cast(DataArray, ds[temp_name])
3370         result.name = series.name
3371         return result
3372 
3373     def to_cdms2(self) -> cdms2_Variable:
3374         """Convert this array into a cdms2.Variable"""
3375         from ..convert import to_cdms2
3376 
3377         return to_cdms2(self)
3378 
3379     @classmethod
3380     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
3381         """Convert a cdms2.Variable into an xarray.DataArray"""
3382         from ..convert import from_cdms2
3383 
3384         return from_cdms2(variable)
3385 
3386     def to_iris(self) -> iris_Cube:
3387         """Convert this array into a iris.cube.Cube"""
3388         from ..convert import to_iris
3389 
3390         return to_iris(self)
3391 
3392     @classmethod
3393     def from_iris(cls, cube: iris_Cube) -> DataArray:
3394         """Convert a iris.cube.Cube into an xarray.DataArray"""
3395         from ..convert import from_iris
3396 
3397         return from_iris(cube)
3398 
3399     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
3400         """Helper function for equals, broadcast_equals, and identical"""
3401 
3402         def compat(x, y):
3403             return getattr(x.variable, compat_str)(y.variable)
3404 
3405         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
3406             self, other
3407         )
3408 
3409     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
3410         """Two DataArrays are broadcast equal if they are equal after
3411         broadcasting them against each other such that they have the same
3412         dimensions.
3413 
3414         Parameters
3415         ----------
3416         other : DataArray
3417             DataArray to compare to.
3418 
3419         Returns
3420         ----------
3421         equal : bool
3422             True if the two DataArrays are broadcast equal.
3423 
3424         See Also
3425         --------
3426         DataArray.equals
3427         DataArray.identical
3428         """
3429         try:
3430             return self._all_compat(other, "broadcast_equals")
3431         except (TypeError, AttributeError):
3432             return False
3433 
3434     def equals(self: T_DataArray, other: T_DataArray) -> bool:
3435         """True if two DataArrays have the same dimensions, coordinates and
3436         values; otherwise False.
3437 
3438         DataArrays can still be equal (like pandas objects) if they have NaN
3439         values in the same locations.
3440 
3441         This method is necessary because `v1 == v2` for ``DataArray``
3442         does element-wise comparisons (like numpy.ndarrays).
3443 
3444         Parameters
3445         ----------
3446         other : DataArray
3447             DataArray to compare to.
3448 
3449         Returns
3450         ----------
3451         equal : bool
3452             True if the two DataArrays are equal.
3453 
3454         See Also
3455         --------
3456         DataArray.broadcast_equals
3457         DataArray.identical
3458         """
3459         try:
3460             return self._all_compat(other, "equals")
3461         except (TypeError, AttributeError):
3462             return False
3463 
3464     def identical(self: T_DataArray, other: T_DataArray) -> bool:
3465         """Like equals, but also checks the array name and attributes, and
3466         attributes on all coordinates.
3467 
3468         Parameters
3469         ----------
3470         other : DataArray
3471             DataArray to compare to.
3472 
3473         Returns
3474         ----------
3475         equal : bool
3476             True if the two DataArrays are identical.
3477 
3478         See Also
3479         --------
3480         DataArray.broadcast_equals
3481         DataArray.equals
3482         """
3483         try:
3484             return self.name == other.name and self._all_compat(other, "identical")
3485         except (TypeError, AttributeError):
3486             return False
3487 
3488     def _result_name(self, other: Any = None) -> Hashable | None:
3489         # use the same naming heuristics as pandas:
3490         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
3491         other_name = getattr(other, "name", _default)
3492         if other_name is _default or other_name == self.name:
3493             return self.name
3494         else:
3495             return None
3496 
3497     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
3498         new_var = self.variable.__array_wrap__(obj, context)
3499         return self._replace(new_var)
3500 
3501     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
3502         return self.dot(obj)
3503 
3504     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3505         # currently somewhat duplicative, as only other DataArrays are
3506         # compatible with matmul
3507         return computation.dot(other, self)
3508 
3509     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
3510         keep_attrs = kwargs.pop("keep_attrs", None)
3511         if keep_attrs is None:
3512             keep_attrs = _get_keep_attrs(default=True)
3513         with warnings.catch_warnings():
3514             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
3515             warnings.filterwarnings(
3516                 "ignore", r"Mean of empty slice", category=RuntimeWarning
3517             )
3518             with np.errstate(all="ignore"):
3519                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
3520             if keep_attrs:
3521                 da.attrs = self.attrs
3522             return da
3523 
3524     def _binary_op(
3525         self: T_DataArray,
3526         other: Any,
3527         f: Callable,
3528         reflexive: bool = False,
3529     ) -> T_DataArray:
3530         from .groupby import GroupBy
3531 
3532         if isinstance(other, (Dataset, GroupBy)):
3533             return NotImplemented
3534         if isinstance(other, DataArray):
3535             align_type = OPTIONS["arithmetic_join"]
3536             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
3537         other_variable = getattr(other, "variable", other)
3538         other_coords = getattr(other, "coords", None)
3539 
3540         variable = (
3541             f(self.variable, other_variable)
3542             if not reflexive
3543             else f(other_variable, self.variable)
3544         )
3545         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
3546         name = self._result_name(other)
3547 
3548         return self._replace(variable, coords, name, indexes=indexes)
3549 
3550     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
3551         from .groupby import GroupBy
3552 
3553         if isinstance(other, GroupBy):
3554             raise TypeError(
3555                 "in-place operations between a DataArray and "
3556                 "a grouped object are not permitted"
3557             )
3558         # n.b. we can't align other to self (with other.reindex_like(self))
3559         # because `other` may be converted into floats, which would cause
3560         # in-place arithmetic to fail unpredictably. Instead, we simply
3561         # don't support automatic alignment with in-place arithmetic.
3562         other_coords = getattr(other, "coords", None)
3563         other_variable = getattr(other, "variable", other)
3564         try:
3565             with self.coords._merge_inplace(other_coords):
3566                 f(self.variable, other_variable)
3567         except MergeError as exc:
3568             raise MergeError(
3569                 "Automatic alignment is not supported for in-place operations.\n"
3570                 "Consider aligning the indices manually or using a not-in-place operation.\n"
3571                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3572             ) from exc
3573         return self
3574 
3575     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
3576         self.attrs = other.attrs
3577 
3578     plot = utils.UncachedAccessor(_PlotMethods)
3579 
3580     def _title_for_slice(self, truncate: int = 50) -> str:
3581         """
3582         If the dataarray has 1 dimensional coordinates or comes from a slice
3583         we can show that info in the title
3584 
3585         Parameters
3586         ----------
3587         truncate : int, default: 50
3588             maximum number of characters for title
3589 
3590         Returns
3591         -------
3592         title : string
3593             Can be used for plot titles
3594 
3595         """
3596         one_dims = []
3597         for dim, coord in self.coords.items():
3598             if coord.size == 1:
3599                 one_dims.append(
3600                     "{dim} = {v}{unit}".format(
3601                         dim=dim,
3602                         v=format_item(coord.values),
3603                         unit=_get_units_from_attrs(coord),
3604                     )
3605                 )
3606 
3607         title = ", ".join(one_dims)
3608         if len(title) > truncate:
3609             title = title[: (truncate - 3)] + "..."
3610 
3611         return title
3612 
3613     def diff(
3614         self: T_DataArray,
3615         dim: Hashable,
3616         n: int = 1,
3617         label: Literal["upper", "lower"] = "upper",
3618     ) -> T_DataArray:
3619         """Calculate the n-th order discrete difference along given axis.
3620 
3621         Parameters
3622         ----------
3623         dim : Hashable
3624             Dimension over which to calculate the finite difference.
3625         n : int, default: 1
3626             The number of times values are differenced.
3627         label : {"upper", "lower"}, default: "upper"
3628             The new coordinate in dimension ``dim`` will have the
3629             values of either the minuend's or subtrahend's coordinate
3630             for values 'upper' and 'lower', respectively.
3631 
3632         Returns
3633         -------
3634         difference : DataArray
3635             The n-th order finite difference of this object.
3636 
3637         Notes
3638         -----
3639         `n` matches numpy's behavior and is different from pandas' first argument named
3640         `periods`.
3641 
3642         Examples
3643         --------
3644         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3645         >>> arr.diff("x")
3646         <xarray.DataArray (x: 3)>
3647         array([0, 1, 0])
3648         Coordinates:
3649           * x        (x) int64 2 3 4
3650         >>> arr.diff("x", 2)
3651         <xarray.DataArray (x: 2)>
3652         array([ 1, -1])
3653         Coordinates:
3654           * x        (x) int64 3 4
3655 
3656         See Also
3657         --------
3658         DataArray.differentiate
3659         """
3660         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3661         return self._from_temp_dataset(ds)
3662 
3663     def shift(
3664         self: T_DataArray,
3665         shifts: Mapping[Any, int] | None = None,
3666         fill_value: Any = dtypes.NA,
3667         **shifts_kwargs: int,
3668     ) -> T_DataArray:
3669         """Shift this DataArray by an offset along one or more dimensions.
3670 
3671         Only the data is moved; coordinates stay in place. This is consistent
3672         with the behavior of ``shift`` in pandas.
3673 
3674         Values shifted from beyond array bounds will appear at one end of
3675         each dimension, which are filled according to `fill_value`. For periodic
3676         offsets instead see `roll`.
3677 
3678         Parameters
3679         ----------
3680         shifts : mapping of Hashable to int or None, optional
3681             Integer offset to shift along each of the given dimensions.
3682             Positive offsets shift to the right; negative offsets shift to the
3683             left.
3684         fill_value : scalar, optional
3685             Value to use for newly missing values
3686         **shifts_kwargs
3687             The keyword arguments form of ``shifts``.
3688             One of shifts or shifts_kwargs must be provided.
3689 
3690         Returns
3691         -------
3692         shifted : DataArray
3693             DataArray with the same coordinates and attributes but shifted
3694             data.
3695 
3696         See Also
3697         --------
3698         roll
3699 
3700         Examples
3701         --------
3702         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3703         >>> arr.shift(x=1)
3704         <xarray.DataArray (x: 3)>
3705         array([nan,  5.,  6.])
3706         Dimensions without coordinates: x
3707         """
3708         variable = self.variable.shift(
3709             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3710         )
3711         return self._replace(variable=variable)
3712 
3713     def roll(
3714         self: T_DataArray,
3715         shifts: Mapping[Hashable, int] | None = None,
3716         roll_coords: bool = False,
3717         **shifts_kwargs: int,
3718     ) -> T_DataArray:
3719         """Roll this array by an offset along one or more dimensions.
3720 
3721         Unlike shift, roll treats the given dimensions as periodic, so will not
3722         create any missing values to be filled.
3723 
3724         Unlike shift, roll may rotate all variables, including coordinates
3725         if specified. The direction of rotation is consistent with
3726         :py:func:`numpy.roll`.
3727 
3728         Parameters
3729         ----------
3730         shifts : mapping of Hashable to int, optional
3731             Integer offset to rotate each of the given dimensions.
3732             Positive offsets roll to the right; negative offsets roll to the
3733             left.
3734         roll_coords : bool, default: False
3735             Indicates whether to roll the coordinates by the offset too.
3736         **shifts_kwargs : {dim: offset, ...}, optional
3737             The keyword arguments form of ``shifts``.
3738             One of shifts or shifts_kwargs must be provided.
3739 
3740         Returns
3741         -------
3742         rolled : DataArray
3743             DataArray with the same attributes but rolled data and coordinates.
3744 
3745         See Also
3746         --------
3747         shift
3748 
3749         Examples
3750         --------
3751         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3752         >>> arr.roll(x=1)
3753         <xarray.DataArray (x: 3)>
3754         array([7, 5, 6])
3755         Dimensions without coordinates: x
3756         """
3757         ds = self._to_temp_dataset().roll(
3758             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3759         )
3760         return self._from_temp_dataset(ds)
3761 
3762     @property
3763     def real(self: T_DataArray) -> T_DataArray:
3764         return self._replace(self.variable.real)
3765 
3766     @property
3767     def imag(self: T_DataArray) -> T_DataArray:
3768         return self._replace(self.variable.imag)
3769 
3770     def dot(
3771         self: T_DataArray,
3772         other: T_DataArray,
3773         dims: str | Iterable[Hashable] | Ellipsis | None = None,
3774     ) -> T_DataArray:
3775         """Perform dot product of two DataArrays along their shared dims.
3776 
3777         Equivalent to taking taking tensordot over all shared dims.
3778 
3779         Parameters
3780         ----------
3781         other : DataArray
3782             The other array with which the dot product is performed.
3783         dims : ..., str or Iterable of Hashable, optional
3784             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3785             If not specified, then all the common dimensions are summed over.
3786 
3787         Returns
3788         -------
3789         result : DataArray
3790             Array resulting from the dot product over all shared dimensions.
3791 
3792         See Also
3793         --------
3794         dot
3795         numpy.tensordot
3796 
3797         Examples
3798         --------
3799         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3800         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3801         >>> dm_vals = np.arange(4)
3802         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3803 
3804         >>> dm.dims
3805         ('z',)
3806 
3807         >>> da.dims
3808         ('x', 'y', 'z')
3809 
3810         >>> dot_result = da.dot(dm)
3811         >>> dot_result.dims
3812         ('x', 'y')
3813 
3814         """
3815         if isinstance(other, Dataset):
3816             raise NotImplementedError(
3817                 "dot products are not yet supported with Dataset objects."
3818             )
3819         if not isinstance(other, DataArray):
3820             raise TypeError("dot only operates on DataArrays.")
3821 
3822         return computation.dot(self, other, dims=dims)
3823 
3824     # change type of self and return to T_DataArray once
3825     # https://github.com/python/mypy/issues/12846 is resolved
3826     def sortby(
3827         self,
3828         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
3829         ascending: bool = True,
3830     ) -> DataArray:
3831         """Sort object by labels or values (along an axis).
3832 
3833         Sorts the dataarray, either along specified dimensions,
3834         or according to values of 1-D dataarrays that share dimension
3835         with calling object.
3836 
3837         If the input variables are dataarrays, then the dataarrays are aligned
3838         (via left-join) to the calling object prior to sorting by cell values.
3839         NaNs are sorted to the end, following Numpy convention.
3840 
3841         If multiple sorts along the same dimension is
3842         given, numpy's lexsort is performed along that dimension:
3843         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
3844         and the FIRST key in the sequence is used as the primary sort key,
3845         followed by the 2nd key, etc.
3846 
3847         Parameters
3848         ----------
3849         variables : Hashable, DataArray, or sequence of Hashable or DataArray
3850             1D DataArray objects or name(s) of 1D variable(s) in
3851             coords whose values are used to sort this array.
3852         ascending : bool, default: True
3853             Whether to sort by ascending or descending order.
3854 
3855         Returns
3856         -------
3857         sorted : DataArray
3858             A new dataarray where all the specified dims are sorted by dim
3859             labels.
3860 
3861         See Also
3862         --------
3863         Dataset.sortby
3864         numpy.sort
3865         pandas.sort_values
3866         pandas.sort_index
3867 
3868         Examples
3869         --------
3870         >>> da = xr.DataArray(
3871         ...     np.random.rand(5),
3872         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3873         ...     dims="time",
3874         ... )
3875         >>> da
3876         <xarray.DataArray (time: 5)>
3877         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3878         Coordinates:
3879           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3880 
3881         >>> da.sortby(da)
3882         <xarray.DataArray (time: 5)>
3883         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3884         Coordinates:
3885           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3886         """
3887         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3888         return self._from_temp_dataset(ds)
3889 
3890     def quantile(
3891         self: T_DataArray,
3892         q: ArrayLike,
3893         dim: str | Iterable[Hashable] | None = None,
3894         method: QUANTILE_METHODS = "linear",
3895         keep_attrs: bool | None = None,
3896         skipna: bool | None = None,
3897         interpolation: QUANTILE_METHODS = None,
3898     ) -> T_DataArray:
3899         """Compute the qth quantile of the data along the specified dimension.
3900 
3901         Returns the qth quantiles(s) of the array elements.
3902 
3903         Parameters
3904         ----------
3905         q : float or array-like of float
3906             Quantile to compute, which must be between 0 and 1 inclusive.
3907         dim : str or Iterable of Hashable, optional
3908             Dimension(s) over which to apply quantile.
3909         method : str, default: "linear"
3910             This optional parameter specifies the interpolation method to use when the
3911             desired quantile lies between two data points. The options sorted by their R
3912             type as summarized in the H&F paper [1]_ are:
3913 
3914                 1. "inverted_cdf" (*)
3915                 2. "averaged_inverted_cdf" (*)
3916                 3. "closest_observation" (*)
3917                 4. "interpolated_inverted_cdf" (*)
3918                 5. "hazen" (*)
3919                 6. "weibull" (*)
3920                 7. "linear"  (default)
3921                 8. "median_unbiased" (*)
3922                 9. "normal_unbiased" (*)
3923 
3924             The first three methods are discontiuous. The following discontinuous
3925             variations of the default "linear" (7.) option are also available:
3926 
3927                 * "lower"
3928                 * "higher"
3929                 * "midpoint"
3930                 * "nearest"
3931 
3932             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
3933             was previously called "interpolation", renamed in accordance with numpy
3934             version 1.22.0.
3935 
3936             (*) These methods require numpy version 1.22 or newer.
3937 
3938         keep_attrs : bool or None, optional
3939             If True, the dataset's attributes (`attrs`) will be copied from
3940             the original object to the new one.  If False (default), the new
3941             object will be returned without attributes.
3942         skipna : bool or None, optional
3943             If True, skip missing values (as marked by NaN). By default, only
3944             skips missing values for float dtypes; other dtypes either do not
3945             have a sentinel missing value (int) or skipna=True has not been
3946             implemented (object, datetime64 or timedelta64).
3947 
3948         Returns
3949         -------
3950         quantiles : DataArray
3951             If `q` is a single quantile, then the result
3952             is a scalar. If multiple percentiles are given, first axis of
3953             the result corresponds to the quantile and a quantile dimension
3954             is added to the return array. The other dimensions are the
3955             dimensions that remain after the reduction of the array.
3956 
3957         See Also
3958         --------
3959         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3960 
3961         Examples
3962         --------
3963         >>> da = xr.DataArray(
3964         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3965         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3966         ...     dims=("x", "y"),
3967         ... )
3968         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3969         <xarray.DataArray ()>
3970         array(0.7)
3971         Coordinates:
3972             quantile  float64 0.0
3973         >>> da.quantile(0, dim="x")
3974         <xarray.DataArray (y: 4)>
3975         array([0.7, 4.2, 2.6, 1.5])
3976         Coordinates:
3977           * y         (y) float64 1.0 1.5 2.0 2.5
3978             quantile  float64 0.0
3979         >>> da.quantile([0, 0.5, 1])
3980         <xarray.DataArray (quantile: 3)>
3981         array([0.7, 3.4, 9.4])
3982         Coordinates:
3983           * quantile  (quantile) float64 0.0 0.5 1.0
3984         >>> da.quantile([0, 0.5, 1], dim="x")
3985         <xarray.DataArray (quantile: 3, y: 4)>
3986         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3987                [3.6 , 5.75, 6.  , 1.7 ],
3988                [6.5 , 7.3 , 9.4 , 1.9 ]])
3989         Coordinates:
3990           * y         (y) float64 1.0 1.5 2.0 2.5
3991           * quantile  (quantile) float64 0.0 0.5 1.0
3992 
3993         References
3994         ----------
3995         .. [1] R. J. Hyndman and Y. Fan,
3996            "Sample quantiles in statistical packages,"
3997            The American Statistician, 50(4), pp. 361-365, 1996
3998         """
3999 
4000         ds = self._to_temp_dataset().quantile(
4001             q,
4002             dim=dim,
4003             keep_attrs=keep_attrs,
4004             method=method,
4005             skipna=skipna,
4006             interpolation=interpolation,
4007         )
4008         return self._from_temp_dataset(ds)
4009 
4010     def rank(
4011         self: T_DataArray,
4012         dim: Hashable,
4013         pct: bool = False,
4014         keep_attrs: bool | None = None,
4015     ) -> T_DataArray:
4016         """Ranks the data.
4017 
4018         Equal values are assigned a rank that is the average of the ranks that
4019         would have been otherwise assigned to all of the values within that
4020         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
4021 
4022         NaNs in the input array are returned as NaNs.
4023 
4024         The `bottleneck` library is required.
4025 
4026         Parameters
4027         ----------
4028         dim : Hashable
4029             Dimension over which to compute rank.
4030         pct : bool, default: False
4031             If True, compute percentage ranks, otherwise compute integer ranks.
4032         keep_attrs : bool or None, optional
4033             If True, the dataset's attributes (`attrs`) will be copied from
4034             the original object to the new one.  If False (default), the new
4035             object will be returned without attributes.
4036 
4037         Returns
4038         -------
4039         ranked : DataArray
4040             DataArray with the same coordinates and dtype 'float64'.
4041 
4042         Examples
4043         --------
4044         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4045         >>> arr.rank("x")
4046         <xarray.DataArray (x: 3)>
4047         array([1., 2., 3.])
4048         Dimensions without coordinates: x
4049         """
4050 
4051         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
4052         return self._from_temp_dataset(ds)
4053 
4054     def differentiate(
4055         self: T_DataArray,
4056         coord: Hashable,
4057         edge_order: Literal[1, 2] = 1,
4058         datetime_unit: DatetimeUnitOptions = None,
4059     ) -> T_DataArray:
4060         """ Differentiate the array with the second order accurate central
4061         differences.
4062 
4063         .. note::
4064             This feature is limited to simple cartesian geometry, i.e. coord
4065             must be one dimensional.
4066 
4067         Parameters
4068         ----------
4069         coord : Hashable
4070             The coordinate to be used to compute the gradient.
4071         edge_order : {1, 2}, default: 1
4072             N-th order accurate differences at the boundaries.
4073         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
4074                          "us", "ns", "ps", "fs", "as", None}, optional
4075             Unit to compute gradient. Only valid for datetime coordinate.
4076 
4077         Returns
4078         -------
4079         differentiated: DataArray
4080 
4081         See also
4082         --------
4083         numpy.gradient: corresponding numpy function
4084 
4085         Examples
4086         --------
4087 
4088         >>> da = xr.DataArray(
4089         ...     np.arange(12).reshape(4, 3),
4090         ...     dims=["x", "y"],
4091         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4092         ... )
4093         >>> da
4094         <xarray.DataArray (x: 4, y: 3)>
4095         array([[ 0,  1,  2],
4096                [ 3,  4,  5],
4097                [ 6,  7,  8],
4098                [ 9, 10, 11]])
4099         Coordinates:
4100           * x        (x) float64 0.0 0.1 1.1 1.2
4101         Dimensions without coordinates: y
4102         >>>
4103         >>> da.differentiate("x")
4104         <xarray.DataArray (x: 4, y: 3)>
4105         array([[30.        , 30.        , 30.        ],
4106                [27.54545455, 27.54545455, 27.54545455],
4107                [27.54545455, 27.54545455, 27.54545455],
4108                [30.        , 30.        , 30.        ]])
4109         Coordinates:
4110           * x        (x) float64 0.0 0.1 1.1 1.2
4111         Dimensions without coordinates: y
4112         """
4113         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
4114         return self._from_temp_dataset(ds)
4115 
4116     # change type of self and return to T_DataArray once
4117     # https://github.com/python/mypy/issues/12846 is resolved
4118     def integrate(
4119         self,
4120         coord: Hashable | Sequence[Hashable] = None,
4121         datetime_unit: DatetimeUnitOptions = None,
4122     ) -> DataArray:
4123         """Integrate along the given coordinate using the trapezoidal rule.
4124 
4125         .. note::
4126             This feature is limited to simple cartesian geometry, i.e. coord
4127             must be one dimensional.
4128 
4129         Parameters
4130         ----------
4131         coord : Hashable, or sequence of Hashable
4132             Coordinate(s) used for the integration.
4133         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4134                         'ps', 'fs', 'as', None}, optional
4135             Specify the unit if a datetime coordinate is used.
4136 
4137         Returns
4138         -------
4139         integrated : DataArray
4140 
4141         See also
4142         --------
4143         Dataset.integrate
4144         numpy.trapz : corresponding numpy function
4145 
4146         Examples
4147         --------
4148 
4149         >>> da = xr.DataArray(
4150         ...     np.arange(12).reshape(4, 3),
4151         ...     dims=["x", "y"],
4152         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4153         ... )
4154         >>> da
4155         <xarray.DataArray (x: 4, y: 3)>
4156         array([[ 0,  1,  2],
4157                [ 3,  4,  5],
4158                [ 6,  7,  8],
4159                [ 9, 10, 11]])
4160         Coordinates:
4161           * x        (x) float64 0.0 0.1 1.1 1.2
4162         Dimensions without coordinates: y
4163         >>>
4164         >>> da.integrate("x")
4165         <xarray.DataArray (y: 3)>
4166         array([5.4, 6.6, 7.8])
4167         Dimensions without coordinates: y
4168         """
4169         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
4170         return self._from_temp_dataset(ds)
4171 
4172     # change type of self and return to T_DataArray once
4173     # https://github.com/python/mypy/issues/12846 is resolved
4174     def cumulative_integrate(
4175         self,
4176         coord: Hashable | Sequence[Hashable] = None,
4177         datetime_unit: DatetimeUnitOptions = None,
4178     ) -> DataArray:
4179         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
4180 
4181         .. note::
4182             This feature is limited to simple cartesian geometry, i.e. coord
4183             must be one dimensional.
4184 
4185             The first entry of the cumulative integral is always 0, in order to keep the
4186             length of the dimension unchanged between input and output.
4187 
4188         Parameters
4189         ----------
4190         coord : Hashable, or sequence of Hashable
4191             Coordinate(s) used for the integration.
4192         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4193                         'ps', 'fs', 'as', None}, optional
4194             Specify the unit if a datetime coordinate is used.
4195 
4196         Returns
4197         -------
4198         integrated : DataArray
4199 
4200         See also
4201         --------
4202         Dataset.cumulative_integrate
4203         scipy.integrate.cumulative_trapezoid : corresponding scipy function
4204 
4205         Examples
4206         --------
4207 
4208         >>> da = xr.DataArray(
4209         ...     np.arange(12).reshape(4, 3),
4210         ...     dims=["x", "y"],
4211         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4212         ... )
4213         >>> da
4214         <xarray.DataArray (x: 4, y: 3)>
4215         array([[ 0,  1,  2],
4216                [ 3,  4,  5],
4217                [ 6,  7,  8],
4218                [ 9, 10, 11]])
4219         Coordinates:
4220           * x        (x) float64 0.0 0.1 1.1 1.2
4221         Dimensions without coordinates: y
4222         >>>
4223         >>> da.cumulative_integrate("x")
4224         <xarray.DataArray (x: 4, y: 3)>
4225         array([[0.  , 0.  , 0.  ],
4226                [0.15, 0.25, 0.35],
4227                [4.65, 5.75, 6.85],
4228                [5.4 , 6.6 , 7.8 ]])
4229         Coordinates:
4230           * x        (x) float64 0.0 0.1 1.1 1.2
4231         Dimensions without coordinates: y
4232         """
4233         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
4234         return self._from_temp_dataset(ds)
4235 
4236     def unify_chunks(self) -> DataArray:
4237         """Unify chunk size along all chunked dimensions of this DataArray.
4238 
4239         Returns
4240         -------
4241         DataArray with consistent chunk sizes for all dask-array variables
4242 
4243         See Also
4244         --------
4245         dask.array.core.unify_chunks
4246         """
4247 
4248         return unify_chunks(self)[0]
4249 
4250     def map_blocks(
4251         self,
4252         func: Callable[..., T_Xarray],
4253         args: Sequence[Any] = (),
4254         kwargs: Mapping[str, Any] | None = None,
4255         template: DataArray | Dataset | None = None,
4256     ) -> T_Xarray:
4257         """
4258         Apply a function to each block of this DataArray.
4259 
4260         .. warning::
4261             This method is experimental and its signature may change.
4262 
4263         Parameters
4264         ----------
4265         func : callable
4266             User-provided function that accepts a DataArray as its first
4267             parameter. The function will receive a subset or 'block' of this DataArray (see below),
4268             corresponding to one chunk along each chunked dimension. ``func`` will be
4269             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
4270 
4271             This function must return either a single DataArray or a single Dataset.
4272 
4273             This function cannot add a new chunked dimension.
4274         args : sequence
4275             Passed to func after unpacking and subsetting any xarray objects by blocks.
4276             xarray objects in args must be aligned with this object, otherwise an error is raised.
4277         kwargs : mapping
4278             Passed verbatim to func after unpacking. xarray objects, if any, will not be
4279             subset to blocks. Passing dask collections in kwargs is not allowed.
4280         template : DataArray or Dataset, optional
4281             xarray object representing the final result after compute is called. If not provided,
4282             the function will be first run on mocked-up data, that looks like this object but
4283             has sizes 0, to determine properties of the returned object such as dtype,
4284             variable names, attributes, new dimensions and new indexes (if any).
4285             ``template`` must be provided if the function changes the size of existing dimensions.
4286             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
4287             ``attrs`` set by ``func`` will be ignored.
4288 
4289         Returns
4290         -------
4291         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
4292         function.
4293 
4294         Notes
4295         -----
4296         This function is designed for when ``func`` needs to manipulate a whole xarray object
4297         subset to each block. Each block is loaded into memory. In the more common case where
4298         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
4299 
4300         If none of the variables in this object is backed by dask arrays, calling this function is
4301         equivalent to calling ``func(obj, *args, **kwargs)``.
4302 
4303         See Also
4304         --------
4305         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
4306         xarray.DataArray.map_blocks
4307 
4308         Examples
4309         --------
4310         Calculate an anomaly from climatology using ``.groupby()``. Using
4311         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
4312         its indices, and its methods like ``.groupby()``.
4313 
4314         >>> def calculate_anomaly(da, groupby_type="time.month"):
4315         ...     gb = da.groupby(groupby_type)
4316         ...     clim = gb.mean(dim="time")
4317         ...     return gb - clim
4318         ...
4319         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
4320         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
4321         >>> np.random.seed(123)
4322         >>> array = xr.DataArray(
4323         ...     np.random.rand(len(time)),
4324         ...     dims=["time"],
4325         ...     coords={"time": time, "month": month},
4326         ... ).chunk()
4327         >>> array.map_blocks(calculate_anomaly, template=array).compute()
4328         <xarray.DataArray (time: 24)>
4329         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
4330                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
4331                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
4332                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
4333                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
4334         Coordinates:
4335           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4336             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
4337 
4338         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
4339         to the function being applied in ``xr.map_blocks()``:
4340 
4341         >>> array.map_blocks(
4342         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
4343         ... )  # doctest: +ELLIPSIS
4344         <xarray.DataArray (time: 24)>
4345         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
4346         Coordinates:
4347           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4348             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
4349         """
4350         from .parallel import map_blocks
4351 
4352         return map_blocks(func, self, args, kwargs, template)
4353 
4354     def polyfit(
4355         self,
4356         dim: Hashable,
4357         deg: int,
4358         skipna: bool | None = None,
4359         rcond: float | None = None,
4360         w: Hashable | Any | None = None,
4361         full: bool = False,
4362         cov: bool | Literal["unscaled"] = False,
4363     ) -> Dataset:
4364         """
4365         Least squares polynomial fit.
4366 
4367         This replicates the behaviour of `numpy.polyfit` but differs by skipping
4368         invalid values when `skipna = True`.
4369 
4370         Parameters
4371         ----------
4372         dim : Hashable
4373             Coordinate along which to fit the polynomials.
4374         deg : int
4375             Degree of the fitting polynomial.
4376         skipna : bool or None, optional
4377             If True, removes all invalid values before fitting each 1D slices of the array.
4378             Default is True if data is stored in a dask.array or if there is any
4379             invalid values, False otherwise.
4380         rcond : float or None, optional
4381             Relative condition number to the fit.
4382         w : Hashable, array-like or None, optional
4383             Weights to apply to the y-coordinate of the sample points.
4384             Can be an array-like object or the name of a coordinate in the dataset.
4385         full : bool, default: False
4386             Whether to return the residuals, matrix rank and singular values in addition
4387             to the coefficients.
4388         cov : bool or "unscaled", default: False
4389             Whether to return to the covariance matrix in addition to the coefficients.
4390             The matrix is not scaled if `cov='unscaled'`.
4391 
4392         Returns
4393         -------
4394         polyfit_results : Dataset
4395             A single dataset which contains:
4396 
4397             polyfit_coefficients
4398                 The coefficients of the best fit.
4399             polyfit_residuals
4400                 The residuals of the least-square computation (only included if `full=True`).
4401                 When the matrix rank is deficient, np.nan is returned.
4402             [dim]_matrix_rank
4403                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4404             [dim]_singular_value
4405                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4406             polyfit_covariance
4407                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
4408 
4409         See Also
4410         --------
4411         numpy.polyfit
4412         numpy.polyval
4413         xarray.polyval
4414         """
4415         return self._to_temp_dataset().polyfit(
4416             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
4417         )
4418 
4419     def pad(
4420         self: T_DataArray,
4421         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
4422         mode: PadModeOptions = "constant",
4423         stat_length: int
4424         | tuple[int, int]
4425         | Mapping[Any, tuple[int, int]]
4426         | None = None,
4427         constant_values: float
4428         | tuple[float, float]
4429         | Mapping[Any, tuple[float, float]]
4430         | None = None,
4431         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
4432         reflect_type: PadReflectOptions = None,
4433         **pad_width_kwargs: Any,
4434     ) -> T_DataArray:
4435         """Pad this array along one or more dimensions.
4436 
4437         .. warning::
4438             This function is experimental and its behaviour is likely to change
4439             especially regarding padding of dimension coordinates (or IndexVariables).
4440 
4441         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
4442         coordinates will be padded with the same mode, otherwise coordinates
4443         are padded using the "constant" mode with fill_value dtypes.NA.
4444 
4445         Parameters
4446         ----------
4447         pad_width : mapping of Hashable to tuple of int
4448             Mapping with the form of {dim: (pad_before, pad_after)}
4449             describing the number of values padded along each dimension.
4450             {dim: pad} is a shortcut for pad_before = pad_after = pad
4451         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
4452             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
4453             How to pad the DataArray (taken from numpy docs):
4454 
4455             - "constant": Pads with a constant value.
4456             - "edge": Pads with the edge values of array.
4457             - "linear_ramp": Pads with the linear ramp between end_value and the
4458               array edge value.
4459             - "maximum": Pads with the maximum value of all or part of the
4460               vector along each axis.
4461             - "mean": Pads with the mean value of all or part of the
4462               vector along each axis.
4463             - "median": Pads with the median value of all or part of the
4464               vector along each axis.
4465             - "minimum": Pads with the minimum value of all or part of the
4466               vector along each axis.
4467             - "reflect": Pads with the reflection of the vector mirrored on
4468               the first and last values of the vector along each axis.
4469             - "symmetric": Pads with the reflection of the vector mirrored
4470               along the edge of the array.
4471             - "wrap": Pads with the wrap of the vector along the axis.
4472               The first values are used to pad the end and the
4473               end values are used to pad the beginning.
4474 
4475         stat_length : int, tuple or mapping of Hashable to tuple, default: None
4476             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
4477             values at edge of each axis used to calculate the statistic value.
4478             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
4479             statistic lengths along each dimension.
4480             ((before, after),) yields same before and after statistic lengths
4481             for each dimension.
4482             (stat_length,) or int is a shortcut for before = after = statistic
4483             length for all axes.
4484             Default is ``None``, to use the entire axis.
4485         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4486             Used in 'constant'.  The values to set the padded values for each
4487             axis.
4488             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4489             pad constants along each dimension.
4490             ``((before, after),)`` yields same before and after constants for each
4491             dimension.
4492             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4493             all dimensions.
4494             Default is 0.
4495         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4496             Used in 'linear_ramp'.  The values used for the ending value of the
4497             linear_ramp and that will form the edge of the padded array.
4498             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4499             end values along each dimension.
4500             ``((before, after),)`` yields same before and after end values for each
4501             axis.
4502             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4503             all axes.
4504             Default is 0.
4505         reflect_type : {"even", "odd", None}, optional
4506             Used in "reflect", and "symmetric". The "even" style is the
4507             default with an unaltered reflection around the edge value. For
4508             the "odd" style, the extended part of the array is created by
4509             subtracting the reflected values from two times the edge value.
4510         **pad_width_kwargs
4511             The keyword arguments form of ``pad_width``.
4512             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
4513 
4514         Returns
4515         -------
4516         padded : DataArray
4517             DataArray with the padded coordinates and data.
4518 
4519         See Also
4520         --------
4521         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
4522 
4523         Notes
4524         -----
4525         For ``mode="constant"`` and ``constant_values=None``, integer types will be
4526         promoted to ``float`` and padded with ``np.nan``.
4527 
4528         Padding coordinates will drop their corresponding index (if any) and will reset default
4529         indexes for dimension coordinates.
4530 
4531         Examples
4532         --------
4533         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
4534         >>> arr.pad(x=(1, 2), constant_values=0)
4535         <xarray.DataArray (x: 6)>
4536         array([0, 5, 6, 7, 0, 0])
4537         Coordinates:
4538           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
4539 
4540         >>> da = xr.DataArray(
4541         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
4542         ...     dims=["x", "y"],
4543         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
4544         ... )
4545         >>> da.pad(x=1)
4546         <xarray.DataArray (x: 4, y: 4)>
4547         array([[nan, nan, nan, nan],
4548                [ 0.,  1.,  2.,  3.],
4549                [10., 11., 12., 13.],
4550                [nan, nan, nan, nan]])
4551         Coordinates:
4552           * x        (x) float64 nan 0.0 1.0 nan
4553           * y        (y) int64 10 20 30 40
4554             z        (x) float64 nan 100.0 200.0 nan
4555 
4556         Careful, ``constant_values`` are coerced to the data type of the array which may
4557         lead to a loss of precision:
4558 
4559         >>> da.pad(x=1, constant_values=1.23456789)
4560         <xarray.DataArray (x: 4, y: 4)>
4561         array([[ 1,  1,  1,  1],
4562                [ 0,  1,  2,  3],
4563                [10, 11, 12, 13],
4564                [ 1,  1,  1,  1]])
4565         Coordinates:
4566           * x        (x) float64 nan 0.0 1.0 nan
4567           * y        (y) int64 10 20 30 40
4568             z        (x) float64 nan 100.0 200.0 nan
4569         """
4570         ds = self._to_temp_dataset().pad(
4571             pad_width=pad_width,
4572             mode=mode,
4573             stat_length=stat_length,
4574             constant_values=constant_values,
4575             end_values=end_values,
4576             reflect_type=reflect_type,
4577             **pad_width_kwargs,
4578         )
4579         return self._from_temp_dataset(ds)
4580 
4581     def idxmin(
4582         self,
4583         dim: Hashable | None = None,
4584         skipna: bool | None = None,
4585         fill_value: Any = dtypes.NA,
4586         keep_attrs: bool | None = None,
4587     ) -> DataArray:
4588         """Return the coordinate label of the minimum value along a dimension.
4589 
4590         Returns a new `DataArray` named after the dimension with the values of
4591         the coordinate labels along that dimension corresponding to minimum
4592         values along that dimension.
4593 
4594         In comparison to :py:meth:`~DataArray.argmin`, this returns the
4595         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
4596 
4597         Parameters
4598         ----------
4599         dim : str, optional
4600             Dimension over which to apply `idxmin`.  This is optional for 1D
4601             arrays, but required for arrays with 2 or more dimensions.
4602         skipna : bool or None, default: None
4603             If True, skip missing values (as marked by NaN). By default, only
4604             skips missing values for ``float``, ``complex``, and ``object``
4605             dtypes; other dtypes either do not have a sentinel missing value
4606             (``int``) or ``skipna=True`` has not been implemented
4607             (``datetime64`` or ``timedelta64``).
4608         fill_value : Any, default: NaN
4609             Value to be filled in case all of the values along a dimension are
4610             null.  By default this is NaN.  The fill value and result are
4611             automatically converted to a compatible dtype if possible.
4612             Ignored if ``skipna`` is False.
4613         keep_attrs : bool or None, optional
4614             If True, the attributes (``attrs``) will be copied from the
4615             original object to the new one. If False, the new object
4616             will be returned without attributes.
4617 
4618         Returns
4619         -------
4620         reduced : DataArray
4621             New `DataArray` object with `idxmin` applied to its data and the
4622             indicated dimension removed.
4623 
4624         See Also
4625         --------
4626         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
4627 
4628         Examples
4629         --------
4630         >>> array = xr.DataArray(
4631         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4632         ... )
4633         >>> array.min()
4634         <xarray.DataArray ()>
4635         array(-2)
4636         >>> array.argmin()
4637         <xarray.DataArray ()>
4638         array(4)
4639         >>> array.idxmin()
4640         <xarray.DataArray 'x' ()>
4641         array('e', dtype='<U1')
4642 
4643         >>> array = xr.DataArray(
4644         ...     [
4645         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4646         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4647         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4648         ...     ],
4649         ...     dims=["y", "x"],
4650         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4651         ... )
4652         >>> array.min(dim="x")
4653         <xarray.DataArray (y: 3)>
4654         array([-2., -4.,  1.])
4655         Coordinates:
4656           * y        (y) int64 -1 0 1
4657         >>> array.argmin(dim="x")
4658         <xarray.DataArray (y: 3)>
4659         array([4, 0, 2])
4660         Coordinates:
4661           * y        (y) int64 -1 0 1
4662         >>> array.idxmin(dim="x")
4663         <xarray.DataArray 'x' (y: 3)>
4664         array([16.,  0.,  4.])
4665         Coordinates:
4666           * y        (y) int64 -1 0 1
4667         """
4668         return computation._calc_idxminmax(
4669             array=self,
4670             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4671             dim=dim,
4672             skipna=skipna,
4673             fill_value=fill_value,
4674             keep_attrs=keep_attrs,
4675         )
4676 
4677     def idxmax(
4678         self,
4679         dim: Hashable = None,
4680         skipna: bool | None = None,
4681         fill_value: Any = dtypes.NA,
4682         keep_attrs: bool | None = None,
4683     ) -> DataArray:
4684         """Return the coordinate label of the maximum value along a dimension.
4685 
4686         Returns a new `DataArray` named after the dimension with the values of
4687         the coordinate labels along that dimension corresponding to maximum
4688         values along that dimension.
4689 
4690         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4691         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4692 
4693         Parameters
4694         ----------
4695         dim : Hashable, optional
4696             Dimension over which to apply `idxmax`.  This is optional for 1D
4697             arrays, but required for arrays with 2 or more dimensions.
4698         skipna : bool or None, default: None
4699             If True, skip missing values (as marked by NaN). By default, only
4700             skips missing values for ``float``, ``complex``, and ``object``
4701             dtypes; other dtypes either do not have a sentinel missing value
4702             (``int``) or ``skipna=True`` has not been implemented
4703             (``datetime64`` or ``timedelta64``).
4704         fill_value : Any, default: NaN
4705             Value to be filled in case all of the values along a dimension are
4706             null.  By default this is NaN.  The fill value and result are
4707             automatically converted to a compatible dtype if possible.
4708             Ignored if ``skipna`` is False.
4709         keep_attrs : bool or None, optional
4710             If True, the attributes (``attrs``) will be copied from the
4711             original object to the new one. If False, the new object
4712             will be returned without attributes.
4713 
4714         Returns
4715         -------
4716         reduced : DataArray
4717             New `DataArray` object with `idxmax` applied to its data and the
4718             indicated dimension removed.
4719 
4720         See Also
4721         --------
4722         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4723 
4724         Examples
4725         --------
4726         >>> array = xr.DataArray(
4727         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4728         ... )
4729         >>> array.max()
4730         <xarray.DataArray ()>
4731         array(2)
4732         >>> array.argmax()
4733         <xarray.DataArray ()>
4734         array(1)
4735         >>> array.idxmax()
4736         <xarray.DataArray 'x' ()>
4737         array('b', dtype='<U1')
4738 
4739         >>> array = xr.DataArray(
4740         ...     [
4741         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4742         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4743         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4744         ...     ],
4745         ...     dims=["y", "x"],
4746         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4747         ... )
4748         >>> array.max(dim="x")
4749         <xarray.DataArray (y: 3)>
4750         array([2., 2., 1.])
4751         Coordinates:
4752           * y        (y) int64 -1 0 1
4753         >>> array.argmax(dim="x")
4754         <xarray.DataArray (y: 3)>
4755         array([0, 2, 2])
4756         Coordinates:
4757           * y        (y) int64 -1 0 1
4758         >>> array.idxmax(dim="x")
4759         <xarray.DataArray 'x' (y: 3)>
4760         array([0., 4., 4.])
4761         Coordinates:
4762           * y        (y) int64 -1 0 1
4763         """
4764         return computation._calc_idxminmax(
4765             array=self,
4766             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4767             dim=dim,
4768             skipna=skipna,
4769             fill_value=fill_value,
4770             keep_attrs=keep_attrs,
4771         )
4772 
4773     # change type of self and return to T_DataArray once
4774     # https://github.com/python/mypy/issues/12846 is resolved
4775     def argmin(
4776         self,
4777         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4778         axis: int | None = None,
4779         keep_attrs: bool | None = None,
4780         skipna: bool | None = None,
4781     ) -> DataArray | dict[Hashable, DataArray]:
4782         """Index or indices of the minimum of the DataArray over one or more dimensions.
4783 
4784         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4785         which can be passed directly to isel(). If a single str is passed to 'dim' then
4786         returns a DataArray with dtype int.
4787 
4788         If there are multiple minima, the indices of the first one found will be
4789         returned.
4790 
4791         Parameters
4792         ----------
4793         dim : Hashable, sequence of Hashable, None or ..., optional
4794             The dimensions over which to find the minimum. By default, finds minimum over
4795             all dimensions - for now returning an int for backward compatibility, but
4796             this is deprecated, in future will return a dict with indices for all
4797             dimensions; to return a dict with all dimensions now, pass '...'.
4798         axis : int or None, optional
4799             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4800             can be supplied.
4801         keep_attrs : bool or None, optional
4802             If True, the attributes (`attrs`) will be copied from the original
4803             object to the new one. If False, the new object will be
4804             returned without attributes.
4805         skipna : bool or None, optional
4806             If True, skip missing values (as marked by NaN). By default, only
4807             skips missing values for float dtypes; other dtypes either do not
4808             have a sentinel missing value (int) or skipna=True has not been
4809             implemented (object, datetime64 or timedelta64).
4810 
4811         Returns
4812         -------
4813         result : DataArray or dict of DataArray
4814 
4815         See Also
4816         --------
4817         Variable.argmin, DataArray.idxmin
4818 
4819         Examples
4820         --------
4821         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4822         >>> array.min()
4823         <xarray.DataArray ()>
4824         array(-1)
4825         >>> array.argmin()
4826         <xarray.DataArray ()>
4827         array(2)
4828         >>> array.argmin(...)
4829         {'x': <xarray.DataArray ()>
4830         array(2)}
4831         >>> array.isel(array.argmin(...))
4832         <xarray.DataArray ()>
4833         array(-1)
4834 
4835         >>> array = xr.DataArray(
4836         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4837         ...     dims=("x", "y", "z"),
4838         ... )
4839         >>> array.min(dim="x")
4840         <xarray.DataArray (y: 3, z: 3)>
4841         array([[ 1,  2,  1],
4842                [ 2, -5,  1],
4843                [ 2,  1,  1]])
4844         Dimensions without coordinates: y, z
4845         >>> array.argmin(dim="x")
4846         <xarray.DataArray (y: 3, z: 3)>
4847         array([[1, 0, 0],
4848                [1, 1, 1],
4849                [0, 0, 1]])
4850         Dimensions without coordinates: y, z
4851         >>> array.argmin(dim=["x"])
4852         {'x': <xarray.DataArray (y: 3, z: 3)>
4853         array([[1, 0, 0],
4854                [1, 1, 1],
4855                [0, 0, 1]])
4856         Dimensions without coordinates: y, z}
4857         >>> array.min(dim=("x", "z"))
4858         <xarray.DataArray (y: 3)>
4859         array([ 1, -5,  1])
4860         Dimensions without coordinates: y
4861         >>> array.argmin(dim=["x", "z"])
4862         {'x': <xarray.DataArray (y: 3)>
4863         array([0, 1, 0])
4864         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4865         array([2, 1, 1])
4866         Dimensions without coordinates: y}
4867         >>> array.isel(array.argmin(dim=["x", "z"]))
4868         <xarray.DataArray (y: 3)>
4869         array([ 1, -5,  1])
4870         Dimensions without coordinates: y
4871         """
4872         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4873         if isinstance(result, dict):
4874             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4875         else:
4876             return self._replace_maybe_drop_dims(result)
4877 
4878     # change type of self and return to T_DataArray once
4879     # https://github.com/python/mypy/issues/12846 is resolved
4880     def argmax(
4881         self,
4882         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4883         axis: int | None = None,
4884         keep_attrs: bool | None = None,
4885         skipna: bool | None = None,
4886     ) -> DataArray | dict[Hashable, DataArray]:
4887         """Index or indices of the maximum of the DataArray over one or more dimensions.
4888 
4889         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4890         which can be passed directly to isel(). If a single str is passed to 'dim' then
4891         returns a DataArray with dtype int.
4892 
4893         If there are multiple maxima, the indices of the first one found will be
4894         returned.
4895 
4896         Parameters
4897         ----------
4898         dim : Hashable, sequence of Hashable, None or ..., optional
4899             The dimensions over which to find the maximum. By default, finds maximum over
4900             all dimensions - for now returning an int for backward compatibility, but
4901             this is deprecated, in future will return a dict with indices for all
4902             dimensions; to return a dict with all dimensions now, pass '...'.
4903         axis : int or None, optional
4904             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4905             can be supplied.
4906         keep_attrs : bool or None, optional
4907             If True, the attributes (`attrs`) will be copied from the original
4908             object to the new one. If False, the new object will be
4909             returned without attributes.
4910         skipna : bool or None, optional
4911             If True, skip missing values (as marked by NaN). By default, only
4912             skips missing values for float dtypes; other dtypes either do not
4913             have a sentinel missing value (int) or skipna=True has not been
4914             implemented (object, datetime64 or timedelta64).
4915 
4916         Returns
4917         -------
4918         result : DataArray or dict of DataArray
4919 
4920         See Also
4921         --------
4922         Variable.argmax, DataArray.idxmax
4923 
4924         Examples
4925         --------
4926         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4927         >>> array.max()
4928         <xarray.DataArray ()>
4929         array(3)
4930         >>> array.argmax()
4931         <xarray.DataArray ()>
4932         array(3)
4933         >>> array.argmax(...)
4934         {'x': <xarray.DataArray ()>
4935         array(3)}
4936         >>> array.isel(array.argmax(...))
4937         <xarray.DataArray ()>
4938         array(3)
4939 
4940         >>> array = xr.DataArray(
4941         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4942         ...     dims=("x", "y", "z"),
4943         ... )
4944         >>> array.max(dim="x")
4945         <xarray.DataArray (y: 3, z: 3)>
4946         array([[3, 3, 2],
4947                [3, 5, 2],
4948                [2, 3, 3]])
4949         Dimensions without coordinates: y, z
4950         >>> array.argmax(dim="x")
4951         <xarray.DataArray (y: 3, z: 3)>
4952         array([[0, 1, 1],
4953                [0, 1, 0],
4954                [0, 1, 0]])
4955         Dimensions without coordinates: y, z
4956         >>> array.argmax(dim=["x"])
4957         {'x': <xarray.DataArray (y: 3, z: 3)>
4958         array([[0, 1, 1],
4959                [0, 1, 0],
4960                [0, 1, 0]])
4961         Dimensions without coordinates: y, z}
4962         >>> array.max(dim=("x", "z"))
4963         <xarray.DataArray (y: 3)>
4964         array([3, 5, 3])
4965         Dimensions without coordinates: y
4966         >>> array.argmax(dim=["x", "z"])
4967         {'x': <xarray.DataArray (y: 3)>
4968         array([0, 1, 0])
4969         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4970         array([0, 1, 2])
4971         Dimensions without coordinates: y}
4972         >>> array.isel(array.argmax(dim=["x", "z"]))
4973         <xarray.DataArray (y: 3)>
4974         array([3, 5, 3])
4975         Dimensions without coordinates: y
4976         """
4977         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4978         if isinstance(result, dict):
4979             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4980         else:
4981             return self._replace_maybe_drop_dims(result)
4982 
4983     def query(
4984         self,
4985         queries: Mapping[Any, Any] | None = None,
4986         parser: QueryParserOptions = "pandas",
4987         engine: QueryEngineOptions = None,
4988         missing_dims: ErrorOptionsWithWarn = "raise",
4989         **queries_kwargs: Any,
4990     ) -> DataArray:
4991         """Return a new data array indexed along the specified
4992         dimension(s), where the indexers are given as strings containing
4993         Python expressions to be evaluated against the values in the array.
4994 
4995         Parameters
4996         ----------
4997         queries : dict-like or None, optional
4998             A dict-like with keys matching dimensions and values given by strings
4999             containing Python expressions to be evaluated against the data variables
5000             in the dataset. The expressions will be evaluated using the pandas
5001             eval() function, and can contain any valid Python expressions but cannot
5002             contain any Python statements.
5003         parser : {"pandas", "python"}, default: "pandas"
5004             The parser to use to construct the syntax tree from the expression.
5005             The default of 'pandas' parses code slightly different than standard
5006             Python. Alternatively, you can parse an expression using the 'python'
5007             parser to retain strict Python semantics.
5008         engine : {"python", "numexpr", None}, default: None
5009             The engine used to evaluate the expression. Supported engines are:
5010 
5011             - None: tries to use numexpr, falls back to python
5012             - "numexpr": evaluates expressions using numexpr
5013             - "python": performs operations as if you had eval’d in top level python
5014 
5015         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5016             What to do if dimensions that should be selected from are not present in the
5017             DataArray:
5018 
5019             - "raise": raise an exception
5020             - "warn": raise a warning, and ignore the missing dimensions
5021             - "ignore": ignore the missing dimensions
5022 
5023         **queries_kwargs : {dim: query, ...}, optional
5024             The keyword arguments form of ``queries``.
5025             One of queries or queries_kwargs must be provided.
5026 
5027         Returns
5028         -------
5029         obj : DataArray
5030             A new DataArray with the same contents as this dataset, indexed by
5031             the results of the appropriate queries.
5032 
5033         See Also
5034         --------
5035         DataArray.isel
5036         Dataset.query
5037         pandas.eval
5038 
5039         Examples
5040         --------
5041         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
5042         >>> da
5043         <xarray.DataArray 'a' (x: 5)>
5044         array([0, 1, 2, 3, 4])
5045         Dimensions without coordinates: x
5046         >>> da.query(x="a > 2")
5047         <xarray.DataArray 'a' (x: 2)>
5048         array([3, 4])
5049         Dimensions without coordinates: x
5050         """
5051 
5052         ds = self._to_dataset_whole(shallow_copy=True)
5053         ds = ds.query(
5054             queries=queries,
5055             parser=parser,
5056             engine=engine,
5057             missing_dims=missing_dims,
5058             **queries_kwargs,
5059         )
5060         return ds[self.name]
5061 
5062     def curvefit(
5063         self,
5064         coords: str | DataArray | Iterable[str | DataArray],
5065         func: Callable[..., Any],
5066         reduce_dims: Hashable | Iterable[Hashable] | None = None,
5067         skipna: bool = True,
5068         p0: dict[str, Any] | None = None,
5069         bounds: dict[str, Any] | None = None,
5070         param_names: Sequence[str] | None = None,
5071         kwargs: dict[str, Any] | None = None,
5072     ) -> Dataset:
5073         """
5074         Curve fitting optimization for arbitrary functions.
5075 
5076         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
5077 
5078         Parameters
5079         ----------
5080         coords : Hashable, DataArray, or sequence of DataArray or Hashable
5081             Independent coordinate(s) over which to perform the curve fitting. Must share
5082             at least one dimension with the calling object. When fitting multi-dimensional
5083             functions, supply `coords` as a sequence in the same order as arguments in
5084             `func`. To fit along existing dimensions of the calling object, `coords` can
5085             also be specified as a str or sequence of strs.
5086         func : callable
5087             User specified function in the form `f(x, *params)` which returns a numpy
5088             array of length `len(x)`. `params` are the fittable parameters which are optimized
5089             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
5090             coordinates, e.g. `f((x0, x1), *params)`.
5091         reduce_dims : Hashable or sequence of Hashable
5092             Additional dimension(s) over which to aggregate while fitting. For example,
5093             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
5094             aggregate all lat and lon points and fit the specified function along the
5095             time dimension.
5096         skipna : bool, default: True
5097             Whether to skip missing values when fitting. Default is True.
5098         p0 : dict-like or None, optional
5099             Optional dictionary of parameter names to initial guesses passed to the
5100             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
5101             be assigned initial values following the default scipy behavior.
5102         bounds : dict-like or None, optional
5103             Optional dictionary of parameter names to bounding values passed to the
5104             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
5105             will be unbounded following the default scipy behavior.
5106         param_names : sequence of Hashable or None, optional
5107             Sequence of names for the fittable parameters of `func`. If not supplied,
5108             this will be automatically determined by arguments of `func`. `param_names`
5109             should be manually supplied when fitting a function that takes a variable
5110             number of parameters.
5111         **kwargs : optional
5112             Additional keyword arguments to passed to scipy curve_fit.
5113 
5114         Returns
5115         -------
5116         curvefit_results : Dataset
5117             A single dataset which contains:
5118 
5119             [var]_curvefit_coefficients
5120                 The coefficients of the best fit.
5121             [var]_curvefit_covariance
5122                 The covariance matrix of the coefficient estimates.
5123 
5124         See Also
5125         --------
5126         DataArray.polyfit
5127         scipy.optimize.curve_fit
5128         """
5129         return self._to_temp_dataset().curvefit(
5130             coords,
5131             func,
5132             reduce_dims=reduce_dims,
5133             skipna=skipna,
5134             p0=p0,
5135             bounds=bounds,
5136             param_names=param_names,
5137             kwargs=kwargs,
5138         )
5139 
5140     def drop_duplicates(
5141         self: T_DataArray,
5142         dim: Hashable | Iterable[Hashable],
5143         keep: Literal["first", "last", False] = "first",
5144     ) -> T_DataArray:
5145         """Returns a new DataArray with duplicate dimension values removed.
5146 
5147         Parameters
5148         ----------
5149         dim : dimension label or labels
5150             Pass `...` to drop duplicates along all dimensions.
5151         keep : {"first", "last", False}, default: "first"
5152             Determines which duplicates (if any) to keep.
5153 
5154             - ``"first"`` : Drop duplicates except for the first occurrence.
5155             - ``"last"`` : Drop duplicates except for the last occurrence.
5156             - False : Drop all duplicates.
5157 
5158         Returns
5159         -------
5160         DataArray
5161 
5162         See Also
5163         --------
5164         Dataset.drop_duplicates
5165         """
5166         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
5167         return self._from_temp_dataset(deduplicated)
5168 
5169     def convert_calendar(
5170         self,
5171         calendar: str,
5172         dim: str = "time",
5173         align_on: str | None = None,
5174         missing: Any | None = None,
5175         use_cftime: bool | None = None,
5176     ) -> DataArray:
5177         """Convert the DataArray to another calendar.
5178 
5179         Only converts the individual timestamps, does not modify any data except
5180         in dropping invalid/surplus dates or inserting missing dates.
5181 
5182         If the source and target calendars are either no_leap, all_leap or a
5183         standard type, only the type of the time array is modified.
5184         When converting to a leap year from a non-leap year, the 29th of February
5185         is removed from the array. In the other direction the 29th of February
5186         will be missing in the output, unless `missing` is specified,
5187         in which case that value is inserted.
5188 
5189         For conversions involving `360_day` calendars, see Notes.
5190 
5191         This method is safe to use with sub-daily data as it doesn't touch the
5192         time part of the timestamps.
5193 
5194         Parameters
5195         ---------
5196         calendar : str
5197             The target calendar name.
5198         dim : str
5199             Name of the time coordinate.
5200         align_on : {None, 'date', 'year'}
5201             Must be specified when either source or target is a `360_day` calendar,
5202            ignored otherwise. See Notes.
5203         missing : Optional[any]
5204             By default, i.e. if the value is None, this method will simply attempt
5205             to convert the dates in the source calendar to the same dates in the
5206             target calendar, and drop any of those that are not possible to
5207             represent.  If a value is provided, a new time coordinate will be
5208             created in the target calendar with the same frequency as the original
5209             time coordinate; for any dates that are not present in the source, the
5210             data will be filled with this value.  Note that using this mode requires
5211             that the source data have an inferable frequency; for more information
5212             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
5213             target calendar combinations, this could result in many missing values, see notes.
5214         use_cftime : boolean, optional
5215             Whether to use cftime objects in the output, only used if `calendar`
5216             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
5217             If True, the new time axis uses cftime objects.
5218             If None (default), it uses :py:class:`numpy.datetime64` values if the
5219             date range permits it, and :py:class:`cftime.datetime` objects if not.
5220             If False, it uses :py:class:`numpy.datetime64`  or fails.
5221 
5222         Returns
5223         -------
5224         DataArray
5225             Copy of the dataarray with the time coordinate converted to the
5226             target calendar. If 'missing' was None (default), invalid dates in
5227             the new calendar are dropped, but missing dates are not inserted.
5228             If `missing` was given, the new data is reindexed to have a time axis
5229             with the same frequency as the source, but in the new calendar; any
5230             missing datapoints are filled with `missing`.
5231 
5232         Notes
5233         -----
5234         Passing a value to `missing` is only usable if the source's time coordinate as an
5235         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
5236         if the target coordinate, generated from this frequency, has dates equivalent to the
5237         source. It is usually **not** appropriate to use this mode with:
5238 
5239         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
5240         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
5241             or 'mH' where 24 % m != 0).
5242 
5243         If one of the source or target calendars is `"360_day"`, `align_on` must
5244         be specified and two options are offered.
5245 
5246         - "year"
5247             The dates are translated according to their relative position in the year,
5248             ignoring their original month and day information, meaning that the
5249             missing/surplus days are added/removed at regular intervals.
5250 
5251             From a `360_day` to a standard calendar, the output will be missing the
5252             following dates (day of year in parentheses):
5253 
5254             To a leap year:
5255                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
5256                 September 31st (275) and November 30th (335).
5257             To a non-leap year:
5258                 February 6th (36), April 19th (109), July 2nd (183),
5259                 September 12th (255), November 25th (329).
5260 
5261             From a standard calendar to a `"360_day"`, the following dates in the
5262             source array will be dropped:
5263 
5264             From a leap year:
5265                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
5266                 September 31st (275), December 1st (336)
5267             From a non-leap year:
5268                 February 6th (37), April 20th (110), July 2nd (183),
5269                 September 13th (256), November 25th (329)
5270 
5271             This option is best used on daily and subdaily data.
5272 
5273         - "date"
5274             The month/day information is conserved and invalid dates are dropped
5275             from the output. This means that when converting from a `"360_day"` to a
5276             standard calendar, all 31st (Jan, March, May, July, August, October and
5277             December) will be missing as there is no equivalent dates in the
5278             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
5279             will be dropped as there are no equivalent dates in a standard calendar.
5280 
5281             This option is best used with data on a frequency coarser than daily.
5282         """
5283         return convert_calendar(
5284             self,
5285             calendar,
5286             dim=dim,
5287             align_on=align_on,
5288             missing=missing,
5289             use_cftime=use_cftime,
5290         )
5291 
5292     def interp_calendar(
5293         self,
5294         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
5295         dim: str = "time",
5296     ) -> DataArray:
5297         """Interpolates the DataArray to another calendar based on decimal year measure.
5298 
5299         Each timestamp in `source` and `target` are first converted to their decimal
5300         year equivalent then `source` is interpolated on the target coordinate.
5301         The decimal year of a timestamp is its year plus its sub-year component
5302         converted to the fraction of its year. For example "2000-03-01 12:00" is
5303         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
5304 
5305         This method should only be used when the time (HH:MM:SS) information of
5306         time coordinate is not important.
5307 
5308         Parameters
5309         ----------
5310         target: DataArray or DatetimeIndex or CFTimeIndex
5311             The target time coordinate of a valid dtype
5312             (np.datetime64 or cftime objects)
5313         dim : str
5314             The time coordinate name.
5315 
5316         Return
5317         ------
5318         DataArray
5319             The source interpolated on the decimal years of target,
5320         """
5321         return interp_calendar(self, target, dim=dim)
5322 
5323     def groupby(
5324         self,
5325         group: Hashable | DataArray | IndexVariable,
5326         squeeze: bool = True,
5327         restore_coord_dims: bool = False,
5328     ) -> DataArrayGroupBy:
5329         """Returns a DataArrayGroupBy object for performing grouped operations.
5330 
5331         Parameters
5332         ----------
5333         group : Hashable, DataArray or IndexVariable
5334             Array whose unique values should be used to group this array. If a
5335             Hashable, must be the name of a coordinate contained in this dataarray.
5336         squeeze : bool, default: True
5337             If "group" is a dimension of any arrays in this dataset, `squeeze`
5338             controls whether the subarrays have a dimension of length 1 along
5339             that dimension or if the dimension is squeezed out.
5340         restore_coord_dims : bool, default: False
5341             If True, also restore the dimension order of multi-dimensional
5342             coordinates.
5343 
5344         Returns
5345         -------
5346         grouped : DataArrayGroupBy
5347             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5348             iterated over in the form of `(unique_value, grouped_array)` pairs.
5349 
5350         Examples
5351         --------
5352         Calculate daily anomalies for daily data:
5353 
5354         >>> da = xr.DataArray(
5355         ...     np.linspace(0, 1826, num=1827),
5356         ...     coords=[pd.date_range("1/1/2000", "31/12/2004", freq="D")],
5357         ...     dims="time",
5358         ... )
5359         >>> da
5360         <xarray.DataArray (time: 1827)>
5361         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
5362                1.826e+03])
5363         Coordinates:
5364           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5365         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
5366         <xarray.DataArray (time: 1827)>
5367         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
5368         Coordinates:
5369           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5370             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
5371 
5372         See Also
5373         --------
5374         DataArray.groupby_bins
5375         Dataset.groupby
5376         core.groupby.DataArrayGroupBy
5377         pandas.DataFrame.groupby
5378         """
5379         from .groupby import DataArrayGroupBy
5380 
5381         # While we don't generally check the type of every arg, passing
5382         # multiple dimensions as multiple arguments is common enough, and the
5383         # consequences hidden enough (strings evaluate as true) to warrant
5384         # checking here.
5385         # A future version could make squeeze kwarg only, but would face
5386         # backward-compat issues.
5387         if not isinstance(squeeze, bool):
5388             raise TypeError(
5389                 f"`squeeze` must be True or False, but {squeeze} was supplied"
5390             )
5391 
5392         return DataArrayGroupBy(
5393             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
5394         )
5395 
5396     def groupby_bins(
5397         self,
5398         group: Hashable | DataArray | IndexVariable,
5399         bins: ArrayLike,
5400         right: bool = True,
5401         labels: ArrayLike | Literal[False] | None = None,
5402         precision: int = 3,
5403         include_lowest: bool = False,
5404         squeeze: bool = True,
5405         restore_coord_dims: bool = False,
5406     ) -> DataArrayGroupBy:
5407         """Returns a DataArrayGroupBy object for performing grouped operations.
5408 
5409         Rather than using all unique values of `group`, the values are discretized
5410         first by applying `pandas.cut` [1]_ to `group`.
5411 
5412         Parameters
5413         ----------
5414         group : Hashable, DataArray or IndexVariable
5415             Array whose binned values should be used to group this array. If a
5416             Hashable, must be the name of a coordinate contained in this dataarray.
5417         bins : int or array-like
5418             If bins is an int, it defines the number of equal-width bins in the
5419             range of x. However, in this case, the range of x is extended by .1%
5420             on each side to include the min or max values of x. If bins is a
5421             sequence it defines the bin edges allowing for non-uniform bin
5422             width. No extension of the range of x is done in this case.
5423         right : bool, default: True
5424             Indicates whether the bins include the rightmost edge or not. If
5425             right == True (the default), then the bins [1,2,3,4] indicate
5426             (1,2], (2,3], (3,4].
5427         labels : array-like, False or None, default: None
5428             Used as labels for the resulting bins. Must be of the same length as
5429             the resulting bins. If False, string bin labels are assigned by
5430             `pandas.cut`.
5431         precision : int, default: 3
5432             The precision at which to store and display the bins labels.
5433         include_lowest : bool, default: False
5434             Whether the first interval should be left-inclusive or not.
5435         squeeze : bool, default: True
5436             If "group" is a dimension of any arrays in this dataset, `squeeze`
5437             controls whether the subarrays have a dimension of length 1 along
5438             that dimension or if the dimension is squeezed out.
5439         restore_coord_dims : bool, default: False
5440             If True, also restore the dimension order of multi-dimensional
5441             coordinates.
5442 
5443         Returns
5444         -------
5445         grouped : DataArrayGroupBy
5446             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5447             iterated over in the form of `(unique_value, grouped_array)` pairs.
5448             The name of the group has the added suffix `_bins` in order to
5449             distinguish it from the original variable.
5450 
5451         See Also
5452         --------
5453         DataArray.groupby
5454         Dataset.groupby_bins
5455         core.groupby.DataArrayGroupBy
5456         pandas.DataFrame.groupby
5457 
5458         References
5459         ----------
5460         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
5461         """
5462         from .groupby import DataArrayGroupBy
5463 
5464         return DataArrayGroupBy(
5465             self,
5466             group,
5467             squeeze=squeeze,
5468             bins=bins,
5469             restore_coord_dims=restore_coord_dims,
5470             cut_kwargs={
5471                 "right": right,
5472                 "labels": labels,
5473                 "precision": precision,
5474                 "include_lowest": include_lowest,
5475             },
5476         )
5477 
5478     def weighted(self, weights: DataArray) -> DataArrayWeighted:
5479         """
5480         Weighted DataArray operations.
5481 
5482         Parameters
5483         ----------
5484         weights : DataArray
5485             An array of weights associated with the values in this Dataset.
5486             Each value in the data contributes to the reduction operation
5487             according to its associated weight.
5488 
5489         Notes
5490         -----
5491         ``weights`` must be a DataArray and cannot contain missing values.
5492         Missing values can be replaced by ``weights.fillna(0)``.
5493 
5494         Returns
5495         -------
5496         core.weighted.DataArrayWeighted
5497 
5498         See Also
5499         --------
5500         Dataset.weighted
5501         """
5502         from .weighted import DataArrayWeighted
5503 
5504         return DataArrayWeighted(self, weights)
5505 
5506     def rolling(
5507         self,
5508         dim: Mapping[Any, int] | None = None,
5509         min_periods: int | None = None,
5510         center: bool | Mapping[Any, bool] = False,
5511         **window_kwargs: int,
5512     ) -> DataArrayRolling:
5513         """
5514         Rolling window object for DataArrays.
5515 
5516         Parameters
5517         ----------
5518         dim : dict, optional
5519             Mapping from the dimension name to create the rolling iterator
5520             along (e.g. `time`) to its moving window size.
5521         min_periods : int or None, default: None
5522             Minimum number of observations in window required to have a value
5523             (otherwise result is NA). The default, None, is equivalent to
5524             setting min_periods equal to the size of the window.
5525         center : bool or Mapping to int, default: False
5526             Set the labels at the center of the window.
5527         **window_kwargs : optional
5528             The keyword arguments form of ``dim``.
5529             One of dim or window_kwargs must be provided.
5530 
5531         Returns
5532         -------
5533         core.rolling.DataArrayRolling
5534 
5535         Examples
5536         --------
5537         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
5538 
5539         >>> da = xr.DataArray(
5540         ...     np.linspace(0, 11, num=12),
5541         ...     coords=[
5542         ...         pd.date_range(
5543         ...             "1999-12-15",
5544         ...             periods=12,
5545         ...             freq=pd.DateOffset(months=1),
5546         ...         )
5547         ...     ],
5548         ...     dims="time",
5549         ... )
5550         >>> da
5551         <xarray.DataArray (time: 12)>
5552         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5553         Coordinates:
5554           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5555         >>> da.rolling(time=3, center=True).mean()
5556         <xarray.DataArray (time: 12)>
5557         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
5558         Coordinates:
5559           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5560 
5561         Remove the NaNs using ``dropna()``:
5562 
5563         >>> da.rolling(time=3, center=True).mean().dropna("time")
5564         <xarray.DataArray (time: 10)>
5565         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
5566         Coordinates:
5567           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
5568 
5569         See Also
5570         --------
5571         core.rolling.DataArrayRolling
5572         Dataset.rolling
5573         """
5574         from .rolling import DataArrayRolling
5575 
5576         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
5577         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
5578 
5579     def coarsen(
5580         self,
5581         dim: Mapping[Any, int] | None = None,
5582         boundary: CoarsenBoundaryOptions = "exact",
5583         side: SideOptions | Mapping[Any, SideOptions] = "left",
5584         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
5585         **window_kwargs: int,
5586     ) -> DataArrayCoarsen:
5587         """
5588         Coarsen object for DataArrays.
5589 
5590         Parameters
5591         ----------
5592         dim : mapping of hashable to int, optional
5593             Mapping from the dimension name to the window size.
5594         boundary : {"exact", "trim", "pad"}, default: "exact"
5595             If 'exact', a ValueError will be raised if dimension size is not a
5596             multiple of the window size. If 'trim', the excess entries are
5597             dropped. If 'pad', NA will be padded.
5598         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
5599         coord_func : str or mapping of hashable to str, default: "mean"
5600             function (name) that is applied to the coordinates,
5601             or a mapping from coordinate name to function (name).
5602 
5603         Returns
5604         -------
5605         core.rolling.DataArrayCoarsen
5606 
5607         Examples
5608         --------
5609         Coarsen the long time series by averaging over every four days.
5610 
5611         >>> da = xr.DataArray(
5612         ...     np.linspace(0, 364, num=364),
5613         ...     dims="time",
5614         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
5615         ... )
5616         >>> da  # +doctest: ELLIPSIS
5617         <xarray.DataArray (time: 364)>
5618         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
5619                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
5620                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
5621         ...
5622                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
5623                360.99173554, 361.99449036, 362.99724518, 364.        ])
5624         Coordinates:
5625           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
5626         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
5627         <xarray.DataArray (time: 121)>
5628         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
5629                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
5630                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
5631         ...
5632                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
5633                361.99449036])
5634         Coordinates:
5635           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
5636         >>>
5637 
5638         See Also
5639         --------
5640         core.rolling.DataArrayCoarsen
5641         Dataset.coarsen
5642         """
5643         from .rolling import DataArrayCoarsen
5644 
5645         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
5646         return DataArrayCoarsen(
5647             self,
5648             dim,
5649             boundary=boundary,
5650             side=side,
5651             coord_func=coord_func,
5652         )
5653 
5654     def resample(
5655         self,
5656         indexer: Mapping[Any, str] | None = None,
5657         skipna: bool | None = None,
5658         closed: SideOptions | None = None,
5659         label: SideOptions | None = None,
5660         base: int = 0,
5661         keep_attrs: bool | None = None,
5662         loffset: datetime.timedelta | str | None = None,
5663         restore_coord_dims: bool | None = None,
5664         **indexer_kwargs: str,
5665     ) -> DataArrayResample:
5666         """Returns a Resample object for performing resampling operations.
5667 
5668         Handles both downsampling and upsampling. The resampled
5669         dimension must be a datetime-like coordinate. If any intervals
5670         contain no values from the original object, they will be given
5671         the value ``NaN``.
5672 
5673         Parameters
5674         ----------
5675         indexer : Mapping of Hashable to str, optional
5676             Mapping from the dimension name to resample frequency [1]_. The
5677             dimension must be datetime-like.
5678         skipna : bool, optional
5679             Whether to skip missing values when aggregating in downsampling.
5680         closed : {"left", "right"}, optional
5681             Side of each interval to treat as closed.
5682         label : {"left", "right"}, optional
5683             Side of each interval to use for labeling.
5684         base : int, default = 0
5685             For frequencies that evenly subdivide 1 day, the "origin" of the
5686             aggregated intervals. For example, for "24H" frequency, base could
5687             range from 0 through 23.
5688         loffset : timedelta or str, optional
5689             Offset used to adjust the resampled time labels. Some pandas date
5690             offset strings are supported.
5691         restore_coord_dims : bool, optional
5692             If True, also restore the dimension order of multi-dimensional
5693             coordinates.
5694         **indexer_kwargs : str
5695             The keyword arguments form of ``indexer``.
5696             One of indexer or indexer_kwargs must be provided.
5697 
5698         Returns
5699         -------
5700         resampled : core.resample.DataArrayResample
5701             This object resampled.
5702 
5703         Examples
5704         --------
5705         Downsample monthly time-series data to seasonal data:
5706 
5707         >>> da = xr.DataArray(
5708         ...     np.linspace(0, 11, num=12),
5709         ...     coords=[
5710         ...         pd.date_range(
5711         ...             "1999-12-15",
5712         ...             periods=12,
5713         ...             freq=pd.DateOffset(months=1),
5714         ...         )
5715         ...     ],
5716         ...     dims="time",
5717         ... )
5718         >>> da
5719         <xarray.DataArray (time: 12)>
5720         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5721         Coordinates:
5722           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5723         >>> da.resample(time="QS-DEC").mean()
5724         <xarray.DataArray (time: 4)>
5725         array([ 1.,  4.,  7., 10.])
5726         Coordinates:
5727           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
5728 
5729         Upsample monthly time-series data to daily data:
5730 
5731         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
5732         <xarray.DataArray (time: 337)>
5733         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
5734                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
5735                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
5736         ...
5737                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
5738                10.96774194, 11.        ])
5739         Coordinates:
5740           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5741 
5742         Limit scope of upsampling method
5743 
5744         >>> da.resample(time="1D").nearest(tolerance="1D")
5745         <xarray.DataArray (time: 337)>
5746         array([ 0.,  0., nan, ..., nan, 11., 11.])
5747         Coordinates:
5748           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5749 
5750         See Also
5751         --------
5752         Dataset.resample
5753         pandas.Series.resample
5754         pandas.DataFrame.resample
5755 
5756         References
5757         ----------
5758         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
5759         """
5760         from .resample import DataArrayResample
5761 
5762         return self._resample(
5763             resample_cls=DataArrayResample,
5764             indexer=indexer,
5765             skipna=skipna,
5766             closed=closed,
5767             label=label,
5768             base=base,
5769             keep_attrs=keep_attrs,
5770             loffset=loffset,
5771             restore_coord_dims=restore_coord_dims,
5772             **indexer_kwargs,
5773         )
5774 
5775     # this needs to be at the end, or mypy will confuse with `str`
5776     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
5777     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 09:02:33.534 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>rename</method>
<code>
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)

</code><intended_behavior>The `rename` method, when used to rename a coordinate to a dimension name, needs to ensure that an index is created or updated for the newly dimension coordinate. This behavior would align it more closely with the explicit indexing functionalities seen in `set_index`. The method should check if the renaming operation converts a non-dimension coordinate into a dimension coordinate and, in such cases, either automatically handle the indexing similar to `set_index` or raise a user-friendly error message advising the use of `set_index` for such cases. This modification preserves the existing functionality by still allowing simple renaming operations while ensuring that any operation turning coordinates into dimension coordinates maintains the integrity and expected behavior of DataArray indexing.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
223 class DataArray(
224     AbstractArray, DataWithCoords, DataArrayArithmetic, DataArrayReductions
225 ):
226     """N-dimensional array with labeled coordinates and dimensions.
227 
228     DataArray provides a wrapper around numpy ndarrays that uses
229     labeled dimensions and coordinates to support metadata aware
230     operations. The API is similar to that for the pandas Series or
231     DataFrame, but DataArray objects can have any number of dimensions,
232     and their contents have fixed data types.
233 
234     Additional features over raw numpy arrays:
235 
236     - Apply operations over dimensions by name: ``x.sum('time')``.
237     - Select or assign values by integer location (like numpy):
238       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
239       ``x.sel(time='2014-01-01')``.
240     - Mathematical operations (e.g., ``x - y``) vectorize across
241       multiple dimensions (known in numpy as "broadcasting") based on
242       dimension names, regardless of their original order.
243     - Keep track of arbitrary metadata in the form of a Python
244       dictionary: ``x.attrs``
245     - Convert to a pandas Series: ``x.to_series()``.
246 
247     Getting items from or doing mathematical operations with a
248     DataArray always returns another DataArray.
249 
250     Parameters
251     ----------
252     data : array_like
253         Values for this array. Must be an ``numpy.ndarray``, ndarray
254         like, or castable to an ``ndarray``. If a self-described xarray
255         or pandas object, attempts are made to use this array's
256         metadata to fill in other unspecified arguments. A view of the
257         array's data is used instead of a copy if possible.
258     coords : sequence or dict of array_like, optional
259         Coordinates (tick labels) to use for indexing along each
260         dimension. The following notations are accepted:
261 
262         - mapping {dimension name: array-like}
263         - sequence of tuples that are valid arguments for
264           ``xarray.Variable()``
265           - (dims, data)
266           - (dims, data, attrs)
267           - (dims, data, attrs, encoding)
268 
269         Additionally, it is possible to define a coord whose name
270         does not match the dimension name, or a coord based on multiple
271         dimensions, with one of the following notations:
272 
273         - mapping {coord name: DataArray}
274         - mapping {coord name: Variable}
275         - mapping {coord name: (dimension name, array-like)}
276         - mapping {coord name: (tuple of dimension names, array-like)}
277 
278     dims : Hashable or sequence of Hashable, optional
279         Name(s) of the data dimension(s). Must be either a Hashable
280         (only for 1D data) or a sequence of Hashables with length equal
281         to the number of dimensions. If this argument is omitted,
282         dimension names are taken from ``coords`` (if possible) and
283         otherwise default to ``['dim_0', ... 'dim_n']``.
284     name : str or None, optional
285         Name of this array.
286     attrs : dict_like or None, optional
287         Attributes to assign to the new instance. By default, an empty
288         attribute dictionary is initialized.
289 
290     Examples
291     --------
292     Create data:
293 
294     >>> np.random.seed(0)
295     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
296     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
297     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
298     >>> time = pd.date_range("2014-09-06", periods=3)
299     >>> reference_time = pd.Timestamp("2014-09-05")
300 
301     Initialize a dataarray with multiple dimensions:
302 
303     >>> da = xr.DataArray(
304     ...     data=temperature,
305     ...     dims=["x", "y", "time"],
306     ...     coords=dict(
307     ...         lon=(["x", "y"], lon),
308     ...         lat=(["x", "y"], lat),
309     ...         time=time,
310     ...         reference_time=reference_time,
311     ...     ),
312     ...     attrs=dict(
313     ...         description="Ambient temperature.",
314     ...         units="degC",
315     ...     ),
316     ... )
317     >>> da
318     <xarray.DataArray (x: 2, y: 2, time: 3)>
319     array([[[29.11241877, 18.20125767, 22.82990387],
320             [32.92714559, 29.94046392,  7.18177696]],
321     <BLANKLINE>
322            [[22.60070734, 13.78914233, 14.17424919],
323             [18.28478802, 16.15234857, 26.63418806]]])
324     Coordinates:
325         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
326         lat             (x, y) float64 42.25 42.21 42.63 42.59
327       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
328         reference_time  datetime64[ns] 2014-09-05
329     Dimensions without coordinates: x, y
330     Attributes:
331         description:  Ambient temperature.
332         units:        degC
333 
334     Find out where the coldest temperature was:
335 
336     >>> da.isel(da.argmin(...))
337     <xarray.DataArray ()>
338     array(7.18177696)
339     Coordinates:
340         lon             float64 -99.32
341         lat             float64 42.21
342         time            datetime64[ns] 2014-09-08
343         reference_time  datetime64[ns] 2014-09-05
344     Attributes:
345         description:  Ambient temperature.
346         units:        degC
347     """
348 
349     _cache: dict[str, Any]
350     _coords: dict[Any, Variable]
351     _close: Callable[[], None] | None
352     _indexes: dict[Hashable, Index]
353     _name: Hashable | None
354     _variable: Variable
355 
356     __slots__ = (
357         "_cache",
358         "_coords",
359         "_close",
360         "_indexes",
361         "_name",
362         "_variable",
363         "__weakref__",
364     )
365 
366     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
367 
368     def __init__(
369         self,
370         data: Any = dtypes.NA,
371         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
372         | Mapping[Any, Any]
373         | None = None,
374         dims: Hashable | Sequence[Hashable] | None = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: dict[Hashable, Index] = None,
379         fastpath: bool = False,
380     ) -> None:
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385             assert indexes is not None
386         else:
387             # TODO: (benbovy - explicit indexes) remove
388             # once it becomes part of the public interface
389             if indexes is not None:
390                 raise ValueError("Providing explicit indexes is not supported yet")
391 
392             # try to fill in arguments from data if they weren't supplied
393             if coords is None:
394 
395                 if isinstance(data, DataArray):
396                     coords = data.coords
397                 elif isinstance(data, pd.Series):
398                     coords = [data.index]
399                 elif isinstance(data, pd.DataFrame):
400                     coords = [data.index, data.columns]
401                 elif isinstance(data, (pd.Index, IndexVariable)):
402                     coords = [data]
403 
404             if dims is None:
405                 dims = getattr(data, "dims", getattr(coords, "dims", None))
406             if name is None:
407                 name = getattr(data, "name", None)
408             if attrs is None and not isinstance(data, PANDAS_TYPES):
409                 attrs = getattr(data, "attrs", None)
410 
411             data = _check_data_shape(data, coords, dims)
412             data = as_compatible_data(data)
413             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
414             variable = Variable(dims, data, attrs, fastpath=True)
415             indexes, coords = _create_indexes_from_coords(coords)
416 
417         # These fully describe a DataArray
418         self._variable = variable
419         assert isinstance(coords, dict)
420         self._coords = coords
421         self._name = name
422 
423         # TODO(shoyer): document this argument, once it becomes part of the
424         # public interface.
425         self._indexes = indexes  # type: ignore[assignment]
426 
427         self._close = None
428 
429     @classmethod
430     def _construct_direct(
431         cls: type[T_DataArray],
432         variable: Variable,
433         coords: dict[Any, Variable],
434         name: Hashable,
435         indexes: dict[Hashable, Index],
436     ) -> T_DataArray:
437         """Shortcut around __init__ for internal use when we want to skip
438         costly validation
439         """
440         obj = object.__new__(cls)
441         obj._variable = variable
442         obj._coords = coords
443         obj._name = name
444         obj._indexes = indexes
445         obj._close = None
446         return obj
447 
448     def _replace(
449         self: T_DataArray,
450         variable: Variable = None,
451         coords=None,
452         name: Hashable | None | Default = _default,
453         indexes=None,
454     ) -> T_DataArray:
455         if variable is None:
456             variable = self.variable
457         if coords is None:
458             coords = self._coords
459         if indexes is None:
460             indexes = self._indexes
461         if name is _default:
462             name = self.name
463         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
464 
465     def _replace_maybe_drop_dims(
466         self: T_DataArray,
467         variable: Variable,
468         name: Hashable | None | Default = _default,
469     ) -> T_DataArray:
470         if variable.dims == self.dims and variable.shape == self.shape:
471             coords = self._coords.copy()
472             indexes = self._indexes
473         elif variable.dims == self.dims:
474             # Shape has changed (e.g. from reduce(..., keepdims=True)
475             new_sizes = dict(zip(self.dims, variable.shape))
476             coords = {
477                 k: v
478                 for k, v in self._coords.items()
479                 if v.shape == tuple(new_sizes[d] for d in v.dims)
480             }
481             indexes = filter_indexes_from_coords(self._indexes, set(coords))
482         else:
483             allowed_dims = set(variable.dims)
484             coords = {
485                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
486             }
487             indexes = filter_indexes_from_coords(self._indexes, set(coords))
488         return self._replace(variable, coords, name, indexes=indexes)
489 
490     def _overwrite_indexes(
491         self: T_DataArray,
492         indexes: Mapping[Any, Index],
493         coords: Mapping[Any, Variable] = None,
494         drop_coords: list[Hashable] = None,
495         rename_dims: Mapping[Any, Any] = None,
496     ) -> T_DataArray:
497         """Maybe replace indexes and their corresponding coordinates."""
498         if not indexes:
499             return self
500 
501         if coords is None:
502             coords = {}
503         if drop_coords is None:
504             drop_coords = []
505 
506         new_variable = self.variable.copy()
507         new_coords = self._coords.copy()
508         new_indexes = dict(self._indexes)
509 
510         for name in indexes:
511             new_coords[name] = coords[name]
512             new_indexes[name] = indexes[name]
513 
514         for name in drop_coords:
515             new_coords.pop(name)
516             new_indexes.pop(name)
517 
518         if rename_dims:
519             new_variable.dims = [rename_dims.get(d, d) for d in new_variable.dims]
520 
521         return self._replace(
522             variable=new_variable, coords=new_coords, indexes=new_indexes
523         )
524 
525     def _to_temp_dataset(self) -> Dataset:
526         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
527 
528     def _from_temp_dataset(
529         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
530     ) -> T_DataArray:
531         variable = dataset._variables.pop(_THIS_ARRAY)
532         coords = dataset._variables
533         indexes = dataset._indexes
534         return self._replace(variable, coords, name, indexes=indexes)
535 
536     def _to_dataset_split(self, dim: Hashable) -> Dataset:
537         """splits dataarray along dimension 'dim'"""
538 
539         def subset(dim, label):
540             array = self.loc[{dim: label}]
541             array.attrs = {}
542             return as_variable(array)
543 
544         variables = {label: subset(dim, label) for label in self.get_index(dim)}
545         variables.update({k: v for k, v in self._coords.items() if k != dim})
546         coord_names = set(self._coords) - {dim}
547         indexes = filter_indexes_from_coords(self._indexes, coord_names)
548         dataset = Dataset._construct_direct(
549             variables, coord_names, indexes=indexes, attrs=self.attrs
550         )
551         return dataset
552 
553     def _to_dataset_whole(
554         self, name: Hashable = None, shallow_copy: bool = True
555     ) -> Dataset:
556         if name is None:
557             name = self.name
558         if name is None:
559             raise ValueError(
560                 "unable to convert unnamed DataArray to a "
561                 "Dataset without providing an explicit name"
562             )
563         if name in self.coords:
564             raise ValueError(
565                 "cannot create a Dataset from a DataArray with "
566                 "the same name as one of its coordinates"
567             )
568         # use private APIs for speed: this is called by _to_temp_dataset(),
569         # which is used in the guts of a lot of operations (e.g., reindex)
570         variables = self._coords.copy()
571         variables[name] = self.variable
572         if shallow_copy:
573             for k in variables:
574                 variables[k] = variables[k].copy(deep=False)
575         indexes = self._indexes
576 
577         coord_names = set(self._coords)
578         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
579 
580     def to_dataset(
581         self,
582         dim: Hashable = None,
583         *,
584         name: Hashable = None,
585         promote_attrs: bool = False,
586     ) -> Dataset:
587         """Convert a DataArray to a Dataset.
588 
589         Parameters
590         ----------
591         dim : Hashable, optional
592             Name of the dimension on this array along which to split this array
593             into separate variables. If not provided, this array is converted
594             into a Dataset of one variable.
595         name : Hashable, optional
596             Name to substitute for this array's name. Only valid if ``dim`` is
597             not provided.
598         promote_attrs : bool, default: False
599             Set to True to shallow copy attrs of DataArray to returned Dataset.
600 
601         Returns
602         -------
603         dataset : Dataset
604         """
605         if dim is not None and dim not in self.dims:
606             raise TypeError(
607                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
608             )
609 
610         if dim is not None:
611             if name is not None:
612                 raise TypeError("cannot supply both dim and name arguments")
613             result = self._to_dataset_split(dim)
614         else:
615             result = self._to_dataset_whole(name)
616 
617         if promote_attrs:
618             result.attrs = dict(self.attrs)
619 
620         return result
621 
622     @property
623     def name(self) -> Hashable | None:
624         """The name of this array."""
625         return self._name
626 
627     @name.setter
628     def name(self, value: Hashable | None) -> None:
629         self._name = value
630 
631     @property
632     def variable(self) -> Variable:
633         """Low level interface to the Variable object for this DataArray."""
634         return self._variable
635 
636     @property
637     def dtype(self) -> np.dtype:
638         return self.variable.dtype
639 
640     @property
641     def shape(self) -> tuple[int, ...]:
642         return self.variable.shape
643 
644     @property
645     def size(self) -> int:
646         return self.variable.size
647 
648     @property
649     def nbytes(self) -> int:
650         """
651         Total bytes consumed by the elements of this DataArray's data.
652 
653         If the backend data array does not include ``nbytes``, estimates
654         the bytes consumed based on the ``size`` and ``dtype``.
655         """
656         return self.variable.nbytes
657 
658     @property
659     def ndim(self) -> int:
660         return self.variable.ndim
661 
662     def __len__(self) -> int:
663         return len(self.variable)
664 
665     @property
666     def data(self) -> Any:
667         """
668         The DataArray's data as an array. The underlying array type
669         (e.g. dask, sparse, pint) is preserved.
670 
671         See Also
672         --------
673         DataArray.to_numpy
674         DataArray.as_numpy
675         DataArray.values
676         """
677         return self.variable.data
678 
679     @data.setter
680     def data(self, value: Any) -> None:
681         self.variable.data = value
682 
683     @property
684     def values(self) -> np.ndarray:
685         """
686         The array's data as a numpy.ndarray.
687 
688         If the array's data is not a numpy.ndarray this will attempt to convert
689         it naively using np.array(), which will raise an error if the array
690         type does not support coercion like this (e.g. cupy).
691         """
692         return self.variable.values
693 
694     @values.setter
695     def values(self, value: Any) -> None:
696         self.variable.values = value
697 
698     def to_numpy(self) -> np.ndarray:
699         """
700         Coerces wrapped data to numpy and returns a numpy.ndarray.
701 
702         See Also
703         --------
704         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
705         Dataset.as_numpy
706         DataArray.values
707         DataArray.data
708         """
709         return self.variable.to_numpy()
710 
711     def as_numpy(self: T_DataArray) -> T_DataArray:
712         """
713         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
714 
715         See Also
716         --------
717         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
718         Dataset.as_numpy : Converts all variables in a Dataset.
719         DataArray.values
720         DataArray.data
721         """
722         coords = {k: v.as_numpy() for k, v in self._coords.items()}
723         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
724 
725     @property
726     def _in_memory(self) -> bool:
727         return self.variable._in_memory
728 
729     def to_index(self) -> pd.Index:
730         """Convert this variable to a pandas.Index. Only possible for 1D
731         arrays.
732         """
733         return self.variable.to_index()
734 
735     @property
736     def dims(self) -> tuple[Hashable, ...]:
737         """Tuple of dimension names associated with this array.
738 
739         Note that the type of this property is inconsistent with
740         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
741         consistently named properties.
742 
743         See Also
744         --------
745         DataArray.sizes
746         Dataset.dims
747         """
748         return self.variable.dims
749 
750     @dims.setter
751     def dims(self, value: Any) -> NoReturn:
752         raise AttributeError(
753             "you cannot assign dims on a DataArray. Use "
754             ".rename() or .swap_dims() instead."
755         )
756 
757     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
758         if utils.is_dict_like(key):
759             return key
760         key = indexing.expanded_indexer(key, self.ndim)
761         return dict(zip(self.dims, key))
762 
763     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
764         from .dataset import _get_virtual_variable
765 
766         try:
767             var = self._coords[key]
768         except KeyError:
769             dim_sizes = dict(zip(self.dims, self.shape))
770             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
771 
772         return self._replace_maybe_drop_dims(var, name=key)
773 
774     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
775         if isinstance(key, str):
776             return self._getitem_coord(key)
777         else:
778             # xarray-style array indexing
779             return self.isel(indexers=self._item_key_to_dict(key))
780 
781     def __setitem__(self, key: Any, value: Any) -> None:
782         if isinstance(key, str):
783             self.coords[key] = value
784         else:
785             # Coordinates in key, value and self[key] should be consistent.
786             # TODO Coordinate consistency in key is checked here, but it
787             # causes unnecessary indexing. It should be optimized.
788             obj = self[key]
789             if isinstance(value, DataArray):
790                 assert_coordinate_consistent(value, obj.coords.variables)
791             # DataArray key -> Variable key
792             key = {
793                 k: v.variable if isinstance(v, DataArray) else v
794                 for k, v in self._item_key_to_dict(key).items()
795             }
796             self.variable[key] = value
797 
798     def __delitem__(self, key: Any) -> None:
799         del self.coords[key]
800 
801     @property
802     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
803         """Places to look-up items for attribute-style access"""
804         yield from self._item_sources
805         yield self.attrs
806 
807     @property
808     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
809         """Places to look-up items for key-completion"""
810         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
811 
812         # virtual coordinates
813         # uses empty dict -- everything here can already be found in self.coords.
814         yield HybridMappingProxy(keys=self.dims, mapping={})
815 
816     def __contains__(self, key: Any) -> bool:
817         return key in self.data
818 
819     @property
820     def loc(self) -> _LocIndexer:
821         """Attribute for location based indexing like pandas."""
822         return _LocIndexer(self)
823 
824     @property
825     # Key type needs to be `Any` because of mypy#4167
826     def attrs(self) -> dict[Any, Any]:
827         """Dictionary storing arbitrary metadata with this array."""
828         return self.variable.attrs
829 
830     @attrs.setter
831     def attrs(self, value: Mapping[Any, Any]) -> None:
832         # Disable type checking to work around mypy bug - see mypy#4167
833         self.variable.attrs = value  # type: ignore[assignment]
834 
835     @property
836     def encoding(self) -> dict[Hashable, Any]:
837         """Dictionary of format-specific settings for how this array should be
838         serialized."""
839         return self.variable.encoding
840 
841     @encoding.setter
842     def encoding(self, value: Mapping[Any, Any]) -> None:
843         self.variable.encoding = value
844 
845     @property
846     def indexes(self) -> Indexes:
847         """Mapping of pandas.Index objects used for label based indexing.
848 
849         Raises an error if this Dataset has indexes that cannot be coerced
850         to pandas.Index objects.
851 
852         See Also
853         --------
854         DataArray.xindexes
855 
856         """
857         return self.xindexes.to_pandas_indexes()
858 
859     @property
860     def xindexes(self) -> Indexes:
861         """Mapping of xarray Index objects used for label based indexing."""
862         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
863 
864     @property
865     def coords(self) -> DataArrayCoordinates:
866         """Dictionary-like container of coordinate arrays."""
867         return DataArrayCoordinates(self)
868 
869     @overload
870     def reset_coords(
871         self: T_DataArray,
872         names: Hashable | Iterable[Hashable] | None = None,
873         drop: Literal[False] = False,
874     ) -> Dataset:
875         ...
876 
877     @overload
878     def reset_coords(
879         self: T_DataArray,
880         names: Hashable | Iterable[Hashable] | None = None,
881         *,
882         drop: Literal[True],
883     ) -> T_DataArray:
884         ...
885 
886     def reset_coords(
887         self: T_DataArray,
888         names: Hashable | Iterable[Hashable] | None = None,
889         drop: bool = False,
890     ) -> T_DataArray | Dataset:
891         """Given names of coordinates, reset them to become variables.
892 
893         Parameters
894         ----------
895         names : Hashable or iterable of Hashable, optional
896             Name(s) of non-index coordinates in this dataset to reset into
897             variables. By default, all non-index coordinates are reset.
898         drop : bool, default: False
899             If True, remove coordinates instead of converting them into
900             variables.
901 
902         Returns
903         -------
904         Dataset, or DataArray if ``drop == True``
905         """
906         if names is None:
907             names = set(self.coords) - set(self._indexes)
908         dataset = self.coords.to_dataset().reset_coords(names, drop)
909         if drop:
910             return self._replace(coords=dataset._variables)
911         if self.name is None:
912             raise ValueError(
913                 "cannot reset_coords with drop=False on an unnamed DataArrray"
914             )
915         dataset[self.name] = self.variable
916         return dataset
917 
918     def __dask_tokenize__(self):
919         from dask.base import normalize_token
920 
921         return normalize_token((type(self), self._variable, self._coords, self._name))
922 
923     def __dask_graph__(self):
924         return self._to_temp_dataset().__dask_graph__()
925 
926     def __dask_keys__(self):
927         return self._to_temp_dataset().__dask_keys__()
928 
929     def __dask_layers__(self):
930         return self._to_temp_dataset().__dask_layers__()
931 
932     @property
933     def __dask_optimize__(self):
934         return self._to_temp_dataset().__dask_optimize__
935 
936     @property
937     def __dask_scheduler__(self):
938         return self._to_temp_dataset().__dask_scheduler__
939 
940     def __dask_postcompute__(self):
941         func, args = self._to_temp_dataset().__dask_postcompute__()
942         return self._dask_finalize, (self.name, func) + args
943 
944     def __dask_postpersist__(self):
945         func, args = self._to_temp_dataset().__dask_postpersist__()
946         return self._dask_finalize, (self.name, func) + args
947 
948     @staticmethod
949     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
950         ds = func(results, *args, **kwargs)
951         variable = ds._variables.pop(_THIS_ARRAY)
952         coords = ds._variables
953         indexes = ds._indexes
954         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
955 
956     def load(self: T_DataArray, **kwargs) -> T_DataArray:
957         """Manually trigger loading of this array's data from disk or a
958         remote source into memory and return this array.
959 
960         Normally, it should not be necessary to call this method in user code,
961         because all xarray functions should either work on deferred data or
962         load data automatically. However, this method can be necessary when
963         working with many file objects on disk.
964 
965         Parameters
966         ----------
967         **kwargs : dict
968             Additional keyword arguments passed on to ``dask.compute``.
969 
970         See Also
971         --------
972         dask.compute
973         """
974         ds = self._to_temp_dataset().load(**kwargs)
975         new = self._from_temp_dataset(ds)
976         self._variable = new._variable
977         self._coords = new._coords
978         return self
979 
980     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
981         """Manually trigger loading of this array's data from disk or a
982         remote source into memory and return a new array. The original is
983         left unaltered.
984 
985         Normally, it should not be necessary to call this method in user code,
986         because all xarray functions should either work on deferred data or
987         load data automatically. However, this method can be necessary when
988         working with many file objects on disk.
989 
990         Parameters
991         ----------
992         **kwargs : dict
993             Additional keyword arguments passed on to ``dask.compute``.
994 
995         See Also
996         --------
997         dask.compute
998         """
999         new = self.copy(deep=False)
1000         return new.load(**kwargs)
1001 
1002     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1003         """Trigger computation in constituent dask arrays
1004 
1005         This keeps them as dask arrays but encourages them to keep data in
1006         memory.  This is particularly useful when on a distributed machine.
1007         When on a single machine consider using ``.compute()`` instead.
1008 
1009         Parameters
1010         ----------
1011         **kwargs : dict
1012             Additional keyword arguments passed on to ``dask.persist``.
1013 
1014         See Also
1015         --------
1016         dask.persist
1017         """
1018         ds = self._to_temp_dataset().persist(**kwargs)
1019         return self._from_temp_dataset(ds)
1020 
1021     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1022         """Returns a copy of this array.
1023 
1024         If `deep=True`, a deep copy is made of the data array.
1025         Otherwise, a shallow copy is made, and the returned data array's
1026         values are a new view of this data array's values.
1027 
1028         Use `data` to create a new object with the same structure as
1029         original but entirely new data.
1030 
1031         Parameters
1032         ----------
1033         deep : bool, optional
1034             Whether the data array and its coordinates are loaded into memory
1035             and copied onto the new object. Default is True.
1036         data : array_like, optional
1037             Data to use in the new object. Must have same shape as original.
1038             When `data` is used, `deep` is ignored for all data variables,
1039             and only used for coords.
1040 
1041         Returns
1042         -------
1043         copy : DataArray
1044             New object with dimensions, attributes, coordinates, name,
1045             encoding, and optionally data copied from original.
1046 
1047         Examples
1048         --------
1049         Shallow versus deep copy
1050 
1051         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1052         >>> array.copy()
1053         <xarray.DataArray (x: 3)>
1054         array([1, 2, 3])
1055         Coordinates:
1056           * x        (x) <U1 'a' 'b' 'c'
1057         >>> array_0 = array.copy(deep=False)
1058         >>> array_0[0] = 7
1059         >>> array_0
1060         <xarray.DataArray (x: 3)>
1061         array([7, 2, 3])
1062         Coordinates:
1063           * x        (x) <U1 'a' 'b' 'c'
1064         >>> array
1065         <xarray.DataArray (x: 3)>
1066         array([7, 2, 3])
1067         Coordinates:
1068           * x        (x) <U1 'a' 'b' 'c'
1069 
1070         Changing the data using the ``data`` argument maintains the
1071         structure of the original object, but with the new data. Original
1072         object is unaffected.
1073 
1074         >>> array.copy(data=[0.1, 0.2, 0.3])
1075         <xarray.DataArray (x: 3)>
1076         array([0.1, 0.2, 0.3])
1077         Coordinates:
1078           * x        (x) <U1 'a' 'b' 'c'
1079         >>> array
1080         <xarray.DataArray (x: 3)>
1081         array([7, 2, 3])
1082         Coordinates:
1083           * x        (x) <U1 'a' 'b' 'c'
1084 
1085         See Also
1086         --------
1087         pandas.DataFrame.copy
1088         """
1089         variable = self.variable.copy(deep=deep, data=data)
1090         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1091 
1092         coords = {}
1093         for k, v in self._coords.items():
1094             if k in index_vars:
1095                 coords[k] = index_vars[k]
1096             else:
1097                 coords[k] = v.copy(deep=deep)
1098 
1099         return self._replace(variable, coords, indexes=indexes)
1100 
1101     def __copy__(self: T_DataArray) -> T_DataArray:
1102         return self.copy(deep=False)
1103 
1104     def __deepcopy__(self: T_DataArray, memo=None) -> T_DataArray:
1105         # memo does nothing but is required for compatibility with
1106         # copy.deepcopy
1107         return self.copy(deep=True)
1108 
1109     # mutable objects should not be Hashable
1110     # https://github.com/python/mypy/issues/4266
1111     __hash__ = None  # type: ignore[assignment]
1112 
1113     @property
1114     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1115         """
1116         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1117         the underlying data is not a dask array.
1118 
1119         See Also
1120         --------
1121         DataArray.chunk
1122         DataArray.chunksizes
1123         xarray.unify_chunks
1124         """
1125         return self.variable.chunks
1126 
1127     @property
1128     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1129         """
1130         Mapping from dimension names to block lengths for this dataarray's data, or None if
1131         the underlying data is not a dask array.
1132         Cannot be modified directly, but can be modified by calling .chunk().
1133 
1134         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1135         instead of a tuple of chunk shapes.
1136 
1137         See Also
1138         --------
1139         DataArray.chunk
1140         DataArray.chunks
1141         xarray.unify_chunks
1142         """
1143         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1144         return get_chunksizes(all_variables)
1145 
1146     def chunk(
1147         self: T_DataArray,
1148         chunks: (
1149             int
1150             | Literal["auto"]
1151             | tuple[int, ...]
1152             | tuple[tuple[int, ...], ...]
1153             | Mapping[Any, None | int | tuple[int, ...]]
1154         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1155         name_prefix: str = "xarray-",
1156         token: str | None = None,
1157         lock: bool = False,
1158         inline_array: bool = False,
1159         **chunks_kwargs: Any,
1160     ) -> T_DataArray:
1161         """Coerce this array's data into a dask arrays with the given chunks.
1162 
1163         If this variable is a non-dask array, it will be converted to dask
1164         array. If it's a dask array, it will be rechunked to the given chunk
1165         sizes.
1166 
1167         If neither chunks is not provided for one or more dimensions, chunk
1168         sizes along that dimension will not be updated; non-dask arrays will be
1169         converted into dask arrays with a single block.
1170 
1171         Parameters
1172         ----------
1173         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1174             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1175             ``{"x": 5, "y": 5}``.
1176         name_prefix : str, optional
1177             Prefix for the name of the new dask array.
1178         token : str, optional
1179             Token uniquely identifying this array.
1180         lock : optional
1181             Passed on to :py:func:`dask.array.from_array`, if the array is not
1182             already as dask array.
1183         inline_array: optional
1184             Passed on to :py:func:`dask.array.from_array`, if the array is not
1185             already as dask array.
1186         **chunks_kwargs : {dim: chunks, ...}, optional
1187             The keyword arguments form of ``chunks``.
1188             One of chunks or chunks_kwargs must be provided.
1189 
1190         Returns
1191         -------
1192         chunked : xarray.DataArray
1193 
1194         See Also
1195         --------
1196         DataArray.chunks
1197         DataArray.chunksizes
1198         xarray.unify_chunks
1199         dask.array.from_array
1200         """
1201         if chunks is None:
1202             warnings.warn(
1203                 "None value for 'chunks' is deprecated. "
1204                 "It will raise an error in the future. Use instead '{}'",
1205                 category=FutureWarning,
1206             )
1207             chunks = {}
1208 
1209         if isinstance(chunks, (float, str, int)):
1210             # ignoring type; unclear why it won't accept a Literal into the value.
1211             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1212         elif isinstance(chunks, (tuple, list)):
1213             chunks = dict(zip(self.dims, chunks))
1214         else:
1215             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1216 
1217         ds = self._to_temp_dataset().chunk(
1218             chunks,
1219             name_prefix=name_prefix,
1220             token=token,
1221             lock=lock,
1222             inline_array=inline_array,
1223         )
1224         return self._from_temp_dataset(ds)
1225 
1226     def isel(
1227         self: T_DataArray,
1228         indexers: Mapping[Any, Any] | None = None,
1229         drop: bool = False,
1230         missing_dims: ErrorOptionsWithWarn = "raise",
1231         **indexers_kwargs: Any,
1232     ) -> T_DataArray:
1233         """Return a new DataArray whose data is given by selecting indexes
1234         along the specified dimension(s).
1235 
1236         Parameters
1237         ----------
1238         indexers : dict, optional
1239             A dict with keys matching dimensions and values given
1240             by integers, slice objects or arrays.
1241             indexer can be a integer, slice, array-like or DataArray.
1242             If DataArrays are passed as indexers, xarray-style indexing will be
1243             carried out. See :ref:`indexing` for the details.
1244             One of indexers or indexers_kwargs must be provided.
1245         drop : bool, default: False
1246             If ``drop=True``, drop coordinates variables indexed by integers
1247             instead of making them scalar.
1248         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1249             What to do if dimensions that should be selected from are not present in the
1250             DataArray:
1251             - "raise": raise an exception
1252             - "warn": raise a warning, and ignore the missing dimensions
1253             - "ignore": ignore the missing dimensions
1254         **indexers_kwargs : {dim: indexer, ...}, optional
1255             The keyword arguments form of ``indexers``.
1256 
1257         Returns
1258         -------
1259         indexed : xarray.DataArray
1260 
1261         See Also
1262         --------
1263         Dataset.isel
1264         DataArray.sel
1265 
1266         Examples
1267         --------
1268         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1269         >>> da
1270         <xarray.DataArray (x: 5, y: 5)>
1271         array([[ 0,  1,  2,  3,  4],
1272                [ 5,  6,  7,  8,  9],
1273                [10, 11, 12, 13, 14],
1274                [15, 16, 17, 18, 19],
1275                [20, 21, 22, 23, 24]])
1276         Dimensions without coordinates: x, y
1277 
1278         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1279         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1280         >>> da = da.isel(x=tgt_x, y=tgt_y)
1281         >>> da
1282         <xarray.DataArray (points: 5)>
1283         array([ 0,  6, 12, 18, 24])
1284         Dimensions without coordinates: points
1285         """
1286 
1287         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1288 
1289         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1290             ds = self._to_temp_dataset()._isel_fancy(
1291                 indexers, drop=drop, missing_dims=missing_dims
1292             )
1293             return self._from_temp_dataset(ds)
1294 
1295         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1296         # lists, or zero or one-dimensional np.ndarray's
1297 
1298         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1299         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1300 
1301         coords = {}
1302         for coord_name, coord_value in self._coords.items():
1303             if coord_name in index_variables:
1304                 coord_value = index_variables[coord_name]
1305             else:
1306                 coord_indexers = {
1307                     k: v for k, v in indexers.items() if k in coord_value.dims
1308                 }
1309                 if coord_indexers:
1310                     coord_value = coord_value.isel(coord_indexers)
1311                     if drop and coord_value.ndim == 0:
1312                         continue
1313             coords[coord_name] = coord_value
1314 
1315         return self._replace(variable=variable, coords=coords, indexes=indexes)
1316 
1317     def sel(
1318         self: T_DataArray,
1319         indexers: Mapping[Any, Any] = None,
1320         method: str = None,
1321         tolerance=None,
1322         drop: bool = False,
1323         **indexers_kwargs: Any,
1324     ) -> T_DataArray:
1325         """Return a new DataArray whose data is given by selecting index
1326         labels along the specified dimension(s).
1327 
1328         In contrast to `DataArray.isel`, indexers for this method should use
1329         labels instead of integers.
1330 
1331         Under the hood, this method is powered by using pandas's powerful Index
1332         objects. This makes label based indexing essentially just as fast as
1333         using integer indexing.
1334 
1335         It also means this method uses pandas's (well documented) logic for
1336         indexing. This means you can use string shortcuts for datetime indexes
1337         (e.g., '2000-01' to select all values in January 2000). It also means
1338         that slices are treated as inclusive of both the start and stop values,
1339         unlike normal Python indexing.
1340 
1341         .. warning::
1342 
1343           Do not try to assign values when using any of the indexing methods
1344           ``isel`` or ``sel``::
1345 
1346             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1347             # DO NOT do this
1348             da.isel(x=[0, 1, 2])[1] = -1
1349 
1350           Assigning values with the chained indexing using ``.sel`` or
1351           ``.isel`` fails silently.
1352 
1353         Parameters
1354         ----------
1355         indexers : dict, optional
1356             A dict with keys matching dimensions and values given
1357             by scalars, slices or arrays of tick labels. For dimensions with
1358             multi-index, the indexer may also be a dict-like object with keys
1359             matching index level names.
1360             If DataArrays are passed as indexers, xarray-style indexing will be
1361             carried out. See :ref:`indexing` for the details.
1362             One of indexers or indexers_kwargs must be provided.
1363         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1364             Method to use for inexact matches:
1365 
1366             - None (default): only exact matches
1367             - pad / ffill: propagate last valid index value forward
1368             - backfill / bfill: propagate next valid index value backward
1369             - nearest: use nearest valid index value
1370 
1371         tolerance : optional
1372             Maximum distance between original and new labels for inexact
1373             matches. The values of the index at the matching locations must
1374             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1375         drop : bool, optional
1376             If ``drop=True``, drop coordinates variables in `indexers` instead
1377             of making them scalar.
1378         **indexers_kwargs : {dim: indexer, ...}, optional
1379             The keyword arguments form of ``indexers``.
1380             One of indexers or indexers_kwargs must be provided.
1381 
1382         Returns
1383         -------
1384         obj : DataArray
1385             A new DataArray with the same contents as this DataArray, except the
1386             data and each dimension is indexed by the appropriate indexers.
1387             If indexer DataArrays have coordinates that do not conflict with
1388             this object, then these coordinates will be attached.
1389             In general, each array's data will be a view of the array's data
1390             in this DataArray, unless vectorized indexing was triggered by using
1391             an array indexer, in which case the data will be a copy.
1392 
1393         See Also
1394         --------
1395         Dataset.sel
1396         DataArray.isel
1397 
1398         Examples
1399         --------
1400         >>> da = xr.DataArray(
1401         ...     np.arange(25).reshape(5, 5),
1402         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1403         ...     dims=("x", "y"),
1404         ... )
1405         >>> da
1406         <xarray.DataArray (x: 5, y: 5)>
1407         array([[ 0,  1,  2,  3,  4],
1408                [ 5,  6,  7,  8,  9],
1409                [10, 11, 12, 13, 14],
1410                [15, 16, 17, 18, 19],
1411                [20, 21, 22, 23, 24]])
1412         Coordinates:
1413           * x        (x) int64 0 1 2 3 4
1414           * y        (y) int64 0 1 2 3 4
1415 
1416         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1417         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1418         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1419         >>> da
1420         <xarray.DataArray (points: 5)>
1421         array([ 0,  6, 12, 18, 24])
1422         Coordinates:
1423             x        (points) int64 0 1 2 3 4
1424             y        (points) int64 0 1 2 3 4
1425         Dimensions without coordinates: points
1426         """
1427         ds = self._to_temp_dataset().sel(
1428             indexers=indexers,
1429             drop=drop,
1430             method=method,
1431             tolerance=tolerance,
1432             **indexers_kwargs,
1433         )
1434         return self._from_temp_dataset(ds)
1435 
1436     def head(
1437         self: T_DataArray,
1438         indexers: Mapping[Any, int] | int | None = None,
1439         **indexers_kwargs: Any,
1440     ) -> T_DataArray:
1441         """Return a new DataArray whose data is given by the the first `n`
1442         values along the specified dimension(s). Default `n` = 5
1443 
1444         See Also
1445         --------
1446         Dataset.head
1447         DataArray.tail
1448         DataArray.thin
1449         """
1450         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1451         return self._from_temp_dataset(ds)
1452 
1453     def tail(
1454         self: T_DataArray,
1455         indexers: Mapping[Any, int] | int | None = None,
1456         **indexers_kwargs: Any,
1457     ) -> T_DataArray:
1458         """Return a new DataArray whose data is given by the the last `n`
1459         values along the specified dimension(s). Default `n` = 5
1460 
1461         See Also
1462         --------
1463         Dataset.tail
1464         DataArray.head
1465         DataArray.thin
1466         """
1467         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1468         return self._from_temp_dataset(ds)
1469 
1470     def thin(
1471         self: T_DataArray,
1472         indexers: Mapping[Any, int] | int | None = None,
1473         **indexers_kwargs: Any,
1474     ) -> T_DataArray:
1475         """Return a new DataArray whose data is given by each `n` value
1476         along the specified dimension(s).
1477 
1478         Examples
1479         --------
1480         >>> x_arr = np.arange(0, 26)
1481         >>> x_arr
1482         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1483                17, 18, 19, 20, 21, 22, 23, 24, 25])
1484         >>> x = xr.DataArray(
1485         ...     np.reshape(x_arr, (2, 13)),
1486         ...     dims=("x", "y"),
1487         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1488         ... )
1489         >>> x
1490         <xarray.DataArray (x: 2, y: 13)>
1491         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1492                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1493         Coordinates:
1494           * x        (x) int64 0 1
1495           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1496 
1497         >>>
1498         >>> x.thin(3)
1499         <xarray.DataArray (x: 1, y: 5)>
1500         array([[ 0,  3,  6,  9, 12]])
1501         Coordinates:
1502           * x        (x) int64 0
1503           * y        (y) int64 0 3 6 9 12
1504         >>> x.thin({"x": 2, "y": 5})
1505         <xarray.DataArray (x: 1, y: 3)>
1506         array([[ 0,  5, 10]])
1507         Coordinates:
1508           * x        (x) int64 0
1509           * y        (y) int64 0 5 10
1510 
1511         See Also
1512         --------
1513         Dataset.thin
1514         DataArray.head
1515         DataArray.tail
1516         """
1517         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1518         return self._from_temp_dataset(ds)
1519 
1520     def broadcast_like(
1521         self: T_DataArray,
1522         other: DataArray | Dataset,
1523         exclude: Iterable[Hashable] | None = None,
1524     ) -> T_DataArray:
1525         """Broadcast this DataArray against another Dataset or DataArray.
1526 
1527         This is equivalent to xr.broadcast(other, self)[1]
1528 
1529         xarray objects are broadcast against each other in arithmetic
1530         operations, so this method is not be necessary for most uses.
1531 
1532         If no change is needed, the input data is returned to the output
1533         without being copied.
1534 
1535         If new coords are added by the broadcast, their values are
1536         NaN filled.
1537 
1538         Parameters
1539         ----------
1540         other : Dataset or DataArray
1541             Object against which to broadcast this array.
1542         exclude : iterable of Hashable, optional
1543             Dimensions that must not be broadcasted
1544 
1545         Returns
1546         -------
1547         new_da : DataArray
1548             The caller broadcasted against ``other``.
1549 
1550         Examples
1551         --------
1552         >>> arr1 = xr.DataArray(
1553         ...     np.random.randn(2, 3),
1554         ...     dims=("x", "y"),
1555         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1556         ... )
1557         >>> arr2 = xr.DataArray(
1558         ...     np.random.randn(3, 2),
1559         ...     dims=("x", "y"),
1560         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1561         ... )
1562         >>> arr1
1563         <xarray.DataArray (x: 2, y: 3)>
1564         array([[ 1.76405235,  0.40015721,  0.97873798],
1565                [ 2.2408932 ,  1.86755799, -0.97727788]])
1566         Coordinates:
1567           * x        (x) <U1 'a' 'b'
1568           * y        (y) <U1 'a' 'b' 'c'
1569         >>> arr2
1570         <xarray.DataArray (x: 3, y: 2)>
1571         array([[ 0.95008842, -0.15135721],
1572                [-0.10321885,  0.4105985 ],
1573                [ 0.14404357,  1.45427351]])
1574         Coordinates:
1575           * x        (x) <U1 'a' 'b' 'c'
1576           * y        (y) <U1 'a' 'b'
1577         >>> arr1.broadcast_like(arr2)
1578         <xarray.DataArray (x: 3, y: 3)>
1579         array([[ 1.76405235,  0.40015721,  0.97873798],
1580                [ 2.2408932 ,  1.86755799, -0.97727788],
1581                [        nan,         nan,         nan]])
1582         Coordinates:
1583           * x        (x) <U1 'a' 'b' 'c'
1584           * y        (y) <U1 'a' 'b' 'c'
1585         """
1586         if exclude is None:
1587             exclude = set()
1588         else:
1589             exclude = set(exclude)
1590         args = align(other, self, join="outer", copy=False, exclude=exclude)
1591 
1592         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1593 
1594         return _broadcast_helper(
1595             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1596         )
1597 
1598     def _reindex_callback(
1599         self: T_DataArray,
1600         aligner: alignment.Aligner,
1601         dim_pos_indexers: dict[Hashable, Any],
1602         variables: dict[Hashable, Variable],
1603         indexes: dict[Hashable, Index],
1604         fill_value: Any,
1605         exclude_dims: frozenset[Hashable],
1606         exclude_vars: frozenset[Hashable],
1607     ) -> T_DataArray:
1608         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1609 
1610         if isinstance(fill_value, dict):
1611             fill_value = fill_value.copy()
1612             sentinel = object()
1613             value = fill_value.pop(self.name, sentinel)
1614             if value is not sentinel:
1615                 fill_value[_THIS_ARRAY] = value
1616 
1617         ds = self._to_temp_dataset()
1618         reindexed = ds._reindex_callback(
1619             aligner,
1620             dim_pos_indexers,
1621             variables,
1622             indexes,
1623             fill_value,
1624             exclude_dims,
1625             exclude_vars,
1626         )
1627         return self._from_temp_dataset(reindexed)
1628 
1629     def reindex_like(
1630         self: T_DataArray,
1631         other: DataArray | Dataset,
1632         method: ReindexMethodOptions = None,
1633         tolerance: int | float | Iterable[int | float] | None = None,
1634         copy: bool = True,
1635         fill_value=dtypes.NA,
1636     ) -> T_DataArray:
1637         """Conform this object onto the indexes of another object, filling in
1638         missing values with ``fill_value``. The default fill value is NaN.
1639 
1640         Parameters
1641         ----------
1642         other : Dataset or DataArray
1643             Object with an 'indexes' attribute giving a mapping from dimension
1644             names to pandas.Index objects, which provides coordinates upon
1645             which to index the variables in this dataset. The indexes on this
1646             other object need not be the same as the indexes on this
1647             dataset. Any mis-matched index values will be filled in with
1648             NaN, and any mis-matched dimension names will simply be ignored.
1649         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1650             Method to use for filling index values from other not found on this
1651             data array:
1652 
1653             - None (default): don't fill gaps
1654             - pad / ffill: propagate last valid index value forward
1655             - backfill / bfill: propagate next valid index value backward
1656             - nearest: use nearest valid index value
1657 
1658         tolerance : optional
1659             Maximum distance between original and new labels for inexact
1660             matches. The values of the index at the matching locations must
1661             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1662             Tolerance may be a scalar value, which applies the same tolerance
1663             to all values, or list-like, which applies variable tolerance per
1664             element. List-like must be the same size as the index and its dtype
1665             must exactly match the index’s type.
1666         copy : bool, default: True
1667             If ``copy=True``, data in the return value is always copied. If
1668             ``copy=False`` and reindexing is unnecessary, or can be performed
1669             with only slice operations, then the output may share memory with
1670             the input. In either case, a new xarray object is always returned.
1671         fill_value : scalar or dict-like, optional
1672             Value to use for newly missing values. If a dict-like, maps
1673             variable names (including coordinates) to fill values. Use this
1674             data array's name to refer to the data array's values.
1675 
1676         Returns
1677         -------
1678         reindexed : DataArray
1679             Another dataset array, with this array's data but coordinates from
1680             the other object.
1681 
1682         See Also
1683         --------
1684         DataArray.reindex
1685         align
1686         """
1687         return alignment.reindex_like(
1688             self,
1689             other=other,
1690             method=method,
1691             tolerance=tolerance,
1692             copy=copy,
1693             fill_value=fill_value,
1694         )
1695 
1696     def reindex(
1697         self: T_DataArray,
1698         indexers: Mapping[Any, Any] = None,
1699         method: ReindexMethodOptions = None,
1700         tolerance: float | Iterable[float] | None = None,
1701         copy: bool = True,
1702         fill_value=dtypes.NA,
1703         **indexers_kwargs: Any,
1704     ) -> T_DataArray:
1705         """Conform this object onto the indexes of another object, filling in
1706         missing values with ``fill_value``. The default fill value is NaN.
1707 
1708         Parameters
1709         ----------
1710         indexers : dict, optional
1711             Dictionary with keys given by dimension names and values given by
1712             arrays of coordinates tick labels. Any mis-matched coordinate
1713             values will be filled in with NaN, and any mis-matched dimension
1714             names will simply be ignored.
1715             One of indexers or indexers_kwargs must be provided.
1716         copy : bool, optional
1717             If ``copy=True``, data in the return value is always copied. If
1718             ``copy=False`` and reindexing is unnecessary, or can be performed
1719             with only slice operations, then the output may share memory with
1720             the input. In either case, a new xarray object is always returned.
1721         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1722             Method to use for filling index values in ``indexers`` not found on
1723             this data array:
1724 
1725             - None (default): don't fill gaps
1726             - pad / ffill: propagate last valid index value forward
1727             - backfill / bfill: propagate next valid index value backward
1728             - nearest: use nearest valid index value
1729 
1730         tolerance : float | Iterable[float] | None, default: None
1731             Maximum distance between original and new labels for inexact
1732             matches. The values of the index at the matching locations must
1733             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1734             Tolerance may be a scalar value, which applies the same tolerance
1735             to all values, or list-like, which applies variable tolerance per
1736             element. List-like must be the same size as the index and its dtype
1737             must exactly match the index’s type.
1738         fill_value : scalar or dict-like, optional
1739             Value to use for newly missing values. If a dict-like, maps
1740             variable names (including coordinates) to fill values. Use this
1741             data array's name to refer to the data array's values.
1742         **indexers_kwargs : {dim: indexer, ...}, optional
1743             The keyword arguments form of ``indexers``.
1744             One of indexers or indexers_kwargs must be provided.
1745 
1746         Returns
1747         -------
1748         reindexed : DataArray
1749             Another dataset array, with this array's data but replaced
1750             coordinates.
1751 
1752         Examples
1753         --------
1754         Reverse latitude:
1755 
1756         >>> da = xr.DataArray(
1757         ...     np.arange(4),
1758         ...     coords=[np.array([90, 89, 88, 87])],
1759         ...     dims="lat",
1760         ... )
1761         >>> da
1762         <xarray.DataArray (lat: 4)>
1763         array([0, 1, 2, 3])
1764         Coordinates:
1765           * lat      (lat) int64 90 89 88 87
1766         >>> da.reindex(lat=da.lat[::-1])
1767         <xarray.DataArray (lat: 4)>
1768         array([3, 2, 1, 0])
1769         Coordinates:
1770           * lat      (lat) int64 87 88 89 90
1771 
1772         See Also
1773         --------
1774         DataArray.reindex_like
1775         align
1776         """
1777         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1778         return alignment.reindex(
1779             self,
1780             indexers=indexers,
1781             method=method,
1782             tolerance=tolerance,
1783             copy=copy,
1784             fill_value=fill_value,
1785         )
1786 
1787     def interp(
1788         self: T_DataArray,
1789         coords: Mapping[Any, Any] | None = None,
1790         method: InterpOptions = "linear",
1791         assume_sorted: bool = False,
1792         kwargs: Mapping[str, Any] | None = None,
1793         **coords_kwargs: Any,
1794     ) -> T_DataArray:
1795         """Interpolate a DataArray onto new coordinates
1796 
1797         Performs univariate or multivariate interpolation of a DataArray onto
1798         new coordinates using scipy's interpolation routines. If interpolating
1799         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
1800         called. When interpolating along multiple existing dimensions, an
1801         attempt is made to decompose the interpolation into multiple
1802         1-dimensional interpolations. If this is possible,
1803         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1804         :py:func:`scipy.interpolate.interpn` is called.
1805 
1806         Parameters
1807         ----------
1808         coords : dict, optional
1809             Mapping from dimension names to the new coordinates.
1810             New coordinate can be a scalar, array-like or DataArray.
1811             If DataArrays are passed as new coordinates, their dimensions are
1812             used for the broadcasting. Missing values are skipped.
1813         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1814             The method used to interpolate. The method should be supported by
1815             the scipy interpolator:
1816 
1817             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
1818               "quadratic", "cubic", "polynomial"}
1819             - ``interpn``: {"linear", "nearest"}
1820 
1821             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1822             also be provided.
1823         assume_sorted : bool, default: False
1824             If False, values of x can be in any order and they are sorted
1825             first. If True, x has to be an array of monotonically increasing
1826             values.
1827         kwargs : dict-like or None, default: None
1828             Additional keyword arguments passed to scipy's interpolator. Valid
1829             options and their behavior depend whether ``interp1d`` or
1830             ``interpn`` is used.
1831         **coords_kwargs : {dim: coordinate, ...}, optional
1832             The keyword arguments form of ``coords``.
1833             One of coords or coords_kwargs must be provided.
1834 
1835         Returns
1836         -------
1837         interpolated : DataArray
1838             New dataarray on the new coordinates.
1839 
1840         Notes
1841         -----
1842         scipy is required.
1843 
1844         See Also
1845         --------
1846         scipy.interpolate.interp1d
1847         scipy.interpolate.interpn
1848 
1849         Examples
1850         --------
1851         >>> da = xr.DataArray(
1852         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1853         ...     dims=("x", "y"),
1854         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1855         ... )
1856         >>> da
1857         <xarray.DataArray (x: 3, y: 4)>
1858         array([[ 1.,  4.,  2.,  9.],
1859                [ 2.,  7.,  6., nan],
1860                [ 6., nan,  5.,  8.]])
1861         Coordinates:
1862           * x        (x) int64 0 1 2
1863           * y        (y) int64 10 12 14 16
1864 
1865         1D linear interpolation (the default):
1866 
1867         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1868         <xarray.DataArray (x: 4, y: 4)>
1869         array([[1.  , 4.  , 2.  ,  nan],
1870                [1.75, 6.25, 5.  ,  nan],
1871                [3.  ,  nan, 5.75,  nan],
1872                [5.  ,  nan, 5.25,  nan]])
1873         Coordinates:
1874           * y        (y) int64 10 12 14 16
1875           * x        (x) float64 0.0 0.75 1.25 1.75
1876 
1877         1D nearest interpolation:
1878 
1879         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1880         <xarray.DataArray (x: 4, y: 4)>
1881         array([[ 1.,  4.,  2.,  9.],
1882                [ 2.,  7.,  6., nan],
1883                [ 2.,  7.,  6., nan],
1884                [ 6., nan,  5.,  8.]])
1885         Coordinates:
1886           * y        (y) int64 10 12 14 16
1887           * x        (x) float64 0.0 0.75 1.25 1.75
1888 
1889         1D linear extrapolation:
1890 
1891         >>> da.interp(
1892         ...     x=[1, 1.5, 2.5, 3.5],
1893         ...     method="linear",
1894         ...     kwargs={"fill_value": "extrapolate"},
1895         ... )
1896         <xarray.DataArray (x: 4, y: 4)>
1897         array([[ 2. ,  7. ,  6. ,  nan],
1898                [ 4. ,  nan,  5.5,  nan],
1899                [ 8. ,  nan,  4.5,  nan],
1900                [12. ,  nan,  3.5,  nan]])
1901         Coordinates:
1902           * y        (y) int64 10 12 14 16
1903           * x        (x) float64 1.0 1.5 2.5 3.5
1904 
1905         2D linear interpolation:
1906 
1907         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1908         <xarray.DataArray (x: 4, y: 3)>
1909         array([[2.5  , 3.   ,   nan],
1910                [4.   , 5.625,   nan],
1911                [  nan,   nan,   nan],
1912                [  nan,   nan,   nan]])
1913         Coordinates:
1914           * x        (x) float64 0.0 0.75 1.25 1.75
1915           * y        (y) int64 11 13 15
1916         """
1917         if self.dtype.kind not in "uifc":
1918             raise TypeError(
1919                 "interp only works for a numeric type array. "
1920                 "Given {}.".format(self.dtype)
1921             )
1922         ds = self._to_temp_dataset().interp(
1923             coords,
1924             method=method,
1925             kwargs=kwargs,
1926             assume_sorted=assume_sorted,
1927             **coords_kwargs,
1928         )
1929         return self._from_temp_dataset(ds)
1930 
1931     def interp_like(
1932         self: T_DataArray,
1933         other: DataArray | Dataset,
1934         method: InterpOptions = "linear",
1935         assume_sorted: bool = False,
1936         kwargs: Mapping[str, Any] | None = None,
1937     ) -> T_DataArray:
1938         """Interpolate this object onto the coordinates of another object,
1939         filling out of range values with NaN.
1940 
1941         If interpolating along a single existing dimension,
1942         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
1943         along multiple existing dimensions, an attempt is made to decompose the
1944         interpolation into multiple 1-dimensional interpolations. If this is
1945         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1946         :py:func:`scipy.interpolate.interpn` is called.
1947 
1948         Parameters
1949         ----------
1950         other : Dataset or DataArray
1951             Object with an 'indexes' attribute giving a mapping from dimension
1952             names to an 1d array-like, which provides coordinates upon
1953             which to index the variables in this dataset. Missing values are skipped.
1954         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1955             The method used to interpolate. The method should be supported by
1956             the scipy interpolator:
1957 
1958             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
1959               "polynomial"} when ``interp1d`` is called.
1960             - {"linear", "nearest"} when ``interpn`` is called.
1961 
1962             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1963             also be provided.
1964         assume_sorted : bool, default: False
1965             If False, values of coordinates that are interpolated over can be
1966             in any order and they are sorted first. If True, interpolated
1967             coordinates are assumed to be an array of monotonically increasing
1968             values.
1969         kwargs : dict, optional
1970             Additional keyword passed to scipy's interpolator.
1971 
1972         Returns
1973         -------
1974         interpolated : DataArray
1975             Another dataarray by interpolating this dataarray's data along the
1976             coordinates of the other object.
1977 
1978         Notes
1979         -----
1980         scipy is required.
1981         If the dataarray has object-type coordinates, reindex is used for these
1982         coordinates instead of the interpolation.
1983 
1984         See Also
1985         --------
1986         DataArray.interp
1987         DataArray.reindex_like
1988         """
1989         if self.dtype.kind not in "uifc":
1990             raise TypeError(
1991                 "interp only works for a numeric type array. "
1992                 "Given {}.".format(self.dtype)
1993             )
1994         ds = self._to_temp_dataset().interp_like(
1995             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1996         )
1997         return self._from_temp_dataset(ds)
1998 
1999     # change type of self and return to T_DataArray once
2000     # https://github.com/python/mypy/issues/12846 is resolved
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)
2044 
2045     def swap_dims(
2046         self: T_DataArray,
2047         dims_dict: Mapping[Any, Hashable] | None = None,
2048         **dims_kwargs,
2049     ) -> T_DataArray:
2050         """Returns a new DataArray with swapped dimensions.
2051 
2052         Parameters
2053         ----------
2054         dims_dict : dict-like
2055             Dictionary whose keys are current dimension names and whose values
2056             are new names.
2057         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2058             The keyword arguments form of ``dims_dict``.
2059             One of dims_dict or dims_kwargs must be provided.
2060 
2061         Returns
2062         -------
2063         swapped : DataArray
2064             DataArray with swapped dimensions.
2065 
2066         Examples
2067         --------
2068         >>> arr = xr.DataArray(
2069         ...     data=[0, 1],
2070         ...     dims="x",
2071         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2072         ... )
2073         >>> arr
2074         <xarray.DataArray (x: 2)>
2075         array([0, 1])
2076         Coordinates:
2077           * x        (x) <U1 'a' 'b'
2078             y        (x) int64 0 1
2079 
2080         >>> arr.swap_dims({"x": "y"})
2081         <xarray.DataArray (y: 2)>
2082         array([0, 1])
2083         Coordinates:
2084             x        (y) <U1 'a' 'b'
2085           * y        (y) int64 0 1
2086 
2087         >>> arr.swap_dims({"x": "z"})
2088         <xarray.DataArray (z: 2)>
2089         array([0, 1])
2090         Coordinates:
2091             x        (z) <U1 'a' 'b'
2092             y        (z) int64 0 1
2093         Dimensions without coordinates: z
2094 
2095         See Also
2096         --------
2097         DataArray.rename
2098         Dataset.swap_dims
2099         """
2100         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2101         ds = self._to_temp_dataset().swap_dims(dims_dict)
2102         return self._from_temp_dataset(ds)
2103 
2104     # change type of self and return to T_DataArray once
2105     # https://github.com/python/mypy/issues/12846 is resolved
2106     def expand_dims(
2107         self,
2108         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2109         axis: None | int | Sequence[int] = None,
2110         **dim_kwargs: Any,
2111     ) -> DataArray:
2112         """Return a new object with an additional axis (or axes) inserted at
2113         the corresponding position in the array shape. The new object is a
2114         view into the underlying array, not a copy.
2115 
2116         If dim is already a scalar coordinate, it will be promoted to a 1D
2117         coordinate consisting of a single value.
2118 
2119         Parameters
2120         ----------
2121         dim : Hashable, sequence of Hashable, dict, or None, optional
2122             Dimensions to include on the new variable.
2123             If provided as str or sequence of str, then dimensions are inserted
2124             with length 1. If provided as a dict, then the keys are the new
2125             dimensions and the values are either integers (giving the length of
2126             the new dimensions) or sequence/ndarray (giving the coordinates of
2127             the new dimensions).
2128         axis : int, sequence of int, or None, default: None
2129             Axis position(s) where new axis is to be inserted (position(s) on
2130             the result array). If a sequence of integers is passed,
2131             multiple axes are inserted. In this case, dim arguments should be
2132             same length list. If axis=None is passed, all the axes will be
2133             inserted to the start of the result array.
2134         **dim_kwargs : int or sequence or ndarray
2135             The keywords are arbitrary dimensions being inserted and the values
2136             are either the lengths of the new dims (if int is given), or their
2137             coordinates. Note, this is an alternative to passing a dict to the
2138             dim kwarg and will only be used if dim is None.
2139 
2140         Returns
2141         -------
2142         expanded : DataArray
2143             This object, but with additional dimension(s).
2144 
2145         See Also
2146         --------
2147         Dataset.expand_dims
2148         """
2149         if isinstance(dim, int):
2150             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2151         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2152             if len(dim) != len(set(dim)):
2153                 raise ValueError("dims should not contain duplicate values.")
2154             dim = dict.fromkeys(dim, 1)
2155         elif dim is not None and not isinstance(dim, Mapping):
2156             dim = {cast(Hashable, dim): 1}
2157 
2158         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2159         ds = self._to_temp_dataset().expand_dims(dim, axis)
2160         return self._from_temp_dataset(ds)
2161 
2162     # change type of self and return to T_DataArray once
2163     # https://github.com/python/mypy/issues/12846 is resolved
2164     def set_index(
2165         self,
2166         indexes: Mapping[Any, Hashable | Sequence[Hashable]] = None,
2167         append: bool = False,
2168         **indexes_kwargs: Hashable | Sequence[Hashable],
2169     ) -> DataArray:
2170         """Set DataArray (multi-)indexes using one or more existing
2171         coordinates.
2172 
2173         Parameters
2174         ----------
2175         indexes : {dim: index, ...}
2176             Mapping from names matching dimensions and values given
2177             by (lists of) the names of existing coordinates or variables to set
2178             as new (multi-)index.
2179         append : bool, default: False
2180             If True, append the supplied index(es) to the existing index(es).
2181             Otherwise replace the existing index(es).
2182         **indexes_kwargs : optional
2183             The keyword arguments form of ``indexes``.
2184             One of indexes or indexes_kwargs must be provided.
2185 
2186         Returns
2187         -------
2188         obj : DataArray
2189             Another DataArray, with this data but replaced coordinates.
2190 
2191         Examples
2192         --------
2193         >>> arr = xr.DataArray(
2194         ...     data=np.ones((2, 3)),
2195         ...     dims=["x", "y"],
2196         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2197         ... )
2198         >>> arr
2199         <xarray.DataArray (x: 2, y: 3)>
2200         array([[1., 1., 1.],
2201                [1., 1., 1.]])
2202         Coordinates:
2203           * x        (x) int64 0 1
2204           * y        (y) int64 0 1 2
2205             a        (x) int64 3 4
2206         >>> arr.set_index(x="a")
2207         <xarray.DataArray (x: 2, y: 3)>
2208         array([[1., 1., 1.],
2209                [1., 1., 1.]])
2210         Coordinates:
2211           * x        (x) int64 3 4
2212           * y        (y) int64 0 1 2
2213 
2214         See Also
2215         --------
2216         DataArray.reset_index
2217         """
2218         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2219         return self._from_temp_dataset(ds)
2220 
2221     # change type of self and return to T_DataArray once
2222     # https://github.com/python/mypy/issues/12846 is resolved
2223     def reset_index(
2224         self,
2225         dims_or_levels: Hashable | Sequence[Hashable],
2226         drop: bool = False,
2227     ) -> DataArray:
2228         """Reset the specified index(es) or multi-index level(s).
2229 
2230         Parameters
2231         ----------
2232         dims_or_levels : Hashable or sequence of Hashable
2233             Name(s) of the dimension(s) and/or multi-index level(s) that will
2234             be reset.
2235         drop : bool, default: False
2236             If True, remove the specified indexes and/or multi-index levels
2237             instead of extracting them as new coordinates (default: False).
2238 
2239         Returns
2240         -------
2241         obj : DataArray
2242             Another dataarray, with this dataarray's data but replaced
2243             coordinates.
2244 
2245         See Also
2246         --------
2247         DataArray.set_index
2248         """
2249         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2250         return self._from_temp_dataset(ds)
2251 
2252     def reorder_levels(
2253         self: T_DataArray,
2254         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2255         **dim_order_kwargs: Sequence[int | Hashable],
2256     ) -> T_DataArray:
2257         """Rearrange index levels using input order.
2258 
2259         Parameters
2260         ----------
2261         dim_order dict-like of Hashable to int or Hashable: optional
2262             Mapping from names matching dimensions and values given
2263             by lists representing new level orders. Every given dimension
2264             must have a multi-index.
2265         **dim_order_kwargs : optional
2266             The keyword arguments form of ``dim_order``.
2267             One of dim_order or dim_order_kwargs must be provided.
2268 
2269         Returns
2270         -------
2271         obj : DataArray
2272             Another dataarray, with this dataarray's data but replaced
2273             coordinates.
2274         """
2275         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2276         return self._from_temp_dataset(ds)
2277 
2278     def stack(
2279         self: T_DataArray,
2280         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2281         create_index: bool | None = True,
2282         index_cls: type[Index] = PandasMultiIndex,
2283         **dimensions_kwargs: Sequence[Hashable],
2284     ) -> T_DataArray:
2285         """
2286         Stack any number of existing dimensions into a single new dimension.
2287 
2288         New dimensions will be added at the end, and the corresponding
2289         coordinate variables will be combined into a MultiIndex.
2290 
2291         Parameters
2292         ----------
2293         dimensions : mapping of Hashable to sequence of Hashable
2294             Mapping of the form `new_name=(dim1, dim2, ...)`.
2295             Names of new dimensions, and the existing dimensions that they
2296             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2297             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2298             all dimensions.
2299         create_index : bool or None, default: True
2300             If True, create a multi-index for each of the stacked dimensions.
2301             If False, don't create any index.
2302             If None, create a multi-index only if exactly one single (1-d) coordinate
2303             index is found for every dimension to stack.
2304         index_cls: class, optional
2305             Can be used to pass a custom multi-index type. Must be an Xarray index that
2306             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2307         **dimensions_kwargs
2308             The keyword arguments form of ``dimensions``.
2309             One of dimensions or dimensions_kwargs must be provided.
2310 
2311         Returns
2312         -------
2313         stacked : DataArray
2314             DataArray with stacked data.
2315 
2316         Examples
2317         --------
2318         >>> arr = xr.DataArray(
2319         ...     np.arange(6).reshape(2, 3),
2320         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2321         ... )
2322         >>> arr
2323         <xarray.DataArray (x: 2, y: 3)>
2324         array([[0, 1, 2],
2325                [3, 4, 5]])
2326         Coordinates:
2327           * x        (x) <U1 'a' 'b'
2328           * y        (y) int64 0 1 2
2329         >>> stacked = arr.stack(z=("x", "y"))
2330         >>> stacked.indexes["z"]
2331         MultiIndex([('a', 0),
2332                     ('a', 1),
2333                     ('a', 2),
2334                     ('b', 0),
2335                     ('b', 1),
2336                     ('b', 2)],
2337                    name='z')
2338 
2339         See Also
2340         --------
2341         DataArray.unstack
2342         """
2343         ds = self._to_temp_dataset().stack(
2344             dimensions,
2345             create_index=create_index,
2346             index_cls=index_cls,
2347             **dimensions_kwargs,
2348         )
2349         return self._from_temp_dataset(ds)
2350 
2351     # change type of self and return to T_DataArray once
2352     # https://github.com/python/mypy/issues/12846 is resolved
2353     def unstack(
2354         self,
2355         dim: Hashable | Sequence[Hashable] | None = None,
2356         fill_value: Any = dtypes.NA,
2357         sparse: bool = False,
2358     ) -> DataArray:
2359         """
2360         Unstack existing dimensions corresponding to MultiIndexes into
2361         multiple new dimensions.
2362 
2363         New dimensions will be added at the end.
2364 
2365         Parameters
2366         ----------
2367         dim : Hashable or sequence of Hashable, optional
2368             Dimension(s) over which to unstack. By default unstacks all
2369             MultiIndexes.
2370         fill_value : scalar or dict-like, default: nan
2371             Value to be filled. If a dict-like, maps variable names to
2372             fill values. Use the data array's name to refer to its
2373             name. If not provided or if the dict-like does not contain
2374             all variables, the dtype's NA value will be used.
2375         sparse : bool, default: False
2376             Use sparse-array if True
2377 
2378         Returns
2379         -------
2380         unstacked : DataArray
2381             Array with unstacked data.
2382 
2383         Examples
2384         --------
2385         >>> arr = xr.DataArray(
2386         ...     np.arange(6).reshape(2, 3),
2387         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2388         ... )
2389         >>> arr
2390         <xarray.DataArray (x: 2, y: 3)>
2391         array([[0, 1, 2],
2392                [3, 4, 5]])
2393         Coordinates:
2394           * x        (x) <U1 'a' 'b'
2395           * y        (y) int64 0 1 2
2396         >>> stacked = arr.stack(z=("x", "y"))
2397         >>> stacked.indexes["z"]
2398         MultiIndex([('a', 0),
2399                     ('a', 1),
2400                     ('a', 2),
2401                     ('b', 0),
2402                     ('b', 1),
2403                     ('b', 2)],
2404                    name='z')
2405         >>> roundtripped = stacked.unstack()
2406         >>> arr.identical(roundtripped)
2407         True
2408 
2409         See Also
2410         --------
2411         DataArray.stack
2412         """
2413         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2414         return self._from_temp_dataset(ds)
2415 
2416     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2417         """Unstack DataArray expanding to Dataset along a given level of a
2418         stacked coordinate.
2419 
2420         This is the inverse operation of Dataset.to_stacked_array.
2421 
2422         Parameters
2423         ----------
2424         dim : Hashable
2425             Name of existing dimension to unstack
2426         level : int or Hashable, default: 0
2427             The MultiIndex level to expand to a dataset along. Can either be
2428             the integer index of the level or its name.
2429 
2430         Returns
2431         -------
2432         unstacked: Dataset
2433 
2434         Examples
2435         --------
2436         >>> arr = xr.DataArray(
2437         ...     np.arange(6).reshape(2, 3),
2438         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2439         ... )
2440         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2441         >>> data
2442         <xarray.Dataset>
2443         Dimensions:  (x: 2, y: 3)
2444         Coordinates:
2445           * x        (x) <U1 'a' 'b'
2446           * y        (y) int64 0 1 2
2447         Data variables:
2448             a        (x, y) int64 0 1 2 3 4 5
2449             b        (x) int64 0 3
2450         >>> stacked = data.to_stacked_array("z", ["x"])
2451         >>> stacked.indexes["z"]
2452         MultiIndex([('a', 0.0),
2453                     ('a', 1.0),
2454                     ('a', 2.0),
2455                     ('b', nan)],
2456                    name='z')
2457         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2458         >>> data.identical(roundtripped)
2459         True
2460 
2461         See Also
2462         --------
2463         Dataset.to_stacked_array
2464         """
2465         idx = self._indexes[dim].to_pandas_index()
2466         if not isinstance(idx, pd.MultiIndex):
2467             raise ValueError(f"'{dim}' is not a stacked coordinate")
2468 
2469         level_number = idx._get_level_number(level)
2470         variables = idx.levels[level_number]
2471         variable_dim = idx.names[level_number]
2472 
2473         # pull variables out of datarray
2474         data_dict = {}
2475         for k in variables:
2476             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2477 
2478         # unstacked dataset
2479         return Dataset(data_dict)
2480 
2481     def transpose(
2482         self: T_DataArray,
2483         *dims: Hashable,
2484         transpose_coords: bool = True,
2485         missing_dims: ErrorOptionsWithWarn = "raise",
2486     ) -> T_DataArray:
2487         """Return a new DataArray object with transposed dimensions.
2488 
2489         Parameters
2490         ----------
2491         *dims : Hashable, optional
2492             By default, reverse the dimensions. Otherwise, reorder the
2493             dimensions to this order.
2494         transpose_coords : bool, default: True
2495             If True, also transpose the coordinates of this DataArray.
2496         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2497             What to do if dimensions that should be selected from are not present in the
2498             DataArray:
2499             - "raise": raise an exception
2500             - "warn": raise a warning, and ignore the missing dimensions
2501             - "ignore": ignore the missing dimensions
2502 
2503         Returns
2504         -------
2505         transposed : DataArray
2506             The returned DataArray's array is transposed.
2507 
2508         Notes
2509         -----
2510         This operation returns a view of this array's data. It is
2511         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2512         -- the data will be fully loaded.
2513 
2514         See Also
2515         --------
2516         numpy.transpose
2517         Dataset.transpose
2518         """
2519         if dims:
2520             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2521         variable = self.variable.transpose(*dims)
2522         if transpose_coords:
2523             coords: dict[Hashable, Variable] = {}
2524             for name, coord in self.coords.items():
2525                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2526                 coords[name] = coord.variable.transpose(*coord_dims)
2527             return self._replace(variable, coords)
2528         else:
2529             return self._replace(variable)
2530 
2531     @property
2532     def T(self: T_DataArray) -> T_DataArray:
2533         return self.transpose()
2534 
2535     # change type of self and return to T_DataArray once
2536     # https://github.com/python/mypy/issues/12846 is resolved
2537     def drop_vars(
2538         self,
2539         names: Hashable | Iterable[Hashable],
2540         *,
2541         errors: ErrorOptions = "raise",
2542     ) -> DataArray:
2543         """Returns an array with dropped variables.
2544 
2545         Parameters
2546         ----------
2547         names : Hashable or iterable of Hashable
2548             Name(s) of variables to drop.
2549         errors : {"raise", "ignore"}, default: "raise"
2550             If 'raise', raises a ValueError error if any of the variable
2551             passed are not in the dataset. If 'ignore', any given names that are in the
2552             DataArray are dropped and no error is raised.
2553 
2554         Returns
2555         -------
2556         dropped : Dataset
2557             New Dataset copied from `self` with variables removed.
2558         """
2559         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2560         return self._from_temp_dataset(ds)
2561 
2562     def drop(
2563         self: T_DataArray,
2564         labels: Mapping[Any, Any] | None = None,
2565         dim: Hashable | None = None,
2566         *,
2567         errors: ErrorOptions = "raise",
2568         **labels_kwargs,
2569     ) -> T_DataArray:
2570         """Backward compatible method based on `drop_vars` and `drop_sel`
2571 
2572         Using either `drop_vars` or `drop_sel` is encouraged
2573 
2574         See Also
2575         --------
2576         DataArray.drop_vars
2577         DataArray.drop_sel
2578         """
2579         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
2580         return self._from_temp_dataset(ds)
2581 
2582     def drop_sel(
2583         self: T_DataArray,
2584         labels: Mapping[Any, Any] | None = None,
2585         *,
2586         errors: ErrorOptions = "raise",
2587         **labels_kwargs,
2588     ) -> T_DataArray:
2589         """Drop index labels from this DataArray.
2590 
2591         Parameters
2592         ----------
2593         labels : mapping of Hashable to Any
2594             Index labels to drop
2595         errors : {"raise", "ignore"}, default: "raise"
2596             If 'raise', raises a ValueError error if
2597             any of the index labels passed are not
2598             in the dataset. If 'ignore', any given labels that are in the
2599             dataset are dropped and no error is raised.
2600         **labels_kwargs : {dim: label, ...}, optional
2601             The keyword arguments form of ``dim`` and ``labels``
2602 
2603         Returns
2604         -------
2605         dropped : DataArray
2606         """
2607         if labels_kwargs or isinstance(labels, dict):
2608             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2609 
2610         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2611         return self._from_temp_dataset(ds)
2612 
2613     def drop_isel(
2614         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
2615     ) -> T_DataArray:
2616         """Drop index positions from this DataArray.
2617 
2618         Parameters
2619         ----------
2620         indexers : mapping of Hashable to Any or None, default: None
2621             Index locations to drop
2622         **indexers_kwargs : {dim: position, ...}, optional
2623             The keyword arguments form of ``dim`` and ``positions``
2624 
2625         Returns
2626         -------
2627         dropped : DataArray
2628 
2629         Raises
2630         ------
2631         IndexError
2632         """
2633         dataset = self._to_temp_dataset()
2634         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2635         return self._from_temp_dataset(dataset)
2636 
2637     def dropna(
2638         self: T_DataArray,
2639         dim: Hashable,
2640         how: Literal["any", "all"] = "any",
2641         thresh: int | None = None,
2642     ) -> T_DataArray:
2643         """Returns a new array with dropped labels for missing values along
2644         the provided dimension.
2645 
2646         Parameters
2647         ----------
2648         dim : Hashable
2649             Dimension along which to drop missing values. Dropping along
2650             multiple dimensions simultaneously is not yet supported.
2651         how : {"any", "all"}, default: "any"
2652             - any : if any NA values are present, drop that label
2653             - all : if all values are NA, drop that label
2654 
2655         thresh : int or None, default: None
2656             If supplied, require this many non-NA values.
2657 
2658         Returns
2659         -------
2660         dropped : DataArray
2661         """
2662         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2663         return self._from_temp_dataset(ds)
2664 
2665     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
2666         """Fill missing values in this object.
2667 
2668         This operation follows the normal broadcasting and alignment rules that
2669         xarray uses for binary arithmetic, except the result is aligned to this
2670         object (``join='left'``) instead of aligned to the intersection of
2671         index coordinates (``join='inner'``).
2672 
2673         Parameters
2674         ----------
2675         value : scalar, ndarray or DataArray
2676             Used to fill all matching missing values in this array. If the
2677             argument is a DataArray, it is first aligned with (reindexed to)
2678             this array.
2679 
2680         Returns
2681         -------
2682         filled : DataArray
2683         """
2684         if utils.is_dict_like(value):
2685             raise TypeError(
2686                 "cannot provide fill value as a dictionary with "
2687                 "fillna on a DataArray"
2688             )
2689         out = ops.fillna(self, value)
2690         return out
2691 
2692     def interpolate_na(
2693         self: T_DataArray,
2694         dim: Hashable | None = None,
2695         method: InterpOptions = "linear",
2696         limit: int | None = None,
2697         use_coordinate: bool | str = True,
2698         max_gap: (
2699             None
2700             | int
2701             | float
2702             | str
2703             | pd.Timedelta
2704             | np.timedelta64
2705             | datetime.timedelta
2706         ) = None,
2707         keep_attrs: bool | None = None,
2708         **kwargs: Any,
2709     ) -> T_DataArray:
2710         """Fill in NaNs by interpolating according to different methods.
2711 
2712         Parameters
2713         ----------
2714         dim : Hashable or None, optional
2715             Specifies the dimension along which to interpolate.
2716         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
2717             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
2718             String indicating which method to use for interpolation:
2719 
2720             - 'linear': linear interpolation. Additional keyword
2721               arguments are passed to :py:func:`numpy.interp`
2722             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2723               are passed to :py:func:`scipy.interpolate.interp1d`. If
2724               ``method='polynomial'``, the ``order`` keyword argument must also be
2725               provided.
2726             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2727               respective :py:class:`scipy.interpolate` classes.
2728 
2729         use_coordinate : bool or str, default: True
2730             Specifies which index to use as the x values in the interpolation
2731             formulated as `y = f(x)`. If False, values are treated as if
2732             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2733             used. If ``use_coordinate`` is a string, it specifies the name of a
2734             coordinate variariable to use as the index.
2735         limit : int or None, default: None
2736             Maximum number of consecutive NaNs to fill. Must be greater than 0
2737             or None for no limit. This filling is done regardless of the size of
2738             the gap in the data. To only interpolate over gaps less than a given length,
2739             see ``max_gap``.
2740         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2741             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2742             Use None for no limit. When interpolating along a datetime64 dimension
2743             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2744 
2745             - a string that is valid input for pandas.to_timedelta
2746             - a :py:class:`numpy.timedelta64` object
2747             - a :py:class:`pandas.Timedelta` object
2748             - a :py:class:`datetime.timedelta` object
2749 
2750             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2751             dimensions has not been implemented yet. Gap length is defined as the difference
2752             between coordinate values at the first data point after a gap and the last value
2753             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2754             between coordinate values at the first (last) valid data point and the first (last) NaN.
2755             For example, consider::
2756 
2757                 <xarray.DataArray (x: 9)>
2758                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2759                 Coordinates:
2760                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2761 
2762             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2763         keep_attrs : bool or None, default: None
2764             If True, the dataarray's attributes (`attrs`) will be copied from
2765             the original object to the new one.  If False, the new
2766             object will be returned without attributes.
2767         **kwargs : dict, optional
2768             parameters passed verbatim to the underlying interpolation function
2769 
2770         Returns
2771         -------
2772         interpolated: DataArray
2773             Filled in DataArray.
2774 
2775         See Also
2776         --------
2777         numpy.interp
2778         scipy.interpolate
2779 
2780         Examples
2781         --------
2782         >>> da = xr.DataArray(
2783         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2784         ... )
2785         >>> da
2786         <xarray.DataArray (x: 5)>
2787         array([nan,  2.,  3., nan,  0.])
2788         Coordinates:
2789           * x        (x) int64 0 1 2 3 4
2790 
2791         >>> da.interpolate_na(dim="x", method="linear")
2792         <xarray.DataArray (x: 5)>
2793         array([nan, 2. , 3. , 1.5, 0. ])
2794         Coordinates:
2795           * x        (x) int64 0 1 2 3 4
2796 
2797         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2798         <xarray.DataArray (x: 5)>
2799         array([1. , 2. , 3. , 1.5, 0. ])
2800         Coordinates:
2801           * x        (x) int64 0 1 2 3 4
2802         """
2803         from .missing import interp_na
2804 
2805         return interp_na(
2806             self,
2807             dim=dim,
2808             method=method,
2809             limit=limit,
2810             use_coordinate=use_coordinate,
2811             max_gap=max_gap,
2812             keep_attrs=keep_attrs,
2813             **kwargs,
2814         )
2815 
2816     def ffill(
2817         self: T_DataArray, dim: Hashable, limit: int | None = None
2818     ) -> T_DataArray:
2819         """Fill NaN values by propagating values forward
2820 
2821         *Requires bottleneck.*
2822 
2823         Parameters
2824         ----------
2825         dim : Hashable
2826             Specifies the dimension along which to propagate values when
2827             filling.
2828         limit : int or None, default: None
2829             The maximum number of consecutive NaN values to forward fill. In
2830             other words, if there is a gap with more than this number of
2831             consecutive NaNs, it will only be partially filled. Must be greater
2832             than 0 or None for no limit. Must be None or greater than or equal
2833             to axis length if filling along chunked axes (dimensions).
2834 
2835         Returns
2836         -------
2837         filled : DataArray
2838         """
2839         from .missing import ffill
2840 
2841         return ffill(self, dim, limit=limit)
2842 
2843     def bfill(
2844         self: T_DataArray, dim: Hashable, limit: int | None = None
2845     ) -> T_DataArray:
2846         """Fill NaN values by propagating values backward
2847 
2848         *Requires bottleneck.*
2849 
2850         Parameters
2851         ----------
2852         dim : str
2853             Specifies the dimension along which to propagate values when
2854             filling.
2855         limit : int or None, default: None
2856             The maximum number of consecutive NaN values to backward fill. In
2857             other words, if there is a gap with more than this number of
2858             consecutive NaNs, it will only be partially filled. Must be greater
2859             than 0 or None for no limit. Must be None or greater than or equal
2860             to axis length if filling along chunked axes (dimensions).
2861 
2862         Returns
2863         -------
2864         filled : DataArray
2865         """
2866         from .missing import bfill
2867 
2868         return bfill(self, dim, limit=limit)
2869 
2870     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
2871         """Combine two DataArray objects, with union of coordinates.
2872 
2873         This operation follows the normal broadcasting and alignment rules of
2874         ``join='outer'``.  Default to non-null values of array calling the
2875         method.  Use np.nan to fill in vacant cells after alignment.
2876 
2877         Parameters
2878         ----------
2879         other : DataArray
2880             Used to fill all matching missing values in this array.
2881 
2882         Returns
2883         -------
2884         DataArray
2885         """
2886         return ops.fillna(self, other, join="outer")
2887 
2888     def reduce(
2889         self: T_DataArray,
2890         func: Callable[..., Any],
2891         dim: None | Hashable | Iterable[Hashable] = None,
2892         *,
2893         axis: None | int | Sequence[int] = None,
2894         keep_attrs: bool | None = None,
2895         keepdims: bool = False,
2896         **kwargs: Any,
2897     ) -> T_DataArray:
2898         """Reduce this array by applying `func` along some dimension(s).
2899 
2900         Parameters
2901         ----------
2902         func : callable
2903             Function which can be called in the form
2904             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2905             np.ndarray over an integer valued axis.
2906         dim : Hashable or Iterable of Hashable, optional
2907             Dimension(s) over which to apply `func`.
2908         axis : int or sequence of int, optional
2909             Axis(es) over which to repeatedly apply `func`. Only one of the
2910             'dim' and 'axis' arguments can be supplied. If neither are
2911             supplied, then the reduction is calculated over the flattened array
2912             (by calling `f(x)` without an axis argument).
2913         keep_attrs : bool or None, optional
2914             If True, the variable's attributes (`attrs`) will be copied from
2915             the original object to the new one.  If False (default), the new
2916             object will be returned without attributes.
2917         keepdims : bool, default: False
2918             If True, the dimensions which are reduced are left in the result
2919             as dimensions of size one. Coordinates that use these dimensions
2920             are removed.
2921         **kwargs : dict
2922             Additional keyword arguments passed on to `func`.
2923 
2924         Returns
2925         -------
2926         reduced : DataArray
2927             DataArray with this object's array replaced with an array with
2928             summarized data and the indicated dimension(s) removed.
2929         """
2930 
2931         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2932         return self._replace_maybe_drop_dims(var)
2933 
2934     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
2935         """Convert this array into a pandas object with the same shape.
2936 
2937         The type of the returned object depends on the number of DataArray
2938         dimensions:
2939 
2940         * 0D -> `xarray.DataArray`
2941         * 1D -> `pandas.Series`
2942         * 2D -> `pandas.DataFrame`
2943 
2944         Only works for arrays with 2 or fewer dimensions.
2945 
2946         The DataArray constructor performs the inverse transformation.
2947 
2948         Returns
2949         -------
2950         result : DataArray | Series | DataFrame
2951             DataArray, pandas Series or pandas DataFrame.
2952         """
2953         # TODO: consolidate the info about pandas constructors and the
2954         # attributes that correspond to their indexes into a separate module?
2955         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2956         try:
2957             constructor = constructors[self.ndim]
2958         except KeyError:
2959             raise ValueError(
2960                 f"Cannot convert arrays with {self.ndim} dimensions into "
2961                 "pandas objects. Requires 2 or fewer dimensions."
2962             )
2963         indexes = [self.get_index(dim) for dim in self.dims]
2964         return constructor(self.values, *indexes)
2965 
2966     def to_dataframe(
2967         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
2968     ) -> pd.DataFrame:
2969         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2970 
2971         The DataFrame is indexed by the Cartesian product of index coordinates
2972         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
2973         included as columns in the DataFrame.
2974 
2975         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
2976         doesn't rely on a MultiIndex to build the DataFrame.
2977 
2978         Parameters
2979         ----------
2980         name: Hashable or None, optional
2981             Name to give to this array (required if unnamed).
2982         dim_order: Sequence of Hashable or None, optional
2983             Hierarchical dimension order for the resulting dataframe.
2984             Array content is transposed to this order and then written out as flat
2985             vectors in contiguous order, so the last dimension in this list
2986             will be contiguous in the resulting DataFrame. This has a major
2987             influence on which operations are efficient on the resulting
2988             dataframe.
2989 
2990             If provided, must include all dimensions of this DataArray. By default,
2991             dimensions are sorted according to the DataArray dimensions order.
2992 
2993         Returns
2994         -------
2995         result: DataFrame
2996             DataArray as a pandas DataFrame.
2997 
2998         See also
2999         --------
3000         DataArray.to_pandas
3001         DataArray.to_series
3002         """
3003         if name is None:
3004             name = self.name
3005         if name is None:
3006             raise ValueError(
3007                 "cannot convert an unnamed DataArray to a "
3008                 "DataFrame: use the ``name`` parameter"
3009             )
3010         if self.ndim == 0:
3011             raise ValueError("cannot convert a scalar to a DataFrame")
3012 
3013         # By using a unique name, we can convert a DataArray into a DataFrame
3014         # even if it shares a name with one of its coordinates.
3015         # I would normally use unique_name = object() but that results in a
3016         # dataframe with columns in the wrong order, for reasons I have not
3017         # been able to debug (possibly a pandas bug?).
3018         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3019         ds = self._to_dataset_whole(name=unique_name)
3020 
3021         if dim_order is None:
3022             ordered_dims = dict(zip(self.dims, self.shape))
3023         else:
3024             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3025 
3026         df = ds._to_dataframe(ordered_dims)
3027         df.columns = [name if c == unique_name else c for c in df.columns]
3028         return df
3029 
3030     def to_series(self) -> pd.Series:
3031         """Convert this array into a pandas.Series.
3032 
3033         The Series is indexed by the Cartesian product of index coordinates
3034         (in the form of a :py:class:`pandas.MultiIndex`).
3035 
3036         Returns
3037         -------
3038         result : Series
3039             DataArray as a pandas Series.
3040 
3041         See also
3042         --------
3043         DataArray.to_pandas
3044         DataArray.to_dataframe
3045         """
3046         index = self.coords.to_index()
3047         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3048 
3049     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3050         """Convert this array into a numpy.ma.MaskedArray
3051 
3052         Parameters
3053         ----------
3054         copy : bool, default: True
3055             If True make a copy of the array in the result. If False,
3056             a MaskedArray view of DataArray.values is returned.
3057 
3058         Returns
3059         -------
3060         result : MaskedArray
3061             Masked where invalid values (nan or inf) occur.
3062         """
3063         values = self.to_numpy()  # only compute lazy arrays once
3064         isnull = pd.isnull(values)
3065         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3066 
3067     # path=None writes to bytes
3068     @overload
3069     def to_netcdf(
3070         self,
3071         path: None = None,
3072         mode: Literal["w", "a"] = "w",
3073         format: T_NetcdfTypes | None = None,
3074         group: str | None = None,
3075         engine: T_NetcdfEngine | None = None,
3076         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3077         unlimited_dims: Iterable[Hashable] | None = None,
3078         compute: bool = True,
3079         invalid_netcdf: bool = False,
3080     ) -> bytes:
3081         ...
3082 
3083     # default return None
3084     @overload
3085     def to_netcdf(
3086         self,
3087         path: str | PathLike,
3088         mode: Literal["w", "a"] = "w",
3089         format: T_NetcdfTypes | None = None,
3090         group: str | None = None,
3091         engine: T_NetcdfEngine | None = None,
3092         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3093         unlimited_dims: Iterable[Hashable] | None = None,
3094         compute: Literal[True] = True,
3095         invalid_netcdf: bool = False,
3096     ) -> None:
3097         ...
3098 
3099     # compute=False returns dask.Delayed
3100     @overload
3101     def to_netcdf(
3102         self,
3103         path: str | PathLike,
3104         mode: Literal["w", "a"] = "w",
3105         format: T_NetcdfTypes | None = None,
3106         group: str | None = None,
3107         engine: T_NetcdfEngine | None = None,
3108         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3109         unlimited_dims: Iterable[Hashable] | None = None,
3110         *,
3111         compute: Literal[False],
3112         invalid_netcdf: bool = False,
3113     ) -> Delayed:
3114         ...
3115 
3116     def to_netcdf(
3117         self,
3118         path: str | PathLike | None = None,
3119         mode: Literal["w", "a"] = "w",
3120         format: T_NetcdfTypes | None = None,
3121         group: str | None = None,
3122         engine: T_NetcdfEngine | None = None,
3123         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3124         unlimited_dims: Iterable[Hashable] | None = None,
3125         compute: bool = True,
3126         invalid_netcdf: bool = False,
3127     ) -> bytes | Delayed | None:
3128         """Write dataset contents to a netCDF file.
3129 
3130         Parameters
3131         ----------
3132         path : str, path-like or None, optional
3133             Path to which to save this dataset. File-like objects are only
3134             supported by the scipy engine. If no path is provided, this
3135             function returns the resulting netCDF file as bytes; in this case,
3136             we need to use scipy, which does not support netCDF version 4 (the
3137             default format becomes NETCDF3_64BIT).
3138         mode : {"w", "a"}, default: "w"
3139             Write ('w') or append ('a') mode. If mode='w', any existing file at
3140             this location will be overwritten. If mode='a', existing variables
3141             will be overwritten.
3142         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3143                   "NETCDF3_CLASSIC"}, optional
3144             File format for the resulting netCDF file:
3145 
3146             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3147               features.
3148             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3149               netCDF 3 compatible API features.
3150             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3151               which fully supports 2+ GB files, but is only compatible with
3152               clients linked against netCDF version 3.6.0 or later.
3153             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3154               handle 2+ GB files very well.
3155 
3156             All formats are supported by the netCDF4-python library.
3157             scipy.io.netcdf only supports the last two formats.
3158 
3159             The default format is NETCDF4 if you are saving a file to disk and
3160             have the netCDF4-python library available. Otherwise, xarray falls
3161             back to using scipy to write netCDF files and defaults to the
3162             NETCDF3_64BIT format (scipy does not support netCDF4).
3163         group : str, optional
3164             Path to the netCDF4 group in the given file to open (only works for
3165             format='NETCDF4'). The group(s) will be created if necessary.
3166         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3167             Engine to use when writing netCDF files. If not provided, the
3168             default engine is chosen based on available dependencies, with a
3169             preference for 'netcdf4' if writing to a file on disk.
3170         encoding : dict, optional
3171             Nested dictionary with variable names as keys and dictionaries of
3172             variable specific encodings as values, e.g.,
3173             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3174             "zlib": True}, ...}``
3175 
3176             The `h5netcdf` engine supports both the NetCDF4-style compression
3177             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3178             ones ``{"compression": "gzip", "compression_opts": 9}``.
3179             This allows using any compression plugin installed in the HDF5
3180             library, e.g. LZF.
3181 
3182         unlimited_dims : iterable of Hashable, optional
3183             Dimension(s) that should be serialized as unlimited dimensions.
3184             By default, no dimensions are treated as unlimited dimensions.
3185             Note that unlimited_dims may also be set via
3186             ``dataset.encoding["unlimited_dims"]``.
3187         compute: bool, default: True
3188             If true compute immediately, otherwise return a
3189             ``dask.delayed.Delayed`` object that can be computed later.
3190         invalid_netcdf: bool, default: False
3191             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3192             hdf5 files which are invalid netcdf as described in
3193             https://github.com/h5netcdf/h5netcdf.
3194 
3195         Returns
3196         -------
3197         store: bytes or Delayed or None
3198             * ``bytes`` if path is None
3199             * ``dask.delayed.Delayed`` if compute is False
3200             * None otherwise
3201 
3202         Notes
3203         -----
3204         Only xarray.Dataset objects can be written to netCDF files, so
3205         the xarray.DataArray is converted to a xarray.Dataset object
3206         containing a single variable. If the DataArray has no name, or if the
3207         name is the same as a coordinate name, then it is given the name
3208         ``"__xarray_dataarray_variable__"``.
3209 
3210         See Also
3211         --------
3212         Dataset.to_netcdf
3213         """
3214         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3215 
3216         if self.name is None:
3217             # If no name is set then use a generic xarray name
3218             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3219         elif self.name in self.coords or self.name in self.dims:
3220             # The name is the same as one of the coords names, which netCDF
3221             # doesn't support, so rename it but keep track of the old name
3222             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3223             dataset.attrs[DATAARRAY_NAME] = self.name
3224         else:
3225             # No problems with the name - so we're fine!
3226             dataset = self.to_dataset()
3227 
3228         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3229             dataset,
3230             path,
3231             mode=mode,
3232             format=format,
3233             group=group,
3234             engine=engine,
3235             encoding=encoding,
3236             unlimited_dims=unlimited_dims,
3237             compute=compute,
3238             multifile=False,
3239             invalid_netcdf=invalid_netcdf,
3240         )
3241 
3242     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
3243         """
3244         Convert this xarray.DataArray into a dictionary following xarray
3245         naming conventions.
3246 
3247         Converts all variables and attributes to native Python objects.
3248         Useful for converting to json. To avoid datetime incompatibility
3249         use decode_times=False kwarg in xarray.open_dataset.
3250 
3251         Parameters
3252         ----------
3253         data : bool, default: True
3254             Whether to include the actual data in the dictionary. When set to
3255             False, returns just the schema.
3256         encoding : bool, default: False
3257             Whether to include the Dataset's encoding in the dictionary.
3258 
3259         Returns
3260         -------
3261         dict: dict
3262 
3263         See Also
3264         --------
3265         DataArray.from_dict
3266         Dataset.to_dict
3267         """
3268         d = self.variable.to_dict(data=data)
3269         d.update({"coords": {}, "name": self.name})
3270         for k in self.coords:
3271             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
3272         if encoding:
3273             d["encoding"] = dict(self.encoding)
3274         return d
3275 
3276     @classmethod
3277     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
3278         """Convert a dictionary into an xarray.DataArray
3279 
3280         Parameters
3281         ----------
3282         d : dict
3283             Mapping with a minimum structure of {"dims": [...], "data": [...]}
3284 
3285         Returns
3286         -------
3287         obj : xarray.DataArray
3288 
3289         See Also
3290         --------
3291         DataArray.to_dict
3292         Dataset.from_dict
3293 
3294         Examples
3295         --------
3296         >>> d = {"dims": "t", "data": [1, 2, 3]}
3297         >>> da = xr.DataArray.from_dict(d)
3298         >>> da
3299         <xarray.DataArray (t: 3)>
3300         array([1, 2, 3])
3301         Dimensions without coordinates: t
3302 
3303         >>> d = {
3304         ...     "coords": {
3305         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
3306         ...     },
3307         ...     "attrs": {"title": "air temperature"},
3308         ...     "dims": "t",
3309         ...     "data": [10, 20, 30],
3310         ...     "name": "a",
3311         ... }
3312         >>> da = xr.DataArray.from_dict(d)
3313         >>> da
3314         <xarray.DataArray 'a' (t: 3)>
3315         array([10, 20, 30])
3316         Coordinates:
3317           * t        (t) int64 0 1 2
3318         Attributes:
3319             title:    air temperature
3320         """
3321         coords = None
3322         if "coords" in d:
3323             try:
3324                 coords = {
3325                     k: (v["dims"], v["data"], v.get("attrs"))
3326                     for k, v in d["coords"].items()
3327                 }
3328             except KeyError as e:
3329                 raise ValueError(
3330                     "cannot convert dict when coords are missing the key "
3331                     "'{dims_data}'".format(dims_data=str(e.args[0]))
3332                 )
3333         try:
3334             data = d["data"]
3335         except KeyError:
3336             raise ValueError("cannot convert dict without the key 'data''")
3337         else:
3338             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
3339 
3340         obj.encoding.update(d.get("encoding", {}))
3341 
3342         return obj
3343 
3344     @classmethod
3345     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
3346         """Convert a pandas.Series into an xarray.DataArray.
3347 
3348         If the series's index is a MultiIndex, it will be expanded into a
3349         tensor product of one-dimensional coordinates (filling in missing
3350         values with NaN). Thus this operation should be the inverse of the
3351         `to_series` method.
3352 
3353         Parameters
3354         ----------
3355         series : Series
3356             Pandas Series object to convert.
3357         sparse : bool, default: False
3358             If sparse=True, creates a sparse array instead of a dense NumPy array.
3359             Requires the pydata/sparse package.
3360 
3361         See Also
3362         --------
3363         DataArray.to_series
3364         Dataset.from_dataframe
3365         """
3366         temp_name = "__temporary_name"
3367         df = pd.DataFrame({temp_name: series})
3368         ds = Dataset.from_dataframe(df, sparse=sparse)
3369         result = cast(DataArray, ds[temp_name])
3370         result.name = series.name
3371         return result
3372 
3373     def to_cdms2(self) -> cdms2_Variable:
3374         """Convert this array into a cdms2.Variable"""
3375         from ..convert import to_cdms2
3376 
3377         return to_cdms2(self)
3378 
3379     @classmethod
3380     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
3381         """Convert a cdms2.Variable into an xarray.DataArray"""
3382         from ..convert import from_cdms2
3383 
3384         return from_cdms2(variable)
3385 
3386     def to_iris(self) -> iris_Cube:
3387         """Convert this array into a iris.cube.Cube"""
3388         from ..convert import to_iris
3389 
3390         return to_iris(self)
3391 
3392     @classmethod
3393     def from_iris(cls, cube: iris_Cube) -> DataArray:
3394         """Convert a iris.cube.Cube into an xarray.DataArray"""
3395         from ..convert import from_iris
3396 
3397         return from_iris(cube)
3398 
3399     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
3400         """Helper function for equals, broadcast_equals, and identical"""
3401 
3402         def compat(x, y):
3403             return getattr(x.variable, compat_str)(y.variable)
3404 
3405         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
3406             self, other
3407         )
3408 
3409     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
3410         """Two DataArrays are broadcast equal if they are equal after
3411         broadcasting them against each other such that they have the same
3412         dimensions.
3413 
3414         Parameters
3415         ----------
3416         other : DataArray
3417             DataArray to compare to.
3418 
3419         Returns
3420         ----------
3421         equal : bool
3422             True if the two DataArrays are broadcast equal.
3423 
3424         See Also
3425         --------
3426         DataArray.equals
3427         DataArray.identical
3428         """
3429         try:
3430             return self._all_compat(other, "broadcast_equals")
3431         except (TypeError, AttributeError):
3432             return False
3433 
3434     def equals(self: T_DataArray, other: T_DataArray) -> bool:
3435         """True if two DataArrays have the same dimensions, coordinates and
3436         values; otherwise False.
3437 
3438         DataArrays can still be equal (like pandas objects) if they have NaN
3439         values in the same locations.
3440 
3441         This method is necessary because `v1 == v2` for ``DataArray``
3442         does element-wise comparisons (like numpy.ndarrays).
3443 
3444         Parameters
3445         ----------
3446         other : DataArray
3447             DataArray to compare to.
3448 
3449         Returns
3450         ----------
3451         equal : bool
3452             True if the two DataArrays are equal.
3453 
3454         See Also
3455         --------
3456         DataArray.broadcast_equals
3457         DataArray.identical
3458         """
3459         try:
3460             return self._all_compat(other, "equals")
3461         except (TypeError, AttributeError):
3462             return False
3463 
3464     def identical(self: T_DataArray, other: T_DataArray) -> bool:
3465         """Like equals, but also checks the array name and attributes, and
3466         attributes on all coordinates.
3467 
3468         Parameters
3469         ----------
3470         other : DataArray
3471             DataArray to compare to.
3472 
3473         Returns
3474         ----------
3475         equal : bool
3476             True if the two DataArrays are identical.
3477 
3478         See Also
3479         --------
3480         DataArray.broadcast_equals
3481         DataArray.equals
3482         """
3483         try:
3484             return self.name == other.name and self._all_compat(other, "identical")
3485         except (TypeError, AttributeError):
3486             return False
3487 
3488     def _result_name(self, other: Any = None) -> Hashable | None:
3489         # use the same naming heuristics as pandas:
3490         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
3491         other_name = getattr(other, "name", _default)
3492         if other_name is _default or other_name == self.name:
3493             return self.name
3494         else:
3495             return None
3496 
3497     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
3498         new_var = self.variable.__array_wrap__(obj, context)
3499         return self._replace(new_var)
3500 
3501     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
3502         return self.dot(obj)
3503 
3504     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3505         # currently somewhat duplicative, as only other DataArrays are
3506         # compatible with matmul
3507         return computation.dot(other, self)
3508 
3509     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
3510         keep_attrs = kwargs.pop("keep_attrs", None)
3511         if keep_attrs is None:
3512             keep_attrs = _get_keep_attrs(default=True)
3513         with warnings.catch_warnings():
3514             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
3515             warnings.filterwarnings(
3516                 "ignore", r"Mean of empty slice", category=RuntimeWarning
3517             )
3518             with np.errstate(all="ignore"):
3519                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
3520             if keep_attrs:
3521                 da.attrs = self.attrs
3522             return da
3523 
3524     def _binary_op(
3525         self: T_DataArray,
3526         other: Any,
3527         f: Callable,
3528         reflexive: bool = False,
3529     ) -> T_DataArray:
3530         from .groupby import GroupBy
3531 
3532         if isinstance(other, (Dataset, GroupBy)):
3533             return NotImplemented
3534         if isinstance(other, DataArray):
3535             align_type = OPTIONS["arithmetic_join"]
3536             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
3537         other_variable = getattr(other, "variable", other)
3538         other_coords = getattr(other, "coords", None)
3539 
3540         variable = (
3541             f(self.variable, other_variable)
3542             if not reflexive
3543             else f(other_variable, self.variable)
3544         )
3545         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
3546         name = self._result_name(other)
3547 
3548         return self._replace(variable, coords, name, indexes=indexes)
3549 
3550     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
3551         from .groupby import GroupBy
3552 
3553         if isinstance(other, GroupBy):
3554             raise TypeError(
3555                 "in-place operations between a DataArray and "
3556                 "a grouped object are not permitted"
3557             )
3558         # n.b. we can't align other to self (with other.reindex_like(self))
3559         # because `other` may be converted into floats, which would cause
3560         # in-place arithmetic to fail unpredictably. Instead, we simply
3561         # don't support automatic alignment with in-place arithmetic.
3562         other_coords = getattr(other, "coords", None)
3563         other_variable = getattr(other, "variable", other)
3564         try:
3565             with self.coords._merge_inplace(other_coords):
3566                 f(self.variable, other_variable)
3567         except MergeError as exc:
3568             raise MergeError(
3569                 "Automatic alignment is not supported for in-place operations.\n"
3570                 "Consider aligning the indices manually or using a not-in-place operation.\n"
3571                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3572             ) from exc
3573         return self
3574 
3575     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
3576         self.attrs = other.attrs
3577 
3578     plot = utils.UncachedAccessor(_PlotMethods)
3579 
3580     def _title_for_slice(self, truncate: int = 50) -> str:
3581         """
3582         If the dataarray has 1 dimensional coordinates or comes from a slice
3583         we can show that info in the title
3584 
3585         Parameters
3586         ----------
3587         truncate : int, default: 50
3588             maximum number of characters for title
3589 
3590         Returns
3591         -------
3592         title : string
3593             Can be used for plot titles
3594 
3595         """
3596         one_dims = []
3597         for dim, coord in self.coords.items():
3598             if coord.size == 1:
3599                 one_dims.append(
3600                     "{dim} = {v}{unit}".format(
3601                         dim=dim,
3602                         v=format_item(coord.values),
3603                         unit=_get_units_from_attrs(coord),
3604                     )
3605                 )
3606 
3607         title = ", ".join(one_dims)
3608         if len(title) > truncate:
3609             title = title[: (truncate - 3)] + "..."
3610 
3611         return title
3612 
3613     def diff(
3614         self: T_DataArray,
3615         dim: Hashable,
3616         n: int = 1,
3617         label: Literal["upper", "lower"] = "upper",
3618     ) -> T_DataArray:
3619         """Calculate the n-th order discrete difference along given axis.
3620 
3621         Parameters
3622         ----------
3623         dim : Hashable
3624             Dimension over which to calculate the finite difference.
3625         n : int, default: 1
3626             The number of times values are differenced.
3627         label : {"upper", "lower"}, default: "upper"
3628             The new coordinate in dimension ``dim`` will have the
3629             values of either the minuend's or subtrahend's coordinate
3630             for values 'upper' and 'lower', respectively.
3631 
3632         Returns
3633         -------
3634         difference : DataArray
3635             The n-th order finite difference of this object.
3636 
3637         Notes
3638         -----
3639         `n` matches numpy's behavior and is different from pandas' first argument named
3640         `periods`.
3641 
3642         Examples
3643         --------
3644         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3645         >>> arr.diff("x")
3646         <xarray.DataArray (x: 3)>
3647         array([0, 1, 0])
3648         Coordinates:
3649           * x        (x) int64 2 3 4
3650         >>> arr.diff("x", 2)
3651         <xarray.DataArray (x: 2)>
3652         array([ 1, -1])
3653         Coordinates:
3654           * x        (x) int64 3 4
3655 
3656         See Also
3657         --------
3658         DataArray.differentiate
3659         """
3660         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3661         return self._from_temp_dataset(ds)
3662 
3663     def shift(
3664         self: T_DataArray,
3665         shifts: Mapping[Any, int] | None = None,
3666         fill_value: Any = dtypes.NA,
3667         **shifts_kwargs: int,
3668     ) -> T_DataArray:
3669         """Shift this DataArray by an offset along one or more dimensions.
3670 
3671         Only the data is moved; coordinates stay in place. This is consistent
3672         with the behavior of ``shift`` in pandas.
3673 
3674         Values shifted from beyond array bounds will appear at one end of
3675         each dimension, which are filled according to `fill_value`. For periodic
3676         offsets instead see `roll`.
3677 
3678         Parameters
3679         ----------
3680         shifts : mapping of Hashable to int or None, optional
3681             Integer offset to shift along each of the given dimensions.
3682             Positive offsets shift to the right; negative offsets shift to the
3683             left.
3684         fill_value : scalar, optional
3685             Value to use for newly missing values
3686         **shifts_kwargs
3687             The keyword arguments form of ``shifts``.
3688             One of shifts or shifts_kwargs must be provided.
3689 
3690         Returns
3691         -------
3692         shifted : DataArray
3693             DataArray with the same coordinates and attributes but shifted
3694             data.
3695 
3696         See Also
3697         --------
3698         roll
3699 
3700         Examples
3701         --------
3702         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3703         >>> arr.shift(x=1)
3704         <xarray.DataArray (x: 3)>
3705         array([nan,  5.,  6.])
3706         Dimensions without coordinates: x
3707         """
3708         variable = self.variable.shift(
3709             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3710         )
3711         return self._replace(variable=variable)
3712 
3713     def roll(
3714         self: T_DataArray,
3715         shifts: Mapping[Hashable, int] | None = None,
3716         roll_coords: bool = False,
3717         **shifts_kwargs: int,
3718     ) -> T_DataArray:
3719         """Roll this array by an offset along one or more dimensions.
3720 
3721         Unlike shift, roll treats the given dimensions as periodic, so will not
3722         create any missing values to be filled.
3723 
3724         Unlike shift, roll may rotate all variables, including coordinates
3725         if specified. The direction of rotation is consistent with
3726         :py:func:`numpy.roll`.
3727 
3728         Parameters
3729         ----------
3730         shifts : mapping of Hashable to int, optional
3731             Integer offset to rotate each of the given dimensions.
3732             Positive offsets roll to the right; negative offsets roll to the
3733             left.
3734         roll_coords : bool, default: False
3735             Indicates whether to roll the coordinates by the offset too.
3736         **shifts_kwargs : {dim: offset, ...}, optional
3737             The keyword arguments form of ``shifts``.
3738             One of shifts or shifts_kwargs must be provided.
3739 
3740         Returns
3741         -------
3742         rolled : DataArray
3743             DataArray with the same attributes but rolled data and coordinates.
3744 
3745         See Also
3746         --------
3747         shift
3748 
3749         Examples
3750         --------
3751         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3752         >>> arr.roll(x=1)
3753         <xarray.DataArray (x: 3)>
3754         array([7, 5, 6])
3755         Dimensions without coordinates: x
3756         """
3757         ds = self._to_temp_dataset().roll(
3758             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3759         )
3760         return self._from_temp_dataset(ds)
3761 
3762     @property
3763     def real(self: T_DataArray) -> T_DataArray:
3764         return self._replace(self.variable.real)
3765 
3766     @property
3767     def imag(self: T_DataArray) -> T_DataArray:
3768         return self._replace(self.variable.imag)
3769 
3770     def dot(
3771         self: T_DataArray,
3772         other: T_DataArray,
3773         dims: str | Iterable[Hashable] | Ellipsis | None = None,
3774     ) -> T_DataArray:
3775         """Perform dot product of two DataArrays along their shared dims.
3776 
3777         Equivalent to taking taking tensordot over all shared dims.
3778 
3779         Parameters
3780         ----------
3781         other : DataArray
3782             The other array with which the dot product is performed.
3783         dims : ..., str or Iterable of Hashable, optional
3784             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3785             If not specified, then all the common dimensions are summed over.
3786 
3787         Returns
3788         -------
3789         result : DataArray
3790             Array resulting from the dot product over all shared dimensions.
3791 
3792         See Also
3793         --------
3794         dot
3795         numpy.tensordot
3796 
3797         Examples
3798         --------
3799         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3800         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3801         >>> dm_vals = np.arange(4)
3802         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3803 
3804         >>> dm.dims
3805         ('z',)
3806 
3807         >>> da.dims
3808         ('x', 'y', 'z')
3809 
3810         >>> dot_result = da.dot(dm)
3811         >>> dot_result.dims
3812         ('x', 'y')
3813 
3814         """
3815         if isinstance(other, Dataset):
3816             raise NotImplementedError(
3817                 "dot products are not yet supported with Dataset objects."
3818             )
3819         if not isinstance(other, DataArray):
3820             raise TypeError("dot only operates on DataArrays.")
3821 
3822         return computation.dot(self, other, dims=dims)
3823 
3824     # change type of self and return to T_DataArray once
3825     # https://github.com/python/mypy/issues/12846 is resolved
3826     def sortby(
3827         self,
3828         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
3829         ascending: bool = True,
3830     ) -> DataArray:
3831         """Sort object by labels or values (along an axis).
3832 
3833         Sorts the dataarray, either along specified dimensions,
3834         or according to values of 1-D dataarrays that share dimension
3835         with calling object.
3836 
3837         If the input variables are dataarrays, then the dataarrays are aligned
3838         (via left-join) to the calling object prior to sorting by cell values.
3839         NaNs are sorted to the end, following Numpy convention.
3840 
3841         If multiple sorts along the same dimension is
3842         given, numpy's lexsort is performed along that dimension:
3843         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
3844         and the FIRST key in the sequence is used as the primary sort key,
3845         followed by the 2nd key, etc.
3846 
3847         Parameters
3848         ----------
3849         variables : Hashable, DataArray, or sequence of Hashable or DataArray
3850             1D DataArray objects or name(s) of 1D variable(s) in
3851             coords whose values are used to sort this array.
3852         ascending : bool, default: True
3853             Whether to sort by ascending or descending order.
3854 
3855         Returns
3856         -------
3857         sorted : DataArray
3858             A new dataarray where all the specified dims are sorted by dim
3859             labels.
3860 
3861         See Also
3862         --------
3863         Dataset.sortby
3864         numpy.sort
3865         pandas.sort_values
3866         pandas.sort_index
3867 
3868         Examples
3869         --------
3870         >>> da = xr.DataArray(
3871         ...     np.random.rand(5),
3872         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3873         ...     dims="time",
3874         ... )
3875         >>> da
3876         <xarray.DataArray (time: 5)>
3877         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3878         Coordinates:
3879           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3880 
3881         >>> da.sortby(da)
3882         <xarray.DataArray (time: 5)>
3883         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3884         Coordinates:
3885           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3886         """
3887         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3888         return self._from_temp_dataset(ds)
3889 
3890     def quantile(
3891         self: T_DataArray,
3892         q: ArrayLike,
3893         dim: str | Iterable[Hashable] | None = None,
3894         method: QUANTILE_METHODS = "linear",
3895         keep_attrs: bool | None = None,
3896         skipna: bool | None = None,
3897         interpolation: QUANTILE_METHODS = None,
3898     ) -> T_DataArray:
3899         """Compute the qth quantile of the data along the specified dimension.
3900 
3901         Returns the qth quantiles(s) of the array elements.
3902 
3903         Parameters
3904         ----------
3905         q : float or array-like of float
3906             Quantile to compute, which must be between 0 and 1 inclusive.
3907         dim : str or Iterable of Hashable, optional
3908             Dimension(s) over which to apply quantile.
3909         method : str, default: "linear"
3910             This optional parameter specifies the interpolation method to use when the
3911             desired quantile lies between two data points. The options sorted by their R
3912             type as summarized in the H&F paper [1]_ are:
3913 
3914                 1. "inverted_cdf" (*)
3915                 2. "averaged_inverted_cdf" (*)
3916                 3. "closest_observation" (*)
3917                 4. "interpolated_inverted_cdf" (*)
3918                 5. "hazen" (*)
3919                 6. "weibull" (*)
3920                 7. "linear"  (default)
3921                 8. "median_unbiased" (*)
3922                 9. "normal_unbiased" (*)
3923 
3924             The first three methods are discontiuous. The following discontinuous
3925             variations of the default "linear" (7.) option are also available:
3926 
3927                 * "lower"
3928                 * "higher"
3929                 * "midpoint"
3930                 * "nearest"
3931 
3932             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
3933             was previously called "interpolation", renamed in accordance with numpy
3934             version 1.22.0.
3935 
3936             (*) These methods require numpy version 1.22 or newer.
3937 
3938         keep_attrs : bool or None, optional
3939             If True, the dataset's attributes (`attrs`) will be copied from
3940             the original object to the new one.  If False (default), the new
3941             object will be returned without attributes.
3942         skipna : bool or None, optional
3943             If True, skip missing values (as marked by NaN). By default, only
3944             skips missing values for float dtypes; other dtypes either do not
3945             have a sentinel missing value (int) or skipna=True has not been
3946             implemented (object, datetime64 or timedelta64).
3947 
3948         Returns
3949         -------
3950         quantiles : DataArray
3951             If `q` is a single quantile, then the result
3952             is a scalar. If multiple percentiles are given, first axis of
3953             the result corresponds to the quantile and a quantile dimension
3954             is added to the return array. The other dimensions are the
3955             dimensions that remain after the reduction of the array.
3956 
3957         See Also
3958         --------
3959         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3960 
3961         Examples
3962         --------
3963         >>> da = xr.DataArray(
3964         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3965         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3966         ...     dims=("x", "y"),
3967         ... )
3968         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3969         <xarray.DataArray ()>
3970         array(0.7)
3971         Coordinates:
3972             quantile  float64 0.0
3973         >>> da.quantile(0, dim="x")
3974         <xarray.DataArray (y: 4)>
3975         array([0.7, 4.2, 2.6, 1.5])
3976         Coordinates:
3977           * y         (y) float64 1.0 1.5 2.0 2.5
3978             quantile  float64 0.0
3979         >>> da.quantile([0, 0.5, 1])
3980         <xarray.DataArray (quantile: 3)>
3981         array([0.7, 3.4, 9.4])
3982         Coordinates:
3983           * quantile  (quantile) float64 0.0 0.5 1.0
3984         >>> da.quantile([0, 0.5, 1], dim="x")
3985         <xarray.DataArray (quantile: 3, y: 4)>
3986         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3987                [3.6 , 5.75, 6.  , 1.7 ],
3988                [6.5 , 7.3 , 9.4 , 1.9 ]])
3989         Coordinates:
3990           * y         (y) float64 1.0 1.5 2.0 2.5
3991           * quantile  (quantile) float64 0.0 0.5 1.0
3992 
3993         References
3994         ----------
3995         .. [1] R. J. Hyndman and Y. Fan,
3996            "Sample quantiles in statistical packages,"
3997            The American Statistician, 50(4), pp. 361-365, 1996
3998         """
3999 
4000         ds = self._to_temp_dataset().quantile(
4001             q,
4002             dim=dim,
4003             keep_attrs=keep_attrs,
4004             method=method,
4005             skipna=skipna,
4006             interpolation=interpolation,
4007         )
4008         return self._from_temp_dataset(ds)
4009 
4010     def rank(
4011         self: T_DataArray,
4012         dim: Hashable,
4013         pct: bool = False,
4014         keep_attrs: bool | None = None,
4015     ) -> T_DataArray:
4016         """Ranks the data.
4017 
4018         Equal values are assigned a rank that is the average of the ranks that
4019         would have been otherwise assigned to all of the values within that
4020         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
4021 
4022         NaNs in the input array are returned as NaNs.
4023 
4024         The `bottleneck` library is required.
4025 
4026         Parameters
4027         ----------
4028         dim : Hashable
4029             Dimension over which to compute rank.
4030         pct : bool, default: False
4031             If True, compute percentage ranks, otherwise compute integer ranks.
4032         keep_attrs : bool or None, optional
4033             If True, the dataset's attributes (`attrs`) will be copied from
4034             the original object to the new one.  If False (default), the new
4035             object will be returned without attributes.
4036 
4037         Returns
4038         -------
4039         ranked : DataArray
4040             DataArray with the same coordinates and dtype 'float64'.
4041 
4042         Examples
4043         --------
4044         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4045         >>> arr.rank("x")
4046         <xarray.DataArray (x: 3)>
4047         array([1., 2., 3.])
4048         Dimensions without coordinates: x
4049         """
4050 
4051         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
4052         return self._from_temp_dataset(ds)
4053 
4054     def differentiate(
4055         self: T_DataArray,
4056         coord: Hashable,
4057         edge_order: Literal[1, 2] = 1,
4058         datetime_unit: DatetimeUnitOptions = None,
4059     ) -> T_DataArray:
4060         """ Differentiate the array with the second order accurate central
4061         differences.
4062 
4063         .. note::
4064             This feature is limited to simple cartesian geometry, i.e. coord
4065             must be one dimensional.
4066 
4067         Parameters
4068         ----------
4069         coord : Hashable
4070             The coordinate to be used to compute the gradient.
4071         edge_order : {1, 2}, default: 1
4072             N-th order accurate differences at the boundaries.
4073         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
4074                          "us", "ns", "ps", "fs", "as", None}, optional
4075             Unit to compute gradient. Only valid for datetime coordinate.
4076 
4077         Returns
4078         -------
4079         differentiated: DataArray
4080 
4081         See also
4082         --------
4083         numpy.gradient: corresponding numpy function
4084 
4085         Examples
4086         --------
4087 
4088         >>> da = xr.DataArray(
4089         ...     np.arange(12).reshape(4, 3),
4090         ...     dims=["x", "y"],
4091         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4092         ... )
4093         >>> da
4094         <xarray.DataArray (x: 4, y: 3)>
4095         array([[ 0,  1,  2],
4096                [ 3,  4,  5],
4097                [ 6,  7,  8],
4098                [ 9, 10, 11]])
4099         Coordinates:
4100           * x        (x) float64 0.0 0.1 1.1 1.2
4101         Dimensions without coordinates: y
4102         >>>
4103         >>> da.differentiate("x")
4104         <xarray.DataArray (x: 4, y: 3)>
4105         array([[30.        , 30.        , 30.        ],
4106                [27.54545455, 27.54545455, 27.54545455],
4107                [27.54545455, 27.54545455, 27.54545455],
4108                [30.        , 30.        , 30.        ]])
4109         Coordinates:
4110           * x        (x) float64 0.0 0.1 1.1 1.2
4111         Dimensions without coordinates: y
4112         """
4113         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
4114         return self._from_temp_dataset(ds)
4115 
4116     # change type of self and return to T_DataArray once
4117     # https://github.com/python/mypy/issues/12846 is resolved
4118     def integrate(
4119         self,
4120         coord: Hashable | Sequence[Hashable] = None,
4121         datetime_unit: DatetimeUnitOptions = None,
4122     ) -> DataArray:
4123         """Integrate along the given coordinate using the trapezoidal rule.
4124 
4125         .. note::
4126             This feature is limited to simple cartesian geometry, i.e. coord
4127             must be one dimensional.
4128 
4129         Parameters
4130         ----------
4131         coord : Hashable, or sequence of Hashable
4132             Coordinate(s) used for the integration.
4133         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4134                         'ps', 'fs', 'as', None}, optional
4135             Specify the unit if a datetime coordinate is used.
4136 
4137         Returns
4138         -------
4139         integrated : DataArray
4140 
4141         See also
4142         --------
4143         Dataset.integrate
4144         numpy.trapz : corresponding numpy function
4145 
4146         Examples
4147         --------
4148 
4149         >>> da = xr.DataArray(
4150         ...     np.arange(12).reshape(4, 3),
4151         ...     dims=["x", "y"],
4152         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4153         ... )
4154         >>> da
4155         <xarray.DataArray (x: 4, y: 3)>
4156         array([[ 0,  1,  2],
4157                [ 3,  4,  5],
4158                [ 6,  7,  8],
4159                [ 9, 10, 11]])
4160         Coordinates:
4161           * x        (x) float64 0.0 0.1 1.1 1.2
4162         Dimensions without coordinates: y
4163         >>>
4164         >>> da.integrate("x")
4165         <xarray.DataArray (y: 3)>
4166         array([5.4, 6.6, 7.8])
4167         Dimensions without coordinates: y
4168         """
4169         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
4170         return self._from_temp_dataset(ds)
4171 
4172     # change type of self and return to T_DataArray once
4173     # https://github.com/python/mypy/issues/12846 is resolved
4174     def cumulative_integrate(
4175         self,
4176         coord: Hashable | Sequence[Hashable] = None,
4177         datetime_unit: DatetimeUnitOptions = None,
4178     ) -> DataArray:
4179         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
4180 
4181         .. note::
4182             This feature is limited to simple cartesian geometry, i.e. coord
4183             must be one dimensional.
4184 
4185             The first entry of the cumulative integral is always 0, in order to keep the
4186             length of the dimension unchanged between input and output.
4187 
4188         Parameters
4189         ----------
4190         coord : Hashable, or sequence of Hashable
4191             Coordinate(s) used for the integration.
4192         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4193                         'ps', 'fs', 'as', None}, optional
4194             Specify the unit if a datetime coordinate is used.
4195 
4196         Returns
4197         -------
4198         integrated : DataArray
4199 
4200         See also
4201         --------
4202         Dataset.cumulative_integrate
4203         scipy.integrate.cumulative_trapezoid : corresponding scipy function
4204 
4205         Examples
4206         --------
4207 
4208         >>> da = xr.DataArray(
4209         ...     np.arange(12).reshape(4, 3),
4210         ...     dims=["x", "y"],
4211         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4212         ... )
4213         >>> da
4214         <xarray.DataArray (x: 4, y: 3)>
4215         array([[ 0,  1,  2],
4216                [ 3,  4,  5],
4217                [ 6,  7,  8],
4218                [ 9, 10, 11]])
4219         Coordinates:
4220           * x        (x) float64 0.0 0.1 1.1 1.2
4221         Dimensions without coordinates: y
4222         >>>
4223         >>> da.cumulative_integrate("x")
4224         <xarray.DataArray (x: 4, y: 3)>
4225         array([[0.  , 0.  , 0.  ],
4226                [0.15, 0.25, 0.35],
4227                [4.65, 5.75, 6.85],
4228                [5.4 , 6.6 , 7.8 ]])
4229         Coordinates:
4230           * x        (x) float64 0.0 0.1 1.1 1.2
4231         Dimensions without coordinates: y
4232         """
4233         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
4234         return self._from_temp_dataset(ds)
4235 
4236     def unify_chunks(self) -> DataArray:
4237         """Unify chunk size along all chunked dimensions of this DataArray.
4238 
4239         Returns
4240         -------
4241         DataArray with consistent chunk sizes for all dask-array variables
4242 
4243         See Also
4244         --------
4245         dask.array.core.unify_chunks
4246         """
4247 
4248         return unify_chunks(self)[0]
4249 
4250     def map_blocks(
4251         self,
4252         func: Callable[..., T_Xarray],
4253         args: Sequence[Any] = (),
4254         kwargs: Mapping[str, Any] | None = None,
4255         template: DataArray | Dataset | None = None,
4256     ) -> T_Xarray:
4257         """
4258         Apply a function to each block of this DataArray.
4259 
4260         .. warning::
4261             This method is experimental and its signature may change.
4262 
4263         Parameters
4264         ----------
4265         func : callable
4266             User-provided function that accepts a DataArray as its first
4267             parameter. The function will receive a subset or 'block' of this DataArray (see below),
4268             corresponding to one chunk along each chunked dimension. ``func`` will be
4269             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
4270 
4271             This function must return either a single DataArray or a single Dataset.
4272 
4273             This function cannot add a new chunked dimension.
4274         args : sequence
4275             Passed to func after unpacking and subsetting any xarray objects by blocks.
4276             xarray objects in args must be aligned with this object, otherwise an error is raised.
4277         kwargs : mapping
4278             Passed verbatim to func after unpacking. xarray objects, if any, will not be
4279             subset to blocks. Passing dask collections in kwargs is not allowed.
4280         template : DataArray or Dataset, optional
4281             xarray object representing the final result after compute is called. If not provided,
4282             the function will be first run on mocked-up data, that looks like this object but
4283             has sizes 0, to determine properties of the returned object such as dtype,
4284             variable names, attributes, new dimensions and new indexes (if any).
4285             ``template`` must be provided if the function changes the size of existing dimensions.
4286             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
4287             ``attrs`` set by ``func`` will be ignored.
4288 
4289         Returns
4290         -------
4291         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
4292         function.
4293 
4294         Notes
4295         -----
4296         This function is designed for when ``func`` needs to manipulate a whole xarray object
4297         subset to each block. Each block is loaded into memory. In the more common case where
4298         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
4299 
4300         If none of the variables in this object is backed by dask arrays, calling this function is
4301         equivalent to calling ``func(obj, *args, **kwargs)``.
4302 
4303         See Also
4304         --------
4305         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
4306         xarray.DataArray.map_blocks
4307 
4308         Examples
4309         --------
4310         Calculate an anomaly from climatology using ``.groupby()``. Using
4311         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
4312         its indices, and its methods like ``.groupby()``.
4313 
4314         >>> def calculate_anomaly(da, groupby_type="time.month"):
4315         ...     gb = da.groupby(groupby_type)
4316         ...     clim = gb.mean(dim="time")
4317         ...     return gb - clim
4318         ...
4319         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
4320         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
4321         >>> np.random.seed(123)
4322         >>> array = xr.DataArray(
4323         ...     np.random.rand(len(time)),
4324         ...     dims=["time"],
4325         ...     coords={"time": time, "month": month},
4326         ... ).chunk()
4327         >>> array.map_blocks(calculate_anomaly, template=array).compute()
4328         <xarray.DataArray (time: 24)>
4329         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
4330                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
4331                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
4332                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
4333                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
4334         Coordinates:
4335           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4336             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
4337 
4338         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
4339         to the function being applied in ``xr.map_blocks()``:
4340 
4341         >>> array.map_blocks(
4342         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
4343         ... )  # doctest: +ELLIPSIS
4344         <xarray.DataArray (time: 24)>
4345         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
4346         Coordinates:
4347           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4348             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
4349         """
4350         from .parallel import map_blocks
4351 
4352         return map_blocks(func, self, args, kwargs, template)
4353 
4354     def polyfit(
4355         self,
4356         dim: Hashable,
4357         deg: int,
4358         skipna: bool | None = None,
4359         rcond: float | None = None,
4360         w: Hashable | Any | None = None,
4361         full: bool = False,
4362         cov: bool | Literal["unscaled"] = False,
4363     ) -> Dataset:
4364         """
4365         Least squares polynomial fit.
4366 
4367         This replicates the behaviour of `numpy.polyfit` but differs by skipping
4368         invalid values when `skipna = True`.
4369 
4370         Parameters
4371         ----------
4372         dim : Hashable
4373             Coordinate along which to fit the polynomials.
4374         deg : int
4375             Degree of the fitting polynomial.
4376         skipna : bool or None, optional
4377             If True, removes all invalid values before fitting each 1D slices of the array.
4378             Default is True if data is stored in a dask.array or if there is any
4379             invalid values, False otherwise.
4380         rcond : float or None, optional
4381             Relative condition number to the fit.
4382         w : Hashable, array-like or None, optional
4383             Weights to apply to the y-coordinate of the sample points.
4384             Can be an array-like object or the name of a coordinate in the dataset.
4385         full : bool, default: False
4386             Whether to return the residuals, matrix rank and singular values in addition
4387             to the coefficients.
4388         cov : bool or "unscaled", default: False
4389             Whether to return to the covariance matrix in addition to the coefficients.
4390             The matrix is not scaled if `cov='unscaled'`.
4391 
4392         Returns
4393         -------
4394         polyfit_results : Dataset
4395             A single dataset which contains:
4396 
4397             polyfit_coefficients
4398                 The coefficients of the best fit.
4399             polyfit_residuals
4400                 The residuals of the least-square computation (only included if `full=True`).
4401                 When the matrix rank is deficient, np.nan is returned.
4402             [dim]_matrix_rank
4403                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4404             [dim]_singular_value
4405                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4406             polyfit_covariance
4407                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
4408 
4409         See Also
4410         --------
4411         numpy.polyfit
4412         numpy.polyval
4413         xarray.polyval
4414         """
4415         return self._to_temp_dataset().polyfit(
4416             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
4417         )
4418 
4419     def pad(
4420         self: T_DataArray,
4421         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
4422         mode: PadModeOptions = "constant",
4423         stat_length: int
4424         | tuple[int, int]
4425         | Mapping[Any, tuple[int, int]]
4426         | None = None,
4427         constant_values: float
4428         | tuple[float, float]
4429         | Mapping[Any, tuple[float, float]]
4430         | None = None,
4431         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
4432         reflect_type: PadReflectOptions = None,
4433         **pad_width_kwargs: Any,
4434     ) -> T_DataArray:
4435         """Pad this array along one or more dimensions.
4436 
4437         .. warning::
4438             This function is experimental and its behaviour is likely to change
4439             especially regarding padding of dimension coordinates (or IndexVariables).
4440 
4441         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
4442         coordinates will be padded with the same mode, otherwise coordinates
4443         are padded using the "constant" mode with fill_value dtypes.NA.
4444 
4445         Parameters
4446         ----------
4447         pad_width : mapping of Hashable to tuple of int
4448             Mapping with the form of {dim: (pad_before, pad_after)}
4449             describing the number of values padded along each dimension.
4450             {dim: pad} is a shortcut for pad_before = pad_after = pad
4451         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
4452             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
4453             How to pad the DataArray (taken from numpy docs):
4454 
4455             - "constant": Pads with a constant value.
4456             - "edge": Pads with the edge values of array.
4457             - "linear_ramp": Pads with the linear ramp between end_value and the
4458               array edge value.
4459             - "maximum": Pads with the maximum value of all or part of the
4460               vector along each axis.
4461             - "mean": Pads with the mean value of all or part of the
4462               vector along each axis.
4463             - "median": Pads with the median value of all or part of the
4464               vector along each axis.
4465             - "minimum": Pads with the minimum value of all or part of the
4466               vector along each axis.
4467             - "reflect": Pads with the reflection of the vector mirrored on
4468               the first and last values of the vector along each axis.
4469             - "symmetric": Pads with the reflection of the vector mirrored
4470               along the edge of the array.
4471             - "wrap": Pads with the wrap of the vector along the axis.
4472               The first values are used to pad the end and the
4473               end values are used to pad the beginning.
4474 
4475         stat_length : int, tuple or mapping of Hashable to tuple, default: None
4476             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
4477             values at edge of each axis used to calculate the statistic value.
4478             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
4479             statistic lengths along each dimension.
4480             ((before, after),) yields same before and after statistic lengths
4481             for each dimension.
4482             (stat_length,) or int is a shortcut for before = after = statistic
4483             length for all axes.
4484             Default is ``None``, to use the entire axis.
4485         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4486             Used in 'constant'.  The values to set the padded values for each
4487             axis.
4488             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4489             pad constants along each dimension.
4490             ``((before, after),)`` yields same before and after constants for each
4491             dimension.
4492             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4493             all dimensions.
4494             Default is 0.
4495         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4496             Used in 'linear_ramp'.  The values used for the ending value of the
4497             linear_ramp and that will form the edge of the padded array.
4498             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4499             end values along each dimension.
4500             ``((before, after),)`` yields same before and after end values for each
4501             axis.
4502             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4503             all axes.
4504             Default is 0.
4505         reflect_type : {"even", "odd", None}, optional
4506             Used in "reflect", and "symmetric". The "even" style is the
4507             default with an unaltered reflection around the edge value. For
4508             the "odd" style, the extended part of the array is created by
4509             subtracting the reflected values from two times the edge value.
4510         **pad_width_kwargs
4511             The keyword arguments form of ``pad_width``.
4512             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
4513 
4514         Returns
4515         -------
4516         padded : DataArray
4517             DataArray with the padded coordinates and data.
4518 
4519         See Also
4520         --------
4521         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
4522 
4523         Notes
4524         -----
4525         For ``mode="constant"`` and ``constant_values=None``, integer types will be
4526         promoted to ``float`` and padded with ``np.nan``.
4527 
4528         Padding coordinates will drop their corresponding index (if any) and will reset default
4529         indexes for dimension coordinates.
4530 
4531         Examples
4532         --------
4533         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
4534         >>> arr.pad(x=(1, 2), constant_values=0)
4535         <xarray.DataArray (x: 6)>
4536         array([0, 5, 6, 7, 0, 0])
4537         Coordinates:
4538           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
4539 
4540         >>> da = xr.DataArray(
4541         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
4542         ...     dims=["x", "y"],
4543         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
4544         ... )
4545         >>> da.pad(x=1)
4546         <xarray.DataArray (x: 4, y: 4)>
4547         array([[nan, nan, nan, nan],
4548                [ 0.,  1.,  2.,  3.],
4549                [10., 11., 12., 13.],
4550                [nan, nan, nan, nan]])
4551         Coordinates:
4552           * x        (x) float64 nan 0.0 1.0 nan
4553           * y        (y) int64 10 20 30 40
4554             z        (x) float64 nan 100.0 200.0 nan
4555 
4556         Careful, ``constant_values`` are coerced to the data type of the array which may
4557         lead to a loss of precision:
4558 
4559         >>> da.pad(x=1, constant_values=1.23456789)
4560         <xarray.DataArray (x: 4, y: 4)>
4561         array([[ 1,  1,  1,  1],
4562                [ 0,  1,  2,  3],
4563                [10, 11, 12, 13],
4564                [ 1,  1,  1,  1]])
4565         Coordinates:
4566           * x        (x) float64 nan 0.0 1.0 nan
4567           * y        (y) int64 10 20 30 40
4568             z        (x) float64 nan 100.0 200.0 nan
4569         """
4570         ds = self._to_temp_dataset().pad(
4571             pad_width=pad_width,
4572             mode=mode,
4573             stat_length=stat_length,
4574             constant_values=constant_values,
4575             end_values=end_values,
4576             reflect_type=reflect_type,
4577             **pad_width_kwargs,
4578         )
4579         return self._from_temp_dataset(ds)
4580 
4581     def idxmin(
4582         self,
4583         dim: Hashable | None = None,
4584         skipna: bool | None = None,
4585         fill_value: Any = dtypes.NA,
4586         keep_attrs: bool | None = None,
4587     ) -> DataArray:
4588         """Return the coordinate label of the minimum value along a dimension.
4589 
4590         Returns a new `DataArray` named after the dimension with the values of
4591         the coordinate labels along that dimension corresponding to minimum
4592         values along that dimension.
4593 
4594         In comparison to :py:meth:`~DataArray.argmin`, this returns the
4595         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
4596 
4597         Parameters
4598         ----------
4599         dim : str, optional
4600             Dimension over which to apply `idxmin`.  This is optional for 1D
4601             arrays, but required for arrays with 2 or more dimensions.
4602         skipna : bool or None, default: None
4603             If True, skip missing values (as marked by NaN). By default, only
4604             skips missing values for ``float``, ``complex``, and ``object``
4605             dtypes; other dtypes either do not have a sentinel missing value
4606             (``int``) or ``skipna=True`` has not been implemented
4607             (``datetime64`` or ``timedelta64``).
4608         fill_value : Any, default: NaN
4609             Value to be filled in case all of the values along a dimension are
4610             null.  By default this is NaN.  The fill value and result are
4611             automatically converted to a compatible dtype if possible.
4612             Ignored if ``skipna`` is False.
4613         keep_attrs : bool or None, optional
4614             If True, the attributes (``attrs``) will be copied from the
4615             original object to the new one. If False, the new object
4616             will be returned without attributes.
4617 
4618         Returns
4619         -------
4620         reduced : DataArray
4621             New `DataArray` object with `idxmin` applied to its data and the
4622             indicated dimension removed.
4623 
4624         See Also
4625         --------
4626         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
4627 
4628         Examples
4629         --------
4630         >>> array = xr.DataArray(
4631         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4632         ... )
4633         >>> array.min()
4634         <xarray.DataArray ()>
4635         array(-2)
4636         >>> array.argmin()
4637         <xarray.DataArray ()>
4638         array(4)
4639         >>> array.idxmin()
4640         <xarray.DataArray 'x' ()>
4641         array('e', dtype='<U1')
4642 
4643         >>> array = xr.DataArray(
4644         ...     [
4645         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4646         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4647         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4648         ...     ],
4649         ...     dims=["y", "x"],
4650         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4651         ... )
4652         >>> array.min(dim="x")
4653         <xarray.DataArray (y: 3)>
4654         array([-2., -4.,  1.])
4655         Coordinates:
4656           * y        (y) int64 -1 0 1
4657         >>> array.argmin(dim="x")
4658         <xarray.DataArray (y: 3)>
4659         array([4, 0, 2])
4660         Coordinates:
4661           * y        (y) int64 -1 0 1
4662         >>> array.idxmin(dim="x")
4663         <xarray.DataArray 'x' (y: 3)>
4664         array([16.,  0.,  4.])
4665         Coordinates:
4666           * y        (y) int64 -1 0 1
4667         """
4668         return computation._calc_idxminmax(
4669             array=self,
4670             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4671             dim=dim,
4672             skipna=skipna,
4673             fill_value=fill_value,
4674             keep_attrs=keep_attrs,
4675         )
4676 
4677     def idxmax(
4678         self,
4679         dim: Hashable = None,
4680         skipna: bool | None = None,
4681         fill_value: Any = dtypes.NA,
4682         keep_attrs: bool | None = None,
4683     ) -> DataArray:
4684         """Return the coordinate label of the maximum value along a dimension.
4685 
4686         Returns a new `DataArray` named after the dimension with the values of
4687         the coordinate labels along that dimension corresponding to maximum
4688         values along that dimension.
4689 
4690         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4691         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4692 
4693         Parameters
4694         ----------
4695         dim : Hashable, optional
4696             Dimension over which to apply `idxmax`.  This is optional for 1D
4697             arrays, but required for arrays with 2 or more dimensions.
4698         skipna : bool or None, default: None
4699             If True, skip missing values (as marked by NaN). By default, only
4700             skips missing values for ``float``, ``complex``, and ``object``
4701             dtypes; other dtypes either do not have a sentinel missing value
4702             (``int``) or ``skipna=True`` has not been implemented
4703             (``datetime64`` or ``timedelta64``).
4704         fill_value : Any, default: NaN
4705             Value to be filled in case all of the values along a dimension are
4706             null.  By default this is NaN.  The fill value and result are
4707             automatically converted to a compatible dtype if possible.
4708             Ignored if ``skipna`` is False.
4709         keep_attrs : bool or None, optional
4710             If True, the attributes (``attrs``) will be copied from the
4711             original object to the new one. If False, the new object
4712             will be returned without attributes.
4713 
4714         Returns
4715         -------
4716         reduced : DataArray
4717             New `DataArray` object with `idxmax` applied to its data and the
4718             indicated dimension removed.
4719 
4720         See Also
4721         --------
4722         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4723 
4724         Examples
4725         --------
4726         >>> array = xr.DataArray(
4727         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4728         ... )
4729         >>> array.max()
4730         <xarray.DataArray ()>
4731         array(2)
4732         >>> array.argmax()
4733         <xarray.DataArray ()>
4734         array(1)
4735         >>> array.idxmax()
4736         <xarray.DataArray 'x' ()>
4737         array('b', dtype='<U1')
4738 
4739         >>> array = xr.DataArray(
4740         ...     [
4741         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4742         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4743         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4744         ...     ],
4745         ...     dims=["y", "x"],
4746         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4747         ... )
4748         >>> array.max(dim="x")
4749         <xarray.DataArray (y: 3)>
4750         array([2., 2., 1.])
4751         Coordinates:
4752           * y        (y) int64 -1 0 1
4753         >>> array.argmax(dim="x")
4754         <xarray.DataArray (y: 3)>
4755         array([0, 2, 2])
4756         Coordinates:
4757           * y        (y) int64 -1 0 1
4758         >>> array.idxmax(dim="x")
4759         <xarray.DataArray 'x' (y: 3)>
4760         array([0., 4., 4.])
4761         Coordinates:
4762           * y        (y) int64 -1 0 1
4763         """
4764         return computation._calc_idxminmax(
4765             array=self,
4766             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4767             dim=dim,
4768             skipna=skipna,
4769             fill_value=fill_value,
4770             keep_attrs=keep_attrs,
4771         )
4772 
4773     # change type of self and return to T_DataArray once
4774     # https://github.com/python/mypy/issues/12846 is resolved
4775     def argmin(
4776         self,
4777         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4778         axis: int | None = None,
4779         keep_attrs: bool | None = None,
4780         skipna: bool | None = None,
4781     ) -> DataArray | dict[Hashable, DataArray]:
4782         """Index or indices of the minimum of the DataArray over one or more dimensions.
4783 
4784         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4785         which can be passed directly to isel(). If a single str is passed to 'dim' then
4786         returns a DataArray with dtype int.
4787 
4788         If there are multiple minima, the indices of the first one found will be
4789         returned.
4790 
4791         Parameters
4792         ----------
4793         dim : Hashable, sequence of Hashable, None or ..., optional
4794             The dimensions over which to find the minimum. By default, finds minimum over
4795             all dimensions - for now returning an int for backward compatibility, but
4796             this is deprecated, in future will return a dict with indices for all
4797             dimensions; to return a dict with all dimensions now, pass '...'.
4798         axis : int or None, optional
4799             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4800             can be supplied.
4801         keep_attrs : bool or None, optional
4802             If True, the attributes (`attrs`) will be copied from the original
4803             object to the new one. If False, the new object will be
4804             returned without attributes.
4805         skipna : bool or None, optional
4806             If True, skip missing values (as marked by NaN). By default, only
4807             skips missing values for float dtypes; other dtypes either do not
4808             have a sentinel missing value (int) or skipna=True has not been
4809             implemented (object, datetime64 or timedelta64).
4810 
4811         Returns
4812         -------
4813         result : DataArray or dict of DataArray
4814 
4815         See Also
4816         --------
4817         Variable.argmin, DataArray.idxmin
4818 
4819         Examples
4820         --------
4821         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4822         >>> array.min()
4823         <xarray.DataArray ()>
4824         array(-1)
4825         >>> array.argmin()
4826         <xarray.DataArray ()>
4827         array(2)
4828         >>> array.argmin(...)
4829         {'x': <xarray.DataArray ()>
4830         array(2)}
4831         >>> array.isel(array.argmin(...))
4832         <xarray.DataArray ()>
4833         array(-1)
4834 
4835         >>> array = xr.DataArray(
4836         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4837         ...     dims=("x", "y", "z"),
4838         ... )
4839         >>> array.min(dim="x")
4840         <xarray.DataArray (y: 3, z: 3)>
4841         array([[ 1,  2,  1],
4842                [ 2, -5,  1],
4843                [ 2,  1,  1]])
4844         Dimensions without coordinates: y, z
4845         >>> array.argmin(dim="x")
4846         <xarray.DataArray (y: 3, z: 3)>
4847         array([[1, 0, 0],
4848                [1, 1, 1],
4849                [0, 0, 1]])
4850         Dimensions without coordinates: y, z
4851         >>> array.argmin(dim=["x"])
4852         {'x': <xarray.DataArray (y: 3, z: 3)>
4853         array([[1, 0, 0],
4854                [1, 1, 1],
4855                [0, 0, 1]])
4856         Dimensions without coordinates: y, z}
4857         >>> array.min(dim=("x", "z"))
4858         <xarray.DataArray (y: 3)>
4859         array([ 1, -5,  1])
4860         Dimensions without coordinates: y
4861         >>> array.argmin(dim=["x", "z"])
4862         {'x': <xarray.DataArray (y: 3)>
4863         array([0, 1, 0])
4864         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4865         array([2, 1, 1])
4866         Dimensions without coordinates: y}
4867         >>> array.isel(array.argmin(dim=["x", "z"]))
4868         <xarray.DataArray (y: 3)>
4869         array([ 1, -5,  1])
4870         Dimensions without coordinates: y
4871         """
4872         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4873         if isinstance(result, dict):
4874             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4875         else:
4876             return self._replace_maybe_drop_dims(result)
4877 
4878     # change type of self and return to T_DataArray once
4879     # https://github.com/python/mypy/issues/12846 is resolved
4880     def argmax(
4881         self,
4882         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4883         axis: int | None = None,
4884         keep_attrs: bool | None = None,
4885         skipna: bool | None = None,
4886     ) -> DataArray | dict[Hashable, DataArray]:
4887         """Index or indices of the maximum of the DataArray over one or more dimensions.
4888 
4889         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4890         which can be passed directly to isel(). If a single str is passed to 'dim' then
4891         returns a DataArray with dtype int.
4892 
4893         If there are multiple maxima, the indices of the first one found will be
4894         returned.
4895 
4896         Parameters
4897         ----------
4898         dim : Hashable, sequence of Hashable, None or ..., optional
4899             The dimensions over which to find the maximum. By default, finds maximum over
4900             all dimensions - for now returning an int for backward compatibility, but
4901             this is deprecated, in future will return a dict with indices for all
4902             dimensions; to return a dict with all dimensions now, pass '...'.
4903         axis : int or None, optional
4904             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4905             can be supplied.
4906         keep_attrs : bool or None, optional
4907             If True, the attributes (`attrs`) will be copied from the original
4908             object to the new one. If False, the new object will be
4909             returned without attributes.
4910         skipna : bool or None, optional
4911             If True, skip missing values (as marked by NaN). By default, only
4912             skips missing values for float dtypes; other dtypes either do not
4913             have a sentinel missing value (int) or skipna=True has not been
4914             implemented (object, datetime64 or timedelta64).
4915 
4916         Returns
4917         -------
4918         result : DataArray or dict of DataArray
4919 
4920         See Also
4921         --------
4922         Variable.argmax, DataArray.idxmax
4923 
4924         Examples
4925         --------
4926         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4927         >>> array.max()
4928         <xarray.DataArray ()>
4929         array(3)
4930         >>> array.argmax()
4931         <xarray.DataArray ()>
4932         array(3)
4933         >>> array.argmax(...)
4934         {'x': <xarray.DataArray ()>
4935         array(3)}
4936         >>> array.isel(array.argmax(...))
4937         <xarray.DataArray ()>
4938         array(3)
4939 
4940         >>> array = xr.DataArray(
4941         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4942         ...     dims=("x", "y", "z"),
4943         ... )
4944         >>> array.max(dim="x")
4945         <xarray.DataArray (y: 3, z: 3)>
4946         array([[3, 3, 2],
4947                [3, 5, 2],
4948                [2, 3, 3]])
4949         Dimensions without coordinates: y, z
4950         >>> array.argmax(dim="x")
4951         <xarray.DataArray (y: 3, z: 3)>
4952         array([[0, 1, 1],
4953                [0, 1, 0],
4954                [0, 1, 0]])
4955         Dimensions without coordinates: y, z
4956         >>> array.argmax(dim=["x"])
4957         {'x': <xarray.DataArray (y: 3, z: 3)>
4958         array([[0, 1, 1],
4959                [0, 1, 0],
4960                [0, 1, 0]])
4961         Dimensions without coordinates: y, z}
4962         >>> array.max(dim=("x", "z"))
4963         <xarray.DataArray (y: 3)>
4964         array([3, 5, 3])
4965         Dimensions without coordinates: y
4966         >>> array.argmax(dim=["x", "z"])
4967         {'x': <xarray.DataArray (y: 3)>
4968         array([0, 1, 0])
4969         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4970         array([0, 1, 2])
4971         Dimensions without coordinates: y}
4972         >>> array.isel(array.argmax(dim=["x", "z"]))
4973         <xarray.DataArray (y: 3)>
4974         array([3, 5, 3])
4975         Dimensions without coordinates: y
4976         """
4977         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4978         if isinstance(result, dict):
4979             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4980         else:
4981             return self._replace_maybe_drop_dims(result)
4982 
4983     def query(
4984         self,
4985         queries: Mapping[Any, Any] | None = None,
4986         parser: QueryParserOptions = "pandas",
4987         engine: QueryEngineOptions = None,
4988         missing_dims: ErrorOptionsWithWarn = "raise",
4989         **queries_kwargs: Any,
4990     ) -> DataArray:
4991         """Return a new data array indexed along the specified
4992         dimension(s), where the indexers are given as strings containing
4993         Python expressions to be evaluated against the values in the array.
4994 
4995         Parameters
4996         ----------
4997         queries : dict-like or None, optional
4998             A dict-like with keys matching dimensions and values given by strings
4999             containing Python expressions to be evaluated against the data variables
5000             in the dataset. The expressions will be evaluated using the pandas
5001             eval() function, and can contain any valid Python expressions but cannot
5002             contain any Python statements.
5003         parser : {"pandas", "python"}, default: "pandas"
5004             The parser to use to construct the syntax tree from the expression.
5005             The default of 'pandas' parses code slightly different than standard
5006             Python. Alternatively, you can parse an expression using the 'python'
5007             parser to retain strict Python semantics.
5008         engine : {"python", "numexpr", None}, default: None
5009             The engine used to evaluate the expression. Supported engines are:
5010 
5011             - None: tries to use numexpr, falls back to python
5012             - "numexpr": evaluates expressions using numexpr
5013             - "python": performs operations as if you had eval’d in top level python
5014 
5015         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5016             What to do if dimensions that should be selected from are not present in the
5017             DataArray:
5018 
5019             - "raise": raise an exception
5020             - "warn": raise a warning, and ignore the missing dimensions
5021             - "ignore": ignore the missing dimensions
5022 
5023         **queries_kwargs : {dim: query, ...}, optional
5024             The keyword arguments form of ``queries``.
5025             One of queries or queries_kwargs must be provided.
5026 
5027         Returns
5028         -------
5029         obj : DataArray
5030             A new DataArray with the same contents as this dataset, indexed by
5031             the results of the appropriate queries.
5032 
5033         See Also
5034         --------
5035         DataArray.isel
5036         Dataset.query
5037         pandas.eval
5038 
5039         Examples
5040         --------
5041         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
5042         >>> da
5043         <xarray.DataArray 'a' (x: 5)>
5044         array([0, 1, 2, 3, 4])
5045         Dimensions without coordinates: x
5046         >>> da.query(x="a > 2")
5047         <xarray.DataArray 'a' (x: 2)>
5048         array([3, 4])
5049         Dimensions without coordinates: x
5050         """
5051 
5052         ds = self._to_dataset_whole(shallow_copy=True)
5053         ds = ds.query(
5054             queries=queries,
5055             parser=parser,
5056             engine=engine,
5057             missing_dims=missing_dims,
5058             **queries_kwargs,
5059         )
5060         return ds[self.name]
5061 
5062     def curvefit(
5063         self,
5064         coords: str | DataArray | Iterable[str | DataArray],
5065         func: Callable[..., Any],
5066         reduce_dims: Hashable | Iterable[Hashable] | None = None,
5067         skipna: bool = True,
5068         p0: dict[str, Any] | None = None,
5069         bounds: dict[str, Any] | None = None,
5070         param_names: Sequence[str] | None = None,
5071         kwargs: dict[str, Any] | None = None,
5072     ) -> Dataset:
5073         """
5074         Curve fitting optimization for arbitrary functions.
5075 
5076         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
5077 
5078         Parameters
5079         ----------
5080         coords : Hashable, DataArray, or sequence of DataArray or Hashable
5081             Independent coordinate(s) over which to perform the curve fitting. Must share
5082             at least one dimension with the calling object. When fitting multi-dimensional
5083             functions, supply `coords` as a sequence in the same order as arguments in
5084             `func`. To fit along existing dimensions of the calling object, `coords` can
5085             also be specified as a str or sequence of strs.
5086         func : callable
5087             User specified function in the form `f(x, *params)` which returns a numpy
5088             array of length `len(x)`. `params` are the fittable parameters which are optimized
5089             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
5090             coordinates, e.g. `f((x0, x1), *params)`.
5091         reduce_dims : Hashable or sequence of Hashable
5092             Additional dimension(s) over which to aggregate while fitting. For example,
5093             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
5094             aggregate all lat and lon points and fit the specified function along the
5095             time dimension.
5096         skipna : bool, default: True
5097             Whether to skip missing values when fitting. Default is True.
5098         p0 : dict-like or None, optional
5099             Optional dictionary of parameter names to initial guesses passed to the
5100             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
5101             be assigned initial values following the default scipy behavior.
5102         bounds : dict-like or None, optional
5103             Optional dictionary of parameter names to bounding values passed to the
5104             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
5105             will be unbounded following the default scipy behavior.
5106         param_names : sequence of Hashable or None, optional
5107             Sequence of names for the fittable parameters of `func`. If not supplied,
5108             this will be automatically determined by arguments of `func`. `param_names`
5109             should be manually supplied when fitting a function that takes a variable
5110             number of parameters.
5111         **kwargs : optional
5112             Additional keyword arguments to passed to scipy curve_fit.
5113 
5114         Returns
5115         -------
5116         curvefit_results : Dataset
5117             A single dataset which contains:
5118 
5119             [var]_curvefit_coefficients
5120                 The coefficients of the best fit.
5121             [var]_curvefit_covariance
5122                 The covariance matrix of the coefficient estimates.
5123 
5124         See Also
5125         --------
5126         DataArray.polyfit
5127         scipy.optimize.curve_fit
5128         """
5129         return self._to_temp_dataset().curvefit(
5130             coords,
5131             func,
5132             reduce_dims=reduce_dims,
5133             skipna=skipna,
5134             p0=p0,
5135             bounds=bounds,
5136             param_names=param_names,
5137             kwargs=kwargs,
5138         )
5139 
5140     def drop_duplicates(
5141         self: T_DataArray,
5142         dim: Hashable | Iterable[Hashable],
5143         keep: Literal["first", "last", False] = "first",
5144     ) -> T_DataArray:
5145         """Returns a new DataArray with duplicate dimension values removed.
5146 
5147         Parameters
5148         ----------
5149         dim : dimension label or labels
5150             Pass `...` to drop duplicates along all dimensions.
5151         keep : {"first", "last", False}, default: "first"
5152             Determines which duplicates (if any) to keep.
5153 
5154             - ``"first"`` : Drop duplicates except for the first occurrence.
5155             - ``"last"`` : Drop duplicates except for the last occurrence.
5156             - False : Drop all duplicates.
5157 
5158         Returns
5159         -------
5160         DataArray
5161 
5162         See Also
5163         --------
5164         Dataset.drop_duplicates
5165         """
5166         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
5167         return self._from_temp_dataset(deduplicated)
5168 
5169     def convert_calendar(
5170         self,
5171         calendar: str,
5172         dim: str = "time",
5173         align_on: str | None = None,
5174         missing: Any | None = None,
5175         use_cftime: bool | None = None,
5176     ) -> DataArray:
5177         """Convert the DataArray to another calendar.
5178 
5179         Only converts the individual timestamps, does not modify any data except
5180         in dropping invalid/surplus dates or inserting missing dates.
5181 
5182         If the source and target calendars are either no_leap, all_leap or a
5183         standard type, only the type of the time array is modified.
5184         When converting to a leap year from a non-leap year, the 29th of February
5185         is removed from the array. In the other direction the 29th of February
5186         will be missing in the output, unless `missing` is specified,
5187         in which case that value is inserted.
5188 
5189         For conversions involving `360_day` calendars, see Notes.
5190 
5191         This method is safe to use with sub-daily data as it doesn't touch the
5192         time part of the timestamps.
5193 
5194         Parameters
5195         ---------
5196         calendar : str
5197             The target calendar name.
5198         dim : str
5199             Name of the time coordinate.
5200         align_on : {None, 'date', 'year'}
5201             Must be specified when either source or target is a `360_day` calendar,
5202            ignored otherwise. See Notes.
5203         missing : Optional[any]
5204             By default, i.e. if the value is None, this method will simply attempt
5205             to convert the dates in the source calendar to the same dates in the
5206             target calendar, and drop any of those that are not possible to
5207             represent.  If a value is provided, a new time coordinate will be
5208             created in the target calendar with the same frequency as the original
5209             time coordinate; for any dates that are not present in the source, the
5210             data will be filled with this value.  Note that using this mode requires
5211             that the source data have an inferable frequency; for more information
5212             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
5213             target calendar combinations, this could result in many missing values, see notes.
5214         use_cftime : boolean, optional
5215             Whether to use cftime objects in the output, only used if `calendar`
5216             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
5217             If True, the new time axis uses cftime objects.
5218             If None (default), it uses :py:class:`numpy.datetime64` values if the
5219             date range permits it, and :py:class:`cftime.datetime` objects if not.
5220             If False, it uses :py:class:`numpy.datetime64`  or fails.
5221 
5222         Returns
5223         -------
5224         DataArray
5225             Copy of the dataarray with the time coordinate converted to the
5226             target calendar. If 'missing' was None (default), invalid dates in
5227             the new calendar are dropped, but missing dates are not inserted.
5228             If `missing` was given, the new data is reindexed to have a time axis
5229             with the same frequency as the source, but in the new calendar; any
5230             missing datapoints are filled with `missing`.
5231 
5232         Notes
5233         -----
5234         Passing a value to `missing` is only usable if the source's time coordinate as an
5235         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
5236         if the target coordinate, generated from this frequency, has dates equivalent to the
5237         source. It is usually **not** appropriate to use this mode with:
5238 
5239         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
5240         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
5241             or 'mH' where 24 % m != 0).
5242 
5243         If one of the source or target calendars is `"360_day"`, `align_on` must
5244         be specified and two options are offered.
5245 
5246         - "year"
5247             The dates are translated according to their relative position in the year,
5248             ignoring their original month and day information, meaning that the
5249             missing/surplus days are added/removed at regular intervals.
5250 
5251             From a `360_day` to a standard calendar, the output will be missing the
5252             following dates (day of year in parentheses):
5253 
5254             To a leap year:
5255                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
5256                 September 31st (275) and November 30th (335).
5257             To a non-leap year:
5258                 February 6th (36), April 19th (109), July 2nd (183),
5259                 September 12th (255), November 25th (329).
5260 
5261             From a standard calendar to a `"360_day"`, the following dates in the
5262             source array will be dropped:
5263 
5264             From a leap year:
5265                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
5266                 September 31st (275), December 1st (336)
5267             From a non-leap year:
5268                 February 6th (37), April 20th (110), July 2nd (183),
5269                 September 13th (256), November 25th (329)
5270 
5271             This option is best used on daily and subdaily data.
5272 
5273         - "date"
5274             The month/day information is conserved and invalid dates are dropped
5275             from the output. This means that when converting from a `"360_day"` to a
5276             standard calendar, all 31st (Jan, March, May, July, August, October and
5277             December) will be missing as there is no equivalent dates in the
5278             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
5279             will be dropped as there are no equivalent dates in a standard calendar.
5280 
5281             This option is best used with data on a frequency coarser than daily.
5282         """
5283         return convert_calendar(
5284             self,
5285             calendar,
5286             dim=dim,
5287             align_on=align_on,
5288             missing=missing,
5289             use_cftime=use_cftime,
5290         )
5291 
5292     def interp_calendar(
5293         self,
5294         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
5295         dim: str = "time",
5296     ) -> DataArray:
5297         """Interpolates the DataArray to another calendar based on decimal year measure.
5298 
5299         Each timestamp in `source` and `target` are first converted to their decimal
5300         year equivalent then `source` is interpolated on the target coordinate.
5301         The decimal year of a timestamp is its year plus its sub-year component
5302         converted to the fraction of its year. For example "2000-03-01 12:00" is
5303         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
5304 
5305         This method should only be used when the time (HH:MM:SS) information of
5306         time coordinate is not important.
5307 
5308         Parameters
5309         ----------
5310         target: DataArray or DatetimeIndex or CFTimeIndex
5311             The target time coordinate of a valid dtype
5312             (np.datetime64 or cftime objects)
5313         dim : str
5314             The time coordinate name.
5315 
5316         Return
5317         ------
5318         DataArray
5319             The source interpolated on the decimal years of target,
5320         """
5321         return interp_calendar(self, target, dim=dim)
5322 
5323     def groupby(
5324         self,
5325         group: Hashable | DataArray | IndexVariable,
5326         squeeze: bool = True,
5327         restore_coord_dims: bool = False,
5328     ) -> DataArrayGroupBy:
5329         """Returns a DataArrayGroupBy object for performing grouped operations.
5330 
5331         Parameters
5332         ----------
5333         group : Hashable, DataArray or IndexVariable
5334             Array whose unique values should be used to group this array. If a
5335             Hashable, must be the name of a coordinate contained in this dataarray.
5336         squeeze : bool, default: True
5337             If "group" is a dimension of any arrays in this dataset, `squeeze`
5338             controls whether the subarrays have a dimension of length 1 along
5339             that dimension or if the dimension is squeezed out.
5340         restore_coord_dims : bool, default: False
5341             If True, also restore the dimension order of multi-dimensional
5342             coordinates.
5343 
5344         Returns
5345         -------
5346         grouped : DataArrayGroupBy
5347             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5348             iterated over in the form of `(unique_value, grouped_array)` pairs.
5349 
5350         Examples
5351         --------
5352         Calculate daily anomalies for daily data:
5353 
5354         >>> da = xr.DataArray(
5355         ...     np.linspace(0, 1826, num=1827),
5356         ...     coords=[pd.date_range("1/1/2000", "31/12/2004", freq="D")],
5357         ...     dims="time",
5358         ... )
5359         >>> da
5360         <xarray.DataArray (time: 1827)>
5361         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
5362                1.826e+03])
5363         Coordinates:
5364           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5365         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
5366         <xarray.DataArray (time: 1827)>
5367         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
5368         Coordinates:
5369           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5370             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
5371 
5372         See Also
5373         --------
5374         DataArray.groupby_bins
5375         Dataset.groupby
5376         core.groupby.DataArrayGroupBy
5377         pandas.DataFrame.groupby
5378         """
5379         from .groupby import DataArrayGroupBy
5380 
5381         # While we don't generally check the type of every arg, passing
5382         # multiple dimensions as multiple arguments is common enough, and the
5383         # consequences hidden enough (strings evaluate as true) to warrant
5384         # checking here.
5385         # A future version could make squeeze kwarg only, but would face
5386         # backward-compat issues.
5387         if not isinstance(squeeze, bool):
5388             raise TypeError(
5389                 f"`squeeze` must be True or False, but {squeeze} was supplied"
5390             )
5391 
5392         return DataArrayGroupBy(
5393             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
5394         )
5395 
5396     def groupby_bins(
5397         self,
5398         group: Hashable | DataArray | IndexVariable,
5399         bins: ArrayLike,
5400         right: bool = True,
5401         labels: ArrayLike | Literal[False] | None = None,
5402         precision: int = 3,
5403         include_lowest: bool = False,
5404         squeeze: bool = True,
5405         restore_coord_dims: bool = False,
5406     ) -> DataArrayGroupBy:
5407         """Returns a DataArrayGroupBy object for performing grouped operations.
5408 
5409         Rather than using all unique values of `group`, the values are discretized
5410         first by applying `pandas.cut` [1]_ to `group`.
5411 
5412         Parameters
5413         ----------
5414         group : Hashable, DataArray or IndexVariable
5415             Array whose binned values should be used to group this array. If a
5416             Hashable, must be the name of a coordinate contained in this dataarray.
5417         bins : int or array-like
5418             If bins is an int, it defines the number of equal-width bins in the
5419             range of x. However, in this case, the range of x is extended by .1%
5420             on each side to include the min or max values of x. If bins is a
5421             sequence it defines the bin edges allowing for non-uniform bin
5422             width. No extension of the range of x is done in this case.
5423         right : bool, default: True
5424             Indicates whether the bins include the rightmost edge or not. If
5425             right == True (the default), then the bins [1,2,3,4] indicate
5426             (1,2], (2,3], (3,4].
5427         labels : array-like, False or None, default: None
5428             Used as labels for the resulting bins. Must be of the same length as
5429             the resulting bins. If False, string bin labels are assigned by
5430             `pandas.cut`.
5431         precision : int, default: 3
5432             The precision at which to store and display the bins labels.
5433         include_lowest : bool, default: False
5434             Whether the first interval should be left-inclusive or not.
5435         squeeze : bool, default: True
5436             If "group" is a dimension of any arrays in this dataset, `squeeze`
5437             controls whether the subarrays have a dimension of length 1 along
5438             that dimension or if the dimension is squeezed out.
5439         restore_coord_dims : bool, default: False
5440             If True, also restore the dimension order of multi-dimensional
5441             coordinates.
5442 
5443         Returns
5444         -------
5445         grouped : DataArrayGroupBy
5446             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5447             iterated over in the form of `(unique_value, grouped_array)` pairs.
5448             The name of the group has the added suffix `_bins` in order to
5449             distinguish it from the original variable.
5450 
5451         See Also
5452         --------
5453         DataArray.groupby
5454         Dataset.groupby_bins
5455         core.groupby.DataArrayGroupBy
5456         pandas.DataFrame.groupby
5457 
5458         References
5459         ----------
5460         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
5461         """
5462         from .groupby import DataArrayGroupBy
5463 
5464         return DataArrayGroupBy(
5465             self,
5466             group,
5467             squeeze=squeeze,
5468             bins=bins,
5469             restore_coord_dims=restore_coord_dims,
5470             cut_kwargs={
5471                 "right": right,
5472                 "labels": labels,
5473                 "precision": precision,
5474                 "include_lowest": include_lowest,
5475             },
5476         )
5477 
5478     def weighted(self, weights: DataArray) -> DataArrayWeighted:
5479         """
5480         Weighted DataArray operations.
5481 
5482         Parameters
5483         ----------
5484         weights : DataArray
5485             An array of weights associated with the values in this Dataset.
5486             Each value in the data contributes to the reduction operation
5487             according to its associated weight.
5488 
5489         Notes
5490         -----
5491         ``weights`` must be a DataArray and cannot contain missing values.
5492         Missing values can be replaced by ``weights.fillna(0)``.
5493 
5494         Returns
5495         -------
5496         core.weighted.DataArrayWeighted
5497 
5498         See Also
5499         --------
5500         Dataset.weighted
5501         """
5502         from .weighted import DataArrayWeighted
5503 
5504         return DataArrayWeighted(self, weights)
5505 
5506     def rolling(
5507         self,
5508         dim: Mapping[Any, int] | None = None,
5509         min_periods: int | None = None,
5510         center: bool | Mapping[Any, bool] = False,
5511         **window_kwargs: int,
5512     ) -> DataArrayRolling:
5513         """
5514         Rolling window object for DataArrays.
5515 
5516         Parameters
5517         ----------
5518         dim : dict, optional
5519             Mapping from the dimension name to create the rolling iterator
5520             along (e.g. `time`) to its moving window size.
5521         min_periods : int or None, default: None
5522             Minimum number of observations in window required to have a value
5523             (otherwise result is NA). The default, None, is equivalent to
5524             setting min_periods equal to the size of the window.
5525         center : bool or Mapping to int, default: False
5526             Set the labels at the center of the window.
5527         **window_kwargs : optional
5528             The keyword arguments form of ``dim``.
5529             One of dim or window_kwargs must be provided.
5530 
5531         Returns
5532         -------
5533         core.rolling.DataArrayRolling
5534 
5535         Examples
5536         --------
5537         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
5538 
5539         >>> da = xr.DataArray(
5540         ...     np.linspace(0, 11, num=12),
5541         ...     coords=[
5542         ...         pd.date_range(
5543         ...             "1999-12-15",
5544         ...             periods=12,
5545         ...             freq=pd.DateOffset(months=1),
5546         ...         )
5547         ...     ],
5548         ...     dims="time",
5549         ... )
5550         >>> da
5551         <xarray.DataArray (time: 12)>
5552         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5553         Coordinates:
5554           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5555         >>> da.rolling(time=3, center=True).mean()
5556         <xarray.DataArray (time: 12)>
5557         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
5558         Coordinates:
5559           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5560 
5561         Remove the NaNs using ``dropna()``:
5562 
5563         >>> da.rolling(time=3, center=True).mean().dropna("time")
5564         <xarray.DataArray (time: 10)>
5565         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
5566         Coordinates:
5567           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
5568 
5569         See Also
5570         --------
5571         core.rolling.DataArrayRolling
5572         Dataset.rolling
5573         """
5574         from .rolling import DataArrayRolling
5575 
5576         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
5577         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
5578 
5579     def coarsen(
5580         self,
5581         dim: Mapping[Any, int] | None = None,
5582         boundary: CoarsenBoundaryOptions = "exact",
5583         side: SideOptions | Mapping[Any, SideOptions] = "left",
5584         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
5585         **window_kwargs: int,
5586     ) -> DataArrayCoarsen:
5587         """
5588         Coarsen object for DataArrays.
5589 
5590         Parameters
5591         ----------
5592         dim : mapping of hashable to int, optional
5593             Mapping from the dimension name to the window size.
5594         boundary : {"exact", "trim", "pad"}, default: "exact"
5595             If 'exact', a ValueError will be raised if dimension size is not a
5596             multiple of the window size. If 'trim', the excess entries are
5597             dropped. If 'pad', NA will be padded.
5598         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
5599         coord_func : str or mapping of hashable to str, default: "mean"
5600             function (name) that is applied to the coordinates,
5601             or a mapping from coordinate name to function (name).
5602 
5603         Returns
5604         -------
5605         core.rolling.DataArrayCoarsen
5606 
5607         Examples
5608         --------
5609         Coarsen the long time series by averaging over every four days.
5610 
5611         >>> da = xr.DataArray(
5612         ...     np.linspace(0, 364, num=364),
5613         ...     dims="time",
5614         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
5615         ... )
5616         >>> da  # +doctest: ELLIPSIS
5617         <xarray.DataArray (time: 364)>
5618         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
5619                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
5620                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
5621         ...
5622                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
5623                360.99173554, 361.99449036, 362.99724518, 364.        ])
5624         Coordinates:
5625           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
5626         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
5627         <xarray.DataArray (time: 121)>
5628         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
5629                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
5630                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
5631         ...
5632                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
5633                361.99449036])
5634         Coordinates:
5635           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
5636         >>>
5637 
5638         See Also
5639         --------
5640         core.rolling.DataArrayCoarsen
5641         Dataset.coarsen
5642         """
5643         from .rolling import DataArrayCoarsen
5644 
5645         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
5646         return DataArrayCoarsen(
5647             self,
5648             dim,
5649             boundary=boundary,
5650             side=side,
5651             coord_func=coord_func,
5652         )
5653 
5654     def resample(
5655         self,
5656         indexer: Mapping[Any, str] | None = None,
5657         skipna: bool | None = None,
5658         closed: SideOptions | None = None,
5659         label: SideOptions | None = None,
5660         base: int = 0,
5661         keep_attrs: bool | None = None,
5662         loffset: datetime.timedelta | str | None = None,
5663         restore_coord_dims: bool | None = None,
5664         **indexer_kwargs: str,
5665     ) -> DataArrayResample:
5666         """Returns a Resample object for performing resampling operations.
5667 
5668         Handles both downsampling and upsampling. The resampled
5669         dimension must be a datetime-like coordinate. If any intervals
5670         contain no values from the original object, they will be given
5671         the value ``NaN``.
5672 
5673         Parameters
5674         ----------
5675         indexer : Mapping of Hashable to str, optional
5676             Mapping from the dimension name to resample frequency [1]_. The
5677             dimension must be datetime-like.
5678         skipna : bool, optional
5679             Whether to skip missing values when aggregating in downsampling.
5680         closed : {"left", "right"}, optional
5681             Side of each interval to treat as closed.
5682         label : {"left", "right"}, optional
5683             Side of each interval to use for labeling.
5684         base : int, default = 0
5685             For frequencies that evenly subdivide 1 day, the "origin" of the
5686             aggregated intervals. For example, for "24H" frequency, base could
5687             range from 0 through 23.
5688         loffset : timedelta or str, optional
5689             Offset used to adjust the resampled time labels. Some pandas date
5690             offset strings are supported.
5691         restore_coord_dims : bool, optional
5692             If True, also restore the dimension order of multi-dimensional
5693             coordinates.
5694         **indexer_kwargs : str
5695             The keyword arguments form of ``indexer``.
5696             One of indexer or indexer_kwargs must be provided.
5697 
5698         Returns
5699         -------
5700         resampled : core.resample.DataArrayResample
5701             This object resampled.
5702 
5703         Examples
5704         --------
5705         Downsample monthly time-series data to seasonal data:
5706 
5707         >>> da = xr.DataArray(
5708         ...     np.linspace(0, 11, num=12),
5709         ...     coords=[
5710         ...         pd.date_range(
5711         ...             "1999-12-15",
5712         ...             periods=12,
5713         ...             freq=pd.DateOffset(months=1),
5714         ...         )
5715         ...     ],
5716         ...     dims="time",
5717         ... )
5718         >>> da
5719         <xarray.DataArray (time: 12)>
5720         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5721         Coordinates:
5722           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5723         >>> da.resample(time="QS-DEC").mean()
5724         <xarray.DataArray (time: 4)>
5725         array([ 1.,  4.,  7., 10.])
5726         Coordinates:
5727           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
5728 
5729         Upsample monthly time-series data to daily data:
5730 
5731         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
5732         <xarray.DataArray (time: 337)>
5733         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
5734                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
5735                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
5736         ...
5737                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
5738                10.96774194, 11.        ])
5739         Coordinates:
5740           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5741 
5742         Limit scope of upsampling method
5743 
5744         >>> da.resample(time="1D").nearest(tolerance="1D")
5745         <xarray.DataArray (time: 337)>
5746         array([ 0.,  0., nan, ..., nan, 11., 11.])
5747         Coordinates:
5748           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5749 
5750         See Also
5751         --------
5752         Dataset.resample
5753         pandas.Series.resample
5754         pandas.DataFrame.resample
5755 
5756         References
5757         ----------
5758         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
5759         """
5760         from .resample import DataArrayResample
5761 
5762         return self._resample(
5763             resample_cls=DataArrayResample,
5764             indexer=indexer,
5765             skipna=skipna,
5766             closed=closed,
5767             label=label,
5768             base=base,
5769             keep_attrs=keep_attrs,
5770             loffset=loffset,
5771             restore_coord_dims=restore_coord_dims,
5772             **indexer_kwargs,
5773         )
5774 
5775     # this needs to be at the end, or mypy will confuse with `str`
5776     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
5777     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 09:02:33.535 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 09:02:33.535 | INFO     | Search-only: skipping patch selection
2025-10-15 09:02:33.627 | INFO     | Task pydata__xarray-6999 completed successfully.
2025-10-15 09:02:33.628 | INFO     | No patch generated. You can try running ACR again.
