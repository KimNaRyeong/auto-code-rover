2025-10-15 04:11:59.970 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 04:12:02.817 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=650, output_tokens=12, cost=0.006860
2025-10-15 04:12:18.752 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=830, output_tokens=667, cost=0.028310
2025-10-15 04:12:20.069 | INFO     | registered non reproducing test 0
2025-10-15 04:12:34.023 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2025, output_tokens=678, cost=0.040590
2025-10-15 04:12:35.338 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 04:12:43.019 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=955, output_tokens=379, cost=0.020920
2025-10-15 04:12:43.028 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 04:12:45.523 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=733, output_tokens=89, cost=0.010000
2025-10-15 04:12:45.524 | DEBUG    | Extracted a valid json.
2025-10-15 04:12:45.524 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_class(\"select_related\", \"QuerySet\")",
        "search_method_in_class(\"_clone\", \"QuerySet\")",
        "search_method_in_class(\"prefetch_related\", \"QuerySet\")",
        "search_class(\"ModelA\")",
        "search_class(\"ModelB\")",
        "search_class(\"ModelC\")"
    ],
    "bug_locations": []
}
2025-10-15 04:12:45.549 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 04:12:45.550 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 04:13:05.203 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2316, output_tokens=911, cost=0.050490
2025-10-15 04:13:05.221 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 04:13:15.857 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3443, output_tokens=357, cost=0.045140
2025-10-15 04:13:15.862 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 04:13:24.413 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=711, output_tokens=296, cost=0.015990
2025-10-15 04:13:24.413 | DEBUG    | Extracted a valid json.
2025-10-15 04:13:24.414 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "select_related",
            "intended_behavior": "The select_related method should continue to allow for the addition of related fields to be fetched in a single database query when the method is called on a QuerySet. However, it must ensure that any modifications made after select_related is called on a cloned QuerySet instance do not impact the original QuerySet from which the clone was derived. This could potentially require examining the _chain method or the way query objects are handled to ensure deep copying of relevant parts to prevent shared state between clones and originals."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "_clone",
            "intended_behavior": "The _clone method's intended behavior is to produce a fully independent copy of the QuerySet, which implies no shared state between the original and the cloned instance that could lead to the mutations observed. A thorough review is necessary to ensure that all attributes, especially those that can be modified like query (which houses select_related logic), are correctly deep copied or uniquely instantiated to preserve this independence. This might involve changing the way references to mutable objects (like dicts or custom objects) are copied, ensuring a true deep copy is made where necessary."
        }
    ]
}
2025-10-15 04:13:24.422 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>select_related</method>
<code>
991     def select_related(self, *fields):
992         """
993         Return a new QuerySet instance that will select related objects.
994 
995         If fields are specified, they must be ForeignKey fields and only those
996         related objects are included in the selection.
997 
998         If select_related(None) is called, clear the list.
999         """
1000         self._not_support_combined_queries('select_related')
1001         if self._fields is not None:
1002             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1003 
1004         obj = self._chain()
1005         if fields == (None,):
1006             obj.query.select_related = False
1007         elif fields:
1008             obj.query.add_select_related(fields)
1009         else:
1010             obj.query.select_related = True
1011         return obj

</code><intended_behavior>The select_related method should continue to allow for the addition of related fields to be fetched in a single database query when the method is called on a QuerySet. However, it must ensure that any modifications made after select_related is called on a cloned QuerySet instance do not impact the original QuerySet from which the clone was derived. This could potentially require examining the _chain method or the way query objects are handled to ensure deep copying of relevant parts to prevent shared state between clones and originals.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
188 class QuerySet:
189     """Represent a lazy database lookup for a set of objects."""
190 
191     def __init__(self, model=None, query=None, using=None, hints=None):
192         self.model = model
193         self._db = using
194         self._hints = hints or {}
195         self.query = query or sql.Query(self.model)
196         self._result_cache = None
197         self._sticky_filter = False
198         self._for_write = False
199         self._prefetch_related_lookups = ()
200         self._prefetch_done = False
201         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
202         self._iterable_class = ModelIterable
203         self._fields = None
204 
205     def as_manager(cls):
206         # Address the circular dependency between `Queryset` and `Manager`.
207         from django.db.models.manager import Manager
208         manager = Manager.from_queryset(cls)()
209         manager._built_with_as_manager = True
210         return manager
211     as_manager.queryset_only = True
212     as_manager = classmethod(as_manager)
213 
214     ########################
215     # PYTHON MAGIC METHODS #
216     ########################
217 
218     def __deepcopy__(self, memo):
219         """Don't populate the QuerySet's cache."""
220         obj = self.__class__()
221         for k, v in self.__dict__.items():
222             if k == '_result_cache':
223                 obj.__dict__[k] = None
224             else:
225                 obj.__dict__[k] = copy.deepcopy(v, memo)
226         return obj
227 
228     def __getstate__(self):
229         # Force the cache to be fully populated.
230         self._fetch_all()
231         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
232 
233     def __setstate__(self, state):
234         msg = None
235         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
236         if pickled_version:
237             current_version = get_version()
238             if current_version != pickled_version:
239                 msg = (
240                     "Pickled queryset instance's Django version %s does not "
241                     "match the current version %s." % (pickled_version, current_version)
242                 )
243         else:
244             msg = "Pickled queryset instance's Django version is not specified."
245 
246         if msg:
247             warnings.warn(msg, RuntimeWarning, stacklevel=2)
248 
249         self.__dict__.update(state)
250 
251     def __repr__(self):
252         data = list(self[:REPR_OUTPUT_SIZE + 1])
253         if len(data) > REPR_OUTPUT_SIZE:
254             data[-1] = "...(remaining elements truncated)..."
255         return '<%s %r>' % (self.__class__.__name__, data)
256 
257     def __len__(self):
258         self._fetch_all()
259         return len(self._result_cache)
260 
261     def __iter__(self):
262         """
263         The queryset iterator protocol uses three nested iterators in the
264         default case:
265             1. sql.compiler.execute_sql()
266                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
267                  using cursor.fetchmany(). This part is responsible for
268                  doing some column masking, and returning the rows in chunks.
269             2. sql.compiler.results_iter()
270                - Returns one row at time. At this point the rows are still just
271                  tuples. In some cases the return values are converted to
272                  Python values at this location.
273             3. self.iterator()
274                - Responsible for turning the rows into model objects.
275         """
276         self._fetch_all()
277         return iter(self._result_cache)
278 
279     def __bool__(self):
280         self._fetch_all()
281         return bool(self._result_cache)
282 
283     def __getitem__(self, k):
284         """Retrieve an item or slice from the set of results."""
285         if not isinstance(k, (int, slice)):
286             raise TypeError(
287                 'QuerySet indices must be integers or slices, not %s.'
288                 % type(k).__name__
289             )
290         assert ((not isinstance(k, slice) and (k >= 0)) or
291                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
292                  (k.stop is None or k.stop >= 0))), \
293             "Negative indexing is not supported."
294 
295         if self._result_cache is not None:
296             return self._result_cache[k]
297 
298         if isinstance(k, slice):
299             qs = self._chain()
300             if k.start is not None:
301                 start = int(k.start)
302             else:
303                 start = None
304             if k.stop is not None:
305                 stop = int(k.stop)
306             else:
307                 stop = None
308             qs.query.set_limits(start, stop)
309             return list(qs)[::k.step] if k.step else qs
310 
311         qs = self._chain()
312         qs.query.set_limits(k, k + 1)
313         qs._fetch_all()
314         return qs._result_cache[0]
315 
316     def __and__(self, other):
317         self._merge_sanity_check(other)
318         if isinstance(other, EmptyQuerySet):
319             return other
320         if isinstance(self, EmptyQuerySet):
321             return self
322         combined = self._chain()
323         combined._merge_known_related_objects(other)
324         combined.query.combine(other.query, sql.AND)
325         return combined
326 
327     def __or__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(self, EmptyQuerySet):
330             return other
331         if isinstance(other, EmptyQuerySet):
332             return self
333         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
334         combined = query._chain()
335         combined._merge_known_related_objects(other)
336         if not other.query.can_filter():
337             other = other.model._base_manager.filter(pk__in=other.values('pk'))
338         combined.query.combine(other.query, sql.OR)
339         return combined
340 
341     ####################################
342     # METHODS THAT DO DATABASE QUERIES #
343     ####################################
344 
345     def _iterator(self, use_chunked_fetch, chunk_size):
346         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
347 
348     def iterator(self, chunk_size=2000):
349         """
350         An iterator over the results from applying this QuerySet to the
351         database.
352         """
353         if chunk_size <= 0:
354             raise ValueError('Chunk size must be strictly positive.')
355         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
356         return self._iterator(use_chunked_fetch, chunk_size)
357 
358     def aggregate(self, *args, **kwargs):
359         """
360         Return a dictionary containing the calculations (aggregation)
361         over the current queryset.
362 
363         If args is present the expression is passed as a kwarg using
364         the Aggregate object's default alias.
365         """
366         if self.query.distinct_fields:
367             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
368         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
369         for arg in args:
370             # The default_alias property raises TypeError if default_alias
371             # can't be set automatically or AttributeError if it isn't an
372             # attribute.
373             try:
374                 arg.default_alias
375             except (AttributeError, TypeError):
376                 raise TypeError("Complex aggregates require an alias")
377             kwargs[arg.default_alias] = arg
378 
379         query = self.query.chain()
380         for (alias, aggregate_expr) in kwargs.items():
381             query.add_annotation(aggregate_expr, alias, is_summary=True)
382             if not query.annotations[alias].contains_aggregate:
383                 raise TypeError("%s is not an aggregate expression" % alias)
384         return query.get_aggregation(self.db, kwargs)
385 
386     def count(self):
387         """
388         Perform a SELECT COUNT() and return the number of records as an
389         integer.
390 
391         If the QuerySet is already fully cached, return the length of the
392         cached results set to avoid multiple SELECT COUNT(*) calls.
393         """
394         if self._result_cache is not None:
395             return len(self._result_cache)
396 
397         return self.query.get_count(using=self.db)
398 
399     def get(self, *args, **kwargs):
400         """
401         Perform the query and return a single object matching the given
402         keyword arguments.
403         """
404         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
405         if self.query.can_filter() and not self.query.distinct_fields:
406             clone = clone.order_by()
407         limit = None
408         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
409             limit = MAX_GET_RESULTS
410             clone.query.set_limits(high=limit)
411         num = len(clone)
412         if num == 1:
413             return clone._result_cache[0]
414         if not num:
415             raise self.model.DoesNotExist(
416                 "%s matching query does not exist." %
417                 self.model._meta.object_name
418             )
419         raise self.model.MultipleObjectsReturned(
420             'get() returned more than one %s -- it returned %s!' % (
421                 self.model._meta.object_name,
422                 num if not limit or num < limit else 'more than %s' % (limit - 1),
423             )
424         )
425 
426     def create(self, **kwargs):
427         """
428         Create a new object with the given kwargs, saving it to the database
429         and returning the created object.
430         """
431         obj = self.model(**kwargs)
432         self._for_write = True
433         obj.save(force_insert=True, using=self.db)
434         return obj
435 
436     def _populate_pk_values(self, objs):
437         for obj in objs:
438             if obj.pk is None:
439                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
440 
441     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
442         """
443         Insert each of the instances into the database. Do *not* call
444         save() on each of the instances, do not send any pre/post_save
445         signals, and do not set the primary key attribute if it is an
446         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
447         Multi-table models are not supported.
448         """
449         # When you bulk insert you don't get the primary keys back (if it's an
450         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
451         # you can't insert into the child tables which references this. There
452         # are two workarounds:
453         # 1) This could be implemented if you didn't have an autoincrement pk
454         # 2) You could do it by doing O(n) normal inserts into the parent
455         #    tables to get the primary keys back and then doing a single bulk
456         #    insert into the childmost table.
457         # We currently set the primary keys on the objects when using
458         # PostgreSQL via the RETURNING ID clause. It should be possible for
459         # Oracle as well, but the semantics for extracting the primary keys is
460         # trickier so it's not done yet.
461         assert batch_size is None or batch_size > 0
462         # Check that the parents share the same concrete model with the our
463         # model to detect the inheritance pattern ConcreteGrandParent ->
464         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
465         # would not identify that case as involving multiple tables.
466         for parent in self.model._meta.get_parent_list():
467             if parent._meta.concrete_model is not self.model._meta.concrete_model:
468                 raise ValueError("Can't bulk create a multi-table inherited model")
469         if not objs:
470             return objs
471         self._for_write = True
472         connection = connections[self.db]
473         opts = self.model._meta
474         fields = opts.concrete_fields
475         objs = list(objs)
476         self._populate_pk_values(objs)
477         with transaction.atomic(using=self.db, savepoint=False):
478             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
479             if objs_with_pk:
480                 returned_columns = self._batched_insert(
481                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
482                 )
483                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
484                     for result, field in zip(results, opts.db_returning_fields):
485                         if field != opts.pk:
486                             setattr(obj_with_pk, field.attname, result)
487                 for obj_with_pk in objs_with_pk:
488                     obj_with_pk._state.adding = False
489                     obj_with_pk._state.db = self.db
490             if objs_without_pk:
491                 fields = [f for f in fields if not isinstance(f, AutoField)]
492                 returned_columns = self._batched_insert(
493                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
494                 )
495                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
496                     assert len(returned_columns) == len(objs_without_pk)
497                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         setattr(obj_without_pk, field.attname, result)
500                     obj_without_pk._state.adding = False
501                     obj_without_pk._state.db = self.db
502 
503         return objs
504 
505     def bulk_update(self, objs, fields, batch_size=None):
506         """
507         Update the given fields in each of the given objects in the database.
508         """
509         if batch_size is not None and batch_size < 0:
510             raise ValueError('Batch size must be a positive integer.')
511         if not fields:
512             raise ValueError('Field names must be given to bulk_update().')
513         objs = tuple(objs)
514         if any(obj.pk is None for obj in objs):
515             raise ValueError('All bulk_update() objects must have a primary key set.')
516         fields = [self.model._meta.get_field(name) for name in fields]
517         if any(not f.concrete or f.many_to_many for f in fields):
518             raise ValueError('bulk_update() can only be used with concrete fields.')
519         if any(f.primary_key for f in fields):
520             raise ValueError('bulk_update() cannot be used with primary key fields.')
521         if not objs:
522             return
523         # PK is used twice in the resulting update query, once in the filter
524         # and once in the WHEN. Each field will also have one CAST.
525         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
526         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
527         requires_casting = connections[self.db].features.requires_casted_case_in_updates
528         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
529         updates = []
530         for batch_objs in batches:
531             update_kwargs = {}
532             for field in fields:
533                 when_statements = []
534                 for obj in batch_objs:
535                     attr = getattr(obj, field.attname)
536                     if not isinstance(attr, Expression):
537                         attr = Value(attr, output_field=field)
538                     when_statements.append(When(pk=obj.pk, then=attr))
539                 case_statement = Case(*when_statements, output_field=field)
540                 if requires_casting:
541                     case_statement = Cast(case_statement, output_field=field)
542                 update_kwargs[field.attname] = case_statement
543             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
544         with transaction.atomic(using=self.db, savepoint=False):
545             for pks, update_kwargs in updates:
546                 self.filter(pk__in=pks).update(**update_kwargs)
547     bulk_update.alters_data = True
548 
549     def get_or_create(self, defaults=None, **kwargs):
550         """
551         Look up an object with the given kwargs, creating one if necessary.
552         Return a tuple of (object, created), where created is a boolean
553         specifying whether an object was created.
554         """
555         # The get() needs to be targeted at the write database in order
556         # to avoid potential transaction consistency problems.
557         self._for_write = True
558         try:
559             return self.get(**kwargs), False
560         except self.model.DoesNotExist:
561             params = self._extract_model_params(defaults, **kwargs)
562             return self._create_object_from_params(kwargs, params)
563 
564     def update_or_create(self, defaults=None, **kwargs):
565         """
566         Look up an object with the given kwargs, updating one with defaults
567         if it exists, otherwise create a new one.
568         Return a tuple (object, created), where created is a boolean
569         specifying whether an object was created.
570         """
571         defaults = defaults or {}
572         self._for_write = True
573         with transaction.atomic(using=self.db):
574             try:
575                 obj = self.select_for_update().get(**kwargs)
576             except self.model.DoesNotExist:
577                 params = self._extract_model_params(defaults, **kwargs)
578                 # Lock the row so that a concurrent update is blocked until
579                 # after update_or_create() has performed its save.
580                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
581                 if created:
582                     return obj, created
583             for k, v in defaults.items():
584                 setattr(obj, k, v() if callable(v) else v)
585             obj.save(using=self.db)
586         return obj, False
587 
588     def _create_object_from_params(self, lookup, params, lock=False):
589         """
590         Try to create an object using passed params. Used by get_or_create()
591         and update_or_create().
592         """
593         try:
594             with transaction.atomic(using=self.db):
595                 params = {k: v() if callable(v) else v for k, v in params.items()}
596                 obj = self.create(**params)
597             return obj, True
598         except IntegrityError as e:
599             try:
600                 qs = self.select_for_update() if lock else self
601                 return qs.get(**lookup), False
602             except self.model.DoesNotExist:
603                 pass
604             raise e
605 
606     def _extract_model_params(self, defaults, **kwargs):
607         """
608         Prepare `params` for creating a model instance based on the given
609         kwargs; for use by get_or_create() and update_or_create().
610         """
611         defaults = defaults or {}
612         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
613         params.update(defaults)
614         property_names = self.model._meta._property_names
615         invalid_params = []
616         for param in params:
617             try:
618                 self.model._meta.get_field(param)
619             except exceptions.FieldDoesNotExist:
620                 # It's okay to use a model's property if it has a setter.
621                 if not (param in property_names and getattr(self.model, param).fset):
622                     invalid_params.append(param)
623         if invalid_params:
624             raise exceptions.FieldError(
625                 "Invalid field name(s) for model %s: '%s'." % (
626                     self.model._meta.object_name,
627                     "', '".join(sorted(invalid_params)),
628                 ))
629         return params
630 
631     def _earliest(self, *fields):
632         """
633         Return the earliest object according to fields (if given) or by the
634         model's Meta.get_latest_by.
635         """
636         if fields:
637             order_by = fields
638         else:
639             order_by = getattr(self.model._meta, 'get_latest_by')
640             if order_by and not isinstance(order_by, (tuple, list)):
641                 order_by = (order_by,)
642         if order_by is None:
643             raise ValueError(
644                 "earliest() and latest() require either fields as positional "
645                 "arguments or 'get_latest_by' in the model's Meta."
646             )
647 
648         assert not self.query.is_sliced, \
649             "Cannot change a query once a slice has been taken."
650         obj = self._chain()
651         obj.query.set_limits(high=1)
652         obj.query.clear_ordering(force_empty=True)
653         obj.query.add_ordering(*order_by)
654         return obj.get()
655 
656     def earliest(self, *fields):
657         return self._earliest(*fields)
658 
659     def latest(self, *fields):
660         return self.reverse()._earliest(*fields)
661 
662     def first(self):
663         """Return the first object of a query or None if no match is found."""
664         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
665             return obj
666 
667     def last(self):
668         """Return the last object of a query or None if no match is found."""
669         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
670             return obj
671 
672     def in_bulk(self, id_list=None, *, field_name='pk'):
673         """
674         Return a dictionary mapping each of the given IDs to the object with
675         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
676         """
677         assert not self.query.is_sliced, \
678             "Cannot use 'limit' or 'offset' with in_bulk"
679         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
680             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
681         if id_list is not None:
682             if not id_list:
683                 return {}
684             filter_key = '{}__in'.format(field_name)
685             batch_size = connections[self.db].features.max_query_params
686             id_list = tuple(id_list)
687             # If the database has a limit on the number of query parameters
688             # (e.g. SQLite), retrieve objects in batches if necessary.
689             if batch_size and batch_size < len(id_list):
690                 qs = ()
691                 for offset in range(0, len(id_list), batch_size):
692                     batch = id_list[offset:offset + batch_size]
693                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
694             else:
695                 qs = self.filter(**{filter_key: id_list}).order_by()
696         else:
697             qs = self._chain()
698         return {getattr(obj, field_name): obj for obj in qs}
699 
700     def delete(self):
701         """Delete the records in the current QuerySet."""
702         assert not self.query.is_sliced, \
703             "Cannot use 'limit' or 'offset' with delete."
704 
705         if self._fields is not None:
706             raise TypeError("Cannot call delete() after .values() or .values_list()")
707 
708         del_query = self._chain()
709 
710         # The delete is actually 2 queries - one to find related objects,
711         # and one to delete. Make sure that the discovery of related
712         # objects is performed on the same database as the deletion.
713         del_query._for_write = True
714 
715         # Disable non-supported fields.
716         del_query.query.select_for_update = False
717         del_query.query.select_related = False
718         del_query.query.clear_ordering(force_empty=True)
719 
720         collector = Collector(using=del_query.db)
721         collector.collect(del_query)
722         deleted, _rows_count = collector.delete()
723 
724         # Clear the result cache, in case this QuerySet gets reused.
725         self._result_cache = None
726         return deleted, _rows_count
727 
728     delete.alters_data = True
729     delete.queryset_only = True
730 
731     def _raw_delete(self, using):
732         """
733         Delete objects found from the given queryset in single direct SQL
734         query. No signals are sent and there is no protection for cascades.
735         """
736         return sql.DeleteQuery(self.model).delete_qs(self, using)
737     _raw_delete.alters_data = True
738 
739     def update(self, **kwargs):
740         """
741         Update all elements in the current QuerySet, setting all the given
742         fields to the appropriate values.
743         """
744         assert not self.query.is_sliced, \
745             "Cannot update a query once a slice has been taken."
746         self._for_write = True
747         query = self.query.chain(sql.UpdateQuery)
748         query.add_update_values(kwargs)
749         # Clear any annotations so that they won't be present in subqueries.
750         query.annotations = {}
751         with transaction.mark_for_rollback_on_error(using=self.db):
752             rows = query.get_compiler(self.db).execute_sql(CURSOR)
753         self._result_cache = None
754         return rows
755     update.alters_data = True
756 
757     def _update(self, values):
758         """
759         A version of update() that accepts field objects instead of field names.
760         Used primarily for model saving and not intended for use by general
761         code (it requires too much poking around at model internals to be
762         useful at that level).
763         """
764         assert not self.query.is_sliced, \
765             "Cannot update a query once a slice has been taken."
766         query = self.query.chain(sql.UpdateQuery)
767         query.add_update_fields(values)
768         # Clear any annotations so that they won't be present in subqueries.
769         query.annotations = {}
770         self._result_cache = None
771         return query.get_compiler(self.db).execute_sql(CURSOR)
772     _update.alters_data = True
773     _update.queryset_only = False
774 
775     def exists(self):
776         if self._result_cache is None:
777             return self.query.has_results(using=self.db)
778         return bool(self._result_cache)
779 
780     def _prefetch_related_objects(self):
781         # This method can only be called once the result cache has been filled.
782         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
783         self._prefetch_done = True
784 
785     def explain(self, *, format=None, **options):
786         return self.query.explain(using=self.db, format=format, **options)
787 
788     ##################################################
789     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
790     ##################################################
791 
792     def raw(self, raw_query, params=None, translations=None, using=None):
793         if using is None:
794             using = self.db
795         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
796         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
797         return qs
798 
799     def _values(self, *fields, **expressions):
800         clone = self._chain()
801         if expressions:
802             clone = clone.annotate(**expressions)
803         clone._fields = fields
804         clone.query.set_values(fields)
805         return clone
806 
807     def values(self, *fields, **expressions):
808         fields += tuple(expressions)
809         clone = self._values(*fields, **expressions)
810         clone._iterable_class = ValuesIterable
811         return clone
812 
813     def values_list(self, *fields, flat=False, named=False):
814         if flat and named:
815             raise TypeError("'flat' and 'named' can't be used together.")
816         if flat and len(fields) > 1:
817             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
818 
819         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
820         _fields = []
821         expressions = {}
822         counter = 1
823         for field in fields:
824             if hasattr(field, 'resolve_expression'):
825                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
826                 while True:
827                     field_id = field_id_prefix + str(counter)
828                     counter += 1
829                     if field_id not in field_names:
830                         break
831                 expressions[field_id] = field
832                 _fields.append(field_id)
833             else:
834                 _fields.append(field)
835 
836         clone = self._values(*_fields, **expressions)
837         clone._iterable_class = (
838             NamedValuesListIterable if named
839             else FlatValuesListIterable if flat
840             else ValuesListIterable
841         )
842         return clone
843 
844     def dates(self, field_name, kind, order='ASC'):
845         """
846         Return a list of date objects representing all available dates for
847         the given field_name, scoped to 'kind'.
848         """
849         assert kind in ('year', 'month', 'week', 'day'), \
850             "'kind' must be one of 'year', 'month', 'week', or 'day'."
851         assert order in ('ASC', 'DESC'), \
852             "'order' must be either 'ASC' or 'DESC'."
853         return self.annotate(
854             datefield=Trunc(field_name, kind, output_field=DateField()),
855             plain_field=F(field_name)
856         ).values_list(
857             'datefield', flat=True
858         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
859 
860     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
861         """
862         Return a list of datetime objects representing all available
863         datetimes for the given field_name, scoped to 'kind'.
864         """
865         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
866             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
867         assert order in ('ASC', 'DESC'), \
868             "'order' must be either 'ASC' or 'DESC'."
869         if settings.USE_TZ:
870             if tzinfo is None:
871                 tzinfo = timezone.get_current_timezone()
872         else:
873             tzinfo = None
874         return self.annotate(
875             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
876             plain_field=F(field_name)
877         ).values_list(
878             'datetimefield', flat=True
879         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
880 
881     def none(self):
882         """Return an empty QuerySet."""
883         clone = self._chain()
884         clone.query.set_empty()
885         return clone
886 
887     ##################################################################
888     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
889     ##################################################################
890 
891     def all(self):
892         """
893         Return a new QuerySet that is a copy of the current one. This allows a
894         QuerySet to proxy for a model manager in some cases.
895         """
896         return self._chain()
897 
898     def filter(self, *args, **kwargs):
899         """
900         Return a new QuerySet instance with the args ANDed to the existing
901         set.
902         """
903         self._not_support_combined_queries('filter')
904         return self._filter_or_exclude(False, *args, **kwargs)
905 
906     def exclude(self, *args, **kwargs):
907         """
908         Return a new QuerySet instance with NOT (args) ANDed to the existing
909         set.
910         """
911         self._not_support_combined_queries('exclude')
912         return self._filter_or_exclude(True, *args, **kwargs)
913 
914     def _filter_or_exclude(self, negate, *args, **kwargs):
915         if args or kwargs:
916             assert not self.query.is_sliced, \
917                 "Cannot filter a query once a slice has been taken."
918 
919         clone = self._chain()
920         if negate:
921             clone.query.add_q(~Q(*args, **kwargs))
922         else:
923             clone.query.add_q(Q(*args, **kwargs))
924         return clone
925 
926     def complex_filter(self, filter_obj):
927         """
928         Return a new QuerySet instance with filter_obj added to the filters.
929 
930         filter_obj can be a Q object or a dictionary of keyword lookup
931         arguments.
932 
933         This exists to support framework features such as 'limit_choices_to',
934         and usually it will be more natural to use other methods.
935         """
936         if isinstance(filter_obj, Q):
937             clone = self._chain()
938             clone.query.add_q(filter_obj)
939             return clone
940         else:
941             return self._filter_or_exclude(None, **filter_obj)
942 
943     def _combinator_query(self, combinator, *other_qs, all=False):
944         # Clone the query to inherit the select list and everything
945         clone = self._chain()
946         # Clear limits and ordering so they can be reapplied
947         clone.query.clear_ordering(True)
948         clone.query.clear_limits()
949         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
950         clone.query.combinator = combinator
951         clone.query.combinator_all = all
952         return clone
953 
954     def union(self, *other_qs, all=False):
955         # If the query is an EmptyQuerySet, combine all nonempty querysets.
956         if isinstance(self, EmptyQuerySet):
957             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
958             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
959         return self._combinator_query('union', *other_qs, all=all)
960 
961     def intersection(self, *other_qs):
962         # If any query is an EmptyQuerySet, return it.
963         if isinstance(self, EmptyQuerySet):
964             return self
965         for other in other_qs:
966             if isinstance(other, EmptyQuerySet):
967                 return other
968         return self._combinator_query('intersection', *other_qs)
969 
970     def difference(self, *other_qs):
971         # If the query is an EmptyQuerySet, return it.
972         if isinstance(self, EmptyQuerySet):
973             return self
974         return self._combinator_query('difference', *other_qs)
975 
976     def select_for_update(self, nowait=False, skip_locked=False, of=()):
977         """
978         Return a new QuerySet instance that will select objects with a
979         FOR UPDATE lock.
980         """
981         if nowait and skip_locked:
982             raise ValueError('The nowait option cannot be used with skip_locked.')
983         obj = self._chain()
984         obj._for_write = True
985         obj.query.select_for_update = True
986         obj.query.select_for_update_nowait = nowait
987         obj.query.select_for_update_skip_locked = skip_locked
988         obj.query.select_for_update_of = of
989         return obj
990 
991     def select_related(self, *fields):
992         """
993         Return a new QuerySet instance that will select related objects.
994 
995         If fields are specified, they must be ForeignKey fields and only those
996         related objects are included in the selection.
997 
998         If select_related(None) is called, clear the list.
999         """
1000         self._not_support_combined_queries('select_related')
1001         if self._fields is not None:
1002             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1003 
1004         obj = self._chain()
1005         if fields == (None,):
1006             obj.query.select_related = False
1007         elif fields:
1008             obj.query.add_select_related(fields)
1009         else:
1010             obj.query.select_related = True
1011         return obj
1012 
1013     def prefetch_related(self, *lookups):
1014         """
1015         Return a new QuerySet instance that will prefetch the specified
1016         Many-To-One and Many-To-Many related objects when the QuerySet is
1017         evaluated.
1018 
1019         When prefetch_related() is called more than once, append to the list of
1020         prefetch lookups. If prefetch_related(None) is called, clear the list.
1021         """
1022         self._not_support_combined_queries('prefetch_related')
1023         clone = self._chain()
1024         if lookups == (None,):
1025             clone._prefetch_related_lookups = ()
1026         else:
1027             for lookup in lookups:
1028                 if isinstance(lookup, Prefetch):
1029                     lookup = lookup.prefetch_to
1030                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1031                 if lookup in self.query._filtered_relations:
1032                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1033             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1034         return clone
1035 
1036     def annotate(self, *args, **kwargs):
1037         """
1038         Return a query set in which the returned objects have been annotated
1039         with extra data or aggregations.
1040         """
1041         self._not_support_combined_queries('annotate')
1042         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1043         annotations = {}
1044         for arg in args:
1045             # The default_alias property may raise a TypeError.
1046             try:
1047                 if arg.default_alias in kwargs:
1048                     raise ValueError("The named annotation '%s' conflicts with the "
1049                                      "default name for another annotation."
1050                                      % arg.default_alias)
1051             except TypeError:
1052                 raise TypeError("Complex annotations require an alias")
1053             annotations[arg.default_alias] = arg
1054         annotations.update(kwargs)
1055 
1056         clone = self._chain()
1057         names = self._fields
1058         if names is None:
1059             names = set(chain.from_iterable(
1060                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1061                 for field in self.model._meta.get_fields()
1062             ))
1063 
1064         for alias, annotation in annotations.items():
1065             if alias in names:
1066                 raise ValueError("The annotation '%s' conflicts with a field on "
1067                                  "the model." % alias)
1068             if isinstance(annotation, FilteredRelation):
1069                 clone.query.add_filtered_relation(annotation, alias)
1070             else:
1071                 clone.query.add_annotation(annotation, alias, is_summary=False)
1072 
1073         for alias, annotation in clone.query.annotations.items():
1074             if alias in annotations and annotation.contains_aggregate:
1075                 if clone._fields is None:
1076                     clone.query.group_by = True
1077                 else:
1078                     clone.query.set_group_by()
1079                 break
1080 
1081         return clone
1082 
1083     def order_by(self, *field_names):
1084         """Return a new QuerySet instance with the ordering changed."""
1085         assert not self.query.is_sliced, \
1086             "Cannot reorder a query once a slice has been taken."
1087         obj = self._chain()
1088         obj.query.clear_ordering(force_empty=False)
1089         obj.query.add_ordering(*field_names)
1090         return obj
1091 
1092     def distinct(self, *field_names):
1093         """
1094         Return a new QuerySet instance that will select only distinct results.
1095         """
1096         assert not self.query.is_sliced, \
1097             "Cannot create distinct fields once a slice has been taken."
1098         obj = self._chain()
1099         obj.query.add_distinct_fields(*field_names)
1100         return obj
1101 
1102     def extra(self, select=None, where=None, params=None, tables=None,
1103               order_by=None, select_params=None):
1104         """Add extra SQL fragments to the query."""
1105         self._not_support_combined_queries('extra')
1106         assert not self.query.is_sliced, \
1107             "Cannot change a query once a slice has been taken"
1108         clone = self._chain()
1109         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1110         return clone
1111 
1112     def reverse(self):
1113         """Reverse the ordering of the QuerySet."""
1114         if self.query.is_sliced:
1115             raise TypeError('Cannot reverse a query once a slice has been taken.')
1116         clone = self._chain()
1117         clone.query.standard_ordering = not clone.query.standard_ordering
1118         return clone
1119 
1120     def defer(self, *fields):
1121         """
1122         Defer the loading of data for certain fields until they are accessed.
1123         Add the set of deferred fields to any existing set of deferred fields.
1124         The only exception to this is if None is passed in as the only
1125         parameter, in which case removal all deferrals.
1126         """
1127         self._not_support_combined_queries('defer')
1128         if self._fields is not None:
1129             raise TypeError("Cannot call defer() after .values() or .values_list()")
1130         clone = self._chain()
1131         if fields == (None,):
1132             clone.query.clear_deferred_loading()
1133         else:
1134             clone.query.add_deferred_loading(fields)
1135         return clone
1136 
1137     def only(self, *fields):
1138         """
1139         Essentially, the opposite of defer(). Only the fields passed into this
1140         method and that are not already specified as deferred are loaded
1141         immediately when the queryset is evaluated.
1142         """
1143         self._not_support_combined_queries('only')
1144         if self._fields is not None:
1145             raise TypeError("Cannot call only() after .values() or .values_list()")
1146         if fields == (None,):
1147             # Can only pass None to defer(), not only(), as the rest option.
1148             # That won't stop people trying to do this, so let's be explicit.
1149             raise TypeError("Cannot pass None as an argument to only().")
1150         for field in fields:
1151             field = field.split(LOOKUP_SEP, 1)[0]
1152             if field in self.query._filtered_relations:
1153                 raise ValueError('only() is not supported with FilteredRelation.')
1154         clone = self._chain()
1155         clone.query.add_immediate_loading(fields)
1156         return clone
1157 
1158     def using(self, alias):
1159         """Select which database this QuerySet should execute against."""
1160         clone = self._chain()
1161         clone._db = alias
1162         return clone
1163 
1164     ###################################
1165     # PUBLIC INTROSPECTION ATTRIBUTES #
1166     ###################################
1167 
1168     @property
1169     def ordered(self):
1170         """
1171         Return True if the QuerySet is ordered -- i.e. has an order_by()
1172         clause or a default ordering on the model (or is empty).
1173         """
1174         if isinstance(self, EmptyQuerySet):
1175             return True
1176         if self.query.extra_order_by or self.query.order_by:
1177             return True
1178         elif self.query.default_ordering and self.query.get_meta().ordering:
1179             return True
1180         else:
1181             return False
1182 
1183     @property
1184     def db(self):
1185         """Return the database used if this query is executed now."""
1186         if self._for_write:
1187             return self._db or router.db_for_write(self.model, **self._hints)
1188         return self._db or router.db_for_read(self.model, **self._hints)
1189 
1190     ###################
1191     # PRIVATE METHODS #
1192     ###################
1193 
1194     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1195         """
1196         Insert a new record for the given model. This provides an interface to
1197         the InsertQuery class and is how Model.save() is implemented.
1198         """
1199         self._for_write = True
1200         if using is None:
1201             using = self.db
1202         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1203         query.insert_values(fields, objs, raw=raw)
1204         return query.get_compiler(using=using).execute_sql(returning_fields)
1205     _insert.alters_data = True
1206     _insert.queryset_only = False
1207 
1208     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1209         """
1210         Helper method for bulk_create() to insert objs one batch at a time.
1211         """
1212         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1213             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1214         ops = connections[self.db].ops
1215         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1216         inserted_rows = []
1217         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1218         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1219             if bulk_return and not ignore_conflicts:
1220                 inserted_columns = self._insert(
1221                     item, fields=fields, using=self.db,
1222                     returning_fields=self.model._meta.db_returning_fields,
1223                     ignore_conflicts=ignore_conflicts,
1224                 )
1225                 if isinstance(inserted_columns, list):
1226                     inserted_rows.extend(inserted_columns)
1227                 else:
1228                     inserted_rows.append(inserted_columns)
1229             else:
1230                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1231         return inserted_rows
1232 
1233     def _chain(self, **kwargs):
1234         """
1235         Return a copy of the current QuerySet that's ready for another
1236         operation.
1237         """
1238         obj = self._clone()
1239         if obj._sticky_filter:
1240             obj.query.filter_is_sticky = True
1241             obj._sticky_filter = False
1242         obj.__dict__.update(kwargs)
1243         return obj
1244 
1245     def _clone(self):
1246         """
1247         Return a copy of the current QuerySet. A lightweight alternative
1248         to deepcopy().
1249         """
1250         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1251         c._sticky_filter = self._sticky_filter
1252         c._for_write = self._for_write
1253         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1254         c._known_related_objects = self._known_related_objects
1255         c._iterable_class = self._iterable_class
1256         c._fields = self._fields
1257         return c
1258 
1259     def _fetch_all(self):
1260         if self._result_cache is None:
1261             self._result_cache = list(self._iterable_class(self))
1262         if self._prefetch_related_lookups and not self._prefetch_done:
1263             self._prefetch_related_objects()
1264 
1265     def _next_is_sticky(self):
1266         """
1267         Indicate that the next filter call and the one following that should
1268         be treated as a single filter. This is only important when it comes to
1269         determining when to reuse tables for many-to-many filters. Required so
1270         that we can filter naturally on the results of related managers.
1271 
1272         This doesn't return a clone of the current QuerySet (it returns
1273         "self"). The method is only used internally and should be immediately
1274         followed by a filter() that does create a clone.
1275         """
1276         self._sticky_filter = True
1277         return self
1278 
1279     def _merge_sanity_check(self, other):
1280         """Check that two QuerySet classes may be merged."""
1281         if self._fields is not None and (
1282                 set(self.query.values_select) != set(other.query.values_select) or
1283                 set(self.query.extra_select) != set(other.query.extra_select) or
1284                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1285             raise TypeError(
1286                 "Merging '%s' classes must involve the same values in each case."
1287                 % self.__class__.__name__
1288             )
1289 
1290     def _merge_known_related_objects(self, other):
1291         """
1292         Keep track of all known related objects from either QuerySet instance.
1293         """
1294         for field, objects in other._known_related_objects.items():
1295             self._known_related_objects.setdefault(field, {}).update(objects)
1296 
1297     def resolve_expression(self, *args, **kwargs):
1298         if self._fields and len(self._fields) > 1:
1299             # values() queryset can only be used as nested queries
1300             # if they are set up to select only a single field.
1301             raise TypeError('Cannot use multi-field values as a filter value.')
1302         query = self.query.resolve_expression(*args, **kwargs)
1303         query._db = self._db
1304         return query
1305     resolve_expression.queryset_only = True
1306 
1307     def _add_hints(self, **hints):
1308         """
1309         Update hinting information for use by routers. Add new key/values or
1310         overwrite existing key/values.
1311         """
1312         self._hints.update(hints)
1313 
1314     def _has_filters(self):
1315         """
1316         Check if this QuerySet has any filtering going on. This isn't
1317         equivalent with checking if all objects are present in results, for
1318         example, qs[1:]._has_filters() -> False.
1319         """
1320         return self.query.has_filters()
1321 
1322     @staticmethod
1323     def _validate_values_are_expressions(values, method_name):
1324         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1325         if invalid_args:
1326             raise TypeError(
1327                 'QuerySet.%s() received non-expression(s): %s.' % (
1328                     method_name,
1329                     ', '.join(invalid_args),
1330                 )
1331             )
1332 
1333     def _not_support_combined_queries(self, operation_name):
1334         if self.query.combinator:
1335             raise NotSupportedError(
1336                 'Calling QuerySet.%s() after %s() is not supported.'
1337                 % (operation_name, self.query.combinator)
1338             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>_clone</method>
<code>
1245     def _clone(self):
1246         """
1247         Return a copy of the current QuerySet. A lightweight alternative
1248         to deepcopy().
1249         """
1250         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1251         c._sticky_filter = self._sticky_filter
1252         c._for_write = self._for_write
1253         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1254         c._known_related_objects = self._known_related_objects
1255         c._iterable_class = self._iterable_class
1256         c._fields = self._fields
1257         return c

</code><intended_behavior>The _clone method's intended behavior is to produce a fully independent copy of the QuerySet, which implies no shared state between the original and the cloned instance that could lead to the mutations observed. A thorough review is necessary to ensure that all attributes, especially those that can be modified like query (which houses select_related logic), are correctly deep copied or uniquely instantiated to preserve this independence. This might involve changing the way references to mutable objects (like dicts or custom objects) are copied, ensuring a true deep copy is made where necessary.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
188 class QuerySet:
189     """Represent a lazy database lookup for a set of objects."""
190 
191     def __init__(self, model=None, query=None, using=None, hints=None):
192         self.model = model
193         self._db = using
194         self._hints = hints or {}
195         self.query = query or sql.Query(self.model)
196         self._result_cache = None
197         self._sticky_filter = False
198         self._for_write = False
199         self._prefetch_related_lookups = ()
200         self._prefetch_done = False
201         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
202         self._iterable_class = ModelIterable
203         self._fields = None
204 
205     def as_manager(cls):
206         # Address the circular dependency between `Queryset` and `Manager`.
207         from django.db.models.manager import Manager
208         manager = Manager.from_queryset(cls)()
209         manager._built_with_as_manager = True
210         return manager
211     as_manager.queryset_only = True
212     as_manager = classmethod(as_manager)
213 
214     ########################
215     # PYTHON MAGIC METHODS #
216     ########################
217 
218     def __deepcopy__(self, memo):
219         """Don't populate the QuerySet's cache."""
220         obj = self.__class__()
221         for k, v in self.__dict__.items():
222             if k == '_result_cache':
223                 obj.__dict__[k] = None
224             else:
225                 obj.__dict__[k] = copy.deepcopy(v, memo)
226         return obj
227 
228     def __getstate__(self):
229         # Force the cache to be fully populated.
230         self._fetch_all()
231         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
232 
233     def __setstate__(self, state):
234         msg = None
235         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
236         if pickled_version:
237             current_version = get_version()
238             if current_version != pickled_version:
239                 msg = (
240                     "Pickled queryset instance's Django version %s does not "
241                     "match the current version %s." % (pickled_version, current_version)
242                 )
243         else:
244             msg = "Pickled queryset instance's Django version is not specified."
245 
246         if msg:
247             warnings.warn(msg, RuntimeWarning, stacklevel=2)
248 
249         self.__dict__.update(state)
250 
251     def __repr__(self):
252         data = list(self[:REPR_OUTPUT_SIZE + 1])
253         if len(data) > REPR_OUTPUT_SIZE:
254             data[-1] = "...(remaining elements truncated)..."
255         return '<%s %r>' % (self.__class__.__name__, data)
256 
257     def __len__(self):
258         self._fetch_all()
259         return len(self._result_cache)
260 
261     def __iter__(self):
262         """
263         The queryset iterator protocol uses three nested iterators in the
264         default case:
265             1. sql.compiler.execute_sql()
266                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
267                  using cursor.fetchmany(). This part is responsible for
268                  doing some column masking, and returning the rows in chunks.
269             2. sql.compiler.results_iter()
270                - Returns one row at time. At this point the rows are still just
271                  tuples. In some cases the return values are converted to
272                  Python values at this location.
273             3. self.iterator()
274                - Responsible for turning the rows into model objects.
275         """
276         self._fetch_all()
277         return iter(self._result_cache)
278 
279     def __bool__(self):
280         self._fetch_all()
281         return bool(self._result_cache)
282 
283     def __getitem__(self, k):
284         """Retrieve an item or slice from the set of results."""
285         if not isinstance(k, (int, slice)):
286             raise TypeError(
287                 'QuerySet indices must be integers or slices, not %s.'
288                 % type(k).__name__
289             )
290         assert ((not isinstance(k, slice) and (k >= 0)) or
291                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
292                  (k.stop is None or k.stop >= 0))), \
293             "Negative indexing is not supported."
294 
295         if self._result_cache is not None:
296             return self._result_cache[k]
297 
298         if isinstance(k, slice):
299             qs = self._chain()
300             if k.start is not None:
301                 start = int(k.start)
302             else:
303                 start = None
304             if k.stop is not None:
305                 stop = int(k.stop)
306             else:
307                 stop = None
308             qs.query.set_limits(start, stop)
309             return list(qs)[::k.step] if k.step else qs
310 
311         qs = self._chain()
312         qs.query.set_limits(k, k + 1)
313         qs._fetch_all()
314         return qs._result_cache[0]
315 
316     def __and__(self, other):
317         self._merge_sanity_check(other)
318         if isinstance(other, EmptyQuerySet):
319             return other
320         if isinstance(self, EmptyQuerySet):
321             return self
322         combined = self._chain()
323         combined._merge_known_related_objects(other)
324         combined.query.combine(other.query, sql.AND)
325         return combined
326 
327     def __or__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(self, EmptyQuerySet):
330             return other
331         if isinstance(other, EmptyQuerySet):
332             return self
333         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
334         combined = query._chain()
335         combined._merge_known_related_objects(other)
336         if not other.query.can_filter():
337             other = other.model._base_manager.filter(pk__in=other.values('pk'))
338         combined.query.combine(other.query, sql.OR)
339         return combined
340 
341     ####################################
342     # METHODS THAT DO DATABASE QUERIES #
343     ####################################
344 
345     def _iterator(self, use_chunked_fetch, chunk_size):
346         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
347 
348     def iterator(self, chunk_size=2000):
349         """
350         An iterator over the results from applying this QuerySet to the
351         database.
352         """
353         if chunk_size <= 0:
354             raise ValueError('Chunk size must be strictly positive.')
355         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
356         return self._iterator(use_chunked_fetch, chunk_size)
357 
358     def aggregate(self, *args, **kwargs):
359         """
360         Return a dictionary containing the calculations (aggregation)
361         over the current queryset.
362 
363         If args is present the expression is passed as a kwarg using
364         the Aggregate object's default alias.
365         """
366         if self.query.distinct_fields:
367             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
368         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
369         for arg in args:
370             # The default_alias property raises TypeError if default_alias
371             # can't be set automatically or AttributeError if it isn't an
372             # attribute.
373             try:
374                 arg.default_alias
375             except (AttributeError, TypeError):
376                 raise TypeError("Complex aggregates require an alias")
377             kwargs[arg.default_alias] = arg
378 
379         query = self.query.chain()
380         for (alias, aggregate_expr) in kwargs.items():
381             query.add_annotation(aggregate_expr, alias, is_summary=True)
382             if not query.annotations[alias].contains_aggregate:
383                 raise TypeError("%s is not an aggregate expression" % alias)
384         return query.get_aggregation(self.db, kwargs)
385 
386     def count(self):
387         """
388         Perform a SELECT COUNT() and return the number of records as an
389         integer.
390 
391         If the QuerySet is already fully cached, return the length of the
392         cached results set to avoid multiple SELECT COUNT(*) calls.
393         """
394         if self._result_cache is not None:
395             return len(self._result_cache)
396 
397         return self.query.get_count(using=self.db)
398 
399     def get(self, *args, **kwargs):
400         """
401         Perform the query and return a single object matching the given
402         keyword arguments.
403         """
404         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
405         if self.query.can_filter() and not self.query.distinct_fields:
406             clone = clone.order_by()
407         limit = None
408         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
409             limit = MAX_GET_RESULTS
410             clone.query.set_limits(high=limit)
411         num = len(clone)
412         if num == 1:
413             return clone._result_cache[0]
414         if not num:
415             raise self.model.DoesNotExist(
416                 "%s matching query does not exist." %
417                 self.model._meta.object_name
418             )
419         raise self.model.MultipleObjectsReturned(
420             'get() returned more than one %s -- it returned %s!' % (
421                 self.model._meta.object_name,
422                 num if not limit or num < limit else 'more than %s' % (limit - 1),
423             )
424         )
425 
426     def create(self, **kwargs):
427         """
428         Create a new object with the given kwargs, saving it to the database
429         and returning the created object.
430         """
431         obj = self.model(**kwargs)
432         self._for_write = True
433         obj.save(force_insert=True, using=self.db)
434         return obj
435 
436     def _populate_pk_values(self, objs):
437         for obj in objs:
438             if obj.pk is None:
439                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
440 
441     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
442         """
443         Insert each of the instances into the database. Do *not* call
444         save() on each of the instances, do not send any pre/post_save
445         signals, and do not set the primary key attribute if it is an
446         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
447         Multi-table models are not supported.
448         """
449         # When you bulk insert you don't get the primary keys back (if it's an
450         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
451         # you can't insert into the child tables which references this. There
452         # are two workarounds:
453         # 1) This could be implemented if you didn't have an autoincrement pk
454         # 2) You could do it by doing O(n) normal inserts into the parent
455         #    tables to get the primary keys back and then doing a single bulk
456         #    insert into the childmost table.
457         # We currently set the primary keys on the objects when using
458         # PostgreSQL via the RETURNING ID clause. It should be possible for
459         # Oracle as well, but the semantics for extracting the primary keys is
460         # trickier so it's not done yet.
461         assert batch_size is None or batch_size > 0
462         # Check that the parents share the same concrete model with the our
463         # model to detect the inheritance pattern ConcreteGrandParent ->
464         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
465         # would not identify that case as involving multiple tables.
466         for parent in self.model._meta.get_parent_list():
467             if parent._meta.concrete_model is not self.model._meta.concrete_model:
468                 raise ValueError("Can't bulk create a multi-table inherited model")
469         if not objs:
470             return objs
471         self._for_write = True
472         connection = connections[self.db]
473         opts = self.model._meta
474         fields = opts.concrete_fields
475         objs = list(objs)
476         self._populate_pk_values(objs)
477         with transaction.atomic(using=self.db, savepoint=False):
478             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
479             if objs_with_pk:
480                 returned_columns = self._batched_insert(
481                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
482                 )
483                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
484                     for result, field in zip(results, opts.db_returning_fields):
485                         if field != opts.pk:
486                             setattr(obj_with_pk, field.attname, result)
487                 for obj_with_pk in objs_with_pk:
488                     obj_with_pk._state.adding = False
489                     obj_with_pk._state.db = self.db
490             if objs_without_pk:
491                 fields = [f for f in fields if not isinstance(f, AutoField)]
492                 returned_columns = self._batched_insert(
493                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
494                 )
495                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
496                     assert len(returned_columns) == len(objs_without_pk)
497                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         setattr(obj_without_pk, field.attname, result)
500                     obj_without_pk._state.adding = False
501                     obj_without_pk._state.db = self.db
502 
503         return objs
504 
505     def bulk_update(self, objs, fields, batch_size=None):
506         """
507         Update the given fields in each of the given objects in the database.
508         """
509         if batch_size is not None and batch_size < 0:
510             raise ValueError('Batch size must be a positive integer.')
511         if not fields:
512             raise ValueError('Field names must be given to bulk_update().')
513         objs = tuple(objs)
514         if any(obj.pk is None for obj in objs):
515             raise ValueError('All bulk_update() objects must have a primary key set.')
516         fields = [self.model._meta.get_field(name) for name in fields]
517         if any(not f.concrete or f.many_to_many for f in fields):
518             raise ValueError('bulk_update() can only be used with concrete fields.')
519         if any(f.primary_key for f in fields):
520             raise ValueError('bulk_update() cannot be used with primary key fields.')
521         if not objs:
522             return
523         # PK is used twice in the resulting update query, once in the filter
524         # and once in the WHEN. Each field will also have one CAST.
525         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
526         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
527         requires_casting = connections[self.db].features.requires_casted_case_in_updates
528         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
529         updates = []
530         for batch_objs in batches:
531             update_kwargs = {}
532             for field in fields:
533                 when_statements = []
534                 for obj in batch_objs:
535                     attr = getattr(obj, field.attname)
536                     if not isinstance(attr, Expression):
537                         attr = Value(attr, output_field=field)
538                     when_statements.append(When(pk=obj.pk, then=attr))
539                 case_statement = Case(*when_statements, output_field=field)
540                 if requires_casting:
541                     case_statement = Cast(case_statement, output_field=field)
542                 update_kwargs[field.attname] = case_statement
543             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
544         with transaction.atomic(using=self.db, savepoint=False):
545             for pks, update_kwargs in updates:
546                 self.filter(pk__in=pks).update(**update_kwargs)
547     bulk_update.alters_data = True
548 
549     def get_or_create(self, defaults=None, **kwargs):
550         """
551         Look up an object with the given kwargs, creating one if necessary.
552         Return a tuple of (object, created), where created is a boolean
553         specifying whether an object was created.
554         """
555         # The get() needs to be targeted at the write database in order
556         # to avoid potential transaction consistency problems.
557         self._for_write = True
558         try:
559             return self.get(**kwargs), False
560         except self.model.DoesNotExist:
561             params = self._extract_model_params(defaults, **kwargs)
562             return self._create_object_from_params(kwargs, params)
563 
564     def update_or_create(self, defaults=None, **kwargs):
565         """
566         Look up an object with the given kwargs, updating one with defaults
567         if it exists, otherwise create a new one.
568         Return a tuple (object, created), where created is a boolean
569         specifying whether an object was created.
570         """
571         defaults = defaults or {}
572         self._for_write = True
573         with transaction.atomic(using=self.db):
574             try:
575                 obj = self.select_for_update().get(**kwargs)
576             except self.model.DoesNotExist:
577                 params = self._extract_model_params(defaults, **kwargs)
578                 # Lock the row so that a concurrent update is blocked until
579                 # after update_or_create() has performed its save.
580                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
581                 if created:
582                     return obj, created
583             for k, v in defaults.items():
584                 setattr(obj, k, v() if callable(v) else v)
585             obj.save(using=self.db)
586         return obj, False
587 
588     def _create_object_from_params(self, lookup, params, lock=False):
589         """
590         Try to create an object using passed params. Used by get_or_create()
591         and update_or_create().
592         """
593         try:
594             with transaction.atomic(using=self.db):
595                 params = {k: v() if callable(v) else v for k, v in params.items()}
596                 obj = self.create(**params)
597             return obj, True
598         except IntegrityError as e:
599             try:
600                 qs = self.select_for_update() if lock else self
601                 return qs.get(**lookup), False
602             except self.model.DoesNotExist:
603                 pass
604             raise e
605 
606     def _extract_model_params(self, defaults, **kwargs):
607         """
608         Prepare `params` for creating a model instance based on the given
609         kwargs; for use by get_or_create() and update_or_create().
610         """
611         defaults = defaults or {}
612         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
613         params.update(defaults)
614         property_names = self.model._meta._property_names
615         invalid_params = []
616         for param in params:
617             try:
618                 self.model._meta.get_field(param)
619             except exceptions.FieldDoesNotExist:
620                 # It's okay to use a model's property if it has a setter.
621                 if not (param in property_names and getattr(self.model, param).fset):
622                     invalid_params.append(param)
623         if invalid_params:
624             raise exceptions.FieldError(
625                 "Invalid field name(s) for model %s: '%s'." % (
626                     self.model._meta.object_name,
627                     "', '".join(sorted(invalid_params)),
628                 ))
629         return params
630 
631     def _earliest(self, *fields):
632         """
633         Return the earliest object according to fields (if given) or by the
634         model's Meta.get_latest_by.
635         """
636         if fields:
637             order_by = fields
638         else:
639             order_by = getattr(self.model._meta, 'get_latest_by')
640             if order_by and not isinstance(order_by, (tuple, list)):
641                 order_by = (order_by,)
642         if order_by is None:
643             raise ValueError(
644                 "earliest() and latest() require either fields as positional "
645                 "arguments or 'get_latest_by' in the model's Meta."
646             )
647 
648         assert not self.query.is_sliced, \
649             "Cannot change a query once a slice has been taken."
650         obj = self._chain()
651         obj.query.set_limits(high=1)
652         obj.query.clear_ordering(force_empty=True)
653         obj.query.add_ordering(*order_by)
654         return obj.get()
655 
656     def earliest(self, *fields):
657         return self._earliest(*fields)
658 
659     def latest(self, *fields):
660         return self.reverse()._earliest(*fields)
661 
662     def first(self):
663         """Return the first object of a query or None if no match is found."""
664         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
665             return obj
666 
667     def last(self):
668         """Return the last object of a query or None if no match is found."""
669         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
670             return obj
671 
672     def in_bulk(self, id_list=None, *, field_name='pk'):
673         """
674         Return a dictionary mapping each of the given IDs to the object with
675         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
676         """
677         assert not self.query.is_sliced, \
678             "Cannot use 'limit' or 'offset' with in_bulk"
679         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
680             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
681         if id_list is not None:
682             if not id_list:
683                 return {}
684             filter_key = '{}__in'.format(field_name)
685             batch_size = connections[self.db].features.max_query_params
686             id_list = tuple(id_list)
687             # If the database has a limit on the number of query parameters
688             # (e.g. SQLite), retrieve objects in batches if necessary.
689             if batch_size and batch_size < len(id_list):
690                 qs = ()
691                 for offset in range(0, len(id_list), batch_size):
692                     batch = id_list[offset:offset + batch_size]
693                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
694             else:
695                 qs = self.filter(**{filter_key: id_list}).order_by()
696         else:
697             qs = self._chain()
698         return {getattr(obj, field_name): obj for obj in qs}
699 
700     def delete(self):
701         """Delete the records in the current QuerySet."""
702         assert not self.query.is_sliced, \
703             "Cannot use 'limit' or 'offset' with delete."
704 
705         if self._fields is not None:
706             raise TypeError("Cannot call delete() after .values() or .values_list()")
707 
708         del_query = self._chain()
709 
710         # The delete is actually 2 queries - one to find related objects,
711         # and one to delete. Make sure that the discovery of related
712         # objects is performed on the same database as the deletion.
713         del_query._for_write = True
714 
715         # Disable non-supported fields.
716         del_query.query.select_for_update = False
717         del_query.query.select_related = False
718         del_query.query.clear_ordering(force_empty=True)
719 
720         collector = Collector(using=del_query.db)
721         collector.collect(del_query)
722         deleted, _rows_count = collector.delete()
723 
724         # Clear the result cache, in case this QuerySet gets reused.
725         self._result_cache = None
726         return deleted, _rows_count
727 
728     delete.alters_data = True
729     delete.queryset_only = True
730 
731     def _raw_delete(self, using):
732         """
733         Delete objects found from the given queryset in single direct SQL
734         query. No signals are sent and there is no protection for cascades.
735         """
736         return sql.DeleteQuery(self.model).delete_qs(self, using)
737     _raw_delete.alters_data = True
738 
739     def update(self, **kwargs):
740         """
741         Update all elements in the current QuerySet, setting all the given
742         fields to the appropriate values.
743         """
744         assert not self.query.is_sliced, \
745             "Cannot update a query once a slice has been taken."
746         self._for_write = True
747         query = self.query.chain(sql.UpdateQuery)
748         query.add_update_values(kwargs)
749         # Clear any annotations so that they won't be present in subqueries.
750         query.annotations = {}
751         with transaction.mark_for_rollback_on_error(using=self.db):
752             rows = query.get_compiler(self.db).execute_sql(CURSOR)
753         self._result_cache = None
754         return rows
755     update.alters_data = True
756 
757     def _update(self, values):
758         """
759         A version of update() that accepts field objects instead of field names.
760         Used primarily for model saving and not intended for use by general
761         code (it requires too much poking around at model internals to be
762         useful at that level).
763         """
764         assert not self.query.is_sliced, \
765             "Cannot update a query once a slice has been taken."
766         query = self.query.chain(sql.UpdateQuery)
767         query.add_update_fields(values)
768         # Clear any annotations so that they won't be present in subqueries.
769         query.annotations = {}
770         self._result_cache = None
771         return query.get_compiler(self.db).execute_sql(CURSOR)
772     _update.alters_data = True
773     _update.queryset_only = False
774 
775     def exists(self):
776         if self._result_cache is None:
777             return self.query.has_results(using=self.db)
778         return bool(self._result_cache)
779 
780     def _prefetch_related_objects(self):
781         # This method can only be called once the result cache has been filled.
782         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
783         self._prefetch_done = True
784 
785     def explain(self, *, format=None, **options):
786         return self.query.explain(using=self.db, format=format, **options)
787 
788     ##################################################
789     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
790     ##################################################
791 
792     def raw(self, raw_query, params=None, translations=None, using=None):
793         if using is None:
794             using = self.db
795         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
796         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
797         return qs
798 
799     def _values(self, *fields, **expressions):
800         clone = self._chain()
801         if expressions:
802             clone = clone.annotate(**expressions)
803         clone._fields = fields
804         clone.query.set_values(fields)
805         return clone
806 
807     def values(self, *fields, **expressions):
808         fields += tuple(expressions)
809         clone = self._values(*fields, **expressions)
810         clone._iterable_class = ValuesIterable
811         return clone
812 
813     def values_list(self, *fields, flat=False, named=False):
814         if flat and named:
815             raise TypeError("'flat' and 'named' can't be used together.")
816         if flat and len(fields) > 1:
817             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
818 
819         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
820         _fields = []
821         expressions = {}
822         counter = 1
823         for field in fields:
824             if hasattr(field, 'resolve_expression'):
825                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
826                 while True:
827                     field_id = field_id_prefix + str(counter)
828                     counter += 1
829                     if field_id not in field_names:
830                         break
831                 expressions[field_id] = field
832                 _fields.append(field_id)
833             else:
834                 _fields.append(field)
835 
836         clone = self._values(*_fields, **expressions)
837         clone._iterable_class = (
838             NamedValuesListIterable if named
839             else FlatValuesListIterable if flat
840             else ValuesListIterable
841         )
842         return clone
843 
844     def dates(self, field_name, kind, order='ASC'):
845         """
846         Return a list of date objects representing all available dates for
847         the given field_name, scoped to 'kind'.
848         """
849         assert kind in ('year', 'month', 'week', 'day'), \
850             "'kind' must be one of 'year', 'month', 'week', or 'day'."
851         assert order in ('ASC', 'DESC'), \
852             "'order' must be either 'ASC' or 'DESC'."
853         return self.annotate(
854             datefield=Trunc(field_name, kind, output_field=DateField()),
855             plain_field=F(field_name)
856         ).values_list(
857             'datefield', flat=True
858         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
859 
860     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
861         """
862         Return a list of datetime objects representing all available
863         datetimes for the given field_name, scoped to 'kind'.
864         """
865         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
866             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
867         assert order in ('ASC', 'DESC'), \
868             "'order' must be either 'ASC' or 'DESC'."
869         if settings.USE_TZ:
870             if tzinfo is None:
871                 tzinfo = timezone.get_current_timezone()
872         else:
873             tzinfo = None
874         return self.annotate(
875             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
876             plain_field=F(field_name)
877         ).values_list(
878             'datetimefield', flat=True
879         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
880 
881     def none(self):
882         """Return an empty QuerySet."""
883         clone = self._chain()
884         clone.query.set_empty()
885         return clone
886 
887     ##################################################################
888     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
889     ##################################################################
890 
891     def all(self):
892         """
893         Return a new QuerySet that is a copy of the current one. This allows a
894         QuerySet to proxy for a model manager in some cases.
895         """
896         return self._chain()
897 
898     def filter(self, *args, **kwargs):
899         """
900         Return a new QuerySet instance with the args ANDed to the existing
901         set.
902         """
903         self._not_support_combined_queries('filter')
904         return self._filter_or_exclude(False, *args, **kwargs)
905 
906     def exclude(self, *args, **kwargs):
907         """
908         Return a new QuerySet instance with NOT (args) ANDed to the existing
909         set.
910         """
911         self._not_support_combined_queries('exclude')
912         return self._filter_or_exclude(True, *args, **kwargs)
913 
914     def _filter_or_exclude(self, negate, *args, **kwargs):
915         if args or kwargs:
916             assert not self.query.is_sliced, \
917                 "Cannot filter a query once a slice has been taken."
918 
919         clone = self._chain()
920         if negate:
921             clone.query.add_q(~Q(*args, **kwargs))
922         else:
923             clone.query.add_q(Q(*args, **kwargs))
924         return clone
925 
926     def complex_filter(self, filter_obj):
927         """
928         Return a new QuerySet instance with filter_obj added to the filters.
929 
930         filter_obj can be a Q object or a dictionary of keyword lookup
931         arguments.
932 
933         This exists to support framework features such as 'limit_choices_to',
934         and usually it will be more natural to use other methods.
935         """
936         if isinstance(filter_obj, Q):
937             clone = self._chain()
938             clone.query.add_q(filter_obj)
939             return clone
940         else:
941             return self._filter_or_exclude(None, **filter_obj)
942 
943     def _combinator_query(self, combinator, *other_qs, all=False):
944         # Clone the query to inherit the select list and everything
945         clone = self._chain()
946         # Clear limits and ordering so they can be reapplied
947         clone.query.clear_ordering(True)
948         clone.query.clear_limits()
949         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
950         clone.query.combinator = combinator
951         clone.query.combinator_all = all
952         return clone
953 
954     def union(self, *other_qs, all=False):
955         # If the query is an EmptyQuerySet, combine all nonempty querysets.
956         if isinstance(self, EmptyQuerySet):
957             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
958             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
959         return self._combinator_query('union', *other_qs, all=all)
960 
961     def intersection(self, *other_qs):
962         # If any query is an EmptyQuerySet, return it.
963         if isinstance(self, EmptyQuerySet):
964             return self
965         for other in other_qs:
966             if isinstance(other, EmptyQuerySet):
967                 return other
968         return self._combinator_query('intersection', *other_qs)
969 
970     def difference(self, *other_qs):
971         # If the query is an EmptyQuerySet, return it.
972         if isinstance(self, EmptyQuerySet):
973             return self
974         return self._combinator_query('difference', *other_qs)
975 
976     def select_for_update(self, nowait=False, skip_locked=False, of=()):
977         """
978         Return a new QuerySet instance that will select objects with a
979         FOR UPDATE lock.
980         """
981         if nowait and skip_locked:
982             raise ValueError('The nowait option cannot be used with skip_locked.')
983         obj = self._chain()
984         obj._for_write = True
985         obj.query.select_for_update = True
986         obj.query.select_for_update_nowait = nowait
987         obj.query.select_for_update_skip_locked = skip_locked
988         obj.query.select_for_update_of = of
989         return obj
990 
991     def select_related(self, *fields):
992         """
993         Return a new QuerySet instance that will select related objects.
994 
995         If fields are specified, they must be ForeignKey fields and only those
996         related objects are included in the selection.
997 
998         If select_related(None) is called, clear the list.
999         """
1000         self._not_support_combined_queries('select_related')
1001         if self._fields is not None:
1002             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1003 
1004         obj = self._chain()
1005         if fields == (None,):
1006             obj.query.select_related = False
1007         elif fields:
1008             obj.query.add_select_related(fields)
1009         else:
1010             obj.query.select_related = True
1011         return obj
1012 
1013     def prefetch_related(self, *lookups):
1014         """
1015         Return a new QuerySet instance that will prefetch the specified
1016         Many-To-One and Many-To-Many related objects when the QuerySet is
1017         evaluated.
1018 
1019         When prefetch_related() is called more than once, append to the list of
1020         prefetch lookups. If prefetch_related(None) is called, clear the list.
1021         """
1022         self._not_support_combined_queries('prefetch_related')
1023         clone = self._chain()
1024         if lookups == (None,):
1025             clone._prefetch_related_lookups = ()
1026         else:
1027             for lookup in lookups:
1028                 if isinstance(lookup, Prefetch):
1029                     lookup = lookup.prefetch_to
1030                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1031                 if lookup in self.query._filtered_relations:
1032                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1033             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1034         return clone
1035 
1036     def annotate(self, *args, **kwargs):
1037         """
1038         Return a query set in which the returned objects have been annotated
1039         with extra data or aggregations.
1040         """
1041         self._not_support_combined_queries('annotate')
1042         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1043         annotations = {}
1044         for arg in args:
1045             # The default_alias property may raise a TypeError.
1046             try:
1047                 if arg.default_alias in kwargs:
1048                     raise ValueError("The named annotation '%s' conflicts with the "
1049                                      "default name for another annotation."
1050                                      % arg.default_alias)
1051             except TypeError:
1052                 raise TypeError("Complex annotations require an alias")
1053             annotations[arg.default_alias] = arg
1054         annotations.update(kwargs)
1055 
1056         clone = self._chain()
1057         names = self._fields
1058         if names is None:
1059             names = set(chain.from_iterable(
1060                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1061                 for field in self.model._meta.get_fields()
1062             ))
1063 
1064         for alias, annotation in annotations.items():
1065             if alias in names:
1066                 raise ValueError("The annotation '%s' conflicts with a field on "
1067                                  "the model." % alias)
1068             if isinstance(annotation, FilteredRelation):
1069                 clone.query.add_filtered_relation(annotation, alias)
1070             else:
1071                 clone.query.add_annotation(annotation, alias, is_summary=False)
1072 
1073         for alias, annotation in clone.query.annotations.items():
1074             if alias in annotations and annotation.contains_aggregate:
1075                 if clone._fields is None:
1076                     clone.query.group_by = True
1077                 else:
1078                     clone.query.set_group_by()
1079                 break
1080 
1081         return clone
1082 
1083     def order_by(self, *field_names):
1084         """Return a new QuerySet instance with the ordering changed."""
1085         assert not self.query.is_sliced, \
1086             "Cannot reorder a query once a slice has been taken."
1087         obj = self._chain()
1088         obj.query.clear_ordering(force_empty=False)
1089         obj.query.add_ordering(*field_names)
1090         return obj
1091 
1092     def distinct(self, *field_names):
1093         """
1094         Return a new QuerySet instance that will select only distinct results.
1095         """
1096         assert not self.query.is_sliced, \
1097             "Cannot create distinct fields once a slice has been taken."
1098         obj = self._chain()
1099         obj.query.add_distinct_fields(*field_names)
1100         return obj
1101 
1102     def extra(self, select=None, where=None, params=None, tables=None,
1103               order_by=None, select_params=None):
1104         """Add extra SQL fragments to the query."""
1105         self._not_support_combined_queries('extra')
1106         assert not self.query.is_sliced, \
1107             "Cannot change a query once a slice has been taken"
1108         clone = self._chain()
1109         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1110         return clone
1111 
1112     def reverse(self):
1113         """Reverse the ordering of the QuerySet."""
1114         if self.query.is_sliced:
1115             raise TypeError('Cannot reverse a query once a slice has been taken.')
1116         clone = self._chain()
1117         clone.query.standard_ordering = not clone.query.standard_ordering
1118         return clone
1119 
1120     def defer(self, *fields):
1121         """
1122         Defer the loading of data for certain fields until they are accessed.
1123         Add the set of deferred fields to any existing set of deferred fields.
1124         The only exception to this is if None is passed in as the only
1125         parameter, in which case removal all deferrals.
1126         """
1127         self._not_support_combined_queries('defer')
1128         if self._fields is not None:
1129             raise TypeError("Cannot call defer() after .values() or .values_list()")
1130         clone = self._chain()
1131         if fields == (None,):
1132             clone.query.clear_deferred_loading()
1133         else:
1134             clone.query.add_deferred_loading(fields)
1135         return clone
1136 
1137     def only(self, *fields):
1138         """
1139         Essentially, the opposite of defer(). Only the fields passed into this
1140         method and that are not already specified as deferred are loaded
1141         immediately when the queryset is evaluated.
1142         """
1143         self._not_support_combined_queries('only')
1144         if self._fields is not None:
1145             raise TypeError("Cannot call only() after .values() or .values_list()")
1146         if fields == (None,):
1147             # Can only pass None to defer(), not only(), as the rest option.
1148             # That won't stop people trying to do this, so let's be explicit.
1149             raise TypeError("Cannot pass None as an argument to only().")
1150         for field in fields:
1151             field = field.split(LOOKUP_SEP, 1)[0]
1152             if field in self.query._filtered_relations:
1153                 raise ValueError('only() is not supported with FilteredRelation.')
1154         clone = self._chain()
1155         clone.query.add_immediate_loading(fields)
1156         return clone
1157 
1158     def using(self, alias):
1159         """Select which database this QuerySet should execute against."""
1160         clone = self._chain()
1161         clone._db = alias
1162         return clone
1163 
1164     ###################################
1165     # PUBLIC INTROSPECTION ATTRIBUTES #
1166     ###################################
1167 
1168     @property
1169     def ordered(self):
1170         """
1171         Return True if the QuerySet is ordered -- i.e. has an order_by()
1172         clause or a default ordering on the model (or is empty).
1173         """
1174         if isinstance(self, EmptyQuerySet):
1175             return True
1176         if self.query.extra_order_by or self.query.order_by:
1177             return True
1178         elif self.query.default_ordering and self.query.get_meta().ordering:
1179             return True
1180         else:
1181             return False
1182 
1183     @property
1184     def db(self):
1185         """Return the database used if this query is executed now."""
1186         if self._for_write:
1187             return self._db or router.db_for_write(self.model, **self._hints)
1188         return self._db or router.db_for_read(self.model, **self._hints)
1189 
1190     ###################
1191     # PRIVATE METHODS #
1192     ###################
1193 
1194     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1195         """
1196         Insert a new record for the given model. This provides an interface to
1197         the InsertQuery class and is how Model.save() is implemented.
1198         """
1199         self._for_write = True
1200         if using is None:
1201             using = self.db
1202         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1203         query.insert_values(fields, objs, raw=raw)
1204         return query.get_compiler(using=using).execute_sql(returning_fields)
1205     _insert.alters_data = True
1206     _insert.queryset_only = False
1207 
1208     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1209         """
1210         Helper method for bulk_create() to insert objs one batch at a time.
1211         """
1212         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1213             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1214         ops = connections[self.db].ops
1215         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1216         inserted_rows = []
1217         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1218         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1219             if bulk_return and not ignore_conflicts:
1220                 inserted_columns = self._insert(
1221                     item, fields=fields, using=self.db,
1222                     returning_fields=self.model._meta.db_returning_fields,
1223                     ignore_conflicts=ignore_conflicts,
1224                 )
1225                 if isinstance(inserted_columns, list):
1226                     inserted_rows.extend(inserted_columns)
1227                 else:
1228                     inserted_rows.append(inserted_columns)
1229             else:
1230                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1231         return inserted_rows
1232 
1233     def _chain(self, **kwargs):
1234         """
1235         Return a copy of the current QuerySet that's ready for another
1236         operation.
1237         """
1238         obj = self._clone()
1239         if obj._sticky_filter:
1240             obj.query.filter_is_sticky = True
1241             obj._sticky_filter = False
1242         obj.__dict__.update(kwargs)
1243         return obj
1244 
1245     def _clone(self):
1246         """
1247         Return a copy of the current QuerySet. A lightweight alternative
1248         to deepcopy().
1249         """
1250         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1251         c._sticky_filter = self._sticky_filter
1252         c._for_write = self._for_write
1253         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1254         c._known_related_objects = self._known_related_objects
1255         c._iterable_class = self._iterable_class
1256         c._fields = self._fields
1257         return c
1258 
1259     def _fetch_all(self):
1260         if self._result_cache is None:
1261             self._result_cache = list(self._iterable_class(self))
1262         if self._prefetch_related_lookups and not self._prefetch_done:
1263             self._prefetch_related_objects()
1264 
1265     def _next_is_sticky(self):
1266         """
1267         Indicate that the next filter call and the one following that should
1268         be treated as a single filter. This is only important when it comes to
1269         determining when to reuse tables for many-to-many filters. Required so
1270         that we can filter naturally on the results of related managers.
1271 
1272         This doesn't return a clone of the current QuerySet (it returns
1273         "self"). The method is only used internally and should be immediately
1274         followed by a filter() that does create a clone.
1275         """
1276         self._sticky_filter = True
1277         return self
1278 
1279     def _merge_sanity_check(self, other):
1280         """Check that two QuerySet classes may be merged."""
1281         if self._fields is not None and (
1282                 set(self.query.values_select) != set(other.query.values_select) or
1283                 set(self.query.extra_select) != set(other.query.extra_select) or
1284                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1285             raise TypeError(
1286                 "Merging '%s' classes must involve the same values in each case."
1287                 % self.__class__.__name__
1288             )
1289 
1290     def _merge_known_related_objects(self, other):
1291         """
1292         Keep track of all known related objects from either QuerySet instance.
1293         """
1294         for field, objects in other._known_related_objects.items():
1295             self._known_related_objects.setdefault(field, {}).update(objects)
1296 
1297     def resolve_expression(self, *args, **kwargs):
1298         if self._fields and len(self._fields) > 1:
1299             # values() queryset can only be used as nested queries
1300             # if they are set up to select only a single field.
1301             raise TypeError('Cannot use multi-field values as a filter value.')
1302         query = self.query.resolve_expression(*args, **kwargs)
1303         query._db = self._db
1304         return query
1305     resolve_expression.queryset_only = True
1306 
1307     def _add_hints(self, **hints):
1308         """
1309         Update hinting information for use by routers. Add new key/values or
1310         overwrite existing key/values.
1311         """
1312         self._hints.update(hints)
1313 
1314     def _has_filters(self):
1315         """
1316         Check if this QuerySet has any filtering going on. This isn't
1317         equivalent with checking if all objects are present in results, for
1318         example, qs[1:]._has_filters() -> False.
1319         """
1320         return self.query.has_filters()
1321 
1322     @staticmethod
1323     def _validate_values_are_expressions(values, method_name):
1324         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1325         if invalid_args:
1326             raise TypeError(
1327                 'QuerySet.%s() received non-expression(s): %s.' % (
1328                     method_name,
1329                     ', '.join(invalid_args),
1330                 )
1331             )
1332 
1333     def _not_support_combined_queries(self, operation_name):
1334         if self.query.combinator:
1335             raise NotSupportedError(
1336                 'Calling QuerySet.%s() after %s() is not supported.'
1337                 % (operation_name, self.query.combinator)
1338             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 04:13:24.423 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>select_related</method>
<code>
991     def select_related(self, *fields):
992         """
993         Return a new QuerySet instance that will select related objects.
994 
995         If fields are specified, they must be ForeignKey fields and only those
996         related objects are included in the selection.
997 
998         If select_related(None) is called, clear the list.
999         """
1000         self._not_support_combined_queries('select_related')
1001         if self._fields is not None:
1002             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1003 
1004         obj = self._chain()
1005         if fields == (None,):
1006             obj.query.select_related = False
1007         elif fields:
1008             obj.query.add_select_related(fields)
1009         else:
1010             obj.query.select_related = True
1011         return obj

</code><intended_behavior>The select_related method should continue to allow for the addition of related fields to be fetched in a single database query when the method is called on a QuerySet. However, it must ensure that any modifications made after select_related is called on a cloned QuerySet instance do not impact the original QuerySet from which the clone was derived. This could potentially require examining the _chain method or the way query objects are handled to ensure deep copying of relevant parts to prevent shared state between clones and originals.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
188 class QuerySet:
189     """Represent a lazy database lookup for a set of objects."""
190 
191     def __init__(self, model=None, query=None, using=None, hints=None):
192         self.model = model
193         self._db = using
194         self._hints = hints or {}
195         self.query = query or sql.Query(self.model)
196         self._result_cache = None
197         self._sticky_filter = False
198         self._for_write = False
199         self._prefetch_related_lookups = ()
200         self._prefetch_done = False
201         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
202         self._iterable_class = ModelIterable
203         self._fields = None
204 
205     def as_manager(cls):
206         # Address the circular dependency between `Queryset` and `Manager`.
207         from django.db.models.manager import Manager
208         manager = Manager.from_queryset(cls)()
209         manager._built_with_as_manager = True
210         return manager
211     as_manager.queryset_only = True
212     as_manager = classmethod(as_manager)
213 
214     ########################
215     # PYTHON MAGIC METHODS #
216     ########################
217 
218     def __deepcopy__(self, memo):
219         """Don't populate the QuerySet's cache."""
220         obj = self.__class__()
221         for k, v in self.__dict__.items():
222             if k == '_result_cache':
223                 obj.__dict__[k] = None
224             else:
225                 obj.__dict__[k] = copy.deepcopy(v, memo)
226         return obj
227 
228     def __getstate__(self):
229         # Force the cache to be fully populated.
230         self._fetch_all()
231         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
232 
233     def __setstate__(self, state):
234         msg = None
235         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
236         if pickled_version:
237             current_version = get_version()
238             if current_version != pickled_version:
239                 msg = (
240                     "Pickled queryset instance's Django version %s does not "
241                     "match the current version %s." % (pickled_version, current_version)
242                 )
243         else:
244             msg = "Pickled queryset instance's Django version is not specified."
245 
246         if msg:
247             warnings.warn(msg, RuntimeWarning, stacklevel=2)
248 
249         self.__dict__.update(state)
250 
251     def __repr__(self):
252         data = list(self[:REPR_OUTPUT_SIZE + 1])
253         if len(data) > REPR_OUTPUT_SIZE:
254             data[-1] = "...(remaining elements truncated)..."
255         return '<%s %r>' % (self.__class__.__name__, data)
256 
257     def __len__(self):
258         self._fetch_all()
259         return len(self._result_cache)
260 
261     def __iter__(self):
262         """
263         The queryset iterator protocol uses three nested iterators in the
264         default case:
265             1. sql.compiler.execute_sql()
266                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
267                  using cursor.fetchmany(). This part is responsible for
268                  doing some column masking, and returning the rows in chunks.
269             2. sql.compiler.results_iter()
270                - Returns one row at time. At this point the rows are still just
271                  tuples. In some cases the return values are converted to
272                  Python values at this location.
273             3. self.iterator()
274                - Responsible for turning the rows into model objects.
275         """
276         self._fetch_all()
277         return iter(self._result_cache)
278 
279     def __bool__(self):
280         self._fetch_all()
281         return bool(self._result_cache)
282 
283     def __getitem__(self, k):
284         """Retrieve an item or slice from the set of results."""
285         if not isinstance(k, (int, slice)):
286             raise TypeError(
287                 'QuerySet indices must be integers or slices, not %s.'
288                 % type(k).__name__
289             )
290         assert ((not isinstance(k, slice) and (k >= 0)) or
291                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
292                  (k.stop is None or k.stop >= 0))), \
293             "Negative indexing is not supported."
294 
295         if self._result_cache is not None:
296             return self._result_cache[k]
297 
298         if isinstance(k, slice):
299             qs = self._chain()
300             if k.start is not None:
301                 start = int(k.start)
302             else:
303                 start = None
304             if k.stop is not None:
305                 stop = int(k.stop)
306             else:
307                 stop = None
308             qs.query.set_limits(start, stop)
309             return list(qs)[::k.step] if k.step else qs
310 
311         qs = self._chain()
312         qs.query.set_limits(k, k + 1)
313         qs._fetch_all()
314         return qs._result_cache[0]
315 
316     def __and__(self, other):
317         self._merge_sanity_check(other)
318         if isinstance(other, EmptyQuerySet):
319             return other
320         if isinstance(self, EmptyQuerySet):
321             return self
322         combined = self._chain()
323         combined._merge_known_related_objects(other)
324         combined.query.combine(other.query, sql.AND)
325         return combined
326 
327     def __or__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(self, EmptyQuerySet):
330             return other
331         if isinstance(other, EmptyQuerySet):
332             return self
333         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
334         combined = query._chain()
335         combined._merge_known_related_objects(other)
336         if not other.query.can_filter():
337             other = other.model._base_manager.filter(pk__in=other.values('pk'))
338         combined.query.combine(other.query, sql.OR)
339         return combined
340 
341     ####################################
342     # METHODS THAT DO DATABASE QUERIES #
343     ####################################
344 
345     def _iterator(self, use_chunked_fetch, chunk_size):
346         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
347 
348     def iterator(self, chunk_size=2000):
349         """
350         An iterator over the results from applying this QuerySet to the
351         database.
352         """
353         if chunk_size <= 0:
354             raise ValueError('Chunk size must be strictly positive.')
355         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
356         return self._iterator(use_chunked_fetch, chunk_size)
357 
358     def aggregate(self, *args, **kwargs):
359         """
360         Return a dictionary containing the calculations (aggregation)
361         over the current queryset.
362 
363         If args is present the expression is passed as a kwarg using
364         the Aggregate object's default alias.
365         """
366         if self.query.distinct_fields:
367             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
368         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
369         for arg in args:
370             # The default_alias property raises TypeError if default_alias
371             # can't be set automatically or AttributeError if it isn't an
372             # attribute.
373             try:
374                 arg.default_alias
375             except (AttributeError, TypeError):
376                 raise TypeError("Complex aggregates require an alias")
377             kwargs[arg.default_alias] = arg
378 
379         query = self.query.chain()
380         for (alias, aggregate_expr) in kwargs.items():
381             query.add_annotation(aggregate_expr, alias, is_summary=True)
382             if not query.annotations[alias].contains_aggregate:
383                 raise TypeError("%s is not an aggregate expression" % alias)
384         return query.get_aggregation(self.db, kwargs)
385 
386     def count(self):
387         """
388         Perform a SELECT COUNT() and return the number of records as an
389         integer.
390 
391         If the QuerySet is already fully cached, return the length of the
392         cached results set to avoid multiple SELECT COUNT(*) calls.
393         """
394         if self._result_cache is not None:
395             return len(self._result_cache)
396 
397         return self.query.get_count(using=self.db)
398 
399     def get(self, *args, **kwargs):
400         """
401         Perform the query and return a single object matching the given
402         keyword arguments.
403         """
404         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
405         if self.query.can_filter() and not self.query.distinct_fields:
406             clone = clone.order_by()
407         limit = None
408         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
409             limit = MAX_GET_RESULTS
410             clone.query.set_limits(high=limit)
411         num = len(clone)
412         if num == 1:
413             return clone._result_cache[0]
414         if not num:
415             raise self.model.DoesNotExist(
416                 "%s matching query does not exist." %
417                 self.model._meta.object_name
418             )
419         raise self.model.MultipleObjectsReturned(
420             'get() returned more than one %s -- it returned %s!' % (
421                 self.model._meta.object_name,
422                 num if not limit or num < limit else 'more than %s' % (limit - 1),
423             )
424         )
425 
426     def create(self, **kwargs):
427         """
428         Create a new object with the given kwargs, saving it to the database
429         and returning the created object.
430         """
431         obj = self.model(**kwargs)
432         self._for_write = True
433         obj.save(force_insert=True, using=self.db)
434         return obj
435 
436     def _populate_pk_values(self, objs):
437         for obj in objs:
438             if obj.pk is None:
439                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
440 
441     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
442         """
443         Insert each of the instances into the database. Do *not* call
444         save() on each of the instances, do not send any pre/post_save
445         signals, and do not set the primary key attribute if it is an
446         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
447         Multi-table models are not supported.
448         """
449         # When you bulk insert you don't get the primary keys back (if it's an
450         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
451         # you can't insert into the child tables which references this. There
452         # are two workarounds:
453         # 1) This could be implemented if you didn't have an autoincrement pk
454         # 2) You could do it by doing O(n) normal inserts into the parent
455         #    tables to get the primary keys back and then doing a single bulk
456         #    insert into the childmost table.
457         # We currently set the primary keys on the objects when using
458         # PostgreSQL via the RETURNING ID clause. It should be possible for
459         # Oracle as well, but the semantics for extracting the primary keys is
460         # trickier so it's not done yet.
461         assert batch_size is None or batch_size > 0
462         # Check that the parents share the same concrete model with the our
463         # model to detect the inheritance pattern ConcreteGrandParent ->
464         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
465         # would not identify that case as involving multiple tables.
466         for parent in self.model._meta.get_parent_list():
467             if parent._meta.concrete_model is not self.model._meta.concrete_model:
468                 raise ValueError("Can't bulk create a multi-table inherited model")
469         if not objs:
470             return objs
471         self._for_write = True
472         connection = connections[self.db]
473         opts = self.model._meta
474         fields = opts.concrete_fields
475         objs = list(objs)
476         self._populate_pk_values(objs)
477         with transaction.atomic(using=self.db, savepoint=False):
478             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
479             if objs_with_pk:
480                 returned_columns = self._batched_insert(
481                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
482                 )
483                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
484                     for result, field in zip(results, opts.db_returning_fields):
485                         if field != opts.pk:
486                             setattr(obj_with_pk, field.attname, result)
487                 for obj_with_pk in objs_with_pk:
488                     obj_with_pk._state.adding = False
489                     obj_with_pk._state.db = self.db
490             if objs_without_pk:
491                 fields = [f for f in fields if not isinstance(f, AutoField)]
492                 returned_columns = self._batched_insert(
493                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
494                 )
495                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
496                     assert len(returned_columns) == len(objs_without_pk)
497                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         setattr(obj_without_pk, field.attname, result)
500                     obj_without_pk._state.adding = False
501                     obj_without_pk._state.db = self.db
502 
503         return objs
504 
505     def bulk_update(self, objs, fields, batch_size=None):
506         """
507         Update the given fields in each of the given objects in the database.
508         """
509         if batch_size is not None and batch_size < 0:
510             raise ValueError('Batch size must be a positive integer.')
511         if not fields:
512             raise ValueError('Field names must be given to bulk_update().')
513         objs = tuple(objs)
514         if any(obj.pk is None for obj in objs):
515             raise ValueError('All bulk_update() objects must have a primary key set.')
516         fields = [self.model._meta.get_field(name) for name in fields]
517         if any(not f.concrete or f.many_to_many for f in fields):
518             raise ValueError('bulk_update() can only be used with concrete fields.')
519         if any(f.primary_key for f in fields):
520             raise ValueError('bulk_update() cannot be used with primary key fields.')
521         if not objs:
522             return
523         # PK is used twice in the resulting update query, once in the filter
524         # and once in the WHEN. Each field will also have one CAST.
525         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
526         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
527         requires_casting = connections[self.db].features.requires_casted_case_in_updates
528         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
529         updates = []
530         for batch_objs in batches:
531             update_kwargs = {}
532             for field in fields:
533                 when_statements = []
534                 for obj in batch_objs:
535                     attr = getattr(obj, field.attname)
536                     if not isinstance(attr, Expression):
537                         attr = Value(attr, output_field=field)
538                     when_statements.append(When(pk=obj.pk, then=attr))
539                 case_statement = Case(*when_statements, output_field=field)
540                 if requires_casting:
541                     case_statement = Cast(case_statement, output_field=field)
542                 update_kwargs[field.attname] = case_statement
543             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
544         with transaction.atomic(using=self.db, savepoint=False):
545             for pks, update_kwargs in updates:
546                 self.filter(pk__in=pks).update(**update_kwargs)
547     bulk_update.alters_data = True
548 
549     def get_or_create(self, defaults=None, **kwargs):
550         """
551         Look up an object with the given kwargs, creating one if necessary.
552         Return a tuple of (object, created), where created is a boolean
553         specifying whether an object was created.
554         """
555         # The get() needs to be targeted at the write database in order
556         # to avoid potential transaction consistency problems.
557         self._for_write = True
558         try:
559             return self.get(**kwargs), False
560         except self.model.DoesNotExist:
561             params = self._extract_model_params(defaults, **kwargs)
562             return self._create_object_from_params(kwargs, params)
563 
564     def update_or_create(self, defaults=None, **kwargs):
565         """
566         Look up an object with the given kwargs, updating one with defaults
567         if it exists, otherwise create a new one.
568         Return a tuple (object, created), where created is a boolean
569         specifying whether an object was created.
570         """
571         defaults = defaults or {}
572         self._for_write = True
573         with transaction.atomic(using=self.db):
574             try:
575                 obj = self.select_for_update().get(**kwargs)
576             except self.model.DoesNotExist:
577                 params = self._extract_model_params(defaults, **kwargs)
578                 # Lock the row so that a concurrent update is blocked until
579                 # after update_or_create() has performed its save.
580                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
581                 if created:
582                     return obj, created
583             for k, v in defaults.items():
584                 setattr(obj, k, v() if callable(v) else v)
585             obj.save(using=self.db)
586         return obj, False
587 
588     def _create_object_from_params(self, lookup, params, lock=False):
589         """
590         Try to create an object using passed params. Used by get_or_create()
591         and update_or_create().
592         """
593         try:
594             with transaction.atomic(using=self.db):
595                 params = {k: v() if callable(v) else v for k, v in params.items()}
596                 obj = self.create(**params)
597             return obj, True
598         except IntegrityError as e:
599             try:
600                 qs = self.select_for_update() if lock else self
601                 return qs.get(**lookup), False
602             except self.model.DoesNotExist:
603                 pass
604             raise e
605 
606     def _extract_model_params(self, defaults, **kwargs):
607         """
608         Prepare `params` for creating a model instance based on the given
609         kwargs; for use by get_or_create() and update_or_create().
610         """
611         defaults = defaults or {}
612         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
613         params.update(defaults)
614         property_names = self.model._meta._property_names
615         invalid_params = []
616         for param in params:
617             try:
618                 self.model._meta.get_field(param)
619             except exceptions.FieldDoesNotExist:
620                 # It's okay to use a model's property if it has a setter.
621                 if not (param in property_names and getattr(self.model, param).fset):
622                     invalid_params.append(param)
623         if invalid_params:
624             raise exceptions.FieldError(
625                 "Invalid field name(s) for model %s: '%s'." % (
626                     self.model._meta.object_name,
627                     "', '".join(sorted(invalid_params)),
628                 ))
629         return params
630 
631     def _earliest(self, *fields):
632         """
633         Return the earliest object according to fields (if given) or by the
634         model's Meta.get_latest_by.
635         """
636         if fields:
637             order_by = fields
638         else:
639             order_by = getattr(self.model._meta, 'get_latest_by')
640             if order_by and not isinstance(order_by, (tuple, list)):
641                 order_by = (order_by,)
642         if order_by is None:
643             raise ValueError(
644                 "earliest() and latest() require either fields as positional "
645                 "arguments or 'get_latest_by' in the model's Meta."
646             )
647 
648         assert not self.query.is_sliced, \
649             "Cannot change a query once a slice has been taken."
650         obj = self._chain()
651         obj.query.set_limits(high=1)
652         obj.query.clear_ordering(force_empty=True)
653         obj.query.add_ordering(*order_by)
654         return obj.get()
655 
656     def earliest(self, *fields):
657         return self._earliest(*fields)
658 
659     def latest(self, *fields):
660         return self.reverse()._earliest(*fields)
661 
662     def first(self):
663         """Return the first object of a query or None if no match is found."""
664         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
665             return obj
666 
667     def last(self):
668         """Return the last object of a query or None if no match is found."""
669         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
670             return obj
671 
672     def in_bulk(self, id_list=None, *, field_name='pk'):
673         """
674         Return a dictionary mapping each of the given IDs to the object with
675         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
676         """
677         assert not self.query.is_sliced, \
678             "Cannot use 'limit' or 'offset' with in_bulk"
679         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
680             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
681         if id_list is not None:
682             if not id_list:
683                 return {}
684             filter_key = '{}__in'.format(field_name)
685             batch_size = connections[self.db].features.max_query_params
686             id_list = tuple(id_list)
687             # If the database has a limit on the number of query parameters
688             # (e.g. SQLite), retrieve objects in batches if necessary.
689             if batch_size and batch_size < len(id_list):
690                 qs = ()
691                 for offset in range(0, len(id_list), batch_size):
692                     batch = id_list[offset:offset + batch_size]
693                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
694             else:
695                 qs = self.filter(**{filter_key: id_list}).order_by()
696         else:
697             qs = self._chain()
698         return {getattr(obj, field_name): obj for obj in qs}
699 
700     def delete(self):
701         """Delete the records in the current QuerySet."""
702         assert not self.query.is_sliced, \
703             "Cannot use 'limit' or 'offset' with delete."
704 
705         if self._fields is not None:
706             raise TypeError("Cannot call delete() after .values() or .values_list()")
707 
708         del_query = self._chain()
709 
710         # The delete is actually 2 queries - one to find related objects,
711         # and one to delete. Make sure that the discovery of related
712         # objects is performed on the same database as the deletion.
713         del_query._for_write = True
714 
715         # Disable non-supported fields.
716         del_query.query.select_for_update = False
717         del_query.query.select_related = False
718         del_query.query.clear_ordering(force_empty=True)
719 
720         collector = Collector(using=del_query.db)
721         collector.collect(del_query)
722         deleted, _rows_count = collector.delete()
723 
724         # Clear the result cache, in case this QuerySet gets reused.
725         self._result_cache = None
726         return deleted, _rows_count
727 
728     delete.alters_data = True
729     delete.queryset_only = True
730 
731     def _raw_delete(self, using):
732         """
733         Delete objects found from the given queryset in single direct SQL
734         query. No signals are sent and there is no protection for cascades.
735         """
736         return sql.DeleteQuery(self.model).delete_qs(self, using)
737     _raw_delete.alters_data = True
738 
739     def update(self, **kwargs):
740         """
741         Update all elements in the current QuerySet, setting all the given
742         fields to the appropriate values.
743         """
744         assert not self.query.is_sliced, \
745             "Cannot update a query once a slice has been taken."
746         self._for_write = True
747         query = self.query.chain(sql.UpdateQuery)
748         query.add_update_values(kwargs)
749         # Clear any annotations so that they won't be present in subqueries.
750         query.annotations = {}
751         with transaction.mark_for_rollback_on_error(using=self.db):
752             rows = query.get_compiler(self.db).execute_sql(CURSOR)
753         self._result_cache = None
754         return rows
755     update.alters_data = True
756 
757     def _update(self, values):
758         """
759         A version of update() that accepts field objects instead of field names.
760         Used primarily for model saving and not intended for use by general
761         code (it requires too much poking around at model internals to be
762         useful at that level).
763         """
764         assert not self.query.is_sliced, \
765             "Cannot update a query once a slice has been taken."
766         query = self.query.chain(sql.UpdateQuery)
767         query.add_update_fields(values)
768         # Clear any annotations so that they won't be present in subqueries.
769         query.annotations = {}
770         self._result_cache = None
771         return query.get_compiler(self.db).execute_sql(CURSOR)
772     _update.alters_data = True
773     _update.queryset_only = False
774 
775     def exists(self):
776         if self._result_cache is None:
777             return self.query.has_results(using=self.db)
778         return bool(self._result_cache)
779 
780     def _prefetch_related_objects(self):
781         # This method can only be called once the result cache has been filled.
782         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
783         self._prefetch_done = True
784 
785     def explain(self, *, format=None, **options):
786         return self.query.explain(using=self.db, format=format, **options)
787 
788     ##################################################
789     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
790     ##################################################
791 
792     def raw(self, raw_query, params=None, translations=None, using=None):
793         if using is None:
794             using = self.db
795         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
796         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
797         return qs
798 
799     def _values(self, *fields, **expressions):
800         clone = self._chain()
801         if expressions:
802             clone = clone.annotate(**expressions)
803         clone._fields = fields
804         clone.query.set_values(fields)
805         return clone
806 
807     def values(self, *fields, **expressions):
808         fields += tuple(expressions)
809         clone = self._values(*fields, **expressions)
810         clone._iterable_class = ValuesIterable
811         return clone
812 
813     def values_list(self, *fields, flat=False, named=False):
814         if flat and named:
815             raise TypeError("'flat' and 'named' can't be used together.")
816         if flat and len(fields) > 1:
817             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
818 
819         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
820         _fields = []
821         expressions = {}
822         counter = 1
823         for field in fields:
824             if hasattr(field, 'resolve_expression'):
825                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
826                 while True:
827                     field_id = field_id_prefix + str(counter)
828                     counter += 1
829                     if field_id not in field_names:
830                         break
831                 expressions[field_id] = field
832                 _fields.append(field_id)
833             else:
834                 _fields.append(field)
835 
836         clone = self._values(*_fields, **expressions)
837         clone._iterable_class = (
838             NamedValuesListIterable if named
839             else FlatValuesListIterable if flat
840             else ValuesListIterable
841         )
842         return clone
843 
844     def dates(self, field_name, kind, order='ASC'):
845         """
846         Return a list of date objects representing all available dates for
847         the given field_name, scoped to 'kind'.
848         """
849         assert kind in ('year', 'month', 'week', 'day'), \
850             "'kind' must be one of 'year', 'month', 'week', or 'day'."
851         assert order in ('ASC', 'DESC'), \
852             "'order' must be either 'ASC' or 'DESC'."
853         return self.annotate(
854             datefield=Trunc(field_name, kind, output_field=DateField()),
855             plain_field=F(field_name)
856         ).values_list(
857             'datefield', flat=True
858         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
859 
860     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
861         """
862         Return a list of datetime objects representing all available
863         datetimes for the given field_name, scoped to 'kind'.
864         """
865         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
866             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
867         assert order in ('ASC', 'DESC'), \
868             "'order' must be either 'ASC' or 'DESC'."
869         if settings.USE_TZ:
870             if tzinfo is None:
871                 tzinfo = timezone.get_current_timezone()
872         else:
873             tzinfo = None
874         return self.annotate(
875             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
876             plain_field=F(field_name)
877         ).values_list(
878             'datetimefield', flat=True
879         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
880 
881     def none(self):
882         """Return an empty QuerySet."""
883         clone = self._chain()
884         clone.query.set_empty()
885         return clone
886 
887     ##################################################################
888     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
889     ##################################################################
890 
891     def all(self):
892         """
893         Return a new QuerySet that is a copy of the current one. This allows a
894         QuerySet to proxy for a model manager in some cases.
895         """
896         return self._chain()
897 
898     def filter(self, *args, **kwargs):
899         """
900         Return a new QuerySet instance with the args ANDed to the existing
901         set.
902         """
903         self._not_support_combined_queries('filter')
904         return self._filter_or_exclude(False, *args, **kwargs)
905 
906     def exclude(self, *args, **kwargs):
907         """
908         Return a new QuerySet instance with NOT (args) ANDed to the existing
909         set.
910         """
911         self._not_support_combined_queries('exclude')
912         return self._filter_or_exclude(True, *args, **kwargs)
913 
914     def _filter_or_exclude(self, negate, *args, **kwargs):
915         if args or kwargs:
916             assert not self.query.is_sliced, \
917                 "Cannot filter a query once a slice has been taken."
918 
919         clone = self._chain()
920         if negate:
921             clone.query.add_q(~Q(*args, **kwargs))
922         else:
923             clone.query.add_q(Q(*args, **kwargs))
924         return clone
925 
926     def complex_filter(self, filter_obj):
927         """
928         Return a new QuerySet instance with filter_obj added to the filters.
929 
930         filter_obj can be a Q object or a dictionary of keyword lookup
931         arguments.
932 
933         This exists to support framework features such as 'limit_choices_to',
934         and usually it will be more natural to use other methods.
935         """
936         if isinstance(filter_obj, Q):
937             clone = self._chain()
938             clone.query.add_q(filter_obj)
939             return clone
940         else:
941             return self._filter_or_exclude(None, **filter_obj)
942 
943     def _combinator_query(self, combinator, *other_qs, all=False):
944         # Clone the query to inherit the select list and everything
945         clone = self._chain()
946         # Clear limits and ordering so they can be reapplied
947         clone.query.clear_ordering(True)
948         clone.query.clear_limits()
949         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
950         clone.query.combinator = combinator
951         clone.query.combinator_all = all
952         return clone
953 
954     def union(self, *other_qs, all=False):
955         # If the query is an EmptyQuerySet, combine all nonempty querysets.
956         if isinstance(self, EmptyQuerySet):
957             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
958             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
959         return self._combinator_query('union', *other_qs, all=all)
960 
961     def intersection(self, *other_qs):
962         # If any query is an EmptyQuerySet, return it.
963         if isinstance(self, EmptyQuerySet):
964             return self
965         for other in other_qs:
966             if isinstance(other, EmptyQuerySet):
967                 return other
968         return self._combinator_query('intersection', *other_qs)
969 
970     def difference(self, *other_qs):
971         # If the query is an EmptyQuerySet, return it.
972         if isinstance(self, EmptyQuerySet):
973             return self
974         return self._combinator_query('difference', *other_qs)
975 
976     def select_for_update(self, nowait=False, skip_locked=False, of=()):
977         """
978         Return a new QuerySet instance that will select objects with a
979         FOR UPDATE lock.
980         """
981         if nowait and skip_locked:
982             raise ValueError('The nowait option cannot be used with skip_locked.')
983         obj = self._chain()
984         obj._for_write = True
985         obj.query.select_for_update = True
986         obj.query.select_for_update_nowait = nowait
987         obj.query.select_for_update_skip_locked = skip_locked
988         obj.query.select_for_update_of = of
989         return obj
990 
991     def select_related(self, *fields):
992         """
993         Return a new QuerySet instance that will select related objects.
994 
995         If fields are specified, they must be ForeignKey fields and only those
996         related objects are included in the selection.
997 
998         If select_related(None) is called, clear the list.
999         """
1000         self._not_support_combined_queries('select_related')
1001         if self._fields is not None:
1002             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1003 
1004         obj = self._chain()
1005         if fields == (None,):
1006             obj.query.select_related = False
1007         elif fields:
1008             obj.query.add_select_related(fields)
1009         else:
1010             obj.query.select_related = True
1011         return obj
1012 
1013     def prefetch_related(self, *lookups):
1014         """
1015         Return a new QuerySet instance that will prefetch the specified
1016         Many-To-One and Many-To-Many related objects when the QuerySet is
1017         evaluated.
1018 
1019         When prefetch_related() is called more than once, append to the list of
1020         prefetch lookups. If prefetch_related(None) is called, clear the list.
1021         """
1022         self._not_support_combined_queries('prefetch_related')
1023         clone = self._chain()
1024         if lookups == (None,):
1025             clone._prefetch_related_lookups = ()
1026         else:
1027             for lookup in lookups:
1028                 if isinstance(lookup, Prefetch):
1029                     lookup = lookup.prefetch_to
1030                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1031                 if lookup in self.query._filtered_relations:
1032                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1033             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1034         return clone
1035 
1036     def annotate(self, *args, **kwargs):
1037         """
1038         Return a query set in which the returned objects have been annotated
1039         with extra data or aggregations.
1040         """
1041         self._not_support_combined_queries('annotate')
1042         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1043         annotations = {}
1044         for arg in args:
1045             # The default_alias property may raise a TypeError.
1046             try:
1047                 if arg.default_alias in kwargs:
1048                     raise ValueError("The named annotation '%s' conflicts with the "
1049                                      "default name for another annotation."
1050                                      % arg.default_alias)
1051             except TypeError:
1052                 raise TypeError("Complex annotations require an alias")
1053             annotations[arg.default_alias] = arg
1054         annotations.update(kwargs)
1055 
1056         clone = self._chain()
1057         names = self._fields
1058         if names is None:
1059             names = set(chain.from_iterable(
1060                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1061                 for field in self.model._meta.get_fields()
1062             ))
1063 
1064         for alias, annotation in annotations.items():
1065             if alias in names:
1066                 raise ValueError("The annotation '%s' conflicts with a field on "
1067                                  "the model." % alias)
1068             if isinstance(annotation, FilteredRelation):
1069                 clone.query.add_filtered_relation(annotation, alias)
1070             else:
1071                 clone.query.add_annotation(annotation, alias, is_summary=False)
1072 
1073         for alias, annotation in clone.query.annotations.items():
1074             if alias in annotations and annotation.contains_aggregate:
1075                 if clone._fields is None:
1076                     clone.query.group_by = True
1077                 else:
1078                     clone.query.set_group_by()
1079                 break
1080 
1081         return clone
1082 
1083     def order_by(self, *field_names):
1084         """Return a new QuerySet instance with the ordering changed."""
1085         assert not self.query.is_sliced, \
1086             "Cannot reorder a query once a slice has been taken."
1087         obj = self._chain()
1088         obj.query.clear_ordering(force_empty=False)
1089         obj.query.add_ordering(*field_names)
1090         return obj
1091 
1092     def distinct(self, *field_names):
1093         """
1094         Return a new QuerySet instance that will select only distinct results.
1095         """
1096         assert not self.query.is_sliced, \
1097             "Cannot create distinct fields once a slice has been taken."
1098         obj = self._chain()
1099         obj.query.add_distinct_fields(*field_names)
1100         return obj
1101 
1102     def extra(self, select=None, where=None, params=None, tables=None,
1103               order_by=None, select_params=None):
1104         """Add extra SQL fragments to the query."""
1105         self._not_support_combined_queries('extra')
1106         assert not self.query.is_sliced, \
1107             "Cannot change a query once a slice has been taken"
1108         clone = self._chain()
1109         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1110         return clone
1111 
1112     def reverse(self):
1113         """Reverse the ordering of the QuerySet."""
1114         if self.query.is_sliced:
1115             raise TypeError('Cannot reverse a query once a slice has been taken.')
1116         clone = self._chain()
1117         clone.query.standard_ordering = not clone.query.standard_ordering
1118         return clone
1119 
1120     def defer(self, *fields):
1121         """
1122         Defer the loading of data for certain fields until they are accessed.
1123         Add the set of deferred fields to any existing set of deferred fields.
1124         The only exception to this is if None is passed in as the only
1125         parameter, in which case removal all deferrals.
1126         """
1127         self._not_support_combined_queries('defer')
1128         if self._fields is not None:
1129             raise TypeError("Cannot call defer() after .values() or .values_list()")
1130         clone = self._chain()
1131         if fields == (None,):
1132             clone.query.clear_deferred_loading()
1133         else:
1134             clone.query.add_deferred_loading(fields)
1135         return clone
1136 
1137     def only(self, *fields):
1138         """
1139         Essentially, the opposite of defer(). Only the fields passed into this
1140         method and that are not already specified as deferred are loaded
1141         immediately when the queryset is evaluated.
1142         """
1143         self._not_support_combined_queries('only')
1144         if self._fields is not None:
1145             raise TypeError("Cannot call only() after .values() or .values_list()")
1146         if fields == (None,):
1147             # Can only pass None to defer(), not only(), as the rest option.
1148             # That won't stop people trying to do this, so let's be explicit.
1149             raise TypeError("Cannot pass None as an argument to only().")
1150         for field in fields:
1151             field = field.split(LOOKUP_SEP, 1)[0]
1152             if field in self.query._filtered_relations:
1153                 raise ValueError('only() is not supported with FilteredRelation.')
1154         clone = self._chain()
1155         clone.query.add_immediate_loading(fields)
1156         return clone
1157 
1158     def using(self, alias):
1159         """Select which database this QuerySet should execute against."""
1160         clone = self._chain()
1161         clone._db = alias
1162         return clone
1163 
1164     ###################################
1165     # PUBLIC INTROSPECTION ATTRIBUTES #
1166     ###################################
1167 
1168     @property
1169     def ordered(self):
1170         """
1171         Return True if the QuerySet is ordered -- i.e. has an order_by()
1172         clause or a default ordering on the model (or is empty).
1173         """
1174         if isinstance(self, EmptyQuerySet):
1175             return True
1176         if self.query.extra_order_by or self.query.order_by:
1177             return True
1178         elif self.query.default_ordering and self.query.get_meta().ordering:
1179             return True
1180         else:
1181             return False
1182 
1183     @property
1184     def db(self):
1185         """Return the database used if this query is executed now."""
1186         if self._for_write:
1187             return self._db or router.db_for_write(self.model, **self._hints)
1188         return self._db or router.db_for_read(self.model, **self._hints)
1189 
1190     ###################
1191     # PRIVATE METHODS #
1192     ###################
1193 
1194     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1195         """
1196         Insert a new record for the given model. This provides an interface to
1197         the InsertQuery class and is how Model.save() is implemented.
1198         """
1199         self._for_write = True
1200         if using is None:
1201             using = self.db
1202         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1203         query.insert_values(fields, objs, raw=raw)
1204         return query.get_compiler(using=using).execute_sql(returning_fields)
1205     _insert.alters_data = True
1206     _insert.queryset_only = False
1207 
1208     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1209         """
1210         Helper method for bulk_create() to insert objs one batch at a time.
1211         """
1212         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1213             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1214         ops = connections[self.db].ops
1215         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1216         inserted_rows = []
1217         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1218         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1219             if bulk_return and not ignore_conflicts:
1220                 inserted_columns = self._insert(
1221                     item, fields=fields, using=self.db,
1222                     returning_fields=self.model._meta.db_returning_fields,
1223                     ignore_conflicts=ignore_conflicts,
1224                 )
1225                 if isinstance(inserted_columns, list):
1226                     inserted_rows.extend(inserted_columns)
1227                 else:
1228                     inserted_rows.append(inserted_columns)
1229             else:
1230                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1231         return inserted_rows
1232 
1233     def _chain(self, **kwargs):
1234         """
1235         Return a copy of the current QuerySet that's ready for another
1236         operation.
1237         """
1238         obj = self._clone()
1239         if obj._sticky_filter:
1240             obj.query.filter_is_sticky = True
1241             obj._sticky_filter = False
1242         obj.__dict__.update(kwargs)
1243         return obj
1244 
1245     def _clone(self):
1246         """
1247         Return a copy of the current QuerySet. A lightweight alternative
1248         to deepcopy().
1249         """
1250         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1251         c._sticky_filter = self._sticky_filter
1252         c._for_write = self._for_write
1253         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1254         c._known_related_objects = self._known_related_objects
1255         c._iterable_class = self._iterable_class
1256         c._fields = self._fields
1257         return c
1258 
1259     def _fetch_all(self):
1260         if self._result_cache is None:
1261             self._result_cache = list(self._iterable_class(self))
1262         if self._prefetch_related_lookups and not self._prefetch_done:
1263             self._prefetch_related_objects()
1264 
1265     def _next_is_sticky(self):
1266         """
1267         Indicate that the next filter call and the one following that should
1268         be treated as a single filter. This is only important when it comes to
1269         determining when to reuse tables for many-to-many filters. Required so
1270         that we can filter naturally on the results of related managers.
1271 
1272         This doesn't return a clone of the current QuerySet (it returns
1273         "self"). The method is only used internally and should be immediately
1274         followed by a filter() that does create a clone.
1275         """
1276         self._sticky_filter = True
1277         return self
1278 
1279     def _merge_sanity_check(self, other):
1280         """Check that two QuerySet classes may be merged."""
1281         if self._fields is not None and (
1282                 set(self.query.values_select) != set(other.query.values_select) or
1283                 set(self.query.extra_select) != set(other.query.extra_select) or
1284                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1285             raise TypeError(
1286                 "Merging '%s' classes must involve the same values in each case."
1287                 % self.__class__.__name__
1288             )
1289 
1290     def _merge_known_related_objects(self, other):
1291         """
1292         Keep track of all known related objects from either QuerySet instance.
1293         """
1294         for field, objects in other._known_related_objects.items():
1295             self._known_related_objects.setdefault(field, {}).update(objects)
1296 
1297     def resolve_expression(self, *args, **kwargs):
1298         if self._fields and len(self._fields) > 1:
1299             # values() queryset can only be used as nested queries
1300             # if they are set up to select only a single field.
1301             raise TypeError('Cannot use multi-field values as a filter value.')
1302         query = self.query.resolve_expression(*args, **kwargs)
1303         query._db = self._db
1304         return query
1305     resolve_expression.queryset_only = True
1306 
1307     def _add_hints(self, **hints):
1308         """
1309         Update hinting information for use by routers. Add new key/values or
1310         overwrite existing key/values.
1311         """
1312         self._hints.update(hints)
1313 
1314     def _has_filters(self):
1315         """
1316         Check if this QuerySet has any filtering going on. This isn't
1317         equivalent with checking if all objects are present in results, for
1318         example, qs[1:]._has_filters() -> False.
1319         """
1320         return self.query.has_filters()
1321 
1322     @staticmethod
1323     def _validate_values_are_expressions(values, method_name):
1324         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1325         if invalid_args:
1326             raise TypeError(
1327                 'QuerySet.%s() received non-expression(s): %s.' % (
1328                     method_name,
1329                     ', '.join(invalid_args),
1330                 )
1331             )
1332 
1333     def _not_support_combined_queries(self, operation_name):
1334         if self.query.combinator:
1335             raise NotSupportedError(
1336                 'Calling QuerySet.%s() after %s() is not supported.'
1337                 % (operation_name, self.query.combinator)
1338             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>_clone</method>
<code>
1245     def _clone(self):
1246         """
1247         Return a copy of the current QuerySet. A lightweight alternative
1248         to deepcopy().
1249         """
1250         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1251         c._sticky_filter = self._sticky_filter
1252         c._for_write = self._for_write
1253         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1254         c._known_related_objects = self._known_related_objects
1255         c._iterable_class = self._iterable_class
1256         c._fields = self._fields
1257         return c

</code><intended_behavior>The _clone method's intended behavior is to produce a fully independent copy of the QuerySet, which implies no shared state between the original and the cloned instance that could lead to the mutations observed. A thorough review is necessary to ensure that all attributes, especially those that can be modified like query (which houses select_related logic), are correctly deep copied or uniquely instantiated to preserve this independence. This might involve changing the way references to mutable objects (like dicts or custom objects) are copied, ensuring a true deep copy is made where necessary.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
188 class QuerySet:
189     """Represent a lazy database lookup for a set of objects."""
190 
191     def __init__(self, model=None, query=None, using=None, hints=None):
192         self.model = model
193         self._db = using
194         self._hints = hints or {}
195         self.query = query or sql.Query(self.model)
196         self._result_cache = None
197         self._sticky_filter = False
198         self._for_write = False
199         self._prefetch_related_lookups = ()
200         self._prefetch_done = False
201         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
202         self._iterable_class = ModelIterable
203         self._fields = None
204 
205     def as_manager(cls):
206         # Address the circular dependency between `Queryset` and `Manager`.
207         from django.db.models.manager import Manager
208         manager = Manager.from_queryset(cls)()
209         manager._built_with_as_manager = True
210         return manager
211     as_manager.queryset_only = True
212     as_manager = classmethod(as_manager)
213 
214     ########################
215     # PYTHON MAGIC METHODS #
216     ########################
217 
218     def __deepcopy__(self, memo):
219         """Don't populate the QuerySet's cache."""
220         obj = self.__class__()
221         for k, v in self.__dict__.items():
222             if k == '_result_cache':
223                 obj.__dict__[k] = None
224             else:
225                 obj.__dict__[k] = copy.deepcopy(v, memo)
226         return obj
227 
228     def __getstate__(self):
229         # Force the cache to be fully populated.
230         self._fetch_all()
231         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
232 
233     def __setstate__(self, state):
234         msg = None
235         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
236         if pickled_version:
237             current_version = get_version()
238             if current_version != pickled_version:
239                 msg = (
240                     "Pickled queryset instance's Django version %s does not "
241                     "match the current version %s." % (pickled_version, current_version)
242                 )
243         else:
244             msg = "Pickled queryset instance's Django version is not specified."
245 
246         if msg:
247             warnings.warn(msg, RuntimeWarning, stacklevel=2)
248 
249         self.__dict__.update(state)
250 
251     def __repr__(self):
252         data = list(self[:REPR_OUTPUT_SIZE + 1])
253         if len(data) > REPR_OUTPUT_SIZE:
254             data[-1] = "...(remaining elements truncated)..."
255         return '<%s %r>' % (self.__class__.__name__, data)
256 
257     def __len__(self):
258         self._fetch_all()
259         return len(self._result_cache)
260 
261     def __iter__(self):
262         """
263         The queryset iterator protocol uses three nested iterators in the
264         default case:
265             1. sql.compiler.execute_sql()
266                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
267                  using cursor.fetchmany(). This part is responsible for
268                  doing some column masking, and returning the rows in chunks.
269             2. sql.compiler.results_iter()
270                - Returns one row at time. At this point the rows are still just
271                  tuples. In some cases the return values are converted to
272                  Python values at this location.
273             3. self.iterator()
274                - Responsible for turning the rows into model objects.
275         """
276         self._fetch_all()
277         return iter(self._result_cache)
278 
279     def __bool__(self):
280         self._fetch_all()
281         return bool(self._result_cache)
282 
283     def __getitem__(self, k):
284         """Retrieve an item or slice from the set of results."""
285         if not isinstance(k, (int, slice)):
286             raise TypeError(
287                 'QuerySet indices must be integers or slices, not %s.'
288                 % type(k).__name__
289             )
290         assert ((not isinstance(k, slice) and (k >= 0)) or
291                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
292                  (k.stop is None or k.stop >= 0))), \
293             "Negative indexing is not supported."
294 
295         if self._result_cache is not None:
296             return self._result_cache[k]
297 
298         if isinstance(k, slice):
299             qs = self._chain()
300             if k.start is not None:
301                 start = int(k.start)
302             else:
303                 start = None
304             if k.stop is not None:
305                 stop = int(k.stop)
306             else:
307                 stop = None
308             qs.query.set_limits(start, stop)
309             return list(qs)[::k.step] if k.step else qs
310 
311         qs = self._chain()
312         qs.query.set_limits(k, k + 1)
313         qs._fetch_all()
314         return qs._result_cache[0]
315 
316     def __and__(self, other):
317         self._merge_sanity_check(other)
318         if isinstance(other, EmptyQuerySet):
319             return other
320         if isinstance(self, EmptyQuerySet):
321             return self
322         combined = self._chain()
323         combined._merge_known_related_objects(other)
324         combined.query.combine(other.query, sql.AND)
325         return combined
326 
327     def __or__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(self, EmptyQuerySet):
330             return other
331         if isinstance(other, EmptyQuerySet):
332             return self
333         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
334         combined = query._chain()
335         combined._merge_known_related_objects(other)
336         if not other.query.can_filter():
337             other = other.model._base_manager.filter(pk__in=other.values('pk'))
338         combined.query.combine(other.query, sql.OR)
339         return combined
340 
341     ####################################
342     # METHODS THAT DO DATABASE QUERIES #
343     ####################################
344 
345     def _iterator(self, use_chunked_fetch, chunk_size):
346         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
347 
348     def iterator(self, chunk_size=2000):
349         """
350         An iterator over the results from applying this QuerySet to the
351         database.
352         """
353         if chunk_size <= 0:
354             raise ValueError('Chunk size must be strictly positive.')
355         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
356         return self._iterator(use_chunked_fetch, chunk_size)
357 
358     def aggregate(self, *args, **kwargs):
359         """
360         Return a dictionary containing the calculations (aggregation)
361         over the current queryset.
362 
363         If args is present the expression is passed as a kwarg using
364         the Aggregate object's default alias.
365         """
366         if self.query.distinct_fields:
367             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
368         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
369         for arg in args:
370             # The default_alias property raises TypeError if default_alias
371             # can't be set automatically or AttributeError if it isn't an
372             # attribute.
373             try:
374                 arg.default_alias
375             except (AttributeError, TypeError):
376                 raise TypeError("Complex aggregates require an alias")
377             kwargs[arg.default_alias] = arg
378 
379         query = self.query.chain()
380         for (alias, aggregate_expr) in kwargs.items():
381             query.add_annotation(aggregate_expr, alias, is_summary=True)
382             if not query.annotations[alias].contains_aggregate:
383                 raise TypeError("%s is not an aggregate expression" % alias)
384         return query.get_aggregation(self.db, kwargs)
385 
386     def count(self):
387         """
388         Perform a SELECT COUNT() and return the number of records as an
389         integer.
390 
391         If the QuerySet is already fully cached, return the length of the
392         cached results set to avoid multiple SELECT COUNT(*) calls.
393         """
394         if self._result_cache is not None:
395             return len(self._result_cache)
396 
397         return self.query.get_count(using=self.db)
398 
399     def get(self, *args, **kwargs):
400         """
401         Perform the query and return a single object matching the given
402         keyword arguments.
403         """
404         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
405         if self.query.can_filter() and not self.query.distinct_fields:
406             clone = clone.order_by()
407         limit = None
408         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
409             limit = MAX_GET_RESULTS
410             clone.query.set_limits(high=limit)
411         num = len(clone)
412         if num == 1:
413             return clone._result_cache[0]
414         if not num:
415             raise self.model.DoesNotExist(
416                 "%s matching query does not exist." %
417                 self.model._meta.object_name
418             )
419         raise self.model.MultipleObjectsReturned(
420             'get() returned more than one %s -- it returned %s!' % (
421                 self.model._meta.object_name,
422                 num if not limit or num < limit else 'more than %s' % (limit - 1),
423             )
424         )
425 
426     def create(self, **kwargs):
427         """
428         Create a new object with the given kwargs, saving it to the database
429         and returning the created object.
430         """
431         obj = self.model(**kwargs)
432         self._for_write = True
433         obj.save(force_insert=True, using=self.db)
434         return obj
435 
436     def _populate_pk_values(self, objs):
437         for obj in objs:
438             if obj.pk is None:
439                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
440 
441     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
442         """
443         Insert each of the instances into the database. Do *not* call
444         save() on each of the instances, do not send any pre/post_save
445         signals, and do not set the primary key attribute if it is an
446         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
447         Multi-table models are not supported.
448         """
449         # When you bulk insert you don't get the primary keys back (if it's an
450         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
451         # you can't insert into the child tables which references this. There
452         # are two workarounds:
453         # 1) This could be implemented if you didn't have an autoincrement pk
454         # 2) You could do it by doing O(n) normal inserts into the parent
455         #    tables to get the primary keys back and then doing a single bulk
456         #    insert into the childmost table.
457         # We currently set the primary keys on the objects when using
458         # PostgreSQL via the RETURNING ID clause. It should be possible for
459         # Oracle as well, but the semantics for extracting the primary keys is
460         # trickier so it's not done yet.
461         assert batch_size is None or batch_size > 0
462         # Check that the parents share the same concrete model with the our
463         # model to detect the inheritance pattern ConcreteGrandParent ->
464         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
465         # would not identify that case as involving multiple tables.
466         for parent in self.model._meta.get_parent_list():
467             if parent._meta.concrete_model is not self.model._meta.concrete_model:
468                 raise ValueError("Can't bulk create a multi-table inherited model")
469         if not objs:
470             return objs
471         self._for_write = True
472         connection = connections[self.db]
473         opts = self.model._meta
474         fields = opts.concrete_fields
475         objs = list(objs)
476         self._populate_pk_values(objs)
477         with transaction.atomic(using=self.db, savepoint=False):
478             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
479             if objs_with_pk:
480                 returned_columns = self._batched_insert(
481                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
482                 )
483                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
484                     for result, field in zip(results, opts.db_returning_fields):
485                         if field != opts.pk:
486                             setattr(obj_with_pk, field.attname, result)
487                 for obj_with_pk in objs_with_pk:
488                     obj_with_pk._state.adding = False
489                     obj_with_pk._state.db = self.db
490             if objs_without_pk:
491                 fields = [f for f in fields if not isinstance(f, AutoField)]
492                 returned_columns = self._batched_insert(
493                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
494                 )
495                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
496                     assert len(returned_columns) == len(objs_without_pk)
497                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         setattr(obj_without_pk, field.attname, result)
500                     obj_without_pk._state.adding = False
501                     obj_without_pk._state.db = self.db
502 
503         return objs
504 
505     def bulk_update(self, objs, fields, batch_size=None):
506         """
507         Update the given fields in each of the given objects in the database.
508         """
509         if batch_size is not None and batch_size < 0:
510             raise ValueError('Batch size must be a positive integer.')
511         if not fields:
512             raise ValueError('Field names must be given to bulk_update().')
513         objs = tuple(objs)
514         if any(obj.pk is None for obj in objs):
515             raise ValueError('All bulk_update() objects must have a primary key set.')
516         fields = [self.model._meta.get_field(name) for name in fields]
517         if any(not f.concrete or f.many_to_many for f in fields):
518             raise ValueError('bulk_update() can only be used with concrete fields.')
519         if any(f.primary_key for f in fields):
520             raise ValueError('bulk_update() cannot be used with primary key fields.')
521         if not objs:
522             return
523         # PK is used twice in the resulting update query, once in the filter
524         # and once in the WHEN. Each field will also have one CAST.
525         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
526         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
527         requires_casting = connections[self.db].features.requires_casted_case_in_updates
528         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
529         updates = []
530         for batch_objs in batches:
531             update_kwargs = {}
532             for field in fields:
533                 when_statements = []
534                 for obj in batch_objs:
535                     attr = getattr(obj, field.attname)
536                     if not isinstance(attr, Expression):
537                         attr = Value(attr, output_field=field)
538                     when_statements.append(When(pk=obj.pk, then=attr))
539                 case_statement = Case(*when_statements, output_field=field)
540                 if requires_casting:
541                     case_statement = Cast(case_statement, output_field=field)
542                 update_kwargs[field.attname] = case_statement
543             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
544         with transaction.atomic(using=self.db, savepoint=False):
545             for pks, update_kwargs in updates:
546                 self.filter(pk__in=pks).update(**update_kwargs)
547     bulk_update.alters_data = True
548 
549     def get_or_create(self, defaults=None, **kwargs):
550         """
551         Look up an object with the given kwargs, creating one if necessary.
552         Return a tuple of (object, created), where created is a boolean
553         specifying whether an object was created.
554         """
555         # The get() needs to be targeted at the write database in order
556         # to avoid potential transaction consistency problems.
557         self._for_write = True
558         try:
559             return self.get(**kwargs), False
560         except self.model.DoesNotExist:
561             params = self._extract_model_params(defaults, **kwargs)
562             return self._create_object_from_params(kwargs, params)
563 
564     def update_or_create(self, defaults=None, **kwargs):
565         """
566         Look up an object with the given kwargs, updating one with defaults
567         if it exists, otherwise create a new one.
568         Return a tuple (object, created), where created is a boolean
569         specifying whether an object was created.
570         """
571         defaults = defaults or {}
572         self._for_write = True
573         with transaction.atomic(using=self.db):
574             try:
575                 obj = self.select_for_update().get(**kwargs)
576             except self.model.DoesNotExist:
577                 params = self._extract_model_params(defaults, **kwargs)
578                 # Lock the row so that a concurrent update is blocked until
579                 # after update_or_create() has performed its save.
580                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
581                 if created:
582                     return obj, created
583             for k, v in defaults.items():
584                 setattr(obj, k, v() if callable(v) else v)
585             obj.save(using=self.db)
586         return obj, False
587 
588     def _create_object_from_params(self, lookup, params, lock=False):
589         """
590         Try to create an object using passed params. Used by get_or_create()
591         and update_or_create().
592         """
593         try:
594             with transaction.atomic(using=self.db):
595                 params = {k: v() if callable(v) else v for k, v in params.items()}
596                 obj = self.create(**params)
597             return obj, True
598         except IntegrityError as e:
599             try:
600                 qs = self.select_for_update() if lock else self
601                 return qs.get(**lookup), False
602             except self.model.DoesNotExist:
603                 pass
604             raise e
605 
606     def _extract_model_params(self, defaults, **kwargs):
607         """
608         Prepare `params` for creating a model instance based on the given
609         kwargs; for use by get_or_create() and update_or_create().
610         """
611         defaults = defaults or {}
612         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
613         params.update(defaults)
614         property_names = self.model._meta._property_names
615         invalid_params = []
616         for param in params:
617             try:
618                 self.model._meta.get_field(param)
619             except exceptions.FieldDoesNotExist:
620                 # It's okay to use a model's property if it has a setter.
621                 if not (param in property_names and getattr(self.model, param).fset):
622                     invalid_params.append(param)
623         if invalid_params:
624             raise exceptions.FieldError(
625                 "Invalid field name(s) for model %s: '%s'." % (
626                     self.model._meta.object_name,
627                     "', '".join(sorted(invalid_params)),
628                 ))
629         return params
630 
631     def _earliest(self, *fields):
632         """
633         Return the earliest object according to fields (if given) or by the
634         model's Meta.get_latest_by.
635         """
636         if fields:
637             order_by = fields
638         else:
639             order_by = getattr(self.model._meta, 'get_latest_by')
640             if order_by and not isinstance(order_by, (tuple, list)):
641                 order_by = (order_by,)
642         if order_by is None:
643             raise ValueError(
644                 "earliest() and latest() require either fields as positional "
645                 "arguments or 'get_latest_by' in the model's Meta."
646             )
647 
648         assert not self.query.is_sliced, \
649             "Cannot change a query once a slice has been taken."
650         obj = self._chain()
651         obj.query.set_limits(high=1)
652         obj.query.clear_ordering(force_empty=True)
653         obj.query.add_ordering(*order_by)
654         return obj.get()
655 
656     def earliest(self, *fields):
657         return self._earliest(*fields)
658 
659     def latest(self, *fields):
660         return self.reverse()._earliest(*fields)
661 
662     def first(self):
663         """Return the first object of a query or None if no match is found."""
664         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
665             return obj
666 
667     def last(self):
668         """Return the last object of a query or None if no match is found."""
669         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
670             return obj
671 
672     def in_bulk(self, id_list=None, *, field_name='pk'):
673         """
674         Return a dictionary mapping each of the given IDs to the object with
675         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
676         """
677         assert not self.query.is_sliced, \
678             "Cannot use 'limit' or 'offset' with in_bulk"
679         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
680             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
681         if id_list is not None:
682             if not id_list:
683                 return {}
684             filter_key = '{}__in'.format(field_name)
685             batch_size = connections[self.db].features.max_query_params
686             id_list = tuple(id_list)
687             # If the database has a limit on the number of query parameters
688             # (e.g. SQLite), retrieve objects in batches if necessary.
689             if batch_size and batch_size < len(id_list):
690                 qs = ()
691                 for offset in range(0, len(id_list), batch_size):
692                     batch = id_list[offset:offset + batch_size]
693                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
694             else:
695                 qs = self.filter(**{filter_key: id_list}).order_by()
696         else:
697             qs = self._chain()
698         return {getattr(obj, field_name): obj for obj in qs}
699 
700     def delete(self):
701         """Delete the records in the current QuerySet."""
702         assert not self.query.is_sliced, \
703             "Cannot use 'limit' or 'offset' with delete."
704 
705         if self._fields is not None:
706             raise TypeError("Cannot call delete() after .values() or .values_list()")
707 
708         del_query = self._chain()
709 
710         # The delete is actually 2 queries - one to find related objects,
711         # and one to delete. Make sure that the discovery of related
712         # objects is performed on the same database as the deletion.
713         del_query._for_write = True
714 
715         # Disable non-supported fields.
716         del_query.query.select_for_update = False
717         del_query.query.select_related = False
718         del_query.query.clear_ordering(force_empty=True)
719 
720         collector = Collector(using=del_query.db)
721         collector.collect(del_query)
722         deleted, _rows_count = collector.delete()
723 
724         # Clear the result cache, in case this QuerySet gets reused.
725         self._result_cache = None
726         return deleted, _rows_count
727 
728     delete.alters_data = True
729     delete.queryset_only = True
730 
731     def _raw_delete(self, using):
732         """
733         Delete objects found from the given queryset in single direct SQL
734         query. No signals are sent and there is no protection for cascades.
735         """
736         return sql.DeleteQuery(self.model).delete_qs(self, using)
737     _raw_delete.alters_data = True
738 
739     def update(self, **kwargs):
740         """
741         Update all elements in the current QuerySet, setting all the given
742         fields to the appropriate values.
743         """
744         assert not self.query.is_sliced, \
745             "Cannot update a query once a slice has been taken."
746         self._for_write = True
747         query = self.query.chain(sql.UpdateQuery)
748         query.add_update_values(kwargs)
749         # Clear any annotations so that they won't be present in subqueries.
750         query.annotations = {}
751         with transaction.mark_for_rollback_on_error(using=self.db):
752             rows = query.get_compiler(self.db).execute_sql(CURSOR)
753         self._result_cache = None
754         return rows
755     update.alters_data = True
756 
757     def _update(self, values):
758         """
759         A version of update() that accepts field objects instead of field names.
760         Used primarily for model saving and not intended for use by general
761         code (it requires too much poking around at model internals to be
762         useful at that level).
763         """
764         assert not self.query.is_sliced, \
765             "Cannot update a query once a slice has been taken."
766         query = self.query.chain(sql.UpdateQuery)
767         query.add_update_fields(values)
768         # Clear any annotations so that they won't be present in subqueries.
769         query.annotations = {}
770         self._result_cache = None
771         return query.get_compiler(self.db).execute_sql(CURSOR)
772     _update.alters_data = True
773     _update.queryset_only = False
774 
775     def exists(self):
776         if self._result_cache is None:
777             return self.query.has_results(using=self.db)
778         return bool(self._result_cache)
779 
780     def _prefetch_related_objects(self):
781         # This method can only be called once the result cache has been filled.
782         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
783         self._prefetch_done = True
784 
785     def explain(self, *, format=None, **options):
786         return self.query.explain(using=self.db, format=format, **options)
787 
788     ##################################################
789     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
790     ##################################################
791 
792     def raw(self, raw_query, params=None, translations=None, using=None):
793         if using is None:
794             using = self.db
795         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
796         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
797         return qs
798 
799     def _values(self, *fields, **expressions):
800         clone = self._chain()
801         if expressions:
802             clone = clone.annotate(**expressions)
803         clone._fields = fields
804         clone.query.set_values(fields)
805         return clone
806 
807     def values(self, *fields, **expressions):
808         fields += tuple(expressions)
809         clone = self._values(*fields, **expressions)
810         clone._iterable_class = ValuesIterable
811         return clone
812 
813     def values_list(self, *fields, flat=False, named=False):
814         if flat and named:
815             raise TypeError("'flat' and 'named' can't be used together.")
816         if flat and len(fields) > 1:
817             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
818 
819         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
820         _fields = []
821         expressions = {}
822         counter = 1
823         for field in fields:
824             if hasattr(field, 'resolve_expression'):
825                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
826                 while True:
827                     field_id = field_id_prefix + str(counter)
828                     counter += 1
829                     if field_id not in field_names:
830                         break
831                 expressions[field_id] = field
832                 _fields.append(field_id)
833             else:
834                 _fields.append(field)
835 
836         clone = self._values(*_fields, **expressions)
837         clone._iterable_class = (
838             NamedValuesListIterable if named
839             else FlatValuesListIterable if flat
840             else ValuesListIterable
841         )
842         return clone
843 
844     def dates(self, field_name, kind, order='ASC'):
845         """
846         Return a list of date objects representing all available dates for
847         the given field_name, scoped to 'kind'.
848         """
849         assert kind in ('year', 'month', 'week', 'day'), \
850             "'kind' must be one of 'year', 'month', 'week', or 'day'."
851         assert order in ('ASC', 'DESC'), \
852             "'order' must be either 'ASC' or 'DESC'."
853         return self.annotate(
854             datefield=Trunc(field_name, kind, output_field=DateField()),
855             plain_field=F(field_name)
856         ).values_list(
857             'datefield', flat=True
858         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
859 
860     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
861         """
862         Return a list of datetime objects representing all available
863         datetimes for the given field_name, scoped to 'kind'.
864         """
865         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
866             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
867         assert order in ('ASC', 'DESC'), \
868             "'order' must be either 'ASC' or 'DESC'."
869         if settings.USE_TZ:
870             if tzinfo is None:
871                 tzinfo = timezone.get_current_timezone()
872         else:
873             tzinfo = None
874         return self.annotate(
875             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
876             plain_field=F(field_name)
877         ).values_list(
878             'datetimefield', flat=True
879         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
880 
881     def none(self):
882         """Return an empty QuerySet."""
883         clone = self._chain()
884         clone.query.set_empty()
885         return clone
886 
887     ##################################################################
888     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
889     ##################################################################
890 
891     def all(self):
892         """
893         Return a new QuerySet that is a copy of the current one. This allows a
894         QuerySet to proxy for a model manager in some cases.
895         """
896         return self._chain()
897 
898     def filter(self, *args, **kwargs):
899         """
900         Return a new QuerySet instance with the args ANDed to the existing
901         set.
902         """
903         self._not_support_combined_queries('filter')
904         return self._filter_or_exclude(False, *args, **kwargs)
905 
906     def exclude(self, *args, **kwargs):
907         """
908         Return a new QuerySet instance with NOT (args) ANDed to the existing
909         set.
910         """
911         self._not_support_combined_queries('exclude')
912         return self._filter_or_exclude(True, *args, **kwargs)
913 
914     def _filter_or_exclude(self, negate, *args, **kwargs):
915         if args or kwargs:
916             assert not self.query.is_sliced, \
917                 "Cannot filter a query once a slice has been taken."
918 
919         clone = self._chain()
920         if negate:
921             clone.query.add_q(~Q(*args, **kwargs))
922         else:
923             clone.query.add_q(Q(*args, **kwargs))
924         return clone
925 
926     def complex_filter(self, filter_obj):
927         """
928         Return a new QuerySet instance with filter_obj added to the filters.
929 
930         filter_obj can be a Q object or a dictionary of keyword lookup
931         arguments.
932 
933         This exists to support framework features such as 'limit_choices_to',
934         and usually it will be more natural to use other methods.
935         """
936         if isinstance(filter_obj, Q):
937             clone = self._chain()
938             clone.query.add_q(filter_obj)
939             return clone
940         else:
941             return self._filter_or_exclude(None, **filter_obj)
942 
943     def _combinator_query(self, combinator, *other_qs, all=False):
944         # Clone the query to inherit the select list and everything
945         clone = self._chain()
946         # Clear limits and ordering so they can be reapplied
947         clone.query.clear_ordering(True)
948         clone.query.clear_limits()
949         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
950         clone.query.combinator = combinator
951         clone.query.combinator_all = all
952         return clone
953 
954     def union(self, *other_qs, all=False):
955         # If the query is an EmptyQuerySet, combine all nonempty querysets.
956         if isinstance(self, EmptyQuerySet):
957             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
958             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
959         return self._combinator_query('union', *other_qs, all=all)
960 
961     def intersection(self, *other_qs):
962         # If any query is an EmptyQuerySet, return it.
963         if isinstance(self, EmptyQuerySet):
964             return self
965         for other in other_qs:
966             if isinstance(other, EmptyQuerySet):
967                 return other
968         return self._combinator_query('intersection', *other_qs)
969 
970     def difference(self, *other_qs):
971         # If the query is an EmptyQuerySet, return it.
972         if isinstance(self, EmptyQuerySet):
973             return self
974         return self._combinator_query('difference', *other_qs)
975 
976     def select_for_update(self, nowait=False, skip_locked=False, of=()):
977         """
978         Return a new QuerySet instance that will select objects with a
979         FOR UPDATE lock.
980         """
981         if nowait and skip_locked:
982             raise ValueError('The nowait option cannot be used with skip_locked.')
983         obj = self._chain()
984         obj._for_write = True
985         obj.query.select_for_update = True
986         obj.query.select_for_update_nowait = nowait
987         obj.query.select_for_update_skip_locked = skip_locked
988         obj.query.select_for_update_of = of
989         return obj
990 
991     def select_related(self, *fields):
992         """
993         Return a new QuerySet instance that will select related objects.
994 
995         If fields are specified, they must be ForeignKey fields and only those
996         related objects are included in the selection.
997 
998         If select_related(None) is called, clear the list.
999         """
1000         self._not_support_combined_queries('select_related')
1001         if self._fields is not None:
1002             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1003 
1004         obj = self._chain()
1005         if fields == (None,):
1006             obj.query.select_related = False
1007         elif fields:
1008             obj.query.add_select_related(fields)
1009         else:
1010             obj.query.select_related = True
1011         return obj
1012 
1013     def prefetch_related(self, *lookups):
1014         """
1015         Return a new QuerySet instance that will prefetch the specified
1016         Many-To-One and Many-To-Many related objects when the QuerySet is
1017         evaluated.
1018 
1019         When prefetch_related() is called more than once, append to the list of
1020         prefetch lookups. If prefetch_related(None) is called, clear the list.
1021         """
1022         self._not_support_combined_queries('prefetch_related')
1023         clone = self._chain()
1024         if lookups == (None,):
1025             clone._prefetch_related_lookups = ()
1026         else:
1027             for lookup in lookups:
1028                 if isinstance(lookup, Prefetch):
1029                     lookup = lookup.prefetch_to
1030                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1031                 if lookup in self.query._filtered_relations:
1032                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1033             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1034         return clone
1035 
1036     def annotate(self, *args, **kwargs):
1037         """
1038         Return a query set in which the returned objects have been annotated
1039         with extra data or aggregations.
1040         """
1041         self._not_support_combined_queries('annotate')
1042         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1043         annotations = {}
1044         for arg in args:
1045             # The default_alias property may raise a TypeError.
1046             try:
1047                 if arg.default_alias in kwargs:
1048                     raise ValueError("The named annotation '%s' conflicts with the "
1049                                      "default name for another annotation."
1050                                      % arg.default_alias)
1051             except TypeError:
1052                 raise TypeError("Complex annotations require an alias")
1053             annotations[arg.default_alias] = arg
1054         annotations.update(kwargs)
1055 
1056         clone = self._chain()
1057         names = self._fields
1058         if names is None:
1059             names = set(chain.from_iterable(
1060                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1061                 for field in self.model._meta.get_fields()
1062             ))
1063 
1064         for alias, annotation in annotations.items():
1065             if alias in names:
1066                 raise ValueError("The annotation '%s' conflicts with a field on "
1067                                  "the model." % alias)
1068             if isinstance(annotation, FilteredRelation):
1069                 clone.query.add_filtered_relation(annotation, alias)
1070             else:
1071                 clone.query.add_annotation(annotation, alias, is_summary=False)
1072 
1073         for alias, annotation in clone.query.annotations.items():
1074             if alias in annotations and annotation.contains_aggregate:
1075                 if clone._fields is None:
1076                     clone.query.group_by = True
1077                 else:
1078                     clone.query.set_group_by()
1079                 break
1080 
1081         return clone
1082 
1083     def order_by(self, *field_names):
1084         """Return a new QuerySet instance with the ordering changed."""
1085         assert not self.query.is_sliced, \
1086             "Cannot reorder a query once a slice has been taken."
1087         obj = self._chain()
1088         obj.query.clear_ordering(force_empty=False)
1089         obj.query.add_ordering(*field_names)
1090         return obj
1091 
1092     def distinct(self, *field_names):
1093         """
1094         Return a new QuerySet instance that will select only distinct results.
1095         """
1096         assert not self.query.is_sliced, \
1097             "Cannot create distinct fields once a slice has been taken."
1098         obj = self._chain()
1099         obj.query.add_distinct_fields(*field_names)
1100         return obj
1101 
1102     def extra(self, select=None, where=None, params=None, tables=None,
1103               order_by=None, select_params=None):
1104         """Add extra SQL fragments to the query."""
1105         self._not_support_combined_queries('extra')
1106         assert not self.query.is_sliced, \
1107             "Cannot change a query once a slice has been taken"
1108         clone = self._chain()
1109         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1110         return clone
1111 
1112     def reverse(self):
1113         """Reverse the ordering of the QuerySet."""
1114         if self.query.is_sliced:
1115             raise TypeError('Cannot reverse a query once a slice has been taken.')
1116         clone = self._chain()
1117         clone.query.standard_ordering = not clone.query.standard_ordering
1118         return clone
1119 
1120     def defer(self, *fields):
1121         """
1122         Defer the loading of data for certain fields until they are accessed.
1123         Add the set of deferred fields to any existing set of deferred fields.
1124         The only exception to this is if None is passed in as the only
1125         parameter, in which case removal all deferrals.
1126         """
1127         self._not_support_combined_queries('defer')
1128         if self._fields is not None:
1129             raise TypeError("Cannot call defer() after .values() or .values_list()")
1130         clone = self._chain()
1131         if fields == (None,):
1132             clone.query.clear_deferred_loading()
1133         else:
1134             clone.query.add_deferred_loading(fields)
1135         return clone
1136 
1137     def only(self, *fields):
1138         """
1139         Essentially, the opposite of defer(). Only the fields passed into this
1140         method and that are not already specified as deferred are loaded
1141         immediately when the queryset is evaluated.
1142         """
1143         self._not_support_combined_queries('only')
1144         if self._fields is not None:
1145             raise TypeError("Cannot call only() after .values() or .values_list()")
1146         if fields == (None,):
1147             # Can only pass None to defer(), not only(), as the rest option.
1148             # That won't stop people trying to do this, so let's be explicit.
1149             raise TypeError("Cannot pass None as an argument to only().")
1150         for field in fields:
1151             field = field.split(LOOKUP_SEP, 1)[0]
1152             if field in self.query._filtered_relations:
1153                 raise ValueError('only() is not supported with FilteredRelation.')
1154         clone = self._chain()
1155         clone.query.add_immediate_loading(fields)
1156         return clone
1157 
1158     def using(self, alias):
1159         """Select which database this QuerySet should execute against."""
1160         clone = self._chain()
1161         clone._db = alias
1162         return clone
1163 
1164     ###################################
1165     # PUBLIC INTROSPECTION ATTRIBUTES #
1166     ###################################
1167 
1168     @property
1169     def ordered(self):
1170         """
1171         Return True if the QuerySet is ordered -- i.e. has an order_by()
1172         clause or a default ordering on the model (or is empty).
1173         """
1174         if isinstance(self, EmptyQuerySet):
1175             return True
1176         if self.query.extra_order_by or self.query.order_by:
1177             return True
1178         elif self.query.default_ordering and self.query.get_meta().ordering:
1179             return True
1180         else:
1181             return False
1182 
1183     @property
1184     def db(self):
1185         """Return the database used if this query is executed now."""
1186         if self._for_write:
1187             return self._db or router.db_for_write(self.model, **self._hints)
1188         return self._db or router.db_for_read(self.model, **self._hints)
1189 
1190     ###################
1191     # PRIVATE METHODS #
1192     ###################
1193 
1194     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1195         """
1196         Insert a new record for the given model. This provides an interface to
1197         the InsertQuery class and is how Model.save() is implemented.
1198         """
1199         self._for_write = True
1200         if using is None:
1201             using = self.db
1202         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1203         query.insert_values(fields, objs, raw=raw)
1204         return query.get_compiler(using=using).execute_sql(returning_fields)
1205     _insert.alters_data = True
1206     _insert.queryset_only = False
1207 
1208     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1209         """
1210         Helper method for bulk_create() to insert objs one batch at a time.
1211         """
1212         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1213             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1214         ops = connections[self.db].ops
1215         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1216         inserted_rows = []
1217         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1218         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1219             if bulk_return and not ignore_conflicts:
1220                 inserted_columns = self._insert(
1221                     item, fields=fields, using=self.db,
1222                     returning_fields=self.model._meta.db_returning_fields,
1223                     ignore_conflicts=ignore_conflicts,
1224                 )
1225                 if isinstance(inserted_columns, list):
1226                     inserted_rows.extend(inserted_columns)
1227                 else:
1228                     inserted_rows.append(inserted_columns)
1229             else:
1230                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1231         return inserted_rows
1232 
1233     def _chain(self, **kwargs):
1234         """
1235         Return a copy of the current QuerySet that's ready for another
1236         operation.
1237         """
1238         obj = self._clone()
1239         if obj._sticky_filter:
1240             obj.query.filter_is_sticky = True
1241             obj._sticky_filter = False
1242         obj.__dict__.update(kwargs)
1243         return obj
1244 
1245     def _clone(self):
1246         """
1247         Return a copy of the current QuerySet. A lightweight alternative
1248         to deepcopy().
1249         """
1250         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1251         c._sticky_filter = self._sticky_filter
1252         c._for_write = self._for_write
1253         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1254         c._known_related_objects = self._known_related_objects
1255         c._iterable_class = self._iterable_class
1256         c._fields = self._fields
1257         return c
1258 
1259     def _fetch_all(self):
1260         if self._result_cache is None:
1261             self._result_cache = list(self._iterable_class(self))
1262         if self._prefetch_related_lookups and not self._prefetch_done:
1263             self._prefetch_related_objects()
1264 
1265     def _next_is_sticky(self):
1266         """
1267         Indicate that the next filter call and the one following that should
1268         be treated as a single filter. This is only important when it comes to
1269         determining when to reuse tables for many-to-many filters. Required so
1270         that we can filter naturally on the results of related managers.
1271 
1272         This doesn't return a clone of the current QuerySet (it returns
1273         "self"). The method is only used internally and should be immediately
1274         followed by a filter() that does create a clone.
1275         """
1276         self._sticky_filter = True
1277         return self
1278 
1279     def _merge_sanity_check(self, other):
1280         """Check that two QuerySet classes may be merged."""
1281         if self._fields is not None and (
1282                 set(self.query.values_select) != set(other.query.values_select) or
1283                 set(self.query.extra_select) != set(other.query.extra_select) or
1284                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1285             raise TypeError(
1286                 "Merging '%s' classes must involve the same values in each case."
1287                 % self.__class__.__name__
1288             )
1289 
1290     def _merge_known_related_objects(self, other):
1291         """
1292         Keep track of all known related objects from either QuerySet instance.
1293         """
1294         for field, objects in other._known_related_objects.items():
1295             self._known_related_objects.setdefault(field, {}).update(objects)
1296 
1297     def resolve_expression(self, *args, **kwargs):
1298         if self._fields and len(self._fields) > 1:
1299             # values() queryset can only be used as nested queries
1300             # if they are set up to select only a single field.
1301             raise TypeError('Cannot use multi-field values as a filter value.')
1302         query = self.query.resolve_expression(*args, **kwargs)
1303         query._db = self._db
1304         return query
1305     resolve_expression.queryset_only = True
1306 
1307     def _add_hints(self, **hints):
1308         """
1309         Update hinting information for use by routers. Add new key/values or
1310         overwrite existing key/values.
1311         """
1312         self._hints.update(hints)
1313 
1314     def _has_filters(self):
1315         """
1316         Check if this QuerySet has any filtering going on. This isn't
1317         equivalent with checking if all objects are present in results, for
1318         example, qs[1:]._has_filters() -> False.
1319         """
1320         return self.query.has_filters()
1321 
1322     @staticmethod
1323     def _validate_values_are_expressions(values, method_name):
1324         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1325         if invalid_args:
1326             raise TypeError(
1327                 'QuerySet.%s() received non-expression(s): %s.' % (
1328                     method_name,
1329                     ', '.join(invalid_args),
1330                 )
1331             )
1332 
1333     def _not_support_combined_queries(self, operation_name):
1334         if self.query.combinator:
1335             raise NotSupportedError(
1336                 'Calling QuerySet.%s() after %s() is not supported.'
1337                 % (operation_name, self.query.combinator)
1338             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 04:13:24.423 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 04:13:24.423 | INFO     | Search-only: skipping patch selection
2025-10-15 04:13:24.561 | INFO     | Task django__django-11810 completed successfully.
2025-10-15 04:13:24.562 | INFO     | No patch generated. You can try running ACR again.
