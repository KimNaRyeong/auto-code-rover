[
    {
        "rel_file_path": "django/core/files/uploadhandler.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.0/django/core/files/uploadhandler.py",
        "start": 138,
        "end": 164,
        "class_name": "TemporaryFileUploadHandler",
        "method_name": null,
        "code": "138 class TemporaryFileUploadHandler(FileUploadHandler):\n139     \"\"\"\n140     Upload handler that streams data into a temporary file.\n141     \"\"\"\n142     def new_file(self, *args, **kwargs):\n143         \"\"\"\n144         Create the file object to append to as data is coming in.\n145         \"\"\"\n146         super().new_file(*args, **kwargs)\n147         self.file = TemporaryUploadedFile(self.file_name, self.content_type, 0, self.charset, self.content_type_extra)\n148 \n149     def receive_data_chunk(self, raw_data, start):\n150         self.file.write(raw_data)\n151 \n152     def file_complete(self, file_size):\n153         self.file.seek(0)\n154         self.file.size = file_size\n155         return self.file\n156 \n157     def upload_interrupted(self):\n158         if hasattr(self, 'file'):\n159             temp_location = self.file.temporary_file_path()\n160             try:\n161                 self.file.close()\n162                 os.remove(temp_location)\n163             except FileNotFoundError:\n164                 pass\n",
        "intended_behavior": "Ensure that exceptions raised due to file storage issues are correctly identified and reported, instead of being misinterpreted as CSRF errors."
    },
    {
        "rel_file_path": "django/http/multipartparser.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.0/django/http/multipartparser.py",
        "start": 1,
        "end": 715,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 Multi-part parsing for file uploads.\n3 \n4 Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\n5 file upload handlers for processing.\n6 \"\"\"\n7 import base64\n8 import binascii\n9 import cgi\n10 import collections\n11 import html\n12 from urllib.parse import unquote\n13 \n14 from django.conf import settings\n15 from django.core.exceptions import (\n16     RequestDataTooBig, SuspiciousMultipartForm, TooManyFieldsSent,\n17 )\n18 from django.core.files.uploadhandler import (\n19     SkipFile, StopFutureHandlers, StopUpload,\n20 )\n21 from django.utils.datastructures import MultiValueDict\n22 from django.utils.encoding import force_str\n23 \n24 __all__ = ('MultiPartParser', 'MultiPartParserError', 'InputStreamExhausted')\n25 \n26 \n27 class MultiPartParserError(Exception):\n28     pass\n29 \n30 \n31 class InputStreamExhausted(Exception):\n32     \"\"\"\n33     No more reads are allowed from this device.\n34     \"\"\"\n35     pass\n36 \n37 \n38 RAW = \"raw\"\n39 FILE = \"file\"\n40 FIELD = \"field\"\n41 \n42 \n43 class MultiPartParser:\n44     \"\"\"\n45     A rfc2388 multipart/form-data parser.\n46 \n47     ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n48     and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n49     \"\"\"\n50     def __init__(self, META, input_data, upload_handlers, encoding=None):\n51         \"\"\"\n52         Initialize the MultiPartParser object.\n53 \n54         :META:\n55             The standard ``META`` dictionary in Django request objects.\n56         :input_data:\n57             The raw post data, as a file-like object.\n58         :upload_handlers:\n59             A list of UploadHandler instances that perform operations on the\n60             uploaded data.\n61         :encoding:\n62             The encoding with which to treat the incoming data.\n63         \"\"\"\n64         # Content-Type should contain multipart and the boundary information.\n65         content_type = META.get('CONTENT_TYPE', '')\n66         if not content_type.startswith('multipart/'):\n67             raise MultiPartParserError('Invalid Content-Type: %s' % content_type)\n68 \n69         # Parse the header to get the boundary to split the parts.\n70         try:\n71             ctypes, opts = parse_header(content_type.encode('ascii'))\n72         except UnicodeEncodeError:\n73             raise MultiPartParserError('Invalid non-ASCII Content-Type in multipart: %s' % force_str(content_type))\n74         boundary = opts.get('boundary')\n75         if not boundary or not cgi.valid_boundary(boundary):\n76             raise MultiPartParserError('Invalid boundary in multipart: %s' % force_str(boundary))\n77 \n78         # Content-Length should contain the length of the body we are about\n79         # to receive.\n80         try:\n81             content_length = int(META.get('CONTENT_LENGTH', 0))\n82         except (ValueError, TypeError):\n83             content_length = 0\n84 \n85         if content_length < 0:\n86             # This means we shouldn't continue...raise an error.\n87             raise MultiPartParserError(\"Invalid content length: %r\" % content_length)\n88 \n89         if isinstance(boundary, str):\n90             boundary = boundary.encode('ascii')\n91         self._boundary = boundary\n92         self._input_data = input_data\n93 \n94         # For compatibility with low-level network APIs (with 32-bit integers),\n95         # the chunk size should be < 2^31, but still divisible by 4.\n96         possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]\n97         self._chunk_size = min([2 ** 31 - 4] + possible_sizes)\n98 \n99         self._meta = META\n100         self._encoding = encoding or settings.DEFAULT_CHARSET\n101         self._content_length = content_length\n102         self._upload_handlers = upload_handlers\n103 \n104     def parse(self):\n105         \"\"\"\n106         Parse the POST data and break it into a FILES MultiValueDict and a POST\n107         MultiValueDict.\n108 \n109         Return a tuple containing the POST and FILES dictionary, respectively.\n110         \"\"\"\n111         from django.http import QueryDict\n112 \n113         encoding = self._encoding\n114         handlers = self._upload_handlers\n115 \n116         # HTTP spec says that Content-Length >= 0 is valid\n117         # handling content-length == 0 before continuing\n118         if self._content_length == 0:\n119             return QueryDict(encoding=self._encoding), MultiValueDict()\n120 \n121         # See if any of the handlers take care of the parsing.\n122         # This allows overriding everything if need be.\n123         for handler in handlers:\n124             result = handler.handle_raw_input(\n125                 self._input_data,\n126                 self._meta,\n127                 self._content_length,\n128                 self._boundary,\n129                 encoding,\n130             )\n131             # Check to see if it was handled\n132             if result is not None:\n133                 return result[0], result[1]\n134 \n135         # Create the data structures to be used later.\n136         self._post = QueryDict(mutable=True)\n137         self._files = MultiValueDict()\n138 \n139         # Instantiate the parser and stream:\n140         stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))\n141 \n142         # Whether or not to signal a file-completion at the beginning of the loop.\n143         old_field_name = None\n144         counters = [0] * len(handlers)\n145 \n146         # Number of bytes that have been read.\n147         num_bytes_read = 0\n148         # To count the number of keys in the request.\n149         num_post_keys = 0\n150         # To limit the amount of data read from the request.\n151         read_size = None\n152         # Whether a file upload is finished.\n153         uploaded_file = True\n154 \n155         try:\n156             for item_type, meta_data, field_stream in Parser(stream, self._boundary):\n157                 if old_field_name:\n158                     # We run this at the beginning of the next loop\n159                     # since we cannot be sure a file is complete until\n160                     # we hit the next boundary/part of the multipart content.\n161                     self.handle_file_complete(old_field_name, counters)\n162                     old_field_name = None\n163                     uploaded_file = True\n164 \n165                 try:\n166                     disposition = meta_data['content-disposition'][1]\n167                     field_name = disposition['name'].strip()\n168                 except (KeyError, IndexError, AttributeError):\n169                     continue\n170 \n171                 transfer_encoding = meta_data.get('content-transfer-encoding')\n172                 if transfer_encoding is not None:\n173                     transfer_encoding = transfer_encoding[0].strip()\n174                 field_name = force_str(field_name, encoding, errors='replace')\n175 \n176                 if item_type == FIELD:\n177                     # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.\n178                     num_post_keys += 1\n179                     if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and\n180                             settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):\n181                         raise TooManyFieldsSent(\n182                             'The number of GET/POST parameters exceeded '\n183                             'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'\n184                         )\n185 \n186                     # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.\n187                     if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:\n188                         read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read\n189 \n190                     # This is a post field, we can just set it in the post\n191                     if transfer_encoding == 'base64':\n192                         raw_data = field_stream.read(size=read_size)\n193                         num_bytes_read += len(raw_data)\n194                         try:\n195                             data = base64.b64decode(raw_data)\n196                         except binascii.Error:\n197                             data = raw_data\n198                     else:\n199                         data = field_stream.read(size=read_size)\n200                         num_bytes_read += len(data)\n201 \n202                     # Add two here to make the check consistent with the\n203                     # x-www-form-urlencoded check that includes '&='.\n204                     num_bytes_read += len(field_name) + 2\n205                     if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and\n206                             num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):\n207                         raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')\n208 \n209                     self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))\n210                 elif item_type == FILE:\n211                     # This is a file, use the handler...\n212                     file_name = disposition.get('filename')\n213                     if file_name:\n214                         file_name = force_str(file_name, encoding, errors='replace')\n215                         file_name = self.sanitize_file_name(file_name)\n216                     if not file_name:\n217                         continue\n218 \n219                     content_type, content_type_extra = meta_data.get('content-type', ('', {}))\n220                     content_type = content_type.strip()\n221                     charset = content_type_extra.get('charset')\n222 \n223                     try:\n224                         content_length = int(meta_data.get('content-length')[0])\n225                     except (IndexError, TypeError, ValueError):\n226                         content_length = None\n227 \n228                     counters = [0] * len(handlers)\n229                     uploaded_file = False\n230                     try:\n231                         for handler in handlers:\n232                             try:\n233                                 handler.new_file(\n234                                     field_name, file_name, content_type,\n235                                     content_length, charset, content_type_extra,\n236                                 )\n237                             except StopFutureHandlers:\n238                                 break\n239 \n240                         for chunk in field_stream:\n241                             if transfer_encoding == 'base64':\n242                                 # We only special-case base64 transfer encoding\n243                                 # We should always decode base64 chunks by multiple of 4,\n244                                 # ignoring whitespace.\n245 \n246                                 stripped_chunk = b\"\".join(chunk.split())\n247 \n248                                 remaining = len(stripped_chunk) % 4\n249                                 while remaining != 0:\n250                                     over_chunk = field_stream.read(4 - remaining)\n251                                     stripped_chunk += b\"\".join(over_chunk.split())\n252                                     remaining = len(stripped_chunk) % 4\n253 \n254                                 try:\n255                                     chunk = base64.b64decode(stripped_chunk)\n256                                 except Exception as exc:\n257                                     # Since this is only a chunk, any error is an unfixable error.\n258                                     raise MultiPartParserError(\"Could not decode base64 data.\") from exc\n259 \n260                             for i, handler in enumerate(handlers):\n261                                 chunk_length = len(chunk)\n262                                 chunk = handler.receive_data_chunk(chunk, counters[i])\n263                                 counters[i] += chunk_length\n264                                 if chunk is None:\n265                                     # Don't continue if the chunk received by\n266                                     # the handler is None.\n267                                     break\n268 \n269                     except SkipFile:\n270                         self._close_files()\n271                         # Just use up the rest of this file...\n272                         exhaust(field_stream)\n273                     else:\n274                         # Handle file upload completions on next iteration.\n275                         old_field_name = field_name\n276                 else:\n277                     # If this is neither a FIELD or a FILE, just exhaust the stream.\n278                     exhaust(stream)\n279         except StopUpload as e:\n280             self._close_files()\n281             if not e.connection_reset:\n282                 exhaust(self._input_data)\n283         else:\n284             if not uploaded_file:\n285                 for handler in handlers:\n286                     handler.upload_interrupted()\n287             # Make sure that the request data is all fed\n288             exhaust(self._input_data)\n289 \n290         # Signal that the upload has completed.\n291         # any() shortcircuits if a handler's upload_complete() returns a value.\n292         any(handler.upload_complete() for handler in handlers)\n293         self._post._mutable = False\n294         return self._post, self._files\n295 \n296     def handle_file_complete(self, old_field_name, counters):\n297         \"\"\"\n298         Handle all the signaling that takes place when a file is complete.\n299         \"\"\"\n300         for i, handler in enumerate(self._upload_handlers):\n301             file_obj = handler.file_complete(counters[i])\n302             if file_obj:\n303                 # If it returns a file object, then set the files dict.\n304                 self._files.appendlist(force_str(old_field_name, self._encoding, errors='replace'), file_obj)\n305                 break\n306 \n307     def sanitize_file_name(self, file_name):\n308         \"\"\"\n309         Sanitize the filename of an upload.\n310 \n311         Remove all possible path separators, even though that might remove more\n312         than actually required by the target system. Filenames that could\n313         potentially cause problems (current/parent dir) are also discarded.\n314 \n315         It should be noted that this function could still return a \"filepath\"\n316         like \"C:some_file.txt\" which is handled later on by the storage layer.\n317         So while this function does sanitize filenames to some extent, the\n318         resulting filename should still be considered as untrusted user input.\n319         \"\"\"\n320         file_name = html.unescape(file_name)\n321         file_name = file_name.rsplit('/')[-1]\n322         file_name = file_name.rsplit('\\\\')[-1]\n323 \n324         if file_name in {'', '.', '..'}:\n325             return None\n326         return file_name\n327 \n328     IE_sanitize = sanitize_file_name\n329 \n330     def _close_files(self):\n331         # Free up all file handles.\n332         # FIXME: this currently assumes that upload handlers store the file as 'file'\n333         # We should document that... (Maybe add handler.free_file to complement new_file)\n334         for handler in self._upload_handlers:\n335             if hasattr(handler, 'file'):\n336                 handler.file.close()\n337 \n338 \n339 class LazyStream:\n340     \"\"\"\n341     The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n342 \n343     Given a producer object (an iterator that yields bytestrings), the\n344     LazyStream object will support iteration, reading, and keeping a \"look-back\"\n345     variable in case you need to \"unget\" some bytes.\n346     \"\"\"\n347     def __init__(self, producer, length=None):\n348         \"\"\"\n349         Every LazyStream must have a producer when instantiated.\n350 \n351         A producer is an iterable that returns a string each time it\n352         is called.\n353         \"\"\"\n354         self._producer = producer\n355         self._empty = False\n356         self._leftover = b''\n357         self.length = length\n358         self.position = 0\n359         self._remaining = length\n360         self._unget_history = []\n361 \n362     def tell(self):\n363         return self.position\n364 \n365     def read(self, size=None):\n366         def parts():\n367             remaining = self._remaining if size is None else size\n368             # do the whole thing in one shot if no limit was provided.\n369             if remaining is None:\n370                 yield b''.join(self)\n371                 return\n372 \n373             # otherwise do some bookkeeping to return exactly enough\n374             # of the stream and stashing any extra content we get from\n375             # the producer\n376             while remaining != 0:\n377                 assert remaining > 0, 'remaining bytes to read should never go negative'\n378 \n379                 try:\n380                     chunk = next(self)\n381                 except StopIteration:\n382                     return\n383                 else:\n384                     emitting = chunk[:remaining]\n385                     self.unget(chunk[remaining:])\n386                     remaining -= len(emitting)\n387                     yield emitting\n388 \n389         return b''.join(parts())\n390 \n391     def __next__(self):\n392         \"\"\"\n393         Used when the exact number of bytes to read is unimportant.\n394 \n395         Return whatever chunk is conveniently returned from the iterator.\n396         Useful to avoid unnecessary bookkeeping if performance is an issue.\n397         \"\"\"\n398         if self._leftover:\n399             output = self._leftover\n400             self._leftover = b''\n401         else:\n402             output = next(self._producer)\n403             self._unget_history = []\n404         self.position += len(output)\n405         return output\n406 \n407     def close(self):\n408         \"\"\"\n409         Used to invalidate/disable this lazy stream.\n410 \n411         Replace the producer with an empty list. Any leftover bytes that have\n412         already been read will still be reported upon read() and/or next().\n413         \"\"\"\n414         self._producer = []\n415 \n416     def __iter__(self):\n417         return self\n418 \n419     def unget(self, bytes):\n420         \"\"\"\n421         Place bytes back onto the front of the lazy stream.\n422 \n423         Future calls to read() will return those bytes first. The\n424         stream position and thus tell() will be rewound.\n425         \"\"\"\n426         if not bytes:\n427             return\n428         self._update_unget_history(len(bytes))\n429         self.position -= len(bytes)\n430         self._leftover = bytes + self._leftover\n431 \n432     def _update_unget_history(self, num_bytes):\n433         \"\"\"\n434         Update the unget history as a sanity check to see if we've pushed\n435         back the same number of bytes in one chunk. If we keep ungetting the\n436         same number of bytes many times (here, 50), we're mostly likely in an\n437         infinite loop of some sort. This is usually caused by a\n438         maliciously-malformed MIME request.\n439         \"\"\"\n440         self._unget_history = [num_bytes] + self._unget_history[:49]\n441         number_equal = len([\n442             current_number for current_number in self._unget_history\n443             if current_number == num_bytes\n444         ])\n445 \n446         if number_equal > 40:\n447             raise SuspiciousMultipartForm(\n448                 \"The multipart parser got stuck, which shouldn't happen with\"\n449                 \" normal uploaded files. Check for malicious upload activity;\"\n450                 \" if there is none, report this to the Django developers.\"\n451             )\n452 \n453 \n454 class ChunkIter:\n455     \"\"\"\n456     An iterable that will yield chunks of data. Given a file-like object as the\n457     constructor, yield chunks of read operations from that object.\n458     \"\"\"\n459     def __init__(self, flo, chunk_size=64 * 1024):\n460         self.flo = flo\n461         self.chunk_size = chunk_size\n462 \n463     def __next__(self):\n464         try:\n465             data = self.flo.read(self.chunk_size)\n466         except InputStreamExhausted:\n467             raise StopIteration()\n468         if data:\n469             return data\n470         else:\n471             raise StopIteration()\n472 \n473     def __iter__(self):\n474         return self\n475 \n476 \n477 class InterBoundaryIter:\n478     \"\"\"\n479     A Producer that will iterate over boundaries.\n480     \"\"\"\n481     def __init__(self, stream, boundary):\n482         self._stream = stream\n483         self._boundary = boundary\n484 \n485     def __iter__(self):\n486         return self\n487 \n488     def __next__(self):\n489         try:\n490             return LazyStream(BoundaryIter(self._stream, self._boundary))\n491         except InputStreamExhausted:\n492             raise StopIteration()\n493 \n494 \n495 class BoundaryIter:\n496     \"\"\"\n497     A Producer that is sensitive to boundaries.\n498 \n499     Will happily yield bytes until a boundary is found. Will yield the bytes\n500     before the boundary, throw away the boundary bytes themselves, and push the\n501     post-boundary bytes back on the stream.\n502 \n503     The future calls to next() after locating the boundary will raise a\n504     StopIteration exception.\n505     \"\"\"\n506 \n507     def __init__(self, stream, boundary):\n508         self._stream = stream\n509         self._boundary = boundary\n510         self._done = False\n511         # rollback an additional six bytes because the format is like\n512         # this: CRLF<boundary>[--CRLF]\n513         self._rollback = len(boundary) + 6\n514 \n515         # Try to use mx fast string search if available. Otherwise\n516         # use Python find. Wrap the latter for consistency.\n517         unused_char = self._stream.read(1)\n518         if not unused_char:\n519             raise InputStreamExhausted()\n520         self._stream.unget(unused_char)\n521 \n522     def __iter__(self):\n523         return self\n524 \n525     def __next__(self):\n526         if self._done:\n527             raise StopIteration()\n528 \n529         stream = self._stream\n530         rollback = self._rollback\n531 \n532         bytes_read = 0\n533         chunks = []\n534         for bytes in stream:\n535             bytes_read += len(bytes)\n536             chunks.append(bytes)\n537             if bytes_read > rollback:\n538                 break\n539             if not bytes:\n540                 break\n541         else:\n542             self._done = True\n543 \n544         if not chunks:\n545             raise StopIteration()\n546 \n547         chunk = b''.join(chunks)\n548         boundary = self._find_boundary(chunk)\n549 \n550         if boundary:\n551             end, next = boundary\n552             stream.unget(chunk[next:])\n553             self._done = True\n554             return chunk[:end]\n555         else:\n556             # make sure we don't treat a partial boundary (and\n557             # its separators) as data\n558             if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):\n559                 # There's nothing left, we should just return and mark as done.\n560                 self._done = True\n561                 return chunk\n562             else:\n563                 stream.unget(chunk[-rollback:])\n564                 return chunk[:-rollback]\n565 \n566     def _find_boundary(self, data):\n567         \"\"\"\n568         Find a multipart boundary in data.\n569 \n570         Should no boundary exist in the data, return None. Otherwise, return\n571         a tuple containing the indices of the following:\n572          * the end of current encapsulation\n573          * the start of the next encapsulation\n574         \"\"\"\n575         index = data.find(self._boundary)\n576         if index < 0:\n577             return None\n578         else:\n579             end = index\n580             next = index + len(self._boundary)\n581             # backup over CRLF\n582             last = max(0, end - 1)\n583             if data[last:last + 1] == b'\\n':\n584                 end -= 1\n585             last = max(0, end - 1)\n586             if data[last:last + 1] == b'\\r':\n587                 end -= 1\n588             return end, next\n589 \n590 \n591 def exhaust(stream_or_iterable):\n592     \"\"\"Exhaust an iterator or stream.\"\"\"\n593     try:\n594         iterator = iter(stream_or_iterable)\n595     except TypeError:\n596         iterator = ChunkIter(stream_or_iterable, 16384)\n597     collections.deque(iterator, maxlen=0)  # consume iterator quickly.\n598 \n599 \n600 def parse_boundary_stream(stream, max_header_size):\n601     \"\"\"\n602     Parse one and exactly one stream that encapsulates a boundary.\n603     \"\"\"\n604     # Stream at beginning of header, look for end of header\n605     # and parse it if found. The header must fit within one\n606     # chunk.\n607     chunk = stream.read(max_header_size)\n608 \n609     # 'find' returns the top of these four bytes, so we'll\n610     # need to munch them later to prevent them from polluting\n611     # the payload.\n612     header_end = chunk.find(b'\\r\\n\\r\\n')\n613 \n614     def _parse_header(line):\n615         main_value_pair, params = parse_header(line)\n616         try:\n617             name, value = main_value_pair.split(':', 1)\n618         except ValueError:\n619             raise ValueError(\"Invalid header: %r\" % line)\n620         return name, (value, params)\n621 \n622     if header_end == -1:\n623         # we find no header, so we just mark this fact and pass on\n624         # the stream verbatim\n625         stream.unget(chunk)\n626         return (RAW, {}, stream)\n627 \n628     header = chunk[:header_end]\n629 \n630     # here we place any excess chunk back onto the stream, as\n631     # well as throwing away the CRLFCRLF bytes from above.\n632     stream.unget(chunk[header_end + 4:])\n633 \n634     TYPE = RAW\n635     outdict = {}\n636 \n637     # Eliminate blank lines\n638     for line in header.split(b'\\r\\n'):\n639         # This terminology (\"main value\" and \"dictionary of\n640         # parameters\") is from the Python docs.\n641         try:\n642             name, (value, params) = _parse_header(line)\n643         except ValueError:\n644             continue\n645 \n646         if name == 'content-disposition':\n647             TYPE = FIELD\n648             if params.get('filename'):\n649                 TYPE = FILE\n650 \n651         outdict[name] = value, params\n652 \n653     if TYPE == RAW:\n654         stream.unget(chunk)\n655 \n656     return (TYPE, outdict, stream)\n657 \n658 \n659 class Parser:\n660     def __init__(self, stream, boundary):\n661         self._stream = stream\n662         self._separator = b'--' + boundary\n663 \n664     def __iter__(self):\n665         boundarystream = InterBoundaryIter(self._stream, self._separator)\n666         for sub_stream in boundarystream:\n667             # Iterate over each part\n668             yield parse_boundary_stream(sub_stream, 1024)\n669 \n670 \n671 def parse_header(line):\n672     \"\"\"\n673     Parse the header into a key-value.\n674 \n675     Input (line): bytes, output: str for key/name, bytes for values which\n676     will be decoded later.\n677     \"\"\"\n678     plist = _parse_header_params(b';' + line)\n679     key = plist.pop(0).lower().decode('ascii')\n680     pdict = {}\n681     for p in plist:\n682         i = p.find(b'=')\n683         if i >= 0:\n684             has_encoding = False\n685             name = p[:i].strip().lower().decode('ascii')\n686             if name.endswith('*'):\n687                 # Lang/encoding embedded in the value (like \"filename*=UTF-8''file.ext\")\n688                 # http://tools.ietf.org/html/rfc2231#section-4\n689                 name = name[:-1]\n690                 if p.count(b\"'\") == 2:\n691                     has_encoding = True\n692             value = p[i + 1:].strip()\n693             if len(value) >= 2 and value[:1] == value[-1:] == b'\"':\n694                 value = value[1:-1]\n695                 value = value.replace(b'\\\\\\\\', b'\\\\').replace(b'\\\\\"', b'\"')\n696             if has_encoding:\n697                 encoding, lang, value = value.split(b\"'\")\n698                 value = unquote(value.decode(), encoding=encoding.decode())\n699             pdict[name] = value\n700     return key, pdict\n701 \n702 \n703 def _parse_header_params(s):\n704     plist = []\n705     while s[:1] == b';':\n706         s = s[1:]\n707         end = s.find(b';')\n708         while end > 0 and s.count(b'\"', 0, end) % 2:\n709             end = s.find(b';', end + 1)\n710         if end < 0:\n711             end = len(s)\n712         f = s[:end]\n713         plist.append(f.strip())\n714         s = s[end:]\n715     return plist\n",
        "intended_behavior": "Implement additional error handling for file storage-related issues during file uploads, to accurately report errors related to file storage or permissions."
    },
    {
        "rel_file_path": "django/middleware/csrf.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.0/django/middleware/csrf.py",
        "start": 1,
        "end": 461,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 Cross Site Request Forgery Middleware.\n3 \n4 This module provides a middleware that implements protection\n5 against request forgeries from other sites.\n6 \"\"\"\n7 import logging\n8 import string\n9 from collections import defaultdict\n10 from urllib.parse import urlparse\n11 \n12 from django.conf import settings\n13 from django.core.exceptions import DisallowedHost, ImproperlyConfigured\n14 from django.http.request import HttpHeaders\n15 from django.urls import get_callable\n16 from django.utils.cache import patch_vary_headers\n17 from django.utils.crypto import constant_time_compare, get_random_string\n18 from django.utils.deprecation import MiddlewareMixin\n19 from django.utils.functional import cached_property\n20 from django.utils.http import is_same_domain\n21 from django.utils.log import log_response\n22 from django.utils.regex_helper import _lazy_re_compile\n23 \n24 logger = logging.getLogger('django.security.csrf')\n25 # This matches if any character is not in CSRF_ALLOWED_CHARS.\n26 invalid_token_chars_re = _lazy_re_compile('[^a-zA-Z0-9]')\n27 \n28 REASON_BAD_ORIGIN = \"Origin checking failed - %s does not match any trusted origins.\"\n29 REASON_NO_REFERER = \"Referer checking failed - no Referer.\"\n30 REASON_BAD_REFERER = \"Referer checking failed - %s does not match any trusted origins.\"\n31 REASON_NO_CSRF_COOKIE = \"CSRF cookie not set.\"\n32 REASON_CSRF_TOKEN_MISSING = 'CSRF token missing.'\n33 REASON_MALFORMED_REFERER = \"Referer checking failed - Referer is malformed.\"\n34 REASON_INSECURE_REFERER = \"Referer checking failed - Referer is insecure while host is secure.\"\n35 # The reason strings below are for passing to InvalidTokenFormat. They are\n36 # phrases without a subject because they can be in reference to either the CSRF\n37 # cookie or non-cookie token.\n38 REASON_INCORRECT_LENGTH = 'has incorrect length'\n39 REASON_INVALID_CHARACTERS = 'has invalid characters'\n40 \n41 CSRF_SECRET_LENGTH = 32\n42 CSRF_TOKEN_LENGTH = 2 * CSRF_SECRET_LENGTH\n43 CSRF_ALLOWED_CHARS = string.ascii_letters + string.digits\n44 CSRF_SESSION_KEY = '_csrftoken'\n45 \n46 \n47 def _get_failure_view():\n48     \"\"\"Return the view to be used for CSRF rejections.\"\"\"\n49     return get_callable(settings.CSRF_FAILURE_VIEW)\n50 \n51 \n52 def _get_new_csrf_string():\n53     return get_random_string(CSRF_SECRET_LENGTH, allowed_chars=CSRF_ALLOWED_CHARS)\n54 \n55 \n56 def _mask_cipher_secret(secret):\n57     \"\"\"\n58     Given a secret (assumed to be a string of CSRF_ALLOWED_CHARS), generate a\n59     token by adding a mask and applying it to the secret.\n60     \"\"\"\n61     mask = _get_new_csrf_string()\n62     chars = CSRF_ALLOWED_CHARS\n63     pairs = zip((chars.index(x) for x in secret), (chars.index(x) for x in mask))\n64     cipher = ''.join(chars[(x + y) % len(chars)] for x, y in pairs)\n65     return mask + cipher\n66 \n67 \n68 def _unmask_cipher_token(token):\n69     \"\"\"\n70     Given a token (assumed to be a string of CSRF_ALLOWED_CHARS, of length\n71     CSRF_TOKEN_LENGTH, and that its first half is a mask), use it to decrypt\n72     the second half to produce the original secret.\n73     \"\"\"\n74     mask = token[:CSRF_SECRET_LENGTH]\n75     token = token[CSRF_SECRET_LENGTH:]\n76     chars = CSRF_ALLOWED_CHARS\n77     pairs = zip((chars.index(x) for x in token), (chars.index(x) for x in mask))\n78     return ''.join(chars[x - y] for x, y in pairs)  # Note negative values are ok\n79 \n80 \n81 def _get_new_csrf_token():\n82     return _mask_cipher_secret(_get_new_csrf_string())\n83 \n84 \n85 def get_token(request):\n86     \"\"\"\n87     Return the CSRF token required for a POST form. The token is an\n88     alphanumeric value. A new token is created if one is not already set.\n89 \n90     A side effect of calling this function is to make the csrf_protect\n91     decorator and the CsrfViewMiddleware add a CSRF cookie and a 'Vary: Cookie'\n92     header to the outgoing response.  For this reason, you may need to use this\n93     function lazily, as is done by the csrf context processor.\n94     \"\"\"\n95     if \"CSRF_COOKIE\" not in request.META:\n96         csrf_secret = _get_new_csrf_string()\n97         request.META[\"CSRF_COOKIE\"] = _mask_cipher_secret(csrf_secret)\n98     else:\n99         csrf_secret = _unmask_cipher_token(request.META[\"CSRF_COOKIE\"])\n100     request.META[\"CSRF_COOKIE_USED\"] = True\n101     return _mask_cipher_secret(csrf_secret)\n102 \n103 \n104 def rotate_token(request):\n105     \"\"\"\n106     Change the CSRF token in use for a request - should be done on login\n107     for security purposes.\n108     \"\"\"\n109     request.META.update({\n110         \"CSRF_COOKIE_USED\": True,\n111         \"CSRF_COOKIE\": _get_new_csrf_token(),\n112     })\n113     request.csrf_cookie_needs_reset = True\n114 \n115 \n116 class InvalidTokenFormat(Exception):\n117     def __init__(self, reason):\n118         self.reason = reason\n119 \n120 \n121 def _sanitize_token(token):\n122     if len(token) not in (CSRF_TOKEN_LENGTH, CSRF_SECRET_LENGTH):\n123         raise InvalidTokenFormat(REASON_INCORRECT_LENGTH)\n124     # Make sure all characters are in CSRF_ALLOWED_CHARS.\n125     if invalid_token_chars_re.search(token):\n126         raise InvalidTokenFormat(REASON_INVALID_CHARACTERS)\n127     if len(token) == CSRF_SECRET_LENGTH:\n128         # Older Django versions set cookies to values of CSRF_SECRET_LENGTH\n129         # alphanumeric characters. For backwards compatibility, accept\n130         # such values as unmasked secrets.\n131         # It's easier to mask here and be consistent later, rather than add\n132         # different code paths in the checks, although that might be a tad more\n133         # efficient.\n134         return _mask_cipher_secret(token)\n135     return token\n136 \n137 \n138 def _compare_masked_tokens(request_csrf_token, csrf_token):\n139     # Assume both arguments are sanitized -- that is, strings of\n140     # length CSRF_TOKEN_LENGTH, all CSRF_ALLOWED_CHARS.\n141     return constant_time_compare(\n142         _unmask_cipher_token(request_csrf_token),\n143         _unmask_cipher_token(csrf_token),\n144     )\n145 \n146 \n147 class RejectRequest(Exception):\n148     def __init__(self, reason):\n149         self.reason = reason\n150 \n151 \n152 class CsrfViewMiddleware(MiddlewareMixin):\n153     \"\"\"\n154     Require a present and correct csrfmiddlewaretoken for POST requests that\n155     have a CSRF cookie, and set an outgoing CSRF cookie.\n156 \n157     This middleware should be used in conjunction with the {% csrf_token %}\n158     template tag.\n159     \"\"\"\n160     @cached_property\n161     def csrf_trusted_origins_hosts(self):\n162         return [\n163             urlparse(origin).netloc.lstrip('*')\n164             for origin in settings.CSRF_TRUSTED_ORIGINS\n165         ]\n166 \n167     @cached_property\n168     def allowed_origins_exact(self):\n169         return {\n170             origin for origin in settings.CSRF_TRUSTED_ORIGINS\n171             if '*' not in origin\n172         }\n173 \n174     @cached_property\n175     def allowed_origin_subdomains(self):\n176         \"\"\"\n177         A mapping of allowed schemes to list of allowed netlocs, where all\n178         subdomains of the netloc are allowed.\n179         \"\"\"\n180         allowed_origin_subdomains = defaultdict(list)\n181         for parsed in (urlparse(origin) for origin in settings.CSRF_TRUSTED_ORIGINS if '*' in origin):\n182             allowed_origin_subdomains[parsed.scheme].append(parsed.netloc.lstrip('*'))\n183         return allowed_origin_subdomains\n184 \n185     # The _accept and _reject methods currently only exist for the sake of the\n186     # requires_csrf_token decorator.\n187     def _accept(self, request):\n188         # Avoid checking the request twice by adding a custom attribute to\n189         # request.  This will be relevant when both decorator and middleware\n190         # are used.\n191         request.csrf_processing_done = True\n192         return None\n193 \n194     def _reject(self, request, reason):\n195         response = _get_failure_view()(request, reason=reason)\n196         log_response(\n197             'Forbidden (%s): %s', reason, request.path,\n198             response=response,\n199             request=request,\n200             logger=logger,\n201         )\n202         return response\n203 \n204     def _get_token(self, request):\n205         if settings.CSRF_USE_SESSIONS:\n206             try:\n207                 return request.session.get(CSRF_SESSION_KEY)\n208             except AttributeError:\n209                 raise ImproperlyConfigured(\n210                     'CSRF_USE_SESSIONS is enabled, but request.session is not '\n211                     'set. SessionMiddleware must appear before CsrfViewMiddleware '\n212                     'in MIDDLEWARE.'\n213                 )\n214         else:\n215             try:\n216                 cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]\n217             except KeyError:\n218                 return None\n219 \n220             # This can raise InvalidTokenFormat.\n221             csrf_token = _sanitize_token(cookie_token)\n222 \n223             if csrf_token != cookie_token:\n224                 # Then the cookie token had length CSRF_SECRET_LENGTH, so flag\n225                 # to replace it with the masked version.\n226                 request.csrf_cookie_needs_reset = True\n227             return csrf_token\n228 \n229     def _set_token(self, request, response):\n230         if settings.CSRF_USE_SESSIONS:\n231             if request.session.get(CSRF_SESSION_KEY) != request.META['CSRF_COOKIE']:\n232                 request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']\n233         else:\n234             response.set_cookie(\n235                 settings.CSRF_COOKIE_NAME,\n236                 request.META['CSRF_COOKIE'],\n237                 max_age=settings.CSRF_COOKIE_AGE,\n238                 domain=settings.CSRF_COOKIE_DOMAIN,\n239                 path=settings.CSRF_COOKIE_PATH,\n240                 secure=settings.CSRF_COOKIE_SECURE,\n241                 httponly=settings.CSRF_COOKIE_HTTPONLY,\n242                 samesite=settings.CSRF_COOKIE_SAMESITE,\n243             )\n244             # Set the Vary header since content varies with the CSRF cookie.\n245             patch_vary_headers(response, ('Cookie',))\n246 \n247     def _origin_verified(self, request):\n248         request_origin = request.META['HTTP_ORIGIN']\n249         try:\n250             good_host = request.get_host()\n251         except DisallowedHost:\n252             pass\n253         else:\n254             good_origin = '%s://%s' % (\n255                 'https' if request.is_secure() else 'http',\n256                 good_host,\n257             )\n258             if request_origin == good_origin:\n259                 return True\n260         if request_origin in self.allowed_origins_exact:\n261             return True\n262         try:\n263             parsed_origin = urlparse(request_origin)\n264         except ValueError:\n265             return False\n266         request_scheme = parsed_origin.scheme\n267         request_netloc = parsed_origin.netloc\n268         return any(\n269             is_same_domain(request_netloc, host)\n270             for host in self.allowed_origin_subdomains.get(request_scheme, ())\n271         )\n272 \n273     def _check_referer(self, request):\n274         referer = request.META.get('HTTP_REFERER')\n275         if referer is None:\n276             raise RejectRequest(REASON_NO_REFERER)\n277 \n278         try:\n279             referer = urlparse(referer)\n280         except ValueError:\n281             raise RejectRequest(REASON_MALFORMED_REFERER)\n282 \n283         # Make sure we have a valid URL for Referer.\n284         if '' in (referer.scheme, referer.netloc):\n285             raise RejectRequest(REASON_MALFORMED_REFERER)\n286 \n287         # Ensure that our Referer is also secure.\n288         if referer.scheme != 'https':\n289             raise RejectRequest(REASON_INSECURE_REFERER)\n290 \n291         if any(\n292             is_same_domain(referer.netloc, host)\n293             for host in self.csrf_trusted_origins_hosts\n294         ):\n295             return\n296         # Allow matching the configured cookie domain.\n297         good_referer = (\n298             settings.SESSION_COOKIE_DOMAIN\n299             if settings.CSRF_USE_SESSIONS\n300             else settings.CSRF_COOKIE_DOMAIN\n301         )\n302         if good_referer is None:\n303             # If no cookie domain is configured, allow matching the current\n304             # host:port exactly if it's permitted by ALLOWED_HOSTS.\n305             try:\n306                 # request.get_host() includes the port.\n307                 good_referer = request.get_host()\n308             except DisallowedHost:\n309                 raise RejectRequest(REASON_BAD_REFERER % referer.geturl())\n310         else:\n311             server_port = request.get_port()\n312             if server_port not in ('443', '80'):\n313                 good_referer = '%s:%s' % (good_referer, server_port)\n314 \n315         if not is_same_domain(referer.netloc, good_referer):\n316             raise RejectRequest(REASON_BAD_REFERER % referer.geturl())\n317 \n318     def _bad_token_message(self, reason, token_source):\n319         if token_source != 'POST':\n320             # Assume it is a settings.CSRF_HEADER_NAME value.\n321             header_name = HttpHeaders.parse_header_name(token_source)\n322             token_source = f'the {header_name!r} HTTP header'\n323         return f'CSRF token from {token_source} {reason}.'\n324 \n325     def _check_token(self, request):\n326         # Access csrf_token via self._get_token() as rotate_token() may have\n327         # been called by an authentication middleware during the\n328         # process_request() phase.\n329         try:\n330             csrf_token = self._get_token(request)\n331         except InvalidTokenFormat as exc:\n332             raise RejectRequest(f'CSRF cookie {exc.reason}.')\n333 \n334         if csrf_token is None:\n335             # No CSRF cookie. For POST requests, we insist on a CSRF cookie,\n336             # and in this way we can avoid all CSRF attacks, including login\n337             # CSRF.\n338             raise RejectRequest(REASON_NO_CSRF_COOKIE)\n339 \n340         # Check non-cookie token for match.\n341         request_csrf_token = ''\n342         if request.method == 'POST':\n343             try:\n344                 request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')\n345             except OSError:\n346                 # Handle a broken connection before we've completed reading the\n347                 # POST data. process_view shouldn't raise any exceptions, so\n348                 # we'll ignore and serve the user a 403 (assuming they're still\n349                 # listening, which they probably aren't because of the error).\n350                 pass\n351 \n352         if request_csrf_token == '':\n353             # Fall back to X-CSRFToken, to make things easier for AJAX, and\n354             # possible for PUT/DELETE.\n355             try:\n356                 request_csrf_token = request.META[settings.CSRF_HEADER_NAME]\n357             except KeyError:\n358                 raise RejectRequest(REASON_CSRF_TOKEN_MISSING)\n359             token_source = settings.CSRF_HEADER_NAME\n360         else:\n361             token_source = 'POST'\n362 \n363         try:\n364             request_csrf_token = _sanitize_token(request_csrf_token)\n365         except InvalidTokenFormat as exc:\n366             reason = self._bad_token_message(exc.reason, token_source)\n367             raise RejectRequest(reason)\n368 \n369         if not _compare_masked_tokens(request_csrf_token, csrf_token):\n370             reason = self._bad_token_message('incorrect', token_source)\n371             raise RejectRequest(reason)\n372 \n373     def process_request(self, request):\n374         try:\n375             csrf_token = self._get_token(request)\n376         except InvalidTokenFormat:\n377             csrf_token = _get_new_csrf_token()\n378             request.csrf_cookie_needs_reset = True\n379 \n380         if csrf_token is not None:\n381             # Use same token next time.\n382             request.META['CSRF_COOKIE'] = csrf_token\n383 \n384     def process_view(self, request, callback, callback_args, callback_kwargs):\n385         if getattr(request, 'csrf_processing_done', False):\n386             return None\n387 \n388         # Wait until request.META[\"CSRF_COOKIE\"] has been manipulated before\n389         # bailing out, so that get_token still works\n390         if getattr(callback, 'csrf_exempt', False):\n391             return None\n392 \n393         # Assume that anything not defined as 'safe' by RFC7231 needs protection\n394         if request.method in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):\n395             return self._accept(request)\n396 \n397         if getattr(request, '_dont_enforce_csrf_checks', False):\n398             # Mechanism to turn off CSRF checks for test suite. It comes after\n399             # the creation of CSRF cookies, so that everything else continues\n400             # to work exactly the same (e.g. cookies are sent, etc.), but\n401             # before any branches that call the _reject method.\n402             return self._accept(request)\n403 \n404         # Reject the request if the Origin header doesn't match an allowed\n405         # value.\n406         if 'HTTP_ORIGIN' in request.META:\n407             if not self._origin_verified(request):\n408                 return self._reject(request, REASON_BAD_ORIGIN % request.META['HTTP_ORIGIN'])\n409         elif request.is_secure():\n410             # If the Origin header wasn't provided, reject HTTPS requests if\n411             # the Referer header doesn't match an allowed value.\n412             #\n413             # Suppose user visits http://example.com/\n414             # An active network attacker (man-in-the-middle, MITM) sends a\n415             # POST form that targets https://example.com/detonate-bomb/ and\n416             # submits it via JavaScript.\n417             #\n418             # The attacker will need to provide a CSRF cookie and token, but\n419             # that's no problem for a MITM and the session-independent secret\n420             # we're using. So the MITM can circumvent the CSRF protection. This\n421             # is true for any HTTP connection, but anyone using HTTPS expects\n422             # better! For this reason, for https://example.com/ we need\n423             # additional protection that treats http://example.com/ as\n424             # completely untrusted. Under HTTPS, Barth et al. found that the\n425             # Referer header is missing for same-domain requests in only about\n426             # 0.2% of cases or less, so we can use strict Referer checking.\n427             try:\n428                 self._check_referer(request)\n429             except RejectRequest as exc:\n430                 return self._reject(request, exc.reason)\n431 \n432         try:\n433             self._check_token(request)\n434         except RejectRequest as exc:\n435             return self._reject(request, exc.reason)\n436 \n437         return self._accept(request)\n438 \n439     def process_response(self, request, response):\n440         # Send the CSRF cookie whenever the cookie is being used (even if the\n441         # client already has it) in order to renew the expiry timer, but only\n442         # if it hasn't already been sent during this request-response cycle.\n443         # Also, send the cookie no matter what if a reset was requested.\n444         if (\n445             getattr(request, 'csrf_cookie_needs_reset', False) or (\n446                 request.META.get('CSRF_COOKIE_USED') and\n447                 not getattr(response, 'csrf_cookie_set', False)\n448             )\n449         ):\n450             self._set_token(request, response)\n451             # Update state to prevent _set_token() from being unnecessarily\n452             # called again in process_response() by other instances of\n453             # CsrfViewMiddleware. This can happen e.g. when both a decorator\n454             # and middleware are used. However, the csrf_cookie_needs_reset\n455             # attribute is still respected in subsequent calls e.g. in case\n456             # rotate_token() is called in process_response() later by custom\n457             # middleware but before those subsequent calls.\n458             response.csrf_cookie_set = True\n459             request.csrf_cookie_needs_reset = False\n460 \n461         return response\n",
        "intended_behavior": "Introduce logic to differentiate between CSRF token issues and other exceptions, particularly those caused by file storage problems, to improve error classification and reporting."
    }
]