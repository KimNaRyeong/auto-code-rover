[
    {
        "rel_file_path": "django/db/backends/sqlite3/introspection.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/backends/sqlite3/introspection.py",
        "start": 1,
        "end": 457,
        "class_name": null,
        "method_name": null,
        "code": "1 import re\n2 from collections import namedtuple\n3 \n4 import sqlparse\n5 \n6 from django.db.backends.base.introspection import (\n7     BaseDatabaseIntrospection, FieldInfo as BaseFieldInfo, TableInfo,\n8 )\n9 from django.db.models import Index\n10 from django.utils.regex_helper import _lazy_re_compile\n11 \n12 FieldInfo = namedtuple('FieldInfo', BaseFieldInfo._fields + ('pk', 'has_json_constraint'))\n13 \n14 field_size_re = _lazy_re_compile(r'^\\s*(?:var)?char\\s*\\(\\s*(\\d+)\\s*\\)\\s*$')\n15 \n16 \n17 def get_field_size(name):\n18     \"\"\" Extract the size number from a \"varchar(11)\" type name \"\"\"\n19     m = field_size_re.search(name)\n20     return int(m[1]) if m else None\n21 \n22 \n23 # This light wrapper \"fakes\" a dictionary interface, because some SQLite data\n24 # types include variables in them -- e.g. \"varchar(30)\" -- and can't be matched\n25 # as a simple dictionary lookup.\n26 class FlexibleFieldLookupDict:\n27     # Maps SQL types to Django Field types. Some of the SQL types have multiple\n28     # entries here because SQLite allows for anything and doesn't normalize the\n29     # field type; it uses whatever was given.\n30     base_data_types_reverse = {\n31         'bool': 'BooleanField',\n32         'boolean': 'BooleanField',\n33         'smallint': 'SmallIntegerField',\n34         'smallint unsigned': 'PositiveSmallIntegerField',\n35         'smallinteger': 'SmallIntegerField',\n36         'int': 'IntegerField',\n37         'integer': 'IntegerField',\n38         'bigint': 'BigIntegerField',\n39         'integer unsigned': 'PositiveIntegerField',\n40         'bigint unsigned': 'PositiveBigIntegerField',\n41         'decimal': 'DecimalField',\n42         'real': 'FloatField',\n43         'text': 'TextField',\n44         'char': 'CharField',\n45         'varchar': 'CharField',\n46         'blob': 'BinaryField',\n47         'date': 'DateField',\n48         'datetime': 'DateTimeField',\n49         'time': 'TimeField',\n50     }\n51 \n52     def __getitem__(self, key):\n53         key = key.lower().split('(', 1)[0].strip()\n54         return self.base_data_types_reverse[key]\n55 \n56 \n57 class DatabaseIntrospection(BaseDatabaseIntrospection):\n58     data_types_reverse = FlexibleFieldLookupDict()\n59 \n60     def get_field_type(self, data_type, description):\n61         field_type = super().get_field_type(data_type, description)\n62         if description.pk and field_type in {'BigIntegerField', 'IntegerField', 'SmallIntegerField'}:\n63             # No support for BigAutoField or SmallAutoField as SQLite treats\n64             # all integer primary keys as signed 64-bit integers.\n65             return 'AutoField'\n66         if description.has_json_constraint:\n67             return 'JSONField'\n68         return field_type\n69 \n70     def get_table_list(self, cursor):\n71         \"\"\"Return a list of table and view names in the current database.\"\"\"\n72         # Skip the sqlite_sequence system table used for autoincrement key\n73         # generation.\n74         cursor.execute(\"\"\"\n75             SELECT name, type FROM sqlite_master\n76             WHERE type in ('table', 'view') AND NOT name='sqlite_sequence'\n77             ORDER BY name\"\"\")\n78         return [TableInfo(row[0], row[1][0]) for row in cursor.fetchall()]\n79 \n80     def get_table_description(self, cursor, table_name):\n81         \"\"\"\n82         Return a description of the table with the DB-API cursor.description\n83         interface.\n84         \"\"\"\n85         cursor.execute('PRAGMA table_info(%s)' % self.connection.ops.quote_name(table_name))\n86         table_info = cursor.fetchall()\n87         collations = self._get_column_collations(cursor, table_name)\n88         json_columns = set()\n89         if self.connection.features.can_introspect_json_field:\n90             for line in table_info:\n91                 column = line[1]\n92                 json_constraint_sql = '%%json_valid(\"%s\")%%' % column\n93                 has_json_constraint = cursor.execute(\"\"\"\n94                     SELECT sql\n95                     FROM sqlite_master\n96                     WHERE\n97                         type = 'table' AND\n98                         name = %s AND\n99                         sql LIKE %s\n100                 \"\"\", [table_name, json_constraint_sql]).fetchone()\n101                 if has_json_constraint:\n102                     json_columns.add(column)\n103         return [\n104             FieldInfo(\n105                 name, data_type, None, get_field_size(data_type), None, None,\n106                 not notnull, default, collations.get(name), pk == 1, name in json_columns\n107             )\n108             for cid, name, data_type, notnull, default, pk in table_info\n109         ]\n110 \n111     def get_sequences(self, cursor, table_name, table_fields=()):\n112         pk_col = self.get_primary_key_column(cursor, table_name)\n113         return [{'table': table_name, 'column': pk_col}]\n114 \n115     def get_relations(self, cursor, table_name):\n116         \"\"\"\n117         Return a dictionary of {field_name: (field_name_other_table, other_table)}\n118         representing all relationships to the given table.\n119         \"\"\"\n120         # Dictionary of relations to return\n121         relations = {}\n122 \n123         # Schema for this table\n124         cursor.execute(\n125             \"SELECT sql, type FROM sqlite_master \"\n126             \"WHERE tbl_name = %s AND type IN ('table', 'view')\",\n127             [table_name]\n128         )\n129         create_sql, table_type = cursor.fetchone()\n130         if table_type == 'view':\n131             # It might be a view, then no results will be returned\n132             return relations\n133         results = create_sql[create_sql.index('(') + 1:create_sql.rindex(')')]\n134 \n135         # Walk through and look for references to other tables. SQLite doesn't\n136         # really have enforced references, but since it echoes out the SQL used\n137         # to create the table we can look for REFERENCES statements used there.\n138         for field_desc in results.split(','):\n139             field_desc = field_desc.strip()\n140             if field_desc.startswith(\"UNIQUE\"):\n141                 continue\n142 \n143             m = re.search(r'references (\\S*) ?\\([\"|]?(.*)[\"|]?\\)', field_desc, re.I)\n144             if not m:\n145                 continue\n146             table, column = [s.strip('\"') for s in m.groups()]\n147 \n148             if field_desc.startswith(\"FOREIGN KEY\"):\n149                 # Find name of the target FK field\n150                 m = re.match(r'FOREIGN KEY\\s*\\(([^\\)]*)\\).*', field_desc, re.I)\n151                 field_name = m[1].strip('\"')\n152             else:\n153                 field_name = field_desc.split()[0].strip('\"')\n154 \n155             cursor.execute(\"SELECT sql FROM sqlite_master WHERE tbl_name = %s\", [table])\n156             result = cursor.fetchall()[0]\n157             other_table_results = result[0].strip()\n158             li, ri = other_table_results.index('('), other_table_results.rindex(')')\n159             other_table_results = other_table_results[li + 1:ri]\n160 \n161             for other_desc in other_table_results.split(','):\n162                 other_desc = other_desc.strip()\n163                 if other_desc.startswith('UNIQUE'):\n164                     continue\n165 \n166                 other_name = other_desc.split(' ', 1)[0].strip('\"')\n167                 if other_name == column:\n168                     relations[field_name] = (other_name, table)\n169                     break\n170 \n171         return relations\n172 \n173     def get_key_columns(self, cursor, table_name):\n174         \"\"\"\n175         Return a list of (column_name, referenced_table_name, referenced_column_name)\n176         for all key columns in given table.\n177         \"\"\"\n178         key_columns = []\n179 \n180         # Schema for this table\n181         cursor.execute(\"SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s\", [table_name, \"table\"])\n182         results = cursor.fetchone()[0].strip()\n183         results = results[results.index('(') + 1:results.rindex(')')]\n184 \n185         # Walk through and look for references to other tables. SQLite doesn't\n186         # really have enforced references, but since it echoes out the SQL used\n187         # to create the table we can look for REFERENCES statements used there.\n188         for field_index, field_desc in enumerate(results.split(',')):\n189             field_desc = field_desc.strip()\n190             if field_desc.startswith(\"UNIQUE\"):\n191                 continue\n192 \n193             m = re.search(r'\"(.*)\".*references (.*) \\([\"|](.*)[\"|]\\)', field_desc, re.I)\n194             if not m:\n195                 continue\n196 \n197             # This will append (column_name, referenced_table_name, referenced_column_name) to key_columns\n198             key_columns.append(tuple(s.strip('\"') for s in m.groups()))\n199 \n200         return key_columns\n201 \n202     def get_primary_key_column(self, cursor, table_name):\n203         \"\"\"Return the column name of the primary key for the given table.\"\"\"\n204         cursor.execute(\n205             'PRAGMA table_info(%s)' % self.connection.ops.quote_name(table_name)\n206         )\n207         for _, name, *_, pk in cursor.fetchall():\n208             if pk:\n209                 return name\n210         return None\n211 \n212     def _get_foreign_key_constraints(self, cursor, table_name):\n213         constraints = {}\n214         cursor.execute('PRAGMA foreign_key_list(%s)' % self.connection.ops.quote_name(table_name))\n215         for row in cursor.fetchall():\n216             # Remaining on_update/on_delete/match values are of no interest.\n217             id_, _, table, from_, to = row[:5]\n218             constraints['fk_%d' % id_] = {\n219                 'columns': [from_],\n220                 'primary_key': False,\n221                 'unique': False,\n222                 'foreign_key': (table, to),\n223                 'check': False,\n224                 'index': False,\n225             }\n226         return constraints\n227 \n228     def _parse_column_or_constraint_definition(self, tokens, columns):\n229         token = None\n230         is_constraint_definition = None\n231         field_name = None\n232         constraint_name = None\n233         unique = False\n234         unique_columns = []\n235         check = False\n236         check_columns = []\n237         braces_deep = 0\n238         for token in tokens:\n239             if token.match(sqlparse.tokens.Punctuation, '('):\n240                 braces_deep += 1\n241             elif token.match(sqlparse.tokens.Punctuation, ')'):\n242                 braces_deep -= 1\n243                 if braces_deep < 0:\n244                     # End of columns and constraints for table definition.\n245                     break\n246             elif braces_deep == 0 and token.match(sqlparse.tokens.Punctuation, ','):\n247                 # End of current column or constraint definition.\n248                 break\n249             # Detect column or constraint definition by first token.\n250             if is_constraint_definition is None:\n251                 is_constraint_definition = token.match(sqlparse.tokens.Keyword, 'CONSTRAINT')\n252                 if is_constraint_definition:\n253                     continue\n254             if is_constraint_definition:\n255                 # Detect constraint name by second token.\n256                 if constraint_name is None:\n257                     if token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Keyword):\n258                         constraint_name = token.value\n259                     elif token.ttype == sqlparse.tokens.Literal.String.Symbol:\n260                         constraint_name = token.value[1:-1]\n261                 # Start constraint columns parsing after UNIQUE keyword.\n262                 if token.match(sqlparse.tokens.Keyword, 'UNIQUE'):\n263                     unique = True\n264                     unique_braces_deep = braces_deep\n265                 elif unique:\n266                     if unique_braces_deep == braces_deep:\n267                         if unique_columns:\n268                             # Stop constraint parsing.\n269                             unique = False\n270                         continue\n271                     if token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Keyword):\n272                         unique_columns.append(token.value)\n273                     elif token.ttype == sqlparse.tokens.Literal.String.Symbol:\n274                         unique_columns.append(token.value[1:-1])\n275             else:\n276                 # Detect field name by first token.\n277                 if field_name is None:\n278                     if token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Keyword):\n279                         field_name = token.value\n280                     elif token.ttype == sqlparse.tokens.Literal.String.Symbol:\n281                         field_name = token.value[1:-1]\n282                 if token.match(sqlparse.tokens.Keyword, 'UNIQUE'):\n283                     unique_columns = [field_name]\n284             # Start constraint columns parsing after CHECK keyword.\n285             if token.match(sqlparse.tokens.Keyword, 'CHECK'):\n286                 check = True\n287                 check_braces_deep = braces_deep\n288             elif check:\n289                 if check_braces_deep == braces_deep:\n290                     if check_columns:\n291                         # Stop constraint parsing.\n292                         check = False\n293                     continue\n294                 if token.ttype in (sqlparse.tokens.Name, sqlparse.tokens.Keyword):\n295                     if token.value in columns:\n296                         check_columns.append(token.value)\n297                 elif token.ttype == sqlparse.tokens.Literal.String.Symbol:\n298                     if token.value[1:-1] in columns:\n299                         check_columns.append(token.value[1:-1])\n300         unique_constraint = {\n301             'unique': True,\n302             'columns': unique_columns,\n303             'primary_key': False,\n304             'foreign_key': None,\n305             'check': False,\n306             'index': False,\n307         } if unique_columns else None\n308         check_constraint = {\n309             'check': True,\n310             'columns': check_columns,\n311             'primary_key': False,\n312             'unique': False,\n313             'foreign_key': None,\n314             'index': False,\n315         } if check_columns else None\n316         return constraint_name, unique_constraint, check_constraint, token\n317 \n318     def _parse_table_constraints(self, sql, columns):\n319         # Check constraint parsing is based of SQLite syntax diagram.\n320         # https://www.sqlite.org/syntaxdiagrams.html#table-constraint\n321         statement = sqlparse.parse(sql)[0]\n322         constraints = {}\n323         unnamed_constrains_index = 0\n324         tokens = (token for token in statement.flatten() if not token.is_whitespace)\n325         # Go to columns and constraint definition\n326         for token in tokens:\n327             if token.match(sqlparse.tokens.Punctuation, '('):\n328                 break\n329         # Parse columns and constraint definition\n330         while True:\n331             constraint_name, unique, check, end_token = self._parse_column_or_constraint_definition(tokens, columns)\n332             if unique:\n333                 if constraint_name:\n334                     constraints[constraint_name] = unique\n335                 else:\n336                     unnamed_constrains_index += 1\n337                     constraints['__unnamed_constraint_%s__' % unnamed_constrains_index] = unique\n338             if check:\n339                 if constraint_name:\n340                     constraints[constraint_name] = check\n341                 else:\n342                     unnamed_constrains_index += 1\n343                     constraints['__unnamed_constraint_%s__' % unnamed_constrains_index] = check\n344             if end_token.match(sqlparse.tokens.Punctuation, ')'):\n345                 break\n346         return constraints\n347 \n348     def get_constraints(self, cursor, table_name):\n349         \"\"\"\n350         Retrieve any constraints or keys (unique, pk, fk, check, index) across\n351         one or more columns.\n352         \"\"\"\n353         constraints = {}\n354         # Find inline check constraints.\n355         try:\n356             table_schema = cursor.execute(\n357                 \"SELECT sql FROM sqlite_master WHERE type='table' and name=%s\" % (\n358                     self.connection.ops.quote_name(table_name),\n359                 )\n360             ).fetchone()[0]\n361         except TypeError:\n362             # table_name is a view.\n363             pass\n364         else:\n365             columns = {info.name for info in self.get_table_description(cursor, table_name)}\n366             constraints.update(self._parse_table_constraints(table_schema, columns))\n367 \n368         # Get the index info\n369         cursor.execute(\"PRAGMA index_list(%s)\" % self.connection.ops.quote_name(table_name))\n370         for row in cursor.fetchall():\n371             # SQLite 3.8.9+ has 5 columns, however older versions only give 3\n372             # columns. Discard last 2 columns if there.\n373             number, index, unique = row[:3]\n374             cursor.execute(\n375                 \"SELECT sql FROM sqlite_master \"\n376                 \"WHERE type='index' AND name=%s\" % self.connection.ops.quote_name(index)\n377             )\n378             # There's at most one row.\n379             sql, = cursor.fetchone() or (None,)\n380             # Inline constraints are already detected in\n381             # _parse_table_constraints(). The reasons to avoid fetching inline\n382             # constraints from `PRAGMA index_list` are:\n383             # - Inline constraints can have a different name and information\n384             #   than what `PRAGMA index_list` gives.\n385             # - Not all inline constraints may appear in `PRAGMA index_list`.\n386             if not sql:\n387                 # An inline constraint\n388                 continue\n389             # Get the index info for that index\n390             cursor.execute('PRAGMA index_info(%s)' % self.connection.ops.quote_name(index))\n391             for index_rank, column_rank, column in cursor.fetchall():\n392                 if index not in constraints:\n393                     constraints[index] = {\n394                         \"columns\": [],\n395                         \"primary_key\": False,\n396                         \"unique\": bool(unique),\n397                         \"foreign_key\": None,\n398                         \"check\": False,\n399                         \"index\": True,\n400                     }\n401                 constraints[index]['columns'].append(column)\n402             # Add type and column orders for indexes\n403             if constraints[index]['index']:\n404                 # SQLite doesn't support any index type other than b-tree\n405                 constraints[index]['type'] = Index.suffix\n406                 orders = self._get_index_columns_orders(sql)\n407                 if orders is not None:\n408                     constraints[index]['orders'] = orders\n409         # Get the PK\n410         pk_column = self.get_primary_key_column(cursor, table_name)\n411         if pk_column:\n412             # SQLite doesn't actually give a name to the PK constraint,\n413             # so we invent one. This is fine, as the SQLite backend never\n414             # deletes PK constraints by name, as you can't delete constraints\n415             # in SQLite; we remake the table with a new PK instead.\n416             constraints[\"__primary__\"] = {\n417                 \"columns\": [pk_column],\n418                 \"primary_key\": True,\n419                 \"unique\": False,  # It's not actually a unique constraint.\n420                 \"foreign_key\": None,\n421                 \"check\": False,\n422                 \"index\": False,\n423             }\n424         constraints.update(self._get_foreign_key_constraints(cursor, table_name))\n425         return constraints\n426 \n427     def _get_index_columns_orders(self, sql):\n428         tokens = sqlparse.parse(sql)[0]\n429         for token in tokens:\n430             if isinstance(token, sqlparse.sql.Parenthesis):\n431                 columns = str(token).strip('()').split(', ')\n432                 return ['DESC' if info.endswith('DESC') else 'ASC' for info in columns]\n433         return None\n434 \n435     def _get_column_collations(self, cursor, table_name):\n436         row = cursor.execute(\"\"\"\n437             SELECT sql\n438             FROM sqlite_master\n439             WHERE type = 'table' AND name = %s\n440         \"\"\", [table_name]).fetchone()\n441         if not row:\n442             return {}\n443 \n444         sql = row[0]\n445         columns = str(sqlparse.parse(sql)[0][-1]).strip('()').split(', ')\n446         collations = {}\n447         for column in columns:\n448             tokens = column[1:].split()\n449             column_name = tokens[0].strip('\"')\n450             for index, token in enumerate(tokens):\n451                 if token == 'COLLATE':\n452                     collation = tokens[index + 1]\n453                     break\n454             else:\n455                 collation = None\n456             collations[column_name] = collation\n457         return collations\n",
        "intended_behavior": "Correctly identify when a foreign key references a unique field instead of a primary key in another table, and generate a model field that accurately represents this relationship."
    },
    {
        "rel_file_path": "django/db/backends/mysql/schema.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/backends/mysql/schema.py",
        "start": 1,
        "end": 159,
        "class_name": null,
        "method_name": null,
        "code": "1 from django.db.backends.base.schema import BaseDatabaseSchemaEditor\n2 from django.db.models import NOT_PROVIDED\n3 \n4 \n5 class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):\n6 \n7     sql_rename_table = \"RENAME TABLE %(old_table)s TO %(new_table)s\"\n8 \n9     sql_alter_column_null = \"MODIFY %(column)s %(type)s NULL\"\n10     sql_alter_column_not_null = \"MODIFY %(column)s %(type)s NOT NULL\"\n11     sql_alter_column_type = \"MODIFY %(column)s %(type)s\"\n12     sql_alter_column_collate = \"MODIFY %(column)s %(type)s%(collation)s\"\n13     sql_alter_column_no_default_null = 'ALTER COLUMN %(column)s SET DEFAULT NULL'\n14 \n15     # No 'CASCADE' which works as a no-op in MySQL but is undocumented\n16     sql_delete_column = \"ALTER TABLE %(table)s DROP COLUMN %(column)s\"\n17 \n18     sql_delete_unique = \"ALTER TABLE %(table)s DROP INDEX %(name)s\"\n19     sql_create_column_inline_fk = (\n20         ', ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) '\n21         'REFERENCES %(to_table)s(%(to_column)s)'\n22     )\n23     sql_delete_fk = \"ALTER TABLE %(table)s DROP FOREIGN KEY %(name)s\"\n24 \n25     sql_delete_index = \"DROP INDEX %(name)s ON %(table)s\"\n26 \n27     sql_create_pk = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)\"\n28     sql_delete_pk = \"ALTER TABLE %(table)s DROP PRIMARY KEY\"\n29 \n30     sql_create_index = 'CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(extra)s'\n31 \n32     @property\n33     def sql_delete_check(self):\n34         if self.connection.mysql_is_mariadb:\n35             # The name of the column check constraint is the same as the field\n36             # name on MariaDB. Adding IF EXISTS clause prevents migrations\n37             # crash. Constraint is removed during a \"MODIFY\" column statement.\n38             return 'ALTER TABLE %(table)s DROP CONSTRAINT IF EXISTS %(name)s'\n39         return 'ALTER TABLE %(table)s DROP CHECK %(name)s'\n40 \n41     @property\n42     def sql_rename_column(self):\n43         # MariaDB >= 10.5.2 and MySQL >= 8.0.4 support an\n44         # \"ALTER TABLE ... RENAME COLUMN\" statement.\n45         if self.connection.mysql_is_mariadb:\n46             if self.connection.mysql_version >= (10, 5, 2):\n47                 return super().sql_rename_column\n48         elif self.connection.mysql_version >= (8, 0, 4):\n49             return super().sql_rename_column\n50         return 'ALTER TABLE %(table)s CHANGE %(old_column)s %(new_column)s %(type)s'\n51 \n52     def quote_value(self, value):\n53         self.connection.ensure_connection()\n54         if isinstance(value, str):\n55             value = value.replace('%', '%%')\n56         # MySQLdb escapes to string, PyMySQL to bytes.\n57         quoted = self.connection.connection.escape(value, self.connection.connection.encoders)\n58         if isinstance(value, str) and isinstance(quoted, bytes):\n59             quoted = quoted.decode()\n60         return quoted\n61 \n62     def _is_limited_data_type(self, field):\n63         db_type = field.db_type(self.connection)\n64         return db_type is not None and db_type.lower() in self.connection._limited_data_types\n65 \n66     def skip_default(self, field):\n67         if not self._supports_limited_data_type_defaults:\n68             return self._is_limited_data_type(field)\n69         return False\n70 \n71     def skip_default_on_alter(self, field):\n72         if self._is_limited_data_type(field) and not self.connection.mysql_is_mariadb:\n73             # MySQL doesn't support defaults for BLOB and TEXT in the\n74             # ALTER COLUMN statement.\n75             return True\n76         return False\n77 \n78     @property\n79     def _supports_limited_data_type_defaults(self):\n80         # MariaDB and MySQL >= 8.0.13 support defaults for BLOB and TEXT.\n81         if self.connection.mysql_is_mariadb:\n82             return True\n83         return self.connection.mysql_version >= (8, 0, 13)\n84 \n85     def _column_default_sql(self, field):\n86         if (\n87             not self.connection.mysql_is_mariadb and\n88             self._supports_limited_data_type_defaults and\n89             self._is_limited_data_type(field)\n90         ):\n91             # MySQL supports defaults for BLOB and TEXT columns only if the\n92             # default value is written as an expression i.e. in parentheses.\n93             return '(%s)'\n94         return super()._column_default_sql(field)\n95 \n96     def add_field(self, model, field):\n97         super().add_field(model, field)\n98 \n99         # Simulate the effect of a one-off default.\n100         # field.default may be unhashable, so a set isn't used for \"in\" check.\n101         if self.skip_default(field) and field.default not in (None, NOT_PROVIDED):\n102             effective_default = self.effective_default(field)\n103             self.execute('UPDATE %(table)s SET %(column)s = %%s' % {\n104                 'table': self.quote_name(model._meta.db_table),\n105                 'column': self.quote_name(field.column),\n106             }, [effective_default])\n107 \n108     def _field_should_be_indexed(self, model, field):\n109         if not super()._field_should_be_indexed(model, field):\n110             return False\n111 \n112         storage = self.connection.introspection.get_storage_engine(\n113             self.connection.cursor(), model._meta.db_table\n114         )\n115         # No need to create an index for ForeignKey fields except if\n116         # db_constraint=False because the index from that constraint won't be\n117         # created.\n118         if (storage == \"InnoDB\" and\n119                 field.get_internal_type() == 'ForeignKey' and\n120                 field.db_constraint):\n121             return False\n122         return not self._is_limited_data_type(field)\n123 \n124     def _delete_composed_index(self, model, fields, *args):\n125         \"\"\"\n126         MySQL can remove an implicit FK index on a field when that field is\n127         covered by another index like a unique_together. \"covered\" here means\n128         that the more complex index starts like the simpler one.\n129         http://bugs.mysql.com/bug.php?id=37910 / Django ticket #24757\n130         We check here before removing the [unique|index]_together if we have to\n131         recreate a FK index.\n132         \"\"\"\n133         first_field = model._meta.get_field(fields[0])\n134         if first_field.get_internal_type() == 'ForeignKey':\n135             constraint_names = self._constraint_names(model, [first_field.column], index=True)\n136             if not constraint_names:\n137                 self.execute(\n138                     self._create_index_sql(model, fields=[first_field], suffix='')\n139                 )\n140         return super()._delete_composed_index(model, fields, *args)\n141 \n142     def _set_field_new_type_null_status(self, field, new_type):\n143         \"\"\"\n144         Keep the null property of the old field. If it has changed, it will be\n145         handled separately.\n146         \"\"\"\n147         if field.null:\n148             new_type += \" NULL\"\n149         else:\n150             new_type += \" NOT NULL\"\n151         return new_type\n152 \n153     def _alter_column_type_sql(self, model, old_field, new_field, new_type):\n154         new_type = self._set_field_new_type_null_status(old_field, new_type)\n155         return super()._alter_column_type_sql(model, old_field, new_field, new_type)\n156 \n157     def _rename_field_sql(self, table, old_field, new_field, new_type):\n158         new_type = self._set_field_new_type_null_status(old_field, new_type)\n159         return super()._rename_field_sql(table, old_field, new_field, new_type)\n",
        "intended_behavior": "Correctly identify when a foreign key references a unique field instead of a primary key in another table, and generate a model field that accurately represents this relationship."
    },
    {
        "rel_file_path": "django/db/backends/base/schema.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/backends/base/schema.py",
        "start": 1,
        "end": 1395,
        "class_name": null,
        "method_name": null,
        "code": "1 import logging\n2 from datetime import datetime\n3 \n4 from django.db.backends.ddl_references import (\n5     Columns, Expressions, ForeignKeyName, IndexName, Statement, Table,\n6 )\n7 from django.db.backends.utils import names_digest, split_identifier\n8 from django.db.models import Deferrable, Index\n9 from django.db.models.sql import Query\n10 from django.db.transaction import TransactionManagementError, atomic\n11 from django.utils import timezone\n12 \n13 logger = logging.getLogger('django.db.backends.schema')\n14 \n15 \n16 def _is_relevant_relation(relation, altered_field):\n17     \"\"\"\n18     When altering the given field, must constraints on its model from the given\n19     relation be temporarily dropped?\n20     \"\"\"\n21     field = relation.field\n22     if field.many_to_many:\n23         # M2M reverse field\n24         return False\n25     if altered_field.primary_key and field.to_fields == [None]:\n26         # Foreign key constraint on the primary key, which is being altered.\n27         return True\n28     # Is the constraint targeting the field being altered?\n29     return altered_field.name in field.to_fields\n30 \n31 \n32 def _all_related_fields(model):\n33     return model._meta._get_fields(forward=False, reverse=True, include_hidden=True)\n34 \n35 \n36 def _related_non_m2m_objects(old_field, new_field):\n37     # Filter out m2m objects from reverse relations.\n38     # Return (old_relation, new_relation) tuples.\n39     related_fields = zip(\n40         (obj for obj in _all_related_fields(old_field.model) if _is_relevant_relation(obj, old_field)),\n41         (obj for obj in _all_related_fields(new_field.model) if _is_relevant_relation(obj, new_field)),\n42     )\n43     for old_rel, new_rel in related_fields:\n44         yield old_rel, new_rel\n45         yield from _related_non_m2m_objects(\n46             old_rel.remote_field,\n47             new_rel.remote_field,\n48         )\n49 \n50 \n51 class BaseDatabaseSchemaEditor:\n52     \"\"\"\n53     This class and its subclasses are responsible for emitting schema-changing\n54     statements to the databases - model creation/removal/alteration, field\n55     renaming, index fiddling, and so on.\n56     \"\"\"\n57 \n58     # Overrideable SQL templates\n59     sql_create_table = \"CREATE TABLE %(table)s (%(definition)s)\"\n60     sql_rename_table = \"ALTER TABLE %(old_table)s RENAME TO %(new_table)s\"\n61     sql_retablespace_table = \"ALTER TABLE %(table)s SET TABLESPACE %(new_tablespace)s\"\n62     sql_delete_table = \"DROP TABLE %(table)s CASCADE\"\n63 \n64     sql_create_column = \"ALTER TABLE %(table)s ADD COLUMN %(column)s %(definition)s\"\n65     sql_alter_column = \"ALTER TABLE %(table)s %(changes)s\"\n66     sql_alter_column_type = \"ALTER COLUMN %(column)s TYPE %(type)s\"\n67     sql_alter_column_null = \"ALTER COLUMN %(column)s DROP NOT NULL\"\n68     sql_alter_column_not_null = \"ALTER COLUMN %(column)s SET NOT NULL\"\n69     sql_alter_column_default = \"ALTER COLUMN %(column)s SET DEFAULT %(default)s\"\n70     sql_alter_column_no_default = \"ALTER COLUMN %(column)s DROP DEFAULT\"\n71     sql_alter_column_no_default_null = sql_alter_column_no_default\n72     sql_alter_column_collate = \"ALTER COLUMN %(column)s TYPE %(type)s%(collation)s\"\n73     sql_delete_column = \"ALTER TABLE %(table)s DROP COLUMN %(column)s CASCADE\"\n74     sql_rename_column = \"ALTER TABLE %(table)s RENAME COLUMN %(old_column)s TO %(new_column)s\"\n75     sql_update_with_default = \"UPDATE %(table)s SET %(column)s = %(default)s WHERE %(column)s IS NULL\"\n76 \n77     sql_unique_constraint = \"UNIQUE (%(columns)s)%(deferrable)s\"\n78     sql_check_constraint = \"CHECK (%(check)s)\"\n79     sql_delete_constraint = \"ALTER TABLE %(table)s DROP CONSTRAINT %(name)s\"\n80     sql_constraint = \"CONSTRAINT %(name)s %(constraint)s\"\n81 \n82     sql_create_check = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s CHECK (%(check)s)\"\n83     sql_delete_check = sql_delete_constraint\n84 \n85     sql_create_unique = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s UNIQUE (%(columns)s)%(deferrable)s\"\n86     sql_delete_unique = sql_delete_constraint\n87 \n88     sql_create_fk = (\n89         \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s FOREIGN KEY (%(column)s) \"\n90         \"REFERENCES %(to_table)s (%(to_column)s)%(deferrable)s\"\n91     )\n92     sql_create_inline_fk = None\n93     sql_create_column_inline_fk = None\n94     sql_delete_fk = sql_delete_constraint\n95 \n96     sql_create_index = \"CREATE INDEX %(name)s ON %(table)s (%(columns)s)%(include)s%(extra)s%(condition)s\"\n97     sql_create_unique_index = \"CREATE UNIQUE INDEX %(name)s ON %(table)s (%(columns)s)%(include)s%(condition)s\"\n98     sql_delete_index = \"DROP INDEX %(name)s\"\n99 \n100     sql_create_pk = \"ALTER TABLE %(table)s ADD CONSTRAINT %(name)s PRIMARY KEY (%(columns)s)\"\n101     sql_delete_pk = sql_delete_constraint\n102 \n103     sql_delete_procedure = 'DROP PROCEDURE %(procedure)s'\n104 \n105     def __init__(self, connection, collect_sql=False, atomic=True):\n106         self.connection = connection\n107         self.collect_sql = collect_sql\n108         if self.collect_sql:\n109             self.collected_sql = []\n110         self.atomic_migration = self.connection.features.can_rollback_ddl and atomic\n111 \n112     # State-managing methods\n113 \n114     def __enter__(self):\n115         self.deferred_sql = []\n116         if self.atomic_migration:\n117             self.atomic = atomic(self.connection.alias)\n118             self.atomic.__enter__()\n119         return self\n120 \n121     def __exit__(self, exc_type, exc_value, traceback):\n122         if exc_type is None:\n123             for sql in self.deferred_sql:\n124                 self.execute(sql)\n125         if self.atomic_migration:\n126             self.atomic.__exit__(exc_type, exc_value, traceback)\n127 \n128     # Core utility functions\n129 \n130     def execute(self, sql, params=()):\n131         \"\"\"Execute the given SQL statement, with optional parameters.\"\"\"\n132         # Don't perform the transactional DDL check if SQL is being collected\n133         # as it's not going to be executed anyway.\n134         if not self.collect_sql and self.connection.in_atomic_block and not self.connection.features.can_rollback_ddl:\n135             raise TransactionManagementError(\n136                 \"Executing DDL statements while in a transaction on databases \"\n137                 \"that can't perform a rollback is prohibited.\"\n138             )\n139         # Account for non-string statement objects.\n140         sql = str(sql)\n141         # Log the command we're running, then run it\n142         logger.debug(\"%s; (params %r)\", sql, params, extra={'params': params, 'sql': sql})\n143         if self.collect_sql:\n144             ending = \"\" if sql.rstrip().endswith(\";\") else \";\"\n145             if params is not None:\n146                 self.collected_sql.append((sql % tuple(map(self.quote_value, params))) + ending)\n147             else:\n148                 self.collected_sql.append(sql + ending)\n149         else:\n150             with self.connection.cursor() as cursor:\n151                 cursor.execute(sql, params)\n152 \n153     def quote_name(self, name):\n154         return self.connection.ops.quote_name(name)\n155 \n156     def table_sql(self, model):\n157         \"\"\"Take a model and return its table definition.\"\"\"\n158         # Add any unique_togethers (always deferred, as some fields might be\n159         # created afterward, like geometry fields with some backends).\n160         for field_names in model._meta.unique_together:\n161             fields = [model._meta.get_field(field) for field in field_names]\n162             self.deferred_sql.append(self._create_unique_sql(model, fields))\n163         # Create column SQL, add FK deferreds if needed.\n164         column_sqls = []\n165         params = []\n166         for field in model._meta.local_fields:\n167             # SQL.\n168             definition, extra_params = self.column_sql(model, field)\n169             if definition is None:\n170                 continue\n171             # Check constraints can go on the column SQL here.\n172             db_params = field.db_parameters(connection=self.connection)\n173             if db_params['check']:\n174                 definition += ' ' + self.sql_check_constraint % db_params\n175             # Autoincrement SQL (for backends with inline variant).\n176             col_type_suffix = field.db_type_suffix(connection=self.connection)\n177             if col_type_suffix:\n178                 definition += ' %s' % col_type_suffix\n179             params.extend(extra_params)\n180             # FK.\n181             if field.remote_field and field.db_constraint:\n182                 to_table = field.remote_field.model._meta.db_table\n183                 to_column = field.remote_field.model._meta.get_field(field.remote_field.field_name).column\n184                 if self.sql_create_inline_fk:\n185                     definition += ' ' + self.sql_create_inline_fk % {\n186                         'to_table': self.quote_name(to_table),\n187                         'to_column': self.quote_name(to_column),\n188                     }\n189                 elif self.connection.features.supports_foreign_keys:\n190                     self.deferred_sql.append(self._create_fk_sql(model, field, '_fk_%(to_table)s_%(to_column)s'))\n191             # Add the SQL to our big list.\n192             column_sqls.append('%s %s' % (\n193                 self.quote_name(field.column),\n194                 definition,\n195             ))\n196             # Autoincrement SQL (for backends with post table definition\n197             # variant).\n198             if field.get_internal_type() in ('AutoField', 'BigAutoField', 'SmallAutoField'):\n199                 autoinc_sql = self.connection.ops.autoinc_sql(model._meta.db_table, field.column)\n200                 if autoinc_sql:\n201                     self.deferred_sql.extend(autoinc_sql)\n202         constraints = [constraint.constraint_sql(model, self) for constraint in model._meta.constraints]\n203         sql = self.sql_create_table % {\n204             'table': self.quote_name(model._meta.db_table),\n205             'definition': ', '.join(constraint for constraint in (*column_sqls, *constraints) if constraint),\n206         }\n207         if model._meta.db_tablespace:\n208             tablespace_sql = self.connection.ops.tablespace_sql(model._meta.db_tablespace)\n209             if tablespace_sql:\n210                 sql += ' ' + tablespace_sql\n211         return sql, params\n212 \n213     # Field <-> database mapping functions\n214 \n215     def _iter_column_sql(self, column_db_type, params, model, field, include_default):\n216         yield column_db_type\n217         collation = getattr(field, 'db_collation', None)\n218         if collation:\n219             yield self._collate_sql(collation)\n220         # Work out nullability.\n221         null = field.null\n222         # Include a default value, if requested.\n223         include_default = (\n224             include_default and\n225             not self.skip_default(field) and\n226             # Don't include a default value if it's a nullable field and the\n227             # default cannot be dropped in the ALTER COLUMN statement (e.g.\n228             # MySQL longtext and longblob).\n229             not (null and self.skip_default_on_alter(field))\n230         )\n231         if include_default:\n232             default_value = self.effective_default(field)\n233             if default_value is not None:\n234                 column_default = 'DEFAULT ' + self._column_default_sql(field)\n235                 if self.connection.features.requires_literal_defaults:\n236                     # Some databases can't take defaults as a parameter (Oracle).\n237                     # If this is the case, the individual schema backend should\n238                     # implement prepare_default().\n239                     yield column_default % self.prepare_default(default_value)\n240                 else:\n241                     yield column_default\n242                     params.append(default_value)\n243         # Oracle treats the empty string ('') as null, so coerce the null\n244         # option whenever '' is a possible value.\n245         if (field.empty_strings_allowed and not field.primary_key and\n246                 self.connection.features.interprets_empty_strings_as_nulls):\n247             null = True\n248         if not null:\n249             yield 'NOT NULL'\n250         elif not self.connection.features.implied_column_null:\n251             yield 'NULL'\n252         if field.primary_key:\n253             yield 'PRIMARY KEY'\n254         elif field.unique:\n255             yield 'UNIQUE'\n256         # Optionally add the tablespace if it's an implicitly indexed column.\n257         tablespace = field.db_tablespace or model._meta.db_tablespace\n258         if tablespace and self.connection.features.supports_tablespaces and field.unique:\n259             yield self.connection.ops.tablespace_sql(tablespace, inline=True)\n260 \n261     def column_sql(self, model, field, include_default=False):\n262         \"\"\"\n263         Return the column definition for a field. The field must already have\n264         had set_attributes_from_name() called.\n265         \"\"\"\n266         # Get the column's type and use that as the basis of the SQL.\n267         db_params = field.db_parameters(connection=self.connection)\n268         column_db_type = db_params['type']\n269         # Check for fields that aren't actually columns (e.g. M2M).\n270         if column_db_type is None:\n271             return None, None\n272         params = []\n273         return ' '.join(\n274             # This appends to the params being returned.\n275             self._iter_column_sql(column_db_type, params, model, field, include_default)\n276         ), params\n277 \n278     def skip_default(self, field):\n279         \"\"\"\n280         Some backends don't accept default values for certain columns types\n281         (i.e. MySQL longtext and longblob).\n282         \"\"\"\n283         return False\n284 \n285     def skip_default_on_alter(self, field):\n286         \"\"\"\n287         Some backends don't accept default values for certain columns types\n288         (i.e. MySQL longtext and longblob) in the ALTER COLUMN statement.\n289         \"\"\"\n290         return False\n291 \n292     def prepare_default(self, value):\n293         \"\"\"\n294         Only used for backends which have requires_literal_defaults feature\n295         \"\"\"\n296         raise NotImplementedError(\n297             'subclasses of BaseDatabaseSchemaEditor for backends which have '\n298             'requires_literal_defaults must provide a prepare_default() method'\n299         )\n300 \n301     def _column_default_sql(self, field):\n302         \"\"\"\n303         Return the SQL to use in a DEFAULT clause. The resulting string should\n304         contain a '%s' placeholder for a default value.\n305         \"\"\"\n306         return '%s'\n307 \n308     @staticmethod\n309     def _effective_default(field):\n310         # This method allows testing its logic without a connection.\n311         if field.has_default():\n312             default = field.get_default()\n313         elif not field.null and field.blank and field.empty_strings_allowed:\n314             if field.get_internal_type() == \"BinaryField\":\n315                 default = b''\n316             else:\n317                 default = ''\n318         elif getattr(field, 'auto_now', False) or getattr(field, 'auto_now_add', False):\n319             internal_type = field.get_internal_type()\n320             if internal_type == 'DateTimeField':\n321                 default = timezone.now()\n322             else:\n323                 default = datetime.now()\n324                 if internal_type == 'DateField':\n325                     default = default.date()\n326                 elif internal_type == 'TimeField':\n327                     default = default.time()\n328         else:\n329             default = None\n330         return default\n331 \n332     def effective_default(self, field):\n333         \"\"\"Return a field's effective database default value.\"\"\"\n334         return field.get_db_prep_save(self._effective_default(field), self.connection)\n335 \n336     def quote_value(self, value):\n337         \"\"\"\n338         Return a quoted version of the value so it's safe to use in an SQL\n339         string. This is not safe against injection from user code; it is\n340         intended only for use in making SQL scripts or preparing default values\n341         for particularly tricky backends (defaults are not user-defined, though,\n342         so this is safe).\n343         \"\"\"\n344         raise NotImplementedError()\n345 \n346     # Actions\n347 \n348     def create_model(self, model):\n349         \"\"\"\n350         Create a table and any accompanying indexes or unique constraints for\n351         the given `model`.\n352         \"\"\"\n353         sql, params = self.table_sql(model)\n354         # Prevent using [] as params, in the case a literal '%' is used in the definition\n355         self.execute(sql, params or None)\n356 \n357         # Add any field index and index_together's (deferred as SQLite _remake_table needs it)\n358         self.deferred_sql.extend(self._model_indexes_sql(model))\n359 \n360         # Make M2M tables\n361         for field in model._meta.local_many_to_many:\n362             if field.remote_field.through._meta.auto_created:\n363                 self.create_model(field.remote_field.through)\n364 \n365     def delete_model(self, model):\n366         \"\"\"Delete a model from the database.\"\"\"\n367         # Handle auto-created intermediary models\n368         for field in model._meta.local_many_to_many:\n369             if field.remote_field.through._meta.auto_created:\n370                 self.delete_model(field.remote_field.through)\n371 \n372         # Delete the table\n373         self.execute(self.sql_delete_table % {\n374             \"table\": self.quote_name(model._meta.db_table),\n375         })\n376         # Remove all deferred statements referencing the deleted table.\n377         for sql in list(self.deferred_sql):\n378             if isinstance(sql, Statement) and sql.references_table(model._meta.db_table):\n379                 self.deferred_sql.remove(sql)\n380 \n381     def add_index(self, model, index):\n382         \"\"\"Add an index on a model.\"\"\"\n383         if (\n384             index.contains_expressions and\n385             not self.connection.features.supports_expression_indexes\n386         ):\n387             return None\n388         # Index.create_sql returns interpolated SQL which makes params=None a\n389         # necessity to avoid escaping attempts on execution.\n390         self.execute(index.create_sql(model, self), params=None)\n391 \n392     def remove_index(self, model, index):\n393         \"\"\"Remove an index from a model.\"\"\"\n394         if (\n395             index.contains_expressions and\n396             not self.connection.features.supports_expression_indexes\n397         ):\n398             return None\n399         self.execute(index.remove_sql(model, self))\n400 \n401     def add_constraint(self, model, constraint):\n402         \"\"\"Add a constraint to a model.\"\"\"\n403         sql = constraint.create_sql(model, self)\n404         if sql:\n405             # Constraint.create_sql returns interpolated SQL which makes\n406             # params=None a necessity to avoid escaping attempts on execution.\n407             self.execute(sql, params=None)\n408 \n409     def remove_constraint(self, model, constraint):\n410         \"\"\"Remove a constraint from a model.\"\"\"\n411         sql = constraint.remove_sql(model, self)\n412         if sql:\n413             self.execute(sql)\n414 \n415     def alter_unique_together(self, model, old_unique_together, new_unique_together):\n416         \"\"\"\n417         Deal with a model changing its unique_together. The input\n418         unique_togethers must be doubly-nested, not the single-nested\n419         [\"foo\", \"bar\"] format.\n420         \"\"\"\n421         olds = {tuple(fields) for fields in old_unique_together}\n422         news = {tuple(fields) for fields in new_unique_together}\n423         # Deleted uniques\n424         for fields in olds.difference(news):\n425             self._delete_composed_index(model, fields, {'unique': True}, self.sql_delete_unique)\n426         # Created uniques\n427         for field_names in news.difference(olds):\n428             fields = [model._meta.get_field(field) for field in field_names]\n429             self.execute(self._create_unique_sql(model, fields))\n430 \n431     def alter_index_together(self, model, old_index_together, new_index_together):\n432         \"\"\"\n433         Deal with a model changing its index_together. The input\n434         index_togethers must be doubly-nested, not the single-nested\n435         [\"foo\", \"bar\"] format.\n436         \"\"\"\n437         olds = {tuple(fields) for fields in old_index_together}\n438         news = {tuple(fields) for fields in new_index_together}\n439         # Deleted indexes\n440         for fields in olds.difference(news):\n441             self._delete_composed_index(\n442                 model,\n443                 fields,\n444                 {'index': True, 'unique': False},\n445                 self.sql_delete_index,\n446             )\n447         # Created indexes\n448         for field_names in news.difference(olds):\n449             fields = [model._meta.get_field(field) for field in field_names]\n450             self.execute(self._create_index_sql(model, fields=fields, suffix='_idx'))\n451 \n452     def _delete_composed_index(self, model, fields, constraint_kwargs, sql):\n453         meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n454         meta_index_names = {constraint.name for constraint in model._meta.indexes}\n455         columns = [model._meta.get_field(field).column for field in fields]\n456         constraint_names = self._constraint_names(\n457             model, columns, exclude=meta_constraint_names | meta_index_names,\n458             **constraint_kwargs\n459         )\n460         if len(constraint_names) != 1:\n461             raise ValueError(\"Found wrong number (%s) of constraints for %s(%s)\" % (\n462                 len(constraint_names),\n463                 model._meta.db_table,\n464                 \", \".join(columns),\n465             ))\n466         self.execute(self._delete_constraint_sql(sql, model, constraint_names[0]))\n467 \n468     def alter_db_table(self, model, old_db_table, new_db_table):\n469         \"\"\"Rename the table a model points to.\"\"\"\n470         if (old_db_table == new_db_table or\n471             (self.connection.features.ignores_table_name_case and\n472                 old_db_table.lower() == new_db_table.lower())):\n473             return\n474         self.execute(self.sql_rename_table % {\n475             \"old_table\": self.quote_name(old_db_table),\n476             \"new_table\": self.quote_name(new_db_table),\n477         })\n478         # Rename all references to the old table name.\n479         for sql in self.deferred_sql:\n480             if isinstance(sql, Statement):\n481                 sql.rename_table_references(old_db_table, new_db_table)\n482 \n483     def alter_db_tablespace(self, model, old_db_tablespace, new_db_tablespace):\n484         \"\"\"Move a model's table between tablespaces.\"\"\"\n485         self.execute(self.sql_retablespace_table % {\n486             \"table\": self.quote_name(model._meta.db_table),\n487             \"old_tablespace\": self.quote_name(old_db_tablespace),\n488             \"new_tablespace\": self.quote_name(new_db_tablespace),\n489         })\n490 \n491     def add_field(self, model, field):\n492         \"\"\"\n493         Create a field on a model. Usually involves adding a column, but may\n494         involve adding a table instead (for M2M fields).\n495         \"\"\"\n496         # Special-case implicit M2M tables\n497         if field.many_to_many and field.remote_field.through._meta.auto_created:\n498             return self.create_model(field.remote_field.through)\n499         # Get the column's definition\n500         definition, params = self.column_sql(model, field, include_default=True)\n501         # It might not actually have a column behind it\n502         if definition is None:\n503             return\n504         # Check constraints can go on the column SQL here\n505         db_params = field.db_parameters(connection=self.connection)\n506         if db_params['check']:\n507             definition += \" \" + self.sql_check_constraint % db_params\n508         if field.remote_field and self.connection.features.supports_foreign_keys and field.db_constraint:\n509             constraint_suffix = '_fk_%(to_table)s_%(to_column)s'\n510             # Add FK constraint inline, if supported.\n511             if self.sql_create_column_inline_fk:\n512                 to_table = field.remote_field.model._meta.db_table\n513                 to_column = field.remote_field.model._meta.get_field(field.remote_field.field_name).column\n514                 namespace, _ = split_identifier(model._meta.db_table)\n515                 definition += \" \" + self.sql_create_column_inline_fk % {\n516                     'name': self._fk_constraint_name(model, field, constraint_suffix),\n517                     'namespace': '%s.' % self.quote_name(namespace) if namespace else '',\n518                     'column': self.quote_name(field.column),\n519                     'to_table': self.quote_name(to_table),\n520                     'to_column': self.quote_name(to_column),\n521                     'deferrable': self.connection.ops.deferrable_sql()\n522                 }\n523             # Otherwise, add FK constraints later.\n524             else:\n525                 self.deferred_sql.append(self._create_fk_sql(model, field, constraint_suffix))\n526         # Build the SQL and run it\n527         sql = self.sql_create_column % {\n528             \"table\": self.quote_name(model._meta.db_table),\n529             \"column\": self.quote_name(field.column),\n530             \"definition\": definition,\n531         }\n532         self.execute(sql, params)\n533         # Drop the default if we need to\n534         # (Django usually does not use in-database defaults)\n535         if not self.skip_default_on_alter(field) and self.effective_default(field) is not None:\n536             changes_sql, params = self._alter_column_default_sql(model, None, field, drop=True)\n537             sql = self.sql_alter_column % {\n538                 \"table\": self.quote_name(model._meta.db_table),\n539                 \"changes\": changes_sql,\n540             }\n541             self.execute(sql, params)\n542         # Add an index, if required\n543         self.deferred_sql.extend(self._field_indexes_sql(model, field))\n544         # Reset connection if required\n545         if self.connection.features.connection_persists_old_columns:\n546             self.connection.close()\n547 \n548     def remove_field(self, model, field):\n549         \"\"\"\n550         Remove a field from a model. Usually involves deleting a column,\n551         but for M2Ms may involve deleting a table.\n552         \"\"\"\n553         # Special-case implicit M2M tables\n554         if field.many_to_many and field.remote_field.through._meta.auto_created:\n555             return self.delete_model(field.remote_field.through)\n556         # It might not actually have a column behind it\n557         if field.db_parameters(connection=self.connection)['type'] is None:\n558             return\n559         # Drop any FK constraints, MySQL requires explicit deletion\n560         if field.remote_field:\n561             fk_names = self._constraint_names(model, [field.column], foreign_key=True)\n562             for fk_name in fk_names:\n563                 self.execute(self._delete_fk_sql(model, fk_name))\n564         # Delete the column\n565         sql = self.sql_delete_column % {\n566             \"table\": self.quote_name(model._meta.db_table),\n567             \"column\": self.quote_name(field.column),\n568         }\n569         self.execute(sql)\n570         # Reset connection if required\n571         if self.connection.features.connection_persists_old_columns:\n572             self.connection.close()\n573         # Remove all deferred statements referencing the deleted column.\n574         for sql in list(self.deferred_sql):\n575             if isinstance(sql, Statement) and sql.references_column(model._meta.db_table, field.column):\n576                 self.deferred_sql.remove(sql)\n577 \n578     def alter_field(self, model, old_field, new_field, strict=False):\n579         \"\"\"\n580         Allow a field's type, uniqueness, nullability, default, column,\n581         constraints, etc. to be modified.\n582         `old_field` is required to compute the necessary changes.\n583         If `strict` is True, raise errors if the old column does not match\n584         `old_field` precisely.\n585         \"\"\"\n586         if not self._field_should_be_altered(old_field, new_field):\n587             return\n588         # Ensure this field is even column-based\n589         old_db_params = old_field.db_parameters(connection=self.connection)\n590         old_type = old_db_params['type']\n591         new_db_params = new_field.db_parameters(connection=self.connection)\n592         new_type = new_db_params['type']\n593         if ((old_type is None and old_field.remote_field is None) or\n594                 (new_type is None and new_field.remote_field is None)):\n595             raise ValueError(\n596                 \"Cannot alter field %s into %s - they do not properly define \"\n597                 \"db_type (are you using a badly-written custom field?)\" %\n598                 (old_field, new_field),\n599             )\n600         elif old_type is None and new_type is None and (\n601                 old_field.remote_field.through and new_field.remote_field.through and\n602                 old_field.remote_field.through._meta.auto_created and\n603                 new_field.remote_field.through._meta.auto_created):\n604             return self._alter_many_to_many(model, old_field, new_field, strict)\n605         elif old_type is None and new_type is None and (\n606                 old_field.remote_field.through and new_field.remote_field.through and\n607                 not old_field.remote_field.through._meta.auto_created and\n608                 not new_field.remote_field.through._meta.auto_created):\n609             # Both sides have through models; this is a no-op.\n610             return\n611         elif old_type is None or new_type is None:\n612             raise ValueError(\n613                 \"Cannot alter field %s into %s - they are not compatible types \"\n614                 \"(you cannot alter to or from M2M fields, or add or remove \"\n615                 \"through= on M2M fields)\" % (old_field, new_field)\n616             )\n617 \n618         self._alter_field(model, old_field, new_field, old_type, new_type,\n619                           old_db_params, new_db_params, strict)\n620 \n621     def _alter_field(self, model, old_field, new_field, old_type, new_type,\n622                      old_db_params, new_db_params, strict=False):\n623         \"\"\"Perform a \"physical\" (non-ManyToMany) field update.\"\"\"\n624         # Drop any FK constraints, we'll remake them later\n625         fks_dropped = set()\n626         if (\n627             self.connection.features.supports_foreign_keys and\n628             old_field.remote_field and\n629             old_field.db_constraint\n630         ):\n631             fk_names = self._constraint_names(model, [old_field.column], foreign_key=True)\n632             if strict and len(fk_names) != 1:\n633                 raise ValueError(\"Found wrong number (%s) of foreign key constraints for %s.%s\" % (\n634                     len(fk_names),\n635                     model._meta.db_table,\n636                     old_field.column,\n637                 ))\n638             for fk_name in fk_names:\n639                 fks_dropped.add((old_field.column,))\n640                 self.execute(self._delete_fk_sql(model, fk_name))\n641         # Has unique been removed?\n642         if old_field.unique and (not new_field.unique or self._field_became_primary_key(old_field, new_field)):\n643             # Find the unique constraint for this field\n644             meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n645             constraint_names = self._constraint_names(\n646                 model, [old_field.column], unique=True, primary_key=False,\n647                 exclude=meta_constraint_names,\n648             )\n649             if strict and len(constraint_names) != 1:\n650                 raise ValueError(\"Found wrong number (%s) of unique constraints for %s.%s\" % (\n651                     len(constraint_names),\n652                     model._meta.db_table,\n653                     old_field.column,\n654                 ))\n655             for constraint_name in constraint_names:\n656                 self.execute(self._delete_unique_sql(model, constraint_name))\n657         # Drop incoming FK constraints if the field is a primary key or unique,\n658         # which might be a to_field target, and things are going to change.\n659         drop_foreign_keys = (\n660             self.connection.features.supports_foreign_keys and (\n661                 (old_field.primary_key and new_field.primary_key) or\n662                 (old_field.unique and new_field.unique)\n663             ) and old_type != new_type\n664         )\n665         if drop_foreign_keys:\n666             # '_meta.related_field' also contains M2M reverse fields, these\n667             # will be filtered out\n668             for _old_rel, new_rel in _related_non_m2m_objects(old_field, new_field):\n669                 rel_fk_names = self._constraint_names(\n670                     new_rel.related_model, [new_rel.field.column], foreign_key=True\n671                 )\n672                 for fk_name in rel_fk_names:\n673                     self.execute(self._delete_fk_sql(new_rel.related_model, fk_name))\n674         # Removed an index? (no strict check, as multiple indexes are possible)\n675         # Remove indexes if db_index switched to False or a unique constraint\n676         # will now be used in lieu of an index. The following lines from the\n677         # truth table show all True cases; the rest are False:\n678         #\n679         # old_field.db_index | old_field.unique | new_field.db_index | new_field.unique\n680         # ------------------------------------------------------------------------------\n681         # True               | False            | False              | False\n682         # True               | False            | False              | True\n683         # True               | False            | True               | True\n684         if old_field.db_index and not old_field.unique and (not new_field.db_index or new_field.unique):\n685             # Find the index for this field\n686             meta_index_names = {index.name for index in model._meta.indexes}\n687             # Retrieve only BTREE indexes since this is what's created with\n688             # db_index=True.\n689             index_names = self._constraint_names(\n690                 model, [old_field.column], index=True, type_=Index.suffix,\n691                 exclude=meta_index_names,\n692             )\n693             for index_name in index_names:\n694                 # The only way to check if an index was created with\n695                 # db_index=True or with Index(['field'], name='foo')\n696                 # is to look at its name (refs #28053).\n697                 self.execute(self._delete_index_sql(model, index_name))\n698         # Change check constraints?\n699         if old_db_params['check'] != new_db_params['check'] and old_db_params['check']:\n700             meta_constraint_names = {constraint.name for constraint in model._meta.constraints}\n701             constraint_names = self._constraint_names(\n702                 model, [old_field.column], check=True,\n703                 exclude=meta_constraint_names,\n704             )\n705             if strict and len(constraint_names) != 1:\n706                 raise ValueError(\"Found wrong number (%s) of check constraints for %s.%s\" % (\n707                     len(constraint_names),\n708                     model._meta.db_table,\n709                     old_field.column,\n710                 ))\n711             for constraint_name in constraint_names:\n712                 self.execute(self._delete_check_sql(model, constraint_name))\n713         # Have they renamed the column?\n714         if old_field.column != new_field.column:\n715             self.execute(self._rename_field_sql(model._meta.db_table, old_field, new_field, new_type))\n716             # Rename all references to the renamed column.\n717             for sql in self.deferred_sql:\n718                 if isinstance(sql, Statement):\n719                     sql.rename_column_references(model._meta.db_table, old_field.column, new_field.column)\n720         # Next, start accumulating actions to do\n721         actions = []\n722         null_actions = []\n723         post_actions = []\n724         # Collation change?\n725         old_collation = getattr(old_field, 'db_collation', None)\n726         new_collation = getattr(new_field, 'db_collation', None)\n727         if old_collation != new_collation:\n728             # Collation change handles also a type change.\n729             fragment = self._alter_column_collation_sql(model, new_field, new_type, new_collation)\n730             actions.append(fragment)\n731         # Type change?\n732         elif old_type != new_type:\n733             fragment, other_actions = self._alter_column_type_sql(model, old_field, new_field, new_type)\n734             actions.append(fragment)\n735             post_actions.extend(other_actions)\n736         # When changing a column NULL constraint to NOT NULL with a given\n737         # default value, we need to perform 4 steps:\n738         #  1. Add a default for new incoming writes\n739         #  2. Update existing NULL rows with new default\n740         #  3. Replace NULL constraint with NOT NULL\n741         #  4. Drop the default again.\n742         # Default change?\n743         needs_database_default = False\n744         if old_field.null and not new_field.null:\n745             old_default = self.effective_default(old_field)\n746             new_default = self.effective_default(new_field)\n747             if (\n748                 not self.skip_default_on_alter(new_field) and\n749                 old_default != new_default and\n750                 new_default is not None\n751             ):\n752                 needs_database_default = True\n753                 actions.append(self._alter_column_default_sql(model, old_field, new_field))\n754         # Nullability change?\n755         if old_field.null != new_field.null:\n756             fragment = self._alter_column_null_sql(model, old_field, new_field)\n757             if fragment:\n758                 null_actions.append(fragment)\n759         # Only if we have a default and there is a change from NULL to NOT NULL\n760         four_way_default_alteration = (\n761             new_field.has_default() and\n762             (old_field.null and not new_field.null)\n763         )\n764         if actions or null_actions:\n765             if not four_way_default_alteration:\n766                 # If we don't have to do a 4-way default alteration we can\n767                 # directly run a (NOT) NULL alteration\n768                 actions = actions + null_actions\n769             # Combine actions together if we can (e.g. postgres)\n770             if self.connection.features.supports_combined_alters and actions:\n771                 sql, params = tuple(zip(*actions))\n772                 actions = [(\", \".join(sql), sum(params, []))]\n773             # Apply those actions\n774             for sql, params in actions:\n775                 self.execute(\n776                     self.sql_alter_column % {\n777                         \"table\": self.quote_name(model._meta.db_table),\n778                         \"changes\": sql,\n779                     },\n780                     params,\n781                 )\n782             if four_way_default_alteration:\n783                 # Update existing rows with default value\n784                 self.execute(\n785                     self.sql_update_with_default % {\n786                         \"table\": self.quote_name(model._meta.db_table),\n787                         \"column\": self.quote_name(new_field.column),\n788                         \"default\": \"%s\",\n789                     },\n790                     [new_default],\n791                 )\n792                 # Since we didn't run a NOT NULL change before we need to do it\n793                 # now\n794                 for sql, params in null_actions:\n795                     self.execute(\n796                         self.sql_alter_column % {\n797                             \"table\": self.quote_name(model._meta.db_table),\n798                             \"changes\": sql,\n799                         },\n800                         params,\n801                     )\n802         if post_actions:\n803             for sql, params in post_actions:\n804                 self.execute(sql, params)\n805         # If primary_key changed to False, delete the primary key constraint.\n806         if old_field.primary_key and not new_field.primary_key:\n807             self._delete_primary_key(model, strict)\n808         # Added a unique?\n809         if self._unique_should_be_added(old_field, new_field):\n810             self.execute(self._create_unique_sql(model, [new_field]))\n811         # Added an index? Add an index if db_index switched to True or a unique\n812         # constraint will no longer be used in lieu of an index. The following\n813         # lines from the truth table show all True cases; the rest are False:\n814         #\n815         # old_field.db_index | old_field.unique | new_field.db_index | new_field.unique\n816         # ------------------------------------------------------------------------------\n817         # False              | False            | True               | False\n818         # False              | True             | True               | False\n819         # True               | True             | True               | False\n820         if (not old_field.db_index or old_field.unique) and new_field.db_index and not new_field.unique:\n821             self.execute(self._create_index_sql(model, fields=[new_field]))\n822         # Type alteration on primary key? Then we need to alter the column\n823         # referring to us.\n824         rels_to_update = []\n825         if drop_foreign_keys:\n826             rels_to_update.extend(_related_non_m2m_objects(old_field, new_field))\n827         # Changed to become primary key?\n828         if self._field_became_primary_key(old_field, new_field):\n829             # Make the new one\n830             self.execute(self._create_primary_key_sql(model, new_field))\n831             # Update all referencing columns\n832             rels_to_update.extend(_related_non_m2m_objects(old_field, new_field))\n833         # Handle our type alters on the other end of rels from the PK stuff above\n834         for old_rel, new_rel in rels_to_update:\n835             rel_db_params = new_rel.field.db_parameters(connection=self.connection)\n836             rel_type = rel_db_params['type']\n837             fragment, other_actions = self._alter_column_type_sql(\n838                 new_rel.related_model, old_rel.field, new_rel.field, rel_type\n839             )\n840             self.execute(\n841                 self.sql_alter_column % {\n842                     \"table\": self.quote_name(new_rel.related_model._meta.db_table),\n843                     \"changes\": fragment[0],\n844                 },\n845                 fragment[1],\n846             )\n847             for sql, params in other_actions:\n848                 self.execute(sql, params)\n849         # Does it have a foreign key?\n850         if (self.connection.features.supports_foreign_keys and new_field.remote_field and\n851                 (fks_dropped or not old_field.remote_field or not old_field.db_constraint) and\n852                 new_field.db_constraint):\n853             self.execute(self._create_fk_sql(model, new_field, \"_fk_%(to_table)s_%(to_column)s\"))\n854         # Rebuild FKs that pointed to us if we previously had to drop them\n855         if drop_foreign_keys:\n856             for _, rel in rels_to_update:\n857                 if rel.field.db_constraint:\n858                     self.execute(self._create_fk_sql(rel.related_model, rel.field, \"_fk\"))\n859         # Does it have check constraints we need to add?\n860         if old_db_params['check'] != new_db_params['check'] and new_db_params['check']:\n861             constraint_name = self._create_index_name(model._meta.db_table, [new_field.column], suffix='_check')\n862             self.execute(self._create_check_sql(model, constraint_name, new_db_params['check']))\n863         # Drop the default if we need to\n864         # (Django usually does not use in-database defaults)\n865         if needs_database_default:\n866             changes_sql, params = self._alter_column_default_sql(model, old_field, new_field, drop=True)\n867             sql = self.sql_alter_column % {\n868                 \"table\": self.quote_name(model._meta.db_table),\n869                 \"changes\": changes_sql,\n870             }\n871             self.execute(sql, params)\n872         # Reset connection if required\n873         if self.connection.features.connection_persists_old_columns:\n874             self.connection.close()\n875 \n876     def _alter_column_null_sql(self, model, old_field, new_field):\n877         \"\"\"\n878         Hook to specialize column null alteration.\n879 \n880         Return a (sql, params) fragment to set a column to null or non-null\n881         as required by new_field, or None if no changes are required.\n882         \"\"\"\n883         if (\n884             self.connection.features.interprets_empty_strings_as_nulls and\n885             new_field.empty_strings_allowed\n886         ):\n887             # The field is nullable in the database anyway, leave it alone.\n888             return\n889         else:\n890             new_db_params = new_field.db_parameters(connection=self.connection)\n891             sql = self.sql_alter_column_null if new_field.null else self.sql_alter_column_not_null\n892             return (\n893                 sql % {\n894                     'column': self.quote_name(new_field.column),\n895                     'type': new_db_params['type'],\n896                 },\n897                 [],\n898             )\n899 \n900     def _alter_column_default_sql(self, model, old_field, new_field, drop=False):\n901         \"\"\"\n902         Hook to specialize column default alteration.\n903 \n904         Return a (sql, params) fragment to add or drop (depending on the drop\n905         argument) a default to new_field's column.\n906         \"\"\"\n907         new_default = self.effective_default(new_field)\n908         default = self._column_default_sql(new_field)\n909         params = [new_default]\n910 \n911         if drop:\n912             params = []\n913         elif self.connection.features.requires_literal_defaults:\n914             # Some databases (Oracle) can't take defaults as a parameter\n915             # If this is the case, the SchemaEditor for that database should\n916             # implement prepare_default().\n917             default = self.prepare_default(new_default)\n918             params = []\n919 \n920         new_db_params = new_field.db_parameters(connection=self.connection)\n921         if drop:\n922             if new_field.null:\n923                 sql = self.sql_alter_column_no_default_null\n924             else:\n925                 sql = self.sql_alter_column_no_default\n926         else:\n927             sql = self.sql_alter_column_default\n928         return (\n929             sql % {\n930                 'column': self.quote_name(new_field.column),\n931                 'type': new_db_params['type'],\n932                 'default': default,\n933             },\n934             params,\n935         )\n936 \n937     def _alter_column_type_sql(self, model, old_field, new_field, new_type):\n938         \"\"\"\n939         Hook to specialize column type alteration for different backends,\n940         for cases when a creation type is different to an alteration type\n941         (e.g. SERIAL in PostgreSQL, PostGIS fields).\n942 \n943         Return a two-tuple of: an SQL fragment of (sql, params) to insert into\n944         an ALTER TABLE statement and a list of extra (sql, params) tuples to\n945         run once the field is altered.\n946         \"\"\"\n947         return (\n948             (\n949                 self.sql_alter_column_type % {\n950                     \"column\": self.quote_name(new_field.column),\n951                     \"type\": new_type,\n952                 },\n953                 [],\n954             ),\n955             [],\n956         )\n957 \n958     def _alter_column_collation_sql(self, model, new_field, new_type, new_collation):\n959         return (\n960             self.sql_alter_column_collate % {\n961                 'column': self.quote_name(new_field.column),\n962                 'type': new_type,\n963                 'collation': ' ' + self._collate_sql(new_collation) if new_collation else '',\n964             },\n965             [],\n966         )\n967 \n968     def _alter_many_to_many(self, model, old_field, new_field, strict):\n969         \"\"\"Alter M2Ms to repoint their to= endpoints.\"\"\"\n970         # Rename the through table\n971         if old_field.remote_field.through._meta.db_table != new_field.remote_field.through._meta.db_table:\n972             self.alter_db_table(old_field.remote_field.through, old_field.remote_field.through._meta.db_table,\n973                                 new_field.remote_field.through._meta.db_table)\n974         # Repoint the FK to the other side\n975         self.alter_field(\n976             new_field.remote_field.through,\n977             # We need the field that points to the target model, so we can tell alter_field to change it -\n978             # this is m2m_reverse_field_name() (as opposed to m2m_field_name, which points to our model)\n979             old_field.remote_field.through._meta.get_field(old_field.m2m_reverse_field_name()),\n980             new_field.remote_field.through._meta.get_field(new_field.m2m_reverse_field_name()),\n981         )\n982         self.alter_field(\n983             new_field.remote_field.through,\n984             # for self-referential models we need to alter field from the other end too\n985             old_field.remote_field.through._meta.get_field(old_field.m2m_field_name()),\n986             new_field.remote_field.through._meta.get_field(new_field.m2m_field_name()),\n987         )\n988 \n989     def _create_index_name(self, table_name, column_names, suffix=\"\"):\n990         \"\"\"\n991         Generate a unique name for an index/unique constraint.\n992 \n993         The name is divided into 3 parts: the table name, the column names,\n994         and a unique digest and suffix.\n995         \"\"\"\n996         _, table_name = split_identifier(table_name)\n997         hash_suffix_part = '%s%s' % (names_digest(table_name, *column_names, length=8), suffix)\n998         max_length = self.connection.ops.max_name_length() or 200\n999         # If everything fits into max_length, use that name.\n1000         index_name = '%s_%s_%s' % (table_name, '_'.join(column_names), hash_suffix_part)\n1001         if len(index_name) <= max_length:\n1002             return index_name\n1003         # Shorten a long suffix.\n1004         if len(hash_suffix_part) > max_length / 3:\n1005             hash_suffix_part = hash_suffix_part[:max_length // 3]\n1006         other_length = (max_length - len(hash_suffix_part)) // 2 - 1\n1007         index_name = '%s_%s_%s' % (\n1008             table_name[:other_length],\n1009             '_'.join(column_names)[:other_length],\n1010             hash_suffix_part,\n1011         )\n1012         # Prepend D if needed to prevent the name from starting with an\n1013         # underscore or a number (not permitted on Oracle).\n1014         if index_name[0] == \"_\" or index_name[0].isdigit():\n1015             index_name = \"D%s\" % index_name[:-1]\n1016         return index_name\n1017 \n1018     def _get_index_tablespace_sql(self, model, fields, db_tablespace=None):\n1019         if db_tablespace is None:\n1020             if len(fields) == 1 and fields[0].db_tablespace:\n1021                 db_tablespace = fields[0].db_tablespace\n1022             elif model._meta.db_tablespace:\n1023                 db_tablespace = model._meta.db_tablespace\n1024         if db_tablespace is not None:\n1025             return ' ' + self.connection.ops.tablespace_sql(db_tablespace)\n1026         return ''\n1027 \n1028     def _index_condition_sql(self, condition):\n1029         if condition:\n1030             return ' WHERE ' + condition\n1031         return ''\n1032 \n1033     def _index_include_sql(self, model, columns):\n1034         if not columns or not self.connection.features.supports_covering_indexes:\n1035             return ''\n1036         return Statement(\n1037             ' INCLUDE (%(columns)s)',\n1038             columns=Columns(model._meta.db_table, columns, self.quote_name),\n1039         )\n1040 \n1041     def _create_index_sql(self, model, *, fields=None, name=None, suffix='', using='',\n1042                           db_tablespace=None, col_suffixes=(), sql=None, opclasses=(),\n1043                           condition=None, include=None, expressions=None):\n1044         \"\"\"\n1045         Return the SQL statement to create the index for one or several fields\n1046         or expressions. `sql` can be specified if the syntax differs from the\n1047         standard (GIS indexes, ...).\n1048         \"\"\"\n1049         fields = fields or []\n1050         expressions = expressions or []\n1051         compiler = Query(model, alias_cols=False).get_compiler(\n1052             connection=self.connection,\n1053         )\n1054         tablespace_sql = self._get_index_tablespace_sql(model, fields, db_tablespace=db_tablespace)\n1055         columns = [field.column for field in fields]\n1056         sql_create_index = sql or self.sql_create_index\n1057         table = model._meta.db_table\n1058 \n1059         def create_index_name(*args, **kwargs):\n1060             nonlocal name\n1061             if name is None:\n1062                 name = self._create_index_name(*args, **kwargs)\n1063             return self.quote_name(name)\n1064 \n1065         return Statement(\n1066             sql_create_index,\n1067             table=Table(table, self.quote_name),\n1068             name=IndexName(table, columns, suffix, create_index_name),\n1069             using=using,\n1070             columns=(\n1071                 self._index_columns(table, columns, col_suffixes, opclasses)\n1072                 if columns\n1073                 else Expressions(table, expressions, compiler, self.quote_value)\n1074             ),\n1075             extra=tablespace_sql,\n1076             condition=self._index_condition_sql(condition),\n1077             include=self._index_include_sql(model, include),\n1078         )\n1079 \n1080     def _delete_index_sql(self, model, name, sql=None):\n1081         return Statement(\n1082             sql or self.sql_delete_index,\n1083             table=Table(model._meta.db_table, self.quote_name),\n1084             name=self.quote_name(name),\n1085         )\n1086 \n1087     def _index_columns(self, table, columns, col_suffixes, opclasses):\n1088         return Columns(table, columns, self.quote_name, col_suffixes=col_suffixes)\n1089 \n1090     def _model_indexes_sql(self, model):\n1091         \"\"\"\n1092         Return a list of all index SQL statements (field indexes,\n1093         index_together, Meta.indexes) for the specified model.\n1094         \"\"\"\n1095         if not model._meta.managed or model._meta.proxy or model._meta.swapped:\n1096             return []\n1097         output = []\n1098         for field in model._meta.local_fields:\n1099             output.extend(self._field_indexes_sql(model, field))\n1100 \n1101         for field_names in model._meta.index_together:\n1102             fields = [model._meta.get_field(field) for field in field_names]\n1103             output.append(self._create_index_sql(model, fields=fields, suffix='_idx'))\n1104 \n1105         for index in model._meta.indexes:\n1106             if (\n1107                 not index.contains_expressions or\n1108                 self.connection.features.supports_expression_indexes\n1109             ):\n1110                 output.append(index.create_sql(model, self))\n1111         return output\n1112 \n1113     def _field_indexes_sql(self, model, field):\n1114         \"\"\"\n1115         Return a list of all index SQL statements for the specified field.\n1116         \"\"\"\n1117         output = []\n1118         if self._field_should_be_indexed(model, field):\n1119             output.append(self._create_index_sql(model, fields=[field]))\n1120         return output\n1121 \n1122     def _field_should_be_altered(self, old_field, new_field):\n1123         _, old_path, old_args, old_kwargs = old_field.deconstruct()\n1124         _, new_path, new_args, new_kwargs = new_field.deconstruct()\n1125         # Don't alter when:\n1126         # - changing only a field name\n1127         # - changing an attribute that doesn't affect the schema\n1128         # - adding only a db_column and the column name is not changed\n1129         non_database_attrs = [\n1130             'blank',\n1131             'db_column',\n1132             'editable',\n1133             'error_messages',\n1134             'help_text',\n1135             'limit_choices_to',\n1136             # Database-level options are not supported, see #21961.\n1137             'on_delete',\n1138             'related_name',\n1139             'related_query_name',\n1140             'validators',\n1141             'verbose_name',\n1142         ]\n1143         for attr in non_database_attrs:\n1144             old_kwargs.pop(attr, None)\n1145             new_kwargs.pop(attr, None)\n1146         return (\n1147             self.quote_name(old_field.column) != self.quote_name(new_field.column) or\n1148             (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs)\n1149         )\n1150 \n1151     def _field_should_be_indexed(self, model, field):\n1152         return field.db_index and not field.unique\n1153 \n1154     def _field_became_primary_key(self, old_field, new_field):\n1155         return not old_field.primary_key and new_field.primary_key\n1156 \n1157     def _unique_should_be_added(self, old_field, new_field):\n1158         return (\n1159             not new_field.primary_key and\n1160             new_field.unique and\n1161             (not old_field.unique or old_field.primary_key)\n1162         )\n1163 \n1164     def _rename_field_sql(self, table, old_field, new_field, new_type):\n1165         return self.sql_rename_column % {\n1166             \"table\": self.quote_name(table),\n1167             \"old_column\": self.quote_name(old_field.column),\n1168             \"new_column\": self.quote_name(new_field.column),\n1169             \"type\": new_type,\n1170         }\n1171 \n1172     def _create_fk_sql(self, model, field, suffix):\n1173         table = Table(model._meta.db_table, self.quote_name)\n1174         name = self._fk_constraint_name(model, field, suffix)\n1175         column = Columns(model._meta.db_table, [field.column], self.quote_name)\n1176         to_table = Table(field.target_field.model._meta.db_table, self.quote_name)\n1177         to_column = Columns(field.target_field.model._meta.db_table, [field.target_field.column], self.quote_name)\n1178         deferrable = self.connection.ops.deferrable_sql()\n1179         return Statement(\n1180             self.sql_create_fk,\n1181             table=table,\n1182             name=name,\n1183             column=column,\n1184             to_table=to_table,\n1185             to_column=to_column,\n1186             deferrable=deferrable,\n1187         )\n1188 \n1189     def _fk_constraint_name(self, model, field, suffix):\n1190         def create_fk_name(*args, **kwargs):\n1191             return self.quote_name(self._create_index_name(*args, **kwargs))\n1192 \n1193         return ForeignKeyName(\n1194             model._meta.db_table,\n1195             [field.column],\n1196             split_identifier(field.target_field.model._meta.db_table)[1],\n1197             [field.target_field.column],\n1198             suffix,\n1199             create_fk_name,\n1200         )\n1201 \n1202     def _delete_fk_sql(self, model, name):\n1203         return self._delete_constraint_sql(self.sql_delete_fk, model, name)\n1204 \n1205     def _deferrable_constraint_sql(self, deferrable):\n1206         if deferrable is None:\n1207             return ''\n1208         if deferrable == Deferrable.DEFERRED:\n1209             return ' DEFERRABLE INITIALLY DEFERRED'\n1210         if deferrable == Deferrable.IMMEDIATE:\n1211             return ' DEFERRABLE INITIALLY IMMEDIATE'\n1212 \n1213     def _unique_sql(\n1214         self, model, fields, name, condition=None, deferrable=None,\n1215         include=None, opclasses=None, expressions=None,\n1216     ):\n1217         if (\n1218             deferrable and\n1219             not self.connection.features.supports_deferrable_unique_constraints\n1220         ):\n1221             return None\n1222         if condition or include or opclasses or expressions:\n1223             # Databases support conditional, covering, and functional unique\n1224             # constraints via a unique index.\n1225             sql = self._create_unique_sql(\n1226                 model,\n1227                 fields,\n1228                 name=name,\n1229                 condition=condition,\n1230                 include=include,\n1231                 opclasses=opclasses,\n1232                 expressions=expressions,\n1233             )\n1234             if sql:\n1235                 self.deferred_sql.append(sql)\n1236             return None\n1237         constraint = self.sql_unique_constraint % {\n1238             'columns': ', '.join([self.quote_name(field.column) for field in fields]),\n1239             'deferrable': self._deferrable_constraint_sql(deferrable),\n1240         }\n1241         return self.sql_constraint % {\n1242             'name': self.quote_name(name),\n1243             'constraint': constraint,\n1244         }\n1245 \n1246     def _create_unique_sql(\n1247         self, model, fields, name=None, condition=None, deferrable=None,\n1248         include=None, opclasses=None, expressions=None,\n1249     ):\n1250         if (\n1251             (\n1252                 deferrable and\n1253                 not self.connection.features.supports_deferrable_unique_constraints\n1254             ) or\n1255             (condition and not self.connection.features.supports_partial_indexes) or\n1256             (include and not self.connection.features.supports_covering_indexes) or\n1257             (expressions and not self.connection.features.supports_expression_indexes)\n1258         ):\n1259             return None\n1260 \n1261         def create_unique_name(*args, **kwargs):\n1262             return self.quote_name(self._create_index_name(*args, **kwargs))\n1263 \n1264         compiler = Query(model, alias_cols=False).get_compiler(connection=self.connection)\n1265         table = model._meta.db_table\n1266         columns = [field.column for field in fields]\n1267         if name is None:\n1268             name = IndexName(table, columns, '_uniq', create_unique_name)\n1269         else:\n1270             name = self.quote_name(name)\n1271         if condition or include or opclasses or expressions:\n1272             sql = self.sql_create_unique_index\n1273         else:\n1274             sql = self.sql_create_unique\n1275         if columns:\n1276             columns = self._index_columns(table, columns, col_suffixes=(), opclasses=opclasses)\n1277         else:\n1278             columns = Expressions(table, expressions, compiler, self.quote_value)\n1279         return Statement(\n1280             sql,\n1281             table=Table(table, self.quote_name),\n1282             name=name,\n1283             columns=columns,\n1284             condition=self._index_condition_sql(condition),\n1285             deferrable=self._deferrable_constraint_sql(deferrable),\n1286             include=self._index_include_sql(model, include),\n1287         )\n1288 \n1289     def _delete_unique_sql(\n1290         self, model, name, condition=None, deferrable=None, include=None,\n1291         opclasses=None, expressions=None,\n1292     ):\n1293         if (\n1294             (\n1295                 deferrable and\n1296                 not self.connection.features.supports_deferrable_unique_constraints\n1297             ) or\n1298             (condition and not self.connection.features.supports_partial_indexes) or\n1299             (include and not self.connection.features.supports_covering_indexes) or\n1300             (expressions and not self.connection.features.supports_expression_indexes)\n1301 \n1302         ):\n1303             return None\n1304         if condition or include or opclasses or expressions:\n1305             sql = self.sql_delete_index\n1306         else:\n1307             sql = self.sql_delete_unique\n1308         return self._delete_constraint_sql(sql, model, name)\n1309 \n1310     def _check_sql(self, name, check):\n1311         return self.sql_constraint % {\n1312             'name': self.quote_name(name),\n1313             'constraint': self.sql_check_constraint % {'check': check},\n1314         }\n1315 \n1316     def _create_check_sql(self, model, name, check):\n1317         return Statement(\n1318             self.sql_create_check,\n1319             table=Table(model._meta.db_table, self.quote_name),\n1320             name=self.quote_name(name),\n1321             check=check,\n1322         )\n1323 \n1324     def _delete_check_sql(self, model, name):\n1325         return self._delete_constraint_sql(self.sql_delete_check, model, name)\n1326 \n1327     def _delete_constraint_sql(self, template, model, name):\n1328         return Statement(\n1329             template,\n1330             table=Table(model._meta.db_table, self.quote_name),\n1331             name=self.quote_name(name),\n1332         )\n1333 \n1334     def _constraint_names(self, model, column_names=None, unique=None,\n1335                           primary_key=None, index=None, foreign_key=None,\n1336                           check=None, type_=None, exclude=None):\n1337         \"\"\"Return all constraint names matching the columns and conditions.\"\"\"\n1338         if column_names is not None:\n1339             column_names = [\n1340                 self.connection.introspection.identifier_converter(name)\n1341                 for name in column_names\n1342             ]\n1343         with self.connection.cursor() as cursor:\n1344             constraints = self.connection.introspection.get_constraints(cursor, model._meta.db_table)\n1345         result = []\n1346         for name, infodict in constraints.items():\n1347             if column_names is None or column_names == infodict['columns']:\n1348                 if unique is not None and infodict['unique'] != unique:\n1349                     continue\n1350                 if primary_key is not None and infodict['primary_key'] != primary_key:\n1351                     continue\n1352                 if index is not None and infodict['index'] != index:\n1353                     continue\n1354                 if check is not None and infodict['check'] != check:\n1355                     continue\n1356                 if foreign_key is not None and not infodict['foreign_key']:\n1357                     continue\n1358                 if type_ is not None and infodict['type'] != type_:\n1359                     continue\n1360                 if not exclude or name not in exclude:\n1361                     result.append(name)\n1362         return result\n1363 \n1364     def _delete_primary_key(self, model, strict=False):\n1365         constraint_names = self._constraint_names(model, primary_key=True)\n1366         if strict and len(constraint_names) != 1:\n1367             raise ValueError('Found wrong number (%s) of PK constraints for %s' % (\n1368                 len(constraint_names),\n1369                 model._meta.db_table,\n1370             ))\n1371         for constraint_name in constraint_names:\n1372             self.execute(self._delete_primary_key_sql(model, constraint_name))\n1373 \n1374     def _create_primary_key_sql(self, model, field):\n1375         return Statement(\n1376             self.sql_create_pk,\n1377             table=Table(model._meta.db_table, self.quote_name),\n1378             name=self.quote_name(\n1379                 self._create_index_name(model._meta.db_table, [field.column], suffix=\"_pk\")\n1380             ),\n1381             columns=Columns(model._meta.db_table, [field.column], self.quote_name),\n1382         )\n1383 \n1384     def _delete_primary_key_sql(self, model, name):\n1385         return self._delete_constraint_sql(self.sql_delete_pk, model, name)\n1386 \n1387     def _collate_sql(self, collation):\n1388         return 'COLLATE ' + self.quote_name(collation)\n1389 \n1390     def remove_procedure(self, procedure_name, param_types=()):\n1391         sql = self.sql_delete_procedure % {\n1392             'procedure': self.quote_name(procedure_name),\n1393             'param_types': ','.join(param_types),\n1394         }\n1395         self.execute(sql)\n",
        "intended_behavior": "Correctly identify when a foreign key references a unique field instead of a primary key in another table, and generate a model field that accurately represents this relationship."
    }
]