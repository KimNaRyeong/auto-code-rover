2025-10-15 09:30:43.082 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 09:30:44.839 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=718, output_tokens=12, cost=0.007540
2025-10-15 09:31:03.892 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=898, output_tokens=651, cost=0.028510
2025-10-15 09:31:04.929 | INFO     | registered non reproducing test 0
2025-10-15 09:31:25.602 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2229, output_tokens=677, cost=0.042600
2025-10-15 09:31:26.623 | INFO     | registered non reproducing test 1
2025-10-15 09:31:47.142 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2025, output_tokens=730, cost=0.042150
2025-10-15 09:31:48.184 | INFO     | registered non reproducing test 2
2025-10-15 09:31:48.184 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 09:31:48.189 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 09:31:57.451 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1027, output_tokens=308, cost=0.019510
2025-10-15 09:31:57.458 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 09:32:02.453 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=662, output_tokens=114, cost=0.010040
2025-10-15 09:32:02.455 | DEBUG    | Extracted a valid json.
2025-10-15 09:32:02.455 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code_in_file(\"from __future__ import absolute_import\", \"app/codein/callback.py\")",
        "search_code_in_file(\"from __future__ import absolute_import\", \"app/codein/types.py\")",
        "search_code_in_file(\"print\", \"app/main.py\")",
        "search_code_in_file(\"print\", \"tests/test_utils.py\")",
        "search_code(\"Linter.run_jobs\")",
        "search_code(\"Reporter.handle_message\")"
    ],
    "bug_locations": []
}
2025-10-15 09:32:02.516 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 09:32:02.517 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 09:32:18.122 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1622, output_tokens=551, cost=0.032750
2025-10-15 09:32:18.132 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 09:32:21.167 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2389, output_tokens=75, cost=0.026140
2025-10-15 09:32:21.171 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 09:32:24.353 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=429, output_tokens=85, cost=0.006840
2025-10-15 09:32:24.354 | DEBUG    | Extracted a valid json.
2025-10-15 09:32:24.354 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"Linter\")",
        "search_code(\"jobs\")",
        "search_code(\"Reporter\")",
        "search_code(\"handle_message\")",
        "search_code(\"from __future__ import absolute_import\")",
        "search_code(\"print statement used\")",
        "search_code(\"pylintrc\")"
    ],
    "bug_locations": []
}
2025-10-15 09:32:24.674 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 09:32:24.675 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 09:32:51.007 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5404, output_tokens=918, cost=0.081580
2025-10-15 09:32:51.015 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 09:33:03.609 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6538, output_tokens=519, cost=0.080950
2025-10-15 09:33:03.617 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 09:33:20.772 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=873, output_tokens=480, cost=0.023130
2025-10-15 09:33:20.773 | DEBUG    | Extracted a valid json.
2025-10-15 09:33:20.773 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "pylint/lint/parallel.py",
            "class": "",
            "method": "Relevant methods handling the distribution of tasks for parallel execution",
            "intended_behavior": "When distributing tasks for linting in parallel, the division should ensure that all parts of the codebase are covered without overlaps or omissions. The aggregation of results from these tasks must accurately collect and combine all findings. It might need a review of the task distribution logic and result aggregation to prevent any loss of lint messages or inconsistencies in the output."
        },
        {
            "file": "pylint/lint/pylinter.py",
            "class": "",
            "method": "Potentially in methods responsible for initializing the linting process, especially where parallel processing setup occurs",
            "intended_behavior": "Ensure that the initiation of the linting process correctly prepares the environment for a consistent linting experience, whether running in single or multiple job modes. This could involve making sure that configurations (from `pylintrc`) are consistently applied across all jobs and that any initial setup that could affect output is equally established in each parallel process."
        },
        {
            "file": "pylint/reporters/base_reporter.py",
            "class": "",
            "method": "The `handle_message` method and any other utility functions responsible for collating and reporting the lint messages from various processes",
            "intended_behavior": "The reporting system should be capable of handling messages from multiple sources (parallel jobs) without misordering or losing messages. This might require implementing a more robust method of collecting messages in a thread-safe manner and ensuring the order of messages is preserved (or appropriately handled) to produce consistent output regardless of the number of jobs."
        },
        {
            "file": "pylint/lint/parallel.py",
            "class": "",
            "method": "The methods responsible for handling the results aggregation from parallel execution",
            "intended_behavior": "Methods should ensure no results are lost during aggregation and that they handle the merging of results from parallel jobs in a manner that ensures the consistency and completeness of the linting report. This could include mechanisms to verify that all spawned jobs have successfully completed and reported back their findings before concluding the linting process."
        }
    ]
}
2025-10-15 09:33:20.788 | DEBUG    | Bug location extracted successfully: [<file>pylint/lint/parallel.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import functools
8 from collections import defaultdict
9 from collections.abc import Iterable, Sequence
10 from typing import TYPE_CHECKING, Any
11 
12 import dill
13 
14 from pylint import reporters
15 from pylint.lint.utils import _augment_sys_path
16 from pylint.message import Message
17 from pylint.typing import FileItem
18 from pylint.utils import LinterStats, merge_stats
19 
20 try:
21     import multiprocessing
22 except ImportError:
23     multiprocessing = None  # type: ignore[assignment]
24 
25 try:
26     from concurrent.futures import ProcessPoolExecutor
27 except ImportError:
28     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
29 
30 if TYPE_CHECKING:
31     from pylint.lint import PyLinter
32 
33 # PyLinter object used by worker processes when checking files using parallel mode
34 # should only be used by the worker processes
35 _worker_linter: PyLinter | None = None
36 
37 
38 def _worker_initialize(
39     linter: bytes, extra_packages_paths: Sequence[str] | None = None
40 ) -> None:
41     """Function called to initialize a worker for a Process within a concurrent Pool.
42 
43     :param linter: A linter-class (PyLinter) instance pickled with dill
44     :param extra_packages_paths: Extra entries to be added to `sys.path`
45     """
46     global _worker_linter  # pylint: disable=global-statement
47     _worker_linter = dill.loads(linter)
48     assert _worker_linter
49 
50     # On the worker process side the messages are just collected and passed back to
51     # parent process as _worker_check_file function's return value
52     _worker_linter.set_reporter(reporters.CollectingReporter())
53     _worker_linter.open()
54 
55     # Re-register dynamic plugins, since the pool does not have access to the
56     # astroid module that existed when the linter was pickled.
57     _worker_linter.load_plugin_modules(_worker_linter._dynamic_plugins, force=True)
58     _worker_linter.load_plugin_configuration()
59 
60     if extra_packages_paths:
61         _augment_sys_path(extra_packages_paths)
62 
63 
64 def _worker_check_single_file(
65     file_item: FileItem,
66 ) -> tuple[
67     int,
68     str,
69     str,
70     str,
71     list[Message],
72     LinterStats,
73     int,
74     defaultdict[str, list[Any]],
75 ]:
76     if not _worker_linter:
77         raise RuntimeError("Worker linter not yet initialised")
78     _worker_linter.open()
79     _worker_linter.check_single_file_item(file_item)
80     mapreduce_data = defaultdict(list)
81     for checker in _worker_linter.get_checkers():
82         data = checker.get_map_data()
83         if data is not None:
84             mapreduce_data[checker.name].append(data)
85     msgs = _worker_linter.reporter.messages
86     assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)
87     _worker_linter.reporter.reset()
88     return (
89         id(multiprocessing.current_process()),
90         _worker_linter.current_name,
91         file_item.filepath,
92         _worker_linter.file_state.base_name,
93         msgs,
94         _worker_linter.stats,
95         _worker_linter.msg_status,
96         mapreduce_data,
97     )
98 
99 
100 def _merge_mapreduce_data(
101     linter: PyLinter,
102     all_mapreduce_data: defaultdict[int, list[defaultdict[str, list[Any]]]],
103 ) -> None:
104     """Merges map/reduce data across workers, invoking relevant APIs on checkers."""
105     # First collate the data and prepare it, so we can send it to the checkers for
106     # validation. The intent here is to collect all the mapreduce data for all checker-
107     # runs across processes - that will then be passed to a static method on the
108     # checkers to be reduced and further processed.
109     collated_map_reduce_data: defaultdict[str, list[Any]] = defaultdict(list)
110     for linter_data in all_mapreduce_data.values():
111         for run_data in linter_data:
112             for checker_name, data in run_data.items():
113                 collated_map_reduce_data[checker_name].extend(data)
114 
115     # Send the data to checkers that support/require consolidated data
116     original_checkers = linter.get_checkers()
117     for checker in original_checkers:
118         if checker.name in collated_map_reduce_data:
119             # Assume that if the check has returned map/reduce data that it has the
120             # reducer function
121             checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])
122 
123 
124 def check_parallel(
125     linter: PyLinter,
126     jobs: int,
127     files: Iterable[FileItem],
128     extra_packages_paths: Sequence[str] | None = None,
129 ) -> None:
130     """Use the given linter to lint the files with given amount of workers (jobs).
131 
132     This splits the work filestream-by-filestream. If you need to do work across
133     multiple files, as in the similarity-checker, then implement the map/reduce mixin functionality.
134     """
135     # The linter is inherited by all the pool's workers, i.e. the linter
136     # is identical to the linter object here. This is required so that
137     # a custom PyLinter object can be used.
138     initializer = functools.partial(
139         _worker_initialize, extra_packages_paths=extra_packages_paths
140     )
141     with ProcessPoolExecutor(
142         max_workers=jobs, initializer=initializer, initargs=(dill.dumps(linter),)
143     ) as executor:
144         linter.open()
145         all_stats = []
146         all_mapreduce_data: defaultdict[
147             int, list[defaultdict[str, list[Any]]]
148         ] = defaultdict(list)
149 
150         # Maps each file to be worked on by a single _worker_check_single_file() call,
151         # collecting any map/reduce data by checker module so that we can 'reduce' it
152         # later.
153         for (
154             worker_idx,  # used to merge map/reduce data across workers
155             module,
156             file_path,
157             base_name,
158             messages,
159             stats,
160             msg_status,
161             mapreduce_data,
162         ) in executor.map(_worker_check_single_file, files):
163             linter.file_state.base_name = base_name
164             linter.file_state._is_base_filestate = False
165             linter.set_current_module(module, file_path)
166             for msg in messages:
167                 linter.reporter.handle_message(msg)
168             all_stats.append(stats)
169             all_mapreduce_data[worker_idx].append(mapreduce_data)
170             linter.msg_status |= msg_status
171 
172     _merge_mapreduce_data(linter, all_mapreduce_data)
173     linter.stats = merge_stats([linter.stats, *all_stats])

</code><intended_behavior>When distributing tasks for linting in parallel, the division should ensure that all parts of the codebase are covered without overlaps or omissions. The aggregation of results from these tasks must accurately collect and combine all findings. It might need a review of the task distribution logic and result aggregation to prevent any loss of lint messages or inconsistencies in the output.</intended_behavior>, <file>pylint/lint/pylinter.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import argparse
8 import collections
9 import contextlib
10 import functools
11 import os
12 import sys
13 import tokenize
14 import traceback
15 from collections import defaultdict
16 from collections.abc import Callable, Iterable, Iterator, Sequence
17 from io import TextIOWrapper
18 from pathlib import Path
19 from re import Pattern
20 from types import ModuleType
21 from typing import Any, Protocol
22 
23 import astroid
24 from astroid import nodes
25 
26 from pylint import checkers, exceptions, interfaces, reporters
27 from pylint.checkers.base_checker import BaseChecker
28 from pylint.config.arguments_manager import _ArgumentsManager
29 from pylint.constants import (
30     MAIN_CHECKER_NAME,
31     MSG_TYPES,
32     MSG_TYPES_STATUS,
33     WarningScope,
34 )
35 from pylint.interfaces import HIGH
36 from pylint.lint.base_options import _make_linter_options
37 from pylint.lint.caching import load_results, save_results
38 from pylint.lint.expand_modules import (
39     _is_ignored_file,
40     discover_package_path,
41     expand_modules,
42 )
43 from pylint.lint.message_state_handler import _MessageStateHandler
44 from pylint.lint.parallel import check_parallel
45 from pylint.lint.report_functions import (
46     report_messages_by_module_stats,
47     report_messages_stats,
48     report_total_messages_stats,
49 )
50 from pylint.lint.utils import (
51     _is_relative_to,
52     augmented_sys_path,
53     get_fatal_error_message,
54     prepare_crash_report,
55 )
56 from pylint.message import Message, MessageDefinition, MessageDefinitionStore
57 from pylint.reporters.base_reporter import BaseReporter
58 from pylint.reporters.text import TextReporter
59 from pylint.reporters.ureports import nodes as report_nodes
60 from pylint.typing import (
61     DirectoryNamespaceDict,
62     FileItem,
63     ManagedMessage,
64     MessageDefinitionTuple,
65     MessageLocationTuple,
66     ModuleDescriptionDict,
67     Options,
68 )
69 from pylint.utils import ASTWalker, FileState, LinterStats, utils
70 
71 MANAGER = astroid.MANAGER
72 
73 
74 class GetAstProtocol(Protocol):
75     def __call__(
76         self, filepath: str, modname: str, data: str | None = None
77     ) -> nodes.Module:
78         ...
79 
80 
81 def _read_stdin() -> str:
82     # See https://github.com/python/typeshed/pull/5623 for rationale behind assertion
83     assert isinstance(sys.stdin, TextIOWrapper)
84     sys.stdin = TextIOWrapper(sys.stdin.detach(), encoding="utf-8")
85     return sys.stdin.read()
86 
87 
88 def _load_reporter_by_class(reporter_class: str) -> type[BaseReporter]:
89     qname = reporter_class
90     module_part = astroid.modutils.get_module_part(qname)
91     module = astroid.modutils.load_module_from_name(module_part)
92     class_name = qname.split(".")[-1]
93     klass = getattr(module, class_name)
94     assert issubclass(klass, BaseReporter), f"{klass} is not a BaseReporter"
95     return klass  # type: ignore[no-any-return]
96 
97 
98 # Python Linter class #########################################################
99 
100 # pylint: disable-next=consider-using-namedtuple-or-dataclass
101 MSGS: dict[str, MessageDefinitionTuple] = {
102     "F0001": (
103         "%s",
104         "fatal",
105         "Used when an error occurred preventing the analysis of a \
106               module (unable to find it for instance).",
107         {"scope": WarningScope.LINE},
108     ),
109     "F0002": (
110         "%s: %s",
111         "astroid-error",
112         "Used when an unexpected error occurred while building the "
113         "Astroid  representation. This is usually accompanied by a "
114         "traceback. Please report such errors !",
115         {"scope": WarningScope.LINE},
116     ),
117     "F0010": (
118         "error while code parsing: %s",
119         "parse-error",
120         "Used when an exception occurred while building the Astroid "
121         "representation which could be handled by astroid.",
122         {"scope": WarningScope.LINE},
123     ),
124     "F0011": (
125         "error while parsing the configuration: %s",
126         "config-parse-error",
127         "Used when an exception occurred while parsing a pylint configuration file.",
128         {"scope": WarningScope.LINE},
129     ),
130     "I0001": (
131         "Unable to run raw checkers on built-in module %s",
132         "raw-checker-failed",
133         "Used to inform that a built-in module has not been checked "
134         "using the raw checkers.",
135         {
136             "scope": WarningScope.LINE,
137             "default_enabled": False,
138         },
139     ),
140     "I0010": (
141         "Unable to consider inline option %r",
142         "bad-inline-option",
143         "Used when an inline option is either badly formatted or can't "
144         "be used inside modules.",
145         {
146             "scope": WarningScope.LINE,
147             "default_enabled": False,
148         },
149     ),
150     "I0011": (
151         "Locally disabling %s (%s)",
152         "locally-disabled",
153         "Used when an inline option disables a message or a messages category.",
154         {
155             "scope": WarningScope.LINE,
156             "default_enabled": False,
157         },
158     ),
159     "I0013": (
160         "Ignoring entire file",
161         "file-ignored",
162         "Used to inform that the file will not be checked",
163         {
164             "scope": WarningScope.LINE,
165             "default_enabled": False,
166         },
167     ),
168     "I0020": (
169         "Suppressed %s (from line %d)",
170         "suppressed-message",
171         "A message was triggered on a line, but suppressed explicitly "
172         "by a disable= comment in the file. This message is not "
173         "generated for messages that are ignored due to configuration "
174         "settings.",
175         {
176             "scope": WarningScope.LINE,
177             "default_enabled": False,
178         },
179     ),
180     "I0021": (
181         "Useless suppression of %s",
182         "useless-suppression",
183         "Reported when a message is explicitly disabled for a line or "
184         "a block of code, but never triggered.",
185         {
186             "scope": WarningScope.LINE,
187             "default_enabled": False,
188         },
189     ),
190     "I0022": (
191         'Pragma "%s" is deprecated, use "%s" instead',
192         "deprecated-pragma",
193         "Some inline pylint options have been renamed or reworked, "
194         "only the most recent form should be used. "
195         "NOTE:skip-all is only available with pylint >= 0.26",
196         {
197             "old_names": [("I0014", "deprecated-disable-all")],
198             "scope": WarningScope.LINE,
199             "default_enabled": False,
200         },
201     ),
202     "E0001": (
203         "%s",
204         "syntax-error",
205         "Used when a syntax error is raised for a module.",
206         {"scope": WarningScope.LINE},
207     ),
208     "E0011": (
209         "Unrecognized file option %r",
210         "unrecognized-inline-option",
211         "Used when an unknown inline option is encountered.",
212         {"scope": WarningScope.LINE},
213     ),
214     "W0012": (
215         "Unknown option value for '%s', expected a valid pylint message and got '%s'",
216         "unknown-option-value",
217         "Used when an unknown value is encountered for an option.",
218         {
219             "scope": WarningScope.LINE,
220             "old_names": [("E0012", "bad-option-value")],
221         },
222     ),
223     "R0022": (
224         "Useless option value for '%s', %s",
225         "useless-option-value",
226         "Used when a value for an option that is now deleted from pylint"
227         " is encountered.",
228         {
229             "scope": WarningScope.LINE,
230             "old_names": [("E0012", "bad-option-value")],
231         },
232     ),
233     "E0013": (
234         "Plugin '%s' is impossible to load, is it installed ? ('%s')",
235         "bad-plugin-value",
236         "Used when a bad value is used in 'load-plugins'.",
237         {"scope": WarningScope.LINE},
238     ),
239     "E0014": (
240         "Out-of-place setting encountered in top level configuration-section '%s' : '%s'",
241         "bad-configuration-section",
242         "Used when we detect a setting in the top level of a toml configuration that"
243         " shouldn't be there.",
244         {"scope": WarningScope.LINE},
245     ),
246     "E0015": (
247         "Unrecognized option found: %s",
248         "unrecognized-option",
249         "Used when we detect an option that we do not recognize.",
250         {"scope": WarningScope.LINE},
251     ),
252 }
253 
254 
255 # pylint: disable=too-many-instance-attributes,too-many-public-methods
256 class PyLinter(
257     _ArgumentsManager,
258     _MessageStateHandler,
259     reporters.ReportsHandlerMixIn,
260     checkers.BaseChecker,
261 ):
262     """Lint Python modules using external checkers.
263 
264     This is the main checker controlling the other ones and the reports
265     generation. It is itself both a raw checker and an astroid checker in order
266     to:
267     * handle message activation / deactivation at the module level
268     * handle some basic but necessary stats' data (number of classes, methods...)
269 
270     IDE plugin developers: you may have to call
271     `astroid.MANAGER.clear_cache()` across runs if you want
272     to ensure the latest code version is actually checked.
273 
274     This class needs to support pickling for parallel linting to work. The exception
275     is reporter member; see check_parallel function for more details.
276     """
277 
278     name = MAIN_CHECKER_NAME
279     msgs = MSGS
280     # Will be used like this : datetime.now().strftime(crash_file_path)
281     crash_file_path: str = "pylint-crash-%Y-%m-%d-%H-%M-%S.txt"
282 
283     option_groups_descs = {
284         "Messages control": "Options controlling analysis messages",
285         "Reports": "Options related to output formatting and reporting",
286     }
287 
288     def __init__(
289         self,
290         options: Options = (),
291         reporter: reporters.BaseReporter | reporters.MultiReporter | None = None,
292         option_groups: tuple[tuple[str, str], ...] = (),
293         # TODO: Deprecate passing the pylintrc parameter
294         pylintrc: str | None = None,  # pylint: disable=unused-argument
295     ) -> None:
296         _ArgumentsManager.__init__(self, prog="pylint")
297         _MessageStateHandler.__init__(self, self)
298 
299         # Some stuff has to be done before initialization of other ancestors...
300         # messages store / checkers / reporter / astroid manager
301 
302         # Attributes for reporters
303         self.reporter: reporters.BaseReporter | reporters.MultiReporter
304         if reporter:
305             self.set_reporter(reporter)
306         else:
307             self.set_reporter(TextReporter())
308         self._reporters: dict[str, type[reporters.BaseReporter]] = {}
309         """Dictionary of possible but non-initialized reporters."""
310 
311         # Attributes for checkers and plugins
312         self._checkers: defaultdict[
313             str, list[checkers.BaseChecker]
314         ] = collections.defaultdict(list)
315         """Dictionary of registered and initialized checkers."""
316         self._dynamic_plugins: dict[str, ModuleType | ModuleNotFoundError | bool] = {}
317         """Set of loaded plugin names."""
318 
319         # Attributes related to stats
320         self.stats = LinterStats()
321 
322         # Attributes related to (command-line) options and their parsing
323         self.options: Options = options + _make_linter_options(self)
324         for opt_group in option_groups:
325             self.option_groups_descs[opt_group[0]] = opt_group[1]
326         self._option_groups: tuple[tuple[str, str], ...] = (
327             *option_groups,
328             ("Messages control", "Options controlling analysis messages"),
329             ("Reports", "Options related to output formatting and reporting"),
330         )
331         self.fail_on_symbols: list[str] = []
332         """List of message symbols on which pylint should fail, set by --fail-on."""
333         self._error_mode = False
334 
335         reporters.ReportsHandlerMixIn.__init__(self)
336         checkers.BaseChecker.__init__(self, self)
337         # provided reports
338         self.reports = (
339             ("RP0001", "Messages by category", report_total_messages_stats),
340             (
341                 "RP0002",
342                 "% errors / warnings by module",
343                 report_messages_by_module_stats,
344             ),
345             ("RP0003", "Messages", report_messages_stats),
346         )
347 
348         # Attributes related to registering messages and their handling
349         self.msgs_store = MessageDefinitionStore(self.config.py_version)
350         self.msg_status = 0
351         self._by_id_managed_msgs: list[ManagedMessage] = []
352 
353         # Attributes related to visiting files
354         self.file_state = FileState("", self.msgs_store, is_base_filestate=True)
355         self.current_name: str = ""
356         self.current_file: str | None = None
357         self._ignore_file = False
358         self._ignore_paths: list[Pattern[str]] = []
359 
360         self.register_checker(self)
361 
362     def load_default_plugins(self) -> None:
363         checkers.initialize(self)
364         reporters.initialize(self)
365 
366     def load_plugin_modules(self, modnames: Iterable[str], force: bool = False) -> None:
367         """Check a list of pylint plugins modules, load and register them.
368 
369         If a module cannot be loaded, never try to load it again and instead
370         store the error message for later use in ``load_plugin_configuration``
371         below.
372 
373         If `force` is True (useful when multiprocessing), then the plugin is
374         reloaded regardless if an entry exists in self._dynamic_plugins.
375         """
376         for modname in modnames:
377             if modname in self._dynamic_plugins and not force:
378                 continue
379             try:
380                 module = astroid.modutils.load_module_from_name(modname)
381                 module.register(self)
382                 self._dynamic_plugins[modname] = module
383             except ModuleNotFoundError as mnf_e:
384                 self._dynamic_plugins[modname] = mnf_e
385 
386     def load_plugin_configuration(self) -> None:
387         """Call the configuration hook for plugins.
388 
389         This walks through the list of plugins, grabs the "load_configuration"
390         hook, if exposed, and calls it to allow plugins to configure specific
391         settings.
392 
393         The result of attempting to load the plugin of the given name
394         is stored in the dynamic plugins dictionary in ``load_plugin_modules`` above.
395 
396         ..note::
397             This function previously always tried to load modules again, which
398             led to some confusion and silent failure conditions as described
399             in GitHub issue #7264. Making it use the stored result is more efficient, and
400             means that we avoid the ``init-hook`` problems from before.
401         """
402         for modname, module_or_error in self._dynamic_plugins.items():
403             if isinstance(module_or_error, ModuleNotFoundError):
404                 self.add_message(
405                     "bad-plugin-value", args=(modname, module_or_error), line=0
406                 )
407             elif hasattr(module_or_error, "load_configuration"):
408                 module_or_error.load_configuration(self)
409 
410         # We re-set all the dictionary values to True here to make sure the dict
411         # is pickle-able. This is only a problem in multiprocessing/parallel mode.
412         # (e.g. invoking pylint -j 2)
413         self._dynamic_plugins = {
414             modname: not isinstance(val, ModuleNotFoundError)
415             for modname, val in self._dynamic_plugins.items()
416         }
417 
418     def _load_reporters(self, reporter_names: str) -> None:
419         """Load the reporters if they are available on _reporters."""
420         if not self._reporters:
421             return
422         sub_reporters = []
423         output_files = []
424         with contextlib.ExitStack() as stack:
425             for reporter_name in reporter_names.split(","):
426                 reporter_name, *reporter_output = reporter_name.split(":", 1)
427 
428                 reporter = self._load_reporter_by_name(reporter_name)
429                 sub_reporters.append(reporter)
430                 if reporter_output:
431                     output_file = stack.enter_context(
432                         open(reporter_output[0], "w", encoding="utf-8")
433                     )
434                     reporter.out = output_file
435                     output_files.append(output_file)
436 
437             # Extend the lifetime of all opened output files
438             close_output_files = stack.pop_all().close
439 
440         if len(sub_reporters) > 1 or output_files:
441             self.set_reporter(
442                 reporters.MultiReporter(
443                     sub_reporters,
444                     close_output_files,
445                 )
446             )
447         else:
448             self.set_reporter(sub_reporters[0])
449 
450     def _load_reporter_by_name(self, reporter_name: str) -> reporters.BaseReporter:
451         name = reporter_name.lower()
452         if name in self._reporters:
453             return self._reporters[name]()
454 
455         try:
456             reporter_class = _load_reporter_by_class(reporter_name)
457         except (ImportError, AttributeError, AssertionError) as e:
458             raise exceptions.InvalidReporterError(name) from e
459 
460         return reporter_class()
461 
462     def set_reporter(
463         self, reporter: reporters.BaseReporter | reporters.MultiReporter
464     ) -> None:
465         """Set the reporter used to display messages and reports."""
466         self.reporter = reporter
467         reporter.linter = self
468 
469     def register_reporter(self, reporter_class: type[reporters.BaseReporter]) -> None:
470         """Registers a reporter class on the _reporters attribute."""
471         self._reporters[reporter_class.name] = reporter_class
472 
473     def report_order(self) -> list[BaseChecker]:
474         reports = sorted(self._reports, key=lambda x: getattr(x, "name", ""))
475         try:
476             # Remove the current reporter and add it
477             # at the end of the list.
478             reports.pop(reports.index(self))
479         except ValueError:
480             pass
481         else:
482             reports.append(self)
483         return reports
484 
485     # checkers manipulation methods ############################################
486 
487     def register_checker(self, checker: checkers.BaseChecker) -> None:
488         """This method auto registers the checker."""
489         self._checkers[checker.name].append(checker)
490         for r_id, r_title, r_cb in checker.reports:
491             self.register_report(r_id, r_title, r_cb, checker)
492         if hasattr(checker, "msgs"):
493             self.msgs_store.register_messages_from_checker(checker)
494             for message in checker.messages:
495                 if not message.default_enabled:
496                     self.disable(message.msgid)
497         # Register the checker, but disable all of its messages.
498         if not getattr(checker, "enabled", True):
499             self.disable(checker.name)
500 
501     def enable_fail_on_messages(self) -> None:
502         """Enable 'fail on' msgs.
503 
504         Convert values in config.fail_on (which might be msg category, msg id,
505         or symbol) to specific msgs, then enable and flag them for later.
506         """
507         fail_on_vals = self.config.fail_on
508         if not fail_on_vals:
509             return
510 
511         fail_on_cats = set()
512         fail_on_msgs = set()
513         for val in fail_on_vals:
514             # If value is a category, add category, else add message
515             if val in MSG_TYPES:
516                 fail_on_cats.add(val)
517             else:
518                 fail_on_msgs.add(val)
519 
520         # For every message in every checker, if cat or msg flagged, enable check
521         for all_checkers in self._checkers.values():
522             for checker in all_checkers:
523                 for msg in checker.messages:
524                     if msg.msgid in fail_on_msgs or msg.symbol in fail_on_msgs:
525                         # message id/symbol matched, enable and flag it
526                         self.enable(msg.msgid)
527                         self.fail_on_symbols.append(msg.symbol)
528                     elif msg.msgid[0] in fail_on_cats:
529                         # message starts with a category value, flag (but do not enable) it
530                         self.fail_on_symbols.append(msg.symbol)
531 
532     def any_fail_on_issues(self) -> bool:
533         return any(x in self.fail_on_symbols for x in self.stats.by_msg.keys())
534 
535     def disable_reporters(self) -> None:
536         """Disable all reporters."""
537         for _reporters in self._reports.values():
538             for report_id, _, _ in _reporters:
539                 self.disable_report(report_id)
540 
541     def _parse_error_mode(self) -> None:
542         """Parse the current state of the error mode.
543 
544         Error mode: enable only errors; no reports, no persistent.
545         """
546         if not self._error_mode:
547             return
548 
549         self.disable_noerror_messages()
550         self.disable("miscellaneous")
551         self.set_option("reports", False)
552         self.set_option("persistent", False)
553         self.set_option("score", False)
554 
555     # code checking methods ###################################################
556 
557     def get_checkers(self) -> list[BaseChecker]:
558         """Return all available checkers as an ordered list."""
559         return sorted(c for _checkers in self._checkers.values() for c in _checkers)
560 
561     def get_checker_names(self) -> list[str]:
562         """Get all the checker names that this linter knows about."""
563         return sorted(
564             {
565                 checker.name
566                 for checker in self.get_checkers()
567                 if checker.name != MAIN_CHECKER_NAME
568             }
569         )
570 
571     def prepare_checkers(self) -> list[BaseChecker]:
572         """Return checkers needed for activated messages and reports."""
573         if not self.config.reports:
574             self.disable_reporters()
575         # get needed checkers
576         needed_checkers: list[BaseChecker] = [self]
577         for checker in self.get_checkers()[1:]:
578             messages = {msg for msg in checker.msgs if self.is_message_enabled(msg)}
579             if messages or any(self.report_is_enabled(r[0]) for r in checker.reports):
580                 needed_checkers.append(checker)
581         return needed_checkers
582 
583     # pylint: disable=unused-argument
584     @staticmethod
585     def should_analyze_file(modname: str, path: str, is_argument: bool = False) -> bool:
586         """Returns whether a module should be checked.
587 
588         This implementation returns True for all python source file, indicating
589         that all files should be linted.
590 
591         Subclasses may override this method to indicate that modules satisfying
592         certain conditions should not be linted.
593 
594         :param str modname: The name of the module to be checked.
595         :param str path: The full path to the source code of the module.
596         :param bool is_argument: Whether the file is an argument to pylint or not.
597                                  Files which respect this property are always
598                                  checked, since the user requested it explicitly.
599         :returns: True if the module should be checked.
600         """
601         if is_argument:
602             return True
603         return path.endswith(".py")
604 
605     # pylint: enable=unused-argument
606 
607     def initialize(self) -> None:
608         """Initialize linter for linting.
609 
610         This method is called before any linting is done.
611         """
612         self._ignore_paths = self.config.ignore_paths
613         # initialize msgs_state now that all messages have been registered into
614         # the store
615         for msg in self.msgs_store.messages:
616             if not msg.may_be_emitted(self.config.py_version):
617                 self._msgs_state[msg.msgid] = False
618 
619     def _discover_files(self, files_or_modules: Sequence[str]) -> Iterator[str]:
620         """Discover python modules and packages in sub-directory.
621 
622         Returns iterator of paths to discovered modules and packages.
623         """
624         for something in files_or_modules:
625             if os.path.isdir(something) and not os.path.isfile(
626                 os.path.join(something, "__init__.py")
627             ):
628                 skip_subtrees: list[str] = []
629                 for root, _, files in os.walk(something):
630                     if any(root.startswith(s) for s in skip_subtrees):
631                         # Skip subtree of already discovered package.
632                         continue
633 
634                     if _is_ignored_file(
635                         root,
636                         self.config.ignore,
637                         self.config.ignore_patterns,
638                         self.config.ignore_paths,
639                     ):
640                         skip_subtrees.append(root)
641                         continue
642 
643                     if "__init__.py" in files:
644                         skip_subtrees.append(root)
645                         yield root
646                     else:
647                         yield from (
648                             os.path.join(root, file)
649                             for file in files
650                             if file.endswith(".py")
651                         )
652             else:
653                 yield something
654 
655     def check(self, files_or_modules: Sequence[str]) -> None:
656         """Main checking entry: check a list of files or modules from their name.
657 
658         files_or_modules is either a string or list of strings presenting modules to check.
659         """
660         self.initialize()
661         if self.config.recursive:
662             files_or_modules = tuple(self._discover_files(files_or_modules))
663         if self.config.from_stdin:
664             if len(files_or_modules) != 1:
665                 raise exceptions.InvalidArgsError(
666                     "Missing filename required for --from-stdin"
667                 )
668 
669         extra_packages_paths = list(
670             {
671                 discover_package_path(file_or_module, self.config.source_roots)
672                 for file_or_module in files_or_modules
673             }
674         )
675 
676         # TODO: Move the parallel invocation into step 3 of the checking process
677         if not self.config.from_stdin and self.config.jobs > 1:
678             original_sys_path = sys.path[:]
679             check_parallel(
680                 self,
681                 self.config.jobs,
682                 self._iterate_file_descrs(files_or_modules),
683                 extra_packages_paths,
684             )
685             sys.path = original_sys_path
686             return
687 
688         # 1) Get all FileItems
689         with augmented_sys_path(extra_packages_paths):
690             if self.config.from_stdin:
691                 fileitems = self._get_file_descr_from_stdin(files_or_modules[0])
692                 data: str | None = _read_stdin()
693             else:
694                 fileitems = self._iterate_file_descrs(files_or_modules)
695                 data = None
696 
697         # The contextmanager also opens all checkers and sets up the PyLinter class
698         with augmented_sys_path(extra_packages_paths):
699             with self._astroid_module_checker() as check_astroid_module:
700                 # 2) Get the AST for each FileItem
701                 ast_per_fileitem = self._get_asts(fileitems, data)
702 
703                 # 3) Lint each ast
704                 self._lint_files(ast_per_fileitem, check_astroid_module)
705 
706     def _get_asts(
707         self, fileitems: Iterator[FileItem], data: str | None
708     ) -> dict[FileItem, nodes.Module | None]:
709         """Get the AST for all given FileItems."""
710         ast_per_fileitem: dict[FileItem, nodes.Module | None] = {}
711 
712         for fileitem in fileitems:
713             self.set_current_module(fileitem.name, fileitem.filepath)
714 
715             try:
716                 ast_per_fileitem[fileitem] = self.get_ast(
717                     fileitem.filepath, fileitem.name, data
718                 )
719             except astroid.AstroidBuildingError as ex:
720                 template_path = prepare_crash_report(
721                     ex, fileitem.filepath, self.crash_file_path
722                 )
723                 msg = get_fatal_error_message(fileitem.filepath, template_path)
724                 self.add_message(
725                     "astroid-error",
726                     args=(fileitem.filepath, msg),
727                     confidence=HIGH,
728                 )
729 
730         return ast_per_fileitem
731 
732     def check_single_file_item(self, file: FileItem) -> None:
733         """Check single file item.
734 
735         The arguments are the same that are documented in _check_files
736 
737         initialize() should be called before calling this method
738         """
739         with self._astroid_module_checker() as check_astroid_module:
740             self._check_file(self.get_ast, check_astroid_module, file)
741 
742     def _lint_files(
743         self,
744         ast_mapping: dict[FileItem, nodes.Module | None],
745         check_astroid_module: Callable[[nodes.Module], bool | None],
746     ) -> None:
747         """Lint all AST modules from a mapping.."""
748         for fileitem, module in ast_mapping.items():
749             if module is None:
750                 continue
751             try:
752                 self._lint_file(fileitem, module, check_astroid_module)
753             except Exception as ex:  # pylint: disable=broad-except
754                 template_path = prepare_crash_report(
755                     ex, fileitem.filepath, self.crash_file_path
756                 )
757                 msg = get_fatal_error_message(fileitem.filepath, template_path)
758                 if isinstance(ex, astroid.AstroidError):
759                     self.add_message(
760                         "astroid-error", args=(fileitem.filepath, msg), confidence=HIGH
761                     )
762                 else:
763                     self.add_message("fatal", args=msg, confidence=HIGH)
764 
765     def _lint_file(
766         self,
767         file: FileItem,
768         module: nodes.Module,
769         check_astroid_module: Callable[[nodes.Module], bool | None],
770     ) -> None:
771         """Lint a file using the passed utility function check_astroid_module).
772 
773         :param FileItem file: data about the file
774         :param nodes.Module module: the ast module to lint
775         :param Callable check_astroid_module: callable checking an AST taking the following
776                arguments
777         - ast: AST of the module
778         :raises AstroidError: for any failures stemming from astroid
779         """
780         self.set_current_module(file.name, file.filepath)
781         self._ignore_file = False
782         self.file_state = FileState(file.modpath, self.msgs_store, module)
783         # fix the current file (if the source file was not available or
784         # if it's actually a c extension)
785         self.current_file = module.file
786 
787         try:
788             check_astroid_module(module)
789         except Exception as e:
790             raise astroid.AstroidError from e
791 
792         # warn about spurious inline messages handling
793         spurious_messages = self.file_state.iter_spurious_suppression_messages(
794             self.msgs_store
795         )
796         for msgid, line, args in spurious_messages:
797             self.add_message(msgid, line, None, args)
798 
799     def _check_file(
800         self,
801         get_ast: GetAstProtocol,
802         check_astroid_module: Callable[[nodes.Module], bool | None],
803         file: FileItem,
804     ) -> None:
805         """Check a file using the passed utility functions (get_ast and
806         check_astroid_module).
807 
808         :param callable get_ast: callable returning AST from defined file taking the
809                                  following arguments
810         - filepath: path to the file to check
811         - name: Python module name
812         :param callable check_astroid_module: callable checking an AST taking the following
813                arguments
814         - ast: AST of the module
815         :param FileItem file: data about the file
816         :raises AstroidError: for any failures stemming from astroid
817         """
818         self.set_current_module(file.name, file.filepath)
819         # get the module representation
820         ast_node = get_ast(file.filepath, file.name)
821         if ast_node is None:
822             return
823 
824         self._ignore_file = False
825 
826         self.file_state = FileState(file.modpath, self.msgs_store, ast_node)
827         # fix the current file (if the source file was not available or
828         # if it's actually a c extension)
829         self.current_file = ast_node.file
830         try:
831             check_astroid_module(ast_node)
832         except Exception as e:  # pragma: no cover
833             raise astroid.AstroidError from e
834         # warn about spurious inline messages handling
835         spurious_messages = self.file_state.iter_spurious_suppression_messages(
836             self.msgs_store
837         )
838         for msgid, line, args in spurious_messages:
839             self.add_message(msgid, line, None, args)
840 
841     def _get_file_descr_from_stdin(self, filepath: str) -> Iterator[FileItem]:
842         """Return file description (tuple of module name, file path, base name) from
843         given file path.
844 
845         This method is used for creating suitable file description for _check_files when the
846         source is standard input.
847         """
848         if _is_ignored_file(
849             filepath,
850             self.config.ignore,
851             self.config.ignore_patterns,
852             self.config.ignore_paths,
853         ):
854             return
855 
856         try:
857             # Note that this function does not really perform an
858             # __import__ but may raise an ImportError exception, which
859             # we want to catch here.
860             modname = ".".join(astroid.modutils.modpath_from_file(filepath))
861         except ImportError:
862             modname = os.path.splitext(os.path.basename(filepath))[0]
863 
864         yield FileItem(modname, filepath, filepath)
865 
866     def _iterate_file_descrs(
867         self, files_or_modules: Sequence[str]
868     ) -> Iterator[FileItem]:
869         """Return generator yielding file descriptions (tuples of module name, file
870         path, base name).
871 
872         The returned generator yield one item for each Python module that should be linted.
873         """
874         for descr in self._expand_files(files_or_modules).values():
875             name, filepath, is_arg = descr["name"], descr["path"], descr["isarg"]
876             if self.should_analyze_file(name, filepath, is_argument=is_arg):
877                 yield FileItem(name, filepath, descr["basename"])
878 
879     def _expand_files(
880         self, files_or_modules: Sequence[str]
881     ) -> dict[str, ModuleDescriptionDict]:
882         """Get modules and errors from a list of modules and handle errors."""
883         result, errors = expand_modules(
884             files_or_modules,
885             self.config.source_roots,
886             self.config.ignore,
887             self.config.ignore_patterns,
888             self._ignore_paths,
889         )
890         for error in errors:
891             message = modname = error["mod"]
892             key = error["key"]
893             self.set_current_module(modname)
894             if key == "fatal":
895                 message = str(error["ex"]).replace(os.getcwd() + os.sep, "")
896             self.add_message(key, args=message)
897         return result
898 
899     def set_current_module(self, modname: str, filepath: str | None = None) -> None:
900         """Set the name of the currently analyzed module and
901         init statistics for it.
902         """
903         if not modname and filepath is None:
904             return
905         self.reporter.on_set_current_module(modname or "", filepath)
906         self.current_name = modname
907         self.current_file = filepath or modname
908         self.stats.init_single_module(modname or "")
909 
910         # If there is an actual filepath we might need to update the config attribute
911         if filepath:
912             namespace = self._get_namespace_for_file(
913                 Path(filepath), self._directory_namespaces
914             )
915             if namespace:
916                 self.config = namespace or self._base_config
917 
918     def _get_namespace_for_file(
919         self, filepath: Path, namespaces: DirectoryNamespaceDict
920     ) -> argparse.Namespace | None:
921         for directory in namespaces:
922             if _is_relative_to(filepath, directory):
923                 namespace = self._get_namespace_for_file(
924                     filepath, namespaces[directory][1]
925                 )
926                 if namespace is None:
927                     return namespaces[directory][0]
928         return None
929 
930     @contextlib.contextmanager
931     def _astroid_module_checker(
932         self,
933     ) -> Iterator[Callable[[nodes.Module], bool | None]]:
934         """Context manager for checking ASTs.
935 
936         The value in the context is callable accepting AST as its only argument.
937         """
938         walker = ASTWalker(self)
939         _checkers = self.prepare_checkers()
940         tokencheckers = [
941             c for c in _checkers if isinstance(c, checkers.BaseTokenChecker)
942         ]
943         rawcheckers = [
944             c for c in _checkers if isinstance(c, checkers.BaseRawFileChecker)
945         ]
946         for checker in _checkers:
947             checker.open()
948             walker.add_checker(checker)
949 
950         yield functools.partial(
951             self.check_astroid_module,
952             walker=walker,
953             tokencheckers=tokencheckers,
954             rawcheckers=rawcheckers,
955         )
956 
957         # notify global end
958         self.stats.statement = walker.nbstatements
959         for checker in reversed(_checkers):
960             checker.close()
961 
962     def get_ast(
963         self, filepath: str, modname: str, data: str | None = None
964     ) -> nodes.Module | None:
965         """Return an ast(roid) representation of a module or a string.
966 
967         :param filepath: path to checked file.
968         :param str modname: The name of the module to be checked.
969         :param str data: optional contents of the checked file.
970         :returns: the AST
971         :rtype: astroid.nodes.Module
972         :raises AstroidBuildingError: Whenever we encounter an unexpected exception
973         """
974         try:
975             if data is None:
976                 return MANAGER.ast_from_file(filepath, modname, source=True)
977             return astroid.builder.AstroidBuilder(MANAGER).string_build(
978                 data, modname, filepath
979             )
980         except astroid.AstroidSyntaxError as ex:
981             line = getattr(ex.error, "lineno", None)
982             if line is None:
983                 line = 0
984             self.add_message(
985                 "syntax-error",
986                 line=line,
987                 col_offset=getattr(ex.error, "offset", None),
988                 args=f"Parsing failed: '{ex.error}'",
989                 confidence=HIGH,
990             )
991         except astroid.AstroidBuildingError as ex:
992             self.add_message("parse-error", args=ex)
993         except Exception as ex:
994             traceback.print_exc()
995             # We raise BuildingError here as this is essentially an astroid issue
996             # Creating an issue template and adding the 'astroid-error' message is handled
997             # by caller: _check_files
998             raise astroid.AstroidBuildingError(
999                 "Building error when trying to create ast representation of module '{modname}'",
1000                 modname=modname,
1001             ) from ex
1002         return None
1003 
1004     def check_astroid_module(
1005         self,
1006         ast_node: nodes.Module,
1007         walker: ASTWalker,
1008         rawcheckers: list[checkers.BaseRawFileChecker],
1009         tokencheckers: list[checkers.BaseTokenChecker],
1010     ) -> bool | None:
1011         """Check a module from its astroid representation.
1012 
1013         For return value see _check_astroid_module
1014         """
1015         before_check_statements = walker.nbstatements
1016 
1017         retval = self._check_astroid_module(
1018             ast_node, walker, rawcheckers, tokencheckers
1019         )
1020         self.stats.by_module[self.current_name]["statement"] = (
1021             walker.nbstatements - before_check_statements
1022         )
1023 
1024         return retval
1025 
1026     def _check_astroid_module(
1027         self,
1028         node: nodes.Module,
1029         walker: ASTWalker,
1030         rawcheckers: list[checkers.BaseRawFileChecker],
1031         tokencheckers: list[checkers.BaseTokenChecker],
1032     ) -> bool | None:
1033         """Check given AST node with given walker and checkers.
1034 
1035         :param astroid.nodes.Module node: AST node of the module to check
1036         :param pylint.utils.ast_walker.ASTWalker walker: AST walker
1037         :param list rawcheckers: List of token checkers to use
1038         :param list tokencheckers: List of raw checkers to use
1039 
1040         :returns: True if the module was checked, False if ignored,
1041             None if the module contents could not be parsed
1042         """
1043         try:
1044             tokens = utils.tokenize_module(node)
1045         except tokenize.TokenError as ex:
1046             self.add_message("syntax-error", line=ex.args[1][0], args=ex.args[0])
1047             return None
1048 
1049         if not node.pure_python:
1050             self.add_message("raw-checker-failed", args=node.name)
1051         else:
1052             # assert astroid.file.endswith('.py')
1053             # Parse module/block level option pragma's
1054             self.process_tokens(tokens)
1055             if self._ignore_file:
1056                 return False
1057             # run raw and tokens checkers
1058             for raw_checker in rawcheckers:
1059                 raw_checker.process_module(node)
1060             for token_checker in tokencheckers:
1061                 token_checker.process_tokens(tokens)
1062         # generate events to astroid checkers
1063         walker.walk(node)
1064         return True
1065 
1066     def open(self) -> None:
1067         """Initialize counters."""
1068         self.stats = LinterStats()
1069         MANAGER.always_load_extensions = self.config.unsafe_load_any_extension
1070         MANAGER.max_inferable_values = self.config.limit_inference_results
1071         MANAGER.extension_package_whitelist.update(self.config.extension_pkg_allow_list)
1072         if self.config.extension_pkg_whitelist:
1073             MANAGER.extension_package_whitelist.update(
1074                 self.config.extension_pkg_whitelist
1075             )
1076         self.stats.reset_message_count()
1077 
1078     def generate_reports(self) -> int | None:
1079         """Close the whole package /module, it's time to make reports !
1080 
1081         if persistent run, pickle results for later comparison
1082         """
1083         # Display whatever messages are left on the reporter.
1084         self.reporter.display_messages(report_nodes.Section())
1085         if not self.file_state._is_base_filestate:
1086             # load previous results if any
1087             previous_stats = load_results(self.file_state.base_name)
1088             self.reporter.on_close(self.stats, previous_stats)
1089             if self.config.reports:
1090                 sect = self.make_reports(self.stats, previous_stats)
1091             else:
1092                 sect = report_nodes.Section()
1093 
1094             if self.config.reports:
1095                 self.reporter.display_reports(sect)
1096             score_value = self._report_evaluation()
1097             # save results if persistent run
1098             if self.config.persistent:
1099                 save_results(self.stats, self.file_state.base_name)
1100         else:
1101             self.reporter.on_close(self.stats, LinterStats())
1102             score_value = None
1103         return score_value
1104 
1105     def _report_evaluation(self) -> int | None:
1106         """Make the global evaluation report."""
1107         # check with at least a statement (usually 0 when there is a
1108         # syntax error preventing pylint from further processing)
1109         note = None
1110         previous_stats = load_results(self.file_state.base_name)
1111         if self.stats.statement == 0:
1112             return note
1113 
1114         # get a global note for the code
1115         evaluation = self.config.evaluation
1116         try:
1117             stats_dict = {
1118                 "fatal": self.stats.fatal,
1119                 "error": self.stats.error,
1120                 "warning": self.stats.warning,
1121                 "refactor": self.stats.refactor,
1122                 "convention": self.stats.convention,
1123                 "statement": self.stats.statement,
1124                 "info": self.stats.info,
1125             }
1126             note = eval(evaluation, {}, stats_dict)  # pylint: disable=eval-used
1127         except Exception as ex:  # pylint: disable=broad-except
1128             msg = f"An exception occurred while rating: {ex}"
1129         else:
1130             self.stats.global_note = note
1131             msg = f"Your code has been rated at {note:.2f}/10"
1132             if previous_stats:
1133                 pnote = previous_stats.global_note
1134                 if pnote is not None:
1135                     msg += f" (previous run: {pnote:.2f}/10, {note - pnote:+.2f})"
1136 
1137         if self.config.score:
1138             sect = report_nodes.EvaluationSection(msg)
1139             self.reporter.display_reports(sect)
1140         return note
1141 
1142     def _add_one_message(
1143         self,
1144         message_definition: MessageDefinition,
1145         line: int | None,
1146         node: nodes.NodeNG | None,
1147         args: Any | None,
1148         confidence: interfaces.Confidence | None,
1149         col_offset: int | None,
1150         end_lineno: int | None,
1151         end_col_offset: int | None,
1152     ) -> None:
1153         """After various checks have passed a single Message is
1154         passed to the reporter and added to stats.
1155         """
1156         message_definition.check_message_definition(line, node)
1157 
1158         # Look up "location" data of node if not yet supplied
1159         if node:
1160             if node.position:
1161                 if not line:
1162                     line = node.position.lineno
1163                 if not col_offset:
1164                     col_offset = node.position.col_offset
1165                 if not end_lineno:
1166                     end_lineno = node.position.end_lineno
1167                 if not end_col_offset:
1168                     end_col_offset = node.position.end_col_offset
1169             else:
1170                 if not line:
1171                     line = node.fromlineno
1172                 if not col_offset:
1173                     col_offset = node.col_offset
1174                 if not end_lineno:
1175                     end_lineno = node.end_lineno
1176                 if not end_col_offset:
1177                     end_col_offset = node.end_col_offset
1178 
1179         # should this message be displayed
1180         if not self.is_message_enabled(message_definition.msgid, line, confidence):
1181             self.file_state.handle_ignored_message(
1182                 self._get_message_state_scope(
1183                     message_definition.msgid, line, confidence
1184                 ),
1185                 message_definition.msgid,
1186                 line,
1187             )
1188             return
1189 
1190         # update stats
1191         msg_cat = MSG_TYPES[message_definition.msgid[0]]
1192         self.msg_status |= MSG_TYPES_STATUS[message_definition.msgid[0]]
1193         self.stats.increase_single_message_count(msg_cat, 1)
1194         self.stats.increase_single_module_message_count(self.current_name, msg_cat, 1)
1195         try:
1196             self.stats.by_msg[message_definition.symbol] += 1
1197         except KeyError:
1198             self.stats.by_msg[message_definition.symbol] = 1
1199         # Interpolate arguments into message string
1200         msg = message_definition.msg
1201         if args is not None:
1202             msg %= args
1203         # get module and object
1204         if node is None:
1205             module, obj = self.current_name, ""
1206             abspath = self.current_file
1207         else:
1208             module, obj = utils.get_module_and_frameid(node)
1209             abspath = node.root().file
1210         if abspath is not None:
1211             path = abspath.replace(self.reporter.path_strip_prefix, "", 1)
1212         else:
1213             path = "configuration"
1214         # add the message
1215         self.reporter.handle_message(
1216             Message(
1217                 message_definition.msgid,
1218                 message_definition.symbol,
1219                 MessageLocationTuple(
1220                     abspath or "",
1221                     path,
1222                     module or "",
1223                     obj,
1224                     line or 1,
1225                     col_offset or 0,
1226                     end_lineno,
1227                     end_col_offset,
1228                 ),
1229                 msg,
1230                 confidence,
1231             )
1232         )
1233 
1234     def add_message(
1235         self,
1236         msgid: str,
1237         line: int | None = None,
1238         node: nodes.NodeNG | None = None,
1239         args: Any | None = None,
1240         confidence: interfaces.Confidence | None = None,
1241         col_offset: int | None = None,
1242         end_lineno: int | None = None,
1243         end_col_offset: int | None = None,
1244     ) -> None:
1245         """Adds a message given by ID or name.
1246 
1247         If provided, the message string is expanded using args.
1248 
1249         AST checkers must provide the node argument (but may optionally
1250         provide line if the line number is different), raw and token checkers
1251         must provide the line argument.
1252         """
1253         if confidence is None:
1254             confidence = interfaces.UNDEFINED
1255         message_definitions = self.msgs_store.get_message_definitions(msgid)
1256         for message_definition in message_definitions:
1257             self._add_one_message(
1258                 message_definition,
1259                 line,
1260                 node,
1261                 args,
1262                 confidence,
1263                 col_offset,
1264                 end_lineno,
1265                 end_col_offset,
1266             )
1267 
1268     def add_ignored_message(
1269         self,
1270         msgid: str,
1271         line: int,
1272         node: nodes.NodeNG | None = None,
1273         confidence: interfaces.Confidence | None = interfaces.UNDEFINED,
1274     ) -> None:
1275         """Prepares a message to be added to the ignored message storage.
1276 
1277         Some checks return early in special cases and never reach add_message(),
1278         even though they would normally issue a message.
1279         This creates false positives for useless-suppression.
1280         This function avoids this by adding those message to the ignored msgs attribute
1281         """
1282         message_definitions = self.msgs_store.get_message_definitions(msgid)
1283         for message_definition in message_definitions:
1284             message_definition.check_message_definition(line, node)
1285             self.file_state.handle_ignored_message(
1286                 self._get_message_state_scope(
1287                     message_definition.msgid, line, confidence
1288                 ),
1289                 message_definition.msgid,
1290                 line,
1291             )
1292 
1293     def _emit_stashed_messages(self) -> None:
1294         for keys, values in self._stashed_messages.items():
1295             modname, symbol = keys
1296             self.linter.set_current_module(modname)
1297             for args in values:
1298                 self.add_message(
1299                     symbol,
1300                     args=args,
1301                     line=0,
1302                     confidence=HIGH,
1303                 )
1304         self._stashed_messages = collections.defaultdict(list)

</code><intended_behavior>Ensure that the initiation of the linting process correctly prepares the environment for a consistent linting experience, whether running in single or multiple job modes. This could involve making sure that configurations (from `pylintrc`) are consistently applied across all jobs and that any initial setup that could affect output is equally established in each parallel process.</intended_behavior>, <file>pylint/reporters/base_reporter.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import os
8 import sys
9 from typing import TYPE_CHECKING, TextIO
10 
11 from pylint.message import Message
12 from pylint.reporters.ureports.nodes import Text
13 from pylint.utils import LinterStats
14 
15 if TYPE_CHECKING:
16     from pylint.lint.pylinter import PyLinter
17     from pylint.reporters.ureports.nodes import Section
18 
19 
20 class BaseReporter:
21     """Base class for reporters.
22 
23     symbols: show short symbolic names for messages.
24     """
25 
26     extension = ""
27 
28     name = "base"
29     """Name of the reporter."""
30 
31     def __init__(self, output: TextIO | None = None) -> None:
32         self.linter: PyLinter
33         self.section = 0
34         self.out: TextIO = output or sys.stdout
35         self.messages: list[Message] = []
36         # Build the path prefix to strip to get relative paths
37         self.path_strip_prefix = os.getcwd() + os.sep
38 
39     def handle_message(self, msg: Message) -> None:
40         """Handle a new message triggered on the current file."""
41         self.messages.append(msg)
42 
43     def writeln(self, string: str = "") -> None:
44         """Write a line in the output buffer."""
45         print(string, file=self.out)
46 
47     def display_reports(self, layout: Section) -> None:
48         """Display results encapsulated in the layout tree."""
49         self.section = 0
50         if layout.report_id:
51             if isinstance(layout.children[0].children[0], Text):
52                 layout.children[0].children[0].data += f" ({layout.report_id})"
53             else:
54                 raise ValueError(f"Incorrect child for {layout.children[0].children}")
55         self._display(layout)
56 
57     def _display(self, layout: Section) -> None:
58         """Display the layout."""
59         raise NotImplementedError()
60 
61     def display_messages(self, layout: Section | None) -> None:
62         """Hook for displaying the messages of the reporter.
63 
64         This will be called whenever the underlying messages
65         needs to be displayed. For some reporters, it probably
66         doesn't make sense to display messages as soon as they
67         are available, so some mechanism of storing them could be used.
68         This method can be implemented to display them after they've
69         been aggregated.
70         """
71 
72     # Event callbacks
73 
74     def on_set_current_module(self, module: str, filepath: str | None) -> None:
75         """Hook called when a module starts to be analysed."""
76 
77     def on_close(
78         self,
79         stats: LinterStats,
80         previous_stats: LinterStats | None,
81     ) -> None:
82         """Hook called when a module finished analyzing."""

</code><intended_behavior>The reporting system should be capable of handling messages from multiple sources (parallel jobs) without misordering or losing messages. This might require implementing a more robust method of collecting messages in a thread-safe manner and ensuring the order of messages is preserved (or appropriately handled) to produce consistent output regardless of the number of jobs.</intended_behavior>, <file>pylint/lint/parallel.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import functools
8 from collections import defaultdict
9 from collections.abc import Iterable, Sequence
10 from typing import TYPE_CHECKING, Any
11 
12 import dill
13 
14 from pylint import reporters
15 from pylint.lint.utils import _augment_sys_path
16 from pylint.message import Message
17 from pylint.typing import FileItem
18 from pylint.utils import LinterStats, merge_stats
19 
20 try:
21     import multiprocessing
22 except ImportError:
23     multiprocessing = None  # type: ignore[assignment]
24 
25 try:
26     from concurrent.futures import ProcessPoolExecutor
27 except ImportError:
28     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
29 
30 if TYPE_CHECKING:
31     from pylint.lint import PyLinter
32 
33 # PyLinter object used by worker processes when checking files using parallel mode
34 # should only be used by the worker processes
35 _worker_linter: PyLinter | None = None
36 
37 
38 def _worker_initialize(
39     linter: bytes, extra_packages_paths: Sequence[str] | None = None
40 ) -> None:
41     """Function called to initialize a worker for a Process within a concurrent Pool.
42 
43     :param linter: A linter-class (PyLinter) instance pickled with dill
44     :param extra_packages_paths: Extra entries to be added to `sys.path`
45     """
46     global _worker_linter  # pylint: disable=global-statement
47     _worker_linter = dill.loads(linter)
48     assert _worker_linter
49 
50     # On the worker process side the messages are just collected and passed back to
51     # parent process as _worker_check_file function's return value
52     _worker_linter.set_reporter(reporters.CollectingReporter())
53     _worker_linter.open()
54 
55     # Re-register dynamic plugins, since the pool does not have access to the
56     # astroid module that existed when the linter was pickled.
57     _worker_linter.load_plugin_modules(_worker_linter._dynamic_plugins, force=True)
58     _worker_linter.load_plugin_configuration()
59 
60     if extra_packages_paths:
61         _augment_sys_path(extra_packages_paths)
62 
63 
64 def _worker_check_single_file(
65     file_item: FileItem,
66 ) -> tuple[
67     int,
68     str,
69     str,
70     str,
71     list[Message],
72     LinterStats,
73     int,
74     defaultdict[str, list[Any]],
75 ]:
76     if not _worker_linter:
77         raise RuntimeError("Worker linter not yet initialised")
78     _worker_linter.open()
79     _worker_linter.check_single_file_item(file_item)
80     mapreduce_data = defaultdict(list)
81     for checker in _worker_linter.get_checkers():
82         data = checker.get_map_data()
83         if data is not None:
84             mapreduce_data[checker.name].append(data)
85     msgs = _worker_linter.reporter.messages
86     assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)
87     _worker_linter.reporter.reset()
88     return (
89         id(multiprocessing.current_process()),
90         _worker_linter.current_name,
91         file_item.filepath,
92         _worker_linter.file_state.base_name,
93         msgs,
94         _worker_linter.stats,
95         _worker_linter.msg_status,
96         mapreduce_data,
97     )
98 
99 
100 def _merge_mapreduce_data(
101     linter: PyLinter,
102     all_mapreduce_data: defaultdict[int, list[defaultdict[str, list[Any]]]],
103 ) -> None:
104     """Merges map/reduce data across workers, invoking relevant APIs on checkers."""
105     # First collate the data and prepare it, so we can send it to the checkers for
106     # validation. The intent here is to collect all the mapreduce data for all checker-
107     # runs across processes - that will then be passed to a static method on the
108     # checkers to be reduced and further processed.
109     collated_map_reduce_data: defaultdict[str, list[Any]] = defaultdict(list)
110     for linter_data in all_mapreduce_data.values():
111         for run_data in linter_data:
112             for checker_name, data in run_data.items():
113                 collated_map_reduce_data[checker_name].extend(data)
114 
115     # Send the data to checkers that support/require consolidated data
116     original_checkers = linter.get_checkers()
117     for checker in original_checkers:
118         if checker.name in collated_map_reduce_data:
119             # Assume that if the check has returned map/reduce data that it has the
120             # reducer function
121             checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])
122 
123 
124 def check_parallel(
125     linter: PyLinter,
126     jobs: int,
127     files: Iterable[FileItem],
128     extra_packages_paths: Sequence[str] | None = None,
129 ) -> None:
130     """Use the given linter to lint the files with given amount of workers (jobs).
131 
132     This splits the work filestream-by-filestream. If you need to do work across
133     multiple files, as in the similarity-checker, then implement the map/reduce mixin functionality.
134     """
135     # The linter is inherited by all the pool's workers, i.e. the linter
136     # is identical to the linter object here. This is required so that
137     # a custom PyLinter object can be used.
138     initializer = functools.partial(
139         _worker_initialize, extra_packages_paths=extra_packages_paths
140     )
141     with ProcessPoolExecutor(
142         max_workers=jobs, initializer=initializer, initargs=(dill.dumps(linter),)
143     ) as executor:
144         linter.open()
145         all_stats = []
146         all_mapreduce_data: defaultdict[
147             int, list[defaultdict[str, list[Any]]]
148         ] = defaultdict(list)
149 
150         # Maps each file to be worked on by a single _worker_check_single_file() call,
151         # collecting any map/reduce data by checker module so that we can 'reduce' it
152         # later.
153         for (
154             worker_idx,  # used to merge map/reduce data across workers
155             module,
156             file_path,
157             base_name,
158             messages,
159             stats,
160             msg_status,
161             mapreduce_data,
162         ) in executor.map(_worker_check_single_file, files):
163             linter.file_state.base_name = base_name
164             linter.file_state._is_base_filestate = False
165             linter.set_current_module(module, file_path)
166             for msg in messages:
167                 linter.reporter.handle_message(msg)
168             all_stats.append(stats)
169             all_mapreduce_data[worker_idx].append(mapreduce_data)
170             linter.msg_status |= msg_status
171 
172     _merge_mapreduce_data(linter, all_mapreduce_data)
173     linter.stats = merge_stats([linter.stats, *all_stats])

</code><intended_behavior>Methods should ensure no results are lost during aggregation and that they handle the merging of results from parallel jobs in a manner that ensures the consistency and completeness of the linting report. This could include mechanisms to verify that all spawned jobs have successfully completed and reported back their findings before concluding the linting process.</intended_behavior>]
2025-10-15 09:33:20.789 | INFO     | Search completed. Bug locations: [<file>pylint/lint/parallel.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import functools
8 from collections import defaultdict
9 from collections.abc import Iterable, Sequence
10 from typing import TYPE_CHECKING, Any
11 
12 import dill
13 
14 from pylint import reporters
15 from pylint.lint.utils import _augment_sys_path
16 from pylint.message import Message
17 from pylint.typing import FileItem
18 from pylint.utils import LinterStats, merge_stats
19 
20 try:
21     import multiprocessing
22 except ImportError:
23     multiprocessing = None  # type: ignore[assignment]
24 
25 try:
26     from concurrent.futures import ProcessPoolExecutor
27 except ImportError:
28     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
29 
30 if TYPE_CHECKING:
31     from pylint.lint import PyLinter
32 
33 # PyLinter object used by worker processes when checking files using parallel mode
34 # should only be used by the worker processes
35 _worker_linter: PyLinter | None = None
36 
37 
38 def _worker_initialize(
39     linter: bytes, extra_packages_paths: Sequence[str] | None = None
40 ) -> None:
41     """Function called to initialize a worker for a Process within a concurrent Pool.
42 
43     :param linter: A linter-class (PyLinter) instance pickled with dill
44     :param extra_packages_paths: Extra entries to be added to `sys.path`
45     """
46     global _worker_linter  # pylint: disable=global-statement
47     _worker_linter = dill.loads(linter)
48     assert _worker_linter
49 
50     # On the worker process side the messages are just collected and passed back to
51     # parent process as _worker_check_file function's return value
52     _worker_linter.set_reporter(reporters.CollectingReporter())
53     _worker_linter.open()
54 
55     # Re-register dynamic plugins, since the pool does not have access to the
56     # astroid module that existed when the linter was pickled.
57     _worker_linter.load_plugin_modules(_worker_linter._dynamic_plugins, force=True)
58     _worker_linter.load_plugin_configuration()
59 
60     if extra_packages_paths:
61         _augment_sys_path(extra_packages_paths)
62 
63 
64 def _worker_check_single_file(
65     file_item: FileItem,
66 ) -> tuple[
67     int,
68     str,
69     str,
70     str,
71     list[Message],
72     LinterStats,
73     int,
74     defaultdict[str, list[Any]],
75 ]:
76     if not _worker_linter:
77         raise RuntimeError("Worker linter not yet initialised")
78     _worker_linter.open()
79     _worker_linter.check_single_file_item(file_item)
80     mapreduce_data = defaultdict(list)
81     for checker in _worker_linter.get_checkers():
82         data = checker.get_map_data()
83         if data is not None:
84             mapreduce_data[checker.name].append(data)
85     msgs = _worker_linter.reporter.messages
86     assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)
87     _worker_linter.reporter.reset()
88     return (
89         id(multiprocessing.current_process()),
90         _worker_linter.current_name,
91         file_item.filepath,
92         _worker_linter.file_state.base_name,
93         msgs,
94         _worker_linter.stats,
95         _worker_linter.msg_status,
96         mapreduce_data,
97     )
98 
99 
100 def _merge_mapreduce_data(
101     linter: PyLinter,
102     all_mapreduce_data: defaultdict[int, list[defaultdict[str, list[Any]]]],
103 ) -> None:
104     """Merges map/reduce data across workers, invoking relevant APIs on checkers."""
105     # First collate the data and prepare it, so we can send it to the checkers for
106     # validation. The intent here is to collect all the mapreduce data for all checker-
107     # runs across processes - that will then be passed to a static method on the
108     # checkers to be reduced and further processed.
109     collated_map_reduce_data: defaultdict[str, list[Any]] = defaultdict(list)
110     for linter_data in all_mapreduce_data.values():
111         for run_data in linter_data:
112             for checker_name, data in run_data.items():
113                 collated_map_reduce_data[checker_name].extend(data)
114 
115     # Send the data to checkers that support/require consolidated data
116     original_checkers = linter.get_checkers()
117     for checker in original_checkers:
118         if checker.name in collated_map_reduce_data:
119             # Assume that if the check has returned map/reduce data that it has the
120             # reducer function
121             checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])
122 
123 
124 def check_parallel(
125     linter: PyLinter,
126     jobs: int,
127     files: Iterable[FileItem],
128     extra_packages_paths: Sequence[str] | None = None,
129 ) -> None:
130     """Use the given linter to lint the files with given amount of workers (jobs).
131 
132     This splits the work filestream-by-filestream. If you need to do work across
133     multiple files, as in the similarity-checker, then implement the map/reduce mixin functionality.
134     """
135     # The linter is inherited by all the pool's workers, i.e. the linter
136     # is identical to the linter object here. This is required so that
137     # a custom PyLinter object can be used.
138     initializer = functools.partial(
139         _worker_initialize, extra_packages_paths=extra_packages_paths
140     )
141     with ProcessPoolExecutor(
142         max_workers=jobs, initializer=initializer, initargs=(dill.dumps(linter),)
143     ) as executor:
144         linter.open()
145         all_stats = []
146         all_mapreduce_data: defaultdict[
147             int, list[defaultdict[str, list[Any]]]
148         ] = defaultdict(list)
149 
150         # Maps each file to be worked on by a single _worker_check_single_file() call,
151         # collecting any map/reduce data by checker module so that we can 'reduce' it
152         # later.
153         for (
154             worker_idx,  # used to merge map/reduce data across workers
155             module,
156             file_path,
157             base_name,
158             messages,
159             stats,
160             msg_status,
161             mapreduce_data,
162         ) in executor.map(_worker_check_single_file, files):
163             linter.file_state.base_name = base_name
164             linter.file_state._is_base_filestate = False
165             linter.set_current_module(module, file_path)
166             for msg in messages:
167                 linter.reporter.handle_message(msg)
168             all_stats.append(stats)
169             all_mapreduce_data[worker_idx].append(mapreduce_data)
170             linter.msg_status |= msg_status
171 
172     _merge_mapreduce_data(linter, all_mapreduce_data)
173     linter.stats = merge_stats([linter.stats, *all_stats])

</code><intended_behavior>When distributing tasks for linting in parallel, the division should ensure that all parts of the codebase are covered without overlaps or omissions. The aggregation of results from these tasks must accurately collect and combine all findings. It might need a review of the task distribution logic and result aggregation to prevent any loss of lint messages or inconsistencies in the output.</intended_behavior>, <file>pylint/lint/pylinter.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import argparse
8 import collections
9 import contextlib
10 import functools
11 import os
12 import sys
13 import tokenize
14 import traceback
15 from collections import defaultdict
16 from collections.abc import Callable, Iterable, Iterator, Sequence
17 from io import TextIOWrapper
18 from pathlib import Path
19 from re import Pattern
20 from types import ModuleType
21 from typing import Any, Protocol
22 
23 import astroid
24 from astroid import nodes
25 
26 from pylint import checkers, exceptions, interfaces, reporters
27 from pylint.checkers.base_checker import BaseChecker
28 from pylint.config.arguments_manager import _ArgumentsManager
29 from pylint.constants import (
30     MAIN_CHECKER_NAME,
31     MSG_TYPES,
32     MSG_TYPES_STATUS,
33     WarningScope,
34 )
35 from pylint.interfaces import HIGH
36 from pylint.lint.base_options import _make_linter_options
37 from pylint.lint.caching import load_results, save_results
38 from pylint.lint.expand_modules import (
39     _is_ignored_file,
40     discover_package_path,
41     expand_modules,
42 )
43 from pylint.lint.message_state_handler import _MessageStateHandler
44 from pylint.lint.parallel import check_parallel
45 from pylint.lint.report_functions import (
46     report_messages_by_module_stats,
47     report_messages_stats,
48     report_total_messages_stats,
49 )
50 from pylint.lint.utils import (
51     _is_relative_to,
52     augmented_sys_path,
53     get_fatal_error_message,
54     prepare_crash_report,
55 )
56 from pylint.message import Message, MessageDefinition, MessageDefinitionStore
57 from pylint.reporters.base_reporter import BaseReporter
58 from pylint.reporters.text import TextReporter
59 from pylint.reporters.ureports import nodes as report_nodes
60 from pylint.typing import (
61     DirectoryNamespaceDict,
62     FileItem,
63     ManagedMessage,
64     MessageDefinitionTuple,
65     MessageLocationTuple,
66     ModuleDescriptionDict,
67     Options,
68 )
69 from pylint.utils import ASTWalker, FileState, LinterStats, utils
70 
71 MANAGER = astroid.MANAGER
72 
73 
74 class GetAstProtocol(Protocol):
75     def __call__(
76         self, filepath: str, modname: str, data: str | None = None
77     ) -> nodes.Module:
78         ...
79 
80 
81 def _read_stdin() -> str:
82     # See https://github.com/python/typeshed/pull/5623 for rationale behind assertion
83     assert isinstance(sys.stdin, TextIOWrapper)
84     sys.stdin = TextIOWrapper(sys.stdin.detach(), encoding="utf-8")
85     return sys.stdin.read()
86 
87 
88 def _load_reporter_by_class(reporter_class: str) -> type[BaseReporter]:
89     qname = reporter_class
90     module_part = astroid.modutils.get_module_part(qname)
91     module = astroid.modutils.load_module_from_name(module_part)
92     class_name = qname.split(".")[-1]
93     klass = getattr(module, class_name)
94     assert issubclass(klass, BaseReporter), f"{klass} is not a BaseReporter"
95     return klass  # type: ignore[no-any-return]
96 
97 
98 # Python Linter class #########################################################
99 
100 # pylint: disable-next=consider-using-namedtuple-or-dataclass
101 MSGS: dict[str, MessageDefinitionTuple] = {
102     "F0001": (
103         "%s",
104         "fatal",
105         "Used when an error occurred preventing the analysis of a \
106               module (unable to find it for instance).",
107         {"scope": WarningScope.LINE},
108     ),
109     "F0002": (
110         "%s: %s",
111         "astroid-error",
112         "Used when an unexpected error occurred while building the "
113         "Astroid  representation. This is usually accompanied by a "
114         "traceback. Please report such errors !",
115         {"scope": WarningScope.LINE},
116     ),
117     "F0010": (
118         "error while code parsing: %s",
119         "parse-error",
120         "Used when an exception occurred while building the Astroid "
121         "representation which could be handled by astroid.",
122         {"scope": WarningScope.LINE},
123     ),
124     "F0011": (
125         "error while parsing the configuration: %s",
126         "config-parse-error",
127         "Used when an exception occurred while parsing a pylint configuration file.",
128         {"scope": WarningScope.LINE},
129     ),
130     "I0001": (
131         "Unable to run raw checkers on built-in module %s",
132         "raw-checker-failed",
133         "Used to inform that a built-in module has not been checked "
134         "using the raw checkers.",
135         {
136             "scope": WarningScope.LINE,
137             "default_enabled": False,
138         },
139     ),
140     "I0010": (
141         "Unable to consider inline option %r",
142         "bad-inline-option",
143         "Used when an inline option is either badly formatted or can't "
144         "be used inside modules.",
145         {
146             "scope": WarningScope.LINE,
147             "default_enabled": False,
148         },
149     ),
150     "I0011": (
151         "Locally disabling %s (%s)",
152         "locally-disabled",
153         "Used when an inline option disables a message or a messages category.",
154         {
155             "scope": WarningScope.LINE,
156             "default_enabled": False,
157         },
158     ),
159     "I0013": (
160         "Ignoring entire file",
161         "file-ignored",
162         "Used to inform that the file will not be checked",
163         {
164             "scope": WarningScope.LINE,
165             "default_enabled": False,
166         },
167     ),
168     "I0020": (
169         "Suppressed %s (from line %d)",
170         "suppressed-message",
171         "A message was triggered on a line, but suppressed explicitly "
172         "by a disable= comment in the file. This message is not "
173         "generated for messages that are ignored due to configuration "
174         "settings.",
175         {
176             "scope": WarningScope.LINE,
177             "default_enabled": False,
178         },
179     ),
180     "I0021": (
181         "Useless suppression of %s",
182         "useless-suppression",
183         "Reported when a message is explicitly disabled for a line or "
184         "a block of code, but never triggered.",
185         {
186             "scope": WarningScope.LINE,
187             "default_enabled": False,
188         },
189     ),
190     "I0022": (
191         'Pragma "%s" is deprecated, use "%s" instead',
192         "deprecated-pragma",
193         "Some inline pylint options have been renamed or reworked, "
194         "only the most recent form should be used. "
195         "NOTE:skip-all is only available with pylint >= 0.26",
196         {
197             "old_names": [("I0014", "deprecated-disable-all")],
198             "scope": WarningScope.LINE,
199             "default_enabled": False,
200         },
201     ),
202     "E0001": (
203         "%s",
204         "syntax-error",
205         "Used when a syntax error is raised for a module.",
206         {"scope": WarningScope.LINE},
207     ),
208     "E0011": (
209         "Unrecognized file option %r",
210         "unrecognized-inline-option",
211         "Used when an unknown inline option is encountered.",
212         {"scope": WarningScope.LINE},
213     ),
214     "W0012": (
215         "Unknown option value for '%s', expected a valid pylint message and got '%s'",
216         "unknown-option-value",
217         "Used when an unknown value is encountered for an option.",
218         {
219             "scope": WarningScope.LINE,
220             "old_names": [("E0012", "bad-option-value")],
221         },
222     ),
223     "R0022": (
224         "Useless option value for '%s', %s",
225         "useless-option-value",
226         "Used when a value for an option that is now deleted from pylint"
227         " is encountered.",
228         {
229             "scope": WarningScope.LINE,
230             "old_names": [("E0012", "bad-option-value")],
231         },
232     ),
233     "E0013": (
234         "Plugin '%s' is impossible to load, is it installed ? ('%s')",
235         "bad-plugin-value",
236         "Used when a bad value is used in 'load-plugins'.",
237         {"scope": WarningScope.LINE},
238     ),
239     "E0014": (
240         "Out-of-place setting encountered in top level configuration-section '%s' : '%s'",
241         "bad-configuration-section",
242         "Used when we detect a setting in the top level of a toml configuration that"
243         " shouldn't be there.",
244         {"scope": WarningScope.LINE},
245     ),
246     "E0015": (
247         "Unrecognized option found: %s",
248         "unrecognized-option",
249         "Used when we detect an option that we do not recognize.",
250         {"scope": WarningScope.LINE},
251     ),
252 }
253 
254 
255 # pylint: disable=too-many-instance-attributes,too-many-public-methods
256 class PyLinter(
257     _ArgumentsManager,
258     _MessageStateHandler,
259     reporters.ReportsHandlerMixIn,
260     checkers.BaseChecker,
261 ):
262     """Lint Python modules using external checkers.
263 
264     This is the main checker controlling the other ones and the reports
265     generation. It is itself both a raw checker and an astroid checker in order
266     to:
267     * handle message activation / deactivation at the module level
268     * handle some basic but necessary stats' data (number of classes, methods...)
269 
270     IDE plugin developers: you may have to call
271     `astroid.MANAGER.clear_cache()` across runs if you want
272     to ensure the latest code version is actually checked.
273 
274     This class needs to support pickling for parallel linting to work. The exception
275     is reporter member; see check_parallel function for more details.
276     """
277 
278     name = MAIN_CHECKER_NAME
279     msgs = MSGS
280     # Will be used like this : datetime.now().strftime(crash_file_path)
281     crash_file_path: str = "pylint-crash-%Y-%m-%d-%H-%M-%S.txt"
282 
283     option_groups_descs = {
284         "Messages control": "Options controlling analysis messages",
285         "Reports": "Options related to output formatting and reporting",
286     }
287 
288     def __init__(
289         self,
290         options: Options = (),
291         reporter: reporters.BaseReporter | reporters.MultiReporter | None = None,
292         option_groups: tuple[tuple[str, str], ...] = (),
293         # TODO: Deprecate passing the pylintrc parameter
294         pylintrc: str | None = None,  # pylint: disable=unused-argument
295     ) -> None:
296         _ArgumentsManager.__init__(self, prog="pylint")
297         _MessageStateHandler.__init__(self, self)
298 
299         # Some stuff has to be done before initialization of other ancestors...
300         # messages store / checkers / reporter / astroid manager
301 
302         # Attributes for reporters
303         self.reporter: reporters.BaseReporter | reporters.MultiReporter
304         if reporter:
305             self.set_reporter(reporter)
306         else:
307             self.set_reporter(TextReporter())
308         self._reporters: dict[str, type[reporters.BaseReporter]] = {}
309         """Dictionary of possible but non-initialized reporters."""
310 
311         # Attributes for checkers and plugins
312         self._checkers: defaultdict[
313             str, list[checkers.BaseChecker]
314         ] = collections.defaultdict(list)
315         """Dictionary of registered and initialized checkers."""
316         self._dynamic_plugins: dict[str, ModuleType | ModuleNotFoundError | bool] = {}
317         """Set of loaded plugin names."""
318 
319         # Attributes related to stats
320         self.stats = LinterStats()
321 
322         # Attributes related to (command-line) options and their parsing
323         self.options: Options = options + _make_linter_options(self)
324         for opt_group in option_groups:
325             self.option_groups_descs[opt_group[0]] = opt_group[1]
326         self._option_groups: tuple[tuple[str, str], ...] = (
327             *option_groups,
328             ("Messages control", "Options controlling analysis messages"),
329             ("Reports", "Options related to output formatting and reporting"),
330         )
331         self.fail_on_symbols: list[str] = []
332         """List of message symbols on which pylint should fail, set by --fail-on."""
333         self._error_mode = False
334 
335         reporters.ReportsHandlerMixIn.__init__(self)
336         checkers.BaseChecker.__init__(self, self)
337         # provided reports
338         self.reports = (
339             ("RP0001", "Messages by category", report_total_messages_stats),
340             (
341                 "RP0002",
342                 "% errors / warnings by module",
343                 report_messages_by_module_stats,
344             ),
345             ("RP0003", "Messages", report_messages_stats),
346         )
347 
348         # Attributes related to registering messages and their handling
349         self.msgs_store = MessageDefinitionStore(self.config.py_version)
350         self.msg_status = 0
351         self._by_id_managed_msgs: list[ManagedMessage] = []
352 
353         # Attributes related to visiting files
354         self.file_state = FileState("", self.msgs_store, is_base_filestate=True)
355         self.current_name: str = ""
356         self.current_file: str | None = None
357         self._ignore_file = False
358         self._ignore_paths: list[Pattern[str]] = []
359 
360         self.register_checker(self)
361 
362     def load_default_plugins(self) -> None:
363         checkers.initialize(self)
364         reporters.initialize(self)
365 
366     def load_plugin_modules(self, modnames: Iterable[str], force: bool = False) -> None:
367         """Check a list of pylint plugins modules, load and register them.
368 
369         If a module cannot be loaded, never try to load it again and instead
370         store the error message for later use in ``load_plugin_configuration``
371         below.
372 
373         If `force` is True (useful when multiprocessing), then the plugin is
374         reloaded regardless if an entry exists in self._dynamic_plugins.
375         """
376         for modname in modnames:
377             if modname in self._dynamic_plugins and not force:
378                 continue
379             try:
380                 module = astroid.modutils.load_module_from_name(modname)
381                 module.register(self)
382                 self._dynamic_plugins[modname] = module
383             except ModuleNotFoundError as mnf_e:
384                 self._dynamic_plugins[modname] = mnf_e
385 
386     def load_plugin_configuration(self) -> None:
387         """Call the configuration hook for plugins.
388 
389         This walks through the list of plugins, grabs the "load_configuration"
390         hook, if exposed, and calls it to allow plugins to configure specific
391         settings.
392 
393         The result of attempting to load the plugin of the given name
394         is stored in the dynamic plugins dictionary in ``load_plugin_modules`` above.
395 
396         ..note::
397             This function previously always tried to load modules again, which
398             led to some confusion and silent failure conditions as described
399             in GitHub issue #7264. Making it use the stored result is more efficient, and
400             means that we avoid the ``init-hook`` problems from before.
401         """
402         for modname, module_or_error in self._dynamic_plugins.items():
403             if isinstance(module_or_error, ModuleNotFoundError):
404                 self.add_message(
405                     "bad-plugin-value", args=(modname, module_or_error), line=0
406                 )
407             elif hasattr(module_or_error, "load_configuration"):
408                 module_or_error.load_configuration(self)
409 
410         # We re-set all the dictionary values to True here to make sure the dict
411         # is pickle-able. This is only a problem in multiprocessing/parallel mode.
412         # (e.g. invoking pylint -j 2)
413         self._dynamic_plugins = {
414             modname: not isinstance(val, ModuleNotFoundError)
415             for modname, val in self._dynamic_plugins.items()
416         }
417 
418     def _load_reporters(self, reporter_names: str) -> None:
419         """Load the reporters if they are available on _reporters."""
420         if not self._reporters:
421             return
422         sub_reporters = []
423         output_files = []
424         with contextlib.ExitStack() as stack:
425             for reporter_name in reporter_names.split(","):
426                 reporter_name, *reporter_output = reporter_name.split(":", 1)
427 
428                 reporter = self._load_reporter_by_name(reporter_name)
429                 sub_reporters.append(reporter)
430                 if reporter_output:
431                     output_file = stack.enter_context(
432                         open(reporter_output[0], "w", encoding="utf-8")
433                     )
434                     reporter.out = output_file
435                     output_files.append(output_file)
436 
437             # Extend the lifetime of all opened output files
438             close_output_files = stack.pop_all().close
439 
440         if len(sub_reporters) > 1 or output_files:
441             self.set_reporter(
442                 reporters.MultiReporter(
443                     sub_reporters,
444                     close_output_files,
445                 )
446             )
447         else:
448             self.set_reporter(sub_reporters[0])
449 
450     def _load_reporter_by_name(self, reporter_name: str) -> reporters.BaseReporter:
451         name = reporter_name.lower()
452         if name in self._reporters:
453             return self._reporters[name]()
454 
455         try:
456             reporter_class = _load_reporter_by_class(reporter_name)
457         except (ImportError, AttributeError, AssertionError) as e:
458             raise exceptions.InvalidReporterError(name) from e
459 
460         return reporter_class()
461 
462     def set_reporter(
463         self, reporter: reporters.BaseReporter | reporters.MultiReporter
464     ) -> None:
465         """Set the reporter used to display messages and reports."""
466         self.reporter = reporter
467         reporter.linter = self
468 
469     def register_reporter(self, reporter_class: type[reporters.BaseReporter]) -> None:
470         """Registers a reporter class on the _reporters attribute."""
471         self._reporters[reporter_class.name] = reporter_class
472 
473     def report_order(self) -> list[BaseChecker]:
474         reports = sorted(self._reports, key=lambda x: getattr(x, "name", ""))
475         try:
476             # Remove the current reporter and add it
477             # at the end of the list.
478             reports.pop(reports.index(self))
479         except ValueError:
480             pass
481         else:
482             reports.append(self)
483         return reports
484 
485     # checkers manipulation methods ############################################
486 
487     def register_checker(self, checker: checkers.BaseChecker) -> None:
488         """This method auto registers the checker."""
489         self._checkers[checker.name].append(checker)
490         for r_id, r_title, r_cb in checker.reports:
491             self.register_report(r_id, r_title, r_cb, checker)
492         if hasattr(checker, "msgs"):
493             self.msgs_store.register_messages_from_checker(checker)
494             for message in checker.messages:
495                 if not message.default_enabled:
496                     self.disable(message.msgid)
497         # Register the checker, but disable all of its messages.
498         if not getattr(checker, "enabled", True):
499             self.disable(checker.name)
500 
501     def enable_fail_on_messages(self) -> None:
502         """Enable 'fail on' msgs.
503 
504         Convert values in config.fail_on (which might be msg category, msg id,
505         or symbol) to specific msgs, then enable and flag them for later.
506         """
507         fail_on_vals = self.config.fail_on
508         if not fail_on_vals:
509             return
510 
511         fail_on_cats = set()
512         fail_on_msgs = set()
513         for val in fail_on_vals:
514             # If value is a category, add category, else add message
515             if val in MSG_TYPES:
516                 fail_on_cats.add(val)
517             else:
518                 fail_on_msgs.add(val)
519 
520         # For every message in every checker, if cat or msg flagged, enable check
521         for all_checkers in self._checkers.values():
522             for checker in all_checkers:
523                 for msg in checker.messages:
524                     if msg.msgid in fail_on_msgs or msg.symbol in fail_on_msgs:
525                         # message id/symbol matched, enable and flag it
526                         self.enable(msg.msgid)
527                         self.fail_on_symbols.append(msg.symbol)
528                     elif msg.msgid[0] in fail_on_cats:
529                         # message starts with a category value, flag (but do not enable) it
530                         self.fail_on_symbols.append(msg.symbol)
531 
532     def any_fail_on_issues(self) -> bool:
533         return any(x in self.fail_on_symbols for x in self.stats.by_msg.keys())
534 
535     def disable_reporters(self) -> None:
536         """Disable all reporters."""
537         for _reporters in self._reports.values():
538             for report_id, _, _ in _reporters:
539                 self.disable_report(report_id)
540 
541     def _parse_error_mode(self) -> None:
542         """Parse the current state of the error mode.
543 
544         Error mode: enable only errors; no reports, no persistent.
545         """
546         if not self._error_mode:
547             return
548 
549         self.disable_noerror_messages()
550         self.disable("miscellaneous")
551         self.set_option("reports", False)
552         self.set_option("persistent", False)
553         self.set_option("score", False)
554 
555     # code checking methods ###################################################
556 
557     def get_checkers(self) -> list[BaseChecker]:
558         """Return all available checkers as an ordered list."""
559         return sorted(c for _checkers in self._checkers.values() for c in _checkers)
560 
561     def get_checker_names(self) -> list[str]:
562         """Get all the checker names that this linter knows about."""
563         return sorted(
564             {
565                 checker.name
566                 for checker in self.get_checkers()
567                 if checker.name != MAIN_CHECKER_NAME
568             }
569         )
570 
571     def prepare_checkers(self) -> list[BaseChecker]:
572         """Return checkers needed for activated messages and reports."""
573         if not self.config.reports:
574             self.disable_reporters()
575         # get needed checkers
576         needed_checkers: list[BaseChecker] = [self]
577         for checker in self.get_checkers()[1:]:
578             messages = {msg for msg in checker.msgs if self.is_message_enabled(msg)}
579             if messages or any(self.report_is_enabled(r[0]) for r in checker.reports):
580                 needed_checkers.append(checker)
581         return needed_checkers
582 
583     # pylint: disable=unused-argument
584     @staticmethod
585     def should_analyze_file(modname: str, path: str, is_argument: bool = False) -> bool:
586         """Returns whether a module should be checked.
587 
588         This implementation returns True for all python source file, indicating
589         that all files should be linted.
590 
591         Subclasses may override this method to indicate that modules satisfying
592         certain conditions should not be linted.
593 
594         :param str modname: The name of the module to be checked.
595         :param str path: The full path to the source code of the module.
596         :param bool is_argument: Whether the file is an argument to pylint or not.
597                                  Files which respect this property are always
598                                  checked, since the user requested it explicitly.
599         :returns: True if the module should be checked.
600         """
601         if is_argument:
602             return True
603         return path.endswith(".py")
604 
605     # pylint: enable=unused-argument
606 
607     def initialize(self) -> None:
608         """Initialize linter for linting.
609 
610         This method is called before any linting is done.
611         """
612         self._ignore_paths = self.config.ignore_paths
613         # initialize msgs_state now that all messages have been registered into
614         # the store
615         for msg in self.msgs_store.messages:
616             if not msg.may_be_emitted(self.config.py_version):
617                 self._msgs_state[msg.msgid] = False
618 
619     def _discover_files(self, files_or_modules: Sequence[str]) -> Iterator[str]:
620         """Discover python modules and packages in sub-directory.
621 
622         Returns iterator of paths to discovered modules and packages.
623         """
624         for something in files_or_modules:
625             if os.path.isdir(something) and not os.path.isfile(
626                 os.path.join(something, "__init__.py")
627             ):
628                 skip_subtrees: list[str] = []
629                 for root, _, files in os.walk(something):
630                     if any(root.startswith(s) for s in skip_subtrees):
631                         # Skip subtree of already discovered package.
632                         continue
633 
634                     if _is_ignored_file(
635                         root,
636                         self.config.ignore,
637                         self.config.ignore_patterns,
638                         self.config.ignore_paths,
639                     ):
640                         skip_subtrees.append(root)
641                         continue
642 
643                     if "__init__.py" in files:
644                         skip_subtrees.append(root)
645                         yield root
646                     else:
647                         yield from (
648                             os.path.join(root, file)
649                             for file in files
650                             if file.endswith(".py")
651                         )
652             else:
653                 yield something
654 
655     def check(self, files_or_modules: Sequence[str]) -> None:
656         """Main checking entry: check a list of files or modules from their name.
657 
658         files_or_modules is either a string or list of strings presenting modules to check.
659         """
660         self.initialize()
661         if self.config.recursive:
662             files_or_modules = tuple(self._discover_files(files_or_modules))
663         if self.config.from_stdin:
664             if len(files_or_modules) != 1:
665                 raise exceptions.InvalidArgsError(
666                     "Missing filename required for --from-stdin"
667                 )
668 
669         extra_packages_paths = list(
670             {
671                 discover_package_path(file_or_module, self.config.source_roots)
672                 for file_or_module in files_or_modules
673             }
674         )
675 
676         # TODO: Move the parallel invocation into step 3 of the checking process
677         if not self.config.from_stdin and self.config.jobs > 1:
678             original_sys_path = sys.path[:]
679             check_parallel(
680                 self,
681                 self.config.jobs,
682                 self._iterate_file_descrs(files_or_modules),
683                 extra_packages_paths,
684             )
685             sys.path = original_sys_path
686             return
687 
688         # 1) Get all FileItems
689         with augmented_sys_path(extra_packages_paths):
690             if self.config.from_stdin:
691                 fileitems = self._get_file_descr_from_stdin(files_or_modules[0])
692                 data: str | None = _read_stdin()
693             else:
694                 fileitems = self._iterate_file_descrs(files_or_modules)
695                 data = None
696 
697         # The contextmanager also opens all checkers and sets up the PyLinter class
698         with augmented_sys_path(extra_packages_paths):
699             with self._astroid_module_checker() as check_astroid_module:
700                 # 2) Get the AST for each FileItem
701                 ast_per_fileitem = self._get_asts(fileitems, data)
702 
703                 # 3) Lint each ast
704                 self._lint_files(ast_per_fileitem, check_astroid_module)
705 
706     def _get_asts(
707         self, fileitems: Iterator[FileItem], data: str | None
708     ) -> dict[FileItem, nodes.Module | None]:
709         """Get the AST for all given FileItems."""
710         ast_per_fileitem: dict[FileItem, nodes.Module | None] = {}
711 
712         for fileitem in fileitems:
713             self.set_current_module(fileitem.name, fileitem.filepath)
714 
715             try:
716                 ast_per_fileitem[fileitem] = self.get_ast(
717                     fileitem.filepath, fileitem.name, data
718                 )
719             except astroid.AstroidBuildingError as ex:
720                 template_path = prepare_crash_report(
721                     ex, fileitem.filepath, self.crash_file_path
722                 )
723                 msg = get_fatal_error_message(fileitem.filepath, template_path)
724                 self.add_message(
725                     "astroid-error",
726                     args=(fileitem.filepath, msg),
727                     confidence=HIGH,
728                 )
729 
730         return ast_per_fileitem
731 
732     def check_single_file_item(self, file: FileItem) -> None:
733         """Check single file item.
734 
735         The arguments are the same that are documented in _check_files
736 
737         initialize() should be called before calling this method
738         """
739         with self._astroid_module_checker() as check_astroid_module:
740             self._check_file(self.get_ast, check_astroid_module, file)
741 
742     def _lint_files(
743         self,
744         ast_mapping: dict[FileItem, nodes.Module | None],
745         check_astroid_module: Callable[[nodes.Module], bool | None],
746     ) -> None:
747         """Lint all AST modules from a mapping.."""
748         for fileitem, module in ast_mapping.items():
749             if module is None:
750                 continue
751             try:
752                 self._lint_file(fileitem, module, check_astroid_module)
753             except Exception as ex:  # pylint: disable=broad-except
754                 template_path = prepare_crash_report(
755                     ex, fileitem.filepath, self.crash_file_path
756                 )
757                 msg = get_fatal_error_message(fileitem.filepath, template_path)
758                 if isinstance(ex, astroid.AstroidError):
759                     self.add_message(
760                         "astroid-error", args=(fileitem.filepath, msg), confidence=HIGH
761                     )
762                 else:
763                     self.add_message("fatal", args=msg, confidence=HIGH)
764 
765     def _lint_file(
766         self,
767         file: FileItem,
768         module: nodes.Module,
769         check_astroid_module: Callable[[nodes.Module], bool | None],
770     ) -> None:
771         """Lint a file using the passed utility function check_astroid_module).
772 
773         :param FileItem file: data about the file
774         :param nodes.Module module: the ast module to lint
775         :param Callable check_astroid_module: callable checking an AST taking the following
776                arguments
777         - ast: AST of the module
778         :raises AstroidError: for any failures stemming from astroid
779         """
780         self.set_current_module(file.name, file.filepath)
781         self._ignore_file = False
782         self.file_state = FileState(file.modpath, self.msgs_store, module)
783         # fix the current file (if the source file was not available or
784         # if it's actually a c extension)
785         self.current_file = module.file
786 
787         try:
788             check_astroid_module(module)
789         except Exception as e:
790             raise astroid.AstroidError from e
791 
792         # warn about spurious inline messages handling
793         spurious_messages = self.file_state.iter_spurious_suppression_messages(
794             self.msgs_store
795         )
796         for msgid, line, args in spurious_messages:
797             self.add_message(msgid, line, None, args)
798 
799     def _check_file(
800         self,
801         get_ast: GetAstProtocol,
802         check_astroid_module: Callable[[nodes.Module], bool | None],
803         file: FileItem,
804     ) -> None:
805         """Check a file using the passed utility functions (get_ast and
806         check_astroid_module).
807 
808         :param callable get_ast: callable returning AST from defined file taking the
809                                  following arguments
810         - filepath: path to the file to check
811         - name: Python module name
812         :param callable check_astroid_module: callable checking an AST taking the following
813                arguments
814         - ast: AST of the module
815         :param FileItem file: data about the file
816         :raises AstroidError: for any failures stemming from astroid
817         """
818         self.set_current_module(file.name, file.filepath)
819         # get the module representation
820         ast_node = get_ast(file.filepath, file.name)
821         if ast_node is None:
822             return
823 
824         self._ignore_file = False
825 
826         self.file_state = FileState(file.modpath, self.msgs_store, ast_node)
827         # fix the current file (if the source file was not available or
828         # if it's actually a c extension)
829         self.current_file = ast_node.file
830         try:
831             check_astroid_module(ast_node)
832         except Exception as e:  # pragma: no cover
833             raise astroid.AstroidError from e
834         # warn about spurious inline messages handling
835         spurious_messages = self.file_state.iter_spurious_suppression_messages(
836             self.msgs_store
837         )
838         for msgid, line, args in spurious_messages:
839             self.add_message(msgid, line, None, args)
840 
841     def _get_file_descr_from_stdin(self, filepath: str) -> Iterator[FileItem]:
842         """Return file description (tuple of module name, file path, base name) from
843         given file path.
844 
845         This method is used for creating suitable file description for _check_files when the
846         source is standard input.
847         """
848         if _is_ignored_file(
849             filepath,
850             self.config.ignore,
851             self.config.ignore_patterns,
852             self.config.ignore_paths,
853         ):
854             return
855 
856         try:
857             # Note that this function does not really perform an
858             # __import__ but may raise an ImportError exception, which
859             # we want to catch here.
860             modname = ".".join(astroid.modutils.modpath_from_file(filepath))
861         except ImportError:
862             modname = os.path.splitext(os.path.basename(filepath))[0]
863 
864         yield FileItem(modname, filepath, filepath)
865 
866     def _iterate_file_descrs(
867         self, files_or_modules: Sequence[str]
868     ) -> Iterator[FileItem]:
869         """Return generator yielding file descriptions (tuples of module name, file
870         path, base name).
871 
872         The returned generator yield one item for each Python module that should be linted.
873         """
874         for descr in self._expand_files(files_or_modules).values():
875             name, filepath, is_arg = descr["name"], descr["path"], descr["isarg"]
876             if self.should_analyze_file(name, filepath, is_argument=is_arg):
877                 yield FileItem(name, filepath, descr["basename"])
878 
879     def _expand_files(
880         self, files_or_modules: Sequence[str]
881     ) -> dict[str, ModuleDescriptionDict]:
882         """Get modules and errors from a list of modules and handle errors."""
883         result, errors = expand_modules(
884             files_or_modules,
885             self.config.source_roots,
886             self.config.ignore,
887             self.config.ignore_patterns,
888             self._ignore_paths,
889         )
890         for error in errors:
891             message = modname = error["mod"]
892             key = error["key"]
893             self.set_current_module(modname)
894             if key == "fatal":
895                 message = str(error["ex"]).replace(os.getcwd() + os.sep, "")
896             self.add_message(key, args=message)
897         return result
898 
899     def set_current_module(self, modname: str, filepath: str | None = None) -> None:
900         """Set the name of the currently analyzed module and
901         init statistics for it.
902         """
903         if not modname and filepath is None:
904             return
905         self.reporter.on_set_current_module(modname or "", filepath)
906         self.current_name = modname
907         self.current_file = filepath or modname
908         self.stats.init_single_module(modname or "")
909 
910         # If there is an actual filepath we might need to update the config attribute
911         if filepath:
912             namespace = self._get_namespace_for_file(
913                 Path(filepath), self._directory_namespaces
914             )
915             if namespace:
916                 self.config = namespace or self._base_config
917 
918     def _get_namespace_for_file(
919         self, filepath: Path, namespaces: DirectoryNamespaceDict
920     ) -> argparse.Namespace | None:
921         for directory in namespaces:
922             if _is_relative_to(filepath, directory):
923                 namespace = self._get_namespace_for_file(
924                     filepath, namespaces[directory][1]
925                 )
926                 if namespace is None:
927                     return namespaces[directory][0]
928         return None
929 
930     @contextlib.contextmanager
931     def _astroid_module_checker(
932         self,
933     ) -> Iterator[Callable[[nodes.Module], bool | None]]:
934         """Context manager for checking ASTs.
935 
936         The value in the context is callable accepting AST as its only argument.
937         """
938         walker = ASTWalker(self)
939         _checkers = self.prepare_checkers()
940         tokencheckers = [
941             c for c in _checkers if isinstance(c, checkers.BaseTokenChecker)
942         ]
943         rawcheckers = [
944             c for c in _checkers if isinstance(c, checkers.BaseRawFileChecker)
945         ]
946         for checker in _checkers:
947             checker.open()
948             walker.add_checker(checker)
949 
950         yield functools.partial(
951             self.check_astroid_module,
952             walker=walker,
953             tokencheckers=tokencheckers,
954             rawcheckers=rawcheckers,
955         )
956 
957         # notify global end
958         self.stats.statement = walker.nbstatements
959         for checker in reversed(_checkers):
960             checker.close()
961 
962     def get_ast(
963         self, filepath: str, modname: str, data: str | None = None
964     ) -> nodes.Module | None:
965         """Return an ast(roid) representation of a module or a string.
966 
967         :param filepath: path to checked file.
968         :param str modname: The name of the module to be checked.
969         :param str data: optional contents of the checked file.
970         :returns: the AST
971         :rtype: astroid.nodes.Module
972         :raises AstroidBuildingError: Whenever we encounter an unexpected exception
973         """
974         try:
975             if data is None:
976                 return MANAGER.ast_from_file(filepath, modname, source=True)
977             return astroid.builder.AstroidBuilder(MANAGER).string_build(
978                 data, modname, filepath
979             )
980         except astroid.AstroidSyntaxError as ex:
981             line = getattr(ex.error, "lineno", None)
982             if line is None:
983                 line = 0
984             self.add_message(
985                 "syntax-error",
986                 line=line,
987                 col_offset=getattr(ex.error, "offset", None),
988                 args=f"Parsing failed: '{ex.error}'",
989                 confidence=HIGH,
990             )
991         except astroid.AstroidBuildingError as ex:
992             self.add_message("parse-error", args=ex)
993         except Exception as ex:
994             traceback.print_exc()
995             # We raise BuildingError here as this is essentially an astroid issue
996             # Creating an issue template and adding the 'astroid-error' message is handled
997             # by caller: _check_files
998             raise astroid.AstroidBuildingError(
999                 "Building error when trying to create ast representation of module '{modname}'",
1000                 modname=modname,
1001             ) from ex
1002         return None
1003 
1004     def check_astroid_module(
1005         self,
1006         ast_node: nodes.Module,
1007         walker: ASTWalker,
1008         rawcheckers: list[checkers.BaseRawFileChecker],
1009         tokencheckers: list[checkers.BaseTokenChecker],
1010     ) -> bool | None:
1011         """Check a module from its astroid representation.
1012 
1013         For return value see _check_astroid_module
1014         """
1015         before_check_statements = walker.nbstatements
1016 
1017         retval = self._check_astroid_module(
1018             ast_node, walker, rawcheckers, tokencheckers
1019         )
1020         self.stats.by_module[self.current_name]["statement"] = (
1021             walker.nbstatements - before_check_statements
1022         )
1023 
1024         return retval
1025 
1026     def _check_astroid_module(
1027         self,
1028         node: nodes.Module,
1029         walker: ASTWalker,
1030         rawcheckers: list[checkers.BaseRawFileChecker],
1031         tokencheckers: list[checkers.BaseTokenChecker],
1032     ) -> bool | None:
1033         """Check given AST node with given walker and checkers.
1034 
1035         :param astroid.nodes.Module node: AST node of the module to check
1036         :param pylint.utils.ast_walker.ASTWalker walker: AST walker
1037         :param list rawcheckers: List of token checkers to use
1038         :param list tokencheckers: List of raw checkers to use
1039 
1040         :returns: True if the module was checked, False if ignored,
1041             None if the module contents could not be parsed
1042         """
1043         try:
1044             tokens = utils.tokenize_module(node)
1045         except tokenize.TokenError as ex:
1046             self.add_message("syntax-error", line=ex.args[1][0], args=ex.args[0])
1047             return None
1048 
1049         if not node.pure_python:
1050             self.add_message("raw-checker-failed", args=node.name)
1051         else:
1052             # assert astroid.file.endswith('.py')
1053             # Parse module/block level option pragma's
1054             self.process_tokens(tokens)
1055             if self._ignore_file:
1056                 return False
1057             # run raw and tokens checkers
1058             for raw_checker in rawcheckers:
1059                 raw_checker.process_module(node)
1060             for token_checker in tokencheckers:
1061                 token_checker.process_tokens(tokens)
1062         # generate events to astroid checkers
1063         walker.walk(node)
1064         return True
1065 
1066     def open(self) -> None:
1067         """Initialize counters."""
1068         self.stats = LinterStats()
1069         MANAGER.always_load_extensions = self.config.unsafe_load_any_extension
1070         MANAGER.max_inferable_values = self.config.limit_inference_results
1071         MANAGER.extension_package_whitelist.update(self.config.extension_pkg_allow_list)
1072         if self.config.extension_pkg_whitelist:
1073             MANAGER.extension_package_whitelist.update(
1074                 self.config.extension_pkg_whitelist
1075             )
1076         self.stats.reset_message_count()
1077 
1078     def generate_reports(self) -> int | None:
1079         """Close the whole package /module, it's time to make reports !
1080 
1081         if persistent run, pickle results for later comparison
1082         """
1083         # Display whatever messages are left on the reporter.
1084         self.reporter.display_messages(report_nodes.Section())
1085         if not self.file_state._is_base_filestate:
1086             # load previous results if any
1087             previous_stats = load_results(self.file_state.base_name)
1088             self.reporter.on_close(self.stats, previous_stats)
1089             if self.config.reports:
1090                 sect = self.make_reports(self.stats, previous_stats)
1091             else:
1092                 sect = report_nodes.Section()
1093 
1094             if self.config.reports:
1095                 self.reporter.display_reports(sect)
1096             score_value = self._report_evaluation()
1097             # save results if persistent run
1098             if self.config.persistent:
1099                 save_results(self.stats, self.file_state.base_name)
1100         else:
1101             self.reporter.on_close(self.stats, LinterStats())
1102             score_value = None
1103         return score_value
1104 
1105     def _report_evaluation(self) -> int | None:
1106         """Make the global evaluation report."""
1107         # check with at least a statement (usually 0 when there is a
1108         # syntax error preventing pylint from further processing)
1109         note = None
1110         previous_stats = load_results(self.file_state.base_name)
1111         if self.stats.statement == 0:
1112             return note
1113 
1114         # get a global note for the code
1115         evaluation = self.config.evaluation
1116         try:
1117             stats_dict = {
1118                 "fatal": self.stats.fatal,
1119                 "error": self.stats.error,
1120                 "warning": self.stats.warning,
1121                 "refactor": self.stats.refactor,
1122                 "convention": self.stats.convention,
1123                 "statement": self.stats.statement,
1124                 "info": self.stats.info,
1125             }
1126             note = eval(evaluation, {}, stats_dict)  # pylint: disable=eval-used
1127         except Exception as ex:  # pylint: disable=broad-except
1128             msg = f"An exception occurred while rating: {ex}"
1129         else:
1130             self.stats.global_note = note
1131             msg = f"Your code has been rated at {note:.2f}/10"
1132             if previous_stats:
1133                 pnote = previous_stats.global_note
1134                 if pnote is not None:
1135                     msg += f" (previous run: {pnote:.2f}/10, {note - pnote:+.2f})"
1136 
1137         if self.config.score:
1138             sect = report_nodes.EvaluationSection(msg)
1139             self.reporter.display_reports(sect)
1140         return note
1141 
1142     def _add_one_message(
1143         self,
1144         message_definition: MessageDefinition,
1145         line: int | None,
1146         node: nodes.NodeNG | None,
1147         args: Any | None,
1148         confidence: interfaces.Confidence | None,
1149         col_offset: int | None,
1150         end_lineno: int | None,
1151         end_col_offset: int | None,
1152     ) -> None:
1153         """After various checks have passed a single Message is
1154         passed to the reporter and added to stats.
1155         """
1156         message_definition.check_message_definition(line, node)
1157 
1158         # Look up "location" data of node if not yet supplied
1159         if node:
1160             if node.position:
1161                 if not line:
1162                     line = node.position.lineno
1163                 if not col_offset:
1164                     col_offset = node.position.col_offset
1165                 if not end_lineno:
1166                     end_lineno = node.position.end_lineno
1167                 if not end_col_offset:
1168                     end_col_offset = node.position.end_col_offset
1169             else:
1170                 if not line:
1171                     line = node.fromlineno
1172                 if not col_offset:
1173                     col_offset = node.col_offset
1174                 if not end_lineno:
1175                     end_lineno = node.end_lineno
1176                 if not end_col_offset:
1177                     end_col_offset = node.end_col_offset
1178 
1179         # should this message be displayed
1180         if not self.is_message_enabled(message_definition.msgid, line, confidence):
1181             self.file_state.handle_ignored_message(
1182                 self._get_message_state_scope(
1183                     message_definition.msgid, line, confidence
1184                 ),
1185                 message_definition.msgid,
1186                 line,
1187             )
1188             return
1189 
1190         # update stats
1191         msg_cat = MSG_TYPES[message_definition.msgid[0]]
1192         self.msg_status |= MSG_TYPES_STATUS[message_definition.msgid[0]]
1193         self.stats.increase_single_message_count(msg_cat, 1)
1194         self.stats.increase_single_module_message_count(self.current_name, msg_cat, 1)
1195         try:
1196             self.stats.by_msg[message_definition.symbol] += 1
1197         except KeyError:
1198             self.stats.by_msg[message_definition.symbol] = 1
1199         # Interpolate arguments into message string
1200         msg = message_definition.msg
1201         if args is not None:
1202             msg %= args
1203         # get module and object
1204         if node is None:
1205             module, obj = self.current_name, ""
1206             abspath = self.current_file
1207         else:
1208             module, obj = utils.get_module_and_frameid(node)
1209             abspath = node.root().file
1210         if abspath is not None:
1211             path = abspath.replace(self.reporter.path_strip_prefix, "", 1)
1212         else:
1213             path = "configuration"
1214         # add the message
1215         self.reporter.handle_message(
1216             Message(
1217                 message_definition.msgid,
1218                 message_definition.symbol,
1219                 MessageLocationTuple(
1220                     abspath or "",
1221                     path,
1222                     module or "",
1223                     obj,
1224                     line or 1,
1225                     col_offset or 0,
1226                     end_lineno,
1227                     end_col_offset,
1228                 ),
1229                 msg,
1230                 confidence,
1231             )
1232         )
1233 
1234     def add_message(
1235         self,
1236         msgid: str,
1237         line: int | None = None,
1238         node: nodes.NodeNG | None = None,
1239         args: Any | None = None,
1240         confidence: interfaces.Confidence | None = None,
1241         col_offset: int | None = None,
1242         end_lineno: int | None = None,
1243         end_col_offset: int | None = None,
1244     ) -> None:
1245         """Adds a message given by ID or name.
1246 
1247         If provided, the message string is expanded using args.
1248 
1249         AST checkers must provide the node argument (but may optionally
1250         provide line if the line number is different), raw and token checkers
1251         must provide the line argument.
1252         """
1253         if confidence is None:
1254             confidence = interfaces.UNDEFINED
1255         message_definitions = self.msgs_store.get_message_definitions(msgid)
1256         for message_definition in message_definitions:
1257             self._add_one_message(
1258                 message_definition,
1259                 line,
1260                 node,
1261                 args,
1262                 confidence,
1263                 col_offset,
1264                 end_lineno,
1265                 end_col_offset,
1266             )
1267 
1268     def add_ignored_message(
1269         self,
1270         msgid: str,
1271         line: int,
1272         node: nodes.NodeNG | None = None,
1273         confidence: interfaces.Confidence | None = interfaces.UNDEFINED,
1274     ) -> None:
1275         """Prepares a message to be added to the ignored message storage.
1276 
1277         Some checks return early in special cases and never reach add_message(),
1278         even though they would normally issue a message.
1279         This creates false positives for useless-suppression.
1280         This function avoids this by adding those message to the ignored msgs attribute
1281         """
1282         message_definitions = self.msgs_store.get_message_definitions(msgid)
1283         for message_definition in message_definitions:
1284             message_definition.check_message_definition(line, node)
1285             self.file_state.handle_ignored_message(
1286                 self._get_message_state_scope(
1287                     message_definition.msgid, line, confidence
1288                 ),
1289                 message_definition.msgid,
1290                 line,
1291             )
1292 
1293     def _emit_stashed_messages(self) -> None:
1294         for keys, values in self._stashed_messages.items():
1295             modname, symbol = keys
1296             self.linter.set_current_module(modname)
1297             for args in values:
1298                 self.add_message(
1299                     symbol,
1300                     args=args,
1301                     line=0,
1302                     confidence=HIGH,
1303                 )
1304         self._stashed_messages = collections.defaultdict(list)

</code><intended_behavior>Ensure that the initiation of the linting process correctly prepares the environment for a consistent linting experience, whether running in single or multiple job modes. This could involve making sure that configurations (from `pylintrc`) are consistently applied across all jobs and that any initial setup that could affect output is equally established in each parallel process.</intended_behavior>, <file>pylint/reporters/base_reporter.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import os
8 import sys
9 from typing import TYPE_CHECKING, TextIO
10 
11 from pylint.message import Message
12 from pylint.reporters.ureports.nodes import Text
13 from pylint.utils import LinterStats
14 
15 if TYPE_CHECKING:
16     from pylint.lint.pylinter import PyLinter
17     from pylint.reporters.ureports.nodes import Section
18 
19 
20 class BaseReporter:
21     """Base class for reporters.
22 
23     symbols: show short symbolic names for messages.
24     """
25 
26     extension = ""
27 
28     name = "base"
29     """Name of the reporter."""
30 
31     def __init__(self, output: TextIO | None = None) -> None:
32         self.linter: PyLinter
33         self.section = 0
34         self.out: TextIO = output or sys.stdout
35         self.messages: list[Message] = []
36         # Build the path prefix to strip to get relative paths
37         self.path_strip_prefix = os.getcwd() + os.sep
38 
39     def handle_message(self, msg: Message) -> None:
40         """Handle a new message triggered on the current file."""
41         self.messages.append(msg)
42 
43     def writeln(self, string: str = "") -> None:
44         """Write a line in the output buffer."""
45         print(string, file=self.out)
46 
47     def display_reports(self, layout: Section) -> None:
48         """Display results encapsulated in the layout tree."""
49         self.section = 0
50         if layout.report_id:
51             if isinstance(layout.children[0].children[0], Text):
52                 layout.children[0].children[0].data += f" ({layout.report_id})"
53             else:
54                 raise ValueError(f"Incorrect child for {layout.children[0].children}")
55         self._display(layout)
56 
57     def _display(self, layout: Section) -> None:
58         """Display the layout."""
59         raise NotImplementedError()
60 
61     def display_messages(self, layout: Section | None) -> None:
62         """Hook for displaying the messages of the reporter.
63 
64         This will be called whenever the underlying messages
65         needs to be displayed. For some reporters, it probably
66         doesn't make sense to display messages as soon as they
67         are available, so some mechanism of storing them could be used.
68         This method can be implemented to display them after they've
69         been aggregated.
70         """
71 
72     # Event callbacks
73 
74     def on_set_current_module(self, module: str, filepath: str | None) -> None:
75         """Hook called when a module starts to be analysed."""
76 
77     def on_close(
78         self,
79         stats: LinterStats,
80         previous_stats: LinterStats | None,
81     ) -> None:
82         """Hook called when a module finished analyzing."""

</code><intended_behavior>The reporting system should be capable of handling messages from multiple sources (parallel jobs) without misordering or losing messages. This might require implementing a more robust method of collecting messages in a thread-safe manner and ensuring the order of messages is preserved (or appropriately handled) to produce consistent output regardless of the number of jobs.</intended_behavior>, <file>pylint/lint/parallel.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import functools
8 from collections import defaultdict
9 from collections.abc import Iterable, Sequence
10 from typing import TYPE_CHECKING, Any
11 
12 import dill
13 
14 from pylint import reporters
15 from pylint.lint.utils import _augment_sys_path
16 from pylint.message import Message
17 from pylint.typing import FileItem
18 from pylint.utils import LinterStats, merge_stats
19 
20 try:
21     import multiprocessing
22 except ImportError:
23     multiprocessing = None  # type: ignore[assignment]
24 
25 try:
26     from concurrent.futures import ProcessPoolExecutor
27 except ImportError:
28     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
29 
30 if TYPE_CHECKING:
31     from pylint.lint import PyLinter
32 
33 # PyLinter object used by worker processes when checking files using parallel mode
34 # should only be used by the worker processes
35 _worker_linter: PyLinter | None = None
36 
37 
38 def _worker_initialize(
39     linter: bytes, extra_packages_paths: Sequence[str] | None = None
40 ) -> None:
41     """Function called to initialize a worker for a Process within a concurrent Pool.
42 
43     :param linter: A linter-class (PyLinter) instance pickled with dill
44     :param extra_packages_paths: Extra entries to be added to `sys.path`
45     """
46     global _worker_linter  # pylint: disable=global-statement
47     _worker_linter = dill.loads(linter)
48     assert _worker_linter
49 
50     # On the worker process side the messages are just collected and passed back to
51     # parent process as _worker_check_file function's return value
52     _worker_linter.set_reporter(reporters.CollectingReporter())
53     _worker_linter.open()
54 
55     # Re-register dynamic plugins, since the pool does not have access to the
56     # astroid module that existed when the linter was pickled.
57     _worker_linter.load_plugin_modules(_worker_linter._dynamic_plugins, force=True)
58     _worker_linter.load_plugin_configuration()
59 
60     if extra_packages_paths:
61         _augment_sys_path(extra_packages_paths)
62 
63 
64 def _worker_check_single_file(
65     file_item: FileItem,
66 ) -> tuple[
67     int,
68     str,
69     str,
70     str,
71     list[Message],
72     LinterStats,
73     int,
74     defaultdict[str, list[Any]],
75 ]:
76     if not _worker_linter:
77         raise RuntimeError("Worker linter not yet initialised")
78     _worker_linter.open()
79     _worker_linter.check_single_file_item(file_item)
80     mapreduce_data = defaultdict(list)
81     for checker in _worker_linter.get_checkers():
82         data = checker.get_map_data()
83         if data is not None:
84             mapreduce_data[checker.name].append(data)
85     msgs = _worker_linter.reporter.messages
86     assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)
87     _worker_linter.reporter.reset()
88     return (
89         id(multiprocessing.current_process()),
90         _worker_linter.current_name,
91         file_item.filepath,
92         _worker_linter.file_state.base_name,
93         msgs,
94         _worker_linter.stats,
95         _worker_linter.msg_status,
96         mapreduce_data,
97     )
98 
99 
100 def _merge_mapreduce_data(
101     linter: PyLinter,
102     all_mapreduce_data: defaultdict[int, list[defaultdict[str, list[Any]]]],
103 ) -> None:
104     """Merges map/reduce data across workers, invoking relevant APIs on checkers."""
105     # First collate the data and prepare it, so we can send it to the checkers for
106     # validation. The intent here is to collect all the mapreduce data for all checker-
107     # runs across processes - that will then be passed to a static method on the
108     # checkers to be reduced and further processed.
109     collated_map_reduce_data: defaultdict[str, list[Any]] = defaultdict(list)
110     for linter_data in all_mapreduce_data.values():
111         for run_data in linter_data:
112             for checker_name, data in run_data.items():
113                 collated_map_reduce_data[checker_name].extend(data)
114 
115     # Send the data to checkers that support/require consolidated data
116     original_checkers = linter.get_checkers()
117     for checker in original_checkers:
118         if checker.name in collated_map_reduce_data:
119             # Assume that if the check has returned map/reduce data that it has the
120             # reducer function
121             checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])
122 
123 
124 def check_parallel(
125     linter: PyLinter,
126     jobs: int,
127     files: Iterable[FileItem],
128     extra_packages_paths: Sequence[str] | None = None,
129 ) -> None:
130     """Use the given linter to lint the files with given amount of workers (jobs).
131 
132     This splits the work filestream-by-filestream. If you need to do work across
133     multiple files, as in the similarity-checker, then implement the map/reduce mixin functionality.
134     """
135     # The linter is inherited by all the pool's workers, i.e. the linter
136     # is identical to the linter object here. This is required so that
137     # a custom PyLinter object can be used.
138     initializer = functools.partial(
139         _worker_initialize, extra_packages_paths=extra_packages_paths
140     )
141     with ProcessPoolExecutor(
142         max_workers=jobs, initializer=initializer, initargs=(dill.dumps(linter),)
143     ) as executor:
144         linter.open()
145         all_stats = []
146         all_mapreduce_data: defaultdict[
147             int, list[defaultdict[str, list[Any]]]
148         ] = defaultdict(list)
149 
150         # Maps each file to be worked on by a single _worker_check_single_file() call,
151         # collecting any map/reduce data by checker module so that we can 'reduce' it
152         # later.
153         for (
154             worker_idx,  # used to merge map/reduce data across workers
155             module,
156             file_path,
157             base_name,
158             messages,
159             stats,
160             msg_status,
161             mapreduce_data,
162         ) in executor.map(_worker_check_single_file, files):
163             linter.file_state.base_name = base_name
164             linter.file_state._is_base_filestate = False
165             linter.set_current_module(module, file_path)
166             for msg in messages:
167                 linter.reporter.handle_message(msg)
168             all_stats.append(stats)
169             all_mapreduce_data[worker_idx].append(mapreduce_data)
170             linter.msg_status |= msg_status
171 
172     _merge_mapreduce_data(linter, all_mapreduce_data)
173     linter.stats = merge_stats([linter.stats, *all_stats])

</code><intended_behavior>Methods should ensure no results are lost during aggregation and that they handle the merging of results from parallel jobs in a manner that ensures the consistency and completeness of the linting report. This could include mechanisms to verify that all spawned jobs have successfully completed and reported back their findings before concluding the linting process.</intended_behavior>]
2025-10-15 09:33:20.789 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 09:33:20.789 | INFO     | Search-only: skipping patch selection
2025-10-15 09:33:20.877 | INFO     | Task pylint-dev__pylint-8757 completed successfully.
2025-10-15 09:33:20.878 | INFO     | No patch generated. You can try running ACR again.
