2025-10-15 03:56:49.663 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 03:56:52.652 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=178, output_tokens=12, cost=0.002140
2025-10-15 03:57:05.150 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=358, output_tokens=560, cost=0.020380
2025-10-15 03:57:06.314 | INFO     | registered non reproducing test 0
2025-10-15 03:57:21.018 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1695, output_tokens=690, cost=0.037650
2025-10-15 03:57:22.366 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 03:57:29.810 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=483, output_tokens=308, cost=0.014070
2025-10-15 03:57:29.816 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 03:57:32.827 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=662, output_tokens=57, cost=0.008330
2025-10-15 03:57:32.829 | DEBUG    | Extracted a valid json.
2025-10-15 03:57:32.829 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Exists\")",
        "search_method_in_class(\"filter\", \"QuerySet\")",
        "search_code(\"objects.none()\")",
        "search_method(\"get_compiler\")"
    ],
    "bug_locations": []
}
2025-10-15 03:57:32.894 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 03:57:32.894 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 03:57:51.957 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1437, output_tokens=940, cost=0.042570
2025-10-15 03:57:51.966 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 03:58:01.244 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2593, output_tokens=366, cost=0.036910
2025-10-15 03:58:01.252 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 03:58:10.983 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=720, output_tokens=362, cost=0.018060
2025-10-15 03:58:10.984 | DEBUG    | Extracted a valid json.
2025-10-15 03:58:10.984 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/expressions.py",
            "class": "Exists",
            "method": "as_sql",
            "intended_behavior": "The `as_sql` method should include logic to correctly handle empty querysets. When `Exists` is applied with an empty queryset, especially in negated form (using `~`), it should still contribute to generating the correct SQL `WHERE` clause without leading to an `EmptyResultSet`. This means ensuring that the SQL construction process accounts for empty querysets in a way that preserves the expected logical behavior of the `Exists` condition. Specifically, for an empty queryset, the `EXISTS` statement should translate to a condition that evaluates to false when not negated and true when negated, ensuring the `WHERE` block is retained in the SQL query."
        },
        {
            "file": "django/db/models/sql/query.py",
            "class": "Query",
            "method": "get_compiler",
            "intended_behavior": "The method should ensure that when compiling a query, the `elide_empty` flag does not lead to the unintentional removal of essential parts of the SQL query, such as the `WHERE` clause, simply because a part of the query involves an empty queryset. It should make a distinction between genuinely unnecessary parts of a query (like an empty `WHERE` clause that has no conditions) and parts that are critical for the logic of the query to be correct, such as conditions derived from empty `Exists` subqueries. This may involve refining the logic that decides what parts of a query are elided to better account for the logical implications of those parts on the overall query."
        }
    ]
}
2025-10-15 03:58:10.989 | DEBUG    | Found override of as_sql in Subquery
2025-10-15 03:58:10.989 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x74155fd22c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x74146d85f0e0>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x74146fb623e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x74146d85f0e0>, <app.raw_tasks.RawSweTask object at 0x74146d85f110>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x74146fb62480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x74146d796a80>
    └ <function run_task_in_subprocess at 0x74146fb62660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x74146d796a80>
    │        │      └ <function run_raw_task at 0x74146fb62700>
    │        └ <function ProcessPoolExecutor.submit at 0x74155f94a020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x74155f949da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x74155f949ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x74155f949f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x74155fb14ea0>
    └ <ForkProcess name='ForkProcess-71' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-71' parent=45407 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x74155fb860c0>)>
    │    │        └ <ForkProcess name='ForkProcess-71' parent=45407 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-71' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-71' parent=45407 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-71' parent=45407 started>
    │    └ <function Popen._launch at 0x74146db7f7e0>
    └ <multiprocessing.popen_fork.Popen object at 0x74146ee6e3f0>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x74155fb158a0>
           └ <ForkProcess name='ForkProcess-71' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x74155fb14e00>
    └ <ForkProcess name='ForkProcess-71' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-71' parent=45407 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x74146fd31280>, <multiprocessing.queues.SimpleQueue object at 0x74146e087b...
    │    │        └ <ForkProcess name='ForkProcess-71' parent=45407 started>
    │    └ <function _process_worker at 0x74155f9491c0>
    └ <ForkProcess name='ForkProcess-71' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x74155fdad640>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x74147096ec30>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x74155fdad640>
        │         └ <function run_raw_task at 0x74146fb62700>
        └ <concurrent.futures.process._CallItem object at 0x74155fdad640>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output/django__django-15382_2025-10-15_03-56-49'
             │            │    └ <function RawSweTask.to_task at 0x74146fb616c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x74147096ec30>
             └ <function do_inference at 0x74146fb628e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x74146fdf0400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7414702104a0>
       │                 │         │            └ SweTask(task_id='django__django-15382', problem_statement="filter on exists-subquery with empty queryset removes whole WHERE ...
       │                 │         └ <app.manage.ProjectApiManager object at 0x741470438170>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output/django__django-15382_2025-10-15_03-56-49/output_0')
       └ <function _run_one_task at 0x74146fdf18a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x74146fe9dbc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x74146e5ade20>
                                  └ <app.manage.ProjectApiManager object at 0x741470438170>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'django/db/models/expressions.py', 'class': 'Exists', 'method': 'as_sql', 'intended_behavior': 'The `as_sql` method ...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x74146fe9d8a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x74146e5aca70>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x74146e5ade20>
    │                 └ <method 'extend' of 'list' objects>
    └ []

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'as_sql'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/mo...
    │    │                      │   └ 'Exists'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/mo...
    │    └ <function SearchBackend._get_inherited_methods at 0x74146fe9d800>
    └ <app.search.search_backend.SearchBackend object at 0x74146e5aca70>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'Subquery', 'method_name': 'as_sql'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x74146fe9cf40>
                                  └ <app.search.search_backend.SearchBackend object at 0x74146e5aca70>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x74146e5aca70>, {'class_name': 'Subquery', 'method_name': 'as_sql'})
           └ <function SearchBackend.search_method_in_class at 0x74146fe9cea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-15 03:58:11.010 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>as_sql</method>
<code>
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params

</code><intended_behavior>The `as_sql` method should include logic to correctly handle empty querysets. When `Exists` is applied with an empty queryset, especially in negated form (using `~`), it should still contribute to generating the correct SQL `WHERE` clause without leading to an `EmptyResultSet`. This means ensuring that the SQL construction process accounts for empty querysets in a way that preserves the expected logical behavior of the `Exists` condition. Specifically, for an empty queryset, the `EXISTS` statement should translate to a condition that evaluates to false when not negated and true when negated, ensuring the `WHERE` block is retained in the SQL query.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1199 class Exists(Subquery):
1200     template = 'EXISTS(%(subquery)s)'
1201     output_field = fields.BooleanField()
1202 
1203     def __init__(self, queryset, negated=False, **kwargs):
1204         self.negated = negated
1205         super().__init__(queryset, **kwargs)
1206 
1207     def __invert__(self):
1208         clone = self.copy()
1209         clone.negated = not self.negated
1210         return clone
1211 
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params
1224 
1225     def select_format(self, compiler, sql, params):
1226         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1227         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1228         # BY list.
1229         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1230             sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
1231         return sql, params

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>get_compiler</method>
<code>
280     def get_compiler(self, using=None, connection=None, elide_empty=True):
281         if using is None and connection is None:
282             raise ValueError("Need either using or connection")
283         if using:
284             connection = connections[using]
285         return connection.ops.compiler(self.compiler)(self, connection, using, elide_empty)

</code><intended_behavior>The method should ensure that when compiling a query, the `elide_empty` flag does not lead to the unintentional removal of essential parts of the SQL query, such as the `WHERE` clause, simply because a part of the query involves an empty queryset. It should make a distinction between genuinely unnecessary parts of a query (like an empty `WHERE` clause that has no conditions) and parts that are critical for the logic of the query to be correct, such as conditions derived from empty `Exists` subqueries. This may involve refining the logic that decides what parts of a query are elided to better account for the logical implications of those parts on the overall query.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
142 class Query(BaseExpression):
143     """A single SQL query."""
144 
145     alias_prefix = 'T'
146     empty_result_set_value = None
147     subq_aliases = frozenset([alias_prefix])
148 
149     compiler = 'SQLCompiler'
150 
151     base_table_class = BaseTable
152     join_class = Join
153 
154     def __init__(self, model, alias_cols=True):
155         self.model = model
156         self.alias_refcount = {}
157         # alias_map is the most important data structure regarding joins.
158         # It's used for recording which joins exist in the query and what
159         # types they are. The key is the alias of the joined table (possibly
160         # the table name) and the value is a Join-like object (see
161         # sql.datastructures.Join for more information).
162         self.alias_map = {}
163         # Whether to provide alias to columns during reference resolving.
164         self.alias_cols = alias_cols
165         # Sometimes the query contains references to aliases in outer queries (as
166         # a result of split_exclude). Correct alias quoting needs to know these
167         # aliases too.
168         # Map external tables to whether they are aliased.
169         self.external_aliases = {}
170         self.table_map = {}     # Maps table names to list of aliases.
171         self.default_cols = True
172         self.default_ordering = True
173         self.standard_ordering = True
174         self.used_aliases = set()
175         self.filter_is_sticky = False
176         self.subquery = False
177 
178         # SQL-related attributes
179         # Select and related select clauses are expressions to use in the
180         # SELECT clause of the query.
181         # The select is used for cases where we want to set up the select
182         # clause to contain other than default fields (values(), subqueries...)
183         # Note that annotations go to annotations dictionary.
184         self.select = ()
185         self.where = WhereNode()
186         # The group_by attribute can have one of the following forms:
187         #  - None: no group by at all in the query
188         #  - A tuple of expressions: group by (at least) those expressions.
189         #    String refs are also allowed for now.
190         #  - True: group by all select fields of the model
191         # See compiler.get_group_by() for details.
192         self.group_by = None
193         self.order_by = ()
194         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
195         self.distinct = False
196         self.distinct_fields = ()
197         self.select_for_update = False
198         self.select_for_update_nowait = False
199         self.select_for_update_skip_locked = False
200         self.select_for_update_of = ()
201         self.select_for_no_key_update = False
202 
203         self.select_related = False
204         # Arbitrary limit for select_related to prevents infinite recursion.
205         self.max_depth = 5
206 
207         # Holds the selects defined by a call to values() or values_list()
208         # excluding annotation_select and extra_select.
209         self.values_select = ()
210 
211         # SQL annotation-related attributes
212         self.annotations = {}  # Maps alias -> Annotation Expression
213         self.annotation_select_mask = None
214         self._annotation_select_cache = None
215 
216         # Set combination attributes
217         self.combinator = None
218         self.combinator_all = False
219         self.combined_queries = ()
220 
221         # These are for extensions. The contents are more or less appended
222         # verbatim to the appropriate clause.
223         self.extra = {}  # Maps col_alias -> (col_sql, params).
224         self.extra_select_mask = None
225         self._extra_select_cache = None
226 
227         self.extra_tables = ()
228         self.extra_order_by = ()
229 
230         # A tuple that is a set of model field names and either True, if these
231         # are the fields to defer, or False if these are the only fields to
232         # load.
233         self.deferred_loading = (frozenset(), True)
234 
235         self._filtered_relations = {}
236 
237         self.explain_info = None
238 
239     @property
240     def output_field(self):
241         if len(self.select) == 1:
242             select = self.select[0]
243             return getattr(select, 'target', None) or select.field
244         elif len(self.annotation_select) == 1:
245             return next(iter(self.annotation_select.values())).output_field
246 
247     @property
248     def has_select_fields(self):
249         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)
250 
251     @cached_property
252     def base_table(self):
253         for alias in self.alias_map:
254             return alias
255 
256     def __str__(self):
257         """
258         Return the query as a string of SQL with the parameter values
259         substituted in (use sql_with_params() to see the unsubstituted string).
260 
261         Parameter values won't necessarily be quoted correctly, since that is
262         done by the database interface at execution time.
263         """
264         sql, params = self.sql_with_params()
265         return sql % params
266 
267     def sql_with_params(self):
268         """
269         Return the query as an SQL string and the parameters that will be
270         substituted into the query.
271         """
272         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
273 
274     def __deepcopy__(self, memo):
275         """Limit the amount of work when a Query is deepcopied."""
276         result = self.clone()
277         memo[id(self)] = result
278         return result
279 
280     def get_compiler(self, using=None, connection=None, elide_empty=True):
281         if using is None and connection is None:
282             raise ValueError("Need either using or connection")
283         if using:
284             connection = connections[using]
285         return connection.ops.compiler(self.compiler)(self, connection, using, elide_empty)
286 
287     def get_meta(self):
288         """
289         Return the Options instance (the model._meta) from which to start
290         processing. Normally, this is self.model._meta, but it can be changed
291         by subclasses.
292         """
293         return self.model._meta
294 
295     def clone(self):
296         """
297         Return a copy of the current Query. A lightweight alternative to
298         to deepcopy().
299         """
300         obj = Empty()
301         obj.__class__ = self.__class__
302         # Copy references to everything.
303         obj.__dict__ = self.__dict__.copy()
304         # Clone attributes that can't use shallow copy.
305         obj.alias_refcount = self.alias_refcount.copy()
306         obj.alias_map = self.alias_map.copy()
307         obj.external_aliases = self.external_aliases.copy()
308         obj.table_map = self.table_map.copy()
309         obj.where = self.where.clone()
310         obj.annotations = self.annotations.copy()
311         if self.annotation_select_mask is not None:
312             obj.annotation_select_mask = self.annotation_select_mask.copy()
313         if self.combined_queries:
314             obj.combined_queries = tuple([
315                 query.clone() for query in self.combined_queries
316             ])
317         # _annotation_select_cache cannot be copied, as doing so breaks the
318         # (necessary) state in which both annotations and
319         # _annotation_select_cache point to the same underlying objects.
320         # It will get re-populated in the cloned queryset the next time it's
321         # used.
322         obj._annotation_select_cache = None
323         obj.extra = self.extra.copy()
324         if self.extra_select_mask is not None:
325             obj.extra_select_mask = self.extra_select_mask.copy()
326         if self._extra_select_cache is not None:
327             obj._extra_select_cache = self._extra_select_cache.copy()
328         if self.select_related is not False:
329             # Use deepcopy because select_related stores fields in nested
330             # dicts.
331             obj.select_related = copy.deepcopy(obj.select_related)
332         if 'subq_aliases' in self.__dict__:
333             obj.subq_aliases = self.subq_aliases.copy()
334         obj.used_aliases = self.used_aliases.copy()
335         obj._filtered_relations = self._filtered_relations.copy()
336         # Clear the cached_property
337         try:
338             del obj.base_table
339         except AttributeError:
340             pass
341         return obj
342 
343     def chain(self, klass=None):
344         """
345         Return a copy of the current Query that's ready for another operation.
346         The klass argument changes the type of the Query, e.g. UpdateQuery.
347         """
348         obj = self.clone()
349         if klass and obj.__class__ != klass:
350             obj.__class__ = klass
351         if not obj.filter_is_sticky:
352             obj.used_aliases = set()
353         obj.filter_is_sticky = False
354         if hasattr(obj, '_setup_query'):
355             obj._setup_query()
356         return obj
357 
358     def relabeled_clone(self, change_map):
359         clone = self.clone()
360         clone.change_aliases(change_map)
361         return clone
362 
363     def _get_col(self, target, field, alias):
364         if not self.alias_cols:
365             alias = None
366         return target.get_col(alias, field)
367 
368     def rewrite_cols(self, annotation, col_cnt):
369         # We must make sure the inner query has the referred columns in it.
370         # If we are aggregating over an annotation, then Django uses Ref()
371         # instances to note this. However, if we are annotating over a column
372         # of a related model, then it might be that column isn't part of the
373         # SELECT clause of the inner query, and we must manually make sure
374         # the column is selected. An example case is:
375         #    .aggregate(Sum('author__awards'))
376         # Resolving this expression results in a join to author, but there
377         # is no guarantee the awards column of author is in the select clause
378         # of the query. Thus we must manually add the column to the inner
379         # query.
380         orig_exprs = annotation.get_source_expressions()
381         new_exprs = []
382         for expr in orig_exprs:
383             # FIXME: These conditions are fairly arbitrary. Identify a better
384             # method of having expressions decide which code path they should
385             # take.
386             if isinstance(expr, Ref):
387                 # Its already a Ref to subquery (see resolve_ref() for
388                 # details)
389                 new_exprs.append(expr)
390             elif isinstance(expr, (WhereNode, Lookup)):
391                 # Decompose the subexpressions further. The code here is
392                 # copied from the else clause, but this condition must appear
393                 # before the contains_aggregate/is_summary condition below.
394                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
395                 new_exprs.append(new_expr)
396             else:
397                 # Reuse aliases of expressions already selected in subquery.
398                 for col_alias, selected_annotation in self.annotation_select.items():
399                     if selected_annotation is expr:
400                         new_expr = Ref(col_alias, expr)
401                         break
402                 else:
403                     # An expression that is not selected the subquery.
404                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
405                         # Reference column or another aggregate. Select it
406                         # under a non-conflicting alias.
407                         col_cnt += 1
408                         col_alias = '__col%d' % col_cnt
409                         self.annotations[col_alias] = expr
410                         self.append_annotation_mask([col_alias])
411                         new_expr = Ref(col_alias, expr)
412                     else:
413                         # Some other expression not referencing database values
414                         # directly. Its subexpression might contain Cols.
415                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
416                 new_exprs.append(new_expr)
417         annotation.set_source_expressions(new_exprs)
418         return annotation, col_cnt
419 
420     def get_aggregation(self, using, added_aggregate_names):
421         """
422         Return the dictionary with the values of the existing aggregations.
423         """
424         if not self.annotation_select:
425             return {}
426         existing_annotations = [
427             annotation for alias, annotation
428             in self.annotations.items()
429             if alias not in added_aggregate_names
430         ]
431         # Decide if we need to use a subquery.
432         #
433         # Existing annotations would cause incorrect results as get_aggregation()
434         # must produce just one result and thus must not use GROUP BY. But we
435         # aren't smart enough to remove the existing annotations from the
436         # query, so those would force us to use GROUP BY.
437         #
438         # If the query has limit or distinct, or uses set operations, then
439         # those operations must be done in a subquery so that the query
440         # aggregates on the limit and/or distinct results instead of applying
441         # the distinct and limit after the aggregation.
442         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or
443                 self.distinct or self.combinator):
444             from django.db.models.sql.subqueries import AggregateQuery
445             inner_query = self.clone()
446             inner_query.subquery = True
447             outer_query = AggregateQuery(self.model, inner_query)
448             inner_query.select_for_update = False
449             inner_query.select_related = False
450             inner_query.set_annotation_mask(self.annotation_select)
451             # Queries with distinct_fields need ordering and when a limit is
452             # applied we must take the slice from the ordered query. Otherwise
453             # no need for ordering.
454             inner_query.clear_ordering(force=False)
455             if not inner_query.distinct:
456                 # If the inner query uses default select and it has some
457                 # aggregate annotations, then we must make sure the inner
458                 # query is grouped by the main model's primary key. However,
459                 # clearing the select clause can alter results if distinct is
460                 # used.
461                 has_existing_aggregate_annotations = any(
462                     annotation for annotation in existing_annotations
463                     if getattr(annotation, 'contains_aggregate', True)
464                 )
465                 if inner_query.default_cols and has_existing_aggregate_annotations:
466                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
467                 inner_query.default_cols = False
468 
469             relabels = {t: 'subquery' for t in inner_query.alias_map}
470             relabels[None] = 'subquery'
471             # Remove any aggregates marked for reduction from the subquery
472             # and move them to the outer AggregateQuery.
473             col_cnt = 0
474             for alias, expression in list(inner_query.annotation_select.items()):
475                 annotation_select_mask = inner_query.annotation_select_mask
476                 if expression.is_summary:
477                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
478                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
479                     del inner_query.annotations[alias]
480                     annotation_select_mask.remove(alias)
481                 # Make sure the annotation_select wont use cached results.
482                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
483             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:
484                 # In case of Model.objects[0:3].count(), there would be no
485                 # field selected in the inner query, yet we must use a subquery.
486                 # So, make sure at least one field is selected.
487                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
488         else:
489             outer_query = self
490             self.select = ()
491             self.default_cols = False
492             self.extra = {}
493 
494         empty_set_result = [
495             expression.empty_result_set_value
496             for expression in outer_query.annotation_select.values()
497         ]
498         elide_empty = not any(result is NotImplemented for result in empty_set_result)
499         outer_query.clear_ordering(force=True)
500         outer_query.clear_limits()
501         outer_query.select_for_update = False
502         outer_query.select_related = False
503         compiler = outer_query.get_compiler(using, elide_empty=elide_empty)
504         result = compiler.execute_sql(SINGLE)
505         if result is None:
506             result = empty_set_result
507 
508         converters = compiler.get_converters(outer_query.annotation_select.values())
509         result = next(compiler.apply_converters((result,), converters))
510 
511         return dict(zip(outer_query.annotation_select, result))
512 
513     def get_count(self, using):
514         """
515         Perform a COUNT() query using the current filter constraints.
516         """
517         obj = self.clone()
518         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
519         return obj.get_aggregation(using, ['__count'])['__count']
520 
521     def has_filters(self):
522         return self.where
523 
524     def exists(self, using, limit=True):
525         q = self.clone()
526         if not q.distinct:
527             if q.group_by is True:
528                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
529                 # Disable GROUP BY aliases to avoid orphaning references to the
530                 # SELECT clause which is about to be cleared.
531                 q.set_group_by(allow_aliases=False)
532             q.clear_select_clause()
533         if q.combined_queries and q.combinator == 'union':
534             limit_combined = connections[using].features.supports_slicing_ordering_in_compound
535             q.combined_queries = tuple(
536                 combined_query.exists(using, limit=limit_combined)
537                 for combined_query in q.combined_queries
538             )
539         q.clear_ordering(force=True)
540         if limit:
541             q.set_limits(high=1)
542         q.add_extra({'a': 1}, None, None, None, None, None)
543         q.set_extra_mask(['a'])
544         return q
545 
546     def has_results(self, using):
547         q = self.exists(using)
548         compiler = q.get_compiler(using=using)
549         return compiler.has_results()
550 
551     def explain(self, using, format=None, **options):
552         q = self.clone()
553         q.explain_info = ExplainInfo(format, options)
554         compiler = q.get_compiler(using=using)
555         return '\n'.join(compiler.explain_query())
556 
557     def combine(self, rhs, connector):
558         """
559         Merge the 'rhs' query into the current one (with any 'rhs' effects
560         being applied *after* (that is, "to the right of") anything in the
561         current query. 'rhs' is not modified during a call to this function.
562 
563         The 'connector' parameter describes how to connect filters from the
564         'rhs' query.
565         """
566         if self.model != rhs.model:
567             raise TypeError('Cannot combine queries on two different base models.')
568         if self.is_sliced:
569             raise TypeError('Cannot combine queries once a slice has been taken.')
570         if self.distinct != rhs.distinct:
571             raise TypeError('Cannot combine a unique query with a non-unique query.')
572         if self.distinct_fields != rhs.distinct_fields:
573             raise TypeError('Cannot combine queries with different distinct fields.')
574 
575         # If lhs and rhs shares the same alias prefix, it is possible to have
576         # conflicting alias changes like T4 -> T5, T5 -> T6, which might end up
577         # as T4 -> T6 while combining two querysets. To prevent this, change an
578         # alias prefix of the rhs and update current aliases accordingly,
579         # except if the alias is the base table since it must be present in the
580         # query on both sides.
581         initial_alias = self.get_initial_alias()
582         rhs.bump_prefix(self, exclude={initial_alias})
583 
584         # Work out how to relabel the rhs aliases, if necessary.
585         change_map = {}
586         conjunction = (connector == AND)
587 
588         # Determine which existing joins can be reused. When combining the
589         # query with AND we must recreate all joins for m2m filters. When
590         # combining with OR we can reuse joins. The reason is that in AND
591         # case a single row can't fulfill a condition like:
592         #     revrel__col=1 & revrel__col=2
593         # But, there might be two different related rows matching this
594         # condition. In OR case a single True is enough, so single row is
595         # enough, too.
596         #
597         # Note that we will be creating duplicate joins for non-m2m joins in
598         # the AND case. The results will be correct but this creates too many
599         # joins. This is something that could be fixed later on.
600         reuse = set() if conjunction else set(self.alias_map)
601         joinpromoter = JoinPromoter(connector, 2, False)
602         joinpromoter.add_votes(
603             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
604         rhs_votes = set()
605         # Now, add the joins from rhs query into the new query (skipping base
606         # table).
607         rhs_tables = list(rhs.alias_map)[1:]
608         for alias in rhs_tables:
609             join = rhs.alias_map[alias]
610             # If the left side of the join was already relabeled, use the
611             # updated alias.
612             join = join.relabeled_clone(change_map)
613             new_alias = self.join(join, reuse=reuse)
614             if join.join_type == INNER:
615                 rhs_votes.add(new_alias)
616             # We can't reuse the same join again in the query. If we have two
617             # distinct joins for the same connection in rhs query, then the
618             # combined query must have two joins, too.
619             reuse.discard(new_alias)
620             if alias != new_alias:
621                 change_map[alias] = new_alias
622             if not rhs.alias_refcount[alias]:
623                 # The alias was unused in the rhs query. Unref it so that it
624                 # will be unused in the new query, too. We have to add and
625                 # unref the alias so that join promotion has information of
626                 # the join type for the unused alias.
627                 self.unref_alias(new_alias)
628         joinpromoter.add_votes(rhs_votes)
629         joinpromoter.update_join_types(self)
630 
631         # Combine subqueries aliases to ensure aliases relabelling properly
632         # handle subqueries when combining where and select clauses.
633         self.subq_aliases |= rhs.subq_aliases
634 
635         # Now relabel a copy of the rhs where-clause and add it to the current
636         # one.
637         w = rhs.where.clone()
638         w.relabel_aliases(change_map)
639         self.where.add(w, connector)
640 
641         # Selection columns and extra extensions are those provided by 'rhs'.
642         if rhs.select:
643             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])
644         else:
645             self.select = ()
646 
647         if connector == OR:
648             # It would be nice to be able to handle this, but the queries don't
649             # really make sense (or return consistent value sets). Not worth
650             # the extra complexity when you can write a real query instead.
651             if self.extra and rhs.extra:
652                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
653         self.extra.update(rhs.extra)
654         extra_select_mask = set()
655         if self.extra_select_mask is not None:
656             extra_select_mask.update(self.extra_select_mask)
657         if rhs.extra_select_mask is not None:
658             extra_select_mask.update(rhs.extra_select_mask)
659         if extra_select_mask:
660             self.set_extra_mask(extra_select_mask)
661         self.extra_tables += rhs.extra_tables
662 
663         # Ordering uses the 'rhs' ordering, unless it has none, in which case
664         # the current ordering is used.
665         self.order_by = rhs.order_by or self.order_by
666         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
667 
668     def deferred_to_data(self, target, callback):
669         """
670         Convert the self.deferred_loading data structure to an alternate data
671         structure, describing the field that *will* be loaded. This is used to
672         compute the columns to select from the database and also by the
673         QuerySet class to work out which fields are being initialized on each
674         model. Models that have all their fields included aren't mentioned in
675         the result, only those that have field restrictions in place.
676 
677         The "target" parameter is the instance that is populated (in place).
678         The "callback" is a function that is called whenever a (model, field)
679         pair need to be added to "target". It accepts three parameters:
680         "target", and the model and list of fields being added for that model.
681         """
682         field_names, defer = self.deferred_loading
683         if not field_names:
684             return
685         orig_opts = self.get_meta()
686         seen = {}
687         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
688         for field_name in field_names:
689             parts = field_name.split(LOOKUP_SEP)
690             cur_model = self.model._meta.concrete_model
691             opts = orig_opts
692             for name in parts[:-1]:
693                 old_model = cur_model
694                 if name in self._filtered_relations:
695                     name = self._filtered_relations[name].relation_name
696                 source = opts.get_field(name)
697                 if is_reverse_o2o(source):
698                     cur_model = source.related_model
699                 else:
700                     cur_model = source.remote_field.model
701                 opts = cur_model._meta
702                 # Even if we're "just passing through" this model, we must add
703                 # both the current model's pk and the related reference field
704                 # (if it's not a reverse relation) to the things we select.
705                 if not is_reverse_o2o(source):
706                     must_include[old_model].add(source)
707                 add_to_dict(must_include, cur_model, opts.pk)
708             field = opts.get_field(parts[-1])
709             is_reverse_object = field.auto_created and not field.concrete
710             model = field.related_model if is_reverse_object else field.model
711             model = model._meta.concrete_model
712             if model == opts.model:
713                 model = cur_model
714             if not is_reverse_o2o(field):
715                 add_to_dict(seen, model, field)
716 
717         if defer:
718             # We need to load all fields for each model, except those that
719             # appear in "seen" (for all models that appear in "seen"). The only
720             # slight complexity here is handling fields that exist on parent
721             # models.
722             workset = {}
723             for model, values in seen.items():
724                 for field in model._meta.local_fields:
725                     if field not in values:
726                         m = field.model._meta.concrete_model
727                         add_to_dict(workset, m, field)
728             for model, values in must_include.items():
729                 # If we haven't included a model in workset, we don't add the
730                 # corresponding must_include fields for that model, since an
731                 # empty set means "include all fields". That's why there's no
732                 # "else" branch here.
733                 if model in workset:
734                     workset[model].update(values)
735             for model, values in workset.items():
736                 callback(target, model, values)
737         else:
738             for model, values in must_include.items():
739                 if model in seen:
740                     seen[model].update(values)
741                 else:
742                     # As we've passed through this model, but not explicitly
743                     # included any fields, we have to make sure it's mentioned
744                     # so that only the "must include" fields are pulled in.
745                     seen[model] = values
746             # Now ensure that every model in the inheritance chain is mentioned
747             # in the parent list. Again, it must be mentioned to ensure that
748             # only "must include" fields are pulled in.
749             for model in orig_opts.get_parent_list():
750                 seen.setdefault(model, set())
751             for model, values in seen.items():
752                 callback(target, model, values)
753 
754     def table_alias(self, table_name, create=False, filtered_relation=None):
755         """
756         Return a table alias for the given table_name and whether this is a
757         new alias or not.
758 
759         If 'create' is true, a new alias is always created. Otherwise, the
760         most recently created alias for the table (if one exists) is reused.
761         """
762         alias_list = self.table_map.get(table_name)
763         if not create and alias_list:
764             alias = alias_list[0]
765             self.alias_refcount[alias] += 1
766             return alias, False
767 
768         # Create a new alias for this table.
769         if alias_list:
770             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
771             alias_list.append(alias)
772         else:
773             # The first occurrence of a table uses the table name directly.
774             alias = filtered_relation.alias if filtered_relation is not None else table_name
775             self.table_map[table_name] = [alias]
776         self.alias_refcount[alias] = 1
777         return alias, True
778 
779     def ref_alias(self, alias):
780         """Increases the reference count for this alias."""
781         self.alias_refcount[alias] += 1
782 
783     def unref_alias(self, alias, amount=1):
784         """Decreases the reference count for this alias."""
785         self.alias_refcount[alias] -= amount
786 
787     def promote_joins(self, aliases):
788         """
789         Promote recursively the join type of given aliases and its children to
790         an outer join. If 'unconditional' is False, only promote the join if
791         it is nullable or the parent join is an outer join.
792 
793         The children promotion is done to avoid join chains that contain a LOUTER
794         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
795         then we must also promote b->c automatically, or otherwise the promotion
796         of a->b doesn't actually change anything in the query results.
797         """
798         aliases = list(aliases)
799         while aliases:
800             alias = aliases.pop(0)
801             if self.alias_map[alias].join_type is None:
802                 # This is the base table (first FROM entry) - this table
803                 # isn't really joined at all in the query, so we should not
804                 # alter its join type.
805                 continue
806             # Only the first alias (skipped above) should have None join_type
807             assert self.alias_map[alias].join_type is not None
808             parent_alias = self.alias_map[alias].parent_alias
809             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
810             already_louter = self.alias_map[alias].join_type == LOUTER
811             if ((self.alias_map[alias].nullable or parent_louter) and
812                     not already_louter):
813                 self.alias_map[alias] = self.alias_map[alias].promote()
814                 # Join type of 'alias' changed, so re-examine all aliases that
815                 # refer to this one.
816                 aliases.extend(
817                     join for join in self.alias_map
818                     if self.alias_map[join].parent_alias == alias and join not in aliases
819                 )
820 
821     def demote_joins(self, aliases):
822         """
823         Change join type from LOUTER to INNER for all joins in aliases.
824 
825         Similarly to promote_joins(), this method must ensure no join chains
826         containing first an outer, then an inner join are generated. If we
827         are demoting b->c join in chain a LOUTER b LOUTER c then we must
828         demote a->b automatically, or otherwise the demotion of b->c doesn't
829         actually change anything in the query results. .
830         """
831         aliases = list(aliases)
832         while aliases:
833             alias = aliases.pop(0)
834             if self.alias_map[alias].join_type == LOUTER:
835                 self.alias_map[alias] = self.alias_map[alias].demote()
836                 parent_alias = self.alias_map[alias].parent_alias
837                 if self.alias_map[parent_alias].join_type == INNER:
838                     aliases.append(parent_alias)
839 
840     def reset_refcounts(self, to_counts):
841         """
842         Reset reference counts for aliases so that they match the value passed
843         in `to_counts`.
844         """
845         for alias, cur_refcount in self.alias_refcount.copy().items():
846             unref_amount = cur_refcount - to_counts.get(alias, 0)
847             self.unref_alias(alias, unref_amount)
848 
849     def change_aliases(self, change_map):
850         """
851         Change the aliases in change_map (which maps old-alias -> new-alias),
852         relabelling any references to them in select columns and the where
853         clause.
854         """
855         # If keys and values of change_map were to intersect, an alias might be
856         # updated twice (e.g. T4 -> T5, T5 -> T6, so also T4 -> T6) depending
857         # on their order in change_map.
858         assert set(change_map).isdisjoint(change_map.values())
859 
860         # 1. Update references in "select" (normal columns plus aliases),
861         # "group by" and "where".
862         self.where.relabel_aliases(change_map)
863         if isinstance(self.group_by, tuple):
864             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])
865         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])
866         self.annotations = self.annotations and {
867             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()
868         }
869 
870         # 2. Rename the alias in the internal table/alias datastructures.
871         for old_alias, new_alias in change_map.items():
872             if old_alias not in self.alias_map:
873                 continue
874             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
875             self.alias_map[new_alias] = alias_data
876             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
877             del self.alias_refcount[old_alias]
878             del self.alias_map[old_alias]
879 
880             table_aliases = self.table_map[alias_data.table_name]
881             for pos, alias in enumerate(table_aliases):
882                 if alias == old_alias:
883                     table_aliases[pos] = new_alias
884                     break
885         self.external_aliases = {
886             # Table is aliased or it's being changed and thus is aliased.
887             change_map.get(alias, alias): (aliased or alias in change_map)
888             for alias, aliased in self.external_aliases.items()
889         }
890 
891     def bump_prefix(self, other_query, exclude=None):
892         """
893         Change the alias prefix to the next letter in the alphabet in a way
894         that the other query's aliases and this query's aliases will not
895         conflict. Even tables that previously had no alias will get an alias
896         after this call. To prevent changing aliases use the exclude parameter.
897         """
898         def prefix_gen():
899             """
900             Generate a sequence of characters in alphabetical order:
901                 -> 'A', 'B', 'C', ...
902 
903             When the alphabet is finished, the sequence will continue with the
904             Cartesian product:
905                 -> 'AA', 'AB', 'AC', ...
906             """
907             alphabet = ascii_uppercase
908             prefix = chr(ord(self.alias_prefix) + 1)
909             yield prefix
910             for n in count(1):
911                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
912                 for s in product(seq, repeat=n):
913                     yield ''.join(s)
914                 prefix = None
915 
916         if self.alias_prefix != other_query.alias_prefix:
917             # No clashes between self and outer query should be possible.
918             return
919 
920         # Explicitly avoid infinite loop. The constant divider is based on how
921         # much depth recursive subquery references add to the stack. This value
922         # might need to be adjusted when adding or removing function calls from
923         # the code path in charge of performing these operations.
924         local_recursion_limit = sys.getrecursionlimit() // 16
925         for pos, prefix in enumerate(prefix_gen()):
926             if prefix not in self.subq_aliases:
927                 self.alias_prefix = prefix
928                 break
929             if pos > local_recursion_limit:
930                 raise RecursionError(
931                     'Maximum recursion depth exceeded: too many subqueries.'
932                 )
933         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
934         other_query.subq_aliases = other_query.subq_aliases.union(self.subq_aliases)
935         if exclude is None:
936             exclude = {}
937         self.change_aliases({
938             alias: '%s%d' % (self.alias_prefix, pos)
939             for pos, alias in enumerate(self.alias_map)
940             if alias not in exclude
941         })
942 
943     def get_initial_alias(self):
944         """
945         Return the first alias for this query, after increasing its reference
946         count.
947         """
948         if self.alias_map:
949             alias = self.base_table
950             self.ref_alias(alias)
951         else:
952             alias = self.join(self.base_table_class(self.get_meta().db_table, None))
953         return alias
954 
955     def count_active_tables(self):
956         """
957         Return the number of tables in this query with a non-zero reference
958         count. After execution, the reference counts are zeroed, so tables
959         added in compiler will not be seen by this method.
960         """
961         return len([1 for count in self.alias_refcount.values() if count])
962 
963     def join(self, join, reuse=None):
964         """
965         Return an alias for the 'join', either reusing an existing alias for
966         that join or creating a new one. 'join' is either a base_table_class or
967         join_class.
968 
969         The 'reuse' parameter can be either None which means all joins are
970         reusable, or it can be a set containing the aliases that can be reused.
971 
972         A join is always created as LOUTER if the lhs alias is LOUTER to make
973         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
974         joins are created as LOUTER if the join is nullable.
975         """
976         reuse_aliases = [
977             a for a, j in self.alias_map.items()
978             if (reuse is None or a in reuse) and j.equals(join)
979         ]
980         if reuse_aliases:
981             if join.table_alias in reuse_aliases:
982                 reuse_alias = join.table_alias
983             else:
984                 # Reuse the most recent alias of the joined table
985                 # (a many-to-many relation may be joined multiple times).
986                 reuse_alias = reuse_aliases[-1]
987             self.ref_alias(reuse_alias)
988             return reuse_alias
989 
990         # No reuse is possible, so we need a new alias.
991         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)
992         if join.join_type:
993             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
994                 join_type = LOUTER
995             else:
996                 join_type = INNER
997             join.join_type = join_type
998         join.table_alias = alias
999         self.alias_map[alias] = join
1000         return alias
1001 
1002     def join_parent_model(self, opts, model, alias, seen):
1003         """
1004         Make sure the given 'model' is joined in the query. If 'model' isn't
1005         a parent of 'opts' or if it is None this method is a no-op.
1006 
1007         The 'alias' is the root alias for starting the join, 'seen' is a dict
1008         of model -> alias of existing joins. It must also contain a mapping
1009         of None -> some alias. This will be returned in the no-op case.
1010         """
1011         if model in seen:
1012             return seen[model]
1013         chain = opts.get_base_chain(model)
1014         if not chain:
1015             return alias
1016         curr_opts = opts
1017         for int_model in chain:
1018             if int_model in seen:
1019                 curr_opts = int_model._meta
1020                 alias = seen[int_model]
1021                 continue
1022             # Proxy model have elements in base chain
1023             # with no parents, assign the new options
1024             # object and skip to the next base in that
1025             # case
1026             if not curr_opts.parents[int_model]:
1027                 curr_opts = int_model._meta
1028                 continue
1029             link_field = curr_opts.get_ancestor_link(int_model)
1030             join_info = self.setup_joins([link_field.name], curr_opts, alias)
1031             curr_opts = int_model._meta
1032             alias = seen[int_model] = join_info.joins[-1]
1033         return alias or seen[None]
1034 
1035     def add_annotation(self, annotation, alias, is_summary=False, select=True):
1036         """Add a single annotation expression to the Query."""
1037         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
1038                                                    summarize=is_summary)
1039         if select:
1040             self.append_annotation_mask([alias])
1041         else:
1042             self.set_annotation_mask(set(self.annotation_select).difference({alias}))
1043         self.annotations[alias] = annotation
1044 
1045     def resolve_expression(self, query, *args, **kwargs):
1046         clone = self.clone()
1047         # Subqueries need to use a different set of aliases than the outer query.
1048         clone.bump_prefix(query)
1049         clone.subquery = True
1050         clone.where.resolve_expression(query, *args, **kwargs)
1051         # Resolve combined queries.
1052         if clone.combinator:
1053             clone.combined_queries = tuple([
1054                 combined_query.resolve_expression(query, *args, **kwargs)
1055                 for combined_query in clone.combined_queries
1056             ])
1057         for key, value in clone.annotations.items():
1058             resolved = value.resolve_expression(query, *args, **kwargs)
1059             if hasattr(resolved, 'external_aliases'):
1060                 resolved.external_aliases.update(clone.external_aliases)
1061             clone.annotations[key] = resolved
1062         # Outer query's aliases are considered external.
1063         for alias, table in query.alias_map.items():
1064             clone.external_aliases[alias] = (
1065                 (isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias) or
1066                 (isinstance(table, BaseTable) and table.table_name != table.table_alias)
1067             )
1068         return clone
1069 
1070     def get_external_cols(self):
1071         exprs = chain(self.annotations.values(), self.where.children)
1072         return [
1073             col for col in self._gen_cols(exprs, include_external=True)
1074             if col.alias in self.external_aliases
1075         ]
1076 
1077     def get_group_by_cols(self, alias=None):
1078         if alias:
1079             return [Ref(alias, self)]
1080         external_cols = self.get_external_cols()
1081         if any(col.possibly_multivalued for col in external_cols):
1082             return [self]
1083         return external_cols
1084 
1085     def as_sql(self, compiler, connection):
1086         # Some backends (e.g. Oracle) raise an error when a subquery contains
1087         # unnecessary ORDER BY clause.
1088         if (
1089             self.subquery and
1090             not connection.features.ignores_unnecessary_order_by_in_subqueries
1091         ):
1092             self.clear_ordering(force=False)
1093         sql, params = self.get_compiler(connection=connection).as_sql()
1094         if self.subquery:
1095             sql = '(%s)' % sql
1096         return sql, params
1097 
1098     def resolve_lookup_value(self, value, can_reuse, allow_joins):
1099         if hasattr(value, 'resolve_expression'):
1100             value = value.resolve_expression(
1101                 self, reuse=can_reuse, allow_joins=allow_joins,
1102             )
1103         elif isinstance(value, (list, tuple)):
1104             # The items of the iterable may be expressions and therefore need
1105             # to be resolved independently.
1106             values = (
1107                 self.resolve_lookup_value(sub_value, can_reuse, allow_joins)
1108                 for sub_value in value
1109             )
1110             type_ = type(value)
1111             if hasattr(type_, '_make'):  # namedtuple
1112                 return type_(*values)
1113             return type_(values)
1114         return value
1115 
1116     def solve_lookup_type(self, lookup):
1117         """
1118         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').
1119         """
1120         lookup_splitted = lookup.split(LOOKUP_SEP)
1121         if self.annotations:
1122             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1123             if expression:
1124                 return expression_lookups, (), expression
1125         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1126         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1127         if len(lookup_parts) > 1 and not field_parts:
1128             raise FieldError(
1129                 'Invalid lookup "%s" for model %s".' %
1130                 (lookup, self.get_meta().model.__name__)
1131             )
1132         return lookup_parts, field_parts, False
1133 
1134     def check_query_object_type(self, value, opts, field):
1135         """
1136         Check whether the object passed while querying is of the correct type.
1137         If not, raise a ValueError specifying the wrong object.
1138         """
1139         if hasattr(value, '_meta'):
1140             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1141                 raise ValueError(
1142                     'Cannot query "%s": Must be "%s" instance.' %
1143                     (value, opts.object_name))
1144 
1145     def check_related_objects(self, field, value, opts):
1146         """Check the type of object passed to query relations."""
1147         if field.is_relation:
1148             # Check that the field and the queryset use the same model in a
1149             # query like .filter(author=Author.objects.all()). For example, the
1150             # opts would be Author's (from the author field) and value.model
1151             # would be Author.objects.all() queryset's .model (Author also).
1152             # The field is the related field on the lhs side.
1153             if (isinstance(value, Query) and not value.has_select_fields and
1154                     not check_rel_lookup_compatibility(value.model, opts, field)):
1155                 raise ValueError(
1156                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1157                     (value.model._meta.object_name, opts.object_name)
1158                 )
1159             elif hasattr(value, '_meta'):
1160                 self.check_query_object_type(value, opts, field)
1161             elif hasattr(value, '__iter__'):
1162                 for v in value:
1163                     self.check_query_object_type(v, opts, field)
1164 
1165     def check_filterable(self, expression):
1166         """Raise an error if expression cannot be used in a WHERE clause."""
1167         if (
1168             hasattr(expression, 'resolve_expression') and
1169             not getattr(expression, 'filterable', True)
1170         ):
1171             raise NotSupportedError(
1172                 expression.__class__.__name__ + ' is disallowed in the filter '
1173                 'clause.'
1174             )
1175         if hasattr(expression, 'get_source_expressions'):
1176             for expr in expression.get_source_expressions():
1177                 self.check_filterable(expr)
1178 
1179     def build_lookup(self, lookups, lhs, rhs):
1180         """
1181         Try to extract transforms and lookup from given lhs.
1182 
1183         The lhs value is something that works like SQLExpression.
1184         The rhs value is what the lookup is going to compare against.
1185         The lookups is a list of names to extract using get_lookup()
1186         and get_transform().
1187         """
1188         # __exact is the default lookup if one isn't given.
1189         *transforms, lookup_name = lookups or ['exact']
1190         for name in transforms:
1191             lhs = self.try_transform(lhs, name)
1192         # First try get_lookup() so that the lookup takes precedence if the lhs
1193         # supports both transform and lookup for the name.
1194         lookup_class = lhs.get_lookup(lookup_name)
1195         if not lookup_class:
1196             if lhs.field.is_relation:
1197                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))
1198             # A lookup wasn't found. Try to interpret the name as a transform
1199             # and do an Exact lookup against it.
1200             lhs = self.try_transform(lhs, lookup_name)
1201             lookup_name = 'exact'
1202             lookup_class = lhs.get_lookup(lookup_name)
1203             if not lookup_class:
1204                 return
1205 
1206         lookup = lookup_class(lhs, rhs)
1207         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
1208         # uses of None as a query value unless the lookup supports it.
1209         if lookup.rhs is None and not lookup.can_use_none_as_rhs:
1210             if lookup_name not in ('exact', 'iexact'):
1211                 raise ValueError("Cannot use None as a query value")
1212             return lhs.get_lookup('isnull')(lhs, True)
1213 
1214         # For Oracle '' is equivalent to null. The check must be done at this
1215         # stage because join promotion can't be done in the compiler. Using
1216         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.
1217         # A similar thing is done in is_nullable(), too.
1218         if (
1219             lookup_name == 'exact' and
1220             lookup.rhs == '' and
1221             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls
1222         ):
1223             return lhs.get_lookup('isnull')(lhs, True)
1224 
1225         return lookup
1226 
1227     def try_transform(self, lhs, name):
1228         """
1229         Helper method for build_lookup(). Try to fetch and initialize
1230         a transform for name parameter from lhs.
1231         """
1232         transform_class = lhs.get_transform(name)
1233         if transform_class:
1234             return transform_class(lhs)
1235         else:
1236             output_field = lhs.output_field.__class__
1237             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())
1238             if suggested_lookups:
1239                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)
1240             else:
1241                 suggestion = '.'
1242             raise FieldError(
1243                 "Unsupported lookup '%s' for %s or join on the field not "
1244                 "permitted%s" % (name, output_field.__name__, suggestion)
1245             )
1246 
1247     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1248                      can_reuse=None, allow_joins=True, split_subq=True,
1249                      check_filterable=True):
1250         """
1251         Build a WhereNode for a single filter clause but don't add it
1252         to this Query. Query.add_q() will then add this filter to the where
1253         Node.
1254 
1255         The 'branch_negated' tells us if the current branch contains any
1256         negations. This will be used to determine if subqueries are needed.
1257 
1258         The 'current_negated' is used to determine if the current filter is
1259         negated or not and this will be used to determine if IS NULL filtering
1260         is needed.
1261 
1262         The difference between current_negated and branch_negated is that
1263         branch_negated is set on first negation, but current_negated is
1264         flipped for each negation.
1265 
1266         Note that add_filter will not do any negating itself, that is done
1267         upper in the code by add_q().
1268 
1269         The 'can_reuse' is a set of reusable joins for multijoins.
1270 
1271         The method will create a filter clause that can be added to the current
1272         query. However, if the filter isn't added to the query then the caller
1273         is responsible for unreffing the joins used.
1274         """
1275         if isinstance(filter_expr, dict):
1276             raise FieldError("Cannot parse keyword query as dict")
1277         if isinstance(filter_expr, Q):
1278             return self._add_q(
1279                 filter_expr,
1280                 branch_negated=branch_negated,
1281                 current_negated=current_negated,
1282                 used_aliases=can_reuse,
1283                 allow_joins=allow_joins,
1284                 split_subq=split_subq,
1285                 check_filterable=check_filterable,
1286             )
1287         if hasattr(filter_expr, 'resolve_expression'):
1288             if not getattr(filter_expr, 'conditional', False):
1289                 raise TypeError('Cannot filter against a non-conditional expression.')
1290             condition = filter_expr.resolve_expression(self, allow_joins=allow_joins)
1291             if not isinstance(condition, Lookup):
1292                 condition = self.build_lookup(['exact'], condition, True)
1293             return WhereNode([condition], connector=AND), []
1294         arg, value = filter_expr
1295         if not arg:
1296             raise FieldError("Cannot parse keyword query %r" % arg)
1297         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1298 
1299         if check_filterable:
1300             self.check_filterable(reffed_expression)
1301 
1302         if not allow_joins and len(parts) > 1:
1303             raise FieldError("Joined field references are not permitted in this query")
1304 
1305         pre_joins = self.alias_refcount.copy()
1306         value = self.resolve_lookup_value(value, can_reuse, allow_joins)
1307         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1308 
1309         if check_filterable:
1310             self.check_filterable(value)
1311 
1312         if reffed_expression:
1313             condition = self.build_lookup(lookups, reffed_expression, value)
1314             return WhereNode([condition], connector=AND), []
1315 
1316         opts = self.get_meta()
1317         alias = self.get_initial_alias()
1318         allow_many = not branch_negated or not split_subq
1319 
1320         try:
1321             join_info = self.setup_joins(
1322                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1323             )
1324 
1325             # Prevent iterator from being consumed by check_related_objects()
1326             if isinstance(value, Iterator):
1327                 value = list(value)
1328             self.check_related_objects(join_info.final_field, value, join_info.opts)
1329 
1330             # split_exclude() needs to know which joins were generated for the
1331             # lookup parts
1332             self._lookup_joins = join_info.joins
1333         except MultiJoin as e:
1334             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1335 
1336         # Update used_joins before trimming since they are reused to determine
1337         # which joins could be later promoted to INNER.
1338         used_joins.update(join_info.joins)
1339         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1340         if can_reuse is not None:
1341             can_reuse.update(join_list)
1342 
1343         if join_info.final_field.is_relation:
1344             # No support for transforms for relational fields
1345             num_lookups = len(lookups)
1346             if num_lookups > 1:
1347                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1348             if len(targets) == 1:
1349                 col = self._get_col(targets[0], join_info.final_field, alias)
1350             else:
1351                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1352         else:
1353             col = self._get_col(targets[0], join_info.final_field, alias)
1354 
1355         condition = self.build_lookup(lookups, col, value)
1356         lookup_type = condition.lookup_name
1357         clause = WhereNode([condition], connector=AND)
1358 
1359         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1360         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1361             require_outer = True
1362             if lookup_type != 'isnull':
1363                 # The condition added here will be SQL like this:
1364                 # NOT (col IS NOT NULL), where the first NOT is added in
1365                 # upper layers of code. The reason for addition is that if col
1366                 # is null, then col != someval will result in SQL "unknown"
1367                 # which isn't the same as in Python. The Python None handling
1368                 # is wanted, and it can be gotten by
1369                 # (col IS NULL OR col != someval)
1370                 #   <=>
1371                 # NOT (col IS NOT NULL AND col = someval).
1372                 if (
1373                     self.is_nullable(targets[0]) or
1374                     self.alias_map[join_list[-1]].join_type == LOUTER
1375                 ):
1376                     lookup_class = targets[0].get_lookup('isnull')
1377                     col = self._get_col(targets[0], join_info.targets[0], alias)
1378                     clause.add(lookup_class(col, False), AND)
1379                 # If someval is a nullable column, someval IS NOT NULL is
1380                 # added.
1381                 if isinstance(value, Col) and self.is_nullable(value.target):
1382                     lookup_class = value.target.get_lookup('isnull')
1383                     clause.add(lookup_class(value, False), AND)
1384         return clause, used_joins if not require_outer else ()
1385 
1386     def add_filter(self, filter_lhs, filter_rhs):
1387         self.add_q(Q((filter_lhs, filter_rhs)))
1388 
1389     def add_q(self, q_object):
1390         """
1391         A preprocessor for the internal _add_q(). Responsible for doing final
1392         join promotion.
1393         """
1394         # For join promotion this case is doing an AND for the added q_object
1395         # and existing conditions. So, any existing inner join forces the join
1396         # type to remain inner. Existing outer joins can however be demoted.
1397         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1398         # rel_a doesn't produce any rows, then the whole condition must fail.
1399         # So, demotion is OK.
1400         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}
1401         clause, _ = self._add_q(q_object, self.used_aliases)
1402         if clause:
1403             self.where.add(clause, AND)
1404         self.demote_joins(existing_inner)
1405 
1406     def build_where(self, filter_expr):
1407         return self.build_filter(filter_expr, allow_joins=False)[0]
1408 
1409     def clear_where(self):
1410         self.where = WhereNode()
1411 
1412     def _add_q(self, q_object, used_aliases, branch_negated=False,
1413                current_negated=False, allow_joins=True, split_subq=True,
1414                check_filterable=True):
1415         """Add a Q-object to the current filter."""
1416         connector = q_object.connector
1417         current_negated = current_negated ^ q_object.negated
1418         branch_negated = branch_negated or q_object.negated
1419         target_clause = WhereNode(connector=connector, negated=q_object.negated)
1420         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1421         for child in q_object.children:
1422             child_clause, needed_inner = self.build_filter(
1423                 child, can_reuse=used_aliases, branch_negated=branch_negated,
1424                 current_negated=current_negated, allow_joins=allow_joins,
1425                 split_subq=split_subq, check_filterable=check_filterable,
1426             )
1427             joinpromoter.add_votes(needed_inner)
1428             if child_clause:
1429                 target_clause.add(child_clause, connector)
1430         needed_inner = joinpromoter.update_join_types(self)
1431         return target_clause, needed_inner
1432 
1433     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):
1434         """Add a FilteredRelation object to the current filter."""
1435         connector = q_object.connector
1436         current_negated ^= q_object.negated
1437         branch_negated = branch_negated or q_object.negated
1438         target_clause = WhereNode(connector=connector, negated=q_object.negated)
1439         for child in q_object.children:
1440             if isinstance(child, Node):
1441                 child_clause = self.build_filtered_relation_q(
1442                     child, reuse=reuse, branch_negated=branch_negated,
1443                     current_negated=current_negated,
1444                 )
1445             else:
1446                 child_clause, _ = self.build_filter(
1447                     child, can_reuse=reuse, branch_negated=branch_negated,
1448                     current_negated=current_negated,
1449                     allow_joins=True, split_subq=False,
1450                 )
1451             target_clause.add(child_clause, connector)
1452         return target_clause
1453 
1454     def add_filtered_relation(self, filtered_relation, alias):
1455         filtered_relation.alias = alias
1456         lookups = dict(get_children_from_q(filtered_relation.condition))
1457         relation_lookup_parts, relation_field_parts, _ = self.solve_lookup_type(filtered_relation.relation_name)
1458         if relation_lookup_parts:
1459             raise ValueError(
1460                 "FilteredRelation's relation_name cannot contain lookups "
1461                 "(got %r)." % filtered_relation.relation_name
1462             )
1463         for lookup in chain(lookups):
1464             lookup_parts, lookup_field_parts, _ = self.solve_lookup_type(lookup)
1465             shift = 2 if not lookup_parts else 1
1466             lookup_field_path = lookup_field_parts[:-shift]
1467             for idx, lookup_field_part in enumerate(lookup_field_path):
1468                 if len(relation_field_parts) > idx:
1469                     if relation_field_parts[idx] != lookup_field_part:
1470                         raise ValueError(
1471                             "FilteredRelation's condition doesn't support "
1472                             "relations outside the %r (got %r)."
1473                             % (filtered_relation.relation_name, lookup)
1474                         )
1475                 else:
1476                     raise ValueError(
1477                         "FilteredRelation's condition doesn't support nested "
1478                         "relations deeper than the relation_name (got %r for "
1479                         "%r)." % (lookup, filtered_relation.relation_name)
1480                     )
1481         self._filtered_relations[filtered_relation.alias] = filtered_relation
1482 
1483     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1484         """
1485         Walk the list of names and turns them into PathInfo tuples. A single
1486         name in 'names' can generate multiple PathInfos (m2m, for example).
1487 
1488         'names' is the path of names to travel, 'opts' is the model Options we
1489         start the name resolving from, 'allow_many' is as for setup_joins().
1490         If fail_on_missing is set to True, then a name that can't be resolved
1491         will generate a FieldError.
1492 
1493         Return a list of PathInfo tuples. In addition return the final field
1494         (the last used join field) and target (which is a field guaranteed to
1495         contain the same value as the final field). Finally, return those names
1496         that weren't found (which are likely transforms and the final lookup).
1497         """
1498         path, names_with_path = [], []
1499         for pos, name in enumerate(names):
1500             cur_names_with_path = (name, [])
1501             if name == 'pk':
1502                 name = opts.pk.name
1503 
1504             field = None
1505             filtered_relation = None
1506             try:
1507                 field = opts.get_field(name)
1508             except FieldDoesNotExist:
1509                 if name in self.annotation_select:
1510                     field = self.annotation_select[name].output_field
1511                 elif name in self._filtered_relations and pos == 0:
1512                     filtered_relation = self._filtered_relations[name]
1513                     if LOOKUP_SEP in filtered_relation.relation_name:
1514                         parts = filtered_relation.relation_name.split(LOOKUP_SEP)
1515                         filtered_relation_path, field, _, _ = self.names_to_path(
1516                             parts, opts, allow_many, fail_on_missing,
1517                         )
1518                         path.extend(filtered_relation_path[:-1])
1519                     else:
1520                         field = opts.get_field(filtered_relation.relation_name)
1521             if field is not None:
1522                 # Fields that contain one-to-many relations with a generic
1523                 # model (like a GenericForeignKey) cannot generate reverse
1524                 # relations and therefore cannot be used for reverse querying.
1525                 if field.is_relation and not field.related_model:
1526                     raise FieldError(
1527                         "Field %r does not generate an automatic reverse "
1528                         "relation and therefore cannot be used for reverse "
1529                         "querying. If it is a GenericForeignKey, consider "
1530                         "adding a GenericRelation." % name
1531                     )
1532                 try:
1533                     model = field.model._meta.concrete_model
1534                 except AttributeError:
1535                     # QuerySet.annotate() may introduce fields that aren't
1536                     # attached to a model.
1537                     model = None
1538             else:
1539                 # We didn't find the current field, so move position back
1540                 # one step.
1541                 pos -= 1
1542                 if pos == -1 or fail_on_missing:
1543                     available = sorted([
1544                         *get_field_names_from_opts(opts),
1545                         *self.annotation_select,
1546                         *self._filtered_relations,
1547                     ])
1548                     raise FieldError("Cannot resolve keyword '%s' into field. "
1549                                      "Choices are: %s" % (name, ", ".join(available)))
1550                 break
1551             # Check if we need any joins for concrete inheritance cases (the
1552             # field lives in parent, but we are currently in one of its
1553             # children)
1554             if model is not opts.model:
1555                 path_to_parent = opts.get_path_to_parent(model)
1556                 if path_to_parent:
1557                     path.extend(path_to_parent)
1558                     cur_names_with_path[1].extend(path_to_parent)
1559                     opts = path_to_parent[-1].to_opts
1560             if hasattr(field, 'path_infos'):
1561                 if filtered_relation:
1562                     pathinfos = field.get_path_info(filtered_relation)
1563                 else:
1564                     pathinfos = field.path_infos
1565                 if not allow_many:
1566                     for inner_pos, p in enumerate(pathinfos):
1567                         if p.m2m:
1568                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1569                             names_with_path.append(cur_names_with_path)
1570                             raise MultiJoin(pos + 1, names_with_path)
1571                 last = pathinfos[-1]
1572                 path.extend(pathinfos)
1573                 final_field = last.join_field
1574                 opts = last.to_opts
1575                 targets = last.target_fields
1576                 cur_names_with_path[1].extend(pathinfos)
1577                 names_with_path.append(cur_names_with_path)
1578             else:
1579                 # Local non-relational field.
1580                 final_field = field
1581                 targets = (field,)
1582                 if fail_on_missing and pos + 1 != len(names):
1583                     raise FieldError(
1584                         "Cannot resolve keyword %r into field. Join on '%s'"
1585                         " not permitted." % (names[pos + 1], name))
1586                 break
1587         return path, final_field, targets, names[pos + 1:]
1588 
1589     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True):
1590         """
1591         Compute the necessary table joins for the passage through the fields
1592         given in 'names'. 'opts' is the Options class for the current model
1593         (which gives the table we are starting from), 'alias' is the alias for
1594         the table to start the joining from.
1595 
1596         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1597         can be None in which case all joins are reusable or a set of aliases
1598         that can be reused. Note that non-reverse foreign keys are always
1599         reusable when using setup_joins().
1600 
1601         If 'allow_many' is False, then any reverse foreign key seen will
1602         generate a MultiJoin exception.
1603 
1604         Return the final field involved in the joins, the target field (used
1605         for any 'where' constraint), the final 'opts' value, the joins, the
1606         field path traveled to generate the joins, and a transform function
1607         that takes a field and alias and is equivalent to `field.get_col(alias)`
1608         in the simple case but wraps field transforms if they were included in
1609         names.
1610 
1611         The target field is the field containing the concrete value. Final
1612         field can be something different, for example foreign key pointing to
1613         that value. Final field is needed for example in some value
1614         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1615         key field for example).
1616         """
1617         joins = [alias]
1618         # The transform can't be applied yet, as joins must be trimmed later.
1619         # To avoid making every caller of this method look up transforms
1620         # directly, compute transforms here and create a partial that converts
1621         # fields to the appropriate wrapped version.
1622 
1623         def final_transformer(field, alias):
1624             if not self.alias_cols:
1625                 alias = None
1626             return field.get_col(alias)
1627 
1628         # Try resolving all the names as fields first. If there's an error,
1629         # treat trailing names as lookups until a field can be resolved.
1630         last_field_exception = None
1631         for pivot in range(len(names), 0, -1):
1632             try:
1633                 path, final_field, targets, rest = self.names_to_path(
1634                     names[:pivot], opts, allow_many, fail_on_missing=True,
1635                 )
1636             except FieldError as exc:
1637                 if pivot == 1:
1638                     # The first item cannot be a lookup, so it's safe
1639                     # to raise the field error here.
1640                     raise
1641                 else:
1642                     last_field_exception = exc
1643             else:
1644                 # The transforms are the remaining items that couldn't be
1645                 # resolved into fields.
1646                 transforms = names[pivot:]
1647                 break
1648         for name in transforms:
1649             def transform(field, alias, *, name, previous):
1650                 try:
1651                     wrapped = previous(field, alias)
1652                     return self.try_transform(wrapped, name)
1653                 except FieldError:
1654                     # FieldError is raised if the transform doesn't exist.
1655                     if isinstance(final_field, Field) and last_field_exception:
1656                         raise last_field_exception
1657                     else:
1658                         raise
1659             final_transformer = functools.partial(transform, name=name, previous=final_transformer)
1660         # Then, add the path to the query's joins. Note that we can't trim
1661         # joins at this stage - we will need the information about join type
1662         # of the trimmed joins.
1663         for join in path:
1664             if join.filtered_relation:
1665                 filtered_relation = join.filtered_relation.clone()
1666                 table_alias = filtered_relation.alias
1667             else:
1668                 filtered_relation = None
1669                 table_alias = None
1670             opts = join.to_opts
1671             if join.direct:
1672                 nullable = self.is_nullable(join.join_field)
1673             else:
1674                 nullable = True
1675             connection = self.join_class(
1676                 opts.db_table, alias, table_alias, INNER, join.join_field,
1677                 nullable, filtered_relation=filtered_relation,
1678             )
1679             reuse = can_reuse if join.m2m else None
1680             alias = self.join(connection, reuse=reuse)
1681             joins.append(alias)
1682             if filtered_relation:
1683                 filtered_relation.path = joins[:]
1684         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)
1685 
1686     def trim_joins(self, targets, joins, path):
1687         """
1688         The 'target' parameter is the final field being joined to, 'joins'
1689         is the full list of join aliases. The 'path' contain the PathInfos
1690         used to create the joins.
1691 
1692         Return the final target field and table alias and the new active
1693         joins.
1694 
1695         Always trim any direct join if the target column is already in the
1696         previous table. Can't trim reverse joins as it's unknown if there's
1697         anything on the other side of the join.
1698         """
1699         joins = joins[:]
1700         for pos, info in enumerate(reversed(path)):
1701             if len(joins) == 1 or not info.direct:
1702                 break
1703             if info.filtered_relation:
1704                 break
1705             join_targets = {t.column for t in info.join_field.foreign_related_fields}
1706             cur_targets = {t.column for t in targets}
1707             if not cur_targets.issubset(join_targets):
1708                 break
1709             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1710             targets = tuple(targets_dict[t.column] for t in targets)
1711             self.unref_alias(joins.pop())
1712         return targets, joins[-1], joins
1713 
1714     @classmethod
1715     def _gen_cols(cls, exprs, include_external=False):
1716         for expr in exprs:
1717             if isinstance(expr, Col):
1718                 yield expr
1719             elif include_external and callable(getattr(expr, 'get_external_cols', None)):
1720                 yield from expr.get_external_cols()
1721             elif hasattr(expr, 'get_source_expressions'):
1722                 yield from cls._gen_cols(
1723                     expr.get_source_expressions(),
1724                     include_external=include_external,
1725                 )
1726 
1727     @classmethod
1728     def _gen_col_aliases(cls, exprs):
1729         yield from (expr.alias for expr in cls._gen_cols(exprs))
1730 
1731     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1732         annotation = self.annotations.get(name)
1733         if annotation is not None:
1734             if not allow_joins:
1735                 for alias in self._gen_col_aliases([annotation]):
1736                     if isinstance(self.alias_map[alias], Join):
1737                         raise FieldError(
1738                             'Joined field references are not permitted in '
1739                             'this query'
1740                         )
1741             if summarize:
1742                 # Summarize currently means we are doing an aggregate() query
1743                 # which is executed as a wrapped subquery if any of the
1744                 # aggregate() elements reference an existing annotation. In
1745                 # that case we need to return a Ref to the subquery's annotation.
1746                 if name not in self.annotation_select:
1747                     raise FieldError(
1748                         "Cannot aggregate over the '%s' alias. Use annotate() "
1749                         "to promote it." % name
1750                     )
1751                 return Ref(name, self.annotation_select[name])
1752             else:
1753                 return annotation
1754         else:
1755             field_list = name.split(LOOKUP_SEP)
1756             annotation = self.annotations.get(field_list[0])
1757             if annotation is not None:
1758                 for transform in field_list[1:]:
1759                     annotation = self.try_transform(annotation, transform)
1760                 return annotation
1761             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)
1762             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1763             if not allow_joins and len(join_list) > 1:
1764                 raise FieldError('Joined field references are not permitted in this query')
1765             if len(targets) > 1:
1766                 raise FieldError("Referencing multicolumn fields with F() objects "
1767                                  "isn't supported")
1768             # Verify that the last lookup in name is a field or a transform:
1769             # transform_function() raises FieldError if not.
1770             transform = join_info.transform_function(targets[0], final_alias)
1771             if reuse is not None:
1772                 reuse.update(join_list)
1773             return transform
1774 
1775     def split_exclude(self, filter_expr, can_reuse, names_with_path):
1776         """
1777         When doing an exclude against any kind of N-to-many relation, we need
1778         to use a subquery. This method constructs the nested query, given the
1779         original exclude filter (filter_expr) and the portion up to the first
1780         N-to-many relation field.
1781 
1782         For example, if the origin filter is ~Q(child__name='foo'), filter_expr
1783         is ('child__name', 'foo') and can_reuse is a set of joins usable for
1784         filters in the original query.
1785 
1786         We will turn this into equivalent of:
1787             WHERE NOT EXISTS(
1788                 SELECT 1
1789                 FROM child
1790                 WHERE name = 'foo' AND child.parent_id = parent.id
1791                 LIMIT 1
1792             )
1793         """
1794         # Generate the inner query.
1795         query = self.__class__(self.model)
1796         query._filtered_relations = self._filtered_relations
1797         filter_lhs, filter_rhs = filter_expr
1798         if isinstance(filter_rhs, OuterRef):
1799             filter_rhs = OuterRef(filter_rhs)
1800         elif isinstance(filter_rhs, F):
1801             filter_rhs = OuterRef(filter_rhs.name)
1802         query.add_filter(filter_lhs, filter_rhs)
1803         query.clear_ordering(force=True)
1804         # Try to have as simple as possible subquery -> trim leading joins from
1805         # the subquery.
1806         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1807 
1808         col = query.select[0]
1809         select_field = col.target
1810         alias = col.alias
1811         if alias in can_reuse:
1812             pk = select_field.model._meta.pk
1813             # Need to add a restriction so that outer query's filters are in effect for
1814             # the subquery, too.
1815             query.bump_prefix(self)
1816             lookup_class = select_field.get_lookup('exact')
1817             # Note that the query.select[0].alias is different from alias
1818             # due to bump_prefix above.
1819             lookup = lookup_class(pk.get_col(query.select[0].alias),
1820                                   pk.get_col(alias))
1821             query.where.add(lookup, AND)
1822             query.external_aliases[alias] = True
1823 
1824         lookup_class = select_field.get_lookup('exact')
1825         lookup = lookup_class(col, ResolvedOuterRef(trimmed_prefix))
1826         query.where.add(lookup, AND)
1827         condition, needed_inner = self.build_filter(Exists(query))
1828 
1829         if contains_louter:
1830             or_null_condition, _ = self.build_filter(
1831                 ('%s__isnull' % trimmed_prefix, True),
1832                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1833             condition.add(or_null_condition, OR)
1834             # Note that the end result will be:
1835             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1836             # This might look crazy but due to how IN works, this seems to be
1837             # correct. If the IS NOT NULL check is removed then outercol NOT
1838             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1839             # outercol IS NULL we will not match the row.
1840         return condition, needed_inner
1841 
1842     def set_empty(self):
1843         self.where.add(NothingNode(), AND)
1844         for query in self.combined_queries:
1845             query.set_empty()
1846 
1847     def is_empty(self):
1848         return any(isinstance(c, NothingNode) for c in self.where.children)
1849 
1850     def set_limits(self, low=None, high=None):
1851         """
1852         Adjust the limits on the rows retrieved. Use low/high to set these,
1853         as it makes it more Pythonic to read and write. When the SQL query is
1854         created, convert them to the appropriate offset and limit values.
1855 
1856         Apply any limits passed in here to the existing constraints. Add low
1857         to the current low value and clamp both to any existing high value.
1858         """
1859         if high is not None:
1860             if self.high_mark is not None:
1861                 self.high_mark = min(self.high_mark, self.low_mark + high)
1862             else:
1863                 self.high_mark = self.low_mark + high
1864         if low is not None:
1865             if self.high_mark is not None:
1866                 self.low_mark = min(self.high_mark, self.low_mark + low)
1867             else:
1868                 self.low_mark = self.low_mark + low
1869 
1870         if self.low_mark == self.high_mark:
1871             self.set_empty()
1872 
1873     def clear_limits(self):
1874         """Clear any existing limits."""
1875         self.low_mark, self.high_mark = 0, None
1876 
1877     @property
1878     def is_sliced(self):
1879         return self.low_mark != 0 or self.high_mark is not None
1880 
1881     def has_limit_one(self):
1882         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1
1883 
1884     def can_filter(self):
1885         """
1886         Return True if adding filters to this instance is still possible.
1887 
1888         Typically, this means no limits or offsets have been put on the results.
1889         """
1890         return not self.is_sliced
1891 
1892     def clear_select_clause(self):
1893         """Remove all fields from SELECT clause."""
1894         self.select = ()
1895         self.default_cols = False
1896         self.select_related = False
1897         self.set_extra_mask(())
1898         self.set_annotation_mask(())
1899 
1900     def clear_select_fields(self):
1901         """
1902         Clear the list of fields to select (but not extra_select columns).
1903         Some queryset types completely replace any existing list of select
1904         columns.
1905         """
1906         self.select = ()
1907         self.values_select = ()
1908 
1909     def add_select_col(self, col, name):
1910         self.select += col,
1911         self.values_select += name,
1912 
1913     def set_select(self, cols):
1914         self.default_cols = False
1915         self.select = tuple(cols)
1916 
1917     def add_distinct_fields(self, *field_names):
1918         """
1919         Add and resolve the given fields to the query's "distinct on" clause.
1920         """
1921         self.distinct_fields = field_names
1922         self.distinct = True
1923 
1924     def add_fields(self, field_names, allow_m2m=True):
1925         """
1926         Add the given (model) fields to the select set. Add the field names in
1927         the order specified.
1928         """
1929         alias = self.get_initial_alias()
1930         opts = self.get_meta()
1931 
1932         try:
1933             cols = []
1934             for name in field_names:
1935                 # Join promotion note - we must not remove any rows here, so
1936                 # if there is no existing joins, use outer join.
1937                 join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1938                 targets, final_alias, joins = self.trim_joins(
1939                     join_info.targets,
1940                     join_info.joins,
1941                     join_info.path,
1942                 )
1943                 for target in targets:
1944                     cols.append(join_info.transform_function(target, final_alias))
1945             if cols:
1946                 self.set_select(cols)
1947         except MultiJoin:
1948             raise FieldError("Invalid field name: '%s'" % name)
1949         except FieldError:
1950             if LOOKUP_SEP in name:
1951                 # For lookups spanning over relationships, show the error
1952                 # from the model on which the lookup failed.
1953                 raise
1954             elif name in self.annotations:
1955                 raise FieldError(
1956                     "Cannot select the '%s' alias. Use annotate() to promote "
1957                     "it." % name
1958                 )
1959             else:
1960                 names = sorted([
1961                     *get_field_names_from_opts(opts), *self.extra,
1962                     *self.annotation_select, *self._filtered_relations
1963                 ])
1964                 raise FieldError("Cannot resolve keyword %r into field. "
1965                                  "Choices are: %s" % (name, ", ".join(names)))
1966 
1967     def add_ordering(self, *ordering):
1968         """
1969         Add items from the 'ordering' sequence to the query's "order by"
1970         clause. These items are either field names (not column names) --
1971         possibly with a direction prefix ('-' or '?') -- or OrderBy
1972         expressions.
1973 
1974         If 'ordering' is empty, clear all ordering from the query.
1975         """
1976         errors = []
1977         for item in ordering:
1978             if isinstance(item, str):
1979                 if item == '?':
1980                     continue
1981                 if item.startswith('-'):
1982                     item = item[1:]
1983                 if item in self.annotations:
1984                     continue
1985                 if self.extra and item in self.extra:
1986                     continue
1987                 # names_to_path() validates the lookup. A descriptive
1988                 # FieldError will be raise if it's not.
1989                 self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)
1990             elif not hasattr(item, 'resolve_expression'):
1991                 errors.append(item)
1992             if getattr(item, 'contains_aggregate', False):
1993                 raise FieldError(
1994                     'Using an aggregate in order_by() without also including '
1995                     'it in annotate() is not allowed: %s' % item
1996                 )
1997         if errors:
1998             raise FieldError('Invalid order_by arguments: %s' % errors)
1999         if ordering:
2000             self.order_by += ordering
2001         else:
2002             self.default_ordering = False
2003 
2004     def clear_ordering(self, force=False, clear_default=True):
2005         """
2006         Remove any ordering settings if the current query allows it without
2007         side effects, set 'force' to True to clear the ordering regardless.
2008         If 'clear_default' is True, there will be no ordering in the resulting
2009         query (not even the model's default).
2010         """
2011         if not force and (self.is_sliced or self.distinct_fields or self.select_for_update):
2012             return
2013         self.order_by = ()
2014         self.extra_order_by = ()
2015         if clear_default:
2016             self.default_ordering = False
2017 
2018     def set_group_by(self, allow_aliases=True):
2019         """
2020         Expand the GROUP BY clause required by the query.
2021 
2022         This will usually be the set of all non-aggregate fields in the
2023         return data. If the database backend supports grouping by the
2024         primary key, and the query would be equivalent, the optimization
2025         will be made automatically.
2026         """
2027         # Column names from JOINs to check collisions with aliases.
2028         if allow_aliases:
2029             column_names = set()
2030             seen_models = set()
2031             for join in list(self.alias_map.values())[1:]:  # Skip base table.
2032                 model = join.join_field.related_model
2033                 if model not in seen_models:
2034                     column_names.update({
2035                         field.column
2036                         for field in model._meta.local_concrete_fields
2037                     })
2038                     seen_models.add(model)
2039 
2040         group_by = list(self.select)
2041         if self.annotation_select:
2042             for alias, annotation in self.annotation_select.items():
2043                 if not allow_aliases or alias in column_names:
2044                     alias = None
2045                 group_by_cols = annotation.get_group_by_cols(alias=alias)
2046                 group_by.extend(group_by_cols)
2047         self.group_by = tuple(group_by)
2048 
2049     def add_select_related(self, fields):
2050         """
2051         Set up the select_related data structure so that we only select
2052         certain related models (as opposed to all models, when
2053         self.select_related=True).
2054         """
2055         if isinstance(self.select_related, bool):
2056             field_dict = {}
2057         else:
2058             field_dict = self.select_related
2059         for field in fields:
2060             d = field_dict
2061             for part in field.split(LOOKUP_SEP):
2062                 d = d.setdefault(part, {})
2063         self.select_related = field_dict
2064 
2065     def add_extra(self, select, select_params, where, params, tables, order_by):
2066         """
2067         Add data to the various extra_* attributes for user-created additions
2068         to the query.
2069         """
2070         if select:
2071             # We need to pair any placeholder markers in the 'select'
2072             # dictionary with their parameters in 'select_params' so that
2073             # subsequent updates to the select dictionary also adjust the
2074             # parameters appropriately.
2075             select_pairs = {}
2076             if select_params:
2077                 param_iter = iter(select_params)
2078             else:
2079                 param_iter = iter([])
2080             for name, entry in select.items():
2081                 entry = str(entry)
2082                 entry_params = []
2083                 pos = entry.find("%s")
2084                 while pos != -1:
2085                     if pos == 0 or entry[pos - 1] != '%':
2086                         entry_params.append(next(param_iter))
2087                     pos = entry.find("%s", pos + 2)
2088                 select_pairs[name] = (entry, entry_params)
2089             self.extra.update(select_pairs)
2090         if where or params:
2091             self.where.add(ExtraWhere(where, params), AND)
2092         if tables:
2093             self.extra_tables += tuple(tables)
2094         if order_by:
2095             self.extra_order_by = order_by
2096 
2097     def clear_deferred_loading(self):
2098         """Remove any fields from the deferred loading set."""
2099         self.deferred_loading = (frozenset(), True)
2100 
2101     def add_deferred_loading(self, field_names):
2102         """
2103         Add the given list of model field names to the set of fields to
2104         exclude from loading from the database when automatic column selection
2105         is done. Add the new field names to any existing field names that
2106         are deferred (or removed from any existing field names that are marked
2107         as the only ones for immediate loading).
2108         """
2109         # Fields on related models are stored in the literal double-underscore
2110         # format, so that we can use a set datastructure. We do the foo__bar
2111         # splitting and handling when computing the SQL column names (as part of
2112         # get_columns()).
2113         existing, defer = self.deferred_loading
2114         if defer:
2115             # Add to existing deferred names.
2116             self.deferred_loading = existing.union(field_names), True
2117         else:
2118             # Remove names from the set of any existing "immediate load" names.
2119             if new_existing := existing.difference(field_names):
2120                 self.deferred_loading = new_existing, False
2121             else:
2122                 self.clear_deferred_loading()
2123                 if new_only := set(field_names).difference(existing):
2124                     self.deferred_loading = new_only, True
2125 
2126     def add_immediate_loading(self, field_names):
2127         """
2128         Add the given list of model field names to the set of fields to
2129         retrieve when the SQL is executed ("immediate loading" fields). The
2130         field names replace any existing immediate loading field names. If
2131         there are field names already specified for deferred loading, remove
2132         those names from the new field_names before storing the new names
2133         for immediate loading. (That is, immediate loading overrides any
2134         existing immediate values, but respects existing deferrals.)
2135         """
2136         existing, defer = self.deferred_loading
2137         field_names = set(field_names)
2138         if 'pk' in field_names:
2139             field_names.remove('pk')
2140             field_names.add(self.get_meta().pk.name)
2141 
2142         if defer:
2143             # Remove any existing deferred names from the current set before
2144             # setting the new names.
2145             self.deferred_loading = field_names.difference(existing), False
2146         else:
2147             # Replace any existing "immediate load" field names.
2148             self.deferred_loading = frozenset(field_names), False
2149 
2150     def get_loaded_field_names(self):
2151         """
2152         If any fields are marked to be deferred, return a dictionary mapping
2153         models to a set of names in those fields that will be loaded. If a
2154         model is not in the returned dictionary, none of its fields are
2155         deferred.
2156 
2157         If no fields are marked for deferral, return an empty dictionary.
2158         """
2159         # We cache this because we call this function multiple times
2160         # (compiler.fill_related_selections, query.iterator)
2161         try:
2162             return self._loaded_field_names_cache
2163         except AttributeError:
2164             collection = {}
2165             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
2166             self._loaded_field_names_cache = collection
2167             return collection
2168 
2169     def get_loaded_field_names_cb(self, target, model, fields):
2170         """Callback used by get_deferred_field_names()."""
2171         target[model] = {f.attname for f in fields}
2172 
2173     def set_annotation_mask(self, names):
2174         """Set the mask of annotations that will be returned by the SELECT."""
2175         if names is None:
2176             self.annotation_select_mask = None
2177         else:
2178             self.annotation_select_mask = set(names)
2179         self._annotation_select_cache = None
2180 
2181     def append_annotation_mask(self, names):
2182         if self.annotation_select_mask is not None:
2183             self.set_annotation_mask(self.annotation_select_mask.union(names))
2184 
2185     def set_extra_mask(self, names):
2186         """
2187         Set the mask of extra select items that will be returned by SELECT.
2188         Don't remove them from the Query since they might be used later.
2189         """
2190         if names is None:
2191             self.extra_select_mask = None
2192         else:
2193             self.extra_select_mask = set(names)
2194         self._extra_select_cache = None
2195 
2196     def set_values(self, fields):
2197         self.select_related = False
2198         self.clear_deferred_loading()
2199         self.clear_select_fields()
2200 
2201         if fields:
2202             field_names = []
2203             extra_names = []
2204             annotation_names = []
2205             if not self.extra and not self.annotations:
2206                 # Shortcut - if there are no extra or annotations, then
2207                 # the values() clause must be just field names.
2208                 field_names = list(fields)
2209             else:
2210                 self.default_cols = False
2211                 for f in fields:
2212                     if f in self.extra_select:
2213                         extra_names.append(f)
2214                     elif f in self.annotation_select:
2215                         annotation_names.append(f)
2216                     else:
2217                         field_names.append(f)
2218             self.set_extra_mask(extra_names)
2219             self.set_annotation_mask(annotation_names)
2220             selected = frozenset(field_names + extra_names + annotation_names)
2221         else:
2222             field_names = [f.attname for f in self.model._meta.concrete_fields]
2223             selected = frozenset(field_names)
2224         # Selected annotations must be known before setting the GROUP BY
2225         # clause.
2226         if self.group_by is True:
2227             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
2228             # Disable GROUP BY aliases to avoid orphaning references to the
2229             # SELECT clause which is about to be cleared.
2230             self.set_group_by(allow_aliases=False)
2231             self.clear_select_fields()
2232         elif self.group_by:
2233             # Resolve GROUP BY annotation references if they are not part of
2234             # the selected fields anymore.
2235             group_by = []
2236             for expr in self.group_by:
2237                 if isinstance(expr, Ref) and expr.refs not in selected:
2238                     expr = self.annotations[expr.refs]
2239                 group_by.append(expr)
2240             self.group_by = tuple(group_by)
2241 
2242         self.values_select = tuple(field_names)
2243         self.add_fields(field_names, True)
2244 
2245     @property
2246     def annotation_select(self):
2247         """
2248         Return the dictionary of aggregate columns that are not masked and
2249         should be used in the SELECT clause. Cache this result for performance.
2250         """
2251         if self._annotation_select_cache is not None:
2252             return self._annotation_select_cache
2253         elif not self.annotations:
2254             return {}
2255         elif self.annotation_select_mask is not None:
2256             self._annotation_select_cache = {
2257                 k: v for k, v in self.annotations.items()
2258                 if k in self.annotation_select_mask
2259             }
2260             return self._annotation_select_cache
2261         else:
2262             return self.annotations
2263 
2264     @property
2265     def extra_select(self):
2266         if self._extra_select_cache is not None:
2267             return self._extra_select_cache
2268         if not self.extra:
2269             return {}
2270         elif self.extra_select_mask is not None:
2271             self._extra_select_cache = {
2272                 k: v for k, v in self.extra.items()
2273                 if k in self.extra_select_mask
2274             }
2275             return self._extra_select_cache
2276         else:
2277             return self.extra
2278 
2279     def trim_start(self, names_with_path):
2280         """
2281         Trim joins from the start of the join path. The candidates for trim
2282         are the PathInfos in names_with_path structure that are m2m joins.
2283 
2284         Also set the select column so the start matches the join.
2285 
2286         This method is meant to be used for generating the subquery joins &
2287         cols in split_exclude().
2288 
2289         Return a lookup usable for doing outerq.filter(lookup=self) and a
2290         boolean indicating if the joins in the prefix contain a LEFT OUTER join.
2291         _"""
2292         all_paths = []
2293         for _, paths in names_with_path:
2294             all_paths.extend(paths)
2295         contains_louter = False
2296         # Trim and operate only on tables that were generated for
2297         # the lookup part of the query. That is, avoid trimming
2298         # joins generated for F() expressions.
2299         lookup_tables = [
2300             t for t in self.alias_map
2301             if t in self._lookup_joins or t == self.base_table
2302         ]
2303         for trimmed_paths, path in enumerate(all_paths):
2304             if path.m2m:
2305                 break
2306             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
2307                 contains_louter = True
2308             alias = lookup_tables[trimmed_paths]
2309             self.unref_alias(alias)
2310         # The path.join_field is a Rel, lets get the other side's field
2311         join_field = path.join_field.field
2312         # Build the filter prefix.
2313         paths_in_prefix = trimmed_paths
2314         trimmed_prefix = []
2315         for name, path in names_with_path:
2316             if paths_in_prefix - len(path) < 0:
2317                 break
2318             trimmed_prefix.append(name)
2319             paths_in_prefix -= len(path)
2320         trimmed_prefix.append(
2321             join_field.foreign_related_fields[0].name)
2322         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
2323         # Lets still see if we can trim the first join from the inner query
2324         # (that is, self). We can't do this for:
2325         # - LEFT JOINs because we would miss those rows that have nothing on
2326         #   the outer side,
2327         # - INNER JOINs from filtered relations because we would miss their
2328         #   filters.
2329         first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]
2330         if first_join.join_type != LOUTER and not first_join.filtered_relation:
2331             select_fields = [r[0] for r in join_field.related_fields]
2332             select_alias = lookup_tables[trimmed_paths + 1]
2333             self.unref_alias(lookup_tables[trimmed_paths])
2334             extra_restriction = join_field.get_extra_restriction(None, lookup_tables[trimmed_paths + 1])
2335             if extra_restriction:
2336                 self.where.add(extra_restriction, AND)
2337         else:
2338             # TODO: It might be possible to trim more joins from the start of the
2339             # inner query if it happens to have a longer join chain containing the
2340             # values in select_fields. Lets punt this one for now.
2341             select_fields = [r[1] for r in join_field.related_fields]
2342             select_alias = lookup_tables[trimmed_paths]
2343         # The found starting point is likely a join_class instead of a
2344         # base_table_class reference. But the first entry in the query's FROM
2345         # clause must not be a JOIN.
2346         for table in self.alias_map:
2347             if self.alias_refcount[table] > 0:
2348                 self.alias_map[table] = self.base_table_class(
2349                     self.alias_map[table].table_name,
2350                     table,
2351                 )
2352                 break
2353         self.set_select([f.get_col(select_alias) for f in select_fields])
2354         return trimmed_prefix, contains_louter
2355 
2356     def is_nullable(self, field):
2357         """
2358         Check if the given field should be treated as nullable.
2359 
2360         Some backends treat '' as null and Django treats such fields as
2361         nullable for those backends. In such situations field.null can be
2362         False even if we should treat the field as nullable.
2363         """
2364         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2365         # (nor should it have) knowledge of which connection is going to be
2366         # used. The proper fix would be to defer all decisions where
2367         # is_nullable() is needed to the compiler stage, but that is not easy
2368         # to do currently.
2369         return field.null or (
2370             field.empty_strings_allowed and
2371             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls
2372         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 03:58:11.010 | INFO     | Search completed. Bug locations: [<file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>as_sql</method>
<code>
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params

</code><intended_behavior>The `as_sql` method should include logic to correctly handle empty querysets. When `Exists` is applied with an empty queryset, especially in negated form (using `~`), it should still contribute to generating the correct SQL `WHERE` clause without leading to an `EmptyResultSet`. This means ensuring that the SQL construction process accounts for empty querysets in a way that preserves the expected logical behavior of the `Exists` condition. Specifically, for an empty queryset, the `EXISTS` statement should translate to a condition that evaluates to false when not negated and true when negated, ensuring the `WHERE` block is retained in the SQL query.</intended_behavior>, <file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1199 class Exists(Subquery):
1200     template = 'EXISTS(%(subquery)s)'
1201     output_field = fields.BooleanField()
1202 
1203     def __init__(self, queryset, negated=False, **kwargs):
1204         self.negated = negated
1205         super().__init__(queryset, **kwargs)
1206 
1207     def __invert__(self):
1208         clone = self.copy()
1209         clone.negated = not self.negated
1210         return clone
1211 
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params
1224 
1225     def select_format(self, compiler, sql, params):
1226         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1227         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1228         # BY list.
1229         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1230             sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
1231         return sql, params

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>get_compiler</method>
<code>
280     def get_compiler(self, using=None, connection=None, elide_empty=True):
281         if using is None and connection is None:
282             raise ValueError("Need either using or connection")
283         if using:
284             connection = connections[using]
285         return connection.ops.compiler(self.compiler)(self, connection, using, elide_empty)

</code><intended_behavior>The method should ensure that when compiling a query, the `elide_empty` flag does not lead to the unintentional removal of essential parts of the SQL query, such as the `WHERE` clause, simply because a part of the query involves an empty queryset. It should make a distinction between genuinely unnecessary parts of a query (like an empty `WHERE` clause that has no conditions) and parts that are critical for the logic of the query to be correct, such as conditions derived from empty `Exists` subqueries. This may involve refining the logic that decides what parts of a query are elided to better account for the logical implications of those parts on the overall query.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
142 class Query(BaseExpression):
143     """A single SQL query."""
144 
145     alias_prefix = 'T'
146     empty_result_set_value = None
147     subq_aliases = frozenset([alias_prefix])
148 
149     compiler = 'SQLCompiler'
150 
151     base_table_class = BaseTable
152     join_class = Join
153 
154     def __init__(self, model, alias_cols=True):
155         self.model = model
156         self.alias_refcount = {}
157         # alias_map is the most important data structure regarding joins.
158         # It's used for recording which joins exist in the query and what
159         # types they are. The key is the alias of the joined table (possibly
160         # the table name) and the value is a Join-like object (see
161         # sql.datastructures.Join for more information).
162         self.alias_map = {}
163         # Whether to provide alias to columns during reference resolving.
164         self.alias_cols = alias_cols
165         # Sometimes the query contains references to aliases in outer queries (as
166         # a result of split_exclude). Correct alias quoting needs to know these
167         # aliases too.
168         # Map external tables to whether they are aliased.
169         self.external_aliases = {}
170         self.table_map = {}     # Maps table names to list of aliases.
171         self.default_cols = True
172         self.default_ordering = True
173         self.standard_ordering = True
174         self.used_aliases = set()
175         self.filter_is_sticky = False
176         self.subquery = False
177 
178         # SQL-related attributes
179         # Select and related select clauses are expressions to use in the
180         # SELECT clause of the query.
181         # The select is used for cases where we want to set up the select
182         # clause to contain other than default fields (values(), subqueries...)
183         # Note that annotations go to annotations dictionary.
184         self.select = ()
185         self.where = WhereNode()
186         # The group_by attribute can have one of the following forms:
187         #  - None: no group by at all in the query
188         #  - A tuple of expressions: group by (at least) those expressions.
189         #    String refs are also allowed for now.
190         #  - True: group by all select fields of the model
191         # See compiler.get_group_by() for details.
192         self.group_by = None
193         self.order_by = ()
194         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
195         self.distinct = False
196         self.distinct_fields = ()
197         self.select_for_update = False
198         self.select_for_update_nowait = False
199         self.select_for_update_skip_locked = False
200         self.select_for_update_of = ()
201         self.select_for_no_key_update = False
202 
203         self.select_related = False
204         # Arbitrary limit for select_related to prevents infinite recursion.
205         self.max_depth = 5
206 
207         # Holds the selects defined by a call to values() or values_list()
208         # excluding annotation_select and extra_select.
209         self.values_select = ()
210 
211         # SQL annotation-related attributes
212         self.annotations = {}  # Maps alias -> Annotation Expression
213         self.annotation_select_mask = None
214         self._annotation_select_cache = None
215 
216         # Set combination attributes
217         self.combinator = None
218         self.combinator_all = False
219         self.combined_queries = ()
220 
221         # These are for extensions. The contents are more or less appended
222         # verbatim to the appropriate clause.
223         self.extra = {}  # Maps col_alias -> (col_sql, params).
224         self.extra_select_mask = None
225         self._extra_select_cache = None
226 
227         self.extra_tables = ()
228         self.extra_order_by = ()
229 
230         # A tuple that is a set of model field names and either True, if these
231         # are the fields to defer, or False if these are the only fields to
232         # load.
233         self.deferred_loading = (frozenset(), True)
234 
235         self._filtered_relations = {}
236 
237         self.explain_info = None
238 
239     @property
240     def output_field(self):
241         if len(self.select) == 1:
242             select = self.select[0]
243             return getattr(select, 'target', None) or select.field
244         elif len(self.annotation_select) == 1:
245             return next(iter(self.annotation_select.values())).output_field
246 
247     @property
248     def has_select_fields(self):
249         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)
250 
251     @cached_property
252     def base_table(self):
253         for alias in self.alias_map:
254             return alias
255 
256     def __str__(self):
257         """
258         Return the query as a string of SQL with the parameter values
259         substituted in (use sql_with_params() to see the unsubstituted string).
260 
261         Parameter values won't necessarily be quoted correctly, since that is
262         done by the database interface at execution time.
263         """
264         sql, params = self.sql_with_params()
265         return sql % params
266 
267     def sql_with_params(self):
268         """
269         Return the query as an SQL string and the parameters that will be
270         substituted into the query.
271         """
272         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
273 
274     def __deepcopy__(self, memo):
275         """Limit the amount of work when a Query is deepcopied."""
276         result = self.clone()
277         memo[id(self)] = result
278         return result
279 
280     def get_compiler(self, using=None, connection=None, elide_empty=True):
281         if using is None and connection is None:
282             raise ValueError("Need either using or connection")
283         if using:
284             connection = connections[using]
285         return connection.ops.compiler(self.compiler)(self, connection, using, elide_empty)
286 
287     def get_meta(self):
288         """
289         Return the Options instance (the model._meta) from which to start
290         processing. Normally, this is self.model._meta, but it can be changed
291         by subclasses.
292         """
293         return self.model._meta
294 
295     def clone(self):
296         """
297         Return a copy of the current Query. A lightweight alternative to
298         to deepcopy().
299         """
300         obj = Empty()
301         obj.__class__ = self.__class__
302         # Copy references to everything.
303         obj.__dict__ = self.__dict__.copy()
304         # Clone attributes that can't use shallow copy.
305         obj.alias_refcount = self.alias_refcount.copy()
306         obj.alias_map = self.alias_map.copy()
307         obj.external_aliases = self.external_aliases.copy()
308         obj.table_map = self.table_map.copy()
309         obj.where = self.where.clone()
310         obj.annotations = self.annotations.copy()
311         if self.annotation_select_mask is not None:
312             obj.annotation_select_mask = self.annotation_select_mask.copy()
313         if self.combined_queries:
314             obj.combined_queries = tuple([
315                 query.clone() for query in self.combined_queries
316             ])
317         # _annotation_select_cache cannot be copied, as doing so breaks the
318         # (necessary) state in which both annotations and
319         # _annotation_select_cache point to the same underlying objects.
320         # It will get re-populated in the cloned queryset the next time it's
321         # used.
322         obj._annotation_select_cache = None
323         obj.extra = self.extra.copy()
324         if self.extra_select_mask is not None:
325             obj.extra_select_mask = self.extra_select_mask.copy()
326         if self._extra_select_cache is not None:
327             obj._extra_select_cache = self._extra_select_cache.copy()
328         if self.select_related is not False:
329             # Use deepcopy because select_related stores fields in nested
330             # dicts.
331             obj.select_related = copy.deepcopy(obj.select_related)
332         if 'subq_aliases' in self.__dict__:
333             obj.subq_aliases = self.subq_aliases.copy()
334         obj.used_aliases = self.used_aliases.copy()
335         obj._filtered_relations = self._filtered_relations.copy()
336         # Clear the cached_property
337         try:
338             del obj.base_table
339         except AttributeError:
340             pass
341         return obj
342 
343     def chain(self, klass=None):
344         """
345         Return a copy of the current Query that's ready for another operation.
346         The klass argument changes the type of the Query, e.g. UpdateQuery.
347         """
348         obj = self.clone()
349         if klass and obj.__class__ != klass:
350             obj.__class__ = klass
351         if not obj.filter_is_sticky:
352             obj.used_aliases = set()
353         obj.filter_is_sticky = False
354         if hasattr(obj, '_setup_query'):
355             obj._setup_query()
356         return obj
357 
358     def relabeled_clone(self, change_map):
359         clone = self.clone()
360         clone.change_aliases(change_map)
361         return clone
362 
363     def _get_col(self, target, field, alias):
364         if not self.alias_cols:
365             alias = None
366         return target.get_col(alias, field)
367 
368     def rewrite_cols(self, annotation, col_cnt):
369         # We must make sure the inner query has the referred columns in it.
370         # If we are aggregating over an annotation, then Django uses Ref()
371         # instances to note this. However, if we are annotating over a column
372         # of a related model, then it might be that column isn't part of the
373         # SELECT clause of the inner query, and we must manually make sure
374         # the column is selected. An example case is:
375         #    .aggregate(Sum('author__awards'))
376         # Resolving this expression results in a join to author, but there
377         # is no guarantee the awards column of author is in the select clause
378         # of the query. Thus we must manually add the column to the inner
379         # query.
380         orig_exprs = annotation.get_source_expressions()
381         new_exprs = []
382         for expr in orig_exprs:
383             # FIXME: These conditions are fairly arbitrary. Identify a better
384             # method of having expressions decide which code path they should
385             # take.
386             if isinstance(expr, Ref):
387                 # Its already a Ref to subquery (see resolve_ref() for
388                 # details)
389                 new_exprs.append(expr)
390             elif isinstance(expr, (WhereNode, Lookup)):
391                 # Decompose the subexpressions further. The code here is
392                 # copied from the else clause, but this condition must appear
393                 # before the contains_aggregate/is_summary condition below.
394                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
395                 new_exprs.append(new_expr)
396             else:
397                 # Reuse aliases of expressions already selected in subquery.
398                 for col_alias, selected_annotation in self.annotation_select.items():
399                     if selected_annotation is expr:
400                         new_expr = Ref(col_alias, expr)
401                         break
402                 else:
403                     # An expression that is not selected the subquery.
404                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
405                         # Reference column or another aggregate. Select it
406                         # under a non-conflicting alias.
407                         col_cnt += 1
408                         col_alias = '__col%d' % col_cnt
409                         self.annotations[col_alias] = expr
410                         self.append_annotation_mask([col_alias])
411                         new_expr = Ref(col_alias, expr)
412                     else:
413                         # Some other expression not referencing database values
414                         # directly. Its subexpression might contain Cols.
415                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
416                 new_exprs.append(new_expr)
417         annotation.set_source_expressions(new_exprs)
418         return annotation, col_cnt
419 
420     def get_aggregation(self, using, added_aggregate_names):
421         """
422         Return the dictionary with the values of the existing aggregations.
423         """
424         if not self.annotation_select:
425             return {}
426         existing_annotations = [
427             annotation for alias, annotation
428             in self.annotations.items()
429             if alias not in added_aggregate_names
430         ]
431         # Decide if we need to use a subquery.
432         #
433         # Existing annotations would cause incorrect results as get_aggregation()
434         # must produce just one result and thus must not use GROUP BY. But we
435         # aren't smart enough to remove the existing annotations from the
436         # query, so those would force us to use GROUP BY.
437         #
438         # If the query has limit or distinct, or uses set operations, then
439         # those operations must be done in a subquery so that the query
440         # aggregates on the limit and/or distinct results instead of applying
441         # the distinct and limit after the aggregation.
442         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or
443                 self.distinct or self.combinator):
444             from django.db.models.sql.subqueries import AggregateQuery
445             inner_query = self.clone()
446             inner_query.subquery = True
447             outer_query = AggregateQuery(self.model, inner_query)
448             inner_query.select_for_update = False
449             inner_query.select_related = False
450             inner_query.set_annotation_mask(self.annotation_select)
451             # Queries with distinct_fields need ordering and when a limit is
452             # applied we must take the slice from the ordered query. Otherwise
453             # no need for ordering.
454             inner_query.clear_ordering(force=False)
455             if not inner_query.distinct:
456                 # If the inner query uses default select and it has some
457                 # aggregate annotations, then we must make sure the inner
458                 # query is grouped by the main model's primary key. However,
459                 # clearing the select clause can alter results if distinct is
460                 # used.
461                 has_existing_aggregate_annotations = any(
462                     annotation for annotation in existing_annotations
463                     if getattr(annotation, 'contains_aggregate', True)
464                 )
465                 if inner_query.default_cols and has_existing_aggregate_annotations:
466                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
467                 inner_query.default_cols = False
468 
469             relabels = {t: 'subquery' for t in inner_query.alias_map}
470             relabels[None] = 'subquery'
471             # Remove any aggregates marked for reduction from the subquery
472             # and move them to the outer AggregateQuery.
473             col_cnt = 0
474             for alias, expression in list(inner_query.annotation_select.items()):
475                 annotation_select_mask = inner_query.annotation_select_mask
476                 if expression.is_summary:
477                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
478                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
479                     del inner_query.annotations[alias]
480                     annotation_select_mask.remove(alias)
481                 # Make sure the annotation_select wont use cached results.
482                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
483             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:
484                 # In case of Model.objects[0:3].count(), there would be no
485                 # field selected in the inner query, yet we must use a subquery.
486                 # So, make sure at least one field is selected.
487                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
488         else:
489             outer_query = self
490             self.select = ()
491             self.default_cols = False
492             self.extra = {}
493 
494         empty_set_result = [
495             expression.empty_result_set_value
496             for expression in outer_query.annotation_select.values()
497         ]
498         elide_empty = not any(result is NotImplemented for result in empty_set_result)
499         outer_query.clear_ordering(force=True)
500         outer_query.clear_limits()
501         outer_query.select_for_update = False
502         outer_query.select_related = False
503         compiler = outer_query.get_compiler(using, elide_empty=elide_empty)
504         result = compiler.execute_sql(SINGLE)
505         if result is None:
506             result = empty_set_result
507 
508         converters = compiler.get_converters(outer_query.annotation_select.values())
509         result = next(compiler.apply_converters((result,), converters))
510 
511         return dict(zip(outer_query.annotation_select, result))
512 
513     def get_count(self, using):
514         """
515         Perform a COUNT() query using the current filter constraints.
516         """
517         obj = self.clone()
518         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
519         return obj.get_aggregation(using, ['__count'])['__count']
520 
521     def has_filters(self):
522         return self.where
523 
524     def exists(self, using, limit=True):
525         q = self.clone()
526         if not q.distinct:
527             if q.group_by is True:
528                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
529                 # Disable GROUP BY aliases to avoid orphaning references to the
530                 # SELECT clause which is about to be cleared.
531                 q.set_group_by(allow_aliases=False)
532             q.clear_select_clause()
533         if q.combined_queries and q.combinator == 'union':
534             limit_combined = connections[using].features.supports_slicing_ordering_in_compound
535             q.combined_queries = tuple(
536                 combined_query.exists(using, limit=limit_combined)
537                 for combined_query in q.combined_queries
538             )
539         q.clear_ordering(force=True)
540         if limit:
541             q.set_limits(high=1)
542         q.add_extra({'a': 1}, None, None, None, None, None)
543         q.set_extra_mask(['a'])
544         return q
545 
546     def has_results(self, using):
547         q = self.exists(using)
548         compiler = q.get_compiler(using=using)
549         return compiler.has_results()
550 
551     def explain(self, using, format=None, **options):
552         q = self.clone()
553         q.explain_info = ExplainInfo(format, options)
554         compiler = q.get_compiler(using=using)
555         return '\n'.join(compiler.explain_query())
556 
557     def combine(self, rhs, connector):
558         """
559         Merge the 'rhs' query into the current one (with any 'rhs' effects
560         being applied *after* (that is, "to the right of") anything in the
561         current query. 'rhs' is not modified during a call to this function.
562 
563         The 'connector' parameter describes how to connect filters from the
564         'rhs' query.
565         """
566         if self.model != rhs.model:
567             raise TypeError('Cannot combine queries on two different base models.')
568         if self.is_sliced:
569             raise TypeError('Cannot combine queries once a slice has been taken.')
570         if self.distinct != rhs.distinct:
571             raise TypeError('Cannot combine a unique query with a non-unique query.')
572         if self.distinct_fields != rhs.distinct_fields:
573             raise TypeError('Cannot combine queries with different distinct fields.')
574 
575         # If lhs and rhs shares the same alias prefix, it is possible to have
576         # conflicting alias changes like T4 -> T5, T5 -> T6, which might end up
577         # as T4 -> T6 while combining two querysets. To prevent this, change an
578         # alias prefix of the rhs and update current aliases accordingly,
579         # except if the alias is the base table since it must be present in the
580         # query on both sides.
581         initial_alias = self.get_initial_alias()
582         rhs.bump_prefix(self, exclude={initial_alias})
583 
584         # Work out how to relabel the rhs aliases, if necessary.
585         change_map = {}
586         conjunction = (connector == AND)
587 
588         # Determine which existing joins can be reused. When combining the
589         # query with AND we must recreate all joins for m2m filters. When
590         # combining with OR we can reuse joins. The reason is that in AND
591         # case a single row can't fulfill a condition like:
592         #     revrel__col=1 & revrel__col=2
593         # But, there might be two different related rows matching this
594         # condition. In OR case a single True is enough, so single row is
595         # enough, too.
596         #
597         # Note that we will be creating duplicate joins for non-m2m joins in
598         # the AND case. The results will be correct but this creates too many
599         # joins. This is something that could be fixed later on.
600         reuse = set() if conjunction else set(self.alias_map)
601         joinpromoter = JoinPromoter(connector, 2, False)
602         joinpromoter.add_votes(
603             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
604         rhs_votes = set()
605         # Now, add the joins from rhs query into the new query (skipping base
606         # table).
607         rhs_tables = list(rhs.alias_map)[1:]
608         for alias in rhs_tables:
609             join = rhs.alias_map[alias]
610             # If the left side of the join was already relabeled, use the
611             # updated alias.
612             join = join.relabeled_clone(change_map)
613             new_alias = self.join(join, reuse=reuse)
614             if join.join_type == INNER:
615                 rhs_votes.add(new_alias)
616             # We can't reuse the same join again in the query. If we have two
617             # distinct joins for the same connection in rhs query, then the
618             # combined query must have two joins, too.
619             reuse.discard(new_alias)
620             if alias != new_alias:
621                 change_map[alias] = new_alias
622             if not rhs.alias_refcount[alias]:
623                 # The alias was unused in the rhs query. Unref it so that it
624                 # will be unused in the new query, too. We have to add and
625                 # unref the alias so that join promotion has information of
626                 # the join type for the unused alias.
627                 self.unref_alias(new_alias)
628         joinpromoter.add_votes(rhs_votes)
629         joinpromoter.update_join_types(self)
630 
631         # Combine subqueries aliases to ensure aliases relabelling properly
632         # handle subqueries when combining where and select clauses.
633         self.subq_aliases |= rhs.subq_aliases
634 
635         # Now relabel a copy of the rhs where-clause and add it to the current
636         # one.
637         w = rhs.where.clone()
638         w.relabel_aliases(change_map)
639         self.where.add(w, connector)
640 
641         # Selection columns and extra extensions are those provided by 'rhs'.
642         if rhs.select:
643             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])
644         else:
645             self.select = ()
646 
647         if connector == OR:
648             # It would be nice to be able to handle this, but the queries don't
649             # really make sense (or return consistent value sets). Not worth
650             # the extra complexity when you can write a real query instead.
651             if self.extra and rhs.extra:
652                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
653         self.extra.update(rhs.extra)
654         extra_select_mask = set()
655         if self.extra_select_mask is not None:
656             extra_select_mask.update(self.extra_select_mask)
657         if rhs.extra_select_mask is not None:
658             extra_select_mask.update(rhs.extra_select_mask)
659         if extra_select_mask:
660             self.set_extra_mask(extra_select_mask)
661         self.extra_tables += rhs.extra_tables
662 
663         # Ordering uses the 'rhs' ordering, unless it has none, in which case
664         # the current ordering is used.
665         self.order_by = rhs.order_by or self.order_by
666         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
667 
668     def deferred_to_data(self, target, callback):
669         """
670         Convert the self.deferred_loading data structure to an alternate data
671         structure, describing the field that *will* be loaded. This is used to
672         compute the columns to select from the database and also by the
673         QuerySet class to work out which fields are being initialized on each
674         model. Models that have all their fields included aren't mentioned in
675         the result, only those that have field restrictions in place.
676 
677         The "target" parameter is the instance that is populated (in place).
678         The "callback" is a function that is called whenever a (model, field)
679         pair need to be added to "target". It accepts three parameters:
680         "target", and the model and list of fields being added for that model.
681         """
682         field_names, defer = self.deferred_loading
683         if not field_names:
684             return
685         orig_opts = self.get_meta()
686         seen = {}
687         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
688         for field_name in field_names:
689             parts = field_name.split(LOOKUP_SEP)
690             cur_model = self.model._meta.concrete_model
691             opts = orig_opts
692             for name in parts[:-1]:
693                 old_model = cur_model
694                 if name in self._filtered_relations:
695                     name = self._filtered_relations[name].relation_name
696                 source = opts.get_field(name)
697                 if is_reverse_o2o(source):
698                     cur_model = source.related_model
699                 else:
700                     cur_model = source.remote_field.model
701                 opts = cur_model._meta
702                 # Even if we're "just passing through" this model, we must add
703                 # both the current model's pk and the related reference field
704                 # (if it's not a reverse relation) to the things we select.
705                 if not is_reverse_o2o(source):
706                     must_include[old_model].add(source)
707                 add_to_dict(must_include, cur_model, opts.pk)
708             field = opts.get_field(parts[-1])
709             is_reverse_object = field.auto_created and not field.concrete
710             model = field.related_model if is_reverse_object else field.model
711             model = model._meta.concrete_model
712             if model == opts.model:
713                 model = cur_model
714             if not is_reverse_o2o(field):
715                 add_to_dict(seen, model, field)
716 
717         if defer:
718             # We need to load all fields for each model, except those that
719             # appear in "seen" (for all models that appear in "seen"). The only
720             # slight complexity here is handling fields that exist on parent
721             # models.
722             workset = {}
723             for model, values in seen.items():
724                 for field in model._meta.local_fields:
725                     if field not in values:
726                         m = field.model._meta.concrete_model
727                         add_to_dict(workset, m, field)
728             for model, values in must_include.items():
729                 # If we haven't included a model in workset, we don't add the
730                 # corresponding must_include fields for that model, since an
731                 # empty set means "include all fields". That's why there's no
732                 # "else" branch here.
733                 if model in workset:
734                     workset[model].update(values)
735             for model, values in workset.items():
736                 callback(target, model, values)
737         else:
738             for model, values in must_include.items():
739                 if model in seen:
740                     seen[model].update(values)
741                 else:
742                     # As we've passed through this model, but not explicitly
743                     # included any fields, we have to make sure it's mentioned
744                     # so that only the "must include" fields are pulled in.
745                     seen[model] = values
746             # Now ensure that every model in the inheritance chain is mentioned
747             # in the parent list. Again, it must be mentioned to ensure that
748             # only "must include" fields are pulled in.
749             for model in orig_opts.get_parent_list():
750                 seen.setdefault(model, set())
751             for model, values in seen.items():
752                 callback(target, model, values)
753 
754     def table_alias(self, table_name, create=False, filtered_relation=None):
755         """
756         Return a table alias for the given table_name and whether this is a
757         new alias or not.
758 
759         If 'create' is true, a new alias is always created. Otherwise, the
760         most recently created alias for the table (if one exists) is reused.
761         """
762         alias_list = self.table_map.get(table_name)
763         if not create and alias_list:
764             alias = alias_list[0]
765             self.alias_refcount[alias] += 1
766             return alias, False
767 
768         # Create a new alias for this table.
769         if alias_list:
770             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
771             alias_list.append(alias)
772         else:
773             # The first occurrence of a table uses the table name directly.
774             alias = filtered_relation.alias if filtered_relation is not None else table_name
775             self.table_map[table_name] = [alias]
776         self.alias_refcount[alias] = 1
777         return alias, True
778 
779     def ref_alias(self, alias):
780         """Increases the reference count for this alias."""
781         self.alias_refcount[alias] += 1
782 
783     def unref_alias(self, alias, amount=1):
784         """Decreases the reference count for this alias."""
785         self.alias_refcount[alias] -= amount
786 
787     def promote_joins(self, aliases):
788         """
789         Promote recursively the join type of given aliases and its children to
790         an outer join. If 'unconditional' is False, only promote the join if
791         it is nullable or the parent join is an outer join.
792 
793         The children promotion is done to avoid join chains that contain a LOUTER
794         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
795         then we must also promote b->c automatically, or otherwise the promotion
796         of a->b doesn't actually change anything in the query results.
797         """
798         aliases = list(aliases)
799         while aliases:
800             alias = aliases.pop(0)
801             if self.alias_map[alias].join_type is None:
802                 # This is the base table (first FROM entry) - this table
803                 # isn't really joined at all in the query, so we should not
804                 # alter its join type.
805                 continue
806             # Only the first alias (skipped above) should have None join_type
807             assert self.alias_map[alias].join_type is not None
808             parent_alias = self.alias_map[alias].parent_alias
809             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
810             already_louter = self.alias_map[alias].join_type == LOUTER
811             if ((self.alias_map[alias].nullable or parent_louter) and
812                     not already_louter):
813                 self.alias_map[alias] = self.alias_map[alias].promote()
814                 # Join type of 'alias' changed, so re-examine all aliases that
815                 # refer to this one.
816                 aliases.extend(
817                     join for join in self.alias_map
818                     if self.alias_map[join].parent_alias == alias and join not in aliases
819                 )
820 
821     def demote_joins(self, aliases):
822         """
823         Change join type from LOUTER to INNER for all joins in aliases.
824 
825         Similarly to promote_joins(), this method must ensure no join chains
826         containing first an outer, then an inner join are generated. If we
827         are demoting b->c join in chain a LOUTER b LOUTER c then we must
828         demote a->b automatically, or otherwise the demotion of b->c doesn't
829         actually change anything in the query results. .
830         """
831         aliases = list(aliases)
832         while aliases:
833             alias = aliases.pop(0)
834             if self.alias_map[alias].join_type == LOUTER:
835                 self.alias_map[alias] = self.alias_map[alias].demote()
836                 parent_alias = self.alias_map[alias].parent_alias
837                 if self.alias_map[parent_alias].join_type == INNER:
838                     aliases.append(parent_alias)
839 
840     def reset_refcounts(self, to_counts):
841         """
842         Reset reference counts for aliases so that they match the value passed
843         in `to_counts`.
844         """
845         for alias, cur_refcount in self.alias_refcount.copy().items():
846             unref_amount = cur_refcount - to_counts.get(alias, 0)
847             self.unref_alias(alias, unref_amount)
848 
849     def change_aliases(self, change_map):
850         """
851         Change the aliases in change_map (which maps old-alias -> new-alias),
852         relabelling any references to them in select columns and the where
853         clause.
854         """
855         # If keys and values of change_map were to intersect, an alias might be
856         # updated twice (e.g. T4 -> T5, T5 -> T6, so also T4 -> T6) depending
857         # on their order in change_map.
858         assert set(change_map).isdisjoint(change_map.values())
859 
860         # 1. Update references in "select" (normal columns plus aliases),
861         # "group by" and "where".
862         self.where.relabel_aliases(change_map)
863         if isinstance(self.group_by, tuple):
864             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])
865         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])
866         self.annotations = self.annotations and {
867             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()
868         }
869 
870         # 2. Rename the alias in the internal table/alias datastructures.
871         for old_alias, new_alias in change_map.items():
872             if old_alias not in self.alias_map:
873                 continue
874             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
875             self.alias_map[new_alias] = alias_data
876             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
877             del self.alias_refcount[old_alias]
878             del self.alias_map[old_alias]
879 
880             table_aliases = self.table_map[alias_data.table_name]
881             for pos, alias in enumerate(table_aliases):
882                 if alias == old_alias:
883                     table_aliases[pos] = new_alias
884                     break
885         self.external_aliases = {
886             # Table is aliased or it's being changed and thus is aliased.
887             change_map.get(alias, alias): (aliased or alias in change_map)
888             for alias, aliased in self.external_aliases.items()
889         }
890 
891     def bump_prefix(self, other_query, exclude=None):
892         """
893         Change the alias prefix to the next letter in the alphabet in a way
894         that the other query's aliases and this query's aliases will not
895         conflict. Even tables that previously had no alias will get an alias
896         after this call. To prevent changing aliases use the exclude parameter.
897         """
898         def prefix_gen():
899             """
900             Generate a sequence of characters in alphabetical order:
901                 -> 'A', 'B', 'C', ...
902 
903             When the alphabet is finished, the sequence will continue with the
904             Cartesian product:
905                 -> 'AA', 'AB', 'AC', ...
906             """
907             alphabet = ascii_uppercase
908             prefix = chr(ord(self.alias_prefix) + 1)
909             yield prefix
910             for n in count(1):
911                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
912                 for s in product(seq, repeat=n):
913                     yield ''.join(s)
914                 prefix = None
915 
916         if self.alias_prefix != other_query.alias_prefix:
917             # No clashes between self and outer query should be possible.
918             return
919 
920         # Explicitly avoid infinite loop. The constant divider is based on how
921         # much depth recursive subquery references add to the stack. This value
922         # might need to be adjusted when adding or removing function calls from
923         # the code path in charge of performing these operations.
924         local_recursion_limit = sys.getrecursionlimit() // 16
925         for pos, prefix in enumerate(prefix_gen()):
926             if prefix not in self.subq_aliases:
927                 self.alias_prefix = prefix
928                 break
929             if pos > local_recursion_limit:
930                 raise RecursionError(
931                     'Maximum recursion depth exceeded: too many subqueries.'
932                 )
933         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
934         other_query.subq_aliases = other_query.subq_aliases.union(self.subq_aliases)
935         if exclude is None:
936             exclude = {}
937         self.change_aliases({
938             alias: '%s%d' % (self.alias_prefix, pos)
939             for pos, alias in enumerate(self.alias_map)
940             if alias not in exclude
941         })
942 
943     def get_initial_alias(self):
944         """
945         Return the first alias for this query, after increasing its reference
946         count.
947         """
948         if self.alias_map:
949             alias = self.base_table
950             self.ref_alias(alias)
951         else:
952             alias = self.join(self.base_table_class(self.get_meta().db_table, None))
953         return alias
954 
955     def count_active_tables(self):
956         """
957         Return the number of tables in this query with a non-zero reference
958         count. After execution, the reference counts are zeroed, so tables
959         added in compiler will not be seen by this method.
960         """
961         return len([1 for count in self.alias_refcount.values() if count])
962 
963     def join(self, join, reuse=None):
964         """
965         Return an alias for the 'join', either reusing an existing alias for
966         that join or creating a new one. 'join' is either a base_table_class or
967         join_class.
968 
969         The 'reuse' parameter can be either None which means all joins are
970         reusable, or it can be a set containing the aliases that can be reused.
971 
972         A join is always created as LOUTER if the lhs alias is LOUTER to make
973         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
974         joins are created as LOUTER if the join is nullable.
975         """
976         reuse_aliases = [
977             a for a, j in self.alias_map.items()
978             if (reuse is None or a in reuse) and j.equals(join)
979         ]
980         if reuse_aliases:
981             if join.table_alias in reuse_aliases:
982                 reuse_alias = join.table_alias
983             else:
984                 # Reuse the most recent alias of the joined table
985                 # (a many-to-many relation may be joined multiple times).
986                 reuse_alias = reuse_aliases[-1]
987             self.ref_alias(reuse_alias)
988             return reuse_alias
989 
990         # No reuse is possible, so we need a new alias.
991         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)
992         if join.join_type:
993             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
994                 join_type = LOUTER
995             else:
996                 join_type = INNER
997             join.join_type = join_type
998         join.table_alias = alias
999         self.alias_map[alias] = join
1000         return alias
1001 
1002     def join_parent_model(self, opts, model, alias, seen):
1003         """
1004         Make sure the given 'model' is joined in the query. If 'model' isn't
1005         a parent of 'opts' or if it is None this method is a no-op.
1006 
1007         The 'alias' is the root alias for starting the join, 'seen' is a dict
1008         of model -> alias of existing joins. It must also contain a mapping
1009         of None -> some alias. This will be returned in the no-op case.
1010         """
1011         if model in seen:
1012             return seen[model]
1013         chain = opts.get_base_chain(model)
1014         if not chain:
1015             return alias
1016         curr_opts = opts
1017         for int_model in chain:
1018             if int_model in seen:
1019                 curr_opts = int_model._meta
1020                 alias = seen[int_model]
1021                 continue
1022             # Proxy model have elements in base chain
1023             # with no parents, assign the new options
1024             # object and skip to the next base in that
1025             # case
1026             if not curr_opts.parents[int_model]:
1027                 curr_opts = int_model._meta
1028                 continue
1029             link_field = curr_opts.get_ancestor_link(int_model)
1030             join_info = self.setup_joins([link_field.name], curr_opts, alias)
1031             curr_opts = int_model._meta
1032             alias = seen[int_model] = join_info.joins[-1]
1033         return alias or seen[None]
1034 
1035     def add_annotation(self, annotation, alias, is_summary=False, select=True):
1036         """Add a single annotation expression to the Query."""
1037         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
1038                                                    summarize=is_summary)
1039         if select:
1040             self.append_annotation_mask([alias])
1041         else:
1042             self.set_annotation_mask(set(self.annotation_select).difference({alias}))
1043         self.annotations[alias] = annotation
1044 
1045     def resolve_expression(self, query, *args, **kwargs):
1046         clone = self.clone()
1047         # Subqueries need to use a different set of aliases than the outer query.
1048         clone.bump_prefix(query)
1049         clone.subquery = True
1050         clone.where.resolve_expression(query, *args, **kwargs)
1051         # Resolve combined queries.
1052         if clone.combinator:
1053             clone.combined_queries = tuple([
1054                 combined_query.resolve_expression(query, *args, **kwargs)
1055                 for combined_query in clone.combined_queries
1056             ])
1057         for key, value in clone.annotations.items():
1058             resolved = value.resolve_expression(query, *args, **kwargs)
1059             if hasattr(resolved, 'external_aliases'):
1060                 resolved.external_aliases.update(clone.external_aliases)
1061             clone.annotations[key] = resolved
1062         # Outer query's aliases are considered external.
1063         for alias, table in query.alias_map.items():
1064             clone.external_aliases[alias] = (
1065                 (isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias) or
1066                 (isinstance(table, BaseTable) and table.table_name != table.table_alias)
1067             )
1068         return clone
1069 
1070     def get_external_cols(self):
1071         exprs = chain(self.annotations.values(), self.where.children)
1072         return [
1073             col for col in self._gen_cols(exprs, include_external=True)
1074             if col.alias in self.external_aliases
1075         ]
1076 
1077     def get_group_by_cols(self, alias=None):
1078         if alias:
1079             return [Ref(alias, self)]
1080         external_cols = self.get_external_cols()
1081         if any(col.possibly_multivalued for col in external_cols):
1082             return [self]
1083         return external_cols
1084 
1085     def as_sql(self, compiler, connection):
1086         # Some backends (e.g. Oracle) raise an error when a subquery contains
1087         # unnecessary ORDER BY clause.
1088         if (
1089             self.subquery and
1090             not connection.features.ignores_unnecessary_order_by_in_subqueries
1091         ):
1092             self.clear_ordering(force=False)
1093         sql, params = self.get_compiler(connection=connection).as_sql()
1094         if self.subquery:
1095             sql = '(%s)' % sql
1096         return sql, params
1097 
1098     def resolve_lookup_value(self, value, can_reuse, allow_joins):
1099         if hasattr(value, 'resolve_expression'):
1100             value = value.resolve_expression(
1101                 self, reuse=can_reuse, allow_joins=allow_joins,
1102             )
1103         elif isinstance(value, (list, tuple)):
1104             # The items of the iterable may be expressions and therefore need
1105             # to be resolved independently.
1106             values = (
1107                 self.resolve_lookup_value(sub_value, can_reuse, allow_joins)
1108                 for sub_value in value
1109             )
1110             type_ = type(value)
1111             if hasattr(type_, '_make'):  # namedtuple
1112                 return type_(*values)
1113             return type_(values)
1114         return value
1115 
1116     def solve_lookup_type(self, lookup):
1117         """
1118         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').
1119         """
1120         lookup_splitted = lookup.split(LOOKUP_SEP)
1121         if self.annotations:
1122             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1123             if expression:
1124                 return expression_lookups, (), expression
1125         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1126         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1127         if len(lookup_parts) > 1 and not field_parts:
1128             raise FieldError(
1129                 'Invalid lookup "%s" for model %s".' %
1130                 (lookup, self.get_meta().model.__name__)
1131             )
1132         return lookup_parts, field_parts, False
1133 
1134     def check_query_object_type(self, value, opts, field):
1135         """
1136         Check whether the object passed while querying is of the correct type.
1137         If not, raise a ValueError specifying the wrong object.
1138         """
1139         if hasattr(value, '_meta'):
1140             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1141                 raise ValueError(
1142                     'Cannot query "%s": Must be "%s" instance.' %
1143                     (value, opts.object_name))
1144 
1145     def check_related_objects(self, field, value, opts):
1146         """Check the type of object passed to query relations."""
1147         if field.is_relation:
1148             # Check that the field and the queryset use the same model in a
1149             # query like .filter(author=Author.objects.all()). For example, the
1150             # opts would be Author's (from the author field) and value.model
1151             # would be Author.objects.all() queryset's .model (Author also).
1152             # The field is the related field on the lhs side.
1153             if (isinstance(value, Query) and not value.has_select_fields and
1154                     not check_rel_lookup_compatibility(value.model, opts, field)):
1155                 raise ValueError(
1156                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1157                     (value.model._meta.object_name, opts.object_name)
1158                 )
1159             elif hasattr(value, '_meta'):
1160                 self.check_query_object_type(value, opts, field)
1161             elif hasattr(value, '__iter__'):
1162                 for v in value:
1163                     self.check_query_object_type(v, opts, field)
1164 
1165     def check_filterable(self, expression):
1166         """Raise an error if expression cannot be used in a WHERE clause."""
1167         if (
1168             hasattr(expression, 'resolve_expression') and
1169             not getattr(expression, 'filterable', True)
1170         ):
1171             raise NotSupportedError(
1172                 expression.__class__.__name__ + ' is disallowed in the filter '
1173                 'clause.'
1174             )
1175         if hasattr(expression, 'get_source_expressions'):
1176             for expr in expression.get_source_expressions():
1177                 self.check_filterable(expr)
1178 
1179     def build_lookup(self, lookups, lhs, rhs):
1180         """
1181         Try to extract transforms and lookup from given lhs.
1182 
1183         The lhs value is something that works like SQLExpression.
1184         The rhs value is what the lookup is going to compare against.
1185         The lookups is a list of names to extract using get_lookup()
1186         and get_transform().
1187         """
1188         # __exact is the default lookup if one isn't given.
1189         *transforms, lookup_name = lookups or ['exact']
1190         for name in transforms:
1191             lhs = self.try_transform(lhs, name)
1192         # First try get_lookup() so that the lookup takes precedence if the lhs
1193         # supports both transform and lookup for the name.
1194         lookup_class = lhs.get_lookup(lookup_name)
1195         if not lookup_class:
1196             if lhs.field.is_relation:
1197                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))
1198             # A lookup wasn't found. Try to interpret the name as a transform
1199             # and do an Exact lookup against it.
1200             lhs = self.try_transform(lhs, lookup_name)
1201             lookup_name = 'exact'
1202             lookup_class = lhs.get_lookup(lookup_name)
1203             if not lookup_class:
1204                 return
1205 
1206         lookup = lookup_class(lhs, rhs)
1207         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
1208         # uses of None as a query value unless the lookup supports it.
1209         if lookup.rhs is None and not lookup.can_use_none_as_rhs:
1210             if lookup_name not in ('exact', 'iexact'):
1211                 raise ValueError("Cannot use None as a query value")
1212             return lhs.get_lookup('isnull')(lhs, True)
1213 
1214         # For Oracle '' is equivalent to null. The check must be done at this
1215         # stage because join promotion can't be done in the compiler. Using
1216         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.
1217         # A similar thing is done in is_nullable(), too.
1218         if (
1219             lookup_name == 'exact' and
1220             lookup.rhs == '' and
1221             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls
1222         ):
1223             return lhs.get_lookup('isnull')(lhs, True)
1224 
1225         return lookup
1226 
1227     def try_transform(self, lhs, name):
1228         """
1229         Helper method for build_lookup(). Try to fetch and initialize
1230         a transform for name parameter from lhs.
1231         """
1232         transform_class = lhs.get_transform(name)
1233         if transform_class:
1234             return transform_class(lhs)
1235         else:
1236             output_field = lhs.output_field.__class__
1237             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())
1238             if suggested_lookups:
1239                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)
1240             else:
1241                 suggestion = '.'
1242             raise FieldError(
1243                 "Unsupported lookup '%s' for %s or join on the field not "
1244                 "permitted%s" % (name, output_field.__name__, suggestion)
1245             )
1246 
1247     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1248                      can_reuse=None, allow_joins=True, split_subq=True,
1249                      check_filterable=True):
1250         """
1251         Build a WhereNode for a single filter clause but don't add it
1252         to this Query. Query.add_q() will then add this filter to the where
1253         Node.
1254 
1255         The 'branch_negated' tells us if the current branch contains any
1256         negations. This will be used to determine if subqueries are needed.
1257 
1258         The 'current_negated' is used to determine if the current filter is
1259         negated or not and this will be used to determine if IS NULL filtering
1260         is needed.
1261 
1262         The difference between current_negated and branch_negated is that
1263         branch_negated is set on first negation, but current_negated is
1264         flipped for each negation.
1265 
1266         Note that add_filter will not do any negating itself, that is done
1267         upper in the code by add_q().
1268 
1269         The 'can_reuse' is a set of reusable joins for multijoins.
1270 
1271         The method will create a filter clause that can be added to the current
1272         query. However, if the filter isn't added to the query then the caller
1273         is responsible for unreffing the joins used.
1274         """
1275         if isinstance(filter_expr, dict):
1276             raise FieldError("Cannot parse keyword query as dict")
1277         if isinstance(filter_expr, Q):
1278             return self._add_q(
1279                 filter_expr,
1280                 branch_negated=branch_negated,
1281                 current_negated=current_negated,
1282                 used_aliases=can_reuse,
1283                 allow_joins=allow_joins,
1284                 split_subq=split_subq,
1285                 check_filterable=check_filterable,
1286             )
1287         if hasattr(filter_expr, 'resolve_expression'):
1288             if not getattr(filter_expr, 'conditional', False):
1289                 raise TypeError('Cannot filter against a non-conditional expression.')
1290             condition = filter_expr.resolve_expression(self, allow_joins=allow_joins)
1291             if not isinstance(condition, Lookup):
1292                 condition = self.build_lookup(['exact'], condition, True)
1293             return WhereNode([condition], connector=AND), []
1294         arg, value = filter_expr
1295         if not arg:
1296             raise FieldError("Cannot parse keyword query %r" % arg)
1297         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1298 
1299         if check_filterable:
1300             self.check_filterable(reffed_expression)
1301 
1302         if not allow_joins and len(parts) > 1:
1303             raise FieldError("Joined field references are not permitted in this query")
1304 
1305         pre_joins = self.alias_refcount.copy()
1306         value = self.resolve_lookup_value(value, can_reuse, allow_joins)
1307         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1308 
1309         if check_filterable:
1310             self.check_filterable(value)
1311 
1312         if reffed_expression:
1313             condition = self.build_lookup(lookups, reffed_expression, value)
1314             return WhereNode([condition], connector=AND), []
1315 
1316         opts = self.get_meta()
1317         alias = self.get_initial_alias()
1318         allow_many = not branch_negated or not split_subq
1319 
1320         try:
1321             join_info = self.setup_joins(
1322                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1323             )
1324 
1325             # Prevent iterator from being consumed by check_related_objects()
1326             if isinstance(value, Iterator):
1327                 value = list(value)
1328             self.check_related_objects(join_info.final_field, value, join_info.opts)
1329 
1330             # split_exclude() needs to know which joins were generated for the
1331             # lookup parts
1332             self._lookup_joins = join_info.joins
1333         except MultiJoin as e:
1334             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1335 
1336         # Update used_joins before trimming since they are reused to determine
1337         # which joins could be later promoted to INNER.
1338         used_joins.update(join_info.joins)
1339         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1340         if can_reuse is not None:
1341             can_reuse.update(join_list)
1342 
1343         if join_info.final_field.is_relation:
1344             # No support for transforms for relational fields
1345             num_lookups = len(lookups)
1346             if num_lookups > 1:
1347                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1348             if len(targets) == 1:
1349                 col = self._get_col(targets[0], join_info.final_field, alias)
1350             else:
1351                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1352         else:
1353             col = self._get_col(targets[0], join_info.final_field, alias)
1354 
1355         condition = self.build_lookup(lookups, col, value)
1356         lookup_type = condition.lookup_name
1357         clause = WhereNode([condition], connector=AND)
1358 
1359         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1360         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1361             require_outer = True
1362             if lookup_type != 'isnull':
1363                 # The condition added here will be SQL like this:
1364                 # NOT (col IS NOT NULL), where the first NOT is added in
1365                 # upper layers of code. The reason for addition is that if col
1366                 # is null, then col != someval will result in SQL "unknown"
1367                 # which isn't the same as in Python. The Python None handling
1368                 # is wanted, and it can be gotten by
1369                 # (col IS NULL OR col != someval)
1370                 #   <=>
1371                 # NOT (col IS NOT NULL AND col = someval).
1372                 if (
1373                     self.is_nullable(targets[0]) or
1374                     self.alias_map[join_list[-1]].join_type == LOUTER
1375                 ):
1376                     lookup_class = targets[0].get_lookup('isnull')
1377                     col = self._get_col(targets[0], join_info.targets[0], alias)
1378                     clause.add(lookup_class(col, False), AND)
1379                 # If someval is a nullable column, someval IS NOT NULL is
1380                 # added.
1381                 if isinstance(value, Col) and self.is_nullable(value.target):
1382                     lookup_class = value.target.get_lookup('isnull')
1383                     clause.add(lookup_class(value, False), AND)
1384         return clause, used_joins if not require_outer else ()
1385 
1386     def add_filter(self, filter_lhs, filter_rhs):
1387         self.add_q(Q((filter_lhs, filter_rhs)))
1388 
1389     def add_q(self, q_object):
1390         """
1391         A preprocessor for the internal _add_q(). Responsible for doing final
1392         join promotion.
1393         """
1394         # For join promotion this case is doing an AND for the added q_object
1395         # and existing conditions. So, any existing inner join forces the join
1396         # type to remain inner. Existing outer joins can however be demoted.
1397         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1398         # rel_a doesn't produce any rows, then the whole condition must fail.
1399         # So, demotion is OK.
1400         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}
1401         clause, _ = self._add_q(q_object, self.used_aliases)
1402         if clause:
1403             self.where.add(clause, AND)
1404         self.demote_joins(existing_inner)
1405 
1406     def build_where(self, filter_expr):
1407         return self.build_filter(filter_expr, allow_joins=False)[0]
1408 
1409     def clear_where(self):
1410         self.where = WhereNode()
1411 
1412     def _add_q(self, q_object, used_aliases, branch_negated=False,
1413                current_negated=False, allow_joins=True, split_subq=True,
1414                check_filterable=True):
1415         """Add a Q-object to the current filter."""
1416         connector = q_object.connector
1417         current_negated = current_negated ^ q_object.negated
1418         branch_negated = branch_negated or q_object.negated
1419         target_clause = WhereNode(connector=connector, negated=q_object.negated)
1420         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1421         for child in q_object.children:
1422             child_clause, needed_inner = self.build_filter(
1423                 child, can_reuse=used_aliases, branch_negated=branch_negated,
1424                 current_negated=current_negated, allow_joins=allow_joins,
1425                 split_subq=split_subq, check_filterable=check_filterable,
1426             )
1427             joinpromoter.add_votes(needed_inner)
1428             if child_clause:
1429                 target_clause.add(child_clause, connector)
1430         needed_inner = joinpromoter.update_join_types(self)
1431         return target_clause, needed_inner
1432 
1433     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):
1434         """Add a FilteredRelation object to the current filter."""
1435         connector = q_object.connector
1436         current_negated ^= q_object.negated
1437         branch_negated = branch_negated or q_object.negated
1438         target_clause = WhereNode(connector=connector, negated=q_object.negated)
1439         for child in q_object.children:
1440             if isinstance(child, Node):
1441                 child_clause = self.build_filtered_relation_q(
1442                     child, reuse=reuse, branch_negated=branch_negated,
1443                     current_negated=current_negated,
1444                 )
1445             else:
1446                 child_clause, _ = self.build_filter(
1447                     child, can_reuse=reuse, branch_negated=branch_negated,
1448                     current_negated=current_negated,
1449                     allow_joins=True, split_subq=False,
1450                 )
1451             target_clause.add(child_clause, connector)
1452         return target_clause
1453 
1454     def add_filtered_relation(self, filtered_relation, alias):
1455         filtered_relation.alias = alias
1456         lookups = dict(get_children_from_q(filtered_relation.condition))
1457         relation_lookup_parts, relation_field_parts, _ = self.solve_lookup_type(filtered_relation.relation_name)
1458         if relation_lookup_parts:
1459             raise ValueError(
1460                 "FilteredRelation's relation_name cannot contain lookups "
1461                 "(got %r)." % filtered_relation.relation_name
1462             )
1463         for lookup in chain(lookups):
1464             lookup_parts, lookup_field_parts, _ = self.solve_lookup_type(lookup)
1465             shift = 2 if not lookup_parts else 1
1466             lookup_field_path = lookup_field_parts[:-shift]
1467             for idx, lookup_field_part in enumerate(lookup_field_path):
1468                 if len(relation_field_parts) > idx:
1469                     if relation_field_parts[idx] != lookup_field_part:
1470                         raise ValueError(
1471                             "FilteredRelation's condition doesn't support "
1472                             "relations outside the %r (got %r)."
1473                             % (filtered_relation.relation_name, lookup)
1474                         )
1475                 else:
1476                     raise ValueError(
1477                         "FilteredRelation's condition doesn't support nested "
1478                         "relations deeper than the relation_name (got %r for "
1479                         "%r)." % (lookup, filtered_relation.relation_name)
1480                     )
1481         self._filtered_relations[filtered_relation.alias] = filtered_relation
1482 
1483     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1484         """
1485         Walk the list of names and turns them into PathInfo tuples. A single
1486         name in 'names' can generate multiple PathInfos (m2m, for example).
1487 
1488         'names' is the path of names to travel, 'opts' is the model Options we
1489         start the name resolving from, 'allow_many' is as for setup_joins().
1490         If fail_on_missing is set to True, then a name that can't be resolved
1491         will generate a FieldError.
1492 
1493         Return a list of PathInfo tuples. In addition return the final field
1494         (the last used join field) and target (which is a field guaranteed to
1495         contain the same value as the final field). Finally, return those names
1496         that weren't found (which are likely transforms and the final lookup).
1497         """
1498         path, names_with_path = [], []
1499         for pos, name in enumerate(names):
1500             cur_names_with_path = (name, [])
1501             if name == 'pk':
1502                 name = opts.pk.name
1503 
1504             field = None
1505             filtered_relation = None
1506             try:
1507                 field = opts.get_field(name)
1508             except FieldDoesNotExist:
1509                 if name in self.annotation_select:
1510                     field = self.annotation_select[name].output_field
1511                 elif name in self._filtered_relations and pos == 0:
1512                     filtered_relation = self._filtered_relations[name]
1513                     if LOOKUP_SEP in filtered_relation.relation_name:
1514                         parts = filtered_relation.relation_name.split(LOOKUP_SEP)
1515                         filtered_relation_path, field, _, _ = self.names_to_path(
1516                             parts, opts, allow_many, fail_on_missing,
1517                         )
1518                         path.extend(filtered_relation_path[:-1])
1519                     else:
1520                         field = opts.get_field(filtered_relation.relation_name)
1521             if field is not None:
1522                 # Fields that contain one-to-many relations with a generic
1523                 # model (like a GenericForeignKey) cannot generate reverse
1524                 # relations and therefore cannot be used for reverse querying.
1525                 if field.is_relation and not field.related_model:
1526                     raise FieldError(
1527                         "Field %r does not generate an automatic reverse "
1528                         "relation and therefore cannot be used for reverse "
1529                         "querying. If it is a GenericForeignKey, consider "
1530                         "adding a GenericRelation." % name
1531                     )
1532                 try:
1533                     model = field.model._meta.concrete_model
1534                 except AttributeError:
1535                     # QuerySet.annotate() may introduce fields that aren't
1536                     # attached to a model.
1537                     model = None
1538             else:
1539                 # We didn't find the current field, so move position back
1540                 # one step.
1541                 pos -= 1
1542                 if pos == -1 or fail_on_missing:
1543                     available = sorted([
1544                         *get_field_names_from_opts(opts),
1545                         *self.annotation_select,
1546                         *self._filtered_relations,
1547                     ])
1548                     raise FieldError("Cannot resolve keyword '%s' into field. "
1549                                      "Choices are: %s" % (name, ", ".join(available)))
1550                 break
1551             # Check if we need any joins for concrete inheritance cases (the
1552             # field lives in parent, but we are currently in one of its
1553             # children)
1554             if model is not opts.model:
1555                 path_to_parent = opts.get_path_to_parent(model)
1556                 if path_to_parent:
1557                     path.extend(path_to_parent)
1558                     cur_names_with_path[1].extend(path_to_parent)
1559                     opts = path_to_parent[-1].to_opts
1560             if hasattr(field, 'path_infos'):
1561                 if filtered_relation:
1562                     pathinfos = field.get_path_info(filtered_relation)
1563                 else:
1564                     pathinfos = field.path_infos
1565                 if not allow_many:
1566                     for inner_pos, p in enumerate(pathinfos):
1567                         if p.m2m:
1568                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1569                             names_with_path.append(cur_names_with_path)
1570                             raise MultiJoin(pos + 1, names_with_path)
1571                 last = pathinfos[-1]
1572                 path.extend(pathinfos)
1573                 final_field = last.join_field
1574                 opts = last.to_opts
1575                 targets = last.target_fields
1576                 cur_names_with_path[1].extend(pathinfos)
1577                 names_with_path.append(cur_names_with_path)
1578             else:
1579                 # Local non-relational field.
1580                 final_field = field
1581                 targets = (field,)
1582                 if fail_on_missing and pos + 1 != len(names):
1583                     raise FieldError(
1584                         "Cannot resolve keyword %r into field. Join on '%s'"
1585                         " not permitted." % (names[pos + 1], name))
1586                 break
1587         return path, final_field, targets, names[pos + 1:]
1588 
1589     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True):
1590         """
1591         Compute the necessary table joins for the passage through the fields
1592         given in 'names'. 'opts' is the Options class for the current model
1593         (which gives the table we are starting from), 'alias' is the alias for
1594         the table to start the joining from.
1595 
1596         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1597         can be None in which case all joins are reusable or a set of aliases
1598         that can be reused. Note that non-reverse foreign keys are always
1599         reusable when using setup_joins().
1600 
1601         If 'allow_many' is False, then any reverse foreign key seen will
1602         generate a MultiJoin exception.
1603 
1604         Return the final field involved in the joins, the target field (used
1605         for any 'where' constraint), the final 'opts' value, the joins, the
1606         field path traveled to generate the joins, and a transform function
1607         that takes a field and alias and is equivalent to `field.get_col(alias)`
1608         in the simple case but wraps field transforms if they were included in
1609         names.
1610 
1611         The target field is the field containing the concrete value. Final
1612         field can be something different, for example foreign key pointing to
1613         that value. Final field is needed for example in some value
1614         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1615         key field for example).
1616         """
1617         joins = [alias]
1618         # The transform can't be applied yet, as joins must be trimmed later.
1619         # To avoid making every caller of this method look up transforms
1620         # directly, compute transforms here and create a partial that converts
1621         # fields to the appropriate wrapped version.
1622 
1623         def final_transformer(field, alias):
1624             if not self.alias_cols:
1625                 alias = None
1626             return field.get_col(alias)
1627 
1628         # Try resolving all the names as fields first. If there's an error,
1629         # treat trailing names as lookups until a field can be resolved.
1630         last_field_exception = None
1631         for pivot in range(len(names), 0, -1):
1632             try:
1633                 path, final_field, targets, rest = self.names_to_path(
1634                     names[:pivot], opts, allow_many, fail_on_missing=True,
1635                 )
1636             except FieldError as exc:
1637                 if pivot == 1:
1638                     # The first item cannot be a lookup, so it's safe
1639                     # to raise the field error here.
1640                     raise
1641                 else:
1642                     last_field_exception = exc
1643             else:
1644                 # The transforms are the remaining items that couldn't be
1645                 # resolved into fields.
1646                 transforms = names[pivot:]
1647                 break
1648         for name in transforms:
1649             def transform(field, alias, *, name, previous):
1650                 try:
1651                     wrapped = previous(field, alias)
1652                     return self.try_transform(wrapped, name)
1653                 except FieldError:
1654                     # FieldError is raised if the transform doesn't exist.
1655                     if isinstance(final_field, Field) and last_field_exception:
1656                         raise last_field_exception
1657                     else:
1658                         raise
1659             final_transformer = functools.partial(transform, name=name, previous=final_transformer)
1660         # Then, add the path to the query's joins. Note that we can't trim
1661         # joins at this stage - we will need the information about join type
1662         # of the trimmed joins.
1663         for join in path:
1664             if join.filtered_relation:
1665                 filtered_relation = join.filtered_relation.clone()
1666                 table_alias = filtered_relation.alias
1667             else:
1668                 filtered_relation = None
1669                 table_alias = None
1670             opts = join.to_opts
1671             if join.direct:
1672                 nullable = self.is_nullable(join.join_field)
1673             else:
1674                 nullable = True
1675             connection = self.join_class(
1676                 opts.db_table, alias, table_alias, INNER, join.join_field,
1677                 nullable, filtered_relation=filtered_relation,
1678             )
1679             reuse = can_reuse if join.m2m else None
1680             alias = self.join(connection, reuse=reuse)
1681             joins.append(alias)
1682             if filtered_relation:
1683                 filtered_relation.path = joins[:]
1684         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)
1685 
1686     def trim_joins(self, targets, joins, path):
1687         """
1688         The 'target' parameter is the final field being joined to, 'joins'
1689         is the full list of join aliases. The 'path' contain the PathInfos
1690         used to create the joins.
1691 
1692         Return the final target field and table alias and the new active
1693         joins.
1694 
1695         Always trim any direct join if the target column is already in the
1696         previous table. Can't trim reverse joins as it's unknown if there's
1697         anything on the other side of the join.
1698         """
1699         joins = joins[:]
1700         for pos, info in enumerate(reversed(path)):
1701             if len(joins) == 1 or not info.direct:
1702                 break
1703             if info.filtered_relation:
1704                 break
1705             join_targets = {t.column for t in info.join_field.foreign_related_fields}
1706             cur_targets = {t.column for t in targets}
1707             if not cur_targets.issubset(join_targets):
1708                 break
1709             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1710             targets = tuple(targets_dict[t.column] for t in targets)
1711             self.unref_alias(joins.pop())
1712         return targets, joins[-1], joins
1713 
1714     @classmethod
1715     def _gen_cols(cls, exprs, include_external=False):
1716         for expr in exprs:
1717             if isinstance(expr, Col):
1718                 yield expr
1719             elif include_external and callable(getattr(expr, 'get_external_cols', None)):
1720                 yield from expr.get_external_cols()
1721             elif hasattr(expr, 'get_source_expressions'):
1722                 yield from cls._gen_cols(
1723                     expr.get_source_expressions(),
1724                     include_external=include_external,
1725                 )
1726 
1727     @classmethod
1728     def _gen_col_aliases(cls, exprs):
1729         yield from (expr.alias for expr in cls._gen_cols(exprs))
1730 
1731     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1732         annotation = self.annotations.get(name)
1733         if annotation is not None:
1734             if not allow_joins:
1735                 for alias in self._gen_col_aliases([annotation]):
1736                     if isinstance(self.alias_map[alias], Join):
1737                         raise FieldError(
1738                             'Joined field references are not permitted in '
1739                             'this query'
1740                         )
1741             if summarize:
1742                 # Summarize currently means we are doing an aggregate() query
1743                 # which is executed as a wrapped subquery if any of the
1744                 # aggregate() elements reference an existing annotation. In
1745                 # that case we need to return a Ref to the subquery's annotation.
1746                 if name not in self.annotation_select:
1747                     raise FieldError(
1748                         "Cannot aggregate over the '%s' alias. Use annotate() "
1749                         "to promote it." % name
1750                     )
1751                 return Ref(name, self.annotation_select[name])
1752             else:
1753                 return annotation
1754         else:
1755             field_list = name.split(LOOKUP_SEP)
1756             annotation = self.annotations.get(field_list[0])
1757             if annotation is not None:
1758                 for transform in field_list[1:]:
1759                     annotation = self.try_transform(annotation, transform)
1760                 return annotation
1761             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)
1762             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1763             if not allow_joins and len(join_list) > 1:
1764                 raise FieldError('Joined field references are not permitted in this query')
1765             if len(targets) > 1:
1766                 raise FieldError("Referencing multicolumn fields with F() objects "
1767                                  "isn't supported")
1768             # Verify that the last lookup in name is a field or a transform:
1769             # transform_function() raises FieldError if not.
1770             transform = join_info.transform_function(targets[0], final_alias)
1771             if reuse is not None:
1772                 reuse.update(join_list)
1773             return transform
1774 
1775     def split_exclude(self, filter_expr, can_reuse, names_with_path):
1776         """
1777         When doing an exclude against any kind of N-to-many relation, we need
1778         to use a subquery. This method constructs the nested query, given the
1779         original exclude filter (filter_expr) and the portion up to the first
1780         N-to-many relation field.
1781 
1782         For example, if the origin filter is ~Q(child__name='foo'), filter_expr
1783         is ('child__name', 'foo') and can_reuse is a set of joins usable for
1784         filters in the original query.
1785 
1786         We will turn this into equivalent of:
1787             WHERE NOT EXISTS(
1788                 SELECT 1
1789                 FROM child
1790                 WHERE name = 'foo' AND child.parent_id = parent.id
1791                 LIMIT 1
1792             )
1793         """
1794         # Generate the inner query.
1795         query = self.__class__(self.model)
1796         query._filtered_relations = self._filtered_relations
1797         filter_lhs, filter_rhs = filter_expr
1798         if isinstance(filter_rhs, OuterRef):
1799             filter_rhs = OuterRef(filter_rhs)
1800         elif isinstance(filter_rhs, F):
1801             filter_rhs = OuterRef(filter_rhs.name)
1802         query.add_filter(filter_lhs, filter_rhs)
1803         query.clear_ordering(force=True)
1804         # Try to have as simple as possible subquery -> trim leading joins from
1805         # the subquery.
1806         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1807 
1808         col = query.select[0]
1809         select_field = col.target
1810         alias = col.alias
1811         if alias in can_reuse:
1812             pk = select_field.model._meta.pk
1813             # Need to add a restriction so that outer query's filters are in effect for
1814             # the subquery, too.
1815             query.bump_prefix(self)
1816             lookup_class = select_field.get_lookup('exact')
1817             # Note that the query.select[0].alias is different from alias
1818             # due to bump_prefix above.
1819             lookup = lookup_class(pk.get_col(query.select[0].alias),
1820                                   pk.get_col(alias))
1821             query.where.add(lookup, AND)
1822             query.external_aliases[alias] = True
1823 
1824         lookup_class = select_field.get_lookup('exact')
1825         lookup = lookup_class(col, ResolvedOuterRef(trimmed_prefix))
1826         query.where.add(lookup, AND)
1827         condition, needed_inner = self.build_filter(Exists(query))
1828 
1829         if contains_louter:
1830             or_null_condition, _ = self.build_filter(
1831                 ('%s__isnull' % trimmed_prefix, True),
1832                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1833             condition.add(or_null_condition, OR)
1834             # Note that the end result will be:
1835             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1836             # This might look crazy but due to how IN works, this seems to be
1837             # correct. If the IS NOT NULL check is removed then outercol NOT
1838             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1839             # outercol IS NULL we will not match the row.
1840         return condition, needed_inner
1841 
1842     def set_empty(self):
1843         self.where.add(NothingNode(), AND)
1844         for query in self.combined_queries:
1845             query.set_empty()
1846 
1847     def is_empty(self):
1848         return any(isinstance(c, NothingNode) for c in self.where.children)
1849 
1850     def set_limits(self, low=None, high=None):
1851         """
1852         Adjust the limits on the rows retrieved. Use low/high to set these,
1853         as it makes it more Pythonic to read and write. When the SQL query is
1854         created, convert them to the appropriate offset and limit values.
1855 
1856         Apply any limits passed in here to the existing constraints. Add low
1857         to the current low value and clamp both to any existing high value.
1858         """
1859         if high is not None:
1860             if self.high_mark is not None:
1861                 self.high_mark = min(self.high_mark, self.low_mark + high)
1862             else:
1863                 self.high_mark = self.low_mark + high
1864         if low is not None:
1865             if self.high_mark is not None:
1866                 self.low_mark = min(self.high_mark, self.low_mark + low)
1867             else:
1868                 self.low_mark = self.low_mark + low
1869 
1870         if self.low_mark == self.high_mark:
1871             self.set_empty()
1872 
1873     def clear_limits(self):
1874         """Clear any existing limits."""
1875         self.low_mark, self.high_mark = 0, None
1876 
1877     @property
1878     def is_sliced(self):
1879         return self.low_mark != 0 or self.high_mark is not None
1880 
1881     def has_limit_one(self):
1882         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1
1883 
1884     def can_filter(self):
1885         """
1886         Return True if adding filters to this instance is still possible.
1887 
1888         Typically, this means no limits or offsets have been put on the results.
1889         """
1890         return not self.is_sliced
1891 
1892     def clear_select_clause(self):
1893         """Remove all fields from SELECT clause."""
1894         self.select = ()
1895         self.default_cols = False
1896         self.select_related = False
1897         self.set_extra_mask(())
1898         self.set_annotation_mask(())
1899 
1900     def clear_select_fields(self):
1901         """
1902         Clear the list of fields to select (but not extra_select columns).
1903         Some queryset types completely replace any existing list of select
1904         columns.
1905         """
1906         self.select = ()
1907         self.values_select = ()
1908 
1909     def add_select_col(self, col, name):
1910         self.select += col,
1911         self.values_select += name,
1912 
1913     def set_select(self, cols):
1914         self.default_cols = False
1915         self.select = tuple(cols)
1916 
1917     def add_distinct_fields(self, *field_names):
1918         """
1919         Add and resolve the given fields to the query's "distinct on" clause.
1920         """
1921         self.distinct_fields = field_names
1922         self.distinct = True
1923 
1924     def add_fields(self, field_names, allow_m2m=True):
1925         """
1926         Add the given (model) fields to the select set. Add the field names in
1927         the order specified.
1928         """
1929         alias = self.get_initial_alias()
1930         opts = self.get_meta()
1931 
1932         try:
1933             cols = []
1934             for name in field_names:
1935                 # Join promotion note - we must not remove any rows here, so
1936                 # if there is no existing joins, use outer join.
1937                 join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1938                 targets, final_alias, joins = self.trim_joins(
1939                     join_info.targets,
1940                     join_info.joins,
1941                     join_info.path,
1942                 )
1943                 for target in targets:
1944                     cols.append(join_info.transform_function(target, final_alias))
1945             if cols:
1946                 self.set_select(cols)
1947         except MultiJoin:
1948             raise FieldError("Invalid field name: '%s'" % name)
1949         except FieldError:
1950             if LOOKUP_SEP in name:
1951                 # For lookups spanning over relationships, show the error
1952                 # from the model on which the lookup failed.
1953                 raise
1954             elif name in self.annotations:
1955                 raise FieldError(
1956                     "Cannot select the '%s' alias. Use annotate() to promote "
1957                     "it." % name
1958                 )
1959             else:
1960                 names = sorted([
1961                     *get_field_names_from_opts(opts), *self.extra,
1962                     *self.annotation_select, *self._filtered_relations
1963                 ])
1964                 raise FieldError("Cannot resolve keyword %r into field. "
1965                                  "Choices are: %s" % (name, ", ".join(names)))
1966 
1967     def add_ordering(self, *ordering):
1968         """
1969         Add items from the 'ordering' sequence to the query's "order by"
1970         clause. These items are either field names (not column names) --
1971         possibly with a direction prefix ('-' or '?') -- or OrderBy
1972         expressions.
1973 
1974         If 'ordering' is empty, clear all ordering from the query.
1975         """
1976         errors = []
1977         for item in ordering:
1978             if isinstance(item, str):
1979                 if item == '?':
1980                     continue
1981                 if item.startswith('-'):
1982                     item = item[1:]
1983                 if item in self.annotations:
1984                     continue
1985                 if self.extra and item in self.extra:
1986                     continue
1987                 # names_to_path() validates the lookup. A descriptive
1988                 # FieldError will be raise if it's not.
1989                 self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)
1990             elif not hasattr(item, 'resolve_expression'):
1991                 errors.append(item)
1992             if getattr(item, 'contains_aggregate', False):
1993                 raise FieldError(
1994                     'Using an aggregate in order_by() without also including '
1995                     'it in annotate() is not allowed: %s' % item
1996                 )
1997         if errors:
1998             raise FieldError('Invalid order_by arguments: %s' % errors)
1999         if ordering:
2000             self.order_by += ordering
2001         else:
2002             self.default_ordering = False
2003 
2004     def clear_ordering(self, force=False, clear_default=True):
2005         """
2006         Remove any ordering settings if the current query allows it without
2007         side effects, set 'force' to True to clear the ordering regardless.
2008         If 'clear_default' is True, there will be no ordering in the resulting
2009         query (not even the model's default).
2010         """
2011         if not force and (self.is_sliced or self.distinct_fields or self.select_for_update):
2012             return
2013         self.order_by = ()
2014         self.extra_order_by = ()
2015         if clear_default:
2016             self.default_ordering = False
2017 
2018     def set_group_by(self, allow_aliases=True):
2019         """
2020         Expand the GROUP BY clause required by the query.
2021 
2022         This will usually be the set of all non-aggregate fields in the
2023         return data. If the database backend supports grouping by the
2024         primary key, and the query would be equivalent, the optimization
2025         will be made automatically.
2026         """
2027         # Column names from JOINs to check collisions with aliases.
2028         if allow_aliases:
2029             column_names = set()
2030             seen_models = set()
2031             for join in list(self.alias_map.values())[1:]:  # Skip base table.
2032                 model = join.join_field.related_model
2033                 if model not in seen_models:
2034                     column_names.update({
2035                         field.column
2036                         for field in model._meta.local_concrete_fields
2037                     })
2038                     seen_models.add(model)
2039 
2040         group_by = list(self.select)
2041         if self.annotation_select:
2042             for alias, annotation in self.annotation_select.items():
2043                 if not allow_aliases or alias in column_names:
2044                     alias = None
2045                 group_by_cols = annotation.get_group_by_cols(alias=alias)
2046                 group_by.extend(group_by_cols)
2047         self.group_by = tuple(group_by)
2048 
2049     def add_select_related(self, fields):
2050         """
2051         Set up the select_related data structure so that we only select
2052         certain related models (as opposed to all models, when
2053         self.select_related=True).
2054         """
2055         if isinstance(self.select_related, bool):
2056             field_dict = {}
2057         else:
2058             field_dict = self.select_related
2059         for field in fields:
2060             d = field_dict
2061             for part in field.split(LOOKUP_SEP):
2062                 d = d.setdefault(part, {})
2063         self.select_related = field_dict
2064 
2065     def add_extra(self, select, select_params, where, params, tables, order_by):
2066         """
2067         Add data to the various extra_* attributes for user-created additions
2068         to the query.
2069         """
2070         if select:
2071             # We need to pair any placeholder markers in the 'select'
2072             # dictionary with their parameters in 'select_params' so that
2073             # subsequent updates to the select dictionary also adjust the
2074             # parameters appropriately.
2075             select_pairs = {}
2076             if select_params:
2077                 param_iter = iter(select_params)
2078             else:
2079                 param_iter = iter([])
2080             for name, entry in select.items():
2081                 entry = str(entry)
2082                 entry_params = []
2083                 pos = entry.find("%s")
2084                 while pos != -1:
2085                     if pos == 0 or entry[pos - 1] != '%':
2086                         entry_params.append(next(param_iter))
2087                     pos = entry.find("%s", pos + 2)
2088                 select_pairs[name] = (entry, entry_params)
2089             self.extra.update(select_pairs)
2090         if where or params:
2091             self.where.add(ExtraWhere(where, params), AND)
2092         if tables:
2093             self.extra_tables += tuple(tables)
2094         if order_by:
2095             self.extra_order_by = order_by
2096 
2097     def clear_deferred_loading(self):
2098         """Remove any fields from the deferred loading set."""
2099         self.deferred_loading = (frozenset(), True)
2100 
2101     def add_deferred_loading(self, field_names):
2102         """
2103         Add the given list of model field names to the set of fields to
2104         exclude from loading from the database when automatic column selection
2105         is done. Add the new field names to any existing field names that
2106         are deferred (or removed from any existing field names that are marked
2107         as the only ones for immediate loading).
2108         """
2109         # Fields on related models are stored in the literal double-underscore
2110         # format, so that we can use a set datastructure. We do the foo__bar
2111         # splitting and handling when computing the SQL column names (as part of
2112         # get_columns()).
2113         existing, defer = self.deferred_loading
2114         if defer:
2115             # Add to existing deferred names.
2116             self.deferred_loading = existing.union(field_names), True
2117         else:
2118             # Remove names from the set of any existing "immediate load" names.
2119             if new_existing := existing.difference(field_names):
2120                 self.deferred_loading = new_existing, False
2121             else:
2122                 self.clear_deferred_loading()
2123                 if new_only := set(field_names).difference(existing):
2124                     self.deferred_loading = new_only, True
2125 
2126     def add_immediate_loading(self, field_names):
2127         """
2128         Add the given list of model field names to the set of fields to
2129         retrieve when the SQL is executed ("immediate loading" fields). The
2130         field names replace any existing immediate loading field names. If
2131         there are field names already specified for deferred loading, remove
2132         those names from the new field_names before storing the new names
2133         for immediate loading. (That is, immediate loading overrides any
2134         existing immediate values, but respects existing deferrals.)
2135         """
2136         existing, defer = self.deferred_loading
2137         field_names = set(field_names)
2138         if 'pk' in field_names:
2139             field_names.remove('pk')
2140             field_names.add(self.get_meta().pk.name)
2141 
2142         if defer:
2143             # Remove any existing deferred names from the current set before
2144             # setting the new names.
2145             self.deferred_loading = field_names.difference(existing), False
2146         else:
2147             # Replace any existing "immediate load" field names.
2148             self.deferred_loading = frozenset(field_names), False
2149 
2150     def get_loaded_field_names(self):
2151         """
2152         If any fields are marked to be deferred, return a dictionary mapping
2153         models to a set of names in those fields that will be loaded. If a
2154         model is not in the returned dictionary, none of its fields are
2155         deferred.
2156 
2157         If no fields are marked for deferral, return an empty dictionary.
2158         """
2159         # We cache this because we call this function multiple times
2160         # (compiler.fill_related_selections, query.iterator)
2161         try:
2162             return self._loaded_field_names_cache
2163         except AttributeError:
2164             collection = {}
2165             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
2166             self._loaded_field_names_cache = collection
2167             return collection
2168 
2169     def get_loaded_field_names_cb(self, target, model, fields):
2170         """Callback used by get_deferred_field_names()."""
2171         target[model] = {f.attname for f in fields}
2172 
2173     def set_annotation_mask(self, names):
2174         """Set the mask of annotations that will be returned by the SELECT."""
2175         if names is None:
2176             self.annotation_select_mask = None
2177         else:
2178             self.annotation_select_mask = set(names)
2179         self._annotation_select_cache = None
2180 
2181     def append_annotation_mask(self, names):
2182         if self.annotation_select_mask is not None:
2183             self.set_annotation_mask(self.annotation_select_mask.union(names))
2184 
2185     def set_extra_mask(self, names):
2186         """
2187         Set the mask of extra select items that will be returned by SELECT.
2188         Don't remove them from the Query since they might be used later.
2189         """
2190         if names is None:
2191             self.extra_select_mask = None
2192         else:
2193             self.extra_select_mask = set(names)
2194         self._extra_select_cache = None
2195 
2196     def set_values(self, fields):
2197         self.select_related = False
2198         self.clear_deferred_loading()
2199         self.clear_select_fields()
2200 
2201         if fields:
2202             field_names = []
2203             extra_names = []
2204             annotation_names = []
2205             if not self.extra and not self.annotations:
2206                 # Shortcut - if there are no extra or annotations, then
2207                 # the values() clause must be just field names.
2208                 field_names = list(fields)
2209             else:
2210                 self.default_cols = False
2211                 for f in fields:
2212                     if f in self.extra_select:
2213                         extra_names.append(f)
2214                     elif f in self.annotation_select:
2215                         annotation_names.append(f)
2216                     else:
2217                         field_names.append(f)
2218             self.set_extra_mask(extra_names)
2219             self.set_annotation_mask(annotation_names)
2220             selected = frozenset(field_names + extra_names + annotation_names)
2221         else:
2222             field_names = [f.attname for f in self.model._meta.concrete_fields]
2223             selected = frozenset(field_names)
2224         # Selected annotations must be known before setting the GROUP BY
2225         # clause.
2226         if self.group_by is True:
2227             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
2228             # Disable GROUP BY aliases to avoid orphaning references to the
2229             # SELECT clause which is about to be cleared.
2230             self.set_group_by(allow_aliases=False)
2231             self.clear_select_fields()
2232         elif self.group_by:
2233             # Resolve GROUP BY annotation references if they are not part of
2234             # the selected fields anymore.
2235             group_by = []
2236             for expr in self.group_by:
2237                 if isinstance(expr, Ref) and expr.refs not in selected:
2238                     expr = self.annotations[expr.refs]
2239                 group_by.append(expr)
2240             self.group_by = tuple(group_by)
2241 
2242         self.values_select = tuple(field_names)
2243         self.add_fields(field_names, True)
2244 
2245     @property
2246     def annotation_select(self):
2247         """
2248         Return the dictionary of aggregate columns that are not masked and
2249         should be used in the SELECT clause. Cache this result for performance.
2250         """
2251         if self._annotation_select_cache is not None:
2252             return self._annotation_select_cache
2253         elif not self.annotations:
2254             return {}
2255         elif self.annotation_select_mask is not None:
2256             self._annotation_select_cache = {
2257                 k: v for k, v in self.annotations.items()
2258                 if k in self.annotation_select_mask
2259             }
2260             return self._annotation_select_cache
2261         else:
2262             return self.annotations
2263 
2264     @property
2265     def extra_select(self):
2266         if self._extra_select_cache is not None:
2267             return self._extra_select_cache
2268         if not self.extra:
2269             return {}
2270         elif self.extra_select_mask is not None:
2271             self._extra_select_cache = {
2272                 k: v for k, v in self.extra.items()
2273                 if k in self.extra_select_mask
2274             }
2275             return self._extra_select_cache
2276         else:
2277             return self.extra
2278 
2279     def trim_start(self, names_with_path):
2280         """
2281         Trim joins from the start of the join path. The candidates for trim
2282         are the PathInfos in names_with_path structure that are m2m joins.
2283 
2284         Also set the select column so the start matches the join.
2285 
2286         This method is meant to be used for generating the subquery joins &
2287         cols in split_exclude().
2288 
2289         Return a lookup usable for doing outerq.filter(lookup=self) and a
2290         boolean indicating if the joins in the prefix contain a LEFT OUTER join.
2291         _"""
2292         all_paths = []
2293         for _, paths in names_with_path:
2294             all_paths.extend(paths)
2295         contains_louter = False
2296         # Trim and operate only on tables that were generated for
2297         # the lookup part of the query. That is, avoid trimming
2298         # joins generated for F() expressions.
2299         lookup_tables = [
2300             t for t in self.alias_map
2301             if t in self._lookup_joins or t == self.base_table
2302         ]
2303         for trimmed_paths, path in enumerate(all_paths):
2304             if path.m2m:
2305                 break
2306             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
2307                 contains_louter = True
2308             alias = lookup_tables[trimmed_paths]
2309             self.unref_alias(alias)
2310         # The path.join_field is a Rel, lets get the other side's field
2311         join_field = path.join_field.field
2312         # Build the filter prefix.
2313         paths_in_prefix = trimmed_paths
2314         trimmed_prefix = []
2315         for name, path in names_with_path:
2316             if paths_in_prefix - len(path) < 0:
2317                 break
2318             trimmed_prefix.append(name)
2319             paths_in_prefix -= len(path)
2320         trimmed_prefix.append(
2321             join_field.foreign_related_fields[0].name)
2322         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
2323         # Lets still see if we can trim the first join from the inner query
2324         # (that is, self). We can't do this for:
2325         # - LEFT JOINs because we would miss those rows that have nothing on
2326         #   the outer side,
2327         # - INNER JOINs from filtered relations because we would miss their
2328         #   filters.
2329         first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]
2330         if first_join.join_type != LOUTER and not first_join.filtered_relation:
2331             select_fields = [r[0] for r in join_field.related_fields]
2332             select_alias = lookup_tables[trimmed_paths + 1]
2333             self.unref_alias(lookup_tables[trimmed_paths])
2334             extra_restriction = join_field.get_extra_restriction(None, lookup_tables[trimmed_paths + 1])
2335             if extra_restriction:
2336                 self.where.add(extra_restriction, AND)
2337         else:
2338             # TODO: It might be possible to trim more joins from the start of the
2339             # inner query if it happens to have a longer join chain containing the
2340             # values in select_fields. Lets punt this one for now.
2341             select_fields = [r[1] for r in join_field.related_fields]
2342             select_alias = lookup_tables[trimmed_paths]
2343         # The found starting point is likely a join_class instead of a
2344         # base_table_class reference. But the first entry in the query's FROM
2345         # clause must not be a JOIN.
2346         for table in self.alias_map:
2347             if self.alias_refcount[table] > 0:
2348                 self.alias_map[table] = self.base_table_class(
2349                     self.alias_map[table].table_name,
2350                     table,
2351                 )
2352                 break
2353         self.set_select([f.get_col(select_alias) for f in select_fields])
2354         return trimmed_prefix, contains_louter
2355 
2356     def is_nullable(self, field):
2357         """
2358         Check if the given field should be treated as nullable.
2359 
2360         Some backends treat '' as null and Django treats such fields as
2361         nullable for those backends. In such situations field.null can be
2362         False even if we should treat the field as nullable.
2363         """
2364         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2365         # (nor should it have) knowledge of which connection is going to be
2366         # used. The proper fix would be to defer all decisions where
2367         # is_nullable() is needed to the compiler stage, but that is not easy
2368         # to do currently.
2369         return field.null or (
2370             field.empty_strings_allowed and
2371             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls
2372         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 03:58:11.011 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 03:58:11.011 | INFO     | Search-only: skipping patch selection
2025-10-15 03:58:11.159 | INFO     | Task django__django-15382 completed successfully.
2025-10-15 03:58:11.160 | INFO     | No patch generated. You can try running ACR again.
