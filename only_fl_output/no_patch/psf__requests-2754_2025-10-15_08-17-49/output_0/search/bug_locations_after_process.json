[
    {
        "rel_file_path": "requests/models.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/psf__requests/setup_psf__requests__3.0/requests/models.py",
        "start": 1,
        "end": 856,
        "class_name": null,
        "method_name": null,
        "code": "1 # -*- coding: utf-8 -*-\n2 \n3 \"\"\"\n4 requests.models\n5 ~~~~~~~~~~~~~~~\n6 \n7 This module contains the primary objects that power Requests.\n8 \"\"\"\n9 \n10 import collections\n11 import datetime\n12 \n13 from io import BytesIO, UnsupportedOperation\n14 from .hooks import default_hooks\n15 from .structures import CaseInsensitiveDict\n16 \n17 import requests\n18 from .auth import HTTPBasicAuth\n19 from .cookies import cookiejar_from_dict, get_cookie_header, _copy_cookie_jar\n20 from .packages.urllib3.fields import RequestField\n21 from .packages.urllib3.filepost import encode_multipart_formdata\n22 from .packages.urllib3.util import parse_url\n23 from .packages.urllib3.exceptions import (\n24     DecodeError, ReadTimeoutError, ProtocolError, LocationParseError)\n25 from .exceptions import (\n26     HTTPError, MissingScheme, InvalidURL, ChunkedEncodingError,\n27     ContentDecodingError, ConnectionError, StreamConsumedError)\n28 from .utils import (\n29     guess_filename, get_auth_from_url, requote_uri,\n30     stream_decode_response_unicode, to_key_val_list, parse_header_links,\n31     iter_slices, guess_json_utf, super_len, to_native_string)\n32 from .compat import (\n33     cookielib, urlunparse, urlsplit, urlencode, str, bytes, StringIO,\n34     is_py2, chardet, builtin_str, basestring)\n35 import json as complexjson\n36 from .status_codes import codes\n37 \n38 #: The set of HTTP status codes that indicate an automatically\n39 #: processable redirect.\n40 REDIRECT_STATI = (\n41     codes.moved,              # 301\n42     codes.found,              # 302\n43     codes.other,              # 303\n44     codes.temporary_redirect, # 307\n45     codes.permanent_redirect, # 308\n46 )\n47 \n48 DEFAULT_REDIRECT_LIMIT = 30\n49 CONTENT_CHUNK_SIZE = 10 * 1024\n50 ITER_CHUNK_SIZE = 512\n51 \n52 \n53 class RequestEncodingMixin(object):\n54     @property\n55     def path_url(self):\n56         \"\"\"Build the path URL to use.\"\"\"\n57 \n58         url = []\n59 \n60         p = urlsplit(self.url)\n61 \n62         path = p.path\n63         if not path:\n64             path = '/'\n65 \n66         url.append(path)\n67 \n68         query = p.query\n69         if query:\n70             url.append('?')\n71             url.append(query)\n72 \n73         return ''.join(url)\n74 \n75     @staticmethod\n76     def _encode_params(data):\n77         \"\"\"Encode parameters in a piece of data.\n78 \n79         Will successfully encode parameters when passed as a dict or a list of\n80         2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n81         if parameters are supplied as a dict.\n82         \"\"\"\n83 \n84         if isinstance(data, (str, bytes)):\n85             return data\n86         elif hasattr(data, 'read'):\n87             return data\n88         elif hasattr(data, '__iter__'):\n89             result = []\n90             for k, vs in to_key_val_list(data):\n91                 if isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\n92                     vs = [vs]\n93                 for v in vs:\n94                     if v is not None:\n95                         result.append(\n96                             (k.encode('utf-8') if isinstance(k, str) else k,\n97                              v.encode('utf-8') if isinstance(v, str) else v))\n98             return urlencode(result, doseq=True)\n99         else:\n100             return data\n101 \n102     @staticmethod\n103     def _encode_files(files, data):\n104         \"\"\"Build the body for a multipart/form-data request.\n105 \n106         Will successfully encode files when passed as a dict or a list of\n107         2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n108         if parameters are supplied as a dict.\n109 \n110         \"\"\"\n111         if (not files):\n112             raise ValueError(\"Files must be provided.\")\n113         elif isinstance(data, basestring):\n114             raise ValueError(\"Data must not be a string.\")\n115 \n116         new_fields = []\n117         fields = to_key_val_list(data or {})\n118         files = to_key_val_list(files or {})\n119 \n120         for field, val in fields:\n121             if isinstance(val, basestring) or not hasattr(val, '__iter__'):\n122                 val = [val]\n123             for v in val:\n124                 if v is not None:\n125                     # Don't call str() on bytestrings: in Py3 it all goes wrong.\n126                     if not isinstance(v, bytes):\n127                         v = str(v)\n128 \n129                     new_fields.append(\n130                         (field.decode('utf-8') if isinstance(field, bytes) else field,\n131                          v.encode('utf-8') if isinstance(v, str) else v))\n132 \n133         for (k, v) in files:\n134             # support for explicit filename\n135             ft = None\n136             fh = None\n137             if isinstance(v, (tuple, list)):\n138                 if len(v) == 2:\n139                     fn, fp = v\n140                 elif len(v) == 3:\n141                     fn, fp, ft = v\n142                 else:\n143                     fn, fp, ft, fh = v\n144             else:\n145                 fn = guess_filename(v) or k\n146                 fp = v\n147 \n148             if isinstance(fp, (str, bytes, bytearray)):\n149                 fdata = fp\n150             else:\n151                 fdata = fp.read()\n152 \n153             rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)\n154             rf.make_multipart(content_type=ft)\n155             new_fields.append(rf)\n156 \n157         body, content_type = encode_multipart_formdata(new_fields)\n158 \n159         return body, content_type\n160 \n161 \n162 class RequestHooksMixin(object):\n163     def register_hook(self, event, hook):\n164         \"\"\"Properly register a hook.\"\"\"\n165 \n166         if event not in self.hooks:\n167             raise ValueError('Unsupported event specified, with event name \"%s\"' % (event))\n168 \n169         if isinstance(hook, collections.Callable):\n170             self.hooks[event].append(hook)\n171         elif hasattr(hook, '__iter__'):\n172             self.hooks[event].extend(h for h in hook if isinstance(h, collections.Callable))\n173 \n174     def deregister_hook(self, event, hook):\n175         \"\"\"Deregister a previously registered hook.\n176         Returns True if the hook existed, False if not.\n177         \"\"\"\n178 \n179         try:\n180             self.hooks[event].remove(hook)\n181             return True\n182         except ValueError:\n183             return False\n184 \n185 \n186 class Request(RequestHooksMixin):\n187     \"\"\"A user-created :class:`Request <Request>` object.\n188 \n189     Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.\n190 \n191     :param method: HTTP method to use.\n192     :param url: URL to send.\n193     :param headers: dictionary of headers to send.\n194     :param files: dictionary of {filename: fileobject} files to multipart upload.\n195     :param data: the body to attach to the request. If a dictionary is provided, form-encoding will take place.\n196     :param json: json for the body to attach to the request (if files or data is not specified).\n197     :param params: dictionary of URL parameters to append to the URL.\n198     :param auth: Auth handler or (user, pass) tuple.\n199     :param cookies: dictionary or CookieJar of cookies to attach to this request.\n200     :param hooks: dictionary of callback hooks, for internal usage.\n201 \n202     Usage::\n203 \n204       >>> import requests\n205       >>> req = requests.Request('GET', 'http://httpbin.org/get')\n206       >>> req.prepare()\n207       <PreparedRequest [GET]>\n208 \n209     \"\"\"\n210     def __init__(self, method=None, url=None, headers=None, files=None,\n211         data=None, params=None, auth=None, cookies=None, hooks=None, json=None):\n212 \n213         # Default empty dicts for dict params.\n214         data = [] if data is None else data\n215         files = [] if files is None else files\n216         headers = {} if headers is None else headers\n217         params = {} if params is None else params\n218         hooks = {} if hooks is None else hooks\n219 \n220         self.hooks = default_hooks()\n221         for (k, v) in list(hooks.items()):\n222             self.register_hook(event=k, hook=v)\n223 \n224         self.method = method\n225         self.url = url\n226         self.headers = headers\n227         self.files = files\n228         self.data = data\n229         self.json = json\n230         self.params = params\n231         self.auth = auth\n232         self.cookies = cookies\n233 \n234     def __repr__(self):\n235         return '<Request [%s]>' % (self.method)\n236 \n237     def prepare(self):\n238         \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.\"\"\"\n239         p = PreparedRequest()\n240         p.prepare(\n241             method=self.method,\n242             url=self.url,\n243             headers=self.headers,\n244             files=self.files,\n245             data=self.data,\n246             json=self.json,\n247             params=self.params,\n248             auth=self.auth,\n249             cookies=self.cookies,\n250             hooks=self.hooks,\n251         )\n252         return p\n253 \n254 \n255 class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n256     \"\"\"The fully mutable :class:`PreparedRequest <PreparedRequest>` object,\n257     containing the exact bytes that will be sent to the server.\n258 \n259     Generated from either a :class:`Request <Request>` object or manually.\n260 \n261     Usage::\n262 \n263       >>> import requests\n264       >>> req = requests.Request('GET', 'http://httpbin.org/get')\n265       >>> r = req.prepare()\n266       <PreparedRequest [GET]>\n267 \n268       >>> s = requests.Session()\n269       >>> s.send(r)\n270       <Response [200]>\n271 \n272     \"\"\"\n273 \n274     def __init__(self):\n275         #: HTTP verb to send to the server.\n276         self.method = None\n277         #: HTTP URL to send the request to.\n278         self.url = None\n279         #: dictionary of HTTP headers.\n280         self.headers = None\n281         # The `CookieJar` used to create the Cookie header will be stored here\n282         # after prepare_cookies is called\n283         self._cookies = None\n284         #: request body to send to the server.\n285         self.body = None\n286         #: dictionary of callback hooks, for internal usage.\n287         self.hooks = default_hooks()\n288 \n289     def prepare(self, method=None, url=None, headers=None, files=None,\n290         data=None, params=None, auth=None, cookies=None, hooks=None, json=None):\n291         \"\"\"Prepares the entire request with the given parameters.\"\"\"\n292 \n293         self.prepare_method(method)\n294         self.prepare_url(url, params)\n295         self.prepare_headers(headers)\n296         self.prepare_cookies(cookies)\n297         self.prepare_body(data, files, json)\n298         self.prepare_auth(auth, url)\n299 \n300         # Note that prepare_auth must be last to enable authentication schemes\n301         # such as OAuth to work on a fully prepared request.\n302 \n303         # This MUST go after prepare_auth. Authenticators could add a hook\n304         self.prepare_hooks(hooks)\n305 \n306     def __repr__(self):\n307         return '<PreparedRequest [%s]>' % (self.method)\n308 \n309     def copy(self):\n310         p = PreparedRequest()\n311         p.method = self.method\n312         p.url = self.url\n313         p.headers = self.headers.copy() if self.headers is not None else None\n314         p._cookies = _copy_cookie_jar(self._cookies)\n315         p.body = self.body\n316         p.hooks = self.hooks\n317         return p\n318 \n319     def prepare_method(self, method):\n320         \"\"\"Prepares the given HTTP method.\"\"\"\n321         self.method = method\n322         if self.method is None:\n323             raise ValueError('Request method cannot be \"None\"')\n324         self.method = to_native_string(self.method).upper()\n325 \n326     def prepare_url(self, url, params):\n327         \"\"\"Prepares the given HTTP URL.\"\"\"\n328         #: Accept objects that have string representations.\n329         #: We're unable to blindly call unicode/str functions\n330         #: as this will include the bytestring indicator (b'')\n331         #: on python 3.x.\n332         #: https://github.com/kennethreitz/requests/pull/2238\n333         if isinstance(url, bytes):\n334             url = url.decode('utf8')\n335         else:\n336             url = unicode(url) if is_py2 else str(url)\n337 \n338         # Ignore any leading and trailing whitespace characters.\n339         url = url.strip()\n340 \n341         # Don't do any URL preparation for non-HTTP schemes like `mailto`,\n342         # `data` etc to work around exceptions from `url_parse`, which\n343         # handles RFC 3986 only.\n344         if ':' in url and not url.lower().startswith('http'):\n345             self.url = url\n346             return\n347 \n348         # Support for unicode domain names and paths.\n349         try:\n350             scheme, auth, host, port, path, query, fragment = parse_url(url)\n351         except LocationParseError as e:\n352             raise InvalidURL(*e.args)\n353 \n354         if not scheme:\n355             error = (\"Invalid URL {0!r}: No scheme supplied. Perhaps you meant http://{0}?\")\n356             error = error.format(to_native_string(url, 'utf8'))\n357 \n358             raise MissingScheme(error)\n359 \n360         if not host:\n361             raise InvalidURL(\"Invalid URL %r: No host supplied\" % url)\n362 \n363         # Only want to apply IDNA to the hostname\n364         try:\n365             host = host.encode('idna').decode('utf-8')\n366         except UnicodeError:\n367             raise InvalidURL('URL has an invalid label.')\n368 \n369         # Carefully reconstruct the network location\n370         netloc = auth or ''\n371         if netloc:\n372             netloc += '@'\n373         netloc += host\n374         if port:\n375             netloc += ':' + str(port)\n376 \n377         # Bare domains aren't valid URLs.\n378         if not path:\n379             path = '/'\n380 \n381         if is_py2:\n382             if isinstance(scheme, str):\n383                 scheme = scheme.encode('utf-8')\n384             if isinstance(netloc, str):\n385                 netloc = netloc.encode('utf-8')\n386             if isinstance(path, str):\n387                 path = path.encode('utf-8')\n388             if isinstance(query, str):\n389                 query = query.encode('utf-8')\n390             if isinstance(fragment, str):\n391                 fragment = fragment.encode('utf-8')\n392 \n393         if isinstance(params, (str, bytes)):\n394             params = to_native_string(params)\n395 \n396         enc_params = self._encode_params(params)\n397         if enc_params:\n398             if query:\n399                 query = '%s&%s' % (query, enc_params)\n400             else:\n401                 query = enc_params\n402 \n403         url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))\n404         self.url = url\n405 \n406     def prepare_headers(self, headers):\n407         \"\"\"Prepares the given HTTP headers.\"\"\"\n408 \n409         if headers:\n410             self.headers = CaseInsensitiveDict((to_native_string(name), value) for name, value in headers.items())\n411         else:\n412             self.headers = CaseInsensitiveDict()\n413 \n414     def prepare_body(self, data, files, json=None):\n415         \"\"\"Prepares the given HTTP body data.\"\"\"\n416 \n417         # Check if file, fo, generator, iterator.\n418         # If not, run through normal process.\n419 \n420         # Nottin' on you.\n421         body = None\n422         content_type = None\n423         length = None\n424 \n425         if not data and json is not None:\n426             content_type = 'application/json'\n427             body = complexjson.dumps(json)\n428 \n429         is_stream = all([\n430             hasattr(data, '__iter__'),\n431             not isinstance(data, (basestring, list, tuple, dict))\n432         ])\n433 \n434         try:\n435             length = super_len(data)\n436         except (TypeError, AttributeError, UnsupportedOperation):\n437             length = None\n438 \n439         if is_stream:\n440             body = data\n441 \n442             if files:\n443                 raise NotImplementedError('Streamed bodies and files are mutually exclusive.')\n444 \n445             if length:\n446                 self.headers['Content-Length'] = builtin_str(length)\n447             else:\n448                 self.headers['Transfer-Encoding'] = 'chunked'\n449         else:\n450             # Multi-part file uploads.\n451             if files:\n452                 (body, content_type) = self._encode_files(files, data)\n453             else:\n454                 if data:\n455                     body = self._encode_params(data)\n456                     if isinstance(data, basestring) or hasattr(data, 'read'):\n457                         content_type = None\n458                     else:\n459                         content_type = 'application/x-www-form-urlencoded'\n460 \n461             self.prepare_content_length(body)\n462 \n463             # Add content-type if it wasn't explicitly provided.\n464             if content_type and ('content-type' not in self.headers):\n465                 self.headers['Content-Type'] = content_type\n466 \n467         self.body = body\n468 \n469     def prepare_content_length(self, body):\n470         if hasattr(body, 'seek') and hasattr(body, 'tell'):\n471             body.seek(0, 2)\n472             self.headers['Content-Length'] = builtin_str(body.tell())\n473             body.seek(0, 0)\n474         elif body is not None:\n475             l = super_len(body)\n476             if l:\n477                 self.headers['Content-Length'] = builtin_str(l)\n478         elif (self.method not in ('GET', 'HEAD')) and (self.headers.get('Content-Length') is None):\n479             self.headers['Content-Length'] = '0'\n480 \n481     def prepare_auth(self, auth, url=''):\n482         \"\"\"Prepares the given HTTP auth data.\"\"\"\n483 \n484         # If no Auth is explicitly provided, extract it from the URL first.\n485         if auth is None:\n486             url_auth = get_auth_from_url(self.url)\n487             auth = url_auth if any(url_auth) else None\n488 \n489         if auth:\n490             if isinstance(auth, tuple) and len(auth) == 2:\n491                 # special-case basic HTTP auth\n492                 auth = HTTPBasicAuth(*auth)\n493 \n494             # Allow auth to make its changes.\n495             r = auth(self)\n496 \n497             # Update self to reflect the auth changes.\n498             self.__dict__.update(r.__dict__)\n499 \n500             # Recompute Content-Length\n501             self.prepare_content_length(self.body)\n502 \n503     def prepare_cookies(self, cookies):\n504         \"\"\"Prepares the given HTTP cookie data.\n505 \n506         This function eventually generates a ``Cookie`` header from the\n507         given cookies using cookielib. Due to cookielib's design, the header\n508         will not be regenerated if it already exists, meaning this function\n509         can only be called once for the life of the\n510         :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls\n511         to ``prepare_cookies`` will have no actual effect, unless the \"Cookie\"\n512         header is removed beforehand.\"\"\"\n513 \n514         if isinstance(cookies, cookielib.CookieJar):\n515             self._cookies = cookies\n516         else:\n517             self._cookies = cookiejar_from_dict(cookies)\n518 \n519         cookie_header = get_cookie_header(self._cookies, self)\n520         if cookie_header is not None:\n521             self.headers['Cookie'] = cookie_header\n522 \n523     def prepare_hooks(self, hooks):\n524         \"\"\"Prepares the given hooks.\"\"\"\n525         # hooks can be passed as None to the prepare method and to this\n526         # method. To prevent iterating over None, simply use an empty list\n527         # if hooks is False-y\n528         hooks = hooks or []\n529         for event in hooks:\n530             self.register_hook(event, hooks[event])\n531 \n532     def send(self, session=None, **send_kwargs):\n533         \"\"\"Sends the PreparedRequest to the given Session.\n534         If none is provided, one is created for you.\"\"\"\n535         session = requests.Session() if session is None else session\n536 \n537         with session:\n538             return session.send(self, **send_kwargs)\n539 \n540 \n541 class Response(object):\n542     \"\"\"The :class:`Response <Response>` object, which contains a\n543     server's response to an HTTP request.\n544     \"\"\"\n545 \n546     __attrs__ = [\n547         '_content', 'status_code', 'headers', 'url', 'history',\n548         'encoding', 'reason', 'cookies', 'elapsed', 'request'\n549     ]\n550 \n551     def __init__(self):\n552         super(Response, self).__init__()\n553 \n554         self._content = False\n555         self._content_consumed = False\n556 \n557         #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n558         self.status_code = None\n559 \n560         #: Case-insensitive Dictionary of Response Headers.\n561         #: For example, ``headers['content-encoding']`` will return the\n562         #: value of a ``'Content-Encoding'`` response header.\n563         self.headers = CaseInsensitiveDict()\n564 \n565         #: File-like object representation of response (for advanced usage).\n566         #: Use of ``raw`` requires that ``stream=True`` be set on the request.\n567         # This requirement does not apply for use internally to Requests.\n568         self.raw = None\n569 \n570         #: Final URL location of Response.\n571         self.url = None\n572 \n573         #: Encoding to decode with when accessing r.text.\n574         self.encoding = None\n575 \n576         #: A list of :class:`Response <Response>` objects from\n577         #: the history of the Request. Any redirect responses will end\n578         #: up here. The list is sorted from the oldest to the most recent request.\n579         self.history = []\n580 \n581         #: Textual reason of responded HTTP Status, e.g. \"Not Found\" or \"OK\".\n582         self.reason = None\n583 \n584         #: A CookieJar of Cookies the server sent back.\n585         self.cookies = cookiejar_from_dict({})\n586 \n587         #: The amount of time elapsed between sending the request\n588         #: and the arrival of the response (as a timedelta).\n589         #: This property specifically measures the time taken between sending\n590         #: the first byte of the request and finishing parsing the headers. It\n591         #: is therefore unaffected by consuming the response content or the\n592         #: value of the ``stream`` keyword argument.\n593         self.elapsed = datetime.timedelta(0)\n594 \n595         #: The :class:`PreparedRequest <PreparedRequest>` object to which this\n596         #: is a response.\n597         self.request = None\n598 \n599     def __getstate__(self):\n600         # Consume everything; accessing the content attribute makes\n601         # sure the content has been fully read.\n602         if not self._content_consumed:\n603             self.content\n604 \n605         return dict(\n606             (attr, getattr(self, attr, None))\n607             for attr in self.__attrs__\n608         )\n609 \n610     def __setstate__(self, state):\n611         for name, value in state.items():\n612             setattr(self, name, value)\n613 \n614         # pickled objects do not have .raw\n615         setattr(self, '_content_consumed', True)\n616         setattr(self, 'raw', None)\n617 \n618     def __repr__(self):\n619         return '<Response [%s]>' % (self.status_code)\n620 \n621     def __iter__(self):\n622         \"\"\"Allows you to use a response as an iterator.\"\"\"\n623         return self.iter_content(128)\n624 \n625     @property\n626     def ok(self):\n627         try:\n628             self.raise_for_status()\n629         except HTTPError:\n630             return False\n631         return True\n632 \n633     @property\n634     def is_redirect(self):\n635         \"\"\"True if this Response is a well-formed HTTP redirect that could have\n636         been processed automatically (by :meth:`Session.resolve_redirects`).\n637         \"\"\"\n638         return ('location' in self.headers and self.status_code in REDIRECT_STATI)\n639 \n640     @property\n641     def is_permanent_redirect(self):\n642         \"\"\"True if this Response one of the permanent versions of redirect\"\"\"\n643         return ('location' in self.headers and self.status_code in (codes.moved_permanently, codes.permanent_redirect))\n644 \n645     @property\n646     def apparent_encoding(self):\n647         \"\"\"The apparent encoding, provided by the chardet library\"\"\"\n648         return chardet.detect(self.content)['encoding']\n649 \n650     def iter_content(self, chunk_size=1, decode_unicode=False):\n651         \"\"\"Iterates over the response data.  When stream=True is set on the\n652         request, this avoids reading the content at once into memory for\n653         large responses.  The chunk size is the number of bytes it should\n654         read into memory.  This is not necessarily the length of each item\n655         returned as decoding can take place.\n656 \n657         If decode_unicode is True, content will be decoded using the best\n658         available encoding based on the response.\n659         \"\"\"\n660 \n661         def generate():\n662             # Special case for urllib3.\n663             if hasattr(self.raw, 'stream'):\n664                 try:\n665                     for chunk in self.raw.stream(chunk_size, decode_content=True):\n666                         yield chunk\n667                 except ProtocolError as e:\n668                     raise ChunkedEncodingError(e)\n669                 except DecodeError as e:\n670                     raise ContentDecodingError(e)\n671                 except ReadTimeoutError as e:\n672                     raise ConnectionError(e)\n673             else:\n674                 # Standard file-like object.\n675                 while True:\n676                     chunk = self.raw.read(chunk_size)\n677                     if not chunk:\n678                         break\n679                     yield chunk\n680 \n681             self._content_consumed = True\n682 \n683         if self._content_consumed and isinstance(self._content, bool):\n684             raise StreamConsumedError()\n685         # simulate reading small chunks of the content\n686         reused_chunks = iter_slices(self._content, chunk_size)\n687 \n688         stream_chunks = generate()\n689 \n690         chunks = reused_chunks if self._content_consumed else stream_chunks\n691 \n692         if decode_unicode:\n693             chunks = stream_decode_response_unicode(chunks, self)\n694 \n695         return chunks\n696 \n697     def iter_lines(self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=None, delimiter=None):\n698         \"\"\"Iterates over the response data, one line at a time.  When\n699         stream=True is set on the request, this avoids reading the\n700         content at once into memory for large responses.\n701 \n702         .. note:: This method is not reentrant safe.\n703         \"\"\"\n704 \n705         pending = None\n706 \n707         for chunk in self.iter_content(chunk_size=chunk_size, decode_unicode=decode_unicode):\n708 \n709             if pending is not None:\n710                 chunk = pending + chunk\n711 \n712             if delimiter:\n713                 lines = chunk.split(delimiter)\n714             else:\n715                 lines = chunk.splitlines()\n716 \n717             if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:\n718                 pending = lines.pop()\n719             else:\n720                 pending = None\n721 \n722             for line in lines:\n723                 yield line\n724 \n725         if pending is not None:\n726             yield pending\n727 \n728     @property\n729     def content(self):\n730         \"\"\"Content of the response, in bytes.\"\"\"\n731 \n732         if self._content is False:\n733             # Read the contents.\n734             try:\n735                 if self._content_consumed:\n736                     raise RuntimeError(\n737                         'The content for this response was already consumed')\n738 \n739                 if self.status_code == 0:\n740                     self._content = None\n741                 else:\n742                     self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()\n743 \n744             except AttributeError:\n745                 self._content = None\n746 \n747         self._content_consumed = True\n748         # don't need to release the connection; that's been handled by urllib3\n749         # since we exhausted the data.\n750         return self._content\n751 \n752     @property\n753     def text(self):\n754         \"\"\"Content of the response, in unicode.\n755 \n756         If Response.encoding is None, encoding will be guessed using\n757         ``chardet``.\n758 \n759         The encoding of the response content is determined based solely on HTTP\n760         headers, following RFC 2616 to the letter. If you can take advantage of\n761         non-HTTP knowledge to make a better guess at the encoding, you should\n762         set ``r.encoding`` appropriately before accessing this property.\n763         \"\"\"\n764 \n765         # Try charset from content-type\n766         content = None\n767         encoding = self.encoding\n768 \n769         if not self.content:\n770             return str('')\n771 \n772         # Fallback to auto-detected encoding.\n773         if self.encoding is None:\n774             encoding = self.apparent_encoding\n775 \n776         # Decode unicode from given encoding.\n777         try:\n778             content = str(self.content, encoding, errors='replace')\n779         except (LookupError, TypeError):\n780             # A LookupError is raised if the encoding was not found which could\n781             # indicate a misspelling or similar mistake.\n782             #\n783             # A TypeError can be raised if encoding is None\n784             #\n785             # So we try blindly encoding.\n786             content = str(self.content, errors='replace')\n787 \n788         return content\n789 \n790     def json(self, **kwargs):\n791         \"\"\"Returns the json-encoded content of a response, if any.\n792 \n793         :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n794         \"\"\"\n795 \n796         if not self.encoding and len(self.content) > 3:\n797             # No encoding set. JSON RFC 4627 section 3 states we should expect\n798             # UTF-8, -16 or -32. Detect which one to use; If the detection or\n799             # decoding fails, fall back to `self.text` (using chardet to make\n800             # a best guess).\n801             encoding = guess_json_utf(self.content)\n802             if encoding is not None:\n803                 try:\n804                     return complexjson.loads(\n805                         self.content.decode(encoding), **kwargs\n806                     )\n807                 except UnicodeDecodeError:\n808                     # Wrong UTF codec detected; usually because it's not UTF-8\n809                     # but some other 8-bit codec.  This is an RFC violation,\n810                     # and the server didn't bother to tell us what codec *was*\n811                     # used.\n812                     pass\n813         return complexjson.loads(self.text, **kwargs)\n814 \n815     @property\n816     def links(self):\n817         \"\"\"Returns the parsed header links of the response, if any.\"\"\"\n818 \n819         header = self.headers.get('link')\n820 \n821         # l = MultiDict()\n822         l = {}\n823 \n824         if header:\n825             links = parse_header_links(header)\n826 \n827             for link in links:\n828                 key = link.get('rel') or link.get('url')\n829                 l[key] = link\n830 \n831         return l\n832 \n833     def raise_for_status(self):\n834         \"\"\"Raises stored :class:`HTTPError`, if one occurred.\"\"\"\n835 \n836         http_error_msg = ''\n837 \n838         if 400 <= self.status_code < 500:\n839             http_error_msg = '%s Client Error: %s for url: %s' % (self.status_code, self.reason, self.url)\n840 \n841         elif 500 <= self.status_code < 600:\n842             http_error_msg = '%s Server Error: %s for url: %s' % (self.status_code, self.reason, self.url)\n843 \n844         if http_error_msg:\n845             raise HTTPError(http_error_msg, response=self)\n846 \n847     def close(self):\n848         \"\"\"Releases the connection back to the pool. Once this method has been\n849         called the underlying ``raw`` object must not be accessed again.\n850 \n851         *Note: Should not normally need to be called explicitly.*\n852         \"\"\"\n853         if not self._content_consumed:\n854             return self.raw.close()\n855 \n856         return self.raw.release_conn()\n",
        "intended_behavior": "The code should ensure that when a redirect URL is processed, non-ASCII characters in the URL are correctly percent-encoded according to RFC 3986."
    },
    {
        "rel_file_path": "requests/utils.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/psf__requests/setup_psf__requests__3.0/requests/utils.py",
        "start": 1,
        "end": 721,
        "class_name": null,
        "method_name": null,
        "code": "1 # -*- coding: utf-8 -*-\n2 \n3 \"\"\"\n4 requests.utils\n5 ~~~~~~~~~~~~~~\n6 \n7 This module provides utility functions that are used within Requests\n8 that are also useful for external consumption.\n9 \n10 \"\"\"\n11 \n12 import cgi\n13 import codecs\n14 import collections\n15 import io\n16 import os\n17 import platform\n18 import re\n19 import sys\n20 import socket\n21 import struct\n22 import warnings\n23 \n24 from . import __version__\n25 from . import certs\n26 from .compat import parse_http_list as _parse_list_header\n27 from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote, is_py2,\n28                      builtin_str, getproxies, proxy_bypass, urlunparse,\n29                      basestring)\n30 from .cookies import RequestsCookieJar, cookiejar_from_dict\n31 from .structures import CaseInsensitiveDict\n32 from .exceptions import InvalidURL, FileModeWarning\n33 \n34 _hush_pyflakes = (RequestsCookieJar,)\n35 \n36 NETRC_FILES = ('.netrc', '_netrc')\n37 \n38 DEFAULT_CA_BUNDLE_PATH = certs.where()\n39 \n40 \n41 def dict_to_sequence(d):\n42     \"\"\"Returns an internal sequence dictionary update.\"\"\"\n43 \n44     if hasattr(d, 'items'):\n45         d = d.items()\n46 \n47     return d\n48 \n49 \n50 def super_len(o):\n51     total_length = 0\n52     current_position = 0\n53 \n54     if hasattr(o, '__len__'):\n55         total_length = len(o)\n56 \n57     elif hasattr(o, 'len'):\n58         total_length = o.len\n59 \n60     elif hasattr(o, 'getvalue'):\n61         # e.g. BytesIO, cStringIO.StringIO\n62         total_length = len(o.getvalue())\n63 \n64     elif hasattr(o, 'fileno'):\n65         try:\n66             fileno = o.fileno()\n67         except io.UnsupportedOperation:\n68             pass\n69         else:\n70             total_length = os.fstat(fileno).st_size\n71 \n72             # Having used fstat to determine the file length, we need to\n73             # confirm that this file was opened up in binary mode.\n74             if 'b' not in o.mode:\n75                 warnings.warn((\n76                     \"Requests has determined the content-length for this \"\n77                     \"request using the binary size of the file: however, the \"\n78                     \"file has been opened in text mode (i.e. without the 'b' \"\n79                     \"flag in the mode). This may lead to an incorrect \"\n80                     \"content-length. In Requests 3.0, support will be removed \"\n81                     \"for files in text mode.\"),\n82                     FileModeWarning\n83                 )\n84 \n85     if hasattr(o, 'tell'):\n86         current_position = o.tell()\n87 \n88     return max(0, total_length - current_position)\n89 \n90 \n91 def get_netrc_auth(url, raise_errors=False):\n92     \"\"\"Returns the Requests tuple auth for a given url from netrc.\"\"\"\n93 \n94     try:\n95         from netrc import netrc, NetrcParseError\n96 \n97         netrc_path = None\n98 \n99         for f in NETRC_FILES:\n100             try:\n101                 loc = os.path.expanduser('~/{0}'.format(f))\n102             except KeyError:\n103                 # os.path.expanduser can fail when $HOME is undefined and\n104                 # getpwuid fails. See http://bugs.python.org/issue20164 &\n105                 # https://github.com/kennethreitz/requests/issues/1846\n106                 return\n107 \n108             if os.path.exists(loc):\n109                 netrc_path = loc\n110                 break\n111 \n112         # Abort early if there isn't one.\n113         if netrc_path is None:\n114             return\n115 \n116         ri = urlparse(url)\n117 \n118         # Strip port numbers from netloc. This weird `if...encode`` dance is\n119         # used for Python 3.2, which doesn't support unicode literals.\n120         splitstr = b':'\n121         if isinstance(url, str):\n122             splitstr = splitstr.decode('ascii')\n123         host = ri.netloc.split(splitstr)[0]\n124 \n125         try:\n126             _netrc = netrc(netrc_path).authenticators(host)\n127             if _netrc:\n128                 # Return with login / password\n129                 login_i = (0 if _netrc[0] else 1)\n130                 return (_netrc[login_i], _netrc[2])\n131         except (NetrcParseError, IOError):\n132             # If there was a parsing error or a permissions issue reading the file,\n133             # we'll just skip netrc auth unless explicitly asked to raise errors.\n134             if raise_errors:\n135                 raise\n136 \n137     # AppEngine hackiness.\n138     except (ImportError, AttributeError):\n139         pass\n140 \n141 \n142 def guess_filename(obj):\n143     \"\"\"Tries to guess the filename of the given object.\"\"\"\n144     name = getattr(obj, 'name', None)\n145     if (name and isinstance(name, basestring) and name[0] != '<' and\n146             name[-1] != '>'):\n147         return os.path.basename(name)\n148 \n149 \n150 def from_key_val_list(value):\n151     \"\"\"Take an object and test to see if it can be represented as a\n152     dictionary. Unless it can not be represented as such, return an\n153     OrderedDict, e.g.,\n154 \n155     ::\n156 \n157         >>> from_key_val_list([('key', 'val')])\n158         OrderedDict([('key', 'val')])\n159         >>> from_key_val_list('string')\n160         ValueError: need more than 1 value to unpack\n161         >>> from_key_val_list({'key': 'val'})\n162         OrderedDict([('key', 'val')])\n163     \"\"\"\n164     if value is None:\n165         return None\n166 \n167     if isinstance(value, (str, bytes, bool, int)):\n168         raise ValueError('cannot encode objects that are not 2-tuples')\n169 \n170     return OrderedDict(value)\n171 \n172 \n173 def to_key_val_list(value):\n174     \"\"\"Take an object and test to see if it can be represented as a\n175     dictionary. If it can be, return a list of tuples, e.g.,\n176 \n177     ::\n178 \n179         >>> to_key_val_list([('key', 'val')])\n180         [('key', 'val')]\n181         >>> to_key_val_list({'key': 'val'})\n182         [('key', 'val')]\n183         >>> to_key_val_list('string')\n184         ValueError: cannot encode objects that are not 2-tuples.\n185     \"\"\"\n186     if value is None:\n187         return None\n188 \n189     if isinstance(value, (str, bytes, bool, int)):\n190         raise ValueError('cannot encode objects that are not 2-tuples')\n191 \n192     if isinstance(value, collections.Mapping):\n193         value = value.items()\n194 \n195     return list(value)\n196 \n197 \n198 # From mitsuhiko/werkzeug (used with permission).\n199 def parse_list_header(value):\n200     \"\"\"Parse lists as described by RFC 2068 Section 2.\n201 \n202     In particular, parse comma-separated lists where the elements of\n203     the list may include quoted-strings.  A quoted-string could\n204     contain a comma.  A non-quoted string could have quotes in the\n205     middle.  Quotes are removed automatically after parsing.\n206 \n207     It basically works like :func:`parse_set_header` just that items\n208     may appear multiple times and case sensitivity is preserved.\n209 \n210     The return value is a standard :class:`list`:\n211 \n212     >>> parse_list_header('token, \"quoted value\"')\n213     ['token', 'quoted value']\n214 \n215     To create a header from the :class:`list` again, use the\n216     :func:`dump_header` function.\n217 \n218     :param value: a string with a list header.\n219     :return: :class:`list`\n220     \"\"\"\n221     result = []\n222     for item in _parse_list_header(value):\n223         if item[:1] == item[-1:] == '\"':\n224             item = unquote_header_value(item[1:-1])\n225         result.append(item)\n226     return result\n227 \n228 \n229 # From mitsuhiko/werkzeug (used with permission).\n230 def parse_dict_header(value):\n231     \"\"\"Parse lists of key, value pairs as described by RFC 2068 Section 2 and\n232     convert them into a python dict:\n233 \n234     >>> d = parse_dict_header('foo=\"is a fish\", bar=\"as well\"')\n235     >>> type(d) is dict\n236     True\n237     >>> sorted(d.items())\n238     [('bar', 'as well'), ('foo', 'is a fish')]\n239 \n240     If there is no value for a key it will be `None`:\n241 \n242     >>> parse_dict_header('key_without_value')\n243     {'key_without_value': None}\n244 \n245     To create a header from the :class:`dict` again, use the\n246     :func:`dump_header` function.\n247 \n248     :param value: a string with a dict header.\n249     :return: :class:`dict`\n250     \"\"\"\n251     result = {}\n252     for item in _parse_list_header(value):\n253         if '=' not in item:\n254             result[item] = None\n255             continue\n256         name, value = item.split('=', 1)\n257         if value[:1] == value[-1:] == '\"':\n258             value = unquote_header_value(value[1:-1])\n259         result[name] = value\n260     return result\n261 \n262 \n263 # From mitsuhiko/werkzeug (used with permission).\n264 def unquote_header_value(value, is_filename=False):\n265     r\"\"\"Unquotes a header value.  (Reversal of :func:`quote_header_value`).\n266     This does not use the real unquoting but what browsers are actually\n267     using for quoting.\n268 \n269     :param value: the header value to unquote.\n270     \"\"\"\n271     if value and value[0] == value[-1] == '\"':\n272         # this is not the real unquoting, but fixing this so that the\n273         # RFC is met will result in bugs with internet explorer and\n274         # probably some other browsers as well.  IE for example is\n275         # uploading files with \"C:\\foo\\bar.txt\" as filename\n276         value = value[1:-1]\n277 \n278         # if this is a filename and the starting characters look like\n279         # a UNC path, then just return the value without quotes.  Using the\n280         # replace sequence below on a UNC path has the effect of turning\n281         # the leading double slash into a single slash and then\n282         # _fix_ie_filename() doesn't work correctly.  See #458.\n283         if not is_filename or value[:2] != '\\\\\\\\':\n284             return value.replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n285     return value\n286 \n287 \n288 def dict_from_cookiejar(cj):\n289     \"\"\"Returns a key/value dictionary from a CookieJar.\n290 \n291     :param cj: CookieJar object to extract cookies from.\n292     \"\"\"\n293 \n294     cookie_dict = {}\n295 \n296     for cookie in cj:\n297         cookie_dict[cookie.name] = cookie.value\n298 \n299     return cookie_dict\n300 \n301 \n302 def add_dict_to_cookiejar(cj, cookie_dict):\n303     \"\"\"Returns a CookieJar from a key/value dictionary.\n304 \n305     :param cj: CookieJar to insert cookies into.\n306     :param cookie_dict: Dict of key/values to insert into CookieJar.\n307     \"\"\"\n308 \n309     cj2 = cookiejar_from_dict(cookie_dict)\n310     cj.update(cj2)\n311     return cj\n312 \n313 \n314 def get_encodings_from_content(content):\n315     \"\"\"Returns encodings from given content string.\n316 \n317     :param content: bytestring to extract encodings from.\n318     \"\"\"\n319     warnings.warn((\n320         'In requests 3.0, get_encodings_from_content will be removed. For '\n321         'more information, please see the discussion on issue #2266. (This'\n322         ' warning should only appear once.)'),\n323         DeprecationWarning)\n324 \n325     charset_re = re.compile(r'<meta.*?charset=[\"\\']*(.+?)[\"\\'>]', flags=re.I)\n326     pragma_re = re.compile(r'<meta.*?content=[\"\\']*;?charset=(.+?)[\"\\'>]', flags=re.I)\n327     xml_re = re.compile(r'^<\\?xml.*?encoding=[\"\\']*(.+?)[\"\\'>]')\n328 \n329     return (charset_re.findall(content) +\n330             pragma_re.findall(content) +\n331             xml_re.findall(content))\n332 \n333 \n334 def get_encoding_from_headers(headers):\n335     \"\"\"Returns encodings from given HTTP Header Dict.\n336 \n337     :param headers: dictionary to extract encoding from.\n338     \"\"\"\n339 \n340     content_type = headers.get('content-type')\n341 \n342     if not content_type:\n343         return None\n344 \n345     content_type, params = cgi.parse_header(content_type)\n346 \n347     if 'charset' in params:\n348         return params['charset'].strip(\"'\\\"\")\n349 \n350     if 'text' in content_type:\n351         return 'ISO-8859-1'\n352 \n353 \n354 def stream_decode_response_unicode(iterator, r):\n355     \"\"\"Stream decodes a iterator.\"\"\"\n356 \n357     if r.encoding is None:\n358         for item in iterator:\n359             yield item\n360         return\n361 \n362     decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')\n363     for chunk in iterator:\n364         rv = decoder.decode(chunk)\n365         if rv:\n366             yield rv\n367     rv = decoder.decode(b'', final=True)\n368     if rv:\n369         yield rv\n370 \n371 \n372 def iter_slices(string, slice_length):\n373     \"\"\"Iterate over slices of a string.\"\"\"\n374     pos = 0\n375     while pos < len(string):\n376         yield string[pos:pos + slice_length]\n377         pos += slice_length\n378 \n379 \n380 def get_unicode_from_response(r):\n381     \"\"\"Returns the requested content back in unicode.\n382 \n383     :param r: Response object to get unicode content from.\n384 \n385     Tried:\n386 \n387     1. charset from content-type\n388     2. fall back and replace all unicode characters\n389 \n390     \"\"\"\n391     warnings.warn((\n392         'In requests 3.0, get_unicode_from_response will be removed. For '\n393         'more information, please see the discussion on issue #2266. (This'\n394         ' warning should only appear once.)'),\n395         DeprecationWarning)\n396 \n397     tried_encodings = []\n398 \n399     # Try charset from content-type\n400     encoding = get_encoding_from_headers(r.headers)\n401 \n402     if encoding:\n403         try:\n404             return str(r.content, encoding)\n405         except UnicodeError:\n406             tried_encodings.append(encoding)\n407 \n408     # Fall back:\n409     try:\n410         return str(r.content, encoding, errors='replace')\n411     except TypeError:\n412         return r.content\n413 \n414 \n415 # The unreserved URI characters (RFC 3986)\n416 UNRESERVED_SET = frozenset(\n417     \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n418     + \"0123456789-._~\")\n419 \n420 \n421 def unquote_unreserved(uri):\n422     \"\"\"Un-escape any percent-escape sequences in a URI that are unreserved\n423     characters. This leaves all reserved, illegal and non-ASCII bytes encoded.\n424     \"\"\"\n425     parts = uri.split('%')\n426     for i in range(1, len(parts)):\n427         h = parts[i][0:2]\n428         if len(h) == 2 and h.isalnum():\n429             try:\n430                 c = chr(int(h, 16))\n431             except ValueError:\n432                 raise InvalidURL(\"Invalid percent-escape sequence: '%s'\" % h)\n433 \n434             if c in UNRESERVED_SET:\n435                 parts[i] = c + parts[i][2:]\n436             else:\n437                 parts[i] = '%' + parts[i]\n438         else:\n439             parts[i] = '%' + parts[i]\n440     return ''.join(parts)\n441 \n442 \n443 def requote_uri(uri):\n444     \"\"\"Re-quote the given URI.\n445 \n446     This function passes the given URI through an unquote/quote cycle to\n447     ensure that it is fully and consistently quoted.\n448     \"\"\"\n449     safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n450     safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n451     try:\n452         # Unquote only the unreserved characters\n453         # Then quote only illegal characters (do not quote reserved,\n454         # unreserved, or '%')\n455         return quote(unquote_unreserved(uri), safe=safe_with_percent)\n456     except InvalidURL:\n457         # We couldn't unquote the given URI, so let's try quoting it, but\n458         # there may be unquoted '%'s in the URI. We need to make sure they're\n459         # properly quoted so they do not cause issues elsewhere.\n460         return quote(uri, safe=safe_without_percent)\n461 \n462 \n463 def address_in_network(ip, net):\n464     \"\"\"\n465     This function allows you to check if on IP belongs to a network subnet\n466     Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24\n467              returns False if ip = 192.168.1.1 and net = 192.168.100.0/24\n468     \"\"\"\n469     ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]\n470     netaddr, bits = net.split('/')\n471     netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]\n472     network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask\n473     return (ipaddr & netmask) == (network & netmask)\n474 \n475 \n476 def dotted_netmask(mask):\n477     \"\"\"\n478     Converts mask from /xx format to xxx.xxx.xxx.xxx\n479     Example: if mask is 24 function returns 255.255.255.0\n480     \"\"\"\n481     bits = 0xffffffff ^ (1 << 32 - mask) - 1\n482     return socket.inet_ntoa(struct.pack('>I', bits))\n483 \n484 \n485 def is_ipv4_address(string_ip):\n486     try:\n487         socket.inet_aton(string_ip)\n488     except socket.error:\n489         return False\n490     return True\n491 \n492 \n493 def is_valid_cidr(string_network):\n494     \"\"\"Very simple check of the cidr format in no_proxy variable\"\"\"\n495     if string_network.count('/') == 1:\n496         try:\n497             mask = int(string_network.split('/')[1])\n498         except ValueError:\n499             return False\n500 \n501         if mask < 1 or mask > 32:\n502             return False\n503 \n504         try:\n505             socket.inet_aton(string_network.split('/')[0])\n506         except socket.error:\n507             return False\n508     else:\n509         return False\n510     return True\n511 \n512 \n513 def should_bypass_proxies(url):\n514     \"\"\"\n515     Returns whether we should bypass proxies or not.\n516     \"\"\"\n517     get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())\n518 \n519     # First check whether no_proxy is defined. If it is, check that the URL\n520     # we're getting isn't in the no_proxy list.\n521     no_proxy = get_proxy('no_proxy')\n522     netloc = urlparse(url).netloc\n523 \n524     if no_proxy:\n525         # We need to check whether we match here. We need to see if we match\n526         # the end of the netloc, both with and without the port.\n527         no_proxy = (\n528             host for host in no_proxy.replace(' ', '').split(',') if host\n529         )\n530 \n531         ip = netloc.split(':')[0]\n532         if is_ipv4_address(ip):\n533             for proxy_ip in no_proxy:\n534                 if is_valid_cidr(proxy_ip):\n535                     if address_in_network(ip, proxy_ip):\n536                         return True\n537         else:\n538             for host in no_proxy:\n539                 if netloc.endswith(host) or netloc.split(':')[0].endswith(host):\n540                     # The URL does match something in no_proxy, so we don't want\n541                     # to apply the proxies on this URL.\n542                     return True\n543 \n544     # If the system proxy settings indicate that this URL should be bypassed,\n545     # don't proxy.\n546     # The proxy_bypass function is incredibly buggy on OS X in early versions\n547     # of Python 2.6, so allow this call to fail. Only catch the specific\n548     # exceptions we've seen, though: this call failing in other ways can reveal\n549     # legitimate problems.\n550     try:\n551         bypass = proxy_bypass(netloc)\n552     except (TypeError, socket.gaierror):\n553         bypass = False\n554 \n555     if bypass:\n556         return True\n557 \n558     return False\n559 \n560 def get_environ_proxies(url):\n561     \"\"\"Return a dict of environment proxies.\"\"\"\n562     if should_bypass_proxies(url):\n563         return {}\n564     else:\n565         return getproxies()\n566 \n567 def select_proxy(url, proxies):\n568     \"\"\"Select a proxy for the url, if applicable.\n569 \n570     :param url: The url being for the request\n571     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs\n572     \"\"\"\n573     proxies = proxies or {}\n574     urlparts = urlparse(url)\n575     proxy = proxies.get(urlparts.scheme+'://'+urlparts.hostname)\n576     if proxy is None:\n577         proxy = proxies.get(urlparts.scheme)\n578     return proxy\n579 \n580 def default_user_agent(name=\"python-requests\"):\n581     \"\"\"Return a string representing the default user agent.\"\"\"\n582     return '%s/%s' % (name, __version__)\n583 \n584 \n585 def default_headers():\n586     return CaseInsensitiveDict({\n587         'User-Agent': default_user_agent(),\n588         'Accept-Encoding': ', '.join(('gzip', 'deflate')),\n589         'Accept': '*/*',\n590         'Connection': 'keep-alive',\n591     })\n592 \n593 \n594 def parse_header_links(value):\n595     \"\"\"Return a dict of parsed link headers proxies.\n596 \n597     i.e. Link: <http:/.../front.jpeg>; rel=front; type=\"image/jpeg\",<http://.../back.jpeg>; rel=back;type=\"image/jpeg\"\n598 \n599     \"\"\"\n600 \n601     links = []\n602 \n603     replace_chars = \" '\\\"\"\n604 \n605     for val in re.split(\", *<\", value):\n606         try:\n607             url, params = val.split(\";\", 1)\n608         except ValueError:\n609             url, params = val, ''\n610 \n611         link = {}\n612 \n613         link[\"url\"] = url.strip(\"<> '\\\"\")\n614 \n615         for param in params.split(\";\"):\n616             try:\n617                 key, value = param.split(\"=\")\n618             except ValueError:\n619                 break\n620 \n621             link[key.strip(replace_chars)] = value.strip(replace_chars)\n622 \n623         links.append(link)\n624 \n625     return links\n626 \n627 \n628 # Null bytes; no need to recreate these on each call to guess_json_utf\n629 _null = '\\x00'.encode('ascii')  # encoding to ASCII for Python 3\n630 _null2 = _null * 2\n631 _null3 = _null * 3\n632 \n633 \n634 def guess_json_utf(data):\n635     # JSON always starts with two ASCII characters, so detection is as\n636     # easy as counting the nulls and from their location and count\n637     # determine the encoding. Also detect a BOM, if present.\n638     sample = data[:4]\n639     if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE):\n640         return 'utf-32'     # BOM included\n641     if sample[:3] == codecs.BOM_UTF8:\n642         return 'utf-8-sig'  # BOM included, MS style (discouraged)\n643     if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):\n644         return 'utf-16'     # BOM included\n645     nullcount = sample.count(_null)\n646     if nullcount == 0:\n647         return 'utf-8'\n648     if nullcount == 2:\n649         if sample[::2] == _null2:   # 1st and 3rd are null\n650             return 'utf-16-be'\n651         if sample[1::2] == _null2:  # 2nd and 4th are null\n652             return 'utf-16-le'\n653         # Did not detect 2 valid UTF-16 ascii-range characters\n654     if nullcount == 3:\n655         if sample[:3] == _null3:\n656             return 'utf-32-be'\n657         if sample[1:] == _null3:\n658             return 'utf-32-le'\n659         # Did not detect a valid UTF-32 ascii-range character\n660     return None\n661 \n662 \n663 def prepend_scheme_if_needed(url, new_scheme):\n664     '''Given a URL that may or may not have a scheme, prepend the given scheme.\n665     Does not replace a present scheme with the one provided as an argument.'''\n666     scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)\n667 \n668     # urlparse is a finicky beast, and sometimes decides that there isn't a\n669     # netloc present. Assume that it's being over-cautious, and switch netloc\n670     # and path if urlparse decided there was no netloc.\n671     if not netloc:\n672         netloc, path = path, netloc\n673 \n674     return urlunparse((scheme, netloc, path, params, query, fragment))\n675 \n676 \n677 def get_auth_from_url(url):\n678     \"\"\"Given a url with authentication components, extract them into a tuple of\n679     username,password.\"\"\"\n680     parsed = urlparse(url)\n681 \n682     try:\n683         auth = (unquote(parsed.username), unquote(parsed.password))\n684     except (AttributeError, TypeError):\n685         auth = ('', '')\n686 \n687     return auth\n688 \n689 \n690 def to_native_string(string, encoding='ascii'):\n691     \"\"\"\n692     Given a string object, regardless of type, returns a representation of that\n693     string in the native string type, encoding and decoding where necessary.\n694     This assumes ASCII unless told otherwise.\n695     \"\"\"\n696     out = None\n697 \n698     if isinstance(string, builtin_str):\n699         out = string\n700     else:\n701         if is_py2:\n702             out = string.encode(encoding)\n703         else:\n704             out = string.decode(encoding)\n705 \n706     return out\n707 \n708 \n709 def urldefragauth(url):\n710     \"\"\"\n711     Given a url remove the fragment and the authentication part\n712     \"\"\"\n713     scheme, netloc, path, params, query, fragment = urlparse(url)\n714 \n715     # see func:`prepend_scheme_if_needed`\n716     if not netloc:\n717         netloc, path = path, netloc\n718 \n719     netloc = netloc.rsplit('@', 1)[-1]\n720 \n721     return urlunparse((scheme, netloc, path, params, query, ''))\n",
        "intended_behavior": "Any utility functions that parse or prepare URLs for requests, including redirected URLs, should accurately encode non-ASCII characters."
    },
    {
        "rel_file_path": "requests/packages/urllib3/util/request.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/psf__requests/setup_psf__requests__3.0/requests/packages/urllib3/util/request.py",
        "start": 1,
        "end": 72,
        "class_name": null,
        "method_name": null,
        "code": "1 from __future__ import absolute_import\n2 from base64 import b64encode\n3 \n4 from ..packages.six import b\n5 \n6 ACCEPT_ENCODING = 'gzip,deflate'\n7 \n8 \n9 def make_headers(keep_alive=None, accept_encoding=None, user_agent=None,\n10                  basic_auth=None, proxy_basic_auth=None, disable_cache=None):\n11     \"\"\"\n12     Shortcuts for generating request headers.\n13 \n14     :param keep_alive:\n15         If ``True``, adds 'connection: keep-alive' header.\n16 \n17     :param accept_encoding:\n18         Can be a boolean, list, or string.\n19         ``True`` translates to 'gzip,deflate'.\n20         List will get joined by comma.\n21         String will be used as provided.\n22 \n23     :param user_agent:\n24         String representing the user-agent you want, such as\n25         \"python-urllib3/0.6\"\n26 \n27     :param basic_auth:\n28         Colon-separated username:password string for 'authorization: basic ...'\n29         auth header.\n30 \n31     :param proxy_basic_auth:\n32         Colon-separated username:password string for 'proxy-authorization: basic ...'\n33         auth header.\n34 \n35     :param disable_cache:\n36         If ``True``, adds 'cache-control: no-cache' header.\n37 \n38     Example::\n39 \n40         >>> make_headers(keep_alive=True, user_agent=\"Batman/1.0\")\n41         {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}\n42         >>> make_headers(accept_encoding=True)\n43         {'accept-encoding': 'gzip,deflate'}\n44     \"\"\"\n45     headers = {}\n46     if accept_encoding:\n47         if isinstance(accept_encoding, str):\n48             pass\n49         elif isinstance(accept_encoding, list):\n50             accept_encoding = ','.join(accept_encoding)\n51         else:\n52             accept_encoding = ACCEPT_ENCODING\n53         headers['accept-encoding'] = accept_encoding\n54 \n55     if user_agent:\n56         headers['user-agent'] = user_agent\n57 \n58     if keep_alive:\n59         headers['connection'] = 'keep-alive'\n60 \n61     if basic_auth:\n62         headers['authorization'] = 'Basic ' + \\\n63             b64encode(b(basic_auth)).decode('utf-8')\n64 \n65     if proxy_basic_auth:\n66         headers['proxy-authorization'] = 'Basic ' + \\\n67             b64encode(b(proxy_basic_auth)).decode('utf-8')\n68 \n69     if disable_cache:\n70         headers['cache-control'] = 'no-cache'\n71 \n72     return headers\n",
        "intended_behavior": "Ensure that utilities related to preparing or modifying request objects correctly handle encoding of non-ASCII characters in URLs."
    },
    {
        "rel_file_path": "requests/packages/urllib3/response.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/psf__requests/setup_psf__requests__3.0/requests/packages/urllib3/response.py",
        "start": 1,
        "end": 514,
        "class_name": null,
        "method_name": null,
        "code": "1 from __future__ import absolute_import\n2 from contextlib import contextmanager\n3 import zlib\n4 import io\n5 from socket import timeout as SocketTimeout\n6 from socket import error as SocketError\n7 \n8 from ._collections import HTTPHeaderDict\n9 from .exceptions import (\n10     ProtocolError, DecodeError, ReadTimeoutError, ResponseNotChunked\n11 )\n12 from .packages.six import string_types as basestring, binary_type, PY3\n13 from .packages.six.moves import http_client as httplib\n14 from .connection import HTTPException, BaseSSLError\n15 from .util.response import is_fp_closed, is_response_to_head\n16 \n17 \n18 class DeflateDecoder(object):\n19 \n20     def __init__(self):\n21         self._first_try = True\n22         self._data = binary_type()\n23         self._obj = zlib.decompressobj()\n24 \n25     def __getattr__(self, name):\n26         return getattr(self._obj, name)\n27 \n28     def decompress(self, data):\n29         if not data:\n30             return data\n31 \n32         if not self._first_try:\n33             return self._obj.decompress(data)\n34 \n35         self._data += data\n36         try:\n37             return self._obj.decompress(data)\n38         except zlib.error:\n39             self._first_try = False\n40             self._obj = zlib.decompressobj(-zlib.MAX_WBITS)\n41             try:\n42                 return self.decompress(self._data)\n43             finally:\n44                 self._data = None\n45 \n46 \n47 class GzipDecoder(object):\n48 \n49     def __init__(self):\n50         self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)\n51 \n52     def __getattr__(self, name):\n53         return getattr(self._obj, name)\n54 \n55     def decompress(self, data):\n56         if not data:\n57             return data\n58         return self._obj.decompress(data)\n59 \n60 \n61 def _get_decoder(mode):\n62     if mode == 'gzip':\n63         return GzipDecoder()\n64 \n65     return DeflateDecoder()\n66 \n67 \n68 class HTTPResponse(io.IOBase):\n69     \"\"\"\n70     HTTP Response container.\n71 \n72     Backwards-compatible to httplib's HTTPResponse but the response ``body`` is\n73     loaded and decoded on-demand when the ``data`` property is accessed.  This\n74     class is also compatible with the Python standard library's :mod:`io`\n75     module, and can hence be treated as a readable object in the context of that\n76     framework.\n77 \n78     Extra parameters for behaviour not present in httplib.HTTPResponse:\n79 \n80     :param preload_content:\n81         If True, the response's body will be preloaded during construction.\n82 \n83     :param decode_content:\n84         If True, attempts to decode specific content-encoding's based on headers\n85         (like 'gzip' and 'deflate') will be skipped and raw data will be used\n86         instead.\n87 \n88     :param original_response:\n89         When this HTTPResponse wrapper is generated from an httplib.HTTPResponse\n90         object, it's convenient to include the original for debug purposes. It's\n91         otherwise unused.\n92     \"\"\"\n93 \n94     CONTENT_DECODERS = ['gzip', 'deflate']\n95     REDIRECT_STATUSES = [301, 302, 303, 307, 308]\n96 \n97     def __init__(self, body='', headers=None, status=0, version=0, reason=None,\n98                  strict=0, preload_content=True, decode_content=True,\n99                  original_response=None, pool=None, connection=None):\n100 \n101         if isinstance(headers, HTTPHeaderDict):\n102             self.headers = headers\n103         else:\n104             self.headers = HTTPHeaderDict(headers)\n105         self.status = status\n106         self.version = version\n107         self.reason = reason\n108         self.strict = strict\n109         self.decode_content = decode_content\n110 \n111         self._decoder = None\n112         self._body = None\n113         self._fp = None\n114         self._original_response = original_response\n115         self._fp_bytes_read = 0\n116 \n117         if body and isinstance(body, (basestring, binary_type)):\n118             self._body = body\n119 \n120         self._pool = pool\n121         self._connection = connection\n122 \n123         if hasattr(body, 'read'):\n124             self._fp = body\n125 \n126         # Are we using the chunked-style of transfer encoding?\n127         self.chunked = False\n128         self.chunk_left = None\n129         tr_enc = self.headers.get('transfer-encoding', '').lower()\n130         # Don't incur the penalty of creating a list and then discarding it\n131         encodings = (enc.strip() for enc in tr_enc.split(\",\"))\n132         if \"chunked\" in encodings:\n133             self.chunked = True\n134 \n135         # If requested, preload the body.\n136         if preload_content and not self._body:\n137             self._body = self.read(decode_content=decode_content)\n138 \n139     def get_redirect_location(self):\n140         \"\"\"\n141         Should we redirect and where to?\n142 \n143         :returns: Truthy redirect location string if we got a redirect status\n144             code and valid location. ``None`` if redirect status and no\n145             location. ``False`` if not a redirect status code.\n146         \"\"\"\n147         if self.status in self.REDIRECT_STATUSES:\n148             return self.headers.get('location')\n149 \n150         return False\n151 \n152     def release_conn(self):\n153         if not self._pool or not self._connection:\n154             return\n155 \n156         self._pool._put_conn(self._connection)\n157         self._connection = None\n158 \n159     @property\n160     def data(self):\n161         # For backwords-compat with earlier urllib3 0.4 and earlier.\n162         if self._body:\n163             return self._body\n164 \n165         if self._fp:\n166             return self.read(cache_content=True)\n167 \n168     def tell(self):\n169         \"\"\"\n170         Obtain the number of bytes pulled over the wire so far. May differ from\n171         the amount of content returned by :meth:``HTTPResponse.read`` if bytes\n172         are encoded on the wire (e.g, compressed).\n173         \"\"\"\n174         return self._fp_bytes_read\n175 \n176     def _init_decoder(self):\n177         \"\"\"\n178         Set-up the _decoder attribute if necessar.\n179         \"\"\"\n180         # Note: content-encoding value should be case-insensitive, per RFC 7230\n181         # Section 3.2\n182         content_encoding = self.headers.get('content-encoding', '').lower()\n183         if self._decoder is None and content_encoding in self.CONTENT_DECODERS:\n184             self._decoder = _get_decoder(content_encoding)\n185 \n186     def _decode(self, data, decode_content, flush_decoder):\n187         \"\"\"\n188         Decode the data passed in and potentially flush the decoder.\n189         \"\"\"\n190         try:\n191             if decode_content and self._decoder:\n192                 data = self._decoder.decompress(data)\n193         except (IOError, zlib.error) as e:\n194             content_encoding = self.headers.get('content-encoding', '').lower()\n195             raise DecodeError(\n196                 \"Received response with content-encoding: %s, but \"\n197                 \"failed to decode it.\" % content_encoding, e)\n198 \n199         if flush_decoder and decode_content:\n200             data += self._flush_decoder()\n201 \n202         return data\n203 \n204     def _flush_decoder(self):\n205         \"\"\"\n206         Flushes the decoder. Should only be called if the decoder is actually\n207         being used.\n208         \"\"\"\n209         if self._decoder:\n210             buf = self._decoder.decompress(b'')\n211             return buf + self._decoder.flush()\n212 \n213         return b''\n214 \n215     @contextmanager\n216     def _error_catcher(self):\n217         \"\"\"\n218         Catch low-level python exceptions, instead re-raising urllib3\n219         variants, so that low-level exceptions are not leaked in the\n220         high-level api.\n221 \n222         On exit, release the connection back to the pool.\n223         \"\"\"\n224         try:\n225             try:\n226                 yield\n227 \n228             except SocketTimeout:\n229                 # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\n230                 # there is yet no clean way to get at it from this context.\n231                 raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n232 \n233             except BaseSSLError as e:\n234                 # FIXME: Is there a better way to differentiate between SSLErrors?\n235                 if 'read operation timed out' not in str(e):  # Defensive:\n236                     # This shouldn't happen but just in case we're missing an edge\n237                     # case, let's avoid swallowing SSL errors.\n238                     raise\n239 \n240                 raise ReadTimeoutError(self._pool, None, 'Read timed out.')\n241 \n242             except (HTTPException, SocketError) as e:\n243                 # This includes IncompleteRead.\n244                 raise ProtocolError('Connection broken: %r' % e, e)\n245 \n246         except Exception:\n247             # The response may not be closed but we're not going to use it anymore\n248             # so close it now to ensure that the connection is released back to the pool.\n249             if self._original_response and not self._original_response.isclosed():\n250                 self._original_response.close()\n251 \n252             # Closing the response may not actually be sufficient to close\n253             # everything, so if we have a hold of the connection close that\n254             # too.\n255             if self._connection is not None:\n256                 self._connection.close()\n257 \n258             raise\n259         finally:\n260             if self._original_response and self._original_response.isclosed():\n261                 self.release_conn()\n262 \n263     def read(self, amt=None, decode_content=None, cache_content=False):\n264         \"\"\"\n265         Similar to :meth:`httplib.HTTPResponse.read`, but with two additional\n266         parameters: ``decode_content`` and ``cache_content``.\n267 \n268         :param amt:\n269             How much of the content to read. If specified, caching is skipped\n270             because it doesn't make sense to cache partial content as the full\n271             response.\n272 \n273         :param decode_content:\n274             If True, will attempt to decode the body based on the\n275             'content-encoding' header.\n276 \n277         :param cache_content:\n278             If True, will save the returned data such that the same result is\n279             returned despite of the state of the underlying file object. This\n280             is useful if you want the ``.data`` property to continue working\n281             after having ``.read()`` the file object. (Overridden if ``amt`` is\n282             set.)\n283         \"\"\"\n284         self._init_decoder()\n285         if decode_content is None:\n286             decode_content = self.decode_content\n287 \n288         if self._fp is None:\n289             return\n290 \n291         flush_decoder = False\n292         data = None\n293 \n294         with self._error_catcher():\n295             if amt is None:\n296                 # cStringIO doesn't like amt=None\n297                 data = self._fp.read()\n298                 flush_decoder = True\n299             else:\n300                 cache_content = False\n301                 data = self._fp.read(amt)\n302                 if amt != 0 and not data:  # Platform-specific: Buggy versions of Python.\n303                     # Close the connection when no data is returned\n304                     #\n305                     # This is redundant to what httplib/http.client _should_\n306                     # already do.  However, versions of python released before\n307                     # December 15, 2012 (http://bugs.python.org/issue16298) do\n308                     # not properly close the connection in all cases. There is\n309                     # no harm in redundantly calling close.\n310                     self._fp.close()\n311                     flush_decoder = True\n312 \n313         if data:\n314             self._fp_bytes_read += len(data)\n315 \n316             data = self._decode(data, decode_content, flush_decoder)\n317 \n318             if cache_content:\n319                 self._body = data\n320 \n321         return data\n322 \n323     def stream(self, amt=2**16, decode_content=None):\n324         \"\"\"\n325         A generator wrapper for the read() method. A call will block until\n326         ``amt`` bytes have been read from the connection or until the\n327         connection is closed.\n328 \n329         :param amt:\n330             How much of the content to read. The generator will return up to\n331             much data per iteration, but may return less. This is particularly\n332             likely when using compressed data. However, the empty string will\n333             never be returned.\n334 \n335         :param decode_content:\n336             If True, will attempt to decode the body based on the\n337             'content-encoding' header.\n338         \"\"\"\n339         if self.chunked:\n340             for line in self.read_chunked(amt, decode_content=decode_content):\n341                 yield line\n342         else:\n343             while not is_fp_closed(self._fp):\n344                 data = self.read(amt=amt, decode_content=decode_content)\n345 \n346                 if data:\n347                     yield data\n348 \n349     @classmethod\n350     def from_httplib(ResponseCls, r, **response_kw):\n351         \"\"\"\n352         Given an :class:`httplib.HTTPResponse` instance ``r``, return a\n353         corresponding :class:`urllib3.response.HTTPResponse` object.\n354 \n355         Remaining parameters are passed to the HTTPResponse constructor, along\n356         with ``original_response=r``.\n357         \"\"\"\n358         headers = r.msg\n359 \n360         if not isinstance(headers, HTTPHeaderDict):\n361             if PY3:  # Python 3\n362                 headers = HTTPHeaderDict(headers.items())\n363             else:  # Python 2\n364                 headers = HTTPHeaderDict.from_httplib(headers)\n365 \n366         # HTTPResponse objects in Python 3 don't have a .strict attribute\n367         strict = getattr(r, 'strict', 0)\n368         resp = ResponseCls(body=r,\n369                            headers=headers,\n370                            status=r.status,\n371                            version=r.version,\n372                            reason=r.reason,\n373                            strict=strict,\n374                            original_response=r,\n375                            **response_kw)\n376         return resp\n377 \n378     # Backwards-compatibility methods for httplib.HTTPResponse\n379     def getheaders(self):\n380         return self.headers\n381 \n382     def getheader(self, name, default=None):\n383         return self.headers.get(name, default)\n384 \n385     # Overrides from io.IOBase\n386     def close(self):\n387         if not self.closed:\n388             self._fp.close()\n389 \n390     @property\n391     def closed(self):\n392         if self._fp is None:\n393             return True\n394         elif hasattr(self._fp, 'closed'):\n395             return self._fp.closed\n396         elif hasattr(self._fp, 'isclosed'):  # Python 2\n397             return self._fp.isclosed()\n398         else:\n399             return True\n400 \n401     def fileno(self):\n402         if self._fp is None:\n403             raise IOError(\"HTTPResponse has no file to get a fileno from\")\n404         elif hasattr(self._fp, \"fileno\"):\n405             return self._fp.fileno()\n406         else:\n407             raise IOError(\"The file-like object this HTTPResponse is wrapped \"\n408                           \"around has no file descriptor\")\n409 \n410     def flush(self):\n411         if self._fp is not None and hasattr(self._fp, 'flush'):\n412             return self._fp.flush()\n413 \n414     def readable(self):\n415         # This method is required for `io` module compatibility.\n416         return True\n417 \n418     def readinto(self, b):\n419         # This method is required for `io` module compatibility.\n420         temp = self.read(len(b))\n421         if len(temp) == 0:\n422             return 0\n423         else:\n424             b[:len(temp)] = temp\n425             return len(temp)\n426 \n427     def _update_chunk_length(self):\n428         # First, we'll figure out length of a chunk and then\n429         # we'll try to read it from socket.\n430         if self.chunk_left is not None:\n431             return\n432         line = self._fp.fp.readline()\n433         line = line.split(b';', 1)[0]\n434         try:\n435             self.chunk_left = int(line, 16)\n436         except ValueError:\n437             # Invalid chunked protocol response, abort.\n438             self.close()\n439             raise httplib.IncompleteRead(line)\n440 \n441     def _handle_chunk(self, amt):\n442         returned_chunk = None\n443         if amt is None:\n444             chunk = self._fp._safe_read(self.chunk_left)\n445             returned_chunk = chunk\n446             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.\n447             self.chunk_left = None\n448         elif amt < self.chunk_left:\n449             value = self._fp._safe_read(amt)\n450             self.chunk_left = self.chunk_left - amt\n451             returned_chunk = value\n452         elif amt == self.chunk_left:\n453             value = self._fp._safe_read(amt)\n454             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.\n455             self.chunk_left = None\n456             returned_chunk = value\n457         else:  # amt > self.chunk_left\n458             returned_chunk = self._fp._safe_read(self.chunk_left)\n459             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.\n460             self.chunk_left = None\n461         return returned_chunk\n462 \n463     def read_chunked(self, amt=None, decode_content=None):\n464         \"\"\"\n465         Similar to :meth:`HTTPResponse.read`, but with an additional\n466         parameter: ``decode_content``.\n467 \n468         :param decode_content:\n469             If True, will attempt to decode the body based on the\n470             'content-encoding' header.\n471         \"\"\"\n472         self._init_decoder()\n473         # FIXME: Rewrite this method and make it a class with a better structured logic.\n474         if not self.chunked:\n475             raise ResponseNotChunked(\n476                 \"Response is not chunked. \"\n477                 \"Header 'transfer-encoding: chunked' is missing.\")\n478 \n479         # Don't bother reading the body of a HEAD request.\n480         if self._original_response and is_response_to_head(self._original_response):\n481             self._original_response.close()\n482             return\n483 \n484         with self._error_catcher():\n485             while True:\n486                 self._update_chunk_length()\n487                 if self.chunk_left == 0:\n488                     break\n489                 chunk = self._handle_chunk(amt)\n490                 decoded = self._decode(chunk, decode_content=decode_content,\n491                                        flush_decoder=False)\n492                 if decoded:\n493                     yield decoded\n494 \n495             if decode_content:\n496                 # On CPython and PyPy, we should never need to flush the\n497                 # decoder. However, on Jython we *might* need to, so\n498                 # lets defensively do it anyway.\n499                 decoded = self._flush_decoder()\n500                 if decoded:  # Platform-specific: Jython.\n501                     yield decoded\n502 \n503             # Chunk content ends with \\r\\n: discard it.\n504             while True:\n505                 line = self._fp.fp.readline()\n506                 if not line:\n507                     # Some sites may not end with '\\r\\n'.\n508                     break\n509                 if line == b'\\r\\n':\n510                     break\n511 \n512             # We read everything; close the \"file\".\n513             if self._original_response:\n514                 self._original_response.close()\n",
        "intended_behavior": "The response handling, especially for 3XX status codes that involve redirects, should ensure that the Location header is correctly interpreted and that any non-ASCII URL is properly encoded before the client is redirected."
    }
]