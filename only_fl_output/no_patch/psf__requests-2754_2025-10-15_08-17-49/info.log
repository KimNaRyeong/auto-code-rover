2025-10-15 08:17:49.272 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 08:17:50.819 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=436, output_tokens=12, cost=0.004720
2025-10-15 08:17:50.820 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 08:17:50.825 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 08:17:59.450 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=745, output_tokens=416, cost=0.019930
2025-10-15 08:17:59.476 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 08:18:03.142 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=770, output_tokens=63, cost=0.009590
2025-10-15 08:18:03.144 | DEBUG    | Extracted a valid json.
2025-10-15 08:18:03.144 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"Redirect /index.html\")",
        "search_code(\".htaccess\")",
        "search_code(\"URLEncoder\")",
        "search_class(\"ApacheConfig\")",
        "search_code(\"encoding\")"
    ],
    "bug_locations": []
}
2025-10-15 08:18:03.190 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 08:18:03.191 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 08:18:20.325 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1664, output_tokens=657, cost=0.036350
2025-10-15 08:18:20.339 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 08:18:36.333 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2537, output_tokens=561, cost=0.042200
2025-10-15 08:18:36.345 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 08:18:42.178 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=915, output_tokens=274, cost=0.017370
2025-10-15 08:18:42.179 | DEBUG    | Extracted a valid json.
2025-10-15 08:18:42.179 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "requests/models.py", 
            "class": "", 
            "method": "", 
            "intended_behavior": "The code should ensure that when a redirect URL is processed, non-ASCII characters in the URL are correctly percent-encoded according to RFC 3986."
        }, 
        {
            "file": "requests/utils.py", 
            "class": "", 
            "method": "", 
            "intended_behavior": "Any utility functions that parse or prepare URLs for requests, including redirected URLs, should accurately encode non-ASCII characters."
        },
        {
            "file": "requests/packages/urllib3/util/request.py", 
            "class": "", 
            "method": "", 
            "intended_behavior": "Ensure that utilities related to preparing or modifying request objects correctly handle encoding of non-ASCII characters in URLs."
        },
        {
            "file": "requests/packages/urllib3/response.py", 
            "class": "", 
            "method": "", 
            "intended_behavior": "The response handling, especially for 3XX status codes that involve redirects, should ensure that the Location header is correctly interpreted and that any non-ASCII URL is properly encoded before the client is redirected."
        }
    ]
}
2025-10-15 08:18:42.187 | DEBUG    | Bug location extracted successfully: [<file>requests/models.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.models
5 ~~~~~~~~~~~~~~~
6 
7 This module contains the primary objects that power Requests.
8 """
9 
10 import collections
11 import datetime
12 
13 from io import BytesIO, UnsupportedOperation
14 from .hooks import default_hooks
15 from .structures import CaseInsensitiveDict
16 
17 import requests
18 from .auth import HTTPBasicAuth
19 from .cookies import cookiejar_from_dict, get_cookie_header, _copy_cookie_jar
20 from .packages.urllib3.fields import RequestField
21 from .packages.urllib3.filepost import encode_multipart_formdata
22 from .packages.urllib3.util import parse_url
23 from .packages.urllib3.exceptions import (
24     DecodeError, ReadTimeoutError, ProtocolError, LocationParseError)
25 from .exceptions import (
26     HTTPError, MissingScheme, InvalidURL, ChunkedEncodingError,
27     ContentDecodingError, ConnectionError, StreamConsumedError)
28 from .utils import (
29     guess_filename, get_auth_from_url, requote_uri,
30     stream_decode_response_unicode, to_key_val_list, parse_header_links,
31     iter_slices, guess_json_utf, super_len, to_native_string)
32 from .compat import (
33     cookielib, urlunparse, urlsplit, urlencode, str, bytes, StringIO,
34     is_py2, chardet, builtin_str, basestring)
35 import json as complexjson
36 from .status_codes import codes
37 
38 #: The set of HTTP status codes that indicate an automatically
39 #: processable redirect.
40 REDIRECT_STATI = (
41     codes.moved,              # 301
42     codes.found,              # 302
43     codes.other,              # 303
44     codes.temporary_redirect, # 307
45     codes.permanent_redirect, # 308
46 )
47 
48 DEFAULT_REDIRECT_LIMIT = 30
49 CONTENT_CHUNK_SIZE = 10 * 1024
50 ITER_CHUNK_SIZE = 512
51 
52 
53 class RequestEncodingMixin(object):
54     @property
55     def path_url(self):
56         """Build the path URL to use."""
57 
58         url = []
59 
60         p = urlsplit(self.url)
61 
62         path = p.path
63         if not path:
64             path = '/'
65 
66         url.append(path)
67 
68         query = p.query
69         if query:
70             url.append('?')
71             url.append(query)
72 
73         return ''.join(url)
74 
75     @staticmethod
76     def _encode_params(data):
77         """Encode parameters in a piece of data.
78 
79         Will successfully encode parameters when passed as a dict or a list of
80         2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
81         if parameters are supplied as a dict.
82         """
83 
84         if isinstance(data, (str, bytes)):
85             return data
86         elif hasattr(data, 'read'):
87             return data
88         elif hasattr(data, '__iter__'):
89             result = []
90             for k, vs in to_key_val_list(data):
91                 if isinstance(vs, basestring) or not hasattr(vs, '__iter__'):
92                     vs = [vs]
93                 for v in vs:
94                     if v is not None:
95                         result.append(
96                             (k.encode('utf-8') if isinstance(k, str) else k,
97                              v.encode('utf-8') if isinstance(v, str) else v))
98             return urlencode(result, doseq=True)
99         else:
100             return data
101 
102     @staticmethod
103     def _encode_files(files, data):
104         """Build the body for a multipart/form-data request.
105 
106         Will successfully encode files when passed as a dict or a list of
107         2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
108         if parameters are supplied as a dict.
109 
110         """
111         if (not files):
112             raise ValueError("Files must be provided.")
113         elif isinstance(data, basestring):
114             raise ValueError("Data must not be a string.")
115 
116         new_fields = []
117         fields = to_key_val_list(data or {})
118         files = to_key_val_list(files or {})
119 
120         for field, val in fields:
121             if isinstance(val, basestring) or not hasattr(val, '__iter__'):
122                 val = [val]
123             for v in val:
124                 if v is not None:
125                     # Don't call str() on bytestrings: in Py3 it all goes wrong.
126                     if not isinstance(v, bytes):
127                         v = str(v)
128 
129                     new_fields.append(
130                         (field.decode('utf-8') if isinstance(field, bytes) else field,
131                          v.encode('utf-8') if isinstance(v, str) else v))
132 
133         for (k, v) in files:
134             # support for explicit filename
135             ft = None
136             fh = None
137             if isinstance(v, (tuple, list)):
138                 if len(v) == 2:
139                     fn, fp = v
140                 elif len(v) == 3:
141                     fn, fp, ft = v
142                 else:
143                     fn, fp, ft, fh = v
144             else:
145                 fn = guess_filename(v) or k
146                 fp = v
147 
148             if isinstance(fp, (str, bytes, bytearray)):
149                 fdata = fp
150             else:
151                 fdata = fp.read()
152 
153             rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)
154             rf.make_multipart(content_type=ft)
155             new_fields.append(rf)
156 
157         body, content_type = encode_multipart_formdata(new_fields)
158 
159         return body, content_type
160 
161 
162 class RequestHooksMixin(object):
163     def register_hook(self, event, hook):
164         """Properly register a hook."""
165 
166         if event not in self.hooks:
167             raise ValueError('Unsupported event specified, with event name "%s"' % (event))
168 
169         if isinstance(hook, collections.Callable):
170             self.hooks[event].append(hook)
171         elif hasattr(hook, '__iter__'):
172             self.hooks[event].extend(h for h in hook if isinstance(h, collections.Callable))
173 
174     def deregister_hook(self, event, hook):
175         """Deregister a previously registered hook.
176         Returns True if the hook existed, False if not.
177         """
178 
179         try:
180             self.hooks[event].remove(hook)
181             return True
182         except ValueError:
183             return False
184 
185 
186 class Request(RequestHooksMixin):
187     """A user-created :class:`Request <Request>` object.
188 
189     Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.
190 
191     :param method: HTTP method to use.
192     :param url: URL to send.
193     :param headers: dictionary of headers to send.
194     :param files: dictionary of {filename: fileobject} files to multipart upload.
195     :param data: the body to attach to the request. If a dictionary is provided, form-encoding will take place.
196     :param json: json for the body to attach to the request (if files or data is not specified).
197     :param params: dictionary of URL parameters to append to the URL.
198     :param auth: Auth handler or (user, pass) tuple.
199     :param cookies: dictionary or CookieJar of cookies to attach to this request.
200     :param hooks: dictionary of callback hooks, for internal usage.
201 
202     Usage::
203 
204       >>> import requests
205       >>> req = requests.Request('GET', 'http://httpbin.org/get')
206       >>> req.prepare()
207       <PreparedRequest [GET]>
208 
209     """
210     def __init__(self, method=None, url=None, headers=None, files=None,
211         data=None, params=None, auth=None, cookies=None, hooks=None, json=None):
212 
213         # Default empty dicts for dict params.
214         data = [] if data is None else data
215         files = [] if files is None else files
216         headers = {} if headers is None else headers
217         params = {} if params is None else params
218         hooks = {} if hooks is None else hooks
219 
220         self.hooks = default_hooks()
221         for (k, v) in list(hooks.items()):
222             self.register_hook(event=k, hook=v)
223 
224         self.method = method
225         self.url = url
226         self.headers = headers
227         self.files = files
228         self.data = data
229         self.json = json
230         self.params = params
231         self.auth = auth
232         self.cookies = cookies
233 
234     def __repr__(self):
235         return '<Request [%s]>' % (self.method)
236 
237     def prepare(self):
238         """Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it."""
239         p = PreparedRequest()
240         p.prepare(
241             method=self.method,
242             url=self.url,
243             headers=self.headers,
244             files=self.files,
245             data=self.data,
246             json=self.json,
247             params=self.params,
248             auth=self.auth,
249             cookies=self.cookies,
250             hooks=self.hooks,
251         )
252         return p
253 
254 
255 class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):
256     """The fully mutable :class:`PreparedRequest <PreparedRequest>` object,
257     containing the exact bytes that will be sent to the server.
258 
259     Generated from either a :class:`Request <Request>` object or manually.
260 
261     Usage::
262 
263       >>> import requests
264       >>> req = requests.Request('GET', 'http://httpbin.org/get')
265       >>> r = req.prepare()
266       <PreparedRequest [GET]>
267 
268       >>> s = requests.Session()
269       >>> s.send(r)
270       <Response [200]>
271 
272     """
273 
274     def __init__(self):
275         #: HTTP verb to send to the server.
276         self.method = None
277         #: HTTP URL to send the request to.
278         self.url = None
279         #: dictionary of HTTP headers.
280         self.headers = None
281         # The `CookieJar` used to create the Cookie header will be stored here
282         # after prepare_cookies is called
283         self._cookies = None
284         #: request body to send to the server.
285         self.body = None
286         #: dictionary of callback hooks, for internal usage.
287         self.hooks = default_hooks()
288 
289     def prepare(self, method=None, url=None, headers=None, files=None,
290         data=None, params=None, auth=None, cookies=None, hooks=None, json=None):
291         """Prepares the entire request with the given parameters."""
292 
293         self.prepare_method(method)
294         self.prepare_url(url, params)
295         self.prepare_headers(headers)
296         self.prepare_cookies(cookies)
297         self.prepare_body(data, files, json)
298         self.prepare_auth(auth, url)
299 
300         # Note that prepare_auth must be last to enable authentication schemes
301         # such as OAuth to work on a fully prepared request.
302 
303         # This MUST go after prepare_auth. Authenticators could add a hook
304         self.prepare_hooks(hooks)
305 
306     def __repr__(self):
307         return '<PreparedRequest [%s]>' % (self.method)
308 
309     def copy(self):
310         p = PreparedRequest()
311         p.method = self.method
312         p.url = self.url
313         p.headers = self.headers.copy() if self.headers is not None else None
314         p._cookies = _copy_cookie_jar(self._cookies)
315         p.body = self.body
316         p.hooks = self.hooks
317         return p
318 
319     def prepare_method(self, method):
320         """Prepares the given HTTP method."""
321         self.method = method
322         if self.method is None:
323             raise ValueError('Request method cannot be "None"')
324         self.method = to_native_string(self.method).upper()
325 
326     def prepare_url(self, url, params):
327         """Prepares the given HTTP URL."""
328         #: Accept objects that have string representations.
329         #: We're unable to blindly call unicode/str functions
330         #: as this will include the bytestring indicator (b'')
331         #: on python 3.x.
332         #: https://github.com/kennethreitz/requests/pull/2238
333         if isinstance(url, bytes):
334             url = url.decode('utf8')
335         else:
336             url = unicode(url) if is_py2 else str(url)
337 
338         # Ignore any leading and trailing whitespace characters.
339         url = url.strip()
340 
341         # Don't do any URL preparation for non-HTTP schemes like `mailto`,
342         # `data` etc to work around exceptions from `url_parse`, which
343         # handles RFC 3986 only.
344         if ':' in url and not url.lower().startswith('http'):
345             self.url = url
346             return
347 
348         # Support for unicode domain names and paths.
349         try:
350             scheme, auth, host, port, path, query, fragment = parse_url(url)
351         except LocationParseError as e:
352             raise InvalidURL(*e.args)
353 
354         if not scheme:
355             error = ("Invalid URL {0!r}: No scheme supplied. Perhaps you meant http://{0}?")
356             error = error.format(to_native_string(url, 'utf8'))
357 
358             raise MissingScheme(error)
359 
360         if not host:
361             raise InvalidURL("Invalid URL %r: No host supplied" % url)
362 
363         # Only want to apply IDNA to the hostname
364         try:
365             host = host.encode('idna').decode('utf-8')
366         except UnicodeError:
367             raise InvalidURL('URL has an invalid label.')
368 
369         # Carefully reconstruct the network location
370         netloc = auth or ''
371         if netloc:
372             netloc += '@'
373         netloc += host
374         if port:
375             netloc += ':' + str(port)
376 
377         # Bare domains aren't valid URLs.
378         if not path:
379             path = '/'
380 
381         if is_py2:
382             if isinstance(scheme, str):
383                 scheme = scheme.encode('utf-8')
384             if isinstance(netloc, str):
385                 netloc = netloc.encode('utf-8')
386             if isinstance(path, str):
387                 path = path.encode('utf-8')
388             if isinstance(query, str):
389                 query = query.encode('utf-8')
390             if isinstance(fragment, str):
391                 fragment = fragment.encode('utf-8')
392 
393         if isinstance(params, (str, bytes)):
394             params = to_native_string(params)
395 
396         enc_params = self._encode_params(params)
397         if enc_params:
398             if query:
399                 query = '%s&%s' % (query, enc_params)
400             else:
401                 query = enc_params
402 
403         url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))
404         self.url = url
405 
406     def prepare_headers(self, headers):
407         """Prepares the given HTTP headers."""
408 
409         if headers:
410             self.headers = CaseInsensitiveDict((to_native_string(name), value) for name, value in headers.items())
411         else:
412             self.headers = CaseInsensitiveDict()
413 
414     def prepare_body(self, data, files, json=None):
415         """Prepares the given HTTP body data."""
416 
417         # Check if file, fo, generator, iterator.
418         # If not, run through normal process.
419 
420         # Nottin' on you.
421         body = None
422         content_type = None
423         length = None
424 
425         if not data and json is not None:
426             content_type = 'application/json'
427             body = complexjson.dumps(json)
428 
429         is_stream = all([
430             hasattr(data, '__iter__'),
431             not isinstance(data, (basestring, list, tuple, dict))
432         ])
433 
434         try:
435             length = super_len(data)
436         except (TypeError, AttributeError, UnsupportedOperation):
437             length = None
438 
439         if is_stream:
440             body = data
441 
442             if files:
443                 raise NotImplementedError('Streamed bodies and files are mutually exclusive.')
444 
445             if length:
446                 self.headers['Content-Length'] = builtin_str(length)
447             else:
448                 self.headers['Transfer-Encoding'] = 'chunked'
449         else:
450             # Multi-part file uploads.
451             if files:
452                 (body, content_type) = self._encode_files(files, data)
453             else:
454                 if data:
455                     body = self._encode_params(data)
456                     if isinstance(data, basestring) or hasattr(data, 'read'):
457                         content_type = None
458                     else:
459                         content_type = 'application/x-www-form-urlencoded'
460 
461             self.prepare_content_length(body)
462 
463             # Add content-type if it wasn't explicitly provided.
464             if content_type and ('content-type' not in self.headers):
465                 self.headers['Content-Type'] = content_type
466 
467         self.body = body
468 
469     def prepare_content_length(self, body):
470         if hasattr(body, 'seek') and hasattr(body, 'tell'):
471             body.seek(0, 2)
472             self.headers['Content-Length'] = builtin_str(body.tell())
473             body.seek(0, 0)
474         elif body is not None:
475             l = super_len(body)
476             if l:
477                 self.headers['Content-Length'] = builtin_str(l)
478         elif (self.method not in ('GET', 'HEAD')) and (self.headers.get('Content-Length') is None):
479             self.headers['Content-Length'] = '0'
480 
481     def prepare_auth(self, auth, url=''):
482         """Prepares the given HTTP auth data."""
483 
484         # If no Auth is explicitly provided, extract it from the URL first.
485         if auth is None:
486             url_auth = get_auth_from_url(self.url)
487             auth = url_auth if any(url_auth) else None
488 
489         if auth:
490             if isinstance(auth, tuple) and len(auth) == 2:
491                 # special-case basic HTTP auth
492                 auth = HTTPBasicAuth(*auth)
493 
494             # Allow auth to make its changes.
495             r = auth(self)
496 
497             # Update self to reflect the auth changes.
498             self.__dict__.update(r.__dict__)
499 
500             # Recompute Content-Length
501             self.prepare_content_length(self.body)
502 
503     def prepare_cookies(self, cookies):
504         """Prepares the given HTTP cookie data.
505 
506         This function eventually generates a ``Cookie`` header from the
507         given cookies using cookielib. Due to cookielib's design, the header
508         will not be regenerated if it already exists, meaning this function
509         can only be called once for the life of the
510         :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls
511         to ``prepare_cookies`` will have no actual effect, unless the "Cookie"
512         header is removed beforehand."""
513 
514         if isinstance(cookies, cookielib.CookieJar):
515             self._cookies = cookies
516         else:
517             self._cookies = cookiejar_from_dict(cookies)
518 
519         cookie_header = get_cookie_header(self._cookies, self)
520         if cookie_header is not None:
521             self.headers['Cookie'] = cookie_header
522 
523     def prepare_hooks(self, hooks):
524         """Prepares the given hooks."""
525         # hooks can be passed as None to the prepare method and to this
526         # method. To prevent iterating over None, simply use an empty list
527         # if hooks is False-y
528         hooks = hooks or []
529         for event in hooks:
530             self.register_hook(event, hooks[event])
531 
532     def send(self, session=None, **send_kwargs):
533         """Sends the PreparedRequest to the given Session.
534         If none is provided, one is created for you."""
535         session = requests.Session() if session is None else session
536 
537         with session:
538             return session.send(self, **send_kwargs)
539 
540 
541 class Response(object):
542     """The :class:`Response <Response>` object, which contains a
543     server's response to an HTTP request.
544     """
545 
546     __attrs__ = [
547         '_content', 'status_code', 'headers', 'url', 'history',
548         'encoding', 'reason', 'cookies', 'elapsed', 'request'
549     ]
550 
551     def __init__(self):
552         super(Response, self).__init__()
553 
554         self._content = False
555         self._content_consumed = False
556 
557         #: Integer Code of responded HTTP Status, e.g. 404 or 200.
558         self.status_code = None
559 
560         #: Case-insensitive Dictionary of Response Headers.
561         #: For example, ``headers['content-encoding']`` will return the
562         #: value of a ``'Content-Encoding'`` response header.
563         self.headers = CaseInsensitiveDict()
564 
565         #: File-like object representation of response (for advanced usage).
566         #: Use of ``raw`` requires that ``stream=True`` be set on the request.
567         # This requirement does not apply for use internally to Requests.
568         self.raw = None
569 
570         #: Final URL location of Response.
571         self.url = None
572 
573         #: Encoding to decode with when accessing r.text.
574         self.encoding = None
575 
576         #: A list of :class:`Response <Response>` objects from
577         #: the history of the Request. Any redirect responses will end
578         #: up here. The list is sorted from the oldest to the most recent request.
579         self.history = []
580 
581         #: Textual reason of responded HTTP Status, e.g. "Not Found" or "OK".
582         self.reason = None
583 
584         #: A CookieJar of Cookies the server sent back.
585         self.cookies = cookiejar_from_dict({})
586 
587         #: The amount of time elapsed between sending the request
588         #: and the arrival of the response (as a timedelta).
589         #: This property specifically measures the time taken between sending
590         #: the first byte of the request and finishing parsing the headers. It
591         #: is therefore unaffected by consuming the response content or the
592         #: value of the ``stream`` keyword argument.
593         self.elapsed = datetime.timedelta(0)
594 
595         #: The :class:`PreparedRequest <PreparedRequest>` object to which this
596         #: is a response.
597         self.request = None
598 
599     def __getstate__(self):
600         # Consume everything; accessing the content attribute makes
601         # sure the content has been fully read.
602         if not self._content_consumed:
603             self.content
604 
605         return dict(
606             (attr, getattr(self, attr, None))
607             for attr in self.__attrs__
608         )
609 
610     def __setstate__(self, state):
611         for name, value in state.items():
612             setattr(self, name, value)
613 
614         # pickled objects do not have .raw
615         setattr(self, '_content_consumed', True)
616         setattr(self, 'raw', None)
617 
618     def __repr__(self):
619         return '<Response [%s]>' % (self.status_code)
620 
621     def __iter__(self):
622         """Allows you to use a response as an iterator."""
623         return self.iter_content(128)
624 
625     @property
626     def ok(self):
627         try:
628             self.raise_for_status()
629         except HTTPError:
630             return False
631         return True
632 
633     @property
634     def is_redirect(self):
635         """True if this Response is a well-formed HTTP redirect that could have
636         been processed automatically (by :meth:`Session.resolve_redirects`).
637         """
638         return ('location' in self.headers and self.status_code in REDIRECT_STATI)
639 
640     @property
641     def is_permanent_redirect(self):
642         """True if this Response one of the permanent versions of redirect"""
643         return ('location' in self.headers and self.status_code in (codes.moved_permanently, codes.permanent_redirect))
644 
645     @property
646     def apparent_encoding(self):
647         """The apparent encoding, provided by the chardet library"""
648         return chardet.detect(self.content)['encoding']
649 
650     def iter_content(self, chunk_size=1, decode_unicode=False):
651         """Iterates over the response data.  When stream=True is set on the
652         request, this avoids reading the content at once into memory for
653         large responses.  The chunk size is the number of bytes it should
654         read into memory.  This is not necessarily the length of each item
655         returned as decoding can take place.
656 
657         If decode_unicode is True, content will be decoded using the best
658         available encoding based on the response.
659         """
660 
661         def generate():
662             # Special case for urllib3.
663             if hasattr(self.raw, 'stream'):
664                 try:
665                     for chunk in self.raw.stream(chunk_size, decode_content=True):
666                         yield chunk
667                 except ProtocolError as e:
668                     raise ChunkedEncodingError(e)
669                 except DecodeError as e:
670                     raise ContentDecodingError(e)
671                 except ReadTimeoutError as e:
672                     raise ConnectionError(e)
673             else:
674                 # Standard file-like object.
675                 while True:
676                     chunk = self.raw.read(chunk_size)
677                     if not chunk:
678                         break
679                     yield chunk
680 
681             self._content_consumed = True
682 
683         if self._content_consumed and isinstance(self._content, bool):
684             raise StreamConsumedError()
685         # simulate reading small chunks of the content
686         reused_chunks = iter_slices(self._content, chunk_size)
687 
688         stream_chunks = generate()
689 
690         chunks = reused_chunks if self._content_consumed else stream_chunks
691 
692         if decode_unicode:
693             chunks = stream_decode_response_unicode(chunks, self)
694 
695         return chunks
696 
697     def iter_lines(self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=None, delimiter=None):
698         """Iterates over the response data, one line at a time.  When
699         stream=True is set on the request, this avoids reading the
700         content at once into memory for large responses.
701 
702         .. note:: This method is not reentrant safe.
703         """
704 
705         pending = None
706 
707         for chunk in self.iter_content(chunk_size=chunk_size, decode_unicode=decode_unicode):
708 
709             if pending is not None:
710                 chunk = pending + chunk
711 
712             if delimiter:
713                 lines = chunk.split(delimiter)
714             else:
715                 lines = chunk.splitlines()
716 
717             if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:
718                 pending = lines.pop()
719             else:
720                 pending = None
721 
722             for line in lines:
723                 yield line
724 
725         if pending is not None:
726             yield pending
727 
728     @property
729     def content(self):
730         """Content of the response, in bytes."""
731 
732         if self._content is False:
733             # Read the contents.
734             try:
735                 if self._content_consumed:
736                     raise RuntimeError(
737                         'The content for this response was already consumed')
738 
739                 if self.status_code == 0:
740                     self._content = None
741                 else:
742                     self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
743 
744             except AttributeError:
745                 self._content = None
746 
747         self._content_consumed = True
748         # don't need to release the connection; that's been handled by urllib3
749         # since we exhausted the data.
750         return self._content
751 
752     @property
753     def text(self):
754         """Content of the response, in unicode.
755 
756         If Response.encoding is None, encoding will be guessed using
757         ``chardet``.
758 
759         The encoding of the response content is determined based solely on HTTP
760         headers, following RFC 2616 to the letter. If you can take advantage of
761         non-HTTP knowledge to make a better guess at the encoding, you should
762         set ``r.encoding`` appropriately before accessing this property.
763         """
764 
765         # Try charset from content-type
766         content = None
767         encoding = self.encoding
768 
769         if not self.content:
770             return str('')
771 
772         # Fallback to auto-detected encoding.
773         if self.encoding is None:
774             encoding = self.apparent_encoding
775 
776         # Decode unicode from given encoding.
777         try:
778             content = str(self.content, encoding, errors='replace')
779         except (LookupError, TypeError):
780             # A LookupError is raised if the encoding was not found which could
781             # indicate a misspelling or similar mistake.
782             #
783             # A TypeError can be raised if encoding is None
784             #
785             # So we try blindly encoding.
786             content = str(self.content, errors='replace')
787 
788         return content
789 
790     def json(self, **kwargs):
791         """Returns the json-encoded content of a response, if any.
792 
793         :param \*\*kwargs: Optional arguments that ``json.loads`` takes.
794         """
795 
796         if not self.encoding and len(self.content) > 3:
797             # No encoding set. JSON RFC 4627 section 3 states we should expect
798             # UTF-8, -16 or -32. Detect which one to use; If the detection or
799             # decoding fails, fall back to `self.text` (using chardet to make
800             # a best guess).
801             encoding = guess_json_utf(self.content)
802             if encoding is not None:
803                 try:
804                     return complexjson.loads(
805                         self.content.decode(encoding), **kwargs
806                     )
807                 except UnicodeDecodeError:
808                     # Wrong UTF codec detected; usually because it's not UTF-8
809                     # but some other 8-bit codec.  This is an RFC violation,
810                     # and the server didn't bother to tell us what codec *was*
811                     # used.
812                     pass
813         return complexjson.loads(self.text, **kwargs)
814 
815     @property
816     def links(self):
817         """Returns the parsed header links of the response, if any."""
818 
819         header = self.headers.get('link')
820 
821         # l = MultiDict()
822         l = {}
823 
824         if header:
825             links = parse_header_links(header)
826 
827             for link in links:
828                 key = link.get('rel') or link.get('url')
829                 l[key] = link
830 
831         return l
832 
833     def raise_for_status(self):
834         """Raises stored :class:`HTTPError`, if one occurred."""
835 
836         http_error_msg = ''
837 
838         if 400 <= self.status_code < 500:
839             http_error_msg = '%s Client Error: %s for url: %s' % (self.status_code, self.reason, self.url)
840 
841         elif 500 <= self.status_code < 600:
842             http_error_msg = '%s Server Error: %s for url: %s' % (self.status_code, self.reason, self.url)
843 
844         if http_error_msg:
845             raise HTTPError(http_error_msg, response=self)
846 
847     def close(self):
848         """Releases the connection back to the pool. Once this method has been
849         called the underlying ``raw`` object must not be accessed again.
850 
851         *Note: Should not normally need to be called explicitly.*
852         """
853         if not self._content_consumed:
854             return self.raw.close()
855 
856         return self.raw.release_conn()

</code><intended_behavior>The code should ensure that when a redirect URL is processed, non-ASCII characters in the URL are correctly percent-encoded according to RFC 3986.</intended_behavior>, <file>requests/utils.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.utils
5 ~~~~~~~~~~~~~~
6 
7 This module provides utility functions that are used within Requests
8 that are also useful for external consumption.
9 
10 """
11 
12 import cgi
13 import codecs
14 import collections
15 import io
16 import os
17 import platform
18 import re
19 import sys
20 import socket
21 import struct
22 import warnings
23 
24 from . import __version__
25 from . import certs
26 from .compat import parse_http_list as _parse_list_header
27 from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote, is_py2,
28                      builtin_str, getproxies, proxy_bypass, urlunparse,
29                      basestring)
30 from .cookies import RequestsCookieJar, cookiejar_from_dict
31 from .structures import CaseInsensitiveDict
32 from .exceptions import InvalidURL, FileModeWarning
33 
34 _hush_pyflakes = (RequestsCookieJar,)
35 
36 NETRC_FILES = ('.netrc', '_netrc')
37 
38 DEFAULT_CA_BUNDLE_PATH = certs.where()
39 
40 
41 def dict_to_sequence(d):
42     """Returns an internal sequence dictionary update."""
43 
44     if hasattr(d, 'items'):
45         d = d.items()
46 
47     return d
48 
49 
50 def super_len(o):
51     total_length = 0
52     current_position = 0
53 
54     if hasattr(o, '__len__'):
55         total_length = len(o)
56 
57     elif hasattr(o, 'len'):
58         total_length = o.len
59 
60     elif hasattr(o, 'getvalue'):
61         # e.g. BytesIO, cStringIO.StringIO
62         total_length = len(o.getvalue())
63 
64     elif hasattr(o, 'fileno'):
65         try:
66             fileno = o.fileno()
67         except io.UnsupportedOperation:
68             pass
69         else:
70             total_length = os.fstat(fileno).st_size
71 
72             # Having used fstat to determine the file length, we need to
73             # confirm that this file was opened up in binary mode.
74             if 'b' not in o.mode:
75                 warnings.warn((
76                     "Requests has determined the content-length for this "
77                     "request using the binary size of the file: however, the "
78                     "file has been opened in text mode (i.e. without the 'b' "
79                     "flag in the mode). This may lead to an incorrect "
80                     "content-length. In Requests 3.0, support will be removed "
81                     "for files in text mode."),
82                     FileModeWarning
83                 )
84 
85     if hasattr(o, 'tell'):
86         current_position = o.tell()
87 
88     return max(0, total_length - current_position)
89 
90 
91 def get_netrc_auth(url, raise_errors=False):
92     """Returns the Requests tuple auth for a given url from netrc."""
93 
94     try:
95         from netrc import netrc, NetrcParseError
96 
97         netrc_path = None
98 
99         for f in NETRC_FILES:
100             try:
101                 loc = os.path.expanduser('~/{0}'.format(f))
102             except KeyError:
103                 # os.path.expanduser can fail when $HOME is undefined and
104                 # getpwuid fails. See http://bugs.python.org/issue20164 &
105                 # https://github.com/kennethreitz/requests/issues/1846
106                 return
107 
108             if os.path.exists(loc):
109                 netrc_path = loc
110                 break
111 
112         # Abort early if there isn't one.
113         if netrc_path is None:
114             return
115 
116         ri = urlparse(url)
117 
118         # Strip port numbers from netloc. This weird `if...encode`` dance is
119         # used for Python 3.2, which doesn't support unicode literals.
120         splitstr = b':'
121         if isinstance(url, str):
122             splitstr = splitstr.decode('ascii')
123         host = ri.netloc.split(splitstr)[0]
124 
125         try:
126             _netrc = netrc(netrc_path).authenticators(host)
127             if _netrc:
128                 # Return with login / password
129                 login_i = (0 if _netrc[0] else 1)
130                 return (_netrc[login_i], _netrc[2])
131         except (NetrcParseError, IOError):
132             # If there was a parsing error or a permissions issue reading the file,
133             # we'll just skip netrc auth unless explicitly asked to raise errors.
134             if raise_errors:
135                 raise
136 
137     # AppEngine hackiness.
138     except (ImportError, AttributeError):
139         pass
140 
141 
142 def guess_filename(obj):
143     """Tries to guess the filename of the given object."""
144     name = getattr(obj, 'name', None)
145     if (name and isinstance(name, basestring) and name[0] != '<' and
146             name[-1] != '>'):
147         return os.path.basename(name)
148 
149 
150 def from_key_val_list(value):
151     """Take an object and test to see if it can be represented as a
152     dictionary. Unless it can not be represented as such, return an
153     OrderedDict, e.g.,
154 
155     ::
156 
157         >>> from_key_val_list([('key', 'val')])
158         OrderedDict([('key', 'val')])
159         >>> from_key_val_list('string')
160         ValueError: need more than 1 value to unpack
161         >>> from_key_val_list({'key': 'val'})
162         OrderedDict([('key', 'val')])
163     """
164     if value is None:
165         return None
166 
167     if isinstance(value, (str, bytes, bool, int)):
168         raise ValueError('cannot encode objects that are not 2-tuples')
169 
170     return OrderedDict(value)
171 
172 
173 def to_key_val_list(value):
174     """Take an object and test to see if it can be represented as a
175     dictionary. If it can be, return a list of tuples, e.g.,
176 
177     ::
178 
179         >>> to_key_val_list([('key', 'val')])
180         [('key', 'val')]
181         >>> to_key_val_list({'key': 'val'})
182         [('key', 'val')]
183         >>> to_key_val_list('string')
184         ValueError: cannot encode objects that are not 2-tuples.
185     """
186     if value is None:
187         return None
188 
189     if isinstance(value, (str, bytes, bool, int)):
190         raise ValueError('cannot encode objects that are not 2-tuples')
191 
192     if isinstance(value, collections.Mapping):
193         value = value.items()
194 
195     return list(value)
196 
197 
198 # From mitsuhiko/werkzeug (used with permission).
199 def parse_list_header(value):
200     """Parse lists as described by RFC 2068 Section 2.
201 
202     In particular, parse comma-separated lists where the elements of
203     the list may include quoted-strings.  A quoted-string could
204     contain a comma.  A non-quoted string could have quotes in the
205     middle.  Quotes are removed automatically after parsing.
206 
207     It basically works like :func:`parse_set_header` just that items
208     may appear multiple times and case sensitivity is preserved.
209 
210     The return value is a standard :class:`list`:
211 
212     >>> parse_list_header('token, "quoted value"')
213     ['token', 'quoted value']
214 
215     To create a header from the :class:`list` again, use the
216     :func:`dump_header` function.
217 
218     :param value: a string with a list header.
219     :return: :class:`list`
220     """
221     result = []
222     for item in _parse_list_header(value):
223         if item[:1] == item[-1:] == '"':
224             item = unquote_header_value(item[1:-1])
225         result.append(item)
226     return result
227 
228 
229 # From mitsuhiko/werkzeug (used with permission).
230 def parse_dict_header(value):
231     """Parse lists of key, value pairs as described by RFC 2068 Section 2 and
232     convert them into a python dict:
233 
234     >>> d = parse_dict_header('foo="is a fish", bar="as well"')
235     >>> type(d) is dict
236     True
237     >>> sorted(d.items())
238     [('bar', 'as well'), ('foo', 'is a fish')]
239 
240     If there is no value for a key it will be `None`:
241 
242     >>> parse_dict_header('key_without_value')
243     {'key_without_value': None}
244 
245     To create a header from the :class:`dict` again, use the
246     :func:`dump_header` function.
247 
248     :param value: a string with a dict header.
249     :return: :class:`dict`
250     """
251     result = {}
252     for item in _parse_list_header(value):
253         if '=' not in item:
254             result[item] = None
255             continue
256         name, value = item.split('=', 1)
257         if value[:1] == value[-1:] == '"':
258             value = unquote_header_value(value[1:-1])
259         result[name] = value
260     return result
261 
262 
263 # From mitsuhiko/werkzeug (used with permission).
264 def unquote_header_value(value, is_filename=False):
265     r"""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
266     This does not use the real unquoting but what browsers are actually
267     using for quoting.
268 
269     :param value: the header value to unquote.
270     """
271     if value and value[0] == value[-1] == '"':
272         # this is not the real unquoting, but fixing this so that the
273         # RFC is met will result in bugs with internet explorer and
274         # probably some other browsers as well.  IE for example is
275         # uploading files with "C:\foo\bar.txt" as filename
276         value = value[1:-1]
277 
278         # if this is a filename and the starting characters look like
279         # a UNC path, then just return the value without quotes.  Using the
280         # replace sequence below on a UNC path has the effect of turning
281         # the leading double slash into a single slash and then
282         # _fix_ie_filename() doesn't work correctly.  See #458.
283         if not is_filename or value[:2] != '\\\\':
284             return value.replace('\\\\', '\\').replace('\\"', '"')
285     return value
286 
287 
288 def dict_from_cookiejar(cj):
289     """Returns a key/value dictionary from a CookieJar.
290 
291     :param cj: CookieJar object to extract cookies from.
292     """
293 
294     cookie_dict = {}
295 
296     for cookie in cj:
297         cookie_dict[cookie.name] = cookie.value
298 
299     return cookie_dict
300 
301 
302 def add_dict_to_cookiejar(cj, cookie_dict):
303     """Returns a CookieJar from a key/value dictionary.
304 
305     :param cj: CookieJar to insert cookies into.
306     :param cookie_dict: Dict of key/values to insert into CookieJar.
307     """
308 
309     cj2 = cookiejar_from_dict(cookie_dict)
310     cj.update(cj2)
311     return cj
312 
313 
314 def get_encodings_from_content(content):
315     """Returns encodings from given content string.
316 
317     :param content: bytestring to extract encodings from.
318     """
319     warnings.warn((
320         'In requests 3.0, get_encodings_from_content will be removed. For '
321         'more information, please see the discussion on issue #2266. (This'
322         ' warning should only appear once.)'),
323         DeprecationWarning)
324 
325     charset_re = re.compile(r'<meta.*?charset=["\']*(.+?)["\'>]', flags=re.I)
326     pragma_re = re.compile(r'<meta.*?content=["\']*;?charset=(.+?)["\'>]', flags=re.I)
327     xml_re = re.compile(r'^<\?xml.*?encoding=["\']*(.+?)["\'>]')
328 
329     return (charset_re.findall(content) +
330             pragma_re.findall(content) +
331             xml_re.findall(content))
332 
333 
334 def get_encoding_from_headers(headers):
335     """Returns encodings from given HTTP Header Dict.
336 
337     :param headers: dictionary to extract encoding from.
338     """
339 
340     content_type = headers.get('content-type')
341 
342     if not content_type:
343         return None
344 
345     content_type, params = cgi.parse_header(content_type)
346 
347     if 'charset' in params:
348         return params['charset'].strip("'\"")
349 
350     if 'text' in content_type:
351         return 'ISO-8859-1'
352 
353 
354 def stream_decode_response_unicode(iterator, r):
355     """Stream decodes a iterator."""
356 
357     if r.encoding is None:
358         for item in iterator:
359             yield item
360         return
361 
362     decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')
363     for chunk in iterator:
364         rv = decoder.decode(chunk)
365         if rv:
366             yield rv
367     rv = decoder.decode(b'', final=True)
368     if rv:
369         yield rv
370 
371 
372 def iter_slices(string, slice_length):
373     """Iterate over slices of a string."""
374     pos = 0
375     while pos < len(string):
376         yield string[pos:pos + slice_length]
377         pos += slice_length
378 
379 
380 def get_unicode_from_response(r):
381     """Returns the requested content back in unicode.
382 
383     :param r: Response object to get unicode content from.
384 
385     Tried:
386 
387     1. charset from content-type
388     2. fall back and replace all unicode characters
389 
390     """
391     warnings.warn((
392         'In requests 3.0, get_unicode_from_response will be removed. For '
393         'more information, please see the discussion on issue #2266. (This'
394         ' warning should only appear once.)'),
395         DeprecationWarning)
396 
397     tried_encodings = []
398 
399     # Try charset from content-type
400     encoding = get_encoding_from_headers(r.headers)
401 
402     if encoding:
403         try:
404             return str(r.content, encoding)
405         except UnicodeError:
406             tried_encodings.append(encoding)
407 
408     # Fall back:
409     try:
410         return str(r.content, encoding, errors='replace')
411     except TypeError:
412         return r.content
413 
414 
415 # The unreserved URI characters (RFC 3986)
416 UNRESERVED_SET = frozenset(
417     "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
418     + "0123456789-._~")
419 
420 
421 def unquote_unreserved(uri):
422     """Un-escape any percent-escape sequences in a URI that are unreserved
423     characters. This leaves all reserved, illegal and non-ASCII bytes encoded.
424     """
425     parts = uri.split('%')
426     for i in range(1, len(parts)):
427         h = parts[i][0:2]
428         if len(h) == 2 and h.isalnum():
429             try:
430                 c = chr(int(h, 16))
431             except ValueError:
432                 raise InvalidURL("Invalid percent-escape sequence: '%s'" % h)
433 
434             if c in UNRESERVED_SET:
435                 parts[i] = c + parts[i][2:]
436             else:
437                 parts[i] = '%' + parts[i]
438         else:
439             parts[i] = '%' + parts[i]
440     return ''.join(parts)
441 
442 
443 def requote_uri(uri):
444     """Re-quote the given URI.
445 
446     This function passes the given URI through an unquote/quote cycle to
447     ensure that it is fully and consistently quoted.
448     """
449     safe_with_percent = "!#$%&'()*+,/:;=?@[]~"
450     safe_without_percent = "!#$&'()*+,/:;=?@[]~"
451     try:
452         # Unquote only the unreserved characters
453         # Then quote only illegal characters (do not quote reserved,
454         # unreserved, or '%')
455         return quote(unquote_unreserved(uri), safe=safe_with_percent)
456     except InvalidURL:
457         # We couldn't unquote the given URI, so let's try quoting it, but
458         # there may be unquoted '%'s in the URI. We need to make sure they're
459         # properly quoted so they do not cause issues elsewhere.
460         return quote(uri, safe=safe_without_percent)
461 
462 
463 def address_in_network(ip, net):
464     """
465     This function allows you to check if on IP belongs to a network subnet
466     Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
467              returns False if ip = 192.168.1.1 and net = 192.168.100.0/24
468     """
469     ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]
470     netaddr, bits = net.split('/')
471     netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]
472     network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask
473     return (ipaddr & netmask) == (network & netmask)
474 
475 
476 def dotted_netmask(mask):
477     """
478     Converts mask from /xx format to xxx.xxx.xxx.xxx
479     Example: if mask is 24 function returns 255.255.255.0
480     """
481     bits = 0xffffffff ^ (1 << 32 - mask) - 1
482     return socket.inet_ntoa(struct.pack('>I', bits))
483 
484 
485 def is_ipv4_address(string_ip):
486     try:
487         socket.inet_aton(string_ip)
488     except socket.error:
489         return False
490     return True
491 
492 
493 def is_valid_cidr(string_network):
494     """Very simple check of the cidr format in no_proxy variable"""
495     if string_network.count('/') == 1:
496         try:
497             mask = int(string_network.split('/')[1])
498         except ValueError:
499             return False
500 
501         if mask < 1 or mask > 32:
502             return False
503 
504         try:
505             socket.inet_aton(string_network.split('/')[0])
506         except socket.error:
507             return False
508     else:
509         return False
510     return True
511 
512 
513 def should_bypass_proxies(url):
514     """
515     Returns whether we should bypass proxies or not.
516     """
517     get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())
518 
519     # First check whether no_proxy is defined. If it is, check that the URL
520     # we're getting isn't in the no_proxy list.
521     no_proxy = get_proxy('no_proxy')
522     netloc = urlparse(url).netloc
523 
524     if no_proxy:
525         # We need to check whether we match here. We need to see if we match
526         # the end of the netloc, both with and without the port.
527         no_proxy = (
528             host for host in no_proxy.replace(' ', '').split(',') if host
529         )
530 
531         ip = netloc.split(':')[0]
532         if is_ipv4_address(ip):
533             for proxy_ip in no_proxy:
534                 if is_valid_cidr(proxy_ip):
535                     if address_in_network(ip, proxy_ip):
536                         return True
537         else:
538             for host in no_proxy:
539                 if netloc.endswith(host) or netloc.split(':')[0].endswith(host):
540                     # The URL does match something in no_proxy, so we don't want
541                     # to apply the proxies on this URL.
542                     return True
543 
544     # If the system proxy settings indicate that this URL should be bypassed,
545     # don't proxy.
546     # The proxy_bypass function is incredibly buggy on OS X in early versions
547     # of Python 2.6, so allow this call to fail. Only catch the specific
548     # exceptions we've seen, though: this call failing in other ways can reveal
549     # legitimate problems.
550     try:
551         bypass = proxy_bypass(netloc)
552     except (TypeError, socket.gaierror):
553         bypass = False
554 
555     if bypass:
556         return True
557 
558     return False
559 
560 def get_environ_proxies(url):
561     """Return a dict of environment proxies."""
562     if should_bypass_proxies(url):
563         return {}
564     else:
565         return getproxies()
566 
567 def select_proxy(url, proxies):
568     """Select a proxy for the url, if applicable.
569 
570     :param url: The url being for the request
571     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
572     """
573     proxies = proxies or {}
574     urlparts = urlparse(url)
575     proxy = proxies.get(urlparts.scheme+'://'+urlparts.hostname)
576     if proxy is None:
577         proxy = proxies.get(urlparts.scheme)
578     return proxy
579 
580 def default_user_agent(name="python-requests"):
581     """Return a string representing the default user agent."""
582     return '%s/%s' % (name, __version__)
583 
584 
585 def default_headers():
586     return CaseInsensitiveDict({
587         'User-Agent': default_user_agent(),
588         'Accept-Encoding': ', '.join(('gzip', 'deflate')),
589         'Accept': '*/*',
590         'Connection': 'keep-alive',
591     })
592 
593 
594 def parse_header_links(value):
595     """Return a dict of parsed link headers proxies.
596 
597     i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"
598 
599     """
600 
601     links = []
602 
603     replace_chars = " '\""
604 
605     for val in re.split(", *<", value):
606         try:
607             url, params = val.split(";", 1)
608         except ValueError:
609             url, params = val, ''
610 
611         link = {}
612 
613         link["url"] = url.strip("<> '\"")
614 
615         for param in params.split(";"):
616             try:
617                 key, value = param.split("=")
618             except ValueError:
619                 break
620 
621             link[key.strip(replace_chars)] = value.strip(replace_chars)
622 
623         links.append(link)
624 
625     return links
626 
627 
628 # Null bytes; no need to recreate these on each call to guess_json_utf
629 _null = '\x00'.encode('ascii')  # encoding to ASCII for Python 3
630 _null2 = _null * 2
631 _null3 = _null * 3
632 
633 
634 def guess_json_utf(data):
635     # JSON always starts with two ASCII characters, so detection is as
636     # easy as counting the nulls and from their location and count
637     # determine the encoding. Also detect a BOM, if present.
638     sample = data[:4]
639     if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE):
640         return 'utf-32'     # BOM included
641     if sample[:3] == codecs.BOM_UTF8:
642         return 'utf-8-sig'  # BOM included, MS style (discouraged)
643     if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
644         return 'utf-16'     # BOM included
645     nullcount = sample.count(_null)
646     if nullcount == 0:
647         return 'utf-8'
648     if nullcount == 2:
649         if sample[::2] == _null2:   # 1st and 3rd are null
650             return 'utf-16-be'
651         if sample[1::2] == _null2:  # 2nd and 4th are null
652             return 'utf-16-le'
653         # Did not detect 2 valid UTF-16 ascii-range characters
654     if nullcount == 3:
655         if sample[:3] == _null3:
656             return 'utf-32-be'
657         if sample[1:] == _null3:
658             return 'utf-32-le'
659         # Did not detect a valid UTF-32 ascii-range character
660     return None
661 
662 
663 def prepend_scheme_if_needed(url, new_scheme):
664     '''Given a URL that may or may not have a scheme, prepend the given scheme.
665     Does not replace a present scheme with the one provided as an argument.'''
666     scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)
667 
668     # urlparse is a finicky beast, and sometimes decides that there isn't a
669     # netloc present. Assume that it's being over-cautious, and switch netloc
670     # and path if urlparse decided there was no netloc.
671     if not netloc:
672         netloc, path = path, netloc
673 
674     return urlunparse((scheme, netloc, path, params, query, fragment))
675 
676 
677 def get_auth_from_url(url):
678     """Given a url with authentication components, extract them into a tuple of
679     username,password."""
680     parsed = urlparse(url)
681 
682     try:
683         auth = (unquote(parsed.username), unquote(parsed.password))
684     except (AttributeError, TypeError):
685         auth = ('', '')
686 
687     return auth
688 
689 
690 def to_native_string(string, encoding='ascii'):
691     """
692     Given a string object, regardless of type, returns a representation of that
693     string in the native string type, encoding and decoding where necessary.
694     This assumes ASCII unless told otherwise.
695     """
696     out = None
697 
698     if isinstance(string, builtin_str):
699         out = string
700     else:
701         if is_py2:
702             out = string.encode(encoding)
703         else:
704             out = string.decode(encoding)
705 
706     return out
707 
708 
709 def urldefragauth(url):
710     """
711     Given a url remove the fragment and the authentication part
712     """
713     scheme, netloc, path, params, query, fragment = urlparse(url)
714 
715     # see func:`prepend_scheme_if_needed`
716     if not netloc:
717         netloc, path = path, netloc
718 
719     netloc = netloc.rsplit('@', 1)[-1]
720 
721     return urlunparse((scheme, netloc, path, params, query, ''))

</code><intended_behavior>Any utility functions that parse or prepare URLs for requests, including redirected URLs, should accurately encode non-ASCII characters.</intended_behavior>, <file>requests/packages/urllib3/util/request.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import absolute_import
2 from base64 import b64encode
3 
4 from ..packages.six import b
5 
6 ACCEPT_ENCODING = 'gzip,deflate'
7 
8 
9 def make_headers(keep_alive=None, accept_encoding=None, user_agent=None,
10                  basic_auth=None, proxy_basic_auth=None, disable_cache=None):
11     """
12     Shortcuts for generating request headers.
13 
14     :param keep_alive:
15         If ``True``, adds 'connection: keep-alive' header.
16 
17     :param accept_encoding:
18         Can be a boolean, list, or string.
19         ``True`` translates to 'gzip,deflate'.
20         List will get joined by comma.
21         String will be used as provided.
22 
23     :param user_agent:
24         String representing the user-agent you want, such as
25         "python-urllib3/0.6"
26 
27     :param basic_auth:
28         Colon-separated username:password string for 'authorization: basic ...'
29         auth header.
30 
31     :param proxy_basic_auth:
32         Colon-separated username:password string for 'proxy-authorization: basic ...'
33         auth header.
34 
35     :param disable_cache:
36         If ``True``, adds 'cache-control: no-cache' header.
37 
38     Example::
39 
40         >>> make_headers(keep_alive=True, user_agent="Batman/1.0")
41         {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}
42         >>> make_headers(accept_encoding=True)
43         {'accept-encoding': 'gzip,deflate'}
44     """
45     headers = {}
46     if accept_encoding:
47         if isinstance(accept_encoding, str):
48             pass
49         elif isinstance(accept_encoding, list):
50             accept_encoding = ','.join(accept_encoding)
51         else:
52             accept_encoding = ACCEPT_ENCODING
53         headers['accept-encoding'] = accept_encoding
54 
55     if user_agent:
56         headers['user-agent'] = user_agent
57 
58     if keep_alive:
59         headers['connection'] = 'keep-alive'
60 
61     if basic_auth:
62         headers['authorization'] = 'Basic ' + \
63             b64encode(b(basic_auth)).decode('utf-8')
64 
65     if proxy_basic_auth:
66         headers['proxy-authorization'] = 'Basic ' + \
67             b64encode(b(proxy_basic_auth)).decode('utf-8')
68 
69     if disable_cache:
70         headers['cache-control'] = 'no-cache'
71 
72     return headers

</code><intended_behavior>Ensure that utilities related to preparing or modifying request objects correctly handle encoding of non-ASCII characters in URLs.</intended_behavior>, <file>requests/packages/urllib3/response.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import absolute_import
2 from contextlib import contextmanager
3 import zlib
4 import io
5 from socket import timeout as SocketTimeout
6 from socket import error as SocketError
7 
8 from ._collections import HTTPHeaderDict
9 from .exceptions import (
10     ProtocolError, DecodeError, ReadTimeoutError, ResponseNotChunked
11 )
12 from .packages.six import string_types as basestring, binary_type, PY3
13 from .packages.six.moves import http_client as httplib
14 from .connection import HTTPException, BaseSSLError
15 from .util.response import is_fp_closed, is_response_to_head
16 
17 
18 class DeflateDecoder(object):
19 
20     def __init__(self):
21         self._first_try = True
22         self._data = binary_type()
23         self._obj = zlib.decompressobj()
24 
25     def __getattr__(self, name):
26         return getattr(self._obj, name)
27 
28     def decompress(self, data):
29         if not data:
30             return data
31 
32         if not self._first_try:
33             return self._obj.decompress(data)
34 
35         self._data += data
36         try:
37             return self._obj.decompress(data)
38         except zlib.error:
39             self._first_try = False
40             self._obj = zlib.decompressobj(-zlib.MAX_WBITS)
41             try:
42                 return self.decompress(self._data)
43             finally:
44                 self._data = None
45 
46 
47 class GzipDecoder(object):
48 
49     def __init__(self):
50         self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
51 
52     def __getattr__(self, name):
53         return getattr(self._obj, name)
54 
55     def decompress(self, data):
56         if not data:
57             return data
58         return self._obj.decompress(data)
59 
60 
61 def _get_decoder(mode):
62     if mode == 'gzip':
63         return GzipDecoder()
64 
65     return DeflateDecoder()
66 
67 
68 class HTTPResponse(io.IOBase):
69     """
70     HTTP Response container.
71 
72     Backwards-compatible to httplib's HTTPResponse but the response ``body`` is
73     loaded and decoded on-demand when the ``data`` property is accessed.  This
74     class is also compatible with the Python standard library's :mod:`io`
75     module, and can hence be treated as a readable object in the context of that
76     framework.
77 
78     Extra parameters for behaviour not present in httplib.HTTPResponse:
79 
80     :param preload_content:
81         If True, the response's body will be preloaded during construction.
82 
83     :param decode_content:
84         If True, attempts to decode specific content-encoding's based on headers
85         (like 'gzip' and 'deflate') will be skipped and raw data will be used
86         instead.
87 
88     :param original_response:
89         When this HTTPResponse wrapper is generated from an httplib.HTTPResponse
90         object, it's convenient to include the original for debug purposes. It's
91         otherwise unused.
92     """
93 
94     CONTENT_DECODERS = ['gzip', 'deflate']
95     REDIRECT_STATUSES = [301, 302, 303, 307, 308]
96 
97     def __init__(self, body='', headers=None, status=0, version=0, reason=None,
98                  strict=0, preload_content=True, decode_content=True,
99                  original_response=None, pool=None, connection=None):
100 
101         if isinstance(headers, HTTPHeaderDict):
102             self.headers = headers
103         else:
104             self.headers = HTTPHeaderDict(headers)
105         self.status = status
106         self.version = version
107         self.reason = reason
108         self.strict = strict
109         self.decode_content = decode_content
110 
111         self._decoder = None
112         self._body = None
113         self._fp = None
114         self._original_response = original_response
115         self._fp_bytes_read = 0
116 
117         if body and isinstance(body, (basestring, binary_type)):
118             self._body = body
119 
120         self._pool = pool
121         self._connection = connection
122 
123         if hasattr(body, 'read'):
124             self._fp = body
125 
126         # Are we using the chunked-style of transfer encoding?
127         self.chunked = False
128         self.chunk_left = None
129         tr_enc = self.headers.get('transfer-encoding', '').lower()
130         # Don't incur the penalty of creating a list and then discarding it
131         encodings = (enc.strip() for enc in tr_enc.split(","))
132         if "chunked" in encodings:
133             self.chunked = True
134 
135         # If requested, preload the body.
136         if preload_content and not self._body:
137             self._body = self.read(decode_content=decode_content)
138 
139     def get_redirect_location(self):
140         """
141         Should we redirect and where to?
142 
143         :returns: Truthy redirect location string if we got a redirect status
144             code and valid location. ``None`` if redirect status and no
145             location. ``False`` if not a redirect status code.
146         """
147         if self.status in self.REDIRECT_STATUSES:
148             return self.headers.get('location')
149 
150         return False
151 
152     def release_conn(self):
153         if not self._pool or not self._connection:
154             return
155 
156         self._pool._put_conn(self._connection)
157         self._connection = None
158 
159     @property
160     def data(self):
161         # For backwords-compat with earlier urllib3 0.4 and earlier.
162         if self._body:
163             return self._body
164 
165         if self._fp:
166             return self.read(cache_content=True)
167 
168     def tell(self):
169         """
170         Obtain the number of bytes pulled over the wire so far. May differ from
171         the amount of content returned by :meth:``HTTPResponse.read`` if bytes
172         are encoded on the wire (e.g, compressed).
173         """
174         return self._fp_bytes_read
175 
176     def _init_decoder(self):
177         """
178         Set-up the _decoder attribute if necessar.
179         """
180         # Note: content-encoding value should be case-insensitive, per RFC 7230
181         # Section 3.2
182         content_encoding = self.headers.get('content-encoding', '').lower()
183         if self._decoder is None and content_encoding in self.CONTENT_DECODERS:
184             self._decoder = _get_decoder(content_encoding)
185 
186     def _decode(self, data, decode_content, flush_decoder):
187         """
188         Decode the data passed in and potentially flush the decoder.
189         """
190         try:
191             if decode_content and self._decoder:
192                 data = self._decoder.decompress(data)
193         except (IOError, zlib.error) as e:
194             content_encoding = self.headers.get('content-encoding', '').lower()
195             raise DecodeError(
196                 "Received response with content-encoding: %s, but "
197                 "failed to decode it." % content_encoding, e)
198 
199         if flush_decoder and decode_content:
200             data += self._flush_decoder()
201 
202         return data
203 
204     def _flush_decoder(self):
205         """
206         Flushes the decoder. Should only be called if the decoder is actually
207         being used.
208         """
209         if self._decoder:
210             buf = self._decoder.decompress(b'')
211             return buf + self._decoder.flush()
212 
213         return b''
214 
215     @contextmanager
216     def _error_catcher(self):
217         """
218         Catch low-level python exceptions, instead re-raising urllib3
219         variants, so that low-level exceptions are not leaked in the
220         high-level api.
221 
222         On exit, release the connection back to the pool.
223         """
224         try:
225             try:
226                 yield
227 
228             except SocketTimeout:
229                 # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
230                 # there is yet no clean way to get at it from this context.
231                 raise ReadTimeoutError(self._pool, None, 'Read timed out.')
232 
233             except BaseSSLError as e:
234                 # FIXME: Is there a better way to differentiate between SSLErrors?
235                 if 'read operation timed out' not in str(e):  # Defensive:
236                     # This shouldn't happen but just in case we're missing an edge
237                     # case, let's avoid swallowing SSL errors.
238                     raise
239 
240                 raise ReadTimeoutError(self._pool, None, 'Read timed out.')
241 
242             except (HTTPException, SocketError) as e:
243                 # This includes IncompleteRead.
244                 raise ProtocolError('Connection broken: %r' % e, e)
245 
246         except Exception:
247             # The response may not be closed but we're not going to use it anymore
248             # so close it now to ensure that the connection is released back to the pool.
249             if self._original_response and not self._original_response.isclosed():
250                 self._original_response.close()
251 
252             # Closing the response may not actually be sufficient to close
253             # everything, so if we have a hold of the connection close that
254             # too.
255             if self._connection is not None:
256                 self._connection.close()
257 
258             raise
259         finally:
260             if self._original_response and self._original_response.isclosed():
261                 self.release_conn()
262 
263     def read(self, amt=None, decode_content=None, cache_content=False):
264         """
265         Similar to :meth:`httplib.HTTPResponse.read`, but with two additional
266         parameters: ``decode_content`` and ``cache_content``.
267 
268         :param amt:
269             How much of the content to read. If specified, caching is skipped
270             because it doesn't make sense to cache partial content as the full
271             response.
272 
273         :param decode_content:
274             If True, will attempt to decode the body based on the
275             'content-encoding' header.
276 
277         :param cache_content:
278             If True, will save the returned data such that the same result is
279             returned despite of the state of the underlying file object. This
280             is useful if you want the ``.data`` property to continue working
281             after having ``.read()`` the file object. (Overridden if ``amt`` is
282             set.)
283         """
284         self._init_decoder()
285         if decode_content is None:
286             decode_content = self.decode_content
287 
288         if self._fp is None:
289             return
290 
291         flush_decoder = False
292         data = None
293 
294         with self._error_catcher():
295             if amt is None:
296                 # cStringIO doesn't like amt=None
297                 data = self._fp.read()
298                 flush_decoder = True
299             else:
300                 cache_content = False
301                 data = self._fp.read(amt)
302                 if amt != 0 and not data:  # Platform-specific: Buggy versions of Python.
303                     # Close the connection when no data is returned
304                     #
305                     # This is redundant to what httplib/http.client _should_
306                     # already do.  However, versions of python released before
307                     # December 15, 2012 (http://bugs.python.org/issue16298) do
308                     # not properly close the connection in all cases. There is
309                     # no harm in redundantly calling close.
310                     self._fp.close()
311                     flush_decoder = True
312 
313         if data:
314             self._fp_bytes_read += len(data)
315 
316             data = self._decode(data, decode_content, flush_decoder)
317 
318             if cache_content:
319                 self._body = data
320 
321         return data
322 
323     def stream(self, amt=2**16, decode_content=None):
324         """
325         A generator wrapper for the read() method. A call will block until
326         ``amt`` bytes have been read from the connection or until the
327         connection is closed.
328 
329         :param amt:
330             How much of the content to read. The generator will return up to
331             much data per iteration, but may return less. This is particularly
332             likely when using compressed data. However, the empty string will
333             never be returned.
334 
335         :param decode_content:
336             If True, will attempt to decode the body based on the
337             'content-encoding' header.
338         """
339         if self.chunked:
340             for line in self.read_chunked(amt, decode_content=decode_content):
341                 yield line
342         else:
343             while not is_fp_closed(self._fp):
344                 data = self.read(amt=amt, decode_content=decode_content)
345 
346                 if data:
347                     yield data
348 
349     @classmethod
350     def from_httplib(ResponseCls, r, **response_kw):
351         """
352         Given an :class:`httplib.HTTPResponse` instance ``r``, return a
353         corresponding :class:`urllib3.response.HTTPResponse` object.
354 
355         Remaining parameters are passed to the HTTPResponse constructor, along
356         with ``original_response=r``.
357         """
358         headers = r.msg
359 
360         if not isinstance(headers, HTTPHeaderDict):
361             if PY3:  # Python 3
362                 headers = HTTPHeaderDict(headers.items())
363             else:  # Python 2
364                 headers = HTTPHeaderDict.from_httplib(headers)
365 
366         # HTTPResponse objects in Python 3 don't have a .strict attribute
367         strict = getattr(r, 'strict', 0)
368         resp = ResponseCls(body=r,
369                            headers=headers,
370                            status=r.status,
371                            version=r.version,
372                            reason=r.reason,
373                            strict=strict,
374                            original_response=r,
375                            **response_kw)
376         return resp
377 
378     # Backwards-compatibility methods for httplib.HTTPResponse
379     def getheaders(self):
380         return self.headers
381 
382     def getheader(self, name, default=None):
383         return self.headers.get(name, default)
384 
385     # Overrides from io.IOBase
386     def close(self):
387         if not self.closed:
388             self._fp.close()
389 
390     @property
391     def closed(self):
392         if self._fp is None:
393             return True
394         elif hasattr(self._fp, 'closed'):
395             return self._fp.closed
396         elif hasattr(self._fp, 'isclosed'):  # Python 2
397             return self._fp.isclosed()
398         else:
399             return True
400 
401     def fileno(self):
402         if self._fp is None:
403             raise IOError("HTTPResponse has no file to get a fileno from")
404         elif hasattr(self._fp, "fileno"):
405             return self._fp.fileno()
406         else:
407             raise IOError("The file-like object this HTTPResponse is wrapped "
408                           "around has no file descriptor")
409 
410     def flush(self):
411         if self._fp is not None and hasattr(self._fp, 'flush'):
412             return self._fp.flush()
413 
414     def readable(self):
415         # This method is required for `io` module compatibility.
416         return True
417 
418     def readinto(self, b):
419         # This method is required for `io` module compatibility.
420         temp = self.read(len(b))
421         if len(temp) == 0:
422             return 0
423         else:
424             b[:len(temp)] = temp
425             return len(temp)
426 
427     def _update_chunk_length(self):
428         # First, we'll figure out length of a chunk and then
429         # we'll try to read it from socket.
430         if self.chunk_left is not None:
431             return
432         line = self._fp.fp.readline()
433         line = line.split(b';', 1)[0]
434         try:
435             self.chunk_left = int(line, 16)
436         except ValueError:
437             # Invalid chunked protocol response, abort.
438             self.close()
439             raise httplib.IncompleteRead(line)
440 
441     def _handle_chunk(self, amt):
442         returned_chunk = None
443         if amt is None:
444             chunk = self._fp._safe_read(self.chunk_left)
445             returned_chunk = chunk
446             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
447             self.chunk_left = None
448         elif amt < self.chunk_left:
449             value = self._fp._safe_read(amt)
450             self.chunk_left = self.chunk_left - amt
451             returned_chunk = value
452         elif amt == self.chunk_left:
453             value = self._fp._safe_read(amt)
454             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
455             self.chunk_left = None
456             returned_chunk = value
457         else:  # amt > self.chunk_left
458             returned_chunk = self._fp._safe_read(self.chunk_left)
459             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
460             self.chunk_left = None
461         return returned_chunk
462 
463     def read_chunked(self, amt=None, decode_content=None):
464         """
465         Similar to :meth:`HTTPResponse.read`, but with an additional
466         parameter: ``decode_content``.
467 
468         :param decode_content:
469             If True, will attempt to decode the body based on the
470             'content-encoding' header.
471         """
472         self._init_decoder()
473         # FIXME: Rewrite this method and make it a class with a better structured logic.
474         if not self.chunked:
475             raise ResponseNotChunked(
476                 "Response is not chunked. "
477                 "Header 'transfer-encoding: chunked' is missing.")
478 
479         # Don't bother reading the body of a HEAD request.
480         if self._original_response and is_response_to_head(self._original_response):
481             self._original_response.close()
482             return
483 
484         with self._error_catcher():
485             while True:
486                 self._update_chunk_length()
487                 if self.chunk_left == 0:
488                     break
489                 chunk = self._handle_chunk(amt)
490                 decoded = self._decode(chunk, decode_content=decode_content,
491                                        flush_decoder=False)
492                 if decoded:
493                     yield decoded
494 
495             if decode_content:
496                 # On CPython and PyPy, we should never need to flush the
497                 # decoder. However, on Jython we *might* need to, so
498                 # lets defensively do it anyway.
499                 decoded = self._flush_decoder()
500                 if decoded:  # Platform-specific: Jython.
501                     yield decoded
502 
503             # Chunk content ends with \r\n: discard it.
504             while True:
505                 line = self._fp.fp.readline()
506                 if not line:
507                     # Some sites may not end with '\r\n'.
508                     break
509                 if line == b'\r\n':
510                     break
511 
512             # We read everything; close the "file".
513             if self._original_response:
514                 self._original_response.close()

</code><intended_behavior>The response handling, especially for 3XX status codes that involve redirects, should ensure that the Location header is correctly interpreted and that any non-ASCII URL is properly encoded before the client is redirected.</intended_behavior>]
2025-10-15 08:18:42.188 | INFO     | Search completed. Bug locations: [<file>requests/models.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.models
5 ~~~~~~~~~~~~~~~
6 
7 This module contains the primary objects that power Requests.
8 """
9 
10 import collections
11 import datetime
12 
13 from io import BytesIO, UnsupportedOperation
14 from .hooks import default_hooks
15 from .structures import CaseInsensitiveDict
16 
17 import requests
18 from .auth import HTTPBasicAuth
19 from .cookies import cookiejar_from_dict, get_cookie_header, _copy_cookie_jar
20 from .packages.urllib3.fields import RequestField
21 from .packages.urllib3.filepost import encode_multipart_formdata
22 from .packages.urllib3.util import parse_url
23 from .packages.urllib3.exceptions import (
24     DecodeError, ReadTimeoutError, ProtocolError, LocationParseError)
25 from .exceptions import (
26     HTTPError, MissingScheme, InvalidURL, ChunkedEncodingError,
27     ContentDecodingError, ConnectionError, StreamConsumedError)
28 from .utils import (
29     guess_filename, get_auth_from_url, requote_uri,
30     stream_decode_response_unicode, to_key_val_list, parse_header_links,
31     iter_slices, guess_json_utf, super_len, to_native_string)
32 from .compat import (
33     cookielib, urlunparse, urlsplit, urlencode, str, bytes, StringIO,
34     is_py2, chardet, builtin_str, basestring)
35 import json as complexjson
36 from .status_codes import codes
37 
38 #: The set of HTTP status codes that indicate an automatically
39 #: processable redirect.
40 REDIRECT_STATI = (
41     codes.moved,              # 301
42     codes.found,              # 302
43     codes.other,              # 303
44     codes.temporary_redirect, # 307
45     codes.permanent_redirect, # 308
46 )
47 
48 DEFAULT_REDIRECT_LIMIT = 30
49 CONTENT_CHUNK_SIZE = 10 * 1024
50 ITER_CHUNK_SIZE = 512
51 
52 
53 class RequestEncodingMixin(object):
54     @property
55     def path_url(self):
56         """Build the path URL to use."""
57 
58         url = []
59 
60         p = urlsplit(self.url)
61 
62         path = p.path
63         if not path:
64             path = '/'
65 
66         url.append(path)
67 
68         query = p.query
69         if query:
70             url.append('?')
71             url.append(query)
72 
73         return ''.join(url)
74 
75     @staticmethod
76     def _encode_params(data):
77         """Encode parameters in a piece of data.
78 
79         Will successfully encode parameters when passed as a dict or a list of
80         2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
81         if parameters are supplied as a dict.
82         """
83 
84         if isinstance(data, (str, bytes)):
85             return data
86         elif hasattr(data, 'read'):
87             return data
88         elif hasattr(data, '__iter__'):
89             result = []
90             for k, vs in to_key_val_list(data):
91                 if isinstance(vs, basestring) or not hasattr(vs, '__iter__'):
92                     vs = [vs]
93                 for v in vs:
94                     if v is not None:
95                         result.append(
96                             (k.encode('utf-8') if isinstance(k, str) else k,
97                              v.encode('utf-8') if isinstance(v, str) else v))
98             return urlencode(result, doseq=True)
99         else:
100             return data
101 
102     @staticmethod
103     def _encode_files(files, data):
104         """Build the body for a multipart/form-data request.
105 
106         Will successfully encode files when passed as a dict or a list of
107         2-tuples. Order is retained if data is a list of 2-tuples but arbitrary
108         if parameters are supplied as a dict.
109 
110         """
111         if (not files):
112             raise ValueError("Files must be provided.")
113         elif isinstance(data, basestring):
114             raise ValueError("Data must not be a string.")
115 
116         new_fields = []
117         fields = to_key_val_list(data or {})
118         files = to_key_val_list(files or {})
119 
120         for field, val in fields:
121             if isinstance(val, basestring) or not hasattr(val, '__iter__'):
122                 val = [val]
123             for v in val:
124                 if v is not None:
125                     # Don't call str() on bytestrings: in Py3 it all goes wrong.
126                     if not isinstance(v, bytes):
127                         v = str(v)
128 
129                     new_fields.append(
130                         (field.decode('utf-8') if isinstance(field, bytes) else field,
131                          v.encode('utf-8') if isinstance(v, str) else v))
132 
133         for (k, v) in files:
134             # support for explicit filename
135             ft = None
136             fh = None
137             if isinstance(v, (tuple, list)):
138                 if len(v) == 2:
139                     fn, fp = v
140                 elif len(v) == 3:
141                     fn, fp, ft = v
142                 else:
143                     fn, fp, ft, fh = v
144             else:
145                 fn = guess_filename(v) or k
146                 fp = v
147 
148             if isinstance(fp, (str, bytes, bytearray)):
149                 fdata = fp
150             else:
151                 fdata = fp.read()
152 
153             rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)
154             rf.make_multipart(content_type=ft)
155             new_fields.append(rf)
156 
157         body, content_type = encode_multipart_formdata(new_fields)
158 
159         return body, content_type
160 
161 
162 class RequestHooksMixin(object):
163     def register_hook(self, event, hook):
164         """Properly register a hook."""
165 
166         if event not in self.hooks:
167             raise ValueError('Unsupported event specified, with event name "%s"' % (event))
168 
169         if isinstance(hook, collections.Callable):
170             self.hooks[event].append(hook)
171         elif hasattr(hook, '__iter__'):
172             self.hooks[event].extend(h for h in hook if isinstance(h, collections.Callable))
173 
174     def deregister_hook(self, event, hook):
175         """Deregister a previously registered hook.
176         Returns True if the hook existed, False if not.
177         """
178 
179         try:
180             self.hooks[event].remove(hook)
181             return True
182         except ValueError:
183             return False
184 
185 
186 class Request(RequestHooksMixin):
187     """A user-created :class:`Request <Request>` object.
188 
189     Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.
190 
191     :param method: HTTP method to use.
192     :param url: URL to send.
193     :param headers: dictionary of headers to send.
194     :param files: dictionary of {filename: fileobject} files to multipart upload.
195     :param data: the body to attach to the request. If a dictionary is provided, form-encoding will take place.
196     :param json: json for the body to attach to the request (if files or data is not specified).
197     :param params: dictionary of URL parameters to append to the URL.
198     :param auth: Auth handler or (user, pass) tuple.
199     :param cookies: dictionary or CookieJar of cookies to attach to this request.
200     :param hooks: dictionary of callback hooks, for internal usage.
201 
202     Usage::
203 
204       >>> import requests
205       >>> req = requests.Request('GET', 'http://httpbin.org/get')
206       >>> req.prepare()
207       <PreparedRequest [GET]>
208 
209     """
210     def __init__(self, method=None, url=None, headers=None, files=None,
211         data=None, params=None, auth=None, cookies=None, hooks=None, json=None):
212 
213         # Default empty dicts for dict params.
214         data = [] if data is None else data
215         files = [] if files is None else files
216         headers = {} if headers is None else headers
217         params = {} if params is None else params
218         hooks = {} if hooks is None else hooks
219 
220         self.hooks = default_hooks()
221         for (k, v) in list(hooks.items()):
222             self.register_hook(event=k, hook=v)
223 
224         self.method = method
225         self.url = url
226         self.headers = headers
227         self.files = files
228         self.data = data
229         self.json = json
230         self.params = params
231         self.auth = auth
232         self.cookies = cookies
233 
234     def __repr__(self):
235         return '<Request [%s]>' % (self.method)
236 
237     def prepare(self):
238         """Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it."""
239         p = PreparedRequest()
240         p.prepare(
241             method=self.method,
242             url=self.url,
243             headers=self.headers,
244             files=self.files,
245             data=self.data,
246             json=self.json,
247             params=self.params,
248             auth=self.auth,
249             cookies=self.cookies,
250             hooks=self.hooks,
251         )
252         return p
253 
254 
255 class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):
256     """The fully mutable :class:`PreparedRequest <PreparedRequest>` object,
257     containing the exact bytes that will be sent to the server.
258 
259     Generated from either a :class:`Request <Request>` object or manually.
260 
261     Usage::
262 
263       >>> import requests
264       >>> req = requests.Request('GET', 'http://httpbin.org/get')
265       >>> r = req.prepare()
266       <PreparedRequest [GET]>
267 
268       >>> s = requests.Session()
269       >>> s.send(r)
270       <Response [200]>
271 
272     """
273 
274     def __init__(self):
275         #: HTTP verb to send to the server.
276         self.method = None
277         #: HTTP URL to send the request to.
278         self.url = None
279         #: dictionary of HTTP headers.
280         self.headers = None
281         # The `CookieJar` used to create the Cookie header will be stored here
282         # after prepare_cookies is called
283         self._cookies = None
284         #: request body to send to the server.
285         self.body = None
286         #: dictionary of callback hooks, for internal usage.
287         self.hooks = default_hooks()
288 
289     def prepare(self, method=None, url=None, headers=None, files=None,
290         data=None, params=None, auth=None, cookies=None, hooks=None, json=None):
291         """Prepares the entire request with the given parameters."""
292 
293         self.prepare_method(method)
294         self.prepare_url(url, params)
295         self.prepare_headers(headers)
296         self.prepare_cookies(cookies)
297         self.prepare_body(data, files, json)
298         self.prepare_auth(auth, url)
299 
300         # Note that prepare_auth must be last to enable authentication schemes
301         # such as OAuth to work on a fully prepared request.
302 
303         # This MUST go after prepare_auth. Authenticators could add a hook
304         self.prepare_hooks(hooks)
305 
306     def __repr__(self):
307         return '<PreparedRequest [%s]>' % (self.method)
308 
309     def copy(self):
310         p = PreparedRequest()
311         p.method = self.method
312         p.url = self.url
313         p.headers = self.headers.copy() if self.headers is not None else None
314         p._cookies = _copy_cookie_jar(self._cookies)
315         p.body = self.body
316         p.hooks = self.hooks
317         return p
318 
319     def prepare_method(self, method):
320         """Prepares the given HTTP method."""
321         self.method = method
322         if self.method is None:
323             raise ValueError('Request method cannot be "None"')
324         self.method = to_native_string(self.method).upper()
325 
326     def prepare_url(self, url, params):
327         """Prepares the given HTTP URL."""
328         #: Accept objects that have string representations.
329         #: We're unable to blindly call unicode/str functions
330         #: as this will include the bytestring indicator (b'')
331         #: on python 3.x.
332         #: https://github.com/kennethreitz/requests/pull/2238
333         if isinstance(url, bytes):
334             url = url.decode('utf8')
335         else:
336             url = unicode(url) if is_py2 else str(url)
337 
338         # Ignore any leading and trailing whitespace characters.
339         url = url.strip()
340 
341         # Don't do any URL preparation for non-HTTP schemes like `mailto`,
342         # `data` etc to work around exceptions from `url_parse`, which
343         # handles RFC 3986 only.
344         if ':' in url and not url.lower().startswith('http'):
345             self.url = url
346             return
347 
348         # Support for unicode domain names and paths.
349         try:
350             scheme, auth, host, port, path, query, fragment = parse_url(url)
351         except LocationParseError as e:
352             raise InvalidURL(*e.args)
353 
354         if not scheme:
355             error = ("Invalid URL {0!r}: No scheme supplied. Perhaps you meant http://{0}?")
356             error = error.format(to_native_string(url, 'utf8'))
357 
358             raise MissingScheme(error)
359 
360         if not host:
361             raise InvalidURL("Invalid URL %r: No host supplied" % url)
362 
363         # Only want to apply IDNA to the hostname
364         try:
365             host = host.encode('idna').decode('utf-8')
366         except UnicodeError:
367             raise InvalidURL('URL has an invalid label.')
368 
369         # Carefully reconstruct the network location
370         netloc = auth or ''
371         if netloc:
372             netloc += '@'
373         netloc += host
374         if port:
375             netloc += ':' + str(port)
376 
377         # Bare domains aren't valid URLs.
378         if not path:
379             path = '/'
380 
381         if is_py2:
382             if isinstance(scheme, str):
383                 scheme = scheme.encode('utf-8')
384             if isinstance(netloc, str):
385                 netloc = netloc.encode('utf-8')
386             if isinstance(path, str):
387                 path = path.encode('utf-8')
388             if isinstance(query, str):
389                 query = query.encode('utf-8')
390             if isinstance(fragment, str):
391                 fragment = fragment.encode('utf-8')
392 
393         if isinstance(params, (str, bytes)):
394             params = to_native_string(params)
395 
396         enc_params = self._encode_params(params)
397         if enc_params:
398             if query:
399                 query = '%s&%s' % (query, enc_params)
400             else:
401                 query = enc_params
402 
403         url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))
404         self.url = url
405 
406     def prepare_headers(self, headers):
407         """Prepares the given HTTP headers."""
408 
409         if headers:
410             self.headers = CaseInsensitiveDict((to_native_string(name), value) for name, value in headers.items())
411         else:
412             self.headers = CaseInsensitiveDict()
413 
414     def prepare_body(self, data, files, json=None):
415         """Prepares the given HTTP body data."""
416 
417         # Check if file, fo, generator, iterator.
418         # If not, run through normal process.
419 
420         # Nottin' on you.
421         body = None
422         content_type = None
423         length = None
424 
425         if not data and json is not None:
426             content_type = 'application/json'
427             body = complexjson.dumps(json)
428 
429         is_stream = all([
430             hasattr(data, '__iter__'),
431             not isinstance(data, (basestring, list, tuple, dict))
432         ])
433 
434         try:
435             length = super_len(data)
436         except (TypeError, AttributeError, UnsupportedOperation):
437             length = None
438 
439         if is_stream:
440             body = data
441 
442             if files:
443                 raise NotImplementedError('Streamed bodies and files are mutually exclusive.')
444 
445             if length:
446                 self.headers['Content-Length'] = builtin_str(length)
447             else:
448                 self.headers['Transfer-Encoding'] = 'chunked'
449         else:
450             # Multi-part file uploads.
451             if files:
452                 (body, content_type) = self._encode_files(files, data)
453             else:
454                 if data:
455                     body = self._encode_params(data)
456                     if isinstance(data, basestring) or hasattr(data, 'read'):
457                         content_type = None
458                     else:
459                         content_type = 'application/x-www-form-urlencoded'
460 
461             self.prepare_content_length(body)
462 
463             # Add content-type if it wasn't explicitly provided.
464             if content_type and ('content-type' not in self.headers):
465                 self.headers['Content-Type'] = content_type
466 
467         self.body = body
468 
469     def prepare_content_length(self, body):
470         if hasattr(body, 'seek') and hasattr(body, 'tell'):
471             body.seek(0, 2)
472             self.headers['Content-Length'] = builtin_str(body.tell())
473             body.seek(0, 0)
474         elif body is not None:
475             l = super_len(body)
476             if l:
477                 self.headers['Content-Length'] = builtin_str(l)
478         elif (self.method not in ('GET', 'HEAD')) and (self.headers.get('Content-Length') is None):
479             self.headers['Content-Length'] = '0'
480 
481     def prepare_auth(self, auth, url=''):
482         """Prepares the given HTTP auth data."""
483 
484         # If no Auth is explicitly provided, extract it from the URL first.
485         if auth is None:
486             url_auth = get_auth_from_url(self.url)
487             auth = url_auth if any(url_auth) else None
488 
489         if auth:
490             if isinstance(auth, tuple) and len(auth) == 2:
491                 # special-case basic HTTP auth
492                 auth = HTTPBasicAuth(*auth)
493 
494             # Allow auth to make its changes.
495             r = auth(self)
496 
497             # Update self to reflect the auth changes.
498             self.__dict__.update(r.__dict__)
499 
500             # Recompute Content-Length
501             self.prepare_content_length(self.body)
502 
503     def prepare_cookies(self, cookies):
504         """Prepares the given HTTP cookie data.
505 
506         This function eventually generates a ``Cookie`` header from the
507         given cookies using cookielib. Due to cookielib's design, the header
508         will not be regenerated if it already exists, meaning this function
509         can only be called once for the life of the
510         :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls
511         to ``prepare_cookies`` will have no actual effect, unless the "Cookie"
512         header is removed beforehand."""
513 
514         if isinstance(cookies, cookielib.CookieJar):
515             self._cookies = cookies
516         else:
517             self._cookies = cookiejar_from_dict(cookies)
518 
519         cookie_header = get_cookie_header(self._cookies, self)
520         if cookie_header is not None:
521             self.headers['Cookie'] = cookie_header
522 
523     def prepare_hooks(self, hooks):
524         """Prepares the given hooks."""
525         # hooks can be passed as None to the prepare method and to this
526         # method. To prevent iterating over None, simply use an empty list
527         # if hooks is False-y
528         hooks = hooks or []
529         for event in hooks:
530             self.register_hook(event, hooks[event])
531 
532     def send(self, session=None, **send_kwargs):
533         """Sends the PreparedRequest to the given Session.
534         If none is provided, one is created for you."""
535         session = requests.Session() if session is None else session
536 
537         with session:
538             return session.send(self, **send_kwargs)
539 
540 
541 class Response(object):
542     """The :class:`Response <Response>` object, which contains a
543     server's response to an HTTP request.
544     """
545 
546     __attrs__ = [
547         '_content', 'status_code', 'headers', 'url', 'history',
548         'encoding', 'reason', 'cookies', 'elapsed', 'request'
549     ]
550 
551     def __init__(self):
552         super(Response, self).__init__()
553 
554         self._content = False
555         self._content_consumed = False
556 
557         #: Integer Code of responded HTTP Status, e.g. 404 or 200.
558         self.status_code = None
559 
560         #: Case-insensitive Dictionary of Response Headers.
561         #: For example, ``headers['content-encoding']`` will return the
562         #: value of a ``'Content-Encoding'`` response header.
563         self.headers = CaseInsensitiveDict()
564 
565         #: File-like object representation of response (for advanced usage).
566         #: Use of ``raw`` requires that ``stream=True`` be set on the request.
567         # This requirement does not apply for use internally to Requests.
568         self.raw = None
569 
570         #: Final URL location of Response.
571         self.url = None
572 
573         #: Encoding to decode with when accessing r.text.
574         self.encoding = None
575 
576         #: A list of :class:`Response <Response>` objects from
577         #: the history of the Request. Any redirect responses will end
578         #: up here. The list is sorted from the oldest to the most recent request.
579         self.history = []
580 
581         #: Textual reason of responded HTTP Status, e.g. "Not Found" or "OK".
582         self.reason = None
583 
584         #: A CookieJar of Cookies the server sent back.
585         self.cookies = cookiejar_from_dict({})
586 
587         #: The amount of time elapsed between sending the request
588         #: and the arrival of the response (as a timedelta).
589         #: This property specifically measures the time taken between sending
590         #: the first byte of the request and finishing parsing the headers. It
591         #: is therefore unaffected by consuming the response content or the
592         #: value of the ``stream`` keyword argument.
593         self.elapsed = datetime.timedelta(0)
594 
595         #: The :class:`PreparedRequest <PreparedRequest>` object to which this
596         #: is a response.
597         self.request = None
598 
599     def __getstate__(self):
600         # Consume everything; accessing the content attribute makes
601         # sure the content has been fully read.
602         if not self._content_consumed:
603             self.content
604 
605         return dict(
606             (attr, getattr(self, attr, None))
607             for attr in self.__attrs__
608         )
609 
610     def __setstate__(self, state):
611         for name, value in state.items():
612             setattr(self, name, value)
613 
614         # pickled objects do not have .raw
615         setattr(self, '_content_consumed', True)
616         setattr(self, 'raw', None)
617 
618     def __repr__(self):
619         return '<Response [%s]>' % (self.status_code)
620 
621     def __iter__(self):
622         """Allows you to use a response as an iterator."""
623         return self.iter_content(128)
624 
625     @property
626     def ok(self):
627         try:
628             self.raise_for_status()
629         except HTTPError:
630             return False
631         return True
632 
633     @property
634     def is_redirect(self):
635         """True if this Response is a well-formed HTTP redirect that could have
636         been processed automatically (by :meth:`Session.resolve_redirects`).
637         """
638         return ('location' in self.headers and self.status_code in REDIRECT_STATI)
639 
640     @property
641     def is_permanent_redirect(self):
642         """True if this Response one of the permanent versions of redirect"""
643         return ('location' in self.headers and self.status_code in (codes.moved_permanently, codes.permanent_redirect))
644 
645     @property
646     def apparent_encoding(self):
647         """The apparent encoding, provided by the chardet library"""
648         return chardet.detect(self.content)['encoding']
649 
650     def iter_content(self, chunk_size=1, decode_unicode=False):
651         """Iterates over the response data.  When stream=True is set on the
652         request, this avoids reading the content at once into memory for
653         large responses.  The chunk size is the number of bytes it should
654         read into memory.  This is not necessarily the length of each item
655         returned as decoding can take place.
656 
657         If decode_unicode is True, content will be decoded using the best
658         available encoding based on the response.
659         """
660 
661         def generate():
662             # Special case for urllib3.
663             if hasattr(self.raw, 'stream'):
664                 try:
665                     for chunk in self.raw.stream(chunk_size, decode_content=True):
666                         yield chunk
667                 except ProtocolError as e:
668                     raise ChunkedEncodingError(e)
669                 except DecodeError as e:
670                     raise ContentDecodingError(e)
671                 except ReadTimeoutError as e:
672                     raise ConnectionError(e)
673             else:
674                 # Standard file-like object.
675                 while True:
676                     chunk = self.raw.read(chunk_size)
677                     if not chunk:
678                         break
679                     yield chunk
680 
681             self._content_consumed = True
682 
683         if self._content_consumed and isinstance(self._content, bool):
684             raise StreamConsumedError()
685         # simulate reading small chunks of the content
686         reused_chunks = iter_slices(self._content, chunk_size)
687 
688         stream_chunks = generate()
689 
690         chunks = reused_chunks if self._content_consumed else stream_chunks
691 
692         if decode_unicode:
693             chunks = stream_decode_response_unicode(chunks, self)
694 
695         return chunks
696 
697     def iter_lines(self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=None, delimiter=None):
698         """Iterates over the response data, one line at a time.  When
699         stream=True is set on the request, this avoids reading the
700         content at once into memory for large responses.
701 
702         .. note:: This method is not reentrant safe.
703         """
704 
705         pending = None
706 
707         for chunk in self.iter_content(chunk_size=chunk_size, decode_unicode=decode_unicode):
708 
709             if pending is not None:
710                 chunk = pending + chunk
711 
712             if delimiter:
713                 lines = chunk.split(delimiter)
714             else:
715                 lines = chunk.splitlines()
716 
717             if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:
718                 pending = lines.pop()
719             else:
720                 pending = None
721 
722             for line in lines:
723                 yield line
724 
725         if pending is not None:
726             yield pending
727 
728     @property
729     def content(self):
730         """Content of the response, in bytes."""
731 
732         if self._content is False:
733             # Read the contents.
734             try:
735                 if self._content_consumed:
736                     raise RuntimeError(
737                         'The content for this response was already consumed')
738 
739                 if self.status_code == 0:
740                     self._content = None
741                 else:
742                     self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()
743 
744             except AttributeError:
745                 self._content = None
746 
747         self._content_consumed = True
748         # don't need to release the connection; that's been handled by urllib3
749         # since we exhausted the data.
750         return self._content
751 
752     @property
753     def text(self):
754         """Content of the response, in unicode.
755 
756         If Response.encoding is None, encoding will be guessed using
757         ``chardet``.
758 
759         The encoding of the response content is determined based solely on HTTP
760         headers, following RFC 2616 to the letter. If you can take advantage of
761         non-HTTP knowledge to make a better guess at the encoding, you should
762         set ``r.encoding`` appropriately before accessing this property.
763         """
764 
765         # Try charset from content-type
766         content = None
767         encoding = self.encoding
768 
769         if not self.content:
770             return str('')
771 
772         # Fallback to auto-detected encoding.
773         if self.encoding is None:
774             encoding = self.apparent_encoding
775 
776         # Decode unicode from given encoding.
777         try:
778             content = str(self.content, encoding, errors='replace')
779         except (LookupError, TypeError):
780             # A LookupError is raised if the encoding was not found which could
781             # indicate a misspelling or similar mistake.
782             #
783             # A TypeError can be raised if encoding is None
784             #
785             # So we try blindly encoding.
786             content = str(self.content, errors='replace')
787 
788         return content
789 
790     def json(self, **kwargs):
791         """Returns the json-encoded content of a response, if any.
792 
793         :param \*\*kwargs: Optional arguments that ``json.loads`` takes.
794         """
795 
796         if not self.encoding and len(self.content) > 3:
797             # No encoding set. JSON RFC 4627 section 3 states we should expect
798             # UTF-8, -16 or -32. Detect which one to use; If the detection or
799             # decoding fails, fall back to `self.text` (using chardet to make
800             # a best guess).
801             encoding = guess_json_utf(self.content)
802             if encoding is not None:
803                 try:
804                     return complexjson.loads(
805                         self.content.decode(encoding), **kwargs
806                     )
807                 except UnicodeDecodeError:
808                     # Wrong UTF codec detected; usually because it's not UTF-8
809                     # but some other 8-bit codec.  This is an RFC violation,
810                     # and the server didn't bother to tell us what codec *was*
811                     # used.
812                     pass
813         return complexjson.loads(self.text, **kwargs)
814 
815     @property
816     def links(self):
817         """Returns the parsed header links of the response, if any."""
818 
819         header = self.headers.get('link')
820 
821         # l = MultiDict()
822         l = {}
823 
824         if header:
825             links = parse_header_links(header)
826 
827             for link in links:
828                 key = link.get('rel') or link.get('url')
829                 l[key] = link
830 
831         return l
832 
833     def raise_for_status(self):
834         """Raises stored :class:`HTTPError`, if one occurred."""
835 
836         http_error_msg = ''
837 
838         if 400 <= self.status_code < 500:
839             http_error_msg = '%s Client Error: %s for url: %s' % (self.status_code, self.reason, self.url)
840 
841         elif 500 <= self.status_code < 600:
842             http_error_msg = '%s Server Error: %s for url: %s' % (self.status_code, self.reason, self.url)
843 
844         if http_error_msg:
845             raise HTTPError(http_error_msg, response=self)
846 
847     def close(self):
848         """Releases the connection back to the pool. Once this method has been
849         called the underlying ``raw`` object must not be accessed again.
850 
851         *Note: Should not normally need to be called explicitly.*
852         """
853         if not self._content_consumed:
854             return self.raw.close()
855 
856         return self.raw.release_conn()

</code><intended_behavior>The code should ensure that when a redirect URL is processed, non-ASCII characters in the URL are correctly percent-encoded according to RFC 3986.</intended_behavior>, <file>requests/utils.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.utils
5 ~~~~~~~~~~~~~~
6 
7 This module provides utility functions that are used within Requests
8 that are also useful for external consumption.
9 
10 """
11 
12 import cgi
13 import codecs
14 import collections
15 import io
16 import os
17 import platform
18 import re
19 import sys
20 import socket
21 import struct
22 import warnings
23 
24 from . import __version__
25 from . import certs
26 from .compat import parse_http_list as _parse_list_header
27 from .compat import (quote, urlparse, bytes, str, OrderedDict, unquote, is_py2,
28                      builtin_str, getproxies, proxy_bypass, urlunparse,
29                      basestring)
30 from .cookies import RequestsCookieJar, cookiejar_from_dict
31 from .structures import CaseInsensitiveDict
32 from .exceptions import InvalidURL, FileModeWarning
33 
34 _hush_pyflakes = (RequestsCookieJar,)
35 
36 NETRC_FILES = ('.netrc', '_netrc')
37 
38 DEFAULT_CA_BUNDLE_PATH = certs.where()
39 
40 
41 def dict_to_sequence(d):
42     """Returns an internal sequence dictionary update."""
43 
44     if hasattr(d, 'items'):
45         d = d.items()
46 
47     return d
48 
49 
50 def super_len(o):
51     total_length = 0
52     current_position = 0
53 
54     if hasattr(o, '__len__'):
55         total_length = len(o)
56 
57     elif hasattr(o, 'len'):
58         total_length = o.len
59 
60     elif hasattr(o, 'getvalue'):
61         # e.g. BytesIO, cStringIO.StringIO
62         total_length = len(o.getvalue())
63 
64     elif hasattr(o, 'fileno'):
65         try:
66             fileno = o.fileno()
67         except io.UnsupportedOperation:
68             pass
69         else:
70             total_length = os.fstat(fileno).st_size
71 
72             # Having used fstat to determine the file length, we need to
73             # confirm that this file was opened up in binary mode.
74             if 'b' not in o.mode:
75                 warnings.warn((
76                     "Requests has determined the content-length for this "
77                     "request using the binary size of the file: however, the "
78                     "file has been opened in text mode (i.e. without the 'b' "
79                     "flag in the mode). This may lead to an incorrect "
80                     "content-length. In Requests 3.0, support will be removed "
81                     "for files in text mode."),
82                     FileModeWarning
83                 )
84 
85     if hasattr(o, 'tell'):
86         current_position = o.tell()
87 
88     return max(0, total_length - current_position)
89 
90 
91 def get_netrc_auth(url, raise_errors=False):
92     """Returns the Requests tuple auth for a given url from netrc."""
93 
94     try:
95         from netrc import netrc, NetrcParseError
96 
97         netrc_path = None
98 
99         for f in NETRC_FILES:
100             try:
101                 loc = os.path.expanduser('~/{0}'.format(f))
102             except KeyError:
103                 # os.path.expanduser can fail when $HOME is undefined and
104                 # getpwuid fails. See http://bugs.python.org/issue20164 &
105                 # https://github.com/kennethreitz/requests/issues/1846
106                 return
107 
108             if os.path.exists(loc):
109                 netrc_path = loc
110                 break
111 
112         # Abort early if there isn't one.
113         if netrc_path is None:
114             return
115 
116         ri = urlparse(url)
117 
118         # Strip port numbers from netloc. This weird `if...encode`` dance is
119         # used for Python 3.2, which doesn't support unicode literals.
120         splitstr = b':'
121         if isinstance(url, str):
122             splitstr = splitstr.decode('ascii')
123         host = ri.netloc.split(splitstr)[0]
124 
125         try:
126             _netrc = netrc(netrc_path).authenticators(host)
127             if _netrc:
128                 # Return with login / password
129                 login_i = (0 if _netrc[0] else 1)
130                 return (_netrc[login_i], _netrc[2])
131         except (NetrcParseError, IOError):
132             # If there was a parsing error or a permissions issue reading the file,
133             # we'll just skip netrc auth unless explicitly asked to raise errors.
134             if raise_errors:
135                 raise
136 
137     # AppEngine hackiness.
138     except (ImportError, AttributeError):
139         pass
140 
141 
142 def guess_filename(obj):
143     """Tries to guess the filename of the given object."""
144     name = getattr(obj, 'name', None)
145     if (name and isinstance(name, basestring) and name[0] != '<' and
146             name[-1] != '>'):
147         return os.path.basename(name)
148 
149 
150 def from_key_val_list(value):
151     """Take an object and test to see if it can be represented as a
152     dictionary. Unless it can not be represented as such, return an
153     OrderedDict, e.g.,
154 
155     ::
156 
157         >>> from_key_val_list([('key', 'val')])
158         OrderedDict([('key', 'val')])
159         >>> from_key_val_list('string')
160         ValueError: need more than 1 value to unpack
161         >>> from_key_val_list({'key': 'val'})
162         OrderedDict([('key', 'val')])
163     """
164     if value is None:
165         return None
166 
167     if isinstance(value, (str, bytes, bool, int)):
168         raise ValueError('cannot encode objects that are not 2-tuples')
169 
170     return OrderedDict(value)
171 
172 
173 def to_key_val_list(value):
174     """Take an object and test to see if it can be represented as a
175     dictionary. If it can be, return a list of tuples, e.g.,
176 
177     ::
178 
179         >>> to_key_val_list([('key', 'val')])
180         [('key', 'val')]
181         >>> to_key_val_list({'key': 'val'})
182         [('key', 'val')]
183         >>> to_key_val_list('string')
184         ValueError: cannot encode objects that are not 2-tuples.
185     """
186     if value is None:
187         return None
188 
189     if isinstance(value, (str, bytes, bool, int)):
190         raise ValueError('cannot encode objects that are not 2-tuples')
191 
192     if isinstance(value, collections.Mapping):
193         value = value.items()
194 
195     return list(value)
196 
197 
198 # From mitsuhiko/werkzeug (used with permission).
199 def parse_list_header(value):
200     """Parse lists as described by RFC 2068 Section 2.
201 
202     In particular, parse comma-separated lists where the elements of
203     the list may include quoted-strings.  A quoted-string could
204     contain a comma.  A non-quoted string could have quotes in the
205     middle.  Quotes are removed automatically after parsing.
206 
207     It basically works like :func:`parse_set_header` just that items
208     may appear multiple times and case sensitivity is preserved.
209 
210     The return value is a standard :class:`list`:
211 
212     >>> parse_list_header('token, "quoted value"')
213     ['token', 'quoted value']
214 
215     To create a header from the :class:`list` again, use the
216     :func:`dump_header` function.
217 
218     :param value: a string with a list header.
219     :return: :class:`list`
220     """
221     result = []
222     for item in _parse_list_header(value):
223         if item[:1] == item[-1:] == '"':
224             item = unquote_header_value(item[1:-1])
225         result.append(item)
226     return result
227 
228 
229 # From mitsuhiko/werkzeug (used with permission).
230 def parse_dict_header(value):
231     """Parse lists of key, value pairs as described by RFC 2068 Section 2 and
232     convert them into a python dict:
233 
234     >>> d = parse_dict_header('foo="is a fish", bar="as well"')
235     >>> type(d) is dict
236     True
237     >>> sorted(d.items())
238     [('bar', 'as well'), ('foo', 'is a fish')]
239 
240     If there is no value for a key it will be `None`:
241 
242     >>> parse_dict_header('key_without_value')
243     {'key_without_value': None}
244 
245     To create a header from the :class:`dict` again, use the
246     :func:`dump_header` function.
247 
248     :param value: a string with a dict header.
249     :return: :class:`dict`
250     """
251     result = {}
252     for item in _parse_list_header(value):
253         if '=' not in item:
254             result[item] = None
255             continue
256         name, value = item.split('=', 1)
257         if value[:1] == value[-1:] == '"':
258             value = unquote_header_value(value[1:-1])
259         result[name] = value
260     return result
261 
262 
263 # From mitsuhiko/werkzeug (used with permission).
264 def unquote_header_value(value, is_filename=False):
265     r"""Unquotes a header value.  (Reversal of :func:`quote_header_value`).
266     This does not use the real unquoting but what browsers are actually
267     using for quoting.
268 
269     :param value: the header value to unquote.
270     """
271     if value and value[0] == value[-1] == '"':
272         # this is not the real unquoting, but fixing this so that the
273         # RFC is met will result in bugs with internet explorer and
274         # probably some other browsers as well.  IE for example is
275         # uploading files with "C:\foo\bar.txt" as filename
276         value = value[1:-1]
277 
278         # if this is a filename and the starting characters look like
279         # a UNC path, then just return the value without quotes.  Using the
280         # replace sequence below on a UNC path has the effect of turning
281         # the leading double slash into a single slash and then
282         # _fix_ie_filename() doesn't work correctly.  See #458.
283         if not is_filename or value[:2] != '\\\\':
284             return value.replace('\\\\', '\\').replace('\\"', '"')
285     return value
286 
287 
288 def dict_from_cookiejar(cj):
289     """Returns a key/value dictionary from a CookieJar.
290 
291     :param cj: CookieJar object to extract cookies from.
292     """
293 
294     cookie_dict = {}
295 
296     for cookie in cj:
297         cookie_dict[cookie.name] = cookie.value
298 
299     return cookie_dict
300 
301 
302 def add_dict_to_cookiejar(cj, cookie_dict):
303     """Returns a CookieJar from a key/value dictionary.
304 
305     :param cj: CookieJar to insert cookies into.
306     :param cookie_dict: Dict of key/values to insert into CookieJar.
307     """
308 
309     cj2 = cookiejar_from_dict(cookie_dict)
310     cj.update(cj2)
311     return cj
312 
313 
314 def get_encodings_from_content(content):
315     """Returns encodings from given content string.
316 
317     :param content: bytestring to extract encodings from.
318     """
319     warnings.warn((
320         'In requests 3.0, get_encodings_from_content will be removed. For '
321         'more information, please see the discussion on issue #2266. (This'
322         ' warning should only appear once.)'),
323         DeprecationWarning)
324 
325     charset_re = re.compile(r'<meta.*?charset=["\']*(.+?)["\'>]', flags=re.I)
326     pragma_re = re.compile(r'<meta.*?content=["\']*;?charset=(.+?)["\'>]', flags=re.I)
327     xml_re = re.compile(r'^<\?xml.*?encoding=["\']*(.+?)["\'>]')
328 
329     return (charset_re.findall(content) +
330             pragma_re.findall(content) +
331             xml_re.findall(content))
332 
333 
334 def get_encoding_from_headers(headers):
335     """Returns encodings from given HTTP Header Dict.
336 
337     :param headers: dictionary to extract encoding from.
338     """
339 
340     content_type = headers.get('content-type')
341 
342     if not content_type:
343         return None
344 
345     content_type, params = cgi.parse_header(content_type)
346 
347     if 'charset' in params:
348         return params['charset'].strip("'\"")
349 
350     if 'text' in content_type:
351         return 'ISO-8859-1'
352 
353 
354 def stream_decode_response_unicode(iterator, r):
355     """Stream decodes a iterator."""
356 
357     if r.encoding is None:
358         for item in iterator:
359             yield item
360         return
361 
362     decoder = codecs.getincrementaldecoder(r.encoding)(errors='replace')
363     for chunk in iterator:
364         rv = decoder.decode(chunk)
365         if rv:
366             yield rv
367     rv = decoder.decode(b'', final=True)
368     if rv:
369         yield rv
370 
371 
372 def iter_slices(string, slice_length):
373     """Iterate over slices of a string."""
374     pos = 0
375     while pos < len(string):
376         yield string[pos:pos + slice_length]
377         pos += slice_length
378 
379 
380 def get_unicode_from_response(r):
381     """Returns the requested content back in unicode.
382 
383     :param r: Response object to get unicode content from.
384 
385     Tried:
386 
387     1. charset from content-type
388     2. fall back and replace all unicode characters
389 
390     """
391     warnings.warn((
392         'In requests 3.0, get_unicode_from_response will be removed. For '
393         'more information, please see the discussion on issue #2266. (This'
394         ' warning should only appear once.)'),
395         DeprecationWarning)
396 
397     tried_encodings = []
398 
399     # Try charset from content-type
400     encoding = get_encoding_from_headers(r.headers)
401 
402     if encoding:
403         try:
404             return str(r.content, encoding)
405         except UnicodeError:
406             tried_encodings.append(encoding)
407 
408     # Fall back:
409     try:
410         return str(r.content, encoding, errors='replace')
411     except TypeError:
412         return r.content
413 
414 
415 # The unreserved URI characters (RFC 3986)
416 UNRESERVED_SET = frozenset(
417     "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
418     + "0123456789-._~")
419 
420 
421 def unquote_unreserved(uri):
422     """Un-escape any percent-escape sequences in a URI that are unreserved
423     characters. This leaves all reserved, illegal and non-ASCII bytes encoded.
424     """
425     parts = uri.split('%')
426     for i in range(1, len(parts)):
427         h = parts[i][0:2]
428         if len(h) == 2 and h.isalnum():
429             try:
430                 c = chr(int(h, 16))
431             except ValueError:
432                 raise InvalidURL("Invalid percent-escape sequence: '%s'" % h)
433 
434             if c in UNRESERVED_SET:
435                 parts[i] = c + parts[i][2:]
436             else:
437                 parts[i] = '%' + parts[i]
438         else:
439             parts[i] = '%' + parts[i]
440     return ''.join(parts)
441 
442 
443 def requote_uri(uri):
444     """Re-quote the given URI.
445 
446     This function passes the given URI through an unquote/quote cycle to
447     ensure that it is fully and consistently quoted.
448     """
449     safe_with_percent = "!#$%&'()*+,/:;=?@[]~"
450     safe_without_percent = "!#$&'()*+,/:;=?@[]~"
451     try:
452         # Unquote only the unreserved characters
453         # Then quote only illegal characters (do not quote reserved,
454         # unreserved, or '%')
455         return quote(unquote_unreserved(uri), safe=safe_with_percent)
456     except InvalidURL:
457         # We couldn't unquote the given URI, so let's try quoting it, but
458         # there may be unquoted '%'s in the URI. We need to make sure they're
459         # properly quoted so they do not cause issues elsewhere.
460         return quote(uri, safe=safe_without_percent)
461 
462 
463 def address_in_network(ip, net):
464     """
465     This function allows you to check if on IP belongs to a network subnet
466     Example: returns True if ip = 192.168.1.1 and net = 192.168.1.0/24
467              returns False if ip = 192.168.1.1 and net = 192.168.100.0/24
468     """
469     ipaddr = struct.unpack('=L', socket.inet_aton(ip))[0]
470     netaddr, bits = net.split('/')
471     netmask = struct.unpack('=L', socket.inet_aton(dotted_netmask(int(bits))))[0]
472     network = struct.unpack('=L', socket.inet_aton(netaddr))[0] & netmask
473     return (ipaddr & netmask) == (network & netmask)
474 
475 
476 def dotted_netmask(mask):
477     """
478     Converts mask from /xx format to xxx.xxx.xxx.xxx
479     Example: if mask is 24 function returns 255.255.255.0
480     """
481     bits = 0xffffffff ^ (1 << 32 - mask) - 1
482     return socket.inet_ntoa(struct.pack('>I', bits))
483 
484 
485 def is_ipv4_address(string_ip):
486     try:
487         socket.inet_aton(string_ip)
488     except socket.error:
489         return False
490     return True
491 
492 
493 def is_valid_cidr(string_network):
494     """Very simple check of the cidr format in no_proxy variable"""
495     if string_network.count('/') == 1:
496         try:
497             mask = int(string_network.split('/')[1])
498         except ValueError:
499             return False
500 
501         if mask < 1 or mask > 32:
502             return False
503 
504         try:
505             socket.inet_aton(string_network.split('/')[0])
506         except socket.error:
507             return False
508     else:
509         return False
510     return True
511 
512 
513 def should_bypass_proxies(url):
514     """
515     Returns whether we should bypass proxies or not.
516     """
517     get_proxy = lambda k: os.environ.get(k) or os.environ.get(k.upper())
518 
519     # First check whether no_proxy is defined. If it is, check that the URL
520     # we're getting isn't in the no_proxy list.
521     no_proxy = get_proxy('no_proxy')
522     netloc = urlparse(url).netloc
523 
524     if no_proxy:
525         # We need to check whether we match here. We need to see if we match
526         # the end of the netloc, both with and without the port.
527         no_proxy = (
528             host for host in no_proxy.replace(' ', '').split(',') if host
529         )
530 
531         ip = netloc.split(':')[0]
532         if is_ipv4_address(ip):
533             for proxy_ip in no_proxy:
534                 if is_valid_cidr(proxy_ip):
535                     if address_in_network(ip, proxy_ip):
536                         return True
537         else:
538             for host in no_proxy:
539                 if netloc.endswith(host) or netloc.split(':')[0].endswith(host):
540                     # The URL does match something in no_proxy, so we don't want
541                     # to apply the proxies on this URL.
542                     return True
543 
544     # If the system proxy settings indicate that this URL should be bypassed,
545     # don't proxy.
546     # The proxy_bypass function is incredibly buggy on OS X in early versions
547     # of Python 2.6, so allow this call to fail. Only catch the specific
548     # exceptions we've seen, though: this call failing in other ways can reveal
549     # legitimate problems.
550     try:
551         bypass = proxy_bypass(netloc)
552     except (TypeError, socket.gaierror):
553         bypass = False
554 
555     if bypass:
556         return True
557 
558     return False
559 
560 def get_environ_proxies(url):
561     """Return a dict of environment proxies."""
562     if should_bypass_proxies(url):
563         return {}
564     else:
565         return getproxies()
566 
567 def select_proxy(url, proxies):
568     """Select a proxy for the url, if applicable.
569 
570     :param url: The url being for the request
571     :param proxies: A dictionary of schemes or schemes and hosts to proxy URLs
572     """
573     proxies = proxies or {}
574     urlparts = urlparse(url)
575     proxy = proxies.get(urlparts.scheme+'://'+urlparts.hostname)
576     if proxy is None:
577         proxy = proxies.get(urlparts.scheme)
578     return proxy
579 
580 def default_user_agent(name="python-requests"):
581     """Return a string representing the default user agent."""
582     return '%s/%s' % (name, __version__)
583 
584 
585 def default_headers():
586     return CaseInsensitiveDict({
587         'User-Agent': default_user_agent(),
588         'Accept-Encoding': ', '.join(('gzip', 'deflate')),
589         'Accept': '*/*',
590         'Connection': 'keep-alive',
591     })
592 
593 
594 def parse_header_links(value):
595     """Return a dict of parsed link headers proxies.
596 
597     i.e. Link: <http:/.../front.jpeg>; rel=front; type="image/jpeg",<http://.../back.jpeg>; rel=back;type="image/jpeg"
598 
599     """
600 
601     links = []
602 
603     replace_chars = " '\""
604 
605     for val in re.split(", *<", value):
606         try:
607             url, params = val.split(";", 1)
608         except ValueError:
609             url, params = val, ''
610 
611         link = {}
612 
613         link["url"] = url.strip("<> '\"")
614 
615         for param in params.split(";"):
616             try:
617                 key, value = param.split("=")
618             except ValueError:
619                 break
620 
621             link[key.strip(replace_chars)] = value.strip(replace_chars)
622 
623         links.append(link)
624 
625     return links
626 
627 
628 # Null bytes; no need to recreate these on each call to guess_json_utf
629 _null = '\x00'.encode('ascii')  # encoding to ASCII for Python 3
630 _null2 = _null * 2
631 _null3 = _null * 3
632 
633 
634 def guess_json_utf(data):
635     # JSON always starts with two ASCII characters, so detection is as
636     # easy as counting the nulls and from their location and count
637     # determine the encoding. Also detect a BOM, if present.
638     sample = data[:4]
639     if sample in (codecs.BOM_UTF32_LE, codecs.BOM32_BE):
640         return 'utf-32'     # BOM included
641     if sample[:3] == codecs.BOM_UTF8:
642         return 'utf-8-sig'  # BOM included, MS style (discouraged)
643     if sample[:2] in (codecs.BOM_UTF16_LE, codecs.BOM_UTF16_BE):
644         return 'utf-16'     # BOM included
645     nullcount = sample.count(_null)
646     if nullcount == 0:
647         return 'utf-8'
648     if nullcount == 2:
649         if sample[::2] == _null2:   # 1st and 3rd are null
650             return 'utf-16-be'
651         if sample[1::2] == _null2:  # 2nd and 4th are null
652             return 'utf-16-le'
653         # Did not detect 2 valid UTF-16 ascii-range characters
654     if nullcount == 3:
655         if sample[:3] == _null3:
656             return 'utf-32-be'
657         if sample[1:] == _null3:
658             return 'utf-32-le'
659         # Did not detect a valid UTF-32 ascii-range character
660     return None
661 
662 
663 def prepend_scheme_if_needed(url, new_scheme):
664     '''Given a URL that may or may not have a scheme, prepend the given scheme.
665     Does not replace a present scheme with the one provided as an argument.'''
666     scheme, netloc, path, params, query, fragment = urlparse(url, new_scheme)
667 
668     # urlparse is a finicky beast, and sometimes decides that there isn't a
669     # netloc present. Assume that it's being over-cautious, and switch netloc
670     # and path if urlparse decided there was no netloc.
671     if not netloc:
672         netloc, path = path, netloc
673 
674     return urlunparse((scheme, netloc, path, params, query, fragment))
675 
676 
677 def get_auth_from_url(url):
678     """Given a url with authentication components, extract them into a tuple of
679     username,password."""
680     parsed = urlparse(url)
681 
682     try:
683         auth = (unquote(parsed.username), unquote(parsed.password))
684     except (AttributeError, TypeError):
685         auth = ('', '')
686 
687     return auth
688 
689 
690 def to_native_string(string, encoding='ascii'):
691     """
692     Given a string object, regardless of type, returns a representation of that
693     string in the native string type, encoding and decoding where necessary.
694     This assumes ASCII unless told otherwise.
695     """
696     out = None
697 
698     if isinstance(string, builtin_str):
699         out = string
700     else:
701         if is_py2:
702             out = string.encode(encoding)
703         else:
704             out = string.decode(encoding)
705 
706     return out
707 
708 
709 def urldefragauth(url):
710     """
711     Given a url remove the fragment and the authentication part
712     """
713     scheme, netloc, path, params, query, fragment = urlparse(url)
714 
715     # see func:`prepend_scheme_if_needed`
716     if not netloc:
717         netloc, path = path, netloc
718 
719     netloc = netloc.rsplit('@', 1)[-1]
720 
721     return urlunparse((scheme, netloc, path, params, query, ''))

</code><intended_behavior>Any utility functions that parse or prepare URLs for requests, including redirected URLs, should accurately encode non-ASCII characters.</intended_behavior>, <file>requests/packages/urllib3/util/request.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import absolute_import
2 from base64 import b64encode
3 
4 from ..packages.six import b
5 
6 ACCEPT_ENCODING = 'gzip,deflate'
7 
8 
9 def make_headers(keep_alive=None, accept_encoding=None, user_agent=None,
10                  basic_auth=None, proxy_basic_auth=None, disable_cache=None):
11     """
12     Shortcuts for generating request headers.
13 
14     :param keep_alive:
15         If ``True``, adds 'connection: keep-alive' header.
16 
17     :param accept_encoding:
18         Can be a boolean, list, or string.
19         ``True`` translates to 'gzip,deflate'.
20         List will get joined by comma.
21         String will be used as provided.
22 
23     :param user_agent:
24         String representing the user-agent you want, such as
25         "python-urllib3/0.6"
26 
27     :param basic_auth:
28         Colon-separated username:password string for 'authorization: basic ...'
29         auth header.
30 
31     :param proxy_basic_auth:
32         Colon-separated username:password string for 'proxy-authorization: basic ...'
33         auth header.
34 
35     :param disable_cache:
36         If ``True``, adds 'cache-control: no-cache' header.
37 
38     Example::
39 
40         >>> make_headers(keep_alive=True, user_agent="Batman/1.0")
41         {'connection': 'keep-alive', 'user-agent': 'Batman/1.0'}
42         >>> make_headers(accept_encoding=True)
43         {'accept-encoding': 'gzip,deflate'}
44     """
45     headers = {}
46     if accept_encoding:
47         if isinstance(accept_encoding, str):
48             pass
49         elif isinstance(accept_encoding, list):
50             accept_encoding = ','.join(accept_encoding)
51         else:
52             accept_encoding = ACCEPT_ENCODING
53         headers['accept-encoding'] = accept_encoding
54 
55     if user_agent:
56         headers['user-agent'] = user_agent
57 
58     if keep_alive:
59         headers['connection'] = 'keep-alive'
60 
61     if basic_auth:
62         headers['authorization'] = 'Basic ' + \
63             b64encode(b(basic_auth)).decode('utf-8')
64 
65     if proxy_basic_auth:
66         headers['proxy-authorization'] = 'Basic ' + \
67             b64encode(b(proxy_basic_auth)).decode('utf-8')
68 
69     if disable_cache:
70         headers['cache-control'] = 'no-cache'
71 
72     return headers

</code><intended_behavior>Ensure that utilities related to preparing or modifying request objects correctly handle encoding of non-ASCII characters in URLs.</intended_behavior>, <file>requests/packages/urllib3/response.py</file>
<class>None</class>
<method>None</method>
<code>
1 from __future__ import absolute_import
2 from contextlib import contextmanager
3 import zlib
4 import io
5 from socket import timeout as SocketTimeout
6 from socket import error as SocketError
7 
8 from ._collections import HTTPHeaderDict
9 from .exceptions import (
10     ProtocolError, DecodeError, ReadTimeoutError, ResponseNotChunked
11 )
12 from .packages.six import string_types as basestring, binary_type, PY3
13 from .packages.six.moves import http_client as httplib
14 from .connection import HTTPException, BaseSSLError
15 from .util.response import is_fp_closed, is_response_to_head
16 
17 
18 class DeflateDecoder(object):
19 
20     def __init__(self):
21         self._first_try = True
22         self._data = binary_type()
23         self._obj = zlib.decompressobj()
24 
25     def __getattr__(self, name):
26         return getattr(self._obj, name)
27 
28     def decompress(self, data):
29         if not data:
30             return data
31 
32         if not self._first_try:
33             return self._obj.decompress(data)
34 
35         self._data += data
36         try:
37             return self._obj.decompress(data)
38         except zlib.error:
39             self._first_try = False
40             self._obj = zlib.decompressobj(-zlib.MAX_WBITS)
41             try:
42                 return self.decompress(self._data)
43             finally:
44                 self._data = None
45 
46 
47 class GzipDecoder(object):
48 
49     def __init__(self):
50         self._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
51 
52     def __getattr__(self, name):
53         return getattr(self._obj, name)
54 
55     def decompress(self, data):
56         if not data:
57             return data
58         return self._obj.decompress(data)
59 
60 
61 def _get_decoder(mode):
62     if mode == 'gzip':
63         return GzipDecoder()
64 
65     return DeflateDecoder()
66 
67 
68 class HTTPResponse(io.IOBase):
69     """
70     HTTP Response container.
71 
72     Backwards-compatible to httplib's HTTPResponse but the response ``body`` is
73     loaded and decoded on-demand when the ``data`` property is accessed.  This
74     class is also compatible with the Python standard library's :mod:`io`
75     module, and can hence be treated as a readable object in the context of that
76     framework.
77 
78     Extra parameters for behaviour not present in httplib.HTTPResponse:
79 
80     :param preload_content:
81         If True, the response's body will be preloaded during construction.
82 
83     :param decode_content:
84         If True, attempts to decode specific content-encoding's based on headers
85         (like 'gzip' and 'deflate') will be skipped and raw data will be used
86         instead.
87 
88     :param original_response:
89         When this HTTPResponse wrapper is generated from an httplib.HTTPResponse
90         object, it's convenient to include the original for debug purposes. It's
91         otherwise unused.
92     """
93 
94     CONTENT_DECODERS = ['gzip', 'deflate']
95     REDIRECT_STATUSES = [301, 302, 303, 307, 308]
96 
97     def __init__(self, body='', headers=None, status=0, version=0, reason=None,
98                  strict=0, preload_content=True, decode_content=True,
99                  original_response=None, pool=None, connection=None):
100 
101         if isinstance(headers, HTTPHeaderDict):
102             self.headers = headers
103         else:
104             self.headers = HTTPHeaderDict(headers)
105         self.status = status
106         self.version = version
107         self.reason = reason
108         self.strict = strict
109         self.decode_content = decode_content
110 
111         self._decoder = None
112         self._body = None
113         self._fp = None
114         self._original_response = original_response
115         self._fp_bytes_read = 0
116 
117         if body and isinstance(body, (basestring, binary_type)):
118             self._body = body
119 
120         self._pool = pool
121         self._connection = connection
122 
123         if hasattr(body, 'read'):
124             self._fp = body
125 
126         # Are we using the chunked-style of transfer encoding?
127         self.chunked = False
128         self.chunk_left = None
129         tr_enc = self.headers.get('transfer-encoding', '').lower()
130         # Don't incur the penalty of creating a list and then discarding it
131         encodings = (enc.strip() for enc in tr_enc.split(","))
132         if "chunked" in encodings:
133             self.chunked = True
134 
135         # If requested, preload the body.
136         if preload_content and not self._body:
137             self._body = self.read(decode_content=decode_content)
138 
139     def get_redirect_location(self):
140         """
141         Should we redirect and where to?
142 
143         :returns: Truthy redirect location string if we got a redirect status
144             code and valid location. ``None`` if redirect status and no
145             location. ``False`` if not a redirect status code.
146         """
147         if self.status in self.REDIRECT_STATUSES:
148             return self.headers.get('location')
149 
150         return False
151 
152     def release_conn(self):
153         if not self._pool or not self._connection:
154             return
155 
156         self._pool._put_conn(self._connection)
157         self._connection = None
158 
159     @property
160     def data(self):
161         # For backwords-compat with earlier urllib3 0.4 and earlier.
162         if self._body:
163             return self._body
164 
165         if self._fp:
166             return self.read(cache_content=True)
167 
168     def tell(self):
169         """
170         Obtain the number of bytes pulled over the wire so far. May differ from
171         the amount of content returned by :meth:``HTTPResponse.read`` if bytes
172         are encoded on the wire (e.g, compressed).
173         """
174         return self._fp_bytes_read
175 
176     def _init_decoder(self):
177         """
178         Set-up the _decoder attribute if necessar.
179         """
180         # Note: content-encoding value should be case-insensitive, per RFC 7230
181         # Section 3.2
182         content_encoding = self.headers.get('content-encoding', '').lower()
183         if self._decoder is None and content_encoding in self.CONTENT_DECODERS:
184             self._decoder = _get_decoder(content_encoding)
185 
186     def _decode(self, data, decode_content, flush_decoder):
187         """
188         Decode the data passed in and potentially flush the decoder.
189         """
190         try:
191             if decode_content and self._decoder:
192                 data = self._decoder.decompress(data)
193         except (IOError, zlib.error) as e:
194             content_encoding = self.headers.get('content-encoding', '').lower()
195             raise DecodeError(
196                 "Received response with content-encoding: %s, but "
197                 "failed to decode it." % content_encoding, e)
198 
199         if flush_decoder and decode_content:
200             data += self._flush_decoder()
201 
202         return data
203 
204     def _flush_decoder(self):
205         """
206         Flushes the decoder. Should only be called if the decoder is actually
207         being used.
208         """
209         if self._decoder:
210             buf = self._decoder.decompress(b'')
211             return buf + self._decoder.flush()
212 
213         return b''
214 
215     @contextmanager
216     def _error_catcher(self):
217         """
218         Catch low-level python exceptions, instead re-raising urllib3
219         variants, so that low-level exceptions are not leaked in the
220         high-level api.
221 
222         On exit, release the connection back to the pool.
223         """
224         try:
225             try:
226                 yield
227 
228             except SocketTimeout:
229                 # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
230                 # there is yet no clean way to get at it from this context.
231                 raise ReadTimeoutError(self._pool, None, 'Read timed out.')
232 
233             except BaseSSLError as e:
234                 # FIXME: Is there a better way to differentiate between SSLErrors?
235                 if 'read operation timed out' not in str(e):  # Defensive:
236                     # This shouldn't happen but just in case we're missing an edge
237                     # case, let's avoid swallowing SSL errors.
238                     raise
239 
240                 raise ReadTimeoutError(self._pool, None, 'Read timed out.')
241 
242             except (HTTPException, SocketError) as e:
243                 # This includes IncompleteRead.
244                 raise ProtocolError('Connection broken: %r' % e, e)
245 
246         except Exception:
247             # The response may not be closed but we're not going to use it anymore
248             # so close it now to ensure that the connection is released back to the pool.
249             if self._original_response and not self._original_response.isclosed():
250                 self._original_response.close()
251 
252             # Closing the response may not actually be sufficient to close
253             # everything, so if we have a hold of the connection close that
254             # too.
255             if self._connection is not None:
256                 self._connection.close()
257 
258             raise
259         finally:
260             if self._original_response and self._original_response.isclosed():
261                 self.release_conn()
262 
263     def read(self, amt=None, decode_content=None, cache_content=False):
264         """
265         Similar to :meth:`httplib.HTTPResponse.read`, but with two additional
266         parameters: ``decode_content`` and ``cache_content``.
267 
268         :param amt:
269             How much of the content to read. If specified, caching is skipped
270             because it doesn't make sense to cache partial content as the full
271             response.
272 
273         :param decode_content:
274             If True, will attempt to decode the body based on the
275             'content-encoding' header.
276 
277         :param cache_content:
278             If True, will save the returned data such that the same result is
279             returned despite of the state of the underlying file object. This
280             is useful if you want the ``.data`` property to continue working
281             after having ``.read()`` the file object. (Overridden if ``amt`` is
282             set.)
283         """
284         self._init_decoder()
285         if decode_content is None:
286             decode_content = self.decode_content
287 
288         if self._fp is None:
289             return
290 
291         flush_decoder = False
292         data = None
293 
294         with self._error_catcher():
295             if amt is None:
296                 # cStringIO doesn't like amt=None
297                 data = self._fp.read()
298                 flush_decoder = True
299             else:
300                 cache_content = False
301                 data = self._fp.read(amt)
302                 if amt != 0 and not data:  # Platform-specific: Buggy versions of Python.
303                     # Close the connection when no data is returned
304                     #
305                     # This is redundant to what httplib/http.client _should_
306                     # already do.  However, versions of python released before
307                     # December 15, 2012 (http://bugs.python.org/issue16298) do
308                     # not properly close the connection in all cases. There is
309                     # no harm in redundantly calling close.
310                     self._fp.close()
311                     flush_decoder = True
312 
313         if data:
314             self._fp_bytes_read += len(data)
315 
316             data = self._decode(data, decode_content, flush_decoder)
317 
318             if cache_content:
319                 self._body = data
320 
321         return data
322 
323     def stream(self, amt=2**16, decode_content=None):
324         """
325         A generator wrapper for the read() method. A call will block until
326         ``amt`` bytes have been read from the connection or until the
327         connection is closed.
328 
329         :param amt:
330             How much of the content to read. The generator will return up to
331             much data per iteration, but may return less. This is particularly
332             likely when using compressed data. However, the empty string will
333             never be returned.
334 
335         :param decode_content:
336             If True, will attempt to decode the body based on the
337             'content-encoding' header.
338         """
339         if self.chunked:
340             for line in self.read_chunked(amt, decode_content=decode_content):
341                 yield line
342         else:
343             while not is_fp_closed(self._fp):
344                 data = self.read(amt=amt, decode_content=decode_content)
345 
346                 if data:
347                     yield data
348 
349     @classmethod
350     def from_httplib(ResponseCls, r, **response_kw):
351         """
352         Given an :class:`httplib.HTTPResponse` instance ``r``, return a
353         corresponding :class:`urllib3.response.HTTPResponse` object.
354 
355         Remaining parameters are passed to the HTTPResponse constructor, along
356         with ``original_response=r``.
357         """
358         headers = r.msg
359 
360         if not isinstance(headers, HTTPHeaderDict):
361             if PY3:  # Python 3
362                 headers = HTTPHeaderDict(headers.items())
363             else:  # Python 2
364                 headers = HTTPHeaderDict.from_httplib(headers)
365 
366         # HTTPResponse objects in Python 3 don't have a .strict attribute
367         strict = getattr(r, 'strict', 0)
368         resp = ResponseCls(body=r,
369                            headers=headers,
370                            status=r.status,
371                            version=r.version,
372                            reason=r.reason,
373                            strict=strict,
374                            original_response=r,
375                            **response_kw)
376         return resp
377 
378     # Backwards-compatibility methods for httplib.HTTPResponse
379     def getheaders(self):
380         return self.headers
381 
382     def getheader(self, name, default=None):
383         return self.headers.get(name, default)
384 
385     # Overrides from io.IOBase
386     def close(self):
387         if not self.closed:
388             self._fp.close()
389 
390     @property
391     def closed(self):
392         if self._fp is None:
393             return True
394         elif hasattr(self._fp, 'closed'):
395             return self._fp.closed
396         elif hasattr(self._fp, 'isclosed'):  # Python 2
397             return self._fp.isclosed()
398         else:
399             return True
400 
401     def fileno(self):
402         if self._fp is None:
403             raise IOError("HTTPResponse has no file to get a fileno from")
404         elif hasattr(self._fp, "fileno"):
405             return self._fp.fileno()
406         else:
407             raise IOError("The file-like object this HTTPResponse is wrapped "
408                           "around has no file descriptor")
409 
410     def flush(self):
411         if self._fp is not None and hasattr(self._fp, 'flush'):
412             return self._fp.flush()
413 
414     def readable(self):
415         # This method is required for `io` module compatibility.
416         return True
417 
418     def readinto(self, b):
419         # This method is required for `io` module compatibility.
420         temp = self.read(len(b))
421         if len(temp) == 0:
422             return 0
423         else:
424             b[:len(temp)] = temp
425             return len(temp)
426 
427     def _update_chunk_length(self):
428         # First, we'll figure out length of a chunk and then
429         # we'll try to read it from socket.
430         if self.chunk_left is not None:
431             return
432         line = self._fp.fp.readline()
433         line = line.split(b';', 1)[0]
434         try:
435             self.chunk_left = int(line, 16)
436         except ValueError:
437             # Invalid chunked protocol response, abort.
438             self.close()
439             raise httplib.IncompleteRead(line)
440 
441     def _handle_chunk(self, amt):
442         returned_chunk = None
443         if amt is None:
444             chunk = self._fp._safe_read(self.chunk_left)
445             returned_chunk = chunk
446             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
447             self.chunk_left = None
448         elif amt < self.chunk_left:
449             value = self._fp._safe_read(amt)
450             self.chunk_left = self.chunk_left - amt
451             returned_chunk = value
452         elif amt == self.chunk_left:
453             value = self._fp._safe_read(amt)
454             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
455             self.chunk_left = None
456             returned_chunk = value
457         else:  # amt > self.chunk_left
458             returned_chunk = self._fp._safe_read(self.chunk_left)
459             self._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
460             self.chunk_left = None
461         return returned_chunk
462 
463     def read_chunked(self, amt=None, decode_content=None):
464         """
465         Similar to :meth:`HTTPResponse.read`, but with an additional
466         parameter: ``decode_content``.
467 
468         :param decode_content:
469             If True, will attempt to decode the body based on the
470             'content-encoding' header.
471         """
472         self._init_decoder()
473         # FIXME: Rewrite this method and make it a class with a better structured logic.
474         if not self.chunked:
475             raise ResponseNotChunked(
476                 "Response is not chunked. "
477                 "Header 'transfer-encoding: chunked' is missing.")
478 
479         # Don't bother reading the body of a HEAD request.
480         if self._original_response and is_response_to_head(self._original_response):
481             self._original_response.close()
482             return
483 
484         with self._error_catcher():
485             while True:
486                 self._update_chunk_length()
487                 if self.chunk_left == 0:
488                     break
489                 chunk = self._handle_chunk(amt)
490                 decoded = self._decode(chunk, decode_content=decode_content,
491                                        flush_decoder=False)
492                 if decoded:
493                     yield decoded
494 
495             if decode_content:
496                 # On CPython and PyPy, we should never need to flush the
497                 # decoder. However, on Jython we *might* need to, so
498                 # lets defensively do it anyway.
499                 decoded = self._flush_decoder()
500                 if decoded:  # Platform-specific: Jython.
501                     yield decoded
502 
503             # Chunk content ends with \r\n: discard it.
504             while True:
505                 line = self._fp.fp.readline()
506                 if not line:
507                     # Some sites may not end with '\r\n'.
508                     break
509                 if line == b'\r\n':
510                     break
511 
512             # We read everything; close the "file".
513             if self._original_response:
514                 self._original_response.close()

</code><intended_behavior>The response handling, especially for 3XX status codes that involve redirects, should ensure that the Location header is correctly interpreted and that any non-ASCII URL is properly encoded before the client is redirected.</intended_behavior>]
2025-10-15 08:18:42.188 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 08:18:42.188 | INFO     | Search-only: skipping patch selection
2025-10-15 08:18:42.256 | INFO     | Task psf__requests-2754 completed successfully.
2025-10-15 08:18:42.257 | INFO     | No patch generated. You can try running ACR again.
