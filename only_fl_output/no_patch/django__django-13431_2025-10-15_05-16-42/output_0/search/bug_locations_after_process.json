[
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.2/django/db/models/query.py",
        "start": 365,
        "end": 391,
        "class_name": "QuerySet",
        "method_name": "aggregate",
        "code": "365     def aggregate(self, *args, **kwargs):\n366         \"\"\"\n367         Return a dictionary containing the calculations (aggregation)\n368         over the current queryset.\n369 \n370         If args is present the expression is passed as a kwarg using\n371         the Aggregate object's default alias.\n372         \"\"\"\n373         if self.query.distinct_fields:\n374             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n376         for arg in args:\n377             # The default_alias property raises TypeError if default_alias\n378             # can't be set automatically or AttributeError if it isn't an\n379             # attribute.\n380             try:\n381                 arg.default_alias\n382             except (AttributeError, TypeError):\n383                 raise TypeError(\"Complex aggregates require an alias\")\n384             kwargs[arg.default_alias] = arg\n385 \n386         query = self.query.chain()\n387         for (alias, aggregate_expr) in kwargs.items():\n388             query.add_annotation(aggregate_expr, alias, is_summary=True)\n389             if not query.annotations[alias].contains_aggregate:\n390                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n391         return query.get_aggregation(self.db, kwargs)\n",
        "intended_behavior": "When constructing the SQL query for aggregation, the method should correctly handle and reference annotated fields created in a preceding annotate call. This means ensuring that any aliases used for annotated fields are consistently referenced throughout the query construction process, especially when those annotated fields are utilized in aggregates. The intersection point where the aggregate queries include annotated fields must reference these annotations correctly, avoiding SQL errors like the unknown column issue because of misreferenced aliases. This adjustment ensures that when Django constructs subqueries involving annotations and aggregates, all references to annotated fields are accurate and coherent, preserving the aliasing without leading to resolution errors."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.2/django/db/models/query.py",
        "start": 175,
        "end": 1380,
        "class_name": "QuerySet",
        "method_name": null,
        "code": "175 class QuerySet:\n176     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n177 \n178     def __init__(self, model=None, query=None, using=None, hints=None):\n179         self.model = model\n180         self._db = using\n181         self._hints = hints or {}\n182         self._query = query or sql.Query(self.model)\n183         self._result_cache = None\n184         self._sticky_filter = False\n185         self._for_write = False\n186         self._prefetch_related_lookups = ()\n187         self._prefetch_done = False\n188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n189         self._iterable_class = ModelIterable\n190         self._fields = None\n191         self._defer_next_filter = False\n192         self._deferred_filter = None\n193 \n194     @property\n195     def query(self):\n196         if self._deferred_filter:\n197             negate, args, kwargs = self._deferred_filter\n198             self._filter_or_exclude_inplace(negate, args, kwargs)\n199             self._deferred_filter = None\n200         return self._query\n201 \n202     @query.setter\n203     def query(self, value):\n204         if value.values_select:\n205             self._iterable_class = ValuesIterable\n206         self._query = value\n207 \n208     def as_manager(cls):\n209         # Address the circular dependency between `Queryset` and `Manager`.\n210         from django.db.models.manager import Manager\n211         manager = Manager.from_queryset(cls)()\n212         manager._built_with_as_manager = True\n213         return manager\n214     as_manager.queryset_only = True\n215     as_manager = classmethod(as_manager)\n216 \n217     ########################\n218     # PYTHON MAGIC METHODS #\n219     ########################\n220 \n221     def __deepcopy__(self, memo):\n222         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n223         obj = self.__class__()\n224         for k, v in self.__dict__.items():\n225             if k == '_result_cache':\n226                 obj.__dict__[k] = None\n227             else:\n228                 obj.__dict__[k] = copy.deepcopy(v, memo)\n229         return obj\n230 \n231     def __getstate__(self):\n232         # Force the cache to be fully populated.\n233         self._fetch_all()\n234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n235 \n236     def __setstate__(self, state):\n237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n238         if pickled_version:\n239             if pickled_version != django.__version__:\n240                 warnings.warn(\n241                     \"Pickled queryset instance's Django version %s does not \"\n242                     \"match the current version %s.\"\n243                     % (pickled_version, django.__version__),\n244                     RuntimeWarning,\n245                     stacklevel=2,\n246                 )\n247         else:\n248             warnings.warn(\n249                 \"Pickled queryset instance's Django version is not specified.\",\n250                 RuntimeWarning,\n251                 stacklevel=2,\n252             )\n253         self.__dict__.update(state)\n254 \n255     def __repr__(self):\n256         data = list(self[:REPR_OUTPUT_SIZE + 1])\n257         if len(data) > REPR_OUTPUT_SIZE:\n258             data[-1] = \"...(remaining elements truncated)...\"\n259         return '<%s %r>' % (self.__class__.__name__, data)\n260 \n261     def __len__(self):\n262         self._fetch_all()\n263         return len(self._result_cache)\n264 \n265     def __iter__(self):\n266         \"\"\"\n267         The queryset iterator protocol uses three nested iterators in the\n268         default case:\n269             1. sql.compiler.execute_sql()\n270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n271                  using cursor.fetchmany(). This part is responsible for\n272                  doing some column masking, and returning the rows in chunks.\n273             2. sql.compiler.results_iter()\n274                - Returns one row at time. At this point the rows are still just\n275                  tuples. In some cases the return values are converted to\n276                  Python values at this location.\n277             3. self.iterator()\n278                - Responsible for turning the rows into model objects.\n279         \"\"\"\n280         self._fetch_all()\n281         return iter(self._result_cache)\n282 \n283     def __bool__(self):\n284         self._fetch_all()\n285         return bool(self._result_cache)\n286 \n287     def __getitem__(self, k):\n288         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n289         if not isinstance(k, (int, slice)):\n290             raise TypeError(\n291                 'QuerySet indices must be integers or slices, not %s.'\n292                 % type(k).__name__\n293             )\n294         assert ((not isinstance(k, slice) and (k >= 0)) or\n295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n296                  (k.stop is None or k.stop >= 0))), \\\n297             \"Negative indexing is not supported.\"\n298 \n299         if self._result_cache is not None:\n300             return self._result_cache[k]\n301 \n302         if isinstance(k, slice):\n303             qs = self._chain()\n304             if k.start is not None:\n305                 start = int(k.start)\n306             else:\n307                 start = None\n308             if k.stop is not None:\n309                 stop = int(k.stop)\n310             else:\n311                 stop = None\n312             qs.query.set_limits(start, stop)\n313             return list(qs)[::k.step] if k.step else qs\n314 \n315         qs = self._chain()\n316         qs.query.set_limits(k, k + 1)\n317         qs._fetch_all()\n318         return qs._result_cache[0]\n319 \n320     def __class_getitem__(cls, *args, **kwargs):\n321         return cls\n322 \n323     def __and__(self, other):\n324         self._merge_sanity_check(other)\n325         if isinstance(other, EmptyQuerySet):\n326             return other\n327         if isinstance(self, EmptyQuerySet):\n328             return self\n329         combined = self._chain()\n330         combined._merge_known_related_objects(other)\n331         combined.query.combine(other.query, sql.AND)\n332         return combined\n333 \n334     def __or__(self, other):\n335         self._merge_sanity_check(other)\n336         if isinstance(self, EmptyQuerySet):\n337             return other\n338         if isinstance(other, EmptyQuerySet):\n339             return self\n340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n341         combined = query._chain()\n342         combined._merge_known_related_objects(other)\n343         if not other.query.can_filter():\n344             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n345         combined.query.combine(other.query, sql.OR)\n346         return combined\n347 \n348     ####################################\n349     # METHODS THAT DO DATABASE QUERIES #\n350     ####################################\n351 \n352     def _iterator(self, use_chunked_fetch, chunk_size):\n353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n354 \n355     def iterator(self, chunk_size=2000):\n356         \"\"\"\n357         An iterator over the results from applying this QuerySet to the\n358         database.\n359         \"\"\"\n360         if chunk_size <= 0:\n361             raise ValueError('Chunk size must be strictly positive.')\n362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n363         return self._iterator(use_chunked_fetch, chunk_size)\n364 \n365     def aggregate(self, *args, **kwargs):\n366         \"\"\"\n367         Return a dictionary containing the calculations (aggregation)\n368         over the current queryset.\n369 \n370         If args is present the expression is passed as a kwarg using\n371         the Aggregate object's default alias.\n372         \"\"\"\n373         if self.query.distinct_fields:\n374             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n376         for arg in args:\n377             # The default_alias property raises TypeError if default_alias\n378             # can't be set automatically or AttributeError if it isn't an\n379             # attribute.\n380             try:\n381                 arg.default_alias\n382             except (AttributeError, TypeError):\n383                 raise TypeError(\"Complex aggregates require an alias\")\n384             kwargs[arg.default_alias] = arg\n385 \n386         query = self.query.chain()\n387         for (alias, aggregate_expr) in kwargs.items():\n388             query.add_annotation(aggregate_expr, alias, is_summary=True)\n389             if not query.annotations[alias].contains_aggregate:\n390                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n391         return query.get_aggregation(self.db, kwargs)\n392 \n393     def count(self):\n394         \"\"\"\n395         Perform a SELECT COUNT() and return the number of records as an\n396         integer.\n397 \n398         If the QuerySet is already fully cached, return the length of the\n399         cached results set to avoid multiple SELECT COUNT(*) calls.\n400         \"\"\"\n401         if self._result_cache is not None:\n402             return len(self._result_cache)\n403 \n404         return self.query.get_count(using=self.db)\n405 \n406     def get(self, *args, **kwargs):\n407         \"\"\"\n408         Perform the query and return a single object matching the given\n409         keyword arguments.\n410         \"\"\"\n411         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n412         if self.query.can_filter() and not self.query.distinct_fields:\n413             clone = clone.order_by()\n414         limit = None\n415         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n416             limit = MAX_GET_RESULTS\n417             clone.query.set_limits(high=limit)\n418         num = len(clone)\n419         if num == 1:\n420             return clone._result_cache[0]\n421         if not num:\n422             raise self.model.DoesNotExist(\n423                 \"%s matching query does not exist.\" %\n424                 self.model._meta.object_name\n425             )\n426         raise self.model.MultipleObjectsReturned(\n427             'get() returned more than one %s -- it returned %s!' % (\n428                 self.model._meta.object_name,\n429                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n430             )\n431         )\n432 \n433     def create(self, **kwargs):\n434         \"\"\"\n435         Create a new object with the given kwargs, saving it to the database\n436         and returning the created object.\n437         \"\"\"\n438         obj = self.model(**kwargs)\n439         self._for_write = True\n440         obj.save(force_insert=True, using=self.db)\n441         return obj\n442 \n443     def _populate_pk_values(self, objs):\n444         for obj in objs:\n445             if obj.pk is None:\n446                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n447 \n448     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n449         \"\"\"\n450         Insert each of the instances into the database. Do *not* call\n451         save() on each of the instances, do not send any pre/post_save\n452         signals, and do not set the primary key attribute if it is an\n453         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n454         Multi-table models are not supported.\n455         \"\"\"\n456         # When you bulk insert you don't get the primary keys back (if it's an\n457         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n458         # you can't insert into the child tables which references this. There\n459         # are two workarounds:\n460         # 1) This could be implemented if you didn't have an autoincrement pk\n461         # 2) You could do it by doing O(n) normal inserts into the parent\n462         #    tables to get the primary keys back and then doing a single bulk\n463         #    insert into the childmost table.\n464         # We currently set the primary keys on the objects when using\n465         # PostgreSQL via the RETURNING ID clause. It should be possible for\n466         # Oracle as well, but the semantics for extracting the primary keys is\n467         # trickier so it's not done yet.\n468         assert batch_size is None or batch_size > 0\n469         # Check that the parents share the same concrete model with the our\n470         # model to detect the inheritance pattern ConcreteGrandParent ->\n471         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n472         # would not identify that case as involving multiple tables.\n473         for parent in self.model._meta.get_parent_list():\n474             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n475                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n476         if not objs:\n477             return objs\n478         self._for_write = True\n479         connection = connections[self.db]\n480         opts = self.model._meta\n481         fields = opts.concrete_fields\n482         objs = list(objs)\n483         self._populate_pk_values(objs)\n484         with transaction.atomic(using=self.db, savepoint=False):\n485             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n486             if objs_with_pk:\n487                 returned_columns = self._batched_insert(\n488                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n489                 )\n490                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n491                     for result, field in zip(results, opts.db_returning_fields):\n492                         if field != opts.pk:\n493                             setattr(obj_with_pk, field.attname, result)\n494                 for obj_with_pk in objs_with_pk:\n495                     obj_with_pk._state.adding = False\n496                     obj_with_pk._state.db = self.db\n497             if objs_without_pk:\n498                 fields = [f for f in fields if not isinstance(f, AutoField)]\n499                 returned_columns = self._batched_insert(\n500                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n501                 )\n502                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n503                     assert len(returned_columns) == len(objs_without_pk)\n504                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n505                     for result, field in zip(results, opts.db_returning_fields):\n506                         setattr(obj_without_pk, field.attname, result)\n507                     obj_without_pk._state.adding = False\n508                     obj_without_pk._state.db = self.db\n509 \n510         return objs\n511 \n512     def bulk_update(self, objs, fields, batch_size=None):\n513         \"\"\"\n514         Update the given fields in each of the given objects in the database.\n515         \"\"\"\n516         if batch_size is not None and batch_size < 0:\n517             raise ValueError('Batch size must be a positive integer.')\n518         if not fields:\n519             raise ValueError('Field names must be given to bulk_update().')\n520         objs = tuple(objs)\n521         if any(obj.pk is None for obj in objs):\n522             raise ValueError('All bulk_update() objects must have a primary key set.')\n523         fields = [self.model._meta.get_field(name) for name in fields]\n524         if any(not f.concrete or f.many_to_many for f in fields):\n525             raise ValueError('bulk_update() can only be used with concrete fields.')\n526         if any(f.primary_key for f in fields):\n527             raise ValueError('bulk_update() cannot be used with primary key fields.')\n528         if not objs:\n529             return\n530         # PK is used twice in the resulting update query, once in the filter\n531         # and once in the WHEN. Each field will also have one CAST.\n532         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n533         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n534         requires_casting = connections[self.db].features.requires_casted_case_in_updates\n535         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n536         updates = []\n537         for batch_objs in batches:\n538             update_kwargs = {}\n539             for field in fields:\n540                 when_statements = []\n541                 for obj in batch_objs:\n542                     attr = getattr(obj, field.attname)\n543                     if not isinstance(attr, Expression):\n544                         attr = Value(attr, output_field=field)\n545                     when_statements.append(When(pk=obj.pk, then=attr))\n546                 case_statement = Case(*when_statements, output_field=field)\n547                 if requires_casting:\n548                     case_statement = Cast(case_statement, output_field=field)\n549                 update_kwargs[field.attname] = case_statement\n550             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n551         with transaction.atomic(using=self.db, savepoint=False):\n552             for pks, update_kwargs in updates:\n553                 self.filter(pk__in=pks).update(**update_kwargs)\n554     bulk_update.alters_data = True\n555 \n556     def get_or_create(self, defaults=None, **kwargs):\n557         \"\"\"\n558         Look up an object with the given kwargs, creating one if necessary.\n559         Return a tuple of (object, created), where created is a boolean\n560         specifying whether an object was created.\n561         \"\"\"\n562         # The get() needs to be targeted at the write database in order\n563         # to avoid potential transaction consistency problems.\n564         self._for_write = True\n565         try:\n566             return self.get(**kwargs), False\n567         except self.model.DoesNotExist:\n568             params = self._extract_model_params(defaults, **kwargs)\n569             # Try to create an object using passed params.\n570             try:\n571                 with transaction.atomic(using=self.db):\n572                     params = dict(resolve_callables(params))\n573                     return self.create(**params), True\n574             except IntegrityError:\n575                 try:\n576                     return self.get(**kwargs), False\n577                 except self.model.DoesNotExist:\n578                     pass\n579                 raise\n580 \n581     def update_or_create(self, defaults=None, **kwargs):\n582         \"\"\"\n583         Look up an object with the given kwargs, updating one with defaults\n584         if it exists, otherwise create a new one.\n585         Return a tuple (object, created), where created is a boolean\n586         specifying whether an object was created.\n587         \"\"\"\n588         defaults = defaults or {}\n589         self._for_write = True\n590         with transaction.atomic(using=self.db):\n591             # Lock the row so that a concurrent update is blocked until\n592             # update_or_create() has performed its save.\n593             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n594             if created:\n595                 return obj, created\n596             for k, v in resolve_callables(defaults):\n597                 setattr(obj, k, v)\n598             obj.save(using=self.db)\n599         return obj, False\n600 \n601     def _extract_model_params(self, defaults, **kwargs):\n602         \"\"\"\n603         Prepare `params` for creating a model instance based on the given\n604         kwargs; for use by get_or_create().\n605         \"\"\"\n606         defaults = defaults or {}\n607         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n608         params.update(defaults)\n609         property_names = self.model._meta._property_names\n610         invalid_params = []\n611         for param in params:\n612             try:\n613                 self.model._meta.get_field(param)\n614             except exceptions.FieldDoesNotExist:\n615                 # It's okay to use a model's property if it has a setter.\n616                 if not (param in property_names and getattr(self.model, param).fset):\n617                     invalid_params.append(param)\n618         if invalid_params:\n619             raise exceptions.FieldError(\n620                 \"Invalid field name(s) for model %s: '%s'.\" % (\n621                     self.model._meta.object_name,\n622                     \"', '\".join(sorted(invalid_params)),\n623                 ))\n624         return params\n625 \n626     def _earliest(self, *fields):\n627         \"\"\"\n628         Return the earliest object according to fields (if given) or by the\n629         model's Meta.get_latest_by.\n630         \"\"\"\n631         if fields:\n632             order_by = fields\n633         else:\n634             order_by = getattr(self.model._meta, 'get_latest_by')\n635             if order_by and not isinstance(order_by, (tuple, list)):\n636                 order_by = (order_by,)\n637         if order_by is None:\n638             raise ValueError(\n639                 \"earliest() and latest() require either fields as positional \"\n640                 \"arguments or 'get_latest_by' in the model's Meta.\"\n641             )\n642 \n643         assert not self.query.is_sliced, \\\n644             \"Cannot change a query once a slice has been taken.\"\n645         obj = self._chain()\n646         obj.query.set_limits(high=1)\n647         obj.query.clear_ordering(force_empty=True)\n648         obj.query.add_ordering(*order_by)\n649         return obj.get()\n650 \n651     def earliest(self, *fields):\n652         return self._earliest(*fields)\n653 \n654     def latest(self, *fields):\n655         return self.reverse()._earliest(*fields)\n656 \n657     def first(self):\n658         \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n659         for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n660             return obj\n661 \n662     def last(self):\n663         \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n664         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n665             return obj\n666 \n667     def in_bulk(self, id_list=None, *, field_name='pk'):\n668         \"\"\"\n669         Return a dictionary mapping each of the given IDs to the object with\n670         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n671         \"\"\"\n672         assert not self.query.is_sliced, \\\n673             \"Cannot use 'limit' or 'offset' with in_bulk\"\n674         opts = self.model._meta\n675         unique_fields = [\n676             constraint.fields[0]\n677             for constraint in opts.total_unique_constraints\n678             if len(constraint.fields) == 1\n679         ]\n680         if (\n681             field_name != 'pk' and\n682             not opts.get_field(field_name).unique and\n683             field_name not in unique_fields and\n684             not self.query.distinct_fields == (field_name,)\n685         ):\n686             raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n687         if id_list is not None:\n688             if not id_list:\n689                 return {}\n690             filter_key = '{}__in'.format(field_name)\n691             batch_size = connections[self.db].features.max_query_params\n692             id_list = tuple(id_list)\n693             # If the database has a limit on the number of query parameters\n694             # (e.g. SQLite), retrieve objects in batches if necessary.\n695             if batch_size and batch_size < len(id_list):\n696                 qs = ()\n697                 for offset in range(0, len(id_list), batch_size):\n698                     batch = id_list[offset:offset + batch_size]\n699                     qs += tuple(self.filter(**{filter_key: batch}).order_by())\n700             else:\n701                 qs = self.filter(**{filter_key: id_list}).order_by()\n702         else:\n703             qs = self._chain()\n704         return {getattr(obj, field_name): obj for obj in qs}\n705 \n706     def delete(self):\n707         \"\"\"Delete the records in the current QuerySet.\"\"\"\n708         self._not_support_combined_queries('delete')\n709         assert not self.query.is_sliced, \\\n710             \"Cannot use 'limit' or 'offset' with delete.\"\n711 \n712         if self._fields is not None:\n713             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n714 \n715         del_query = self._chain()\n716 \n717         # The delete is actually 2 queries - one to find related objects,\n718         # and one to delete. Make sure that the discovery of related\n719         # objects is performed on the same database as the deletion.\n720         del_query._for_write = True\n721 \n722         # Disable non-supported fields.\n723         del_query.query.select_for_update = False\n724         del_query.query.select_related = False\n725         del_query.query.clear_ordering(force_empty=True)\n726 \n727         collector = Collector(using=del_query.db)\n728         collector.collect(del_query)\n729         deleted, _rows_count = collector.delete()\n730 \n731         # Clear the result cache, in case this QuerySet gets reused.\n732         self._result_cache = None\n733         return deleted, _rows_count\n734 \n735     delete.alters_data = True\n736     delete.queryset_only = True\n737 \n738     def _raw_delete(self, using):\n739         \"\"\"\n740         Delete objects found from the given queryset in single direct SQL\n741         query. No signals are sent and there is no protection for cascades.\n742         \"\"\"\n743         query = self.query.clone()\n744         query.__class__ = sql.DeleteQuery\n745         cursor = query.get_compiler(using).execute_sql(CURSOR)\n746         if cursor:\n747             with cursor:\n748                 return cursor.rowcount\n749         return 0\n750     _raw_delete.alters_data = True\n751 \n752     def update(self, **kwargs):\n753         \"\"\"\n754         Update all elements in the current QuerySet, setting all the given\n755         fields to the appropriate values.\n756         \"\"\"\n757         self._not_support_combined_queries('update')\n758         assert not self.query.is_sliced, \\\n759             \"Cannot update a query once a slice has been taken.\"\n760         self._for_write = True\n761         query = self.query.chain(sql.UpdateQuery)\n762         query.add_update_values(kwargs)\n763         # Clear any annotations so that they won't be present in subqueries.\n764         query.annotations = {}\n765         with transaction.mark_for_rollback_on_error(using=self.db):\n766             rows = query.get_compiler(self.db).execute_sql(CURSOR)\n767         self._result_cache = None\n768         return rows\n769     update.alters_data = True\n770 \n771     def _update(self, values):\n772         \"\"\"\n773         A version of update() that accepts field objects instead of field names.\n774         Used primarily for model saving and not intended for use by general\n775         code (it requires too much poking around at model internals to be\n776         useful at that level).\n777         \"\"\"\n778         assert not self.query.is_sliced, \\\n779             \"Cannot update a query once a slice has been taken.\"\n780         query = self.query.chain(sql.UpdateQuery)\n781         query.add_update_fields(values)\n782         # Clear any annotations so that they won't be present in subqueries.\n783         query.annotations = {}\n784         self._result_cache = None\n785         return query.get_compiler(self.db).execute_sql(CURSOR)\n786     _update.alters_data = True\n787     _update.queryset_only = False\n788 \n789     def exists(self):\n790         if self._result_cache is None:\n791             return self.query.has_results(using=self.db)\n792         return bool(self._result_cache)\n793 \n794     def _prefetch_related_objects(self):\n795         # This method can only be called once the result cache has been filled.\n796         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n797         self._prefetch_done = True\n798 \n799     def explain(self, *, format=None, **options):\n800         return self.query.explain(using=self.db, format=format, **options)\n801 \n802     ##################################################\n803     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n804     ##################################################\n805 \n806     def raw(self, raw_query, params=None, translations=None, using=None):\n807         if using is None:\n808             using = self.db\n809         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)\n810         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n811         return qs\n812 \n813     def _values(self, *fields, **expressions):\n814         clone = self._chain()\n815         if expressions:\n816             clone = clone.annotate(**expressions)\n817         clone._fields = fields\n818         clone.query.set_values(fields)\n819         return clone\n820 \n821     def values(self, *fields, **expressions):\n822         fields += tuple(expressions)\n823         clone = self._values(*fields, **expressions)\n824         clone._iterable_class = ValuesIterable\n825         return clone\n826 \n827     def values_list(self, *fields, flat=False, named=False):\n828         if flat and named:\n829             raise TypeError(\"'flat' and 'named' can't be used together.\")\n830         if flat and len(fields) > 1:\n831             raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n832 \n833         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}\n834         _fields = []\n835         expressions = {}\n836         counter = 1\n837         for field in fields:\n838             if hasattr(field, 'resolve_expression'):\n839                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())\n840                 while True:\n841                     field_id = field_id_prefix + str(counter)\n842                     counter += 1\n843                     if field_id not in field_names:\n844                         break\n845                 expressions[field_id] = field\n846                 _fields.append(field_id)\n847             else:\n848                 _fields.append(field)\n849 \n850         clone = self._values(*_fields, **expressions)\n851         clone._iterable_class = (\n852             NamedValuesListIterable if named\n853             else FlatValuesListIterable if flat\n854             else ValuesListIterable\n855         )\n856         return clone\n857 \n858     def dates(self, field_name, kind, order='ASC'):\n859         \"\"\"\n860         Return a list of date objects representing all available dates for\n861         the given field_name, scoped to 'kind'.\n862         \"\"\"\n863         assert kind in ('year', 'month', 'week', 'day'), \\\n864             \"'kind' must be one of 'year', 'month', 'week', or 'day'.\"\n865         assert order in ('ASC', 'DESC'), \\\n866             \"'order' must be either 'ASC' or 'DESC'.\"\n867         return self.annotate(\n868             datefield=Trunc(field_name, kind, output_field=DateField()),\n869             plain_field=F(field_name)\n870         ).values_list(\n871             'datefield', flat=True\n872         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')\n873 \n874     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):\n875         \"\"\"\n876         Return a list of datetime objects representing all available\n877         datetimes for the given field_name, scoped to 'kind'.\n878         \"\"\"\n879         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \\\n880             \"'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'.\"\n881         assert order in ('ASC', 'DESC'), \\\n882             \"'order' must be either 'ASC' or 'DESC'.\"\n883         if settings.USE_TZ:\n884             if tzinfo is None:\n885                 tzinfo = timezone.get_current_timezone()\n886         else:\n887             tzinfo = None\n888         return self.annotate(\n889             datetimefield=Trunc(\n890                 field_name,\n891                 kind,\n892                 output_field=DateTimeField(),\n893                 tzinfo=tzinfo,\n894                 is_dst=is_dst,\n895             ),\n896             plain_field=F(field_name)\n897         ).values_list(\n898             'datetimefield', flat=True\n899         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')\n900 \n901     def none(self):\n902         \"\"\"Return an empty QuerySet.\"\"\"\n903         clone = self._chain()\n904         clone.query.set_empty()\n905         return clone\n906 \n907     ##################################################################\n908     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n909     ##################################################################\n910 \n911     def all(self):\n912         \"\"\"\n913         Return a new QuerySet that is a copy of the current one. This allows a\n914         QuerySet to proxy for a model manager in some cases.\n915         \"\"\"\n916         return self._chain()\n917 \n918     def filter(self, *args, **kwargs):\n919         \"\"\"\n920         Return a new QuerySet instance with the args ANDed to the existing\n921         set.\n922         \"\"\"\n923         self._not_support_combined_queries('filter')\n924         return self._filter_or_exclude(False, args, kwargs)\n925 \n926     def exclude(self, *args, **kwargs):\n927         \"\"\"\n928         Return a new QuerySet instance with NOT (args) ANDed to the existing\n929         set.\n930         \"\"\"\n931         self._not_support_combined_queries('exclude')\n932         return self._filter_or_exclude(True, args, kwargs)\n933 \n934     def _filter_or_exclude(self, negate, args, kwargs):\n935         if args or kwargs:\n936             assert not self.query.is_sliced, \\\n937                 \"Cannot filter a query once a slice has been taken.\"\n938 \n939         clone = self._chain()\n940         if self._defer_next_filter:\n941             self._defer_next_filter = False\n942             clone._deferred_filter = negate, args, kwargs\n943         else:\n944             clone._filter_or_exclude_inplace(negate, args, kwargs)\n945         return clone\n946 \n947     def _filter_or_exclude_inplace(self, negate, args, kwargs):\n948         if negate:\n949             self._query.add_q(~Q(*args, **kwargs))\n950         else:\n951             self._query.add_q(Q(*args, **kwargs))\n952 \n953     def complex_filter(self, filter_obj):\n954         \"\"\"\n955         Return a new QuerySet instance with filter_obj added to the filters.\n956 \n957         filter_obj can be a Q object or a dictionary of keyword lookup\n958         arguments.\n959 \n960         This exists to support framework features such as 'limit_choices_to',\n961         and usually it will be more natural to use other methods.\n962         \"\"\"\n963         if isinstance(filter_obj, Q):\n964             clone = self._chain()\n965             clone.query.add_q(filter_obj)\n966             return clone\n967         else:\n968             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)\n969 \n970     def _combinator_query(self, combinator, *other_qs, all=False):\n971         # Clone the query to inherit the select list and everything\n972         clone = self._chain()\n973         # Clear limits and ordering so they can be reapplied\n974         clone.query.clear_ordering(True)\n975         clone.query.clear_limits()\n976         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)\n977         clone.query.combinator = combinator\n978         clone.query.combinator_all = all\n979         return clone\n980 \n981     def union(self, *other_qs, all=False):\n982         # If the query is an EmptyQuerySet, combine all nonempty querysets.\n983         if isinstance(self, EmptyQuerySet):\n984             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n985             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self\n986         return self._combinator_query('union', *other_qs, all=all)\n987 \n988     def intersection(self, *other_qs):\n989         # If any query is an EmptyQuerySet, return it.\n990         if isinstance(self, EmptyQuerySet):\n991             return self\n992         for other in other_qs:\n993             if isinstance(other, EmptyQuerySet):\n994                 return other\n995         return self._combinator_query('intersection', *other_qs)\n996 \n997     def difference(self, *other_qs):\n998         # If the query is an EmptyQuerySet, return it.\n999         if isinstance(self, EmptyQuerySet):\n1000             return self\n1001         return self._combinator_query('difference', *other_qs)\n1002 \n1003     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):\n1004         \"\"\"\n1005         Return a new QuerySet instance that will select objects with a\n1006         FOR UPDATE lock.\n1007         \"\"\"\n1008         if nowait and skip_locked:\n1009             raise ValueError('The nowait option cannot be used with skip_locked.')\n1010         obj = self._chain()\n1011         obj._for_write = True\n1012         obj.query.select_for_update = True\n1013         obj.query.select_for_update_nowait = nowait\n1014         obj.query.select_for_update_skip_locked = skip_locked\n1015         obj.query.select_for_update_of = of\n1016         obj.query.select_for_no_key_update = no_key\n1017         return obj\n1018 \n1019     def select_related(self, *fields):\n1020         \"\"\"\n1021         Return a new QuerySet instance that will select related objects.\n1022 \n1023         If fields are specified, they must be ForeignKey fields and only those\n1024         related objects are included in the selection.\n1025 \n1026         If select_related(None) is called, clear the list.\n1027         \"\"\"\n1028         self._not_support_combined_queries('select_related')\n1029         if self._fields is not None:\n1030             raise TypeError(\"Cannot call select_related() after .values() or .values_list()\")\n1031 \n1032         obj = self._chain()\n1033         if fields == (None,):\n1034             obj.query.select_related = False\n1035         elif fields:\n1036             obj.query.add_select_related(fields)\n1037         else:\n1038             obj.query.select_related = True\n1039         return obj\n1040 \n1041     def prefetch_related(self, *lookups):\n1042         \"\"\"\n1043         Return a new QuerySet instance that will prefetch the specified\n1044         Many-To-One and Many-To-Many related objects when the QuerySet is\n1045         evaluated.\n1046 \n1047         When prefetch_related() is called more than once, append to the list of\n1048         prefetch lookups. If prefetch_related(None) is called, clear the list.\n1049         \"\"\"\n1050         self._not_support_combined_queries('prefetch_related')\n1051         clone = self._chain()\n1052         if lookups == (None,):\n1053             clone._prefetch_related_lookups = ()\n1054         else:\n1055             for lookup in lookups:\n1056                 if isinstance(lookup, Prefetch):\n1057                     lookup = lookup.prefetch_to\n1058                 lookup = lookup.split(LOOKUP_SEP, 1)[0]\n1059                 if lookup in self.query._filtered_relations:\n1060                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')\n1061             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n1062         return clone\n1063 \n1064     def annotate(self, *args, **kwargs):\n1065         \"\"\"\n1066         Return a query set in which the returned objects have been annotated\n1067         with extra data or aggregations.\n1068         \"\"\"\n1069         self._not_support_combined_queries('annotate')\n1070         return self._annotate(args, kwargs, select=True)\n1071 \n1072     def alias(self, *args, **kwargs):\n1073         \"\"\"\n1074         Return a query set with added aliases for extra data or aggregations.\n1075         \"\"\"\n1076         self._not_support_combined_queries('alias')\n1077         return self._annotate(args, kwargs, select=False)\n1078 \n1079     def _annotate(self, args, kwargs, select=True):\n1080         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n1081         annotations = {}\n1082         for arg in args:\n1083             # The default_alias property may raise a TypeError.\n1084             try:\n1085                 if arg.default_alias in kwargs:\n1086                     raise ValueError(\"The named annotation '%s' conflicts with the \"\n1087                                      \"default name for another annotation.\"\n1088                                      % arg.default_alias)\n1089             except TypeError:\n1090                 raise TypeError(\"Complex annotations require an alias\")\n1091             annotations[arg.default_alias] = arg\n1092         annotations.update(kwargs)\n1093 \n1094         clone = self._chain()\n1095         names = self._fields\n1096         if names is None:\n1097             names = set(chain.from_iterable(\n1098                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n1099                 for field in self.model._meta.get_fields()\n1100             ))\n1101 \n1102         for alias, annotation in annotations.items():\n1103             if alias in names:\n1104                 raise ValueError(\"The annotation '%s' conflicts with a field on \"\n1105                                  \"the model.\" % alias)\n1106             if isinstance(annotation, FilteredRelation):\n1107                 clone.query.add_filtered_relation(annotation, alias)\n1108             else:\n1109                 clone.query.add_annotation(\n1110                     annotation, alias, is_summary=False, select=select,\n1111                 )\n1112         for alias, annotation in clone.query.annotations.items():\n1113             if alias in annotations and annotation.contains_aggregate:\n1114                 if clone._fields is None:\n1115                     clone.query.group_by = True\n1116                 else:\n1117                     clone.query.set_group_by()\n1118                 break\n1119 \n1120         return clone\n1121 \n1122     def order_by(self, *field_names):\n1123         \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n1124         assert not self.query.is_sliced, \\\n1125             \"Cannot reorder a query once a slice has been taken.\"\n1126         obj = self._chain()\n1127         obj.query.clear_ordering(force_empty=False)\n1128         obj.query.add_ordering(*field_names)\n1129         return obj\n1130 \n1131     def distinct(self, *field_names):\n1132         \"\"\"\n1133         Return a new QuerySet instance that will select only distinct results.\n1134         \"\"\"\n1135         self._not_support_combined_queries('distinct')\n1136         assert not self.query.is_sliced, \\\n1137             \"Cannot create distinct fields once a slice has been taken.\"\n1138         obj = self._chain()\n1139         obj.query.add_distinct_fields(*field_names)\n1140         return obj\n1141 \n1142     def extra(self, select=None, where=None, params=None, tables=None,\n1143               order_by=None, select_params=None):\n1144         \"\"\"Add extra SQL fragments to the query.\"\"\"\n1145         self._not_support_combined_queries('extra')\n1146         assert not self.query.is_sliced, \\\n1147             \"Cannot change a query once a slice has been taken\"\n1148         clone = self._chain()\n1149         clone.query.add_extra(select, select_params, where, params, tables, order_by)\n1150         return clone\n1151 \n1152     def reverse(self):\n1153         \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n1154         if self.query.is_sliced:\n1155             raise TypeError('Cannot reverse a query once a slice has been taken.')\n1156         clone = self._chain()\n1157         clone.query.standard_ordering = not clone.query.standard_ordering\n1158         return clone\n1159 \n1160     def defer(self, *fields):\n1161         \"\"\"\n1162         Defer the loading of data for certain fields until they are accessed.\n1163         Add the set of deferred fields to any existing set of deferred fields.\n1164         The only exception to this is if None is passed in as the only\n1165         parameter, in which case removal all deferrals.\n1166         \"\"\"\n1167         self._not_support_combined_queries('defer')\n1168         if self._fields is not None:\n1169             raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n1170         clone = self._chain()\n1171         if fields == (None,):\n1172             clone.query.clear_deferred_loading()\n1173         else:\n1174             clone.query.add_deferred_loading(fields)\n1175         return clone\n1176 \n1177     def only(self, *fields):\n1178         \"\"\"\n1179         Essentially, the opposite of defer(). Only the fields passed into this\n1180         method and that are not already specified as deferred are loaded\n1181         immediately when the queryset is evaluated.\n1182         \"\"\"\n1183         self._not_support_combined_queries('only')\n1184         if self._fields is not None:\n1185             raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n1186         if fields == (None,):\n1187             # Can only pass None to defer(), not only(), as the rest option.\n1188             # That won't stop people trying to do this, so let's be explicit.\n1189             raise TypeError(\"Cannot pass None as an argument to only().\")\n1190         for field in fields:\n1191             field = field.split(LOOKUP_SEP, 1)[0]\n1192             if field in self.query._filtered_relations:\n1193                 raise ValueError('only() is not supported with FilteredRelation.')\n1194         clone = self._chain()\n1195         clone.query.add_immediate_loading(fields)\n1196         return clone\n1197 \n1198     def using(self, alias):\n1199         \"\"\"Select which database this QuerySet should execute against.\"\"\"\n1200         clone = self._chain()\n1201         clone._db = alias\n1202         return clone\n1203 \n1204     ###################################\n1205     # PUBLIC INTROSPECTION ATTRIBUTES #\n1206     ###################################\n1207 \n1208     @property\n1209     def ordered(self):\n1210         \"\"\"\n1211         Return True if the QuerySet is ordered -- i.e. has an order_by()\n1212         clause or a default ordering on the model (or is empty).\n1213         \"\"\"\n1214         if isinstance(self, EmptyQuerySet):\n1215             return True\n1216         if self.query.extra_order_by or self.query.order_by:\n1217             return True\n1218         elif (\n1219             self.query.default_ordering and\n1220             self.query.get_meta().ordering and\n1221             # A default ordering doesn't affect GROUP BY queries.\n1222             not self.query.group_by\n1223         ):\n1224             return True\n1225         else:\n1226             return False\n1227 \n1228     @property\n1229     def db(self):\n1230         \"\"\"Return the database used if this query is executed now.\"\"\"\n1231         if self._for_write:\n1232             return self._db or router.db_for_write(self.model, **self._hints)\n1233         return self._db or router.db_for_read(self.model, **self._hints)\n1234 \n1235     ###################\n1236     # PRIVATE METHODS #\n1237     ###################\n1238 \n1239     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):\n1240         \"\"\"\n1241         Insert a new record for the given model. This provides an interface to\n1242         the InsertQuery class and is how Model.save() is implemented.\n1243         \"\"\"\n1244         self._for_write = True\n1245         if using is None:\n1246             using = self.db\n1247         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)\n1248         query.insert_values(fields, objs, raw=raw)\n1249         return query.get_compiler(using=using).execute_sql(returning_fields)\n1250     _insert.alters_data = True\n1251     _insert.queryset_only = False\n1252 \n1253     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):\n1254         \"\"\"\n1255         Helper method for bulk_create() to insert objs one batch at a time.\n1256         \"\"\"\n1257         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n1258             raise NotSupportedError('This database backend does not support ignoring conflicts.')\n1259         ops = connections[self.db].ops\n1260         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n1261         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n1262         inserted_rows = []\n1263         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n1264         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n1265             if bulk_return and not ignore_conflicts:\n1266                 inserted_rows.extend(self._insert(\n1267                     item, fields=fields, using=self.db,\n1268                     returning_fields=self.model._meta.db_returning_fields,\n1269                     ignore_conflicts=ignore_conflicts,\n1270                 ))\n1271             else:\n1272                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)\n1273         return inserted_rows\n1274 \n1275     def _chain(self, **kwargs):\n1276         \"\"\"\n1277         Return a copy of the current QuerySet that's ready for another\n1278         operation.\n1279         \"\"\"\n1280         obj = self._clone()\n1281         if obj._sticky_filter:\n1282             obj.query.filter_is_sticky = True\n1283             obj._sticky_filter = False\n1284         obj.__dict__.update(kwargs)\n1285         return obj\n1286 \n1287     def _clone(self):\n1288         \"\"\"\n1289         Return a copy of the current QuerySet. A lightweight alternative\n1290         to deepcopy().\n1291         \"\"\"\n1292         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)\n1293         c._sticky_filter = self._sticky_filter\n1294         c._for_write = self._for_write\n1295         c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n1296         c._known_related_objects = self._known_related_objects\n1297         c._iterable_class = self._iterable_class\n1298         c._fields = self._fields\n1299         return c\n1300 \n1301     def _fetch_all(self):\n1302         if self._result_cache is None:\n1303             self._result_cache = list(self._iterable_class(self))\n1304         if self._prefetch_related_lookups and not self._prefetch_done:\n1305             self._prefetch_related_objects()\n1306 \n1307     def _next_is_sticky(self):\n1308         \"\"\"\n1309         Indicate that the next filter call and the one following that should\n1310         be treated as a single filter. This is only important when it comes to\n1311         determining when to reuse tables for many-to-many filters. Required so\n1312         that we can filter naturally on the results of related managers.\n1313 \n1314         This doesn't return a clone of the current QuerySet (it returns\n1315         \"self\"). The method is only used internally and should be immediately\n1316         followed by a filter() that does create a clone.\n1317         \"\"\"\n1318         self._sticky_filter = True\n1319         return self\n1320 \n1321     def _merge_sanity_check(self, other):\n1322         \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n1323         if self._fields is not None and (\n1324                 set(self.query.values_select) != set(other.query.values_select) or\n1325                 set(self.query.extra_select) != set(other.query.extra_select) or\n1326                 set(self.query.annotation_select) != set(other.query.annotation_select)):\n1327             raise TypeError(\n1328                 \"Merging '%s' classes must involve the same values in each case.\"\n1329                 % self.__class__.__name__\n1330             )\n1331 \n1332     def _merge_known_related_objects(self, other):\n1333         \"\"\"\n1334         Keep track of all known related objects from either QuerySet instance.\n1335         \"\"\"\n1336         for field, objects in other._known_related_objects.items():\n1337             self._known_related_objects.setdefault(field, {}).update(objects)\n1338 \n1339     def resolve_expression(self, *args, **kwargs):\n1340         if self._fields and len(self._fields) > 1:\n1341             # values() queryset can only be used as nested queries\n1342             # if they are set up to select only a single field.\n1343             raise TypeError('Cannot use multi-field values as a filter value.')\n1344         query = self.query.resolve_expression(*args, **kwargs)\n1345         query._db = self._db\n1346         return query\n1347     resolve_expression.queryset_only = True\n1348 \n1349     def _add_hints(self, **hints):\n1350         \"\"\"\n1351         Update hinting information for use by routers. Add new key/values or\n1352         overwrite existing key/values.\n1353         \"\"\"\n1354         self._hints.update(hints)\n1355 \n1356     def _has_filters(self):\n1357         \"\"\"\n1358         Check if this QuerySet has any filtering going on. This isn't\n1359         equivalent with checking if all objects are present in results, for\n1360         example, qs[1:]._has_filters() -> False.\n1361         \"\"\"\n1362         return self.query.has_filters()\n1363 \n1364     @staticmethod\n1365     def _validate_values_are_expressions(values, method_name):\n1366         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))\n1367         if invalid_args:\n1368             raise TypeError(\n1369                 'QuerySet.%s() received non-expression(s): %s.' % (\n1370                     method_name,\n1371                     ', '.join(invalid_args),\n1372                 )\n1373             )\n1374 \n1375     def _not_support_combined_queries(self, operation_name):\n1376         if self.query.combinator:\n1377             raise NotSupportedError(\n1378                 'Calling QuerySet.%s() after %s() is not supported.'\n1379                 % (operation_name, self.query.combinator)\n1380             )\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.2/django/db/models/query.py",
        "start": 1064,
        "end": 1070,
        "class_name": "QuerySet",
        "method_name": "annotate",
        "code": "1064     def annotate(self, *args, **kwargs):\n1065         \"\"\"\n1066         Return a query set in which the returned objects have been annotated\n1067         with extra data or aggregations.\n1068         \"\"\"\n1069         self._not_support_combined_queries('annotate')\n1070         return self._annotate(args, kwargs, select=True)\n",
        "intended_behavior": "Although not directly implicated as faulty, ensuring the annotate method's output is correctly utilized by subsequent aggregate calls is essential. The annotate function should clearly define the scope and aliasing of annotated fields so that these can be understood and handled without ambiguity in subsequent operations, specifically aggregate. The method itself may not require changes, but its interaction with aggregate and the overall SQL construction logic should be verified to ensure annotations are appropriately factored into the aggregate's SQL generation logic. This includes validating the chaining of queryset methods where annotations serve as inputs to aggregates, leading to proper SQL command generation that considers the established aliases."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.2/django/db/models/query.py",
        "start": 175,
        "end": 1380,
        "class_name": "QuerySet",
        "method_name": null,
        "code": "175 class QuerySet:\n176     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n177 \n178     def __init__(self, model=None, query=None, using=None, hints=None):\n179         self.model = model\n180         self._db = using\n181         self._hints = hints or {}\n182         self._query = query or sql.Query(self.model)\n183         self._result_cache = None\n184         self._sticky_filter = False\n185         self._for_write = False\n186         self._prefetch_related_lookups = ()\n187         self._prefetch_done = False\n188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n189         self._iterable_class = ModelIterable\n190         self._fields = None\n191         self._defer_next_filter = False\n192         self._deferred_filter = None\n193 \n194     @property\n195     def query(self):\n196         if self._deferred_filter:\n197             negate, args, kwargs = self._deferred_filter\n198             self._filter_or_exclude_inplace(negate, args, kwargs)\n199             self._deferred_filter = None\n200         return self._query\n201 \n202     @query.setter\n203     def query(self, value):\n204         if value.values_select:\n205             self._iterable_class = ValuesIterable\n206         self._query = value\n207 \n208     def as_manager(cls):\n209         # Address the circular dependency between `Queryset` and `Manager`.\n210         from django.db.models.manager import Manager\n211         manager = Manager.from_queryset(cls)()\n212         manager._built_with_as_manager = True\n213         return manager\n214     as_manager.queryset_only = True\n215     as_manager = classmethod(as_manager)\n216 \n217     ########################\n218     # PYTHON MAGIC METHODS #\n219     ########################\n220 \n221     def __deepcopy__(self, memo):\n222         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n223         obj = self.__class__()\n224         for k, v in self.__dict__.items():\n225             if k == '_result_cache':\n226                 obj.__dict__[k] = None\n227             else:\n228                 obj.__dict__[k] = copy.deepcopy(v, memo)\n229         return obj\n230 \n231     def __getstate__(self):\n232         # Force the cache to be fully populated.\n233         self._fetch_all()\n234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n235 \n236     def __setstate__(self, state):\n237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n238         if pickled_version:\n239             if pickled_version != django.__version__:\n240                 warnings.warn(\n241                     \"Pickled queryset instance's Django version %s does not \"\n242                     \"match the current version %s.\"\n243                     % (pickled_version, django.__version__),\n244                     RuntimeWarning,\n245                     stacklevel=2,\n246                 )\n247         else:\n248             warnings.warn(\n249                 \"Pickled queryset instance's Django version is not specified.\",\n250                 RuntimeWarning,\n251                 stacklevel=2,\n252             )\n253         self.__dict__.update(state)\n254 \n255     def __repr__(self):\n256         data = list(self[:REPR_OUTPUT_SIZE + 1])\n257         if len(data) > REPR_OUTPUT_SIZE:\n258             data[-1] = \"...(remaining elements truncated)...\"\n259         return '<%s %r>' % (self.__class__.__name__, data)\n260 \n261     def __len__(self):\n262         self._fetch_all()\n263         return len(self._result_cache)\n264 \n265     def __iter__(self):\n266         \"\"\"\n267         The queryset iterator protocol uses three nested iterators in the\n268         default case:\n269             1. sql.compiler.execute_sql()\n270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n271                  using cursor.fetchmany(). This part is responsible for\n272                  doing some column masking, and returning the rows in chunks.\n273             2. sql.compiler.results_iter()\n274                - Returns one row at time. At this point the rows are still just\n275                  tuples. In some cases the return values are converted to\n276                  Python values at this location.\n277             3. self.iterator()\n278                - Responsible for turning the rows into model objects.\n279         \"\"\"\n280         self._fetch_all()\n281         return iter(self._result_cache)\n282 \n283     def __bool__(self):\n284         self._fetch_all()\n285         return bool(self._result_cache)\n286 \n287     def __getitem__(self, k):\n288         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n289         if not isinstance(k, (int, slice)):\n290             raise TypeError(\n291                 'QuerySet indices must be integers or slices, not %s.'\n292                 % type(k).__name__\n293             )\n294         assert ((not isinstance(k, slice) and (k >= 0)) or\n295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n296                  (k.stop is None or k.stop >= 0))), \\\n297             \"Negative indexing is not supported.\"\n298 \n299         if self._result_cache is not None:\n300             return self._result_cache[k]\n301 \n302         if isinstance(k, slice):\n303             qs = self._chain()\n304             if k.start is not None:\n305                 start = int(k.start)\n306             else:\n307                 start = None\n308             if k.stop is not None:\n309                 stop = int(k.stop)\n310             else:\n311                 stop = None\n312             qs.query.set_limits(start, stop)\n313             return list(qs)[::k.step] if k.step else qs\n314 \n315         qs = self._chain()\n316         qs.query.set_limits(k, k + 1)\n317         qs._fetch_all()\n318         return qs._result_cache[0]\n319 \n320     def __class_getitem__(cls, *args, **kwargs):\n321         return cls\n322 \n323     def __and__(self, other):\n324         self._merge_sanity_check(other)\n325         if isinstance(other, EmptyQuerySet):\n326             return other\n327         if isinstance(self, EmptyQuerySet):\n328             return self\n329         combined = self._chain()\n330         combined._merge_known_related_objects(other)\n331         combined.query.combine(other.query, sql.AND)\n332         return combined\n333 \n334     def __or__(self, other):\n335         self._merge_sanity_check(other)\n336         if isinstance(self, EmptyQuerySet):\n337             return other\n338         if isinstance(other, EmptyQuerySet):\n339             return self\n340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n341         combined = query._chain()\n342         combined._merge_known_related_objects(other)\n343         if not other.query.can_filter():\n344             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n345         combined.query.combine(other.query, sql.OR)\n346         return combined\n347 \n348     ####################################\n349     # METHODS THAT DO DATABASE QUERIES #\n350     ####################################\n351 \n352     def _iterator(self, use_chunked_fetch, chunk_size):\n353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n354 \n355     def iterator(self, chunk_size=2000):\n356         \"\"\"\n357         An iterator over the results from applying this QuerySet to the\n358         database.\n359         \"\"\"\n360         if chunk_size <= 0:\n361             raise ValueError('Chunk size must be strictly positive.')\n362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n363         return self._iterator(use_chunked_fetch, chunk_size)\n364 \n365     def aggregate(self, *args, **kwargs):\n366         \"\"\"\n367         Return a dictionary containing the calculations (aggregation)\n368         over the current queryset.\n369 \n370         If args is present the expression is passed as a kwarg using\n371         the Aggregate object's default alias.\n372         \"\"\"\n373         if self.query.distinct_fields:\n374             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n376         for arg in args:\n377             # The default_alias property raises TypeError if default_alias\n378             # can't be set automatically or AttributeError if it isn't an\n379             # attribute.\n380             try:\n381                 arg.default_alias\n382             except (AttributeError, TypeError):\n383                 raise TypeError(\"Complex aggregates require an alias\")\n384             kwargs[arg.default_alias] = arg\n385 \n386         query = self.query.chain()\n387         for (alias, aggregate_expr) in kwargs.items():\n388             query.add_annotation(aggregate_expr, alias, is_summary=True)\n389             if not query.annotations[alias].contains_aggregate:\n390                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n391         return query.get_aggregation(self.db, kwargs)\n392 \n393     def count(self):\n394         \"\"\"\n395         Perform a SELECT COUNT() and return the number of records as an\n396         integer.\n397 \n398         If the QuerySet is already fully cached, return the length of the\n399         cached results set to avoid multiple SELECT COUNT(*) calls.\n400         \"\"\"\n401         if self._result_cache is not None:\n402             return len(self._result_cache)\n403 \n404         return self.query.get_count(using=self.db)\n405 \n406     def get(self, *args, **kwargs):\n407         \"\"\"\n408         Perform the query and return a single object matching the given\n409         keyword arguments.\n410         \"\"\"\n411         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n412         if self.query.can_filter() and not self.query.distinct_fields:\n413             clone = clone.order_by()\n414         limit = None\n415         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n416             limit = MAX_GET_RESULTS\n417             clone.query.set_limits(high=limit)\n418         num = len(clone)\n419         if num == 1:\n420             return clone._result_cache[0]\n421         if not num:\n422             raise self.model.DoesNotExist(\n423                 \"%s matching query does not exist.\" %\n424                 self.model._meta.object_name\n425             )\n426         raise self.model.MultipleObjectsReturned(\n427             'get() returned more than one %s -- it returned %s!' % (\n428                 self.model._meta.object_name,\n429                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n430             )\n431         )\n432 \n433     def create(self, **kwargs):\n434         \"\"\"\n435         Create a new object with the given kwargs, saving it to the database\n436         and returning the created object.\n437         \"\"\"\n438         obj = self.model(**kwargs)\n439         self._for_write = True\n440         obj.save(force_insert=True, using=self.db)\n441         return obj\n442 \n443     def _populate_pk_values(self, objs):\n444         for obj in objs:\n445             if obj.pk is None:\n446                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n447 \n448     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n449         \"\"\"\n450         Insert each of the instances into the database. Do *not* call\n451         save() on each of the instances, do not send any pre/post_save\n452         signals, and do not set the primary key attribute if it is an\n453         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n454         Multi-table models are not supported.\n455         \"\"\"\n456         # When you bulk insert you don't get the primary keys back (if it's an\n457         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n458         # you can't insert into the child tables which references this. There\n459         # are two workarounds:\n460         # 1) This could be implemented if you didn't have an autoincrement pk\n461         # 2) You could do it by doing O(n) normal inserts into the parent\n462         #    tables to get the primary keys back and then doing a single bulk\n463         #    insert into the childmost table.\n464         # We currently set the primary keys on the objects when using\n465         # PostgreSQL via the RETURNING ID clause. It should be possible for\n466         # Oracle as well, but the semantics for extracting the primary keys is\n467         # trickier so it's not done yet.\n468         assert batch_size is None or batch_size > 0\n469         # Check that the parents share the same concrete model with the our\n470         # model to detect the inheritance pattern ConcreteGrandParent ->\n471         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n472         # would not identify that case as involving multiple tables.\n473         for parent in self.model._meta.get_parent_list():\n474             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n475                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n476         if not objs:\n477             return objs\n478         self._for_write = True\n479         connection = connections[self.db]\n480         opts = self.model._meta\n481         fields = opts.concrete_fields\n482         objs = list(objs)\n483         self._populate_pk_values(objs)\n484         with transaction.atomic(using=self.db, savepoint=False):\n485             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n486             if objs_with_pk:\n487                 returned_columns = self._batched_insert(\n488                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n489                 )\n490                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n491                     for result, field in zip(results, opts.db_returning_fields):\n492                         if field != opts.pk:\n493                             setattr(obj_with_pk, field.attname, result)\n494                 for obj_with_pk in objs_with_pk:\n495                     obj_with_pk._state.adding = False\n496                     obj_with_pk._state.db = self.db\n497             if objs_without_pk:\n498                 fields = [f for f in fields if not isinstance(f, AutoField)]\n499                 returned_columns = self._batched_insert(\n500                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n501                 )\n502                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n503                     assert len(returned_columns) == len(objs_without_pk)\n504                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n505                     for result, field in zip(results, opts.db_returning_fields):\n506                         setattr(obj_without_pk, field.attname, result)\n507                     obj_without_pk._state.adding = False\n508                     obj_without_pk._state.db = self.db\n509 \n510         return objs\n511 \n512     def bulk_update(self, objs, fields, batch_size=None):\n513         \"\"\"\n514         Update the given fields in each of the given objects in the database.\n515         \"\"\"\n516         if batch_size is not None and batch_size < 0:\n517             raise ValueError('Batch size must be a positive integer.')\n518         if not fields:\n519             raise ValueError('Field names must be given to bulk_update().')\n520         objs = tuple(objs)\n521         if any(obj.pk is None for obj in objs):\n522             raise ValueError('All bulk_update() objects must have a primary key set.')\n523         fields = [self.model._meta.get_field(name) for name in fields]\n524         if any(not f.concrete or f.many_to_many for f in fields):\n525             raise ValueError('bulk_update() can only be used with concrete fields.')\n526         if any(f.primary_key for f in fields):\n527             raise ValueError('bulk_update() cannot be used with primary key fields.')\n528         if not objs:\n529             return\n530         # PK is used twice in the resulting update query, once in the filter\n531         # and once in the WHEN. Each field will also have one CAST.\n532         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n533         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n534         requires_casting = connections[self.db].features.requires_casted_case_in_updates\n535         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n536         updates = []\n537         for batch_objs in batches:\n538             update_kwargs = {}\n539             for field in fields:\n540                 when_statements = []\n541                 for obj in batch_objs:\n542                     attr = getattr(obj, field.attname)\n543                     if not isinstance(attr, Expression):\n544                         attr = Value(attr, output_field=field)\n545                     when_statements.append(When(pk=obj.pk, then=attr))\n546                 case_statement = Case(*when_statements, output_field=field)\n547                 if requires_casting:\n548                     case_statement = Cast(case_statement, output_field=field)\n549                 update_kwargs[field.attname] = case_statement\n550             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n551         with transaction.atomic(using=self.db, savepoint=False):\n552             for pks, update_kwargs in updates:\n553                 self.filter(pk__in=pks).update(**update_kwargs)\n554     bulk_update.alters_data = True\n555 \n556     def get_or_create(self, defaults=None, **kwargs):\n557         \"\"\"\n558         Look up an object with the given kwargs, creating one if necessary.\n559         Return a tuple of (object, created), where created is a boolean\n560         specifying whether an object was created.\n561         \"\"\"\n562         # The get() needs to be targeted at the write database in order\n563         # to avoid potential transaction consistency problems.\n564         self._for_write = True\n565         try:\n566             return self.get(**kwargs), False\n567         except self.model.DoesNotExist:\n568             params = self._extract_model_params(defaults, **kwargs)\n569             # Try to create an object using passed params.\n570             try:\n571                 with transaction.atomic(using=self.db):\n572                     params = dict(resolve_callables(params))\n573                     return self.create(**params), True\n574             except IntegrityError:\n575                 try:\n576                     return self.get(**kwargs), False\n577                 except self.model.DoesNotExist:\n578                     pass\n579                 raise\n580 \n581     def update_or_create(self, defaults=None, **kwargs):\n582         \"\"\"\n583         Look up an object with the given kwargs, updating one with defaults\n584         if it exists, otherwise create a new one.\n585         Return a tuple (object, created), where created is a boolean\n586         specifying whether an object was created.\n587         \"\"\"\n588         defaults = defaults or {}\n589         self._for_write = True\n590         with transaction.atomic(using=self.db):\n591             # Lock the row so that a concurrent update is blocked until\n592             # update_or_create() has performed its save.\n593             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n594             if created:\n595                 return obj, created\n596             for k, v in resolve_callables(defaults):\n597                 setattr(obj, k, v)\n598             obj.save(using=self.db)\n599         return obj, False\n600 \n601     def _extract_model_params(self, defaults, **kwargs):\n602         \"\"\"\n603         Prepare `params` for creating a model instance based on the given\n604         kwargs; for use by get_or_create().\n605         \"\"\"\n606         defaults = defaults or {}\n607         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n608         params.update(defaults)\n609         property_names = self.model._meta._property_names\n610         invalid_params = []\n611         for param in params:\n612             try:\n613                 self.model._meta.get_field(param)\n614             except exceptions.FieldDoesNotExist:\n615                 # It's okay to use a model's property if it has a setter.\n616                 if not (param in property_names and getattr(self.model, param).fset):\n617                     invalid_params.append(param)\n618         if invalid_params:\n619             raise exceptions.FieldError(\n620                 \"Invalid field name(s) for model %s: '%s'.\" % (\n621                     self.model._meta.object_name,\n622                     \"', '\".join(sorted(invalid_params)),\n623                 ))\n624         return params\n625 \n626     def _earliest(self, *fields):\n627         \"\"\"\n628         Return the earliest object according to fields (if given) or by the\n629         model's Meta.get_latest_by.\n630         \"\"\"\n631         if fields:\n632             order_by = fields\n633         else:\n634             order_by = getattr(self.model._meta, 'get_latest_by')\n635             if order_by and not isinstance(order_by, (tuple, list)):\n636                 order_by = (order_by,)\n637         if order_by is None:\n638             raise ValueError(\n639                 \"earliest() and latest() require either fields as positional \"\n640                 \"arguments or 'get_latest_by' in the model's Meta.\"\n641             )\n642 \n643         assert not self.query.is_sliced, \\\n644             \"Cannot change a query once a slice has been taken.\"\n645         obj = self._chain()\n646         obj.query.set_limits(high=1)\n647         obj.query.clear_ordering(force_empty=True)\n648         obj.query.add_ordering(*order_by)\n649         return obj.get()\n650 \n651     def earliest(self, *fields):\n652         return self._earliest(*fields)\n653 \n654     def latest(self, *fields):\n655         return self.reverse()._earliest(*fields)\n656 \n657     def first(self):\n658         \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n659         for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n660             return obj\n661 \n662     def last(self):\n663         \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n664         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n665             return obj\n666 \n667     def in_bulk(self, id_list=None, *, field_name='pk'):\n668         \"\"\"\n669         Return a dictionary mapping each of the given IDs to the object with\n670         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n671         \"\"\"\n672         assert not self.query.is_sliced, \\\n673             \"Cannot use 'limit' or 'offset' with in_bulk\"\n674         opts = self.model._meta\n675         unique_fields = [\n676             constraint.fields[0]\n677             for constraint in opts.total_unique_constraints\n678             if len(constraint.fields) == 1\n679         ]\n680         if (\n681             field_name != 'pk' and\n682             not opts.get_field(field_name).unique and\n683             field_name not in unique_fields and\n684             not self.query.distinct_fields == (field_name,)\n685         ):\n686             raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n687         if id_list is not None:\n688             if not id_list:\n689                 return {}\n690             filter_key = '{}__in'.format(field_name)\n691             batch_size = connections[self.db].features.max_query_params\n692             id_list = tuple(id_list)\n693             # If the database has a limit on the number of query parameters\n694             # (e.g. SQLite), retrieve objects in batches if necessary.\n695             if batch_size and batch_size < len(id_list):\n696                 qs = ()\n697                 for offset in range(0, len(id_list), batch_size):\n698                     batch = id_list[offset:offset + batch_size]\n699                     qs += tuple(self.filter(**{filter_key: batch}).order_by())\n700             else:\n701                 qs = self.filter(**{filter_key: id_list}).order_by()\n702         else:\n703             qs = self._chain()\n704         return {getattr(obj, field_name): obj for obj in qs}\n705 \n706     def delete(self):\n707         \"\"\"Delete the records in the current QuerySet.\"\"\"\n708         self._not_support_combined_queries('delete')\n709         assert not self.query.is_sliced, \\\n710             \"Cannot use 'limit' or 'offset' with delete.\"\n711 \n712         if self._fields is not None:\n713             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n714 \n715         del_query = self._chain()\n716 \n717         # The delete is actually 2 queries - one to find related objects,\n718         # and one to delete. Make sure that the discovery of related\n719         # objects is performed on the same database as the deletion.\n720         del_query._for_write = True\n721 \n722         # Disable non-supported fields.\n723         del_query.query.select_for_update = False\n724         del_query.query.select_related = False\n725         del_query.query.clear_ordering(force_empty=True)\n726 \n727         collector = Collector(using=del_query.db)\n728         collector.collect(del_query)\n729         deleted, _rows_count = collector.delete()\n730 \n731         # Clear the result cache, in case this QuerySet gets reused.\n732         self._result_cache = None\n733         return deleted, _rows_count\n734 \n735     delete.alters_data = True\n736     delete.queryset_only = True\n737 \n738     def _raw_delete(self, using):\n739         \"\"\"\n740         Delete objects found from the given queryset in single direct SQL\n741         query. No signals are sent and there is no protection for cascades.\n742         \"\"\"\n743         query = self.query.clone()\n744         query.__class__ = sql.DeleteQuery\n745         cursor = query.get_compiler(using).execute_sql(CURSOR)\n746         if cursor:\n747             with cursor:\n748                 return cursor.rowcount\n749         return 0\n750     _raw_delete.alters_data = True\n751 \n752     def update(self, **kwargs):\n753         \"\"\"\n754         Update all elements in the current QuerySet, setting all the given\n755         fields to the appropriate values.\n756         \"\"\"\n757         self._not_support_combined_queries('update')\n758         assert not self.query.is_sliced, \\\n759             \"Cannot update a query once a slice has been taken.\"\n760         self._for_write = True\n761         query = self.query.chain(sql.UpdateQuery)\n762         query.add_update_values(kwargs)\n763         # Clear any annotations so that they won't be present in subqueries.\n764         query.annotations = {}\n765         with transaction.mark_for_rollback_on_error(using=self.db):\n766             rows = query.get_compiler(self.db).execute_sql(CURSOR)\n767         self._result_cache = None\n768         return rows\n769     update.alters_data = True\n770 \n771     def _update(self, values):\n772         \"\"\"\n773         A version of update() that accepts field objects instead of field names.\n774         Used primarily for model saving and not intended for use by general\n775         code (it requires too much poking around at model internals to be\n776         useful at that level).\n777         \"\"\"\n778         assert not self.query.is_sliced, \\\n779             \"Cannot update a query once a slice has been taken.\"\n780         query = self.query.chain(sql.UpdateQuery)\n781         query.add_update_fields(values)\n782         # Clear any annotations so that they won't be present in subqueries.\n783         query.annotations = {}\n784         self._result_cache = None\n785         return query.get_compiler(self.db).execute_sql(CURSOR)\n786     _update.alters_data = True\n787     _update.queryset_only = False\n788 \n789     def exists(self):\n790         if self._result_cache is None:\n791             return self.query.has_results(using=self.db)\n792         return bool(self._result_cache)\n793 \n794     def _prefetch_related_objects(self):\n795         # This method can only be called once the result cache has been filled.\n796         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n797         self._prefetch_done = True\n798 \n799     def explain(self, *, format=None, **options):\n800         return self.query.explain(using=self.db, format=format, **options)\n801 \n802     ##################################################\n803     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n804     ##################################################\n805 \n806     def raw(self, raw_query, params=None, translations=None, using=None):\n807         if using is None:\n808             using = self.db\n809         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)\n810         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n811         return qs\n812 \n813     def _values(self, *fields, **expressions):\n814         clone = self._chain()\n815         if expressions:\n816             clone = clone.annotate(**expressions)\n817         clone._fields = fields\n818         clone.query.set_values(fields)\n819         return clone\n820 \n821     def values(self, *fields, **expressions):\n822         fields += tuple(expressions)\n823         clone = self._values(*fields, **expressions)\n824         clone._iterable_class = ValuesIterable\n825         return clone\n826 \n827     def values_list(self, *fields, flat=False, named=False):\n828         if flat and named:\n829             raise TypeError(\"'flat' and 'named' can't be used together.\")\n830         if flat and len(fields) > 1:\n831             raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n832 \n833         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}\n834         _fields = []\n835         expressions = {}\n836         counter = 1\n837         for field in fields:\n838             if hasattr(field, 'resolve_expression'):\n839                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())\n840                 while True:\n841                     field_id = field_id_prefix + str(counter)\n842                     counter += 1\n843                     if field_id not in field_names:\n844                         break\n845                 expressions[field_id] = field\n846                 _fields.append(field_id)\n847             else:\n848                 _fields.append(field)\n849 \n850         clone = self._values(*_fields, **expressions)\n851         clone._iterable_class = (\n852             NamedValuesListIterable if named\n853             else FlatValuesListIterable if flat\n854             else ValuesListIterable\n855         )\n856         return clone\n857 \n858     def dates(self, field_name, kind, order='ASC'):\n859         \"\"\"\n860         Return a list of date objects representing all available dates for\n861         the given field_name, scoped to 'kind'.\n862         \"\"\"\n863         assert kind in ('year', 'month', 'week', 'day'), \\\n864             \"'kind' must be one of 'year', 'month', 'week', or 'day'.\"\n865         assert order in ('ASC', 'DESC'), \\\n866             \"'order' must be either 'ASC' or 'DESC'.\"\n867         return self.annotate(\n868             datefield=Trunc(field_name, kind, output_field=DateField()),\n869             plain_field=F(field_name)\n870         ).values_list(\n871             'datefield', flat=True\n872         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')\n873 \n874     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):\n875         \"\"\"\n876         Return a list of datetime objects representing all available\n877         datetimes for the given field_name, scoped to 'kind'.\n878         \"\"\"\n879         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \\\n880             \"'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'.\"\n881         assert order in ('ASC', 'DESC'), \\\n882             \"'order' must be either 'ASC' or 'DESC'.\"\n883         if settings.USE_TZ:\n884             if tzinfo is None:\n885                 tzinfo = timezone.get_current_timezone()\n886         else:\n887             tzinfo = None\n888         return self.annotate(\n889             datetimefield=Trunc(\n890                 field_name,\n891                 kind,\n892                 output_field=DateTimeField(),\n893                 tzinfo=tzinfo,\n894                 is_dst=is_dst,\n895             ),\n896             plain_field=F(field_name)\n897         ).values_list(\n898             'datetimefield', flat=True\n899         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')\n900 \n901     def none(self):\n902         \"\"\"Return an empty QuerySet.\"\"\"\n903         clone = self._chain()\n904         clone.query.set_empty()\n905         return clone\n906 \n907     ##################################################################\n908     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n909     ##################################################################\n910 \n911     def all(self):\n912         \"\"\"\n913         Return a new QuerySet that is a copy of the current one. This allows a\n914         QuerySet to proxy for a model manager in some cases.\n915         \"\"\"\n916         return self._chain()\n917 \n918     def filter(self, *args, **kwargs):\n919         \"\"\"\n920         Return a new QuerySet instance with the args ANDed to the existing\n921         set.\n922         \"\"\"\n923         self._not_support_combined_queries('filter')\n924         return self._filter_or_exclude(False, args, kwargs)\n925 \n926     def exclude(self, *args, **kwargs):\n927         \"\"\"\n928         Return a new QuerySet instance with NOT (args) ANDed to the existing\n929         set.\n930         \"\"\"\n931         self._not_support_combined_queries('exclude')\n932         return self._filter_or_exclude(True, args, kwargs)\n933 \n934     def _filter_or_exclude(self, negate, args, kwargs):\n935         if args or kwargs:\n936             assert not self.query.is_sliced, \\\n937                 \"Cannot filter a query once a slice has been taken.\"\n938 \n939         clone = self._chain()\n940         if self._defer_next_filter:\n941             self._defer_next_filter = False\n942             clone._deferred_filter = negate, args, kwargs\n943         else:\n944             clone._filter_or_exclude_inplace(negate, args, kwargs)\n945         return clone\n946 \n947     def _filter_or_exclude_inplace(self, negate, args, kwargs):\n948         if negate:\n949             self._query.add_q(~Q(*args, **kwargs))\n950         else:\n951             self._query.add_q(Q(*args, **kwargs))\n952 \n953     def complex_filter(self, filter_obj):\n954         \"\"\"\n955         Return a new QuerySet instance with filter_obj added to the filters.\n956 \n957         filter_obj can be a Q object or a dictionary of keyword lookup\n958         arguments.\n959 \n960         This exists to support framework features such as 'limit_choices_to',\n961         and usually it will be more natural to use other methods.\n962         \"\"\"\n963         if isinstance(filter_obj, Q):\n964             clone = self._chain()\n965             clone.query.add_q(filter_obj)\n966             return clone\n967         else:\n968             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)\n969 \n970     def _combinator_query(self, combinator, *other_qs, all=False):\n971         # Clone the query to inherit the select list and everything\n972         clone = self._chain()\n973         # Clear limits and ordering so they can be reapplied\n974         clone.query.clear_ordering(True)\n975         clone.query.clear_limits()\n976         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)\n977         clone.query.combinator = combinator\n978         clone.query.combinator_all = all\n979         return clone\n980 \n981     def union(self, *other_qs, all=False):\n982         # If the query is an EmptyQuerySet, combine all nonempty querysets.\n983         if isinstance(self, EmptyQuerySet):\n984             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n985             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self\n986         return self._combinator_query('union', *other_qs, all=all)\n987 \n988     def intersection(self, *other_qs):\n989         # If any query is an EmptyQuerySet, return it.\n990         if isinstance(self, EmptyQuerySet):\n991             return self\n992         for other in other_qs:\n993             if isinstance(other, EmptyQuerySet):\n994                 return other\n995         return self._combinator_query('intersection', *other_qs)\n996 \n997     def difference(self, *other_qs):\n998         # If the query is an EmptyQuerySet, return it.\n999         if isinstance(self, EmptyQuerySet):\n1000             return self\n1001         return self._combinator_query('difference', *other_qs)\n1002 \n1003     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):\n1004         \"\"\"\n1005         Return a new QuerySet instance that will select objects with a\n1006         FOR UPDATE lock.\n1007         \"\"\"\n1008         if nowait and skip_locked:\n1009             raise ValueError('The nowait option cannot be used with skip_locked.')\n1010         obj = self._chain()\n1011         obj._for_write = True\n1012         obj.query.select_for_update = True\n1013         obj.query.select_for_update_nowait = nowait\n1014         obj.query.select_for_update_skip_locked = skip_locked\n1015         obj.query.select_for_update_of = of\n1016         obj.query.select_for_no_key_update = no_key\n1017         return obj\n1018 \n1019     def select_related(self, *fields):\n1020         \"\"\"\n1021         Return a new QuerySet instance that will select related objects.\n1022 \n1023         If fields are specified, they must be ForeignKey fields and only those\n1024         related objects are included in the selection.\n1025 \n1026         If select_related(None) is called, clear the list.\n1027         \"\"\"\n1028         self._not_support_combined_queries('select_related')\n1029         if self._fields is not None:\n1030             raise TypeError(\"Cannot call select_related() after .values() or .values_list()\")\n1031 \n1032         obj = self._chain()\n1033         if fields == (None,):\n1034             obj.query.select_related = False\n1035         elif fields:\n1036             obj.query.add_select_related(fields)\n1037         else:\n1038             obj.query.select_related = True\n1039         return obj\n1040 \n1041     def prefetch_related(self, *lookups):\n1042         \"\"\"\n1043         Return a new QuerySet instance that will prefetch the specified\n1044         Many-To-One and Many-To-Many related objects when the QuerySet is\n1045         evaluated.\n1046 \n1047         When prefetch_related() is called more than once, append to the list of\n1048         prefetch lookups. If prefetch_related(None) is called, clear the list.\n1049         \"\"\"\n1050         self._not_support_combined_queries('prefetch_related')\n1051         clone = self._chain()\n1052         if lookups == (None,):\n1053             clone._prefetch_related_lookups = ()\n1054         else:\n1055             for lookup in lookups:\n1056                 if isinstance(lookup, Prefetch):\n1057                     lookup = lookup.prefetch_to\n1058                 lookup = lookup.split(LOOKUP_SEP, 1)[0]\n1059                 if lookup in self.query._filtered_relations:\n1060                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')\n1061             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n1062         return clone\n1063 \n1064     def annotate(self, *args, **kwargs):\n1065         \"\"\"\n1066         Return a query set in which the returned objects have been annotated\n1067         with extra data or aggregations.\n1068         \"\"\"\n1069         self._not_support_combined_queries('annotate')\n1070         return self._annotate(args, kwargs, select=True)\n1071 \n1072     def alias(self, *args, **kwargs):\n1073         \"\"\"\n1074         Return a query set with added aliases for extra data or aggregations.\n1075         \"\"\"\n1076         self._not_support_combined_queries('alias')\n1077         return self._annotate(args, kwargs, select=False)\n1078 \n1079     def _annotate(self, args, kwargs, select=True):\n1080         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n1081         annotations = {}\n1082         for arg in args:\n1083             # The default_alias property may raise a TypeError.\n1084             try:\n1085                 if arg.default_alias in kwargs:\n1086                     raise ValueError(\"The named annotation '%s' conflicts with the \"\n1087                                      \"default name for another annotation.\"\n1088                                      % arg.default_alias)\n1089             except TypeError:\n1090                 raise TypeError(\"Complex annotations require an alias\")\n1091             annotations[arg.default_alias] = arg\n1092         annotations.update(kwargs)\n1093 \n1094         clone = self._chain()\n1095         names = self._fields\n1096         if names is None:\n1097             names = set(chain.from_iterable(\n1098                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n1099                 for field in self.model._meta.get_fields()\n1100             ))\n1101 \n1102         for alias, annotation in annotations.items():\n1103             if alias in names:\n1104                 raise ValueError(\"The annotation '%s' conflicts with a field on \"\n1105                                  \"the model.\" % alias)\n1106             if isinstance(annotation, FilteredRelation):\n1107                 clone.query.add_filtered_relation(annotation, alias)\n1108             else:\n1109                 clone.query.add_annotation(\n1110                     annotation, alias, is_summary=False, select=select,\n1111                 )\n1112         for alias, annotation in clone.query.annotations.items():\n1113             if alias in annotations and annotation.contains_aggregate:\n1114                 if clone._fields is None:\n1115                     clone.query.group_by = True\n1116                 else:\n1117                     clone.query.set_group_by()\n1118                 break\n1119 \n1120         return clone\n1121 \n1122     def order_by(self, *field_names):\n1123         \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n1124         assert not self.query.is_sliced, \\\n1125             \"Cannot reorder a query once a slice has been taken.\"\n1126         obj = self._chain()\n1127         obj.query.clear_ordering(force_empty=False)\n1128         obj.query.add_ordering(*field_names)\n1129         return obj\n1130 \n1131     def distinct(self, *field_names):\n1132         \"\"\"\n1133         Return a new QuerySet instance that will select only distinct results.\n1134         \"\"\"\n1135         self._not_support_combined_queries('distinct')\n1136         assert not self.query.is_sliced, \\\n1137             \"Cannot create distinct fields once a slice has been taken.\"\n1138         obj = self._chain()\n1139         obj.query.add_distinct_fields(*field_names)\n1140         return obj\n1141 \n1142     def extra(self, select=None, where=None, params=None, tables=None,\n1143               order_by=None, select_params=None):\n1144         \"\"\"Add extra SQL fragments to the query.\"\"\"\n1145         self._not_support_combined_queries('extra')\n1146         assert not self.query.is_sliced, \\\n1147             \"Cannot change a query once a slice has been taken\"\n1148         clone = self._chain()\n1149         clone.query.add_extra(select, select_params, where, params, tables, order_by)\n1150         return clone\n1151 \n1152     def reverse(self):\n1153         \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n1154         if self.query.is_sliced:\n1155             raise TypeError('Cannot reverse a query once a slice has been taken.')\n1156         clone = self._chain()\n1157         clone.query.standard_ordering = not clone.query.standard_ordering\n1158         return clone\n1159 \n1160     def defer(self, *fields):\n1161         \"\"\"\n1162         Defer the loading of data for certain fields until they are accessed.\n1163         Add the set of deferred fields to any existing set of deferred fields.\n1164         The only exception to this is if None is passed in as the only\n1165         parameter, in which case removal all deferrals.\n1166         \"\"\"\n1167         self._not_support_combined_queries('defer')\n1168         if self._fields is not None:\n1169             raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n1170         clone = self._chain()\n1171         if fields == (None,):\n1172             clone.query.clear_deferred_loading()\n1173         else:\n1174             clone.query.add_deferred_loading(fields)\n1175         return clone\n1176 \n1177     def only(self, *fields):\n1178         \"\"\"\n1179         Essentially, the opposite of defer(). Only the fields passed into this\n1180         method and that are not already specified as deferred are loaded\n1181         immediately when the queryset is evaluated.\n1182         \"\"\"\n1183         self._not_support_combined_queries('only')\n1184         if self._fields is not None:\n1185             raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n1186         if fields == (None,):\n1187             # Can only pass None to defer(), not only(), as the rest option.\n1188             # That won't stop people trying to do this, so let's be explicit.\n1189             raise TypeError(\"Cannot pass None as an argument to only().\")\n1190         for field in fields:\n1191             field = field.split(LOOKUP_SEP, 1)[0]\n1192             if field in self.query._filtered_relations:\n1193                 raise ValueError('only() is not supported with FilteredRelation.')\n1194         clone = self._chain()\n1195         clone.query.add_immediate_loading(fields)\n1196         return clone\n1197 \n1198     def using(self, alias):\n1199         \"\"\"Select which database this QuerySet should execute against.\"\"\"\n1200         clone = self._chain()\n1201         clone._db = alias\n1202         return clone\n1203 \n1204     ###################################\n1205     # PUBLIC INTROSPECTION ATTRIBUTES #\n1206     ###################################\n1207 \n1208     @property\n1209     def ordered(self):\n1210         \"\"\"\n1211         Return True if the QuerySet is ordered -- i.e. has an order_by()\n1212         clause or a default ordering on the model (or is empty).\n1213         \"\"\"\n1214         if isinstance(self, EmptyQuerySet):\n1215             return True\n1216         if self.query.extra_order_by or self.query.order_by:\n1217             return True\n1218         elif (\n1219             self.query.default_ordering and\n1220             self.query.get_meta().ordering and\n1221             # A default ordering doesn't affect GROUP BY queries.\n1222             not self.query.group_by\n1223         ):\n1224             return True\n1225         else:\n1226             return False\n1227 \n1228     @property\n1229     def db(self):\n1230         \"\"\"Return the database used if this query is executed now.\"\"\"\n1231         if self._for_write:\n1232             return self._db or router.db_for_write(self.model, **self._hints)\n1233         return self._db or router.db_for_read(self.model, **self._hints)\n1234 \n1235     ###################\n1236     # PRIVATE METHODS #\n1237     ###################\n1238 \n1239     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):\n1240         \"\"\"\n1241         Insert a new record for the given model. This provides an interface to\n1242         the InsertQuery class and is how Model.save() is implemented.\n1243         \"\"\"\n1244         self._for_write = True\n1245         if using is None:\n1246             using = self.db\n1247         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)\n1248         query.insert_values(fields, objs, raw=raw)\n1249         return query.get_compiler(using=using).execute_sql(returning_fields)\n1250     _insert.alters_data = True\n1251     _insert.queryset_only = False\n1252 \n1253     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):\n1254         \"\"\"\n1255         Helper method for bulk_create() to insert objs one batch at a time.\n1256         \"\"\"\n1257         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n1258             raise NotSupportedError('This database backend does not support ignoring conflicts.')\n1259         ops = connections[self.db].ops\n1260         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n1261         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n1262         inserted_rows = []\n1263         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n1264         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n1265             if bulk_return and not ignore_conflicts:\n1266                 inserted_rows.extend(self._insert(\n1267                     item, fields=fields, using=self.db,\n1268                     returning_fields=self.model._meta.db_returning_fields,\n1269                     ignore_conflicts=ignore_conflicts,\n1270                 ))\n1271             else:\n1272                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)\n1273         return inserted_rows\n1274 \n1275     def _chain(self, **kwargs):\n1276         \"\"\"\n1277         Return a copy of the current QuerySet that's ready for another\n1278         operation.\n1279         \"\"\"\n1280         obj = self._clone()\n1281         if obj._sticky_filter:\n1282             obj.query.filter_is_sticky = True\n1283             obj._sticky_filter = False\n1284         obj.__dict__.update(kwargs)\n1285         return obj\n1286 \n1287     def _clone(self):\n1288         \"\"\"\n1289         Return a copy of the current QuerySet. A lightweight alternative\n1290         to deepcopy().\n1291         \"\"\"\n1292         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)\n1293         c._sticky_filter = self._sticky_filter\n1294         c._for_write = self._for_write\n1295         c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n1296         c._known_related_objects = self._known_related_objects\n1297         c._iterable_class = self._iterable_class\n1298         c._fields = self._fields\n1299         return c\n1300 \n1301     def _fetch_all(self):\n1302         if self._result_cache is None:\n1303             self._result_cache = list(self._iterable_class(self))\n1304         if self._prefetch_related_lookups and not self._prefetch_done:\n1305             self._prefetch_related_objects()\n1306 \n1307     def _next_is_sticky(self):\n1308         \"\"\"\n1309         Indicate that the next filter call and the one following that should\n1310         be treated as a single filter. This is only important when it comes to\n1311         determining when to reuse tables for many-to-many filters. Required so\n1312         that we can filter naturally on the results of related managers.\n1313 \n1314         This doesn't return a clone of the current QuerySet (it returns\n1315         \"self\"). The method is only used internally and should be immediately\n1316         followed by a filter() that does create a clone.\n1317         \"\"\"\n1318         self._sticky_filter = True\n1319         return self\n1320 \n1321     def _merge_sanity_check(self, other):\n1322         \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n1323         if self._fields is not None and (\n1324                 set(self.query.values_select) != set(other.query.values_select) or\n1325                 set(self.query.extra_select) != set(other.query.extra_select) or\n1326                 set(self.query.annotation_select) != set(other.query.annotation_select)):\n1327             raise TypeError(\n1328                 \"Merging '%s' classes must involve the same values in each case.\"\n1329                 % self.__class__.__name__\n1330             )\n1331 \n1332     def _merge_known_related_objects(self, other):\n1333         \"\"\"\n1334         Keep track of all known related objects from either QuerySet instance.\n1335         \"\"\"\n1336         for field, objects in other._known_related_objects.items():\n1337             self._known_related_objects.setdefault(field, {}).update(objects)\n1338 \n1339     def resolve_expression(self, *args, **kwargs):\n1340         if self._fields and len(self._fields) > 1:\n1341             # values() queryset can only be used as nested queries\n1342             # if they are set up to select only a single field.\n1343             raise TypeError('Cannot use multi-field values as a filter value.')\n1344         query = self.query.resolve_expression(*args, **kwargs)\n1345         query._db = self._db\n1346         return query\n1347     resolve_expression.queryset_only = True\n1348 \n1349     def _add_hints(self, **hints):\n1350         \"\"\"\n1351         Update hinting information for use by routers. Add new key/values or\n1352         overwrite existing key/values.\n1353         \"\"\"\n1354         self._hints.update(hints)\n1355 \n1356     def _has_filters(self):\n1357         \"\"\"\n1358         Check if this QuerySet has any filtering going on. This isn't\n1359         equivalent with checking if all objects are present in results, for\n1360         example, qs[1:]._has_filters() -> False.\n1361         \"\"\"\n1362         return self.query.has_filters()\n1363 \n1364     @staticmethod\n1365     def _validate_values_are_expressions(values, method_name):\n1366         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))\n1367         if invalid_args:\n1368             raise TypeError(\n1369                 'QuerySet.%s() received non-expression(s): %s.' % (\n1370                     method_name,\n1371                     ', '.join(invalid_args),\n1372                 )\n1373             )\n1374 \n1375     def _not_support_combined_queries(self, operation_name):\n1376         if self.query.combinator:\n1377             raise NotSupportedError(\n1378                 'Calling QuerySet.%s() after %s() is not supported.'\n1379                 % (operation_name, self.query.combinator)\n1380             )\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]