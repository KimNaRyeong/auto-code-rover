2025-10-15 08:42:39.325 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 08:42:41.808 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=611, output_tokens=12, cost=0.006470
2025-10-15 08:42:54.981 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=791, output_tokens=489, cost=0.022580
2025-10-15 08:42:56.812 | INFO     | registered non reproducing test 0
2025-10-15 08:43:13.383 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2150, output_tokens=529, cost=0.037370
2025-10-15 08:43:15.173 | INFO     | registered non reproducing test 1
2025-10-15 08:43:38.096 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2630, output_tokens=685, cost=0.046850
2025-10-15 08:43:39.953 | INFO     | registered non reproducing test 2
2025-10-15 08:43:39.953 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 08:43:39.959 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 08:43:47.605 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=918, output_tokens=233, cost=0.016170
2025-10-15 08:43:47.609 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 08:43:50.609 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=587, output_tokens=54, cost=0.007490
2025-10-15 08:43:50.611 | DEBUG    | Extracted a valid json.
2025-10-15 08:43:50.611 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"DataArray\")",
        "search_method_in_class(\"astype\", \"DataArray\")",
        "search_code(\"dtype='d', order='F'\")"
    ],
    "bug_locations": []
}
2025-10-15 08:43:50.706 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 08:43:50.707 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 08:44:08.195 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5466, output_tokens=553, cost=0.071250
2025-10-15 08:44:08.207 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 08:44:10.492 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6235, output_tokens=18, cost=0.062890
2025-10-15 08:44:10.495 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 08:44:12.339 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=372, output_tokens=31, cost=0.004650
2025-10-15 08:44:12.340 | DEBUG    | Extracted a valid json.
2025-10-15 08:44:12.340 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_class(\"AbstractArray\")", "search_code(\"def astype(\")"],
    "bug_locations": []
}
2025-10-15 08:44:12.373 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 08:44:12.374 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 08:44:29.243 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6871, output_tokens=514, cost=0.084130
2025-10-15 08:44:29.254 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 08:44:41.651 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=7601, output_tokens=388, cost=0.087650
2025-10-15 08:44:41.660 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 08:44:51.286 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=742, output_tokens=270, cost=0.015520
2025-10-15 08:44:51.287 | DEBUG    | Extracted a valid json.
2025-10-15 08:44:51.287 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/variable.py",
            "class": "Variable",
            "method": "astype",
            "intended_behavior": "The `astype` method should be updated to include an `order` parameter, aligning its functionality with that of `numpy.ndarray.astype`. This method should then handle the `order` parameter appropriately when casting the data type of the arrays. The implementation should ensure that when the `order` parameter is specified, it affects the memory layout of the returned array as expected ('C' for row-major and 'F' for column-major order, etc.). This change must preserve the existing functionality of allowing data type casting while extending it to support order specification."
        },
        {
            "file": "xarray/core/common.py",
            "class": "DataWithCoords",
            "method": "astype",
            "intended_behavior": "Similar to `Variable.astype`, any `astype` method shared across xarray objects which does not currently support the `order` parameter should be modified to include and appropriately handle this parameter. The implementation should ensure that the `order` parameter meaningfully alters the memory layout of the array, respecting the underlying data handling and representation logic in xarray."
        }
    ]
}
2025-10-15 08:44:51.306 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/variable.py</file>
<class>Variable</class>
<method>astype</method>
<code>
373     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
374         """
375         Copy of the Variable object, with data cast to a specified type.
376 
377         Parameters
378         ----------
379         dtype : str or dtype
380              Typecode or data-type to which the array is cast.
381         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
382              Controls what kind of data casting may occur. Defaults to 'unsafe'
383              for backwards compatibility.
384 
385              * 'no' means the data types should not be cast at all.
386              * 'equiv' means only byte-order changes are allowed.
387              * 'safe' means only casts which can preserve values are allowed.
388              * 'same_kind' means only safe casts or casts within a kind,
389                  like float64 to float32, are allowed.
390              * 'unsafe' means any data conversions may be done.
391         copy : bool, optional
392              By default, astype always returns a newly allocated array. If this
393              is set to False and the `dtype` requirement is satisfied, the input
394              array is returned instead of a copy.
395         keep_attrs : bool, optional
396             By default, astype keeps attributes. Set to False to remove
397             attributes in the returned object.
398 
399         Returns
400         -------
401         out : same as object
402             New object with data cast to the specified type.
403 
404         See also
405         --------
406         np.ndarray.astype
407         dask.array.Array.astype
408         """
409         from .computation import apply_ufunc
410 
411         return apply_ufunc(
412             duck_array_ops.astype,
413             self,
414             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
415             keep_attrs=keep_attrs,
416             dask="allowed",
417         )

</code><intended_behavior>The `astype` method should be updated to include an `order` parameter, aligning its functionality with that of `numpy.ndarray.astype`. This method should then handle the `order` parameter appropriately when casting the data type of the arrays. The implementation should ensure that when the `order` parameter is specified, it affects the memory layout of the returned array as expected ('C' for row-major and 'F' for column-major order, etc.). This change must preserve the existing functionality of allowing data type casting while extending it to support order specification.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
283 class Variable(
284     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
285 ):
286     """A netcdf-like variable consisting of dimensions, data and attributes
287     which describe a single Array. A single Variable object is not fully
288     described outside the context of its parent Dataset (if you want such a
289     fully described object, use a DataArray instead).
290 
291     The main functional difference between Variables and numpy arrays is that
292     numerical operations on Variables implement array broadcasting by dimension
293     name. For example, adding an Variable with dimensions `('time',)` to
294     another Variable with dimensions `('space',)` results in a new Variable
295     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
296     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
297     instead of an "axis".
298 
299     Variables are light-weight objects used as the building block for datasets.
300     They are more primitive objects, so operations with them provide marginally
301     higher performance than using DataArrays. However, manipulating data in the
302     form of a Dataset or DataArray should almost always be preferred, because
303     they can use more complete metadata in context of coordinate labels.
304     """
305 
306     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
307 
308     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
309         """
310         Parameters
311         ----------
312         dims : str or sequence of str
313             Name(s) of the the data dimension(s). Must be either a string (only
314             for 1D data) or a sequence of strings with length equal to the
315             number of dimensions.
316         data : array_like
317             Data array which supports numpy-like data access.
318         attrs : dict_like or None, optional
319             Attributes to assign to the new variable. If None (default), an
320             empty attribute dictionary is initialized.
321         encoding : dict_like or None, optional
322             Dictionary specifying how to encode this array's data into a
323             serialized format like netCDF4. Currently used keys (for netCDF)
324             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
325             Well-behaved code to serialize a Variable should ignore
326             unrecognized encoding items.
327         """
328         self._data = as_compatible_data(data, fastpath=fastpath)
329         self._dims = self._parse_dimensions(dims)
330         self._attrs = None
331         self._encoding = None
332         if attrs is not None:
333             self.attrs = attrs
334         if encoding is not None:
335             self.encoding = encoding
336 
337     @property
338     def dtype(self):
339         return self._data.dtype
340 
341     @property
342     def shape(self):
343         return self._data.shape
344 
345     @property
346     def nbytes(self):
347         return self.size * self.dtype.itemsize
348 
349     @property
350     def _in_memory(self):
351         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
352             isinstance(self._data, indexing.MemoryCachedArray)
353             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
354         )
355 
356     @property
357     def data(self):
358         if is_duck_array(self._data):
359             return self._data
360         else:
361             return self.values
362 
363     @data.setter
364     def data(self, data):
365         data = as_compatible_data(data)
366         if data.shape != self.shape:
367             raise ValueError(
368                 f"replacement data must match the Variable's shape. "
369                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
370             )
371         self._data = data
372 
373     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
374         """
375         Copy of the Variable object, with data cast to a specified type.
376 
377         Parameters
378         ----------
379         dtype : str or dtype
380              Typecode or data-type to which the array is cast.
381         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
382              Controls what kind of data casting may occur. Defaults to 'unsafe'
383              for backwards compatibility.
384 
385              * 'no' means the data types should not be cast at all.
386              * 'equiv' means only byte-order changes are allowed.
387              * 'safe' means only casts which can preserve values are allowed.
388              * 'same_kind' means only safe casts or casts within a kind,
389                  like float64 to float32, are allowed.
390              * 'unsafe' means any data conversions may be done.
391         copy : bool, optional
392              By default, astype always returns a newly allocated array. If this
393              is set to False and the `dtype` requirement is satisfied, the input
394              array is returned instead of a copy.
395         keep_attrs : bool, optional
396             By default, astype keeps attributes. Set to False to remove
397             attributes in the returned object.
398 
399         Returns
400         -------
401         out : same as object
402             New object with data cast to the specified type.
403 
404         See also
405         --------
406         np.ndarray.astype
407         dask.array.Array.astype
408         """
409         from .computation import apply_ufunc
410 
411         return apply_ufunc(
412             duck_array_ops.astype,
413             self,
414             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
415             keep_attrs=keep_attrs,
416             dask="allowed",
417         )
418 
419     def load(self, **kwargs):
420         """Manually trigger loading of this variable's data from disk or a
421         remote source into memory and return this variable.
422 
423         Normally, it should not be necessary to call this method in user code,
424         because all xarray functions should either work on deferred data or
425         load data automatically.
426 
427         Parameters
428         ----------
429         **kwargs : dict
430             Additional keyword arguments passed on to ``dask.array.compute``.
431 
432         See Also
433         --------
434         dask.array.compute
435         """
436         if is_duck_dask_array(self._data):
437             self._data = as_compatible_data(self._data.compute(**kwargs))
438         elif not is_duck_array(self._data):
439             self._data = np.asarray(self._data)
440         return self
441 
442     def compute(self, **kwargs):
443         """Manually trigger loading of this variable's data from disk or a
444         remote source into memory and return a new variable. The original is
445         left unaltered.
446 
447         Normally, it should not be necessary to call this method in user code,
448         because all xarray functions should either work on deferred data or
449         load data automatically.
450 
451         Parameters
452         ----------
453         **kwargs : dict
454             Additional keyword arguments passed on to ``dask.array.compute``.
455 
456         See Also
457         --------
458         dask.array.compute
459         """
460         new = self.copy(deep=False)
461         return new.load(**kwargs)
462 
463     def __dask_tokenize__(self):
464         # Use v.data, instead of v._data, in order to cope with the wrappers
465         # around NetCDF and the like
466         from dask.base import normalize_token
467 
468         return normalize_token((type(self), self._dims, self.data, self._attrs))
469 
470     def __dask_graph__(self):
471         if is_duck_dask_array(self._data):
472             return self._data.__dask_graph__()
473         else:
474             return None
475 
476     def __dask_keys__(self):
477         return self._data.__dask_keys__()
478 
479     def __dask_layers__(self):
480         return self._data.__dask_layers__()
481 
482     @property
483     def __dask_optimize__(self):
484         return self._data.__dask_optimize__
485 
486     @property
487     def __dask_scheduler__(self):
488         return self._data.__dask_scheduler__
489 
490     def __dask_postcompute__(self):
491         array_func, array_args = self._data.__dask_postcompute__()
492         return (
493             self._dask_finalize,
494             (array_func, array_args, self._dims, self._attrs, self._encoding),
495         )
496 
497     def __dask_postpersist__(self):
498         array_func, array_args = self._data.__dask_postpersist__()
499         return (
500             self._dask_finalize,
501             (array_func, array_args, self._dims, self._attrs, self._encoding),
502         )
503 
504     @staticmethod
505     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
506         data = array_func(results, *array_args)
507         return Variable(dims, data, attrs=attrs, encoding=encoding)
508 
509     @property
510     def values(self):
511         """The variable's data as a numpy.ndarray"""
512         return _as_array_or_item(self._data)
513 
514     @values.setter
515     def values(self, values):
516         self.data = values
517 
518     def to_base_variable(self):
519         """Return this variable as a base xarray.Variable"""
520         return Variable(
521             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
522         )
523 
524     to_variable = utils.alias(to_base_variable, "to_variable")
525 
526     def to_index_variable(self):
527         """Return this variable as an xarray.IndexVariable"""
528         return IndexVariable(
529             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
530         )
531 
532     to_coord = utils.alias(to_index_variable, "to_coord")
533 
534     def to_index(self):
535         """Convert this variable to a pandas.Index"""
536         return self.to_index_variable().to_index()
537 
538     def to_dict(self, data=True):
539         """Dictionary representation of variable."""
540         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
541         if data:
542             item["data"] = ensure_us_time_resolution(self.values).tolist()
543         else:
544             item.update({"dtype": str(self.dtype), "shape": self.shape})
545         return item
546 
547     @property
548     def dims(self):
549         """Tuple of dimension names with which this variable is associated."""
550         return self._dims
551 
552     @dims.setter
553     def dims(self, value):
554         self._dims = self._parse_dimensions(value)
555 
556     def _parse_dimensions(self, dims):
557         if isinstance(dims, str):
558             dims = (dims,)
559         dims = tuple(dims)
560         if len(dims) != self.ndim:
561             raise ValueError(
562                 "dimensions %s must have the same length as the "
563                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
564             )
565         return dims
566 
567     def _item_key_to_tuple(self, key):
568         if utils.is_dict_like(key):
569             return tuple(key.get(dim, slice(None)) for dim in self.dims)
570         else:
571             return key
572 
573     def _broadcast_indexes(self, key):
574         """Prepare an indexing key for an indexing operation.
575 
576         Parameters
577         -----------
578         key: int, slice, array-like, dict or tuple of integer, slice and array-like
579             Any valid input for indexing.
580 
581         Returns
582         -------
583         dims : tuple
584             Dimension of the resultant variable.
585         indexers : IndexingTuple subclass
586             Tuple of integer, array-like, or slices to use when indexing
587             self._data. The type of this argument indicates the type of
588             indexing to perform, either basic, outer or vectorized.
589         new_order : Optional[Sequence[int]]
590             Optional reordering to do on the result of indexing. If not None,
591             the first len(new_order) indexing should be moved to these
592             positions.
593         """
594         key = self._item_key_to_tuple(key)  # key is a tuple
595         # key is a tuple of full size
596         key = indexing.expanded_indexer(key, self.ndim)
597         # Convert a scalar Variable to an integer
598         key = tuple(
599             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
600         )
601         # Convert a 0d-array to an integer
602         key = tuple(
603             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
604         )
605 
606         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
607             return self._broadcast_indexes_basic(key)
608 
609         self._validate_indexers(key)
610         # Detect it can be mapped as an outer indexer
611         # If all key is unlabeled, or
612         # key can be mapped as an OuterIndexer.
613         if all(not isinstance(k, Variable) for k in key):
614             return self._broadcast_indexes_outer(key)
615 
616         # If all key is 1-dimensional and there are no duplicate labels,
617         # key can be mapped as an OuterIndexer.
618         dims = []
619         for k, d in zip(key, self.dims):
620             if isinstance(k, Variable):
621                 if len(k.dims) > 1:
622                     return self._broadcast_indexes_vectorized(key)
623                 dims.append(k.dims[0])
624             elif not isinstance(k, integer_types):
625                 dims.append(d)
626         if len(set(dims)) == len(dims):
627             return self._broadcast_indexes_outer(key)
628 
629         return self._broadcast_indexes_vectorized(key)
630 
631     def _broadcast_indexes_basic(self, key):
632         dims = tuple(
633             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
634         )
635         return dims, BasicIndexer(key), None
636 
637     def _validate_indexers(self, key):
638         """ Make sanity checks """
639         for dim, k in zip(self.dims, key):
640             if isinstance(k, BASIC_INDEXING_TYPES):
641                 pass
642             else:
643                 if not isinstance(k, Variable):
644                     k = np.asarray(k)
645                     if k.ndim > 1:
646                         raise IndexError(
647                             "Unlabeled multi-dimensional array cannot be "
648                             "used for indexing: {}".format(k)
649                         )
650                 if k.dtype.kind == "b":
651                     if self.shape[self.get_axis_num(dim)] != len(k):
652                         raise IndexError(
653                             "Boolean array size {:d} is used to index array "
654                             "with shape {:s}.".format(len(k), str(self.shape))
655                         )
656                     if k.ndim > 1:
657                         raise IndexError(
658                             "{}-dimensional boolean indexing is "
659                             "not supported. ".format(k.ndim)
660                         )
661                     if getattr(k, "dims", (dim,)) != (dim,):
662                         raise IndexError(
663                             "Boolean indexer should be unlabeled or on the "
664                             "same dimension to the indexed array. Indexer is "
665                             "on {:s} but the target dimension is {:s}.".format(
666                                 str(k.dims), dim
667                             )
668                         )
669 
670     def _broadcast_indexes_outer(self, key):
671         dims = tuple(
672             k.dims[0] if isinstance(k, Variable) else dim
673             for k, dim in zip(key, self.dims)
674             if not isinstance(k, integer_types)
675         )
676 
677         new_key = []
678         for k in key:
679             if isinstance(k, Variable):
680                 k = k.data
681             if not isinstance(k, BASIC_INDEXING_TYPES):
682                 k = np.asarray(k)
683                 if k.size == 0:
684                     # Slice by empty list; numpy could not infer the dtype
685                     k = k.astype(int)
686                 elif k.dtype.kind == "b":
687                     (k,) = np.nonzero(k)
688             new_key.append(k)
689 
690         return dims, OuterIndexer(tuple(new_key)), None
691 
692     def _nonzero(self):
693         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
694         # TODO we should replace dask's native nonzero
695         # after https://github.com/dask/dask/issues/1076 is implemented.
696         nonzeros = np.nonzero(self.data)
697         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
698 
699     def _broadcast_indexes_vectorized(self, key):
700         variables = []
701         out_dims_set = OrderedSet()
702         for dim, value in zip(self.dims, key):
703             if isinstance(value, slice):
704                 out_dims_set.add(dim)
705             else:
706                 variable = (
707                     value
708                     if isinstance(value, Variable)
709                     else as_variable(value, name=dim)
710                 )
711                 if variable.dtype.kind == "b":  # boolean indexing case
712                     (variable,) = variable._nonzero()
713 
714                 variables.append(variable)
715                 out_dims_set.update(variable.dims)
716 
717         variable_dims = set()
718         for variable in variables:
719             variable_dims.update(variable.dims)
720 
721         slices = []
722         for i, (dim, value) in enumerate(zip(self.dims, key)):
723             if isinstance(value, slice):
724                 if dim in variable_dims:
725                     # We only convert slice objects to variables if they share
726                     # a dimension with at least one other variable. Otherwise,
727                     # we can equivalently leave them as slices aknd transpose
728                     # the result. This is significantly faster/more efficient
729                     # for most array backends.
730                     values = np.arange(*value.indices(self.sizes[dim]))
731                     variables.insert(i - len(slices), Variable((dim,), values))
732                 else:
733                     slices.append((i, value))
734 
735         try:
736             variables = _broadcast_compat_variables(*variables)
737         except ValueError:
738             raise IndexError(f"Dimensions of indexers mismatch: {key}")
739 
740         out_key = [variable.data for variable in variables]
741         out_dims = tuple(out_dims_set)
742         slice_positions = set()
743         for i, value in slices:
744             out_key.insert(i, value)
745             new_position = out_dims.index(self.dims[i])
746             slice_positions.add(new_position)
747 
748         if slice_positions:
749             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
750         else:
751             new_order = None
752 
753         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
754 
755     def __getitem__(self: VariableType, key) -> VariableType:
756         """Return a new Variable object whose contents are consistent with
757         getting the provided key from the underlying data.
758 
759         NB. __getitem__ and __setitem__ implement xarray-style indexing,
760         where if keys are unlabeled arrays, we index the array orthogonally
761         with them. If keys are labeled array (such as Variables), they are
762         broadcasted with our usual scheme and then the array is indexed with
763         the broadcasted key, like numpy's fancy indexing.
764 
765         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
766         array `x.values` directly.
767         """
768         dims, indexer, new_order = self._broadcast_indexes(key)
769         data = as_indexable(self._data)[indexer]
770         if new_order:
771             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
772         return self._finalize_indexing_result(dims, data)
773 
774     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
775         """Used by IndexVariable to return IndexVariable objects when possible."""
776         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
777 
778     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
779         """Index this Variable with -1 remapped to fill_value."""
780         # TODO(shoyer): expose this method in public API somewhere (isel?) and
781         # use it for reindex.
782         # TODO(shoyer): add a sanity check that all other integers are
783         # non-negative
784         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
785         # that is actually indexed rather than mapping it to the last value
786         # along each axis.
787 
788         if fill_value is dtypes.NA:
789             fill_value = dtypes.get_fill_value(self.dtype)
790 
791         dims, indexer, new_order = self._broadcast_indexes(key)
792 
793         if self.size:
794             if is_duck_dask_array(self._data):
795                 # dask's indexing is faster this way; also vindex does not
796                 # support negative indices yet:
797                 # https://github.com/dask/dask/pull/2967
798                 actual_indexer = indexing.posify_mask_indexer(indexer)
799             else:
800                 actual_indexer = indexer
801 
802             data = as_indexable(self._data)[actual_indexer]
803             mask = indexing.create_mask(indexer, self.shape, data)
804             # we need to invert the mask in order to pass data first. This helps
805             # pint to choose the correct unit
806             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed
807             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)
808         else:
809             # array cannot be indexed along dimensions of size 0, so just
810             # build the mask directly instead.
811             mask = indexing.create_mask(indexer, self.shape)
812             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
813 
814         if new_order:
815             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
816         return self._finalize_indexing_result(dims, data)
817 
818     def __setitem__(self, key, value):
819         """__setitem__ is overloaded to access the underlying numpy values with
820         orthogonal indexing.
821 
822         See __getitem__ for more details.
823         """
824         dims, index_tuple, new_order = self._broadcast_indexes(key)
825 
826         if not isinstance(value, Variable):
827             value = as_compatible_data(value)
828             if value.ndim > len(dims):
829                 raise ValueError(
830                     "shape mismatch: value array of shape %s could not be "
831                     "broadcast to indexing result with %s dimensions"
832                     % (value.shape, len(dims))
833                 )
834             if value.ndim == 0:
835                 value = Variable((), value)
836             else:
837                 value = Variable(dims[-value.ndim :], value)
838         # broadcast to become assignable
839         value = value.set_dims(dims).data
840 
841         if new_order:
842             value = duck_array_ops.asarray(value)
843             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
844             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
845 
846         indexable = as_indexable(self._data)
847         indexable[index_tuple] = value
848 
849     @property
850     def attrs(self) -> Dict[Hashable, Any]:
851         """Dictionary of local attributes on this variable."""
852         if self._attrs is None:
853             self._attrs = {}
854         return self._attrs
855 
856     @attrs.setter
857     def attrs(self, value: Mapping[Hashable, Any]) -> None:
858         self._attrs = dict(value)
859 
860     @property
861     def encoding(self):
862         """Dictionary of encodings on this variable."""
863         if self._encoding is None:
864             self._encoding = {}
865         return self._encoding
866 
867     @encoding.setter
868     def encoding(self, value):
869         try:
870             self._encoding = dict(value)
871         except ValueError:
872             raise ValueError("encoding must be castable to a dictionary")
873 
874     def copy(self, deep=True, data=None):
875         """Returns a copy of this object.
876 
877         If `deep=True`, the data array is loaded into memory and copied onto
878         the new object. Dimensions, attributes and encodings are always copied.
879 
880         Use `data` to create a new object with the same structure as
881         original but entirely new data.
882 
883         Parameters
884         ----------
885         deep : bool, optional
886             Whether the data array is loaded into memory and copied onto
887             the new object. Default is True.
888         data : array_like, optional
889             Data to use in the new object. Must have same shape as original.
890             When `data` is used, `deep` is ignored.
891 
892         Returns
893         -------
894         object : Variable
895             New object with dimensions, attributes, encodings, and optionally
896             data copied from original.
897 
898         Examples
899         --------
900 
901         Shallow copy versus deep copy
902 
903         >>> var = xr.Variable(data=[1, 2, 3], dims="x")
904         >>> var.copy()
905         <xarray.Variable (x: 3)>
906         array([1, 2, 3])
907         >>> var_0 = var.copy(deep=False)
908         >>> var_0[0] = 7
909         >>> var_0
910         <xarray.Variable (x: 3)>
911         array([7, 2, 3])
912         >>> var
913         <xarray.Variable (x: 3)>
914         array([7, 2, 3])
915 
916         Changing the data using the ``data`` argument maintains the
917         structure of the original object, but with the new data. Original
918         object is unaffected.
919 
920         >>> var.copy(data=[0.1, 0.2, 0.3])
921         <xarray.Variable (x: 3)>
922         array([0.1, 0.2, 0.3])
923         >>> var
924         <xarray.Variable (x: 3)>
925         array([7, 2, 3])
926 
927         See Also
928         --------
929         pandas.DataFrame.copy
930         """
931         if data is None:
932             data = self._data
933 
934             if isinstance(data, indexing.MemoryCachedArray):
935                 # don't share caching between copies
936                 data = indexing.MemoryCachedArray(data.array)
937 
938             if deep:
939                 data = copy.deepcopy(data)
940 
941         else:
942             data = as_compatible_data(data)
943             if self.shape != data.shape:
944                 raise ValueError(
945                     "Data shape {} must match shape of object {}".format(
946                         data.shape, self.shape
947                     )
948                 )
949 
950         # note:
951         # dims is already an immutable tuple
952         # attributes and encoding will be copied when the new Array is created
953         return self._replace(data=data)
954 
955     def _replace(
956         self, dims=_default, data=_default, attrs=_default, encoding=_default
957     ) -> "Variable":
958         if dims is _default:
959             dims = copy.copy(self._dims)
960         if data is _default:
961             data = copy.copy(self.data)
962         if attrs is _default:
963             attrs = copy.copy(self._attrs)
964         if encoding is _default:
965             encoding = copy.copy(self._encoding)
966         return type(self)(dims, data, attrs, encoding, fastpath=True)
967 
968     def __copy__(self):
969         return self.copy(deep=False)
970 
971     def __deepcopy__(self, memo=None):
972         # memo does nothing but is required for compatibility with
973         # copy.deepcopy
974         return self.copy(deep=True)
975 
976     # mutable objects should not be hashable
977     # https://github.com/python/mypy/issues/4266
978     __hash__ = None  # type: ignore
979 
980     @property
981     def chunks(self):
982         """Block dimensions for this array's data or None if it's not a dask
983         array.
984         """
985         return getattr(self._data, "chunks", None)
986 
987     _array_counter = itertools.count()
988 
989     def chunk(self, chunks={}, name=None, lock=False):
990         """Coerce this array's data into a dask arrays with the given chunks.
991 
992         If this variable is a non-dask array, it will be converted to dask
993         array. If it's a dask array, it will be rechunked to the given chunk
994         sizes.
995 
996         If neither chunks is not provided for one or more dimensions, chunk
997         sizes along that dimension will not be updated; non-dask arrays will be
998         converted into dask arrays with a single block.
999 
1000         Parameters
1001         ----------
1002         chunks : int, tuple or dict, optional
1003             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1004             ``{'x': 5, 'y': 5}``.
1005         name : str, optional
1006             Used to generate the name for this array in the internal dask
1007             graph. Does not need not be unique.
1008         lock : optional
1009             Passed on to :py:func:`dask.array.from_array`, if the array is not
1010             already as dask array.
1011 
1012         Returns
1013         -------
1014         chunked : xarray.Variable
1015         """
1016         import dask
1017         import dask.array as da
1018 
1019         if chunks is None:
1020             warnings.warn(
1021                 "None value for 'chunks' is deprecated. "
1022                 "It will raise an error in the future. Use instead '{}'",
1023                 category=FutureWarning,
1024             )
1025             chunks = {}
1026 
1027         if utils.is_dict_like(chunks):
1028             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
1029 
1030         data = self._data
1031         if is_duck_dask_array(data):
1032             data = data.rechunk(chunks)
1033         else:
1034             if isinstance(data, indexing.ExplicitlyIndexed):
1035                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
1036                 # that can't handle general array indexing. For example, in netCDF4 you
1037                 # can do "outer" indexing along two dimensions independent, which works
1038                 # differently from how NumPy handles it.
1039                 # da.from_array works by using lazy indexing with a tuple of slices.
1040                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
1041                 # different indexing types in an explicit way:
1042                 # https://github.com/dask/dask/issues/2883
1043                 data = indexing.ImplicitToExplicitIndexingAdapter(
1044                     data, indexing.OuterIndexer
1045                 )
1046                 if LooseVersion(dask.__version__) < "2.0.0":
1047                     kwargs = {}
1048                 else:
1049                     # All of our lazily loaded backend array classes should use NumPy
1050                     # array operations.
1051                     kwargs = {"meta": np.ndarray}
1052             else:
1053                 kwargs = {}
1054 
1055             if utils.is_dict_like(chunks):
1056                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
1057 
1058             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
1059 
1060         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1061 
1062     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1063         """
1064         use sparse-array as backend.
1065         """
1066         import sparse
1067 
1068         # TODO: what to do if dask-backended?
1069         if fill_value is dtypes.NA:
1070             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1071         else:
1072             dtype = dtypes.result_type(self.dtype, fill_value)
1073 
1074         if sparse_format is _default:
1075             sparse_format = "coo"
1076         try:
1077             as_sparse = getattr(sparse, f"as_{sparse_format.lower()}")
1078         except AttributeError:
1079             raise ValueError(f"{sparse_format} is not a valid sparse format")
1080 
1081         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1082         return self._replace(data=data)
1083 
1084     def _to_dense(self):
1085         """
1086         Change backend from sparse to np.array
1087         """
1088         if hasattr(self._data, "todense"):
1089             return self._replace(data=self._data.todense())
1090         return self.copy(deep=False)
1091 
1092     def isel(
1093         self: VariableType,
1094         indexers: Mapping[Hashable, Any] = None,
1095         missing_dims: str = "raise",
1096         **indexers_kwargs: Any,
1097     ) -> VariableType:
1098         """Return a new array indexed along the specified dimension(s).
1099 
1100         Parameters
1101         ----------
1102         **indexers : {dim: indexer, ...}
1103             Keyword arguments with names matching dimensions and values given
1104             by integers, slice objects or arrays.
1105         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1106             What to do if dimensions that should be selected from are not present in the
1107             DataArray:
1108             - "raise": raise an exception
1109             - "warning": raise a warning, and ignore the missing dimensions
1110             - "ignore": ignore the missing dimensions
1111 
1112         Returns
1113         -------
1114         obj : Array object
1115             A new Array with the selected data and dimensions. In general,
1116             the new variable's data will be a view of this variable's data,
1117             unless numpy fancy indexing was triggered by using an array
1118             indexer, in which case the data will be a copy.
1119         """
1120         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1121 
1122         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1123 
1124         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1125         return self[key]
1126 
1127     def squeeze(self, dim=None):
1128         """Return a new object with squeezed data.
1129 
1130         Parameters
1131         ----------
1132         dim : None or str or tuple of str, optional
1133             Selects a subset of the length one dimensions. If a dimension is
1134             selected with length greater than one, an error is raised. If
1135             None, all length one dimensions are squeezed.
1136 
1137         Returns
1138         -------
1139         squeezed : same type as caller
1140             This object, but with with all or a subset of the dimensions of
1141             length 1 removed.
1142 
1143         See Also
1144         --------
1145         numpy.squeeze
1146         """
1147         dims = common.get_squeeze_dims(self, dim)
1148         return self.isel({d: 0 for d in dims})
1149 
1150     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1151         axis = self.get_axis_num(dim)
1152 
1153         if count > 0:
1154             keep = slice(None, -count)
1155         elif count < 0:
1156             keep = slice(-count, None)
1157         else:
1158             keep = slice(None)
1159 
1160         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1161 
1162         if fill_value is dtypes.NA:
1163             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1164         else:
1165             dtype = self.dtype
1166 
1167         width = min(abs(count), self.shape[axis])
1168         dim_pad = (width, 0) if count >= 0 else (0, width)
1169         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]
1170 
1171         data = duck_array_ops.pad(
1172             trimmed_data.astype(dtype),
1173             pads,
1174             mode="constant",
1175             constant_values=fill_value,
1176         )
1177 
1178         if is_duck_dask_array(data):
1179             # chunked data should come out with the same chunks; this makes
1180             # it feasible to combine shifted and unshifted data
1181             # TODO: remove this once dask.array automatically aligns chunks
1182             data = data.rechunk(self.data.chunks)
1183 
1184         return type(self)(self.dims, data, self._attrs, fastpath=True)
1185 
1186     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1187         """
1188         Return a new Variable with shifted data.
1189 
1190         Parameters
1191         ----------
1192         shifts : mapping of the form {dim: offset}
1193             Integer offset to shift along each of the given dimensions.
1194             Positive offsets shift to the right; negative offsets shift to the
1195             left.
1196         fill_value: scalar, optional
1197             Value to use for newly missing values
1198         **shifts_kwargs
1199             The keyword arguments form of ``shifts``.
1200             One of shifts or shifts_kwargs must be provided.
1201 
1202         Returns
1203         -------
1204         shifted : Variable
1205             Variable with the same dimensions and attributes but shifted data.
1206         """
1207         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1208         result = self
1209         for dim, count in shifts.items():
1210             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1211         return result
1212 
1213     def _pad_options_dim_to_index(
1214         self,
1215         pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],
1216         fill_with_shape=False,
1217     ):
1218         if fill_with_shape:
1219             return [
1220                 (n, n) if d not in pad_option else pad_option[d]
1221                 for d, n in zip(self.dims, self.data.shape)
1222             ]
1223         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]
1224 
1225     def pad(
1226         self,
1227         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
1228         mode: str = "constant",
1229         stat_length: Union[
1230             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
1231         ] = None,
1232         constant_values: Union[
1233             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
1234         ] = None,
1235         end_values: Union[
1236             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
1237         ] = None,
1238         reflect_type: str = None,
1239         **pad_width_kwargs: Any,
1240     ):
1241         """
1242         Return a new Variable with padded data.
1243 
1244         Parameters
1245         ----------
1246         pad_width : mapping of hashable to tuple of int
1247             Mapping with the form of {dim: (pad_before, pad_after)}
1248             describing the number of values padded along each dimension.
1249             {dim: pad} is a shortcut for pad_before = pad_after = pad
1250         mode : str, default: "constant"
1251             See numpy / Dask docs
1252         stat_length : int, tuple or mapping of hashable to tuple
1253             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
1254             values at edge of each axis used to calculate the statistic value.
1255         constant_values : scalar, tuple or mapping of hashable to tuple
1256             Used in 'constant'.  The values to set the padded values for each
1257             axis.
1258         end_values : scalar, tuple or mapping of hashable to tuple
1259             Used in 'linear_ramp'.  The values used for the ending value of the
1260             linear_ramp and that will form the edge of the padded array.
1261         reflect_type : {"even", "odd"}, optional
1262             Used in "reflect", and "symmetric".  The "even" style is the
1263             default with an unaltered reflection around the edge value.  For
1264             the "odd" style, the extended part of the array is created by
1265             subtracting the reflected values from two times the edge value.
1266         **pad_width_kwargs
1267             One of pad_width or pad_width_kwargs must be provided.
1268 
1269         Returns
1270         -------
1271         padded : Variable
1272             Variable with the same dimensions and attributes but padded data.
1273         """
1274         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
1275 
1276         # change default behaviour of pad with mode constant
1277         if mode == "constant" and (
1278             constant_values is None or constant_values is dtypes.NA
1279         ):
1280             dtype, constant_values = dtypes.maybe_promote(self.dtype)
1281         else:
1282             dtype = self.dtype
1283 
1284         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty
1285         if isinstance(stat_length, dict):
1286             stat_length = self._pad_options_dim_to_index(
1287                 stat_length, fill_with_shape=True
1288             )
1289         if isinstance(constant_values, dict):
1290             constant_values = self._pad_options_dim_to_index(constant_values)
1291         if isinstance(end_values, dict):
1292             end_values = self._pad_options_dim_to_index(end_values)
1293 
1294         # workaround for bug in Dask's default value of stat_length https://github.com/dask/dask/issues/5303
1295         if stat_length is None and mode in ["maximum", "mean", "median", "minimum"]:
1296             stat_length = [(n, n) for n in self.data.shape]  # type: ignore
1297 
1298         # change integer values to a tuple of two of those values and change pad_width to index
1299         for k, v in pad_width.items():
1300             if isinstance(v, numbers.Number):
1301                 pad_width[k] = (v, v)
1302         pad_width_by_index = self._pad_options_dim_to_index(pad_width)
1303 
1304         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty
1305         pad_option_kwargs = {}
1306         if stat_length is not None:
1307             pad_option_kwargs["stat_length"] = stat_length
1308         if constant_values is not None:
1309             pad_option_kwargs["constant_values"] = constant_values
1310         if end_values is not None:
1311             pad_option_kwargs["end_values"] = end_values
1312         if reflect_type is not None:
1313             pad_option_kwargs["reflect_type"] = reflect_type  # type: ignore
1314 
1315         array = duck_array_ops.pad(
1316             self.data.astype(dtype, copy=False),
1317             pad_width_by_index,
1318             mode=mode,
1319             **pad_option_kwargs,
1320         )
1321 
1322         return type(self)(self.dims, array)
1323 
1324     def _roll_one_dim(self, dim, count):
1325         axis = self.get_axis_num(dim)
1326 
1327         count %= self.shape[axis]
1328         if count != 0:
1329             indices = [slice(-count, None), slice(None, -count)]
1330         else:
1331             indices = [slice(None)]
1332 
1333         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1334 
1335         data = duck_array_ops.concatenate(arrays, axis)
1336 
1337         if is_duck_dask_array(data):
1338             # chunked data should come out with the same chunks; this makes
1339             # it feasible to combine shifted and unshifted data
1340             # TODO: remove this once dask.array automatically aligns chunks
1341             data = data.rechunk(self.data.chunks)
1342 
1343         return type(self)(self.dims, data, self._attrs, fastpath=True)
1344 
1345     def roll(self, shifts=None, **shifts_kwargs):
1346         """
1347         Return a new Variable with rolld data.
1348 
1349         Parameters
1350         ----------
1351         shifts : mapping of hashable to int
1352             Integer offset to roll along each of the given dimensions.
1353             Positive offsets roll to the right; negative offsets roll to the
1354             left.
1355         **shifts_kwargs
1356             The keyword arguments form of ``shifts``.
1357             One of shifts or shifts_kwargs must be provided.
1358 
1359         Returns
1360         -------
1361         shifted : Variable
1362             Variable with the same dimensions and attributes but rolled data.
1363         """
1364         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1365 
1366         result = self
1367         for dim, count in shifts.items():
1368             result = result._roll_one_dim(dim, count)
1369         return result
1370 
1371     def transpose(self, *dims) -> "Variable":
1372         """Return a new Variable object with transposed dimensions.
1373 
1374         Parameters
1375         ----------
1376         *dims : str, optional
1377             By default, reverse the dimensions. Otherwise, reorder the
1378             dimensions to this order.
1379 
1380         Returns
1381         -------
1382         transposed : Variable
1383             The returned object has transposed data and dimensions with the
1384             same attributes as the original.
1385 
1386         Notes
1387         -----
1388         This operation returns a view of this variable's data. It is
1389         lazy for dask-backed Variables but not for numpy-backed Variables.
1390 
1391         See Also
1392         --------
1393         numpy.transpose
1394         """
1395         if len(dims) == 0:
1396             dims = self.dims[::-1]
1397         dims = tuple(infix_dims(dims, self.dims))
1398         axes = self.get_axis_num(dims)
1399         if len(dims) < 2 or dims == self.dims:
1400             # no need to transpose if only one dimension
1401             # or dims are in same order
1402             return self.copy(deep=False)
1403 
1404         data = as_indexable(self._data).transpose(axes)
1405         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1406 
1407     @property
1408     def T(self) -> "Variable":
1409         return self.transpose()
1410 
1411     def set_dims(self, dims, shape=None):
1412         """Return a new variable with given set of dimensions.
1413         This method might be used to attach new dimension(s) to variable.
1414 
1415         When possible, this operation does not copy this variable's data.
1416 
1417         Parameters
1418         ----------
1419         dims : str or sequence of str or dict
1420             Dimensions to include on the new variable. If a dict, values are
1421             used to provide the sizes of new dimensions; otherwise, new
1422             dimensions are inserted with length 1.
1423 
1424         Returns
1425         -------
1426         Variable
1427         """
1428         if isinstance(dims, str):
1429             dims = [dims]
1430 
1431         if shape is None and utils.is_dict_like(dims):
1432             shape = dims.values()
1433 
1434         missing_dims = set(self.dims) - set(dims)
1435         if missing_dims:
1436             raise ValueError(
1437                 "new dimensions %r must be a superset of "
1438                 "existing dimensions %r" % (dims, self.dims)
1439             )
1440 
1441         self_dims = set(self.dims)
1442         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1443 
1444         if self.dims == expanded_dims:
1445             # don't use broadcast_to unless necessary so the result remains
1446             # writeable if possible
1447             expanded_data = self.data
1448         elif shape is not None:
1449             dims_map = dict(zip(dims, shape))
1450             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1451             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1452         else:
1453             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1454 
1455         expanded_var = Variable(
1456             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1457         )
1458         return expanded_var.transpose(*dims)
1459 
1460     def _stack_once(self, dims, new_dim):
1461         if not set(dims) <= set(self.dims):
1462             raise ValueError("invalid existing dimensions: %s" % dims)
1463 
1464         if new_dim in self.dims:
1465             raise ValueError(
1466                 "cannot create a new dimension with the same "
1467                 "name as an existing dimension"
1468             )
1469 
1470         if len(dims) == 0:
1471             # don't stack
1472             return self.copy(deep=False)
1473 
1474         other_dims = [d for d in self.dims if d not in dims]
1475         dim_order = other_dims + list(dims)
1476         reordered = self.transpose(*dim_order)
1477 
1478         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1479         new_data = reordered.data.reshape(new_shape)
1480         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1481 
1482         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1483 
1484     def stack(self, dimensions=None, **dimensions_kwargs):
1485         """
1486         Stack any number of existing dimensions into a single new dimension.
1487 
1488         New dimensions will be added at the end, and the order of the data
1489         along each new dimension will be in contiguous (C) order.
1490 
1491         Parameters
1492         ----------
1493         dimensions : mapping of hashable to tuple of hashable
1494             Mapping of form new_name=(dim1, dim2, ...) describing the
1495             names of new dimensions, and the existing dimensions that
1496             they replace.
1497         **dimensions_kwargs
1498             The keyword arguments form of ``dimensions``.
1499             One of dimensions or dimensions_kwargs must be provided.
1500 
1501         Returns
1502         -------
1503         stacked : Variable
1504             Variable with the same attributes but stacked data.
1505 
1506         See also
1507         --------
1508         Variable.unstack
1509         """
1510         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1511         result = self
1512         for new_dim, dims in dimensions.items():
1513             result = result._stack_once(dims, new_dim)
1514         return result
1515 
1516     def _unstack_once(self, dims, old_dim):
1517         new_dim_names = tuple(dims.keys())
1518         new_dim_sizes = tuple(dims.values())
1519 
1520         if old_dim not in self.dims:
1521             raise ValueError("invalid existing dimension: %s" % old_dim)
1522 
1523         if set(new_dim_names).intersection(self.dims):
1524             raise ValueError(
1525                 "cannot create a new dimension with the same "
1526                 "name as an existing dimension"
1527             )
1528 
1529         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1530             raise ValueError(
1531                 "the product of the new dimension sizes must "
1532                 "equal the size of the old dimension"
1533             )
1534 
1535         other_dims = [d for d in self.dims if d != old_dim]
1536         dim_order = other_dims + [old_dim]
1537         reordered = self.transpose(*dim_order)
1538 
1539         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1540         new_data = reordered.data.reshape(new_shape)
1541         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1542 
1543         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1544 
1545     def unstack(self, dimensions=None, **dimensions_kwargs):
1546         """
1547         Unstack an existing dimension into multiple new dimensions.
1548 
1549         New dimensions will be added at the end, and the order of the data
1550         along each new dimension will be in contiguous (C) order.
1551 
1552         Parameters
1553         ----------
1554         dimensions : mapping of hashable to mapping of hashable to int
1555             Mapping of the form old_dim={dim1: size1, ...} describing the
1556             names of existing dimensions, and the new dimensions and sizes
1557             that they map to.
1558         **dimensions_kwargs
1559             The keyword arguments form of ``dimensions``.
1560             One of dimensions or dimensions_kwargs must be provided.
1561 
1562         Returns
1563         -------
1564         unstacked : Variable
1565             Variable with the same attributes but unstacked data.
1566 
1567         See also
1568         --------
1569         Variable.stack
1570         """
1571         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1572         result = self
1573         for old_dim, dims in dimensions.items():
1574             result = result._unstack_once(dims, old_dim)
1575         return result
1576 
1577     def fillna(self, value):
1578         return ops.fillna(self, value)
1579 
1580     def where(self, cond, other=dtypes.NA):
1581         return ops.where_method(self, cond, other)
1582 
1583     def reduce(
1584         self,
1585         func,
1586         dim=None,
1587         axis=None,
1588         keep_attrs=None,
1589         keepdims=False,
1590         **kwargs,
1591     ):
1592         """Reduce this array by applying `func` along some dimension(s).
1593 
1594         Parameters
1595         ----------
1596         func : callable
1597             Function which can be called in the form
1598             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1599             np.ndarray over an integer valued axis.
1600         dim : str or sequence of str, optional
1601             Dimension(s) over which to apply `func`.
1602         axis : int or sequence of int, optional
1603             Axis(es) over which to apply `func`. Only one of the 'dim'
1604             and 'axis' arguments can be supplied. If neither are supplied, then
1605             the reduction is calculated over the flattened array (by calling
1606             `func(x)` without an axis argument).
1607         keep_attrs : bool, optional
1608             If True, the variable's attributes (`attrs`) will be copied from
1609             the original object to the new one.  If False (default), the new
1610             object will be returned without attributes.
1611         keepdims : bool, default: False
1612             If True, the dimensions which are reduced are left in the result
1613             as dimensions of size one
1614         **kwargs : dict
1615             Additional keyword arguments passed on to `func`.
1616 
1617         Returns
1618         -------
1619         reduced : Array
1620             Array with summarized data and the indicated dimension(s)
1621             removed.
1622         """
1623         if dim == ...:
1624             dim = None
1625         if dim is not None and axis is not None:
1626             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1627 
1628         if dim is not None:
1629             axis = self.get_axis_num(dim)
1630 
1631         with warnings.catch_warnings():
1632             warnings.filterwarnings(
1633                 "ignore", r"Mean of empty slice", category=RuntimeWarning
1634             )
1635             if axis is not None:
1636                 data = func(self.data, axis=axis, **kwargs)
1637             else:
1638                 data = func(self.data, **kwargs)
1639 
1640         if getattr(data, "shape", ()) == self.shape:
1641             dims = self.dims
1642         else:
1643             removed_axes = (
1644                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1645             )
1646             if keepdims:
1647                 # Insert np.newaxis for removed dims
1648                 slices = tuple(
1649                     np.newaxis if i in removed_axes else slice(None, None)
1650                     for i in range(self.ndim)
1651                 )
1652                 if getattr(data, "shape", None) is None:
1653                     # Reduce has produced a scalar value, not an array-like
1654                     data = np.asanyarray(data)[slices]
1655                 else:
1656                     data = data[slices]
1657                 dims = self.dims
1658             else:
1659                 dims = [
1660                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1661                 ]
1662 
1663         if keep_attrs is None:
1664             keep_attrs = _get_keep_attrs(default=False)
1665         attrs = self._attrs if keep_attrs else None
1666 
1667         return Variable(dims, data, attrs=attrs)
1668 
1669     @classmethod
1670     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1671         """Concatenate variables along a new or existing dimension.
1672 
1673         Parameters
1674         ----------
1675         variables : iterable of Variable
1676             Arrays to stack together. Each variable is expected to have
1677             matching dimensions and shape except for along the stacked
1678             dimension.
1679         dim : str or DataArray, optional
1680             Name of the dimension to stack along. This can either be a new
1681             dimension name, in which case it is added along axis=0, or an
1682             existing dimension name, in which case the location of the
1683             dimension is unchanged. Where to insert the new dimension is
1684             determined by the first variable.
1685         positions : None or list of array-like, optional
1686             List of integer arrays which specifies the integer positions to
1687             which to assign each dataset along the concatenated dimension.
1688             If not supplied, objects are concatenated in the provided order.
1689         shortcut : bool, optional
1690             This option is used internally to speed-up groupby operations.
1691             If `shortcut` is True, some checks of internal consistency between
1692             arrays to concatenate are skipped.
1693 
1694         Returns
1695         -------
1696         stacked : Variable
1697             Concatenated Variable formed by stacking all the supplied variables
1698             along the given dimension.
1699         """
1700         if not isinstance(dim, str):
1701             (dim,) = dim.dims
1702 
1703         # can't do this lazily: we need to loop through variables at least
1704         # twice
1705         variables = list(variables)
1706         first_var = variables[0]
1707 
1708         arrays = [v.data for v in variables]
1709 
1710         if dim in first_var.dims:
1711             axis = first_var.get_axis_num(dim)
1712             dims = first_var.dims
1713             data = duck_array_ops.concatenate(arrays, axis=axis)
1714             if positions is not None:
1715                 # TODO: deprecate this option -- we don't need it for groupby
1716                 # any more.
1717                 indices = nputils.inverse_permutation(np.concatenate(positions))
1718                 data = duck_array_ops.take(data, indices, axis=axis)
1719         else:
1720             axis = 0
1721             dims = (dim,) + first_var.dims
1722             data = duck_array_ops.stack(arrays, axis=axis)
1723 
1724         attrs = dict(first_var.attrs)
1725         encoding = dict(first_var.encoding)
1726         if not shortcut:
1727             for var in variables:
1728                 if var.dims != first_var.dims:
1729                     raise ValueError(
1730                         f"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}"
1731                     )
1732 
1733         return cls(dims, data, attrs, encoding)
1734 
1735     def equals(self, other, equiv=duck_array_ops.array_equiv):
1736         """True if two Variables have the same dimensions and values;
1737         otherwise False.
1738 
1739         Variables can still be equal (like pandas objects) if they have NaN
1740         values in the same locations.
1741 
1742         This method is necessary because `v1 == v2` for Variables
1743         does element-wise comparisons (like numpy.ndarrays).
1744         """
1745         other = getattr(other, "variable", other)
1746         try:
1747             return self.dims == other.dims and (
1748                 self._data is other._data or equiv(self.data, other.data)
1749             )
1750         except (TypeError, AttributeError):
1751             return False
1752 
1753     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1754         """True if two Variables have the values after being broadcast against
1755         each other; otherwise False.
1756 
1757         Variables can still be equal (like pandas objects) if they have NaN
1758         values in the same locations.
1759         """
1760         try:
1761             self, other = broadcast_variables(self, other)
1762         except (ValueError, AttributeError):
1763             return False
1764         return self.equals(other, equiv=equiv)
1765 
1766     def identical(self, other, equiv=duck_array_ops.array_equiv):
1767         """Like equals, but also checks attributes."""
1768         try:
1769             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1770                 other, equiv=equiv
1771             )
1772         except (TypeError, AttributeError):
1773             return False
1774 
1775     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1776         """True if the intersection of two Variable's non-null data is
1777         equal; otherwise false.
1778 
1779         Variables can thus still be equal if there are locations where either,
1780         or both, contain NaN values.
1781         """
1782         return self.broadcast_equals(other, equiv=equiv)
1783 
1784     def quantile(
1785         self, q, dim=None, interpolation="linear", keep_attrs=None, skipna=True
1786     ):
1787         """Compute the qth quantile of the data along the specified dimension.
1788 
1789         Returns the qth quantiles(s) of the array elements.
1790 
1791         Parameters
1792         ----------
1793         q : float or sequence of float
1794             Quantile to compute, which must be between 0 and 1
1795             inclusive.
1796         dim : str or sequence of str, optional
1797             Dimension(s) over which to apply quantile.
1798         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
1799             This optional parameter specifies the interpolation method to
1800             use when the desired quantile lies between two data points
1801             ``i < j``:
1802 
1803                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1804                   the fractional part of the index surrounded by ``i`` and
1805                   ``j``.
1806                 * lower: ``i``.
1807                 * higher: ``j``.
1808                 * nearest: ``i`` or ``j``, whichever is nearest.
1809                 * midpoint: ``(i + j) / 2``.
1810 
1811         keep_attrs : bool, optional
1812             If True, the variable's attributes (`attrs`) will be copied from
1813             the original object to the new one.  If False (default), the new
1814             object will be returned without attributes.
1815 
1816         Returns
1817         -------
1818         quantiles : Variable
1819             If `q` is a single quantile, then the result
1820             is a scalar. If multiple percentiles are given, first axis of
1821             the result corresponds to the quantile and a quantile dimension
1822             is added to the return array. The other dimensions are the
1823             dimensions that remain after the reduction of the array.
1824 
1825         See Also
1826         --------
1827         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,
1828         DataArray.quantile
1829         """
1830 
1831         from .computation import apply_ufunc
1832 
1833         _quantile_func = np.nanquantile if skipna else np.quantile
1834 
1835         if keep_attrs is None:
1836             keep_attrs = _get_keep_attrs(default=False)
1837 
1838         scalar = utils.is_scalar(q)
1839         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1840 
1841         if dim is None:
1842             dim = self.dims
1843 
1844         if utils.is_scalar(dim):
1845             dim = [dim]
1846 
1847         def _wrapper(npa, **kwargs):
1848             # move quantile axis to end. required for apply_ufunc
1849             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)
1850 
1851         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1852         result = apply_ufunc(
1853             _wrapper,
1854             self,
1855             input_core_dims=[dim],
1856             exclude_dims=set(dim),
1857             output_core_dims=[["quantile"]],
1858             output_dtypes=[np.float64],
1859             dask_gufunc_kwargs=dict(output_sizes={"quantile": len(q)}),
1860             dask="parallelized",
1861             kwargs={"q": q, "axis": axis, "interpolation": interpolation},
1862         )
1863 
1864         # for backward compatibility
1865         result = result.transpose("quantile", ...)
1866         if scalar:
1867             result = result.squeeze("quantile")
1868         if keep_attrs:
1869             result.attrs = self._attrs
1870         return result
1871 
1872     def rank(self, dim, pct=False):
1873         """Ranks the data.
1874 
1875         Equal values are assigned a rank that is the average of the ranks that
1876         would have been otherwise assigned to all of the values within that
1877         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1878 
1879         NaNs in the input array are returned as NaNs.
1880 
1881         The `bottleneck` library is required.
1882 
1883         Parameters
1884         ----------
1885         dim : str
1886             Dimension over which to compute rank.
1887         pct : bool, optional
1888             If True, compute percentage ranks, otherwise compute integer ranks.
1889 
1890         Returns
1891         -------
1892         ranked : Variable
1893 
1894         See Also
1895         --------
1896         Dataset.rank, DataArray.rank
1897         """
1898         import bottleneck as bn
1899 
1900         data = self.data
1901 
1902         if is_duck_dask_array(data):
1903             raise TypeError(
1904                 "rank does not work for arrays stored as dask "
1905                 "arrays. Load the data via .compute() or .load() "
1906                 "prior to calling this method."
1907             )
1908         elif not isinstance(data, np.ndarray):
1909             raise TypeError(
1910                 "rank is not implemented for {} objects.".format(type(data))
1911             )
1912 
1913         axis = self.get_axis_num(dim)
1914         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1915         ranked = func(data, axis=axis)
1916         if pct:
1917             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1918             ranked /= count
1919         return Variable(self.dims, ranked)
1920 
1921     def rolling_window(
1922         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1923     ):
1924         """
1925         Make a rolling_window along dim and add a new_dim to the last place.
1926 
1927         Parameters
1928         ----------
1929         dim : str
1930             Dimension over which to compute rolling_window.
1931             For nd-rolling, should be list of dimensions.
1932         window : int
1933             Window size of the rolling
1934             For nd-rolling, should be list of integers.
1935         window_dim : str
1936             New name of the window dimension.
1937             For nd-rolling, should be list of integers.
1938         center : bool, default: False
1939             If True, pad fill_value for both ends. Otherwise, pad in the head
1940             of the axis.
1941         fill_value
1942             value to be filled.
1943 
1944         Returns
1945         -------
1946         Variable that is a view of the original array with a added dimension of
1947         size w.
1948         The return dim: self.dims + (window_dim, )
1949         The return shape: self.shape + (window, )
1950 
1951         Examples
1952         --------
1953         >>> v = Variable(("a", "b"), np.arange(8).reshape((2, 4)))
1954         >>> v.rolling_window("b", 3, "window_dim")
1955         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1956         array([[[nan, nan,  0.],
1957                 [nan,  0.,  1.],
1958                 [ 0.,  1.,  2.],
1959                 [ 1.,  2.,  3.]],
1960         <BLANKLINE>
1961                [[nan, nan,  4.],
1962                 [nan,  4.,  5.],
1963                 [ 4.,  5.,  6.],
1964                 [ 5.,  6.,  7.]]])
1965 
1966         >>> v.rolling_window("b", 3, "window_dim", center=True)
1967         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1968         array([[[nan,  0.,  1.],
1969                 [ 0.,  1.,  2.],
1970                 [ 1.,  2.,  3.],
1971                 [ 2.,  3., nan]],
1972         <BLANKLINE>
1973                [[nan,  4.,  5.],
1974                 [ 4.,  5.,  6.],
1975                 [ 5.,  6.,  7.],
1976                 [ 6.,  7., nan]]])
1977         """
1978         if fill_value is dtypes.NA:  # np.nan is passed
1979             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1980             array = self.astype(dtype, copy=False).data
1981         else:
1982             dtype = self.dtype
1983             array = self.data
1984 
1985         if isinstance(dim, list):
1986             assert len(dim) == len(window)
1987             assert len(dim) == len(window_dim)
1988             assert len(dim) == len(center)
1989         else:
1990             dim = [dim]
1991             window = [window]
1992             window_dim = [window_dim]
1993             center = [center]
1994         axis = [self.get_axis_num(d) for d in dim]
1995         new_dims = self.dims + tuple(window_dim)
1996         return Variable(
1997             new_dims,
1998             duck_array_ops.rolling_window(
1999                 array, axis=axis, window=window, center=center, fill_value=fill_value
2000             ),
2001         )
2002 
2003     def coarsen(
2004         self, windows, func, boundary="exact", side="left", keep_attrs=None, **kwargs
2005     ):
2006         """
2007         Apply reduction function.
2008         """
2009         windows = {k: v for k, v in windows.items() if k in self.dims}
2010         if not windows:
2011             return self.copy()
2012 
2013         if keep_attrs is None:
2014             keep_attrs = _get_keep_attrs(default=False)
2015 
2016         if keep_attrs:
2017             _attrs = self.attrs
2018         else:
2019             _attrs = None
2020 
2021         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
2022         if isinstance(func, str):
2023             name = func
2024             func = getattr(duck_array_ops, name, None)
2025             if func is None:
2026                 raise NameError(f"{name} is not a valid method.")
2027 
2028         return self._replace(data=func(reshaped, axis=axes, **kwargs), attrs=_attrs)
2029 
2030     def _coarsen_reshape(self, windows, boundary, side):
2031         """
2032         Construct a reshaped-array for coarsen
2033         """
2034         if not utils.is_dict_like(boundary):
2035             boundary = {d: boundary for d in windows.keys()}
2036 
2037         if not utils.is_dict_like(side):
2038             side = {d: side for d in windows.keys()}
2039 
2040         # remove unrelated dimensions
2041         boundary = {k: v for k, v in boundary.items() if k in windows}
2042         side = {k: v for k, v in side.items() if k in windows}
2043 
2044         for d, window in windows.items():
2045             if window <= 0:
2046                 raise ValueError(f"window must be > 0. Given {window}")
2047 
2048         variable = self
2049         for d, window in windows.items():
2050             # trim or pad the object
2051             size = variable.shape[self._get_axis_num(d)]
2052             n = int(size / window)
2053             if boundary[d] == "exact":
2054                 if n * window != size:
2055                     raise ValueError(
2056                         "Could not coarsen a dimension of size {} with "
2057                         "window {}".format(size, window)
2058                     )
2059             elif boundary[d] == "trim":
2060                 if side[d] == "left":
2061                     variable = variable.isel({d: slice(0, window * n)})
2062                 else:
2063                     excess = size - window * n
2064                     variable = variable.isel({d: slice(excess, None)})
2065             elif boundary[d] == "pad":  # pad
2066                 pad = window * n - size
2067                 if pad < 0:
2068                     pad += window
2069                 if side[d] == "left":
2070                     pad_width = {d: (0, pad)}
2071                 else:
2072                     pad_width = {d: (pad, 0)}
2073                 variable = variable.pad(pad_width, mode="constant")
2074             else:
2075                 raise TypeError(
2076                     "{} is invalid for boundary. Valid option is 'exact', "
2077                     "'trim' and 'pad'".format(boundary[d])
2078                 )
2079 
2080         shape = []
2081         axes = []
2082         axis_count = 0
2083         for i, d in enumerate(variable.dims):
2084             if d in windows:
2085                 size = variable.shape[i]
2086                 shape.append(int(size / windows[d]))
2087                 shape.append(windows[d])
2088                 axis_count += 1
2089                 axes.append(i + axis_count)
2090             else:
2091                 shape.append(variable.shape[i])
2092 
2093         return variable.data.reshape(shape), tuple(axes)
2094 
2095     def isnull(self, keep_attrs: bool = None):
2096         """Test each value in the array for whether it is a missing value.
2097 
2098         Returns
2099         -------
2100         isnull : Variable
2101             Same type and shape as object, but the dtype of the data is bool.
2102 
2103         See Also
2104         --------
2105         pandas.isnull
2106 
2107         Examples
2108         --------
2109         >>> var = xr.Variable("x", [1, np.nan, 3])
2110         >>> var
2111         <xarray.Variable (x: 3)>
2112         array([ 1., nan,  3.])
2113         >>> var.isnull()
2114         <xarray.Variable (x: 3)>
2115         array([False,  True, False])
2116         """
2117         from .computation import apply_ufunc
2118 
2119         if keep_attrs is None:
2120             keep_attrs = _get_keep_attrs(default=False)
2121 
2122         return apply_ufunc(
2123             duck_array_ops.isnull,
2124             self,
2125             dask="allowed",
2126             keep_attrs=keep_attrs,
2127         )
2128 
2129     def notnull(self, keep_attrs: bool = None):
2130         """Test each value in the array for whether it is not a missing value.
2131 
2132         Returns
2133         -------
2134         notnull : Variable
2135             Same type and shape as object, but the dtype of the data is bool.
2136 
2137         See Also
2138         --------
2139         pandas.notnull
2140 
2141         Examples
2142         --------
2143         >>> var = xr.Variable("x", [1, np.nan, 3])
2144         >>> var
2145         <xarray.Variable (x: 3)>
2146         array([ 1., nan,  3.])
2147         >>> var.notnull()
2148         <xarray.Variable (x: 3)>
2149         array([ True, False,  True])
2150         """
2151         from .computation import apply_ufunc
2152 
2153         if keep_attrs is None:
2154             keep_attrs = _get_keep_attrs(default=False)
2155 
2156         return apply_ufunc(
2157             duck_array_ops.notnull,
2158             self,
2159             dask="allowed",
2160             keep_attrs=keep_attrs,
2161         )
2162 
2163     @property
2164     def real(self):
2165         return type(self)(self.dims, self.data.real, self._attrs)
2166 
2167     @property
2168     def imag(self):
2169         return type(self)(self.dims, self.data.imag, self._attrs)
2170 
2171     def __array_wrap__(self, obj, context=None):
2172         return Variable(self.dims, obj)
2173 
2174     @staticmethod
2175     def _unary_op(f):
2176         @functools.wraps(f)
2177         def func(self, *args, **kwargs):
2178             keep_attrs = kwargs.pop("keep_attrs", None)
2179             if keep_attrs is None:
2180                 keep_attrs = _get_keep_attrs(default=True)
2181             with np.errstate(all="ignore"):
2182                 result = self.__array_wrap__(f(self.data, *args, **kwargs))
2183                 if keep_attrs:
2184                     result.attrs = self.attrs
2185                 return result
2186 
2187         return func
2188 
2189     @staticmethod
2190     def _binary_op(f, reflexive=False, **ignored_kwargs):
2191         @functools.wraps(f)
2192         def func(self, other):
2193             if isinstance(other, (xr.DataArray, xr.Dataset)):
2194                 return NotImplemented
2195             self_data, other_data, dims = _broadcast_compat_data(self, other)
2196             keep_attrs = _get_keep_attrs(default=False)
2197             attrs = self._attrs if keep_attrs else None
2198             with np.errstate(all="ignore"):
2199                 new_data = (
2200                     f(self_data, other_data)
2201                     if not reflexive
2202                     else f(other_data, self_data)
2203                 )
2204             result = Variable(dims, new_data, attrs=attrs)
2205             return result
2206 
2207         return func
2208 
2209     @staticmethod
2210     def _inplace_binary_op(f):
2211         @functools.wraps(f)
2212         def func(self, other):
2213             if isinstance(other, xr.Dataset):
2214                 raise TypeError("cannot add a Dataset to a Variable in-place")
2215             self_data, other_data, dims = _broadcast_compat_data(self, other)
2216             if dims != self.dims:
2217                 raise ValueError("dimensions cannot change for in-place operations")
2218             with np.errstate(all="ignore"):
2219                 self.values = f(self_data, other_data)
2220             return self
2221 
2222         return func
2223 
2224     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2225         """A (private) method to convert datetime array to numeric dtype
2226         See duck_array_ops.datetime_to_numeric
2227         """
2228         numeric_array = duck_array_ops.datetime_to_numeric(
2229             self.data, offset, datetime_unit, dtype
2230         )
2231         return type(self)(self.dims, numeric_array, self._attrs)
2232 
2233     def _unravel_argminmax(
2234         self,
2235         argminmax: str,
2236         dim: Union[Hashable, Sequence[Hashable], None],
2237         axis: Union[int, None],
2238         keep_attrs: Optional[bool],
2239         skipna: Optional[bool],
2240     ) -> Union["Variable", Dict[Hashable, "Variable"]]:
2241         """Apply argmin or argmax over one or more dimensions, returning the result as a
2242         dict of DataArray that can be passed directly to isel.
2243         """
2244         if dim is None and axis is None:
2245             warnings.warn(
2246                 "Behaviour of argmin/argmax with neither dim nor axis argument will "
2247                 "change to return a dict of indices of each dimension. To get a "
2248                 "single, flat index, please use np.argmin(da.data) or "
2249                 "np.argmax(da.data) instead of da.argmin() or da.argmax().",
2250                 DeprecationWarning,
2251                 stacklevel=3,
2252             )
2253 
2254         argminmax_func = getattr(duck_array_ops, argminmax)
2255 
2256         if dim is ...:
2257             # In future, should do this also when (dim is None and axis is None)
2258             dim = self.dims
2259         if (
2260             dim is None
2261             or axis is not None
2262             or not isinstance(dim, Sequence)
2263             or isinstance(dim, str)
2264         ):
2265             # Return int index if single dimension is passed, and is not part of a
2266             # sequence
2267             return self.reduce(
2268                 argminmax_func, dim=dim, axis=axis, keep_attrs=keep_attrs, skipna=skipna
2269             )
2270 
2271         # Get a name for the new dimension that does not conflict with any existing
2272         # dimension
2273         newdimname = "_unravel_argminmax_dim_0"
2274         count = 1
2275         while newdimname in self.dims:
2276             newdimname = f"_unravel_argminmax_dim_{count}"
2277             count += 1
2278 
2279         stacked = self.stack({newdimname: dim})
2280 
2281         result_dims = stacked.dims[:-1]
2282         reduce_shape = tuple(self.sizes[d] for d in dim)
2283 
2284         result_flat_indices = stacked.reduce(argminmax_func, axis=-1, skipna=skipna)
2285 
2286         result_unravelled_indices = duck_array_ops.unravel_index(
2287             result_flat_indices.data, reduce_shape
2288         )
2289 
2290         result = {
2291             d: Variable(dims=result_dims, data=i)
2292             for d, i in zip(dim, result_unravelled_indices)
2293         }
2294 
2295         if keep_attrs is None:
2296             keep_attrs = _get_keep_attrs(default=False)
2297         if keep_attrs:
2298             for v in result.values():
2299                 v.attrs = self.attrs
2300 
2301         return result
2302 
2303     def argmin(
2304         self,
2305         dim: Union[Hashable, Sequence[Hashable]] = None,
2306         axis: int = None,
2307         keep_attrs: bool = None,
2308         skipna: bool = None,
2309     ) -> Union["Variable", Dict[Hashable, "Variable"]]:
2310         """Index or indices of the minimum of the Variable over one or more dimensions.
2311         If a sequence is passed to 'dim', then result returned as dict of Variables,
2312         which can be passed directly to isel(). If a single str is passed to 'dim' then
2313         returns a Variable with dtype int.
2314 
2315         If there are multiple minima, the indices of the first one found will be
2316         returned.
2317 
2318         Parameters
2319         ----------
2320         dim : hashable, sequence of hashable or ..., optional
2321             The dimensions over which to find the minimum. By default, finds minimum over
2322             all dimensions - for now returning an int for backward compatibility, but
2323             this is deprecated, in future will return a dict with indices for all
2324             dimensions; to return a dict with all dimensions now, pass '...'.
2325         axis : int, optional
2326             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2327             can be supplied.
2328         keep_attrs : bool, optional
2329             If True, the attributes (`attrs`) will be copied from the original
2330             object to the new one.  If False (default), the new object will be
2331             returned without attributes.
2332         skipna : bool, optional
2333             If True, skip missing values (as marked by NaN). By default, only
2334             skips missing values for float dtypes; other dtypes either do not
2335             have a sentinel missing value (int) or skipna=True has not been
2336             implemented (object, datetime64 or timedelta64).
2337 
2338         Returns
2339         -------
2340         result : Variable or dict of Variable
2341 
2342         See also
2343         --------
2344         DataArray.argmin, DataArray.idxmin
2345         """
2346         return self._unravel_argminmax("argmin", dim, axis, keep_attrs, skipna)
2347 
2348     def argmax(
2349         self,
2350         dim: Union[Hashable, Sequence[Hashable]] = None,
2351         axis: int = None,
2352         keep_attrs: bool = None,
2353         skipna: bool = None,
2354     ) -> Union["Variable", Dict[Hashable, "Variable"]]:
2355         """Index or indices of the maximum of the Variable over one or more dimensions.
2356         If a sequence is passed to 'dim', then result returned as dict of Variables,
2357         which can be passed directly to isel(). If a single str is passed to 'dim' then
2358         returns a Variable with dtype int.
2359 
2360         If there are multiple maxima, the indices of the first one found will be
2361         returned.
2362 
2363         Parameters
2364         ----------
2365         dim : hashable, sequence of hashable or ..., optional
2366             The dimensions over which to find the maximum. By default, finds maximum over
2367             all dimensions - for now returning an int for backward compatibility, but
2368             this is deprecated, in future will return a dict with indices for all
2369             dimensions; to return a dict with all dimensions now, pass '...'.
2370         axis : int, optional
2371             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2372             can be supplied.
2373         keep_attrs : bool, optional
2374             If True, the attributes (`attrs`) will be copied from the original
2375             object to the new one.  If False (default), the new object will be
2376             returned without attributes.
2377         skipna : bool, optional
2378             If True, skip missing values (as marked by NaN). By default, only
2379             skips missing values for float dtypes; other dtypes either do not
2380             have a sentinel missing value (int) or skipna=True has not been
2381             implemented (object, datetime64 or timedelta64).
2382 
2383         Returns
2384         -------
2385         result : Variable or dict of Variable
2386 
2387         See also
2388         --------
2389         DataArray.argmax, DataArray.idxmax
2390         """
2391         return self._unravel_argminmax("argmax", dim, axis, keep_attrs, skipna)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/common.py</file>
<class>DataWithCoords</class>
<method>astype</method>
<code>
1394     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
1395         """
1396         Copy of the xarray object, with data cast to a specified type.
1397         Leaves coordinate dtype unchanged.
1398 
1399         Parameters
1400         ----------
1401         dtype : str or dtype
1402             Typecode or data-type to which the array is cast.
1403         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
1404             Controls what kind of data casting may occur. Defaults to 'unsafe'
1405             for backwards compatibility.
1406 
1407             * 'no' means the data types should not be cast at all.
1408             * 'equiv' means only byte-order changes are allowed.
1409             * 'safe' means only casts which can preserve values are allowed.
1410             * 'same_kind' means only safe casts or casts within a kind,
1411                 like float64 to float32, are allowed.
1412             * 'unsafe' means any data conversions may be done.
1413         copy : bool, optional
1414             By default, astype always returns a newly allocated array. If this
1415             is set to False and the `dtype` requirement is satisfied, the input
1416             array is returned instead of a copy.
1417         keep_attrs : bool, optional
1418             By default, astype keeps attributes. Set to False to remove
1419             attributes in the returned object.
1420 
1421         Returns
1422         -------
1423         out : same as object
1424             New object with data cast to the specified type.
1425 
1426         See also
1427         --------
1428         np.ndarray.astype
1429         dask.array.Array.astype
1430         """
1431         from .computation import apply_ufunc
1432 
1433         return apply_ufunc(
1434             duck_array_ops.astype,
1435             self,
1436             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
1437             keep_attrs=keep_attrs,
1438             dask="allowed",
1439         )

</code><intended_behavior>Similar to `Variable.astype`, any `astype` method shared across xarray objects which does not currently support the `order` parameter should be modified to include and appropriately handle this parameter. The implementation should ensure that the `order` parameter meaningfully alters the memory layout of the array, respecting the underlying data handling and representation logic in xarray.</intended_behavior>, <file>xarray/core/common.py</file>
<class>DataWithCoords</class>
<method>None</method>
<code>
330 class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
331     """Shared base class for Dataset and DataArray."""
332 
333     __slots__ = ()
334 
335     _rolling_exp_cls = RollingExp
336 
337     def squeeze(
338         self,
339         dim: Union[Hashable, Iterable[Hashable], None] = None,
340         drop: bool = False,
341         axis: Union[int, Iterable[int], None] = None,
342     ):
343         """Return a new object with squeezed data.
344 
345         Parameters
346         ----------
347         dim : None or Hashable or iterable of Hashable, optional
348             Selects a subset of the length one dimensions. If a dimension is
349             selected with length greater than one, an error is raised. If
350             None, all length one dimensions are squeezed.
351         drop : bool, optional
352             If ``drop=True``, drop squeezed coordinates instead of making them
353             scalar.
354         axis : None or int or iterable of int, optional
355             Like dim, but positional.
356 
357         Returns
358         -------
359         squeezed : same type as caller
360             This object, but with with all or a subset of the dimensions of
361             length 1 removed.
362 
363         See Also
364         --------
365         numpy.squeeze
366         """
367         dims = get_squeeze_dims(self, dim, axis)
368         return self.isel(drop=drop, **{d: 0 for d in dims})
369 
370     def get_index(self, key: Hashable) -> pd.Index:
371         """Get an index for a dimension, with fall-back to a default RangeIndex"""
372         if key not in self.dims:
373             raise KeyError(key)
374 
375         try:
376             return self.indexes[key]
377         except KeyError:
378             # need to ensure dtype=int64 in case range is empty on Python 2
379             return pd.Index(range(self.sizes[key]), name=key, dtype=np.int64)
380 
381     def _calc_assign_results(
382         self: C, kwargs: Mapping[Hashable, Union[T, Callable[[C], T]]]
383     ) -> Dict[Hashable, T]:
384         return {k: v(self) if callable(v) else v for k, v in kwargs.items()}
385 
386     def assign_coords(self, coords=None, **coords_kwargs):
387         """Assign new coordinates to this object.
388 
389         Returns a new object with all the original data in addition to the new
390         coordinates.
391 
392         Parameters
393         ----------
394         coords : dict, optional
395             A dict where the keys are the names of the coordinates
396             with the new values to assign. If the values are callable, they are
397             computed on this object and assigned to new coordinate variables.
398             If the values are not callable, (e.g. a ``DataArray``, scalar, or
399             array), they are simply assigned. A new coordinate can also be
400             defined and attached to an existing dimension using a tuple with
401             the first element the dimension name and the second element the
402             values for this new coordinate.
403 
404         **coords_kwargs : optional
405             The keyword arguments form of ``coords``.
406             One of ``coords`` or ``coords_kwargs`` must be provided.
407 
408         Returns
409         -------
410         assigned : same type as caller
411             A new object with the new coordinates in addition to the existing
412             data.
413 
414         Examples
415         --------
416         Convert longitude coordinates from 0-359 to -180-179:
417 
418         >>> da = xr.DataArray(
419         ...     np.random.rand(4),
420         ...     coords=[np.array([358, 359, 0, 1])],
421         ...     dims="lon",
422         ... )
423         >>> da
424         <xarray.DataArray (lon: 4)>
425         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
426         Coordinates:
427           * lon      (lon) int64 358 359 0 1
428         >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))
429         <xarray.DataArray (lon: 4)>
430         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
431         Coordinates:
432           * lon      (lon) int64 -2 -1 0 1
433 
434         The function also accepts dictionary arguments:
435 
436         >>> da.assign_coords({"lon": (((da.lon + 180) % 360) - 180)})
437         <xarray.DataArray (lon: 4)>
438         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
439         Coordinates:
440           * lon      (lon) int64 -2 -1 0 1
441 
442         New coordinate can also be attached to an existing dimension:
443 
444         >>> lon_2 = np.array([300, 289, 0, 1])
445         >>> da.assign_coords(lon_2=("lon", lon_2))
446         <xarray.DataArray (lon: 4)>
447         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
448         Coordinates:
449           * lon      (lon) int64 358 359 0 1
450             lon_2    (lon) int64 300 289 0 1
451 
452         Note that the same result can also be obtained with a dict e.g.
453 
454         >>> _ = da.assign_coords({"lon_2": ("lon", lon_2)})
455 
456         Notes
457         -----
458         Since ``coords_kwargs`` is a dictionary, the order of your arguments
459         may not be preserved, and so the order of the new variables is not well
460         defined. Assigning multiple variables within the same ``assign_coords``
461         is possible, but you cannot reference other variables created within
462         the same ``assign_coords`` call.
463 
464         See also
465         --------
466         Dataset.assign
467         Dataset.swap_dims
468         """
469         coords_kwargs = either_dict_or_kwargs(coords, coords_kwargs, "assign_coords")
470         data = self.copy(deep=False)
471         results = self._calc_assign_results(coords_kwargs)
472         data.coords.update(results)
473         return data
474 
475     def assign_attrs(self, *args, **kwargs):
476         """Assign new attrs to this object.
477 
478         Returns a new object equivalent to ``self.attrs.update(*args, **kwargs)``.
479 
480         Parameters
481         ----------
482         args
483             positional arguments passed into ``attrs.update``.
484         kwargs
485             keyword arguments passed into ``attrs.update``.
486 
487         Returns
488         -------
489         assigned : same type as caller
490             A new object with the new attrs in addition to the existing data.
491 
492         See also
493         --------
494         Dataset.assign
495         """
496         out = self.copy(deep=False)
497         out.attrs.update(*args, **kwargs)
498         return out
499 
500     def pipe(
501         self,
502         func: Union[Callable[..., T], Tuple[Callable[..., T], str]],
503         *args,
504         **kwargs,
505     ) -> T:
506         """
507         Apply ``func(self, *args, **kwargs)``
508 
509         This method replicates the pandas method of the same name.
510 
511         Parameters
512         ----------
513         func : callable
514             function to apply to this xarray object (Dataset/DataArray).
515             ``args``, and ``kwargs`` are passed into ``func``.
516             Alternatively a ``(callable, data_keyword)`` tuple where
517             ``data_keyword`` is a string indicating the keyword of
518             ``callable`` that expects the xarray object.
519         args
520             positional arguments passed into ``func``.
521         kwargs
522             a dictionary of keyword arguments passed into ``func``.
523 
524         Returns
525         -------
526         object : Any
527             the return type of ``func``.
528 
529         Notes
530         -----
531 
532         Use ``.pipe`` when chaining together functions that expect
533         xarray or pandas objects, e.g., instead of writing
534 
535         .. code:: python
536 
537             f(g(h(ds), arg1=a), arg2=b, arg3=c)
538 
539         You can write
540 
541         .. code:: python
542 
543             (ds.pipe(h).pipe(g, arg1=a).pipe(f, arg2=b, arg3=c))
544 
545         If you have a function that takes the data as (say) the second
546         argument, pass a tuple indicating which keyword expects the
547         data. For example, suppose ``f`` takes its data as ``arg2``:
548 
549         .. code:: python
550 
551             (ds.pipe(h).pipe(g, arg1=a).pipe((f, "arg2"), arg1=a, arg3=c))
552 
553         Examples
554         --------
555 
556         >>> import numpy as np
557         >>> import xarray as xr
558         >>> x = xr.Dataset(
559         ...     {
560         ...         "temperature_c": (
561         ...             ("lat", "lon"),
562         ...             20 * np.random.rand(4).reshape(2, 2),
563         ...         ),
564         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
565         ...     },
566         ...     coords={"lat": [10, 20], "lon": [150, 160]},
567         ... )
568         >>> x
569         <xarray.Dataset>
570         Dimensions:        (lat: 2, lon: 2)
571         Coordinates:
572           * lat            (lat) int64 10 20
573           * lon            (lon) int64 150 160
574         Data variables:
575             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
576             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
577 
578         >>> def adder(data, arg):
579         ...     return data + arg
580         ...
581         >>> def div(data, arg):
582         ...     return data / arg
583         ...
584         >>> def sub_mult(data, sub_arg, mult_arg):
585         ...     return (data * mult_arg) - sub_arg
586         ...
587         >>> x.pipe(adder, 2)
588         <xarray.Dataset>
589         Dimensions:        (lat: 2, lon: 2)
590         Coordinates:
591           * lat            (lat) int64 10 20
592           * lon            (lon) int64 150 160
593         Data variables:
594             temperature_c  (lat, lon) float64 12.98 16.3 14.06 12.9
595             precipitation  (lat, lon) float64 2.424 2.646 2.438 2.892
596 
597         >>> x.pipe(adder, arg=2)
598         <xarray.Dataset>
599         Dimensions:        (lat: 2, lon: 2)
600         Coordinates:
601           * lat            (lat) int64 10 20
602           * lon            (lon) int64 150 160
603         Data variables:
604             temperature_c  (lat, lon) float64 12.98 16.3 14.06 12.9
605             precipitation  (lat, lon) float64 2.424 2.646 2.438 2.892
606 
607         >>> (
608         ...     x.pipe(adder, arg=2)
609         ...     .pipe(div, arg=2)
610         ...     .pipe(sub_mult, sub_arg=2, mult_arg=2)
611         ... )
612         <xarray.Dataset>
613         Dimensions:        (lat: 2, lon: 2)
614         Coordinates:
615           * lat            (lat) int64 10 20
616           * lon            (lon) int64 150 160
617         Data variables:
618             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
619             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
620 
621         See Also
622         --------
623         pandas.DataFrame.pipe
624         """
625         if isinstance(func, tuple):
626             func, target = func
627             if target in kwargs:
628                 raise ValueError(
629                     "%s is both the pipe target and a keyword argument" % target
630                 )
631             kwargs[target] = self
632             return func(*args, **kwargs)
633         else:
634             return func(self, *args, **kwargs)
635 
636     def groupby(self, group, squeeze: bool = True, restore_coord_dims: bool = None):
637         """Returns a GroupBy object for performing grouped operations.
638 
639         Parameters
640         ----------
641         group : str, DataArray or IndexVariable
642             Array whose unique values should be used to group this array. If a
643             string, must be the name of a variable contained in this dataset.
644         squeeze : bool, optional
645             If "group" is a dimension of any arrays in this dataset, `squeeze`
646             controls whether the subarrays have a dimension of length 1 along
647             that dimension or if the dimension is squeezed out.
648         restore_coord_dims : bool, optional
649             If True, also restore the dimension order of multi-dimensional
650             coordinates.
651 
652         Returns
653         -------
654         grouped
655             A `GroupBy` object patterned after `pandas.GroupBy` that can be
656             iterated over in the form of `(unique_value, grouped_array)` pairs.
657 
658         Examples
659         --------
660         Calculate daily anomalies for daily data:
661 
662         >>> da = xr.DataArray(
663         ...     np.linspace(0, 1826, num=1827),
664         ...     coords=[pd.date_range("1/1/2000", "31/12/2004", freq="D")],
665         ...     dims="time",
666         ... )
667         >>> da
668         <xarray.DataArray (time: 1827)>
669         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
670                1.826e+03])
671         Coordinates:
672           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
673         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
674         <xarray.DataArray (time: 1827)>
675         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
676         Coordinates:
677           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
678             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
679 
680         See Also
681         --------
682         core.groupby.DataArrayGroupBy
683         core.groupby.DatasetGroupBy
684         """
685         # While we don't generally check the type of every arg, passing
686         # multiple dimensions as multiple arguments is common enough, and the
687         # consequences hidden enough (strings evaluate as true) to warrant
688         # checking here.
689         # A future version could make squeeze kwarg only, but would face
690         # backward-compat issues.
691         if not isinstance(squeeze, bool):
692             raise TypeError(
693                 f"`squeeze` must be True or False, but {squeeze} was supplied"
694             )
695 
696         return self._groupby_cls(
697             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
698         )
699 
700     def groupby_bins(
701         self,
702         group,
703         bins,
704         right: bool = True,
705         labels=None,
706         precision: int = 3,
707         include_lowest: bool = False,
708         squeeze: bool = True,
709         restore_coord_dims: bool = None,
710     ):
711         """Returns a GroupBy object for performing grouped operations.
712 
713         Rather than using all unique values of `group`, the values are discretized
714         first by applying `pandas.cut` [1]_ to `group`.
715 
716         Parameters
717         ----------
718         group : str, DataArray or IndexVariable
719             Array whose binned values should be used to group this array. If a
720             string, must be the name of a variable contained in this dataset.
721         bins : int or array-like
722             If bins is an int, it defines the number of equal-width bins in the
723             range of x. However, in this case, the range of x is extended by .1%
724             on each side to include the min or max values of x. If bins is a
725             sequence it defines the bin edges allowing for non-uniform bin
726             width. No extension of the range of x is done in this case.
727         right : bool, default: True
728             Indicates whether the bins include the rightmost edge or not. If
729             right == True (the default), then the bins [1,2,3,4] indicate
730             (1,2], (2,3], (3,4].
731         labels : array-like or bool, default: None
732             Used as labels for the resulting bins. Must be of the same length as
733             the resulting bins. If False, string bin labels are assigned by
734             `pandas.cut`.
735         precision : int
736             The precision at which to store and display the bins labels.
737         include_lowest : bool
738             Whether the first interval should be left-inclusive or not.
739         squeeze : bool, default: True
740             If "group" is a dimension of any arrays in this dataset, `squeeze`
741             controls whether the subarrays have a dimension of length 1 along
742             that dimension or if the dimension is squeezed out.
743         restore_coord_dims : bool, optional
744             If True, also restore the dimension order of multi-dimensional
745             coordinates.
746 
747         Returns
748         -------
749         grouped
750             A `GroupBy` object patterned after `pandas.GroupBy` that can be
751             iterated over in the form of `(unique_value, grouped_array)` pairs.
752             The name of the group has the added suffix `_bins` in order to
753             distinguish it from the original variable.
754 
755         References
756         ----------
757         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
758         """
759         return self._groupby_cls(
760             self,
761             group,
762             squeeze=squeeze,
763             bins=bins,
764             restore_coord_dims=restore_coord_dims,
765             cut_kwargs={
766                 "right": right,
767                 "labels": labels,
768                 "precision": precision,
769                 "include_lowest": include_lowest,
770             },
771         )
772 
773     def weighted(self, weights):
774         """
775         Weighted operations.
776 
777         Parameters
778         ----------
779         weights : DataArray
780             An array of weights associated with the values in this Dataset.
781             Each value in the data contributes to the reduction operation
782             according to its associated weight.
783 
784         Notes
785         -----
786         ``weights`` must be a DataArray and cannot contain missing values.
787         Missing values can be replaced by ``weights.fillna(0)``.
788         """
789 
790         return self._weighted_cls(self, weights)
791 
792     def rolling(
793         self,
794         dim: Mapping[Hashable, int] = None,
795         min_periods: int = None,
796         center: Union[bool, Mapping[Hashable, bool]] = False,
797         keep_attrs: bool = None,
798         **window_kwargs: int,
799     ):
800         """
801         Rolling window object.
802 
803         Parameters
804         ----------
805         dim: dict, optional
806             Mapping from the dimension name to create the rolling iterator
807             along (e.g. `time`) to its moving window size.
808         min_periods : int, default: None
809             Minimum number of observations in window required to have a value
810             (otherwise result is NA). The default, None, is equivalent to
811             setting min_periods equal to the size of the window.
812         center : bool or mapping, default: False
813             Set the labels at the center of the window.
814         **window_kwargs : optional
815             The keyword arguments form of ``dim``.
816             One of dim or window_kwargs must be provided.
817 
818         Returns
819         -------
820         Rolling object (core.rolling.DataArrayRolling for DataArray,
821         core.rolling.DatasetRolling for Dataset.)
822 
823         Examples
824         --------
825         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
826 
827         >>> da = xr.DataArray(
828         ...     np.linspace(0, 11, num=12),
829         ...     coords=[
830         ...         pd.date_range(
831         ...             "15/12/1999",
832         ...             periods=12,
833         ...             freq=pd.DateOffset(months=1),
834         ...         )
835         ...     ],
836         ...     dims="time",
837         ... )
838         >>> da
839         <xarray.DataArray (time: 12)>
840         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
841         Coordinates:
842           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
843         >>> da.rolling(time=3, center=True).mean()
844         <xarray.DataArray (time: 12)>
845         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
846         Coordinates:
847           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
848 
849         Remove the NaNs using ``dropna()``:
850 
851         >>> da.rolling(time=3, center=True).mean().dropna("time")
852         <xarray.DataArray (time: 10)>
853         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
854         Coordinates:
855           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
856 
857         See Also
858         --------
859         core.rolling.DataArrayRolling
860         core.rolling.DatasetRolling
861         """
862 
863         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
864         return self._rolling_cls(
865             self, dim, min_periods=min_periods, center=center, keep_attrs=keep_attrs
866         )
867 
868     def rolling_exp(
869         self,
870         window: Mapping[Hashable, int] = None,
871         window_type: str = "span",
872         **window_kwargs,
873     ):
874         """
875         Exponentially-weighted moving window.
876         Similar to EWM in pandas
877 
878         Requires the optional Numbagg dependency.
879 
880         Parameters
881         ----------
882         window : mapping of hashable to int, optional
883             A mapping from the name of the dimension to create the rolling
884             exponential window along (e.g. `time`) to the size of the moving window.
885         window_type : {"span", "com", "halflife", "alpha"}, default: "span"
886             The format of the previously supplied window. Each is a simple
887             numerical transformation of the others. Described in detail:
888             https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html
889         **window_kwargs : optional
890             The keyword arguments form of ``window``.
891             One of window or window_kwargs must be provided.
892 
893         See Also
894         --------
895         core.rolling_exp.RollingExp
896         """
897         window = either_dict_or_kwargs(window, window_kwargs, "rolling_exp")
898 
899         return self._rolling_exp_cls(self, window, window_type)
900 
901     def coarsen(
902         self,
903         dim: Mapping[Hashable, int] = None,
904         boundary: str = "exact",
905         side: Union[str, Mapping[Hashable, str]] = "left",
906         coord_func: str = "mean",
907         keep_attrs: bool = None,
908         **window_kwargs: int,
909     ):
910         """
911         Coarsen object.
912 
913         Parameters
914         ----------
915         dim : mapping of hashable to int, optional
916             Mapping from the dimension name to the window size.
917         boundary : {"exact", "trim", "pad"}, default: "exact"
918             If 'exact', a ValueError will be raised if dimension size is not a
919             multiple of the window size. If 'trim', the excess entries are
920             dropped. If 'pad', NA will be padded.
921         side : {"left", "right"} or mapping of str to {"left", "right"}
922         coord_func : str or mapping of hashable to str, default: "mean"
923             function (name) that is applied to the coordinates,
924             or a mapping from coordinate name to function (name).
925         keep_attrs : bool, optional
926             If True, the object's attributes (`attrs`) will be copied from
927             the original object to the new one.  If False (default), the new
928             object will be returned without attributes.
929 
930         Returns
931         -------
932         Coarsen object (core.rolling.DataArrayCoarsen for DataArray,
933         core.rolling.DatasetCoarsen for Dataset.)
934 
935         Examples
936         --------
937         Coarsen the long time series by averaging over every four days.
938 
939         >>> da = xr.DataArray(
940         ...     np.linspace(0, 364, num=364),
941         ...     dims="time",
942         ...     coords={"time": pd.date_range("15/12/1999", periods=364)},
943         ... )
944         >>> da  # +doctest: ELLIPSIS
945         <xarray.DataArray (time: 364)>
946         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
947                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
948                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
949         ...
950                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
951                360.99173554, 361.99449036, 362.99724518, 364.        ])
952         Coordinates:
953           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
954         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
955         <xarray.DataArray (time: 121)>
956         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
957                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
958                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
959         ...
960                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
961                361.99449036])
962         Coordinates:
963           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
964         >>>
965 
966         See Also
967         --------
968         core.rolling.DataArrayCoarsen
969         core.rolling.DatasetCoarsen
970         """
971         if keep_attrs is None:
972             keep_attrs = _get_keep_attrs(default=False)
973 
974         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
975         return self._coarsen_cls(
976             self,
977             dim,
978             boundary=boundary,
979             side=side,
980             coord_func=coord_func,
981             keep_attrs=keep_attrs,
982         )
983 
984     def resample(
985         self,
986         indexer: Mapping[Hashable, str] = None,
987         skipna=None,
988         closed: str = None,
989         label: str = None,
990         base: int = 0,
991         keep_attrs: bool = None,
992         loffset=None,
993         restore_coord_dims: bool = None,
994         **indexer_kwargs: str,
995     ):
996         """Returns a Resample object for performing resampling operations.
997 
998         Handles both downsampling and upsampling. The resampled
999         dimension must be a datetime-like coordinate. If any intervals
1000         contain no values from the original object, they will be given
1001         the value ``NaN``.
1002 
1003         Parameters
1004         ----------
1005         indexer : {dim: freq}, optional
1006             Mapping from the dimension name to resample frequency [1]_. The
1007             dimension must be datetime-like.
1008         skipna : bool, optional
1009             Whether to skip missing values when aggregating in downsampling.
1010         closed : {"left", "right"}, optional
1011             Side of each interval to treat as closed.
1012         label : {"left", "right"}, optional
1013             Side of each interval to use for labeling.
1014         base : int, optional
1015             For frequencies that evenly subdivide 1 day, the "origin" of the
1016             aggregated intervals. For example, for "24H" frequency, base could
1017             range from 0 through 23.
1018         loffset : timedelta or str, optional
1019             Offset used to adjust the resampled time labels. Some pandas date
1020             offset strings are supported.
1021         keep_attrs : bool, optional
1022             If True, the object's attributes (`attrs`) will be copied from
1023             the original object to the new one.  If False (default), the new
1024             object will be returned without attributes.
1025         restore_coord_dims : bool, optional
1026             If True, also restore the dimension order of multi-dimensional
1027             coordinates.
1028         **indexer_kwargs : {dim: freq}
1029             The keyword arguments form of ``indexer``.
1030             One of indexer or indexer_kwargs must be provided.
1031 
1032         Returns
1033         -------
1034         resampled : same type as caller
1035             This object resampled.
1036 
1037         Examples
1038         --------
1039         Downsample monthly time-series data to seasonal data:
1040 
1041         >>> da = xr.DataArray(
1042         ...     np.linspace(0, 11, num=12),
1043         ...     coords=[
1044         ...         pd.date_range(
1045         ...             "15/12/1999",
1046         ...             periods=12,
1047         ...             freq=pd.DateOffset(months=1),
1048         ...         )
1049         ...     ],
1050         ...     dims="time",
1051         ... )
1052         >>> da
1053         <xarray.DataArray (time: 12)>
1054         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
1055         Coordinates:
1056           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
1057         >>> da.resample(time="QS-DEC").mean()
1058         <xarray.DataArray (time: 4)>
1059         array([ 1.,  4.,  7., 10.])
1060         Coordinates:
1061           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
1062 
1063         Upsample monthly time-series data to daily data:
1064 
1065         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
1066         <xarray.DataArray (time: 337)>
1067         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
1068                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
1069                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
1070         ...
1071                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
1072                10.96774194, 11.        ])
1073         Coordinates:
1074           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
1075 
1076         Limit scope of upsampling method
1077 
1078         >>> da.resample(time="1D").nearest(tolerance="1D")
1079         <xarray.DataArray (time: 337)>
1080         array([ 0.,  0., nan, ..., nan, 11., 11.])
1081         Coordinates:
1082           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
1083 
1084         See Also
1085         --------
1086         pandas.Series.resample
1087         pandas.DataFrame.resample
1088 
1089         References
1090         ----------
1091 
1092         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
1093         """
1094         # TODO support non-string indexer after removing the old API.
1095 
1096         from ..coding.cftimeindex import CFTimeIndex
1097         from .dataarray import DataArray
1098         from .resample import RESAMPLE_DIM
1099 
1100         if keep_attrs is None:
1101             keep_attrs = _get_keep_attrs(default=False)
1102 
1103         # note: the second argument (now 'skipna') use to be 'dim'
1104         if (
1105             (skipna is not None and not isinstance(skipna, bool))
1106             or ("how" in indexer_kwargs and "how" not in self.dims)
1107             or ("dim" in indexer_kwargs and "dim" not in self.dims)
1108         ):
1109             raise TypeError(
1110                 "resample() no longer supports the `how` or "
1111                 "`dim` arguments. Instead call methods on resample "
1112                 "objects, e.g., data.resample(time='1D').mean()"
1113             )
1114 
1115         indexer = either_dict_or_kwargs(indexer, indexer_kwargs, "resample")
1116         if len(indexer) != 1:
1117             raise ValueError("Resampling only supported along single dimensions.")
1118         dim, freq = next(iter(indexer.items()))
1119 
1120         dim_name = dim
1121         dim_coord = self[dim]
1122 
1123         # TODO: remove once pandas=1.1 is the minimum required version
1124         with warnings.catch_warnings():
1125             warnings.filterwarnings(
1126                 "ignore",
1127                 r"'(base|loffset)' in .resample\(\) and in Grouper\(\) is deprecated.",
1128                 category=FutureWarning,
1129             )
1130 
1131             if isinstance(self.indexes[dim_name], CFTimeIndex):
1132                 from .resample_cftime import CFTimeGrouper
1133 
1134                 grouper = CFTimeGrouper(freq, closed, label, base, loffset)
1135             else:
1136                 grouper = pd.Grouper(
1137                     freq=freq, closed=closed, label=label, base=base, loffset=loffset
1138                 )
1139         group = DataArray(
1140             dim_coord, coords=dim_coord.coords, dims=dim_coord.dims, name=RESAMPLE_DIM
1141         )
1142         resampler = self._resample_cls(
1143             self,
1144             group=group,
1145             dim=dim_name,
1146             grouper=grouper,
1147             resample_dim=RESAMPLE_DIM,
1148             restore_coord_dims=restore_coord_dims,
1149         )
1150 
1151         return resampler
1152 
1153     def where(self, cond, other=dtypes.NA, drop: bool = False):
1154         """Filter elements from this object according to a condition.
1155 
1156         This operation follows the normal broadcasting and alignment rules that
1157         xarray uses for binary arithmetic.
1158 
1159         Parameters
1160         ----------
1161         cond : DataArray, Dataset, or callable
1162             Locations at which to preserve this object's values. dtype must be `bool`.
1163             If a callable, it must expect this object as its only parameter.
1164         other : scalar, DataArray or Dataset, optional
1165             Value to use for locations in this object where ``cond`` is False.
1166             By default, these locations filled with NA.
1167         drop : bool, optional
1168             If True, coordinate labels that only correspond to False values of
1169             the condition are dropped from the result. Mutually exclusive with
1170             ``other``.
1171 
1172         Returns
1173         -------
1174         DataArray or Dataset
1175             Same xarray type as caller, with dtype float64.
1176 
1177         Examples
1178         --------
1179 
1180         >>> import numpy as np
1181         >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1182         >>> a
1183         <xarray.DataArray (x: 5, y: 5)>
1184         array([[ 0,  1,  2,  3,  4],
1185                [ 5,  6,  7,  8,  9],
1186                [10, 11, 12, 13, 14],
1187                [15, 16, 17, 18, 19],
1188                [20, 21, 22, 23, 24]])
1189         Dimensions without coordinates: x, y
1190 
1191         >>> a.where(a.x + a.y < 4)
1192         <xarray.DataArray (x: 5, y: 5)>
1193         array([[ 0.,  1.,  2.,  3., nan],
1194                [ 5.,  6.,  7., nan, nan],
1195                [10., 11., nan, nan, nan],
1196                [15., nan, nan, nan, nan],
1197                [nan, nan, nan, nan, nan]])
1198         Dimensions without coordinates: x, y
1199 
1200         >>> a.where(a.x + a.y < 5, -1)
1201         <xarray.DataArray (x: 5, y: 5)>
1202         array([[ 0,  1,  2,  3,  4],
1203                [ 5,  6,  7,  8, -1],
1204                [10, 11, 12, -1, -1],
1205                [15, 16, -1, -1, -1],
1206                [20, -1, -1, -1, -1]])
1207         Dimensions without coordinates: x, y
1208 
1209         >>> a.where(a.x + a.y < 4, drop=True)
1210         <xarray.DataArray (x: 4, y: 4)>
1211         array([[ 0.,  1.,  2.,  3.],
1212                [ 5.,  6.,  7., nan],
1213                [10., 11., nan, nan],
1214                [15., nan, nan, nan]])
1215         Dimensions without coordinates: x, y
1216 
1217         >>> a.where(lambda x: x.x + x.y < 4, drop=True)
1218         <xarray.DataArray (x: 4, y: 4)>
1219         array([[ 0.,  1.,  2.,  3.],
1220                [ 5.,  6.,  7., nan],
1221                [10., 11., nan, nan],
1222                [15., nan, nan, nan]])
1223         Dimensions without coordinates: x, y
1224 
1225         See also
1226         --------
1227         numpy.where : corresponding numpy function
1228         where : equivalent function
1229         """
1230         from .alignment import align
1231         from .dataarray import DataArray
1232         from .dataset import Dataset
1233 
1234         if callable(cond):
1235             cond = cond(self)
1236 
1237         if drop:
1238             if other is not dtypes.NA:
1239                 raise ValueError("cannot set `other` if drop=True")
1240 
1241             if not isinstance(cond, (Dataset, DataArray)):
1242                 raise TypeError(
1243                     "cond argument is %r but must be a %r or %r"
1244                     % (cond, Dataset, DataArray)
1245                 )
1246 
1247             # align so we can use integer indexing
1248             self, cond = align(self, cond)
1249 
1250             # get cond with the minimal size needed for the Dataset
1251             if isinstance(cond, Dataset):
1252                 clipcond = cond.to_array().any("variable")
1253             else:
1254                 clipcond = cond
1255 
1256             # clip the data corresponding to coordinate dims that are not used
1257             nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))
1258             indexers = {k: np.unique(v) for k, v in nonzeros}
1259 
1260             self = self.isel(**indexers)
1261             cond = cond.isel(**indexers)
1262 
1263         return ops.where_method(self, cond, other)
1264 
1265     def close(self: Any) -> None:
1266         """Close any files linked to this object"""
1267         if self._file_obj is not None:
1268             self._file_obj.close()
1269         self._file_obj = None
1270 
1271     def isnull(self, keep_attrs: bool = None):
1272         """Test each value in the array for whether it is a missing value.
1273 
1274         Returns
1275         -------
1276         isnull : DataArray or Dataset
1277             Same type and shape as object, but the dtype of the data is bool.
1278 
1279         See Also
1280         --------
1281         pandas.isnull
1282 
1283         Examples
1284         --------
1285         >>> array = xr.DataArray([1, np.nan, 3], dims="x")
1286         >>> array
1287         <xarray.DataArray (x: 3)>
1288         array([ 1., nan,  3.])
1289         Dimensions without coordinates: x
1290         >>> array.isnull()
1291         <xarray.DataArray (x: 3)>
1292         array([False,  True, False])
1293         Dimensions without coordinates: x
1294         """
1295         from .computation import apply_ufunc
1296 
1297         if keep_attrs is None:
1298             keep_attrs = _get_keep_attrs(default=False)
1299 
1300         return apply_ufunc(
1301             duck_array_ops.isnull,
1302             self,
1303             dask="allowed",
1304             keep_attrs=keep_attrs,
1305         )
1306 
1307     def notnull(self, keep_attrs: bool = None):
1308         """Test each value in the array for whether it is not a missing value.
1309 
1310         Returns
1311         -------
1312         notnull : DataArray or Dataset
1313             Same type and shape as object, but the dtype of the data is bool.
1314 
1315         See Also
1316         --------
1317         pandas.notnull
1318 
1319         Examples
1320         --------
1321         >>> array = xr.DataArray([1, np.nan, 3], dims="x")
1322         >>> array
1323         <xarray.DataArray (x: 3)>
1324         array([ 1., nan,  3.])
1325         Dimensions without coordinates: x
1326         >>> array.notnull()
1327         <xarray.DataArray (x: 3)>
1328         array([ True, False,  True])
1329         Dimensions without coordinates: x
1330         """
1331         from .computation import apply_ufunc
1332 
1333         if keep_attrs is None:
1334             keep_attrs = _get_keep_attrs(default=False)
1335 
1336         return apply_ufunc(
1337             duck_array_ops.notnull,
1338             self,
1339             dask="allowed",
1340             keep_attrs=keep_attrs,
1341         )
1342 
1343     def isin(self, test_elements):
1344         """Tests each value in the array for whether it is in test elements.
1345 
1346         Parameters
1347         ----------
1348         test_elements : array_like
1349             The values against which to test each value of `element`.
1350             This argument is flattened if an array or array_like.
1351             See numpy notes for behavior with non-array-like parameters.
1352 
1353         Returns
1354         -------
1355         isin : DataArray or Dataset
1356             Has the same type and shape as this object, but with a bool dtype.
1357 
1358         Examples
1359         --------
1360 
1361         >>> array = xr.DataArray([1, 2, 3], dims="x")
1362         >>> array.isin([1, 3])
1363         <xarray.DataArray (x: 3)>
1364         array([ True, False,  True])
1365         Dimensions without coordinates: x
1366 
1367         See also
1368         --------
1369         numpy.isin
1370         """
1371         from .computation import apply_ufunc
1372         from .dataarray import DataArray
1373         from .dataset import Dataset
1374         from .variable import Variable
1375 
1376         if isinstance(test_elements, Dataset):
1377             raise TypeError(
1378                 "isin() argument must be convertible to an array: {}".format(
1379                     test_elements
1380                 )
1381             )
1382         elif isinstance(test_elements, (Variable, DataArray)):
1383             # need to explicitly pull out data to support dask arrays as the
1384             # second argument
1385             test_elements = test_elements.data
1386 
1387         return apply_ufunc(
1388             duck_array_ops.isin,
1389             self,
1390             kwargs=dict(test_elements=test_elements),
1391             dask="allowed",
1392         )
1393 
1394     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
1395         """
1396         Copy of the xarray object, with data cast to a specified type.
1397         Leaves coordinate dtype unchanged.
1398 
1399         Parameters
1400         ----------
1401         dtype : str or dtype
1402             Typecode or data-type to which the array is cast.
1403         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
1404             Controls what kind of data casting may occur. Defaults to 'unsafe'
1405             for backwards compatibility.
1406 
1407             * 'no' means the data types should not be cast at all.
1408             * 'equiv' means only byte-order changes are allowed.
1409             * 'safe' means only casts which can preserve values are allowed.
1410             * 'same_kind' means only safe casts or casts within a kind,
1411                 like float64 to float32, are allowed.
1412             * 'unsafe' means any data conversions may be done.
1413         copy : bool, optional
1414             By default, astype always returns a newly allocated array. If this
1415             is set to False and the `dtype` requirement is satisfied, the input
1416             array is returned instead of a copy.
1417         keep_attrs : bool, optional
1418             By default, astype keeps attributes. Set to False to remove
1419             attributes in the returned object.
1420 
1421         Returns
1422         -------
1423         out : same as object
1424             New object with data cast to the specified type.
1425 
1426         See also
1427         --------
1428         np.ndarray.astype
1429         dask.array.Array.astype
1430         """
1431         from .computation import apply_ufunc
1432 
1433         return apply_ufunc(
1434             duck_array_ops.astype,
1435             self,
1436             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
1437             keep_attrs=keep_attrs,
1438             dask="allowed",
1439         )
1440 
1441     def __enter__(self: T) -> T:
1442         return self
1443 
1444     def __exit__(self, exc_type, exc_value, traceback) -> None:
1445         self.close()
1446 
1447     def __getitem__(self, value):
1448         # implementations of this class should implement this method
1449         raise NotImplementedError()

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 08:44:51.307 | INFO     | Search completed. Bug locations: [<file>xarray/core/variable.py</file>
<class>Variable</class>
<method>astype</method>
<code>
373     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
374         """
375         Copy of the Variable object, with data cast to a specified type.
376 
377         Parameters
378         ----------
379         dtype : str or dtype
380              Typecode or data-type to which the array is cast.
381         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
382              Controls what kind of data casting may occur. Defaults to 'unsafe'
383              for backwards compatibility.
384 
385              * 'no' means the data types should not be cast at all.
386              * 'equiv' means only byte-order changes are allowed.
387              * 'safe' means only casts which can preserve values are allowed.
388              * 'same_kind' means only safe casts or casts within a kind,
389                  like float64 to float32, are allowed.
390              * 'unsafe' means any data conversions may be done.
391         copy : bool, optional
392              By default, astype always returns a newly allocated array. If this
393              is set to False and the `dtype` requirement is satisfied, the input
394              array is returned instead of a copy.
395         keep_attrs : bool, optional
396             By default, astype keeps attributes. Set to False to remove
397             attributes in the returned object.
398 
399         Returns
400         -------
401         out : same as object
402             New object with data cast to the specified type.
403 
404         See also
405         --------
406         np.ndarray.astype
407         dask.array.Array.astype
408         """
409         from .computation import apply_ufunc
410 
411         return apply_ufunc(
412             duck_array_ops.astype,
413             self,
414             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
415             keep_attrs=keep_attrs,
416             dask="allowed",
417         )

</code><intended_behavior>The `astype` method should be updated to include an `order` parameter, aligning its functionality with that of `numpy.ndarray.astype`. This method should then handle the `order` parameter appropriately when casting the data type of the arrays. The implementation should ensure that when the `order` parameter is specified, it affects the memory layout of the returned array as expected ('C' for row-major and 'F' for column-major order, etc.). This change must preserve the existing functionality of allowing data type casting while extending it to support order specification.</intended_behavior>, <file>xarray/core/variable.py</file>
<class>Variable</class>
<method>None</method>
<code>
283 class Variable(
284     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin
285 ):
286     """A netcdf-like variable consisting of dimensions, data and attributes
287     which describe a single Array. A single Variable object is not fully
288     described outside the context of its parent Dataset (if you want such a
289     fully described object, use a DataArray instead).
290 
291     The main functional difference between Variables and numpy arrays is that
292     numerical operations on Variables implement array broadcasting by dimension
293     name. For example, adding an Variable with dimensions `('time',)` to
294     another Variable with dimensions `('space',)` results in a new Variable
295     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations
296     like ``mean`` or ``sum`` are overwritten to take a "dimension" argument
297     instead of an "axis".
298 
299     Variables are light-weight objects used as the building block for datasets.
300     They are more primitive objects, so operations with them provide marginally
301     higher performance than using DataArrays. However, manipulating data in the
302     form of a Dataset or DataArray should almost always be preferred, because
303     they can use more complete metadata in context of coordinate labels.
304     """
305 
306     __slots__ = ("_dims", "_data", "_attrs", "_encoding")
307 
308     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):
309         """
310         Parameters
311         ----------
312         dims : str or sequence of str
313             Name(s) of the the data dimension(s). Must be either a string (only
314             for 1D data) or a sequence of strings with length equal to the
315             number of dimensions.
316         data : array_like
317             Data array which supports numpy-like data access.
318         attrs : dict_like or None, optional
319             Attributes to assign to the new variable. If None (default), an
320             empty attribute dictionary is initialized.
321         encoding : dict_like or None, optional
322             Dictionary specifying how to encode this array's data into a
323             serialized format like netCDF4. Currently used keys (for netCDF)
324             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.
325             Well-behaved code to serialize a Variable should ignore
326             unrecognized encoding items.
327         """
328         self._data = as_compatible_data(data, fastpath=fastpath)
329         self._dims = self._parse_dimensions(dims)
330         self._attrs = None
331         self._encoding = None
332         if attrs is not None:
333             self.attrs = attrs
334         if encoding is not None:
335             self.encoding = encoding
336 
337     @property
338     def dtype(self):
339         return self._data.dtype
340 
341     @property
342     def shape(self):
343         return self._data.shape
344 
345     @property
346     def nbytes(self):
347         return self.size * self.dtype.itemsize
348 
349     @property
350     def _in_memory(self):
351         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (
352             isinstance(self._data, indexing.MemoryCachedArray)
353             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)
354         )
355 
356     @property
357     def data(self):
358         if is_duck_array(self._data):
359             return self._data
360         else:
361             return self.values
362 
363     @data.setter
364     def data(self, data):
365         data = as_compatible_data(data)
366         if data.shape != self.shape:
367             raise ValueError(
368                 f"replacement data must match the Variable's shape. "
369                 f"replacement data has shape {data.shape}; Variable has shape {self.shape}"
370             )
371         self._data = data
372 
373     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
374         """
375         Copy of the Variable object, with data cast to a specified type.
376 
377         Parameters
378         ----------
379         dtype : str or dtype
380              Typecode or data-type to which the array is cast.
381         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
382              Controls what kind of data casting may occur. Defaults to 'unsafe'
383              for backwards compatibility.
384 
385              * 'no' means the data types should not be cast at all.
386              * 'equiv' means only byte-order changes are allowed.
387              * 'safe' means only casts which can preserve values are allowed.
388              * 'same_kind' means only safe casts or casts within a kind,
389                  like float64 to float32, are allowed.
390              * 'unsafe' means any data conversions may be done.
391         copy : bool, optional
392              By default, astype always returns a newly allocated array. If this
393              is set to False and the `dtype` requirement is satisfied, the input
394              array is returned instead of a copy.
395         keep_attrs : bool, optional
396             By default, astype keeps attributes. Set to False to remove
397             attributes in the returned object.
398 
399         Returns
400         -------
401         out : same as object
402             New object with data cast to the specified type.
403 
404         See also
405         --------
406         np.ndarray.astype
407         dask.array.Array.astype
408         """
409         from .computation import apply_ufunc
410 
411         return apply_ufunc(
412             duck_array_ops.astype,
413             self,
414             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
415             keep_attrs=keep_attrs,
416             dask="allowed",
417         )
418 
419     def load(self, **kwargs):
420         """Manually trigger loading of this variable's data from disk or a
421         remote source into memory and return this variable.
422 
423         Normally, it should not be necessary to call this method in user code,
424         because all xarray functions should either work on deferred data or
425         load data automatically.
426 
427         Parameters
428         ----------
429         **kwargs : dict
430             Additional keyword arguments passed on to ``dask.array.compute``.
431 
432         See Also
433         --------
434         dask.array.compute
435         """
436         if is_duck_dask_array(self._data):
437             self._data = as_compatible_data(self._data.compute(**kwargs))
438         elif not is_duck_array(self._data):
439             self._data = np.asarray(self._data)
440         return self
441 
442     def compute(self, **kwargs):
443         """Manually trigger loading of this variable's data from disk or a
444         remote source into memory and return a new variable. The original is
445         left unaltered.
446 
447         Normally, it should not be necessary to call this method in user code,
448         because all xarray functions should either work on deferred data or
449         load data automatically.
450 
451         Parameters
452         ----------
453         **kwargs : dict
454             Additional keyword arguments passed on to ``dask.array.compute``.
455 
456         See Also
457         --------
458         dask.array.compute
459         """
460         new = self.copy(deep=False)
461         return new.load(**kwargs)
462 
463     def __dask_tokenize__(self):
464         # Use v.data, instead of v._data, in order to cope with the wrappers
465         # around NetCDF and the like
466         from dask.base import normalize_token
467 
468         return normalize_token((type(self), self._dims, self.data, self._attrs))
469 
470     def __dask_graph__(self):
471         if is_duck_dask_array(self._data):
472             return self._data.__dask_graph__()
473         else:
474             return None
475 
476     def __dask_keys__(self):
477         return self._data.__dask_keys__()
478 
479     def __dask_layers__(self):
480         return self._data.__dask_layers__()
481 
482     @property
483     def __dask_optimize__(self):
484         return self._data.__dask_optimize__
485 
486     @property
487     def __dask_scheduler__(self):
488         return self._data.__dask_scheduler__
489 
490     def __dask_postcompute__(self):
491         array_func, array_args = self._data.__dask_postcompute__()
492         return (
493             self._dask_finalize,
494             (array_func, array_args, self._dims, self._attrs, self._encoding),
495         )
496 
497     def __dask_postpersist__(self):
498         array_func, array_args = self._data.__dask_postpersist__()
499         return (
500             self._dask_finalize,
501             (array_func, array_args, self._dims, self._attrs, self._encoding),
502         )
503 
504     @staticmethod
505     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):
506         data = array_func(results, *array_args)
507         return Variable(dims, data, attrs=attrs, encoding=encoding)
508 
509     @property
510     def values(self):
511         """The variable's data as a numpy.ndarray"""
512         return _as_array_or_item(self._data)
513 
514     @values.setter
515     def values(self, values):
516         self.data = values
517 
518     def to_base_variable(self):
519         """Return this variable as a base xarray.Variable"""
520         return Variable(
521             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
522         )
523 
524     to_variable = utils.alias(to_base_variable, "to_variable")
525 
526     def to_index_variable(self):
527         """Return this variable as an xarray.IndexVariable"""
528         return IndexVariable(
529             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True
530         )
531 
532     to_coord = utils.alias(to_index_variable, "to_coord")
533 
534     def to_index(self):
535         """Convert this variable to a pandas.Index"""
536         return self.to_index_variable().to_index()
537 
538     def to_dict(self, data=True):
539         """Dictionary representation of variable."""
540         item = {"dims": self.dims, "attrs": decode_numpy_dict_values(self.attrs)}
541         if data:
542             item["data"] = ensure_us_time_resolution(self.values).tolist()
543         else:
544             item.update({"dtype": str(self.dtype), "shape": self.shape})
545         return item
546 
547     @property
548     def dims(self):
549         """Tuple of dimension names with which this variable is associated."""
550         return self._dims
551 
552     @dims.setter
553     def dims(self, value):
554         self._dims = self._parse_dimensions(value)
555 
556     def _parse_dimensions(self, dims):
557         if isinstance(dims, str):
558             dims = (dims,)
559         dims = tuple(dims)
560         if len(dims) != self.ndim:
561             raise ValueError(
562                 "dimensions %s must have the same length as the "
563                 "number of data dimensions, ndim=%s" % (dims, self.ndim)
564             )
565         return dims
566 
567     def _item_key_to_tuple(self, key):
568         if utils.is_dict_like(key):
569             return tuple(key.get(dim, slice(None)) for dim in self.dims)
570         else:
571             return key
572 
573     def _broadcast_indexes(self, key):
574         """Prepare an indexing key for an indexing operation.
575 
576         Parameters
577         -----------
578         key: int, slice, array-like, dict or tuple of integer, slice and array-like
579             Any valid input for indexing.
580 
581         Returns
582         -------
583         dims : tuple
584             Dimension of the resultant variable.
585         indexers : IndexingTuple subclass
586             Tuple of integer, array-like, or slices to use when indexing
587             self._data. The type of this argument indicates the type of
588             indexing to perform, either basic, outer or vectorized.
589         new_order : Optional[Sequence[int]]
590             Optional reordering to do on the result of indexing. If not None,
591             the first len(new_order) indexing should be moved to these
592             positions.
593         """
594         key = self._item_key_to_tuple(key)  # key is a tuple
595         # key is a tuple of full size
596         key = indexing.expanded_indexer(key, self.ndim)
597         # Convert a scalar Variable to an integer
598         key = tuple(
599             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key
600         )
601         # Convert a 0d-array to an integer
602         key = tuple(
603             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key
604         )
605 
606         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):
607             return self._broadcast_indexes_basic(key)
608 
609         self._validate_indexers(key)
610         # Detect it can be mapped as an outer indexer
611         # If all key is unlabeled, or
612         # key can be mapped as an OuterIndexer.
613         if all(not isinstance(k, Variable) for k in key):
614             return self._broadcast_indexes_outer(key)
615 
616         # If all key is 1-dimensional and there are no duplicate labels,
617         # key can be mapped as an OuterIndexer.
618         dims = []
619         for k, d in zip(key, self.dims):
620             if isinstance(k, Variable):
621                 if len(k.dims) > 1:
622                     return self._broadcast_indexes_vectorized(key)
623                 dims.append(k.dims[0])
624             elif not isinstance(k, integer_types):
625                 dims.append(d)
626         if len(set(dims)) == len(dims):
627             return self._broadcast_indexes_outer(key)
628 
629         return self._broadcast_indexes_vectorized(key)
630 
631     def _broadcast_indexes_basic(self, key):
632         dims = tuple(
633             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)
634         )
635         return dims, BasicIndexer(key), None
636 
637     def _validate_indexers(self, key):
638         """ Make sanity checks """
639         for dim, k in zip(self.dims, key):
640             if isinstance(k, BASIC_INDEXING_TYPES):
641                 pass
642             else:
643                 if not isinstance(k, Variable):
644                     k = np.asarray(k)
645                     if k.ndim > 1:
646                         raise IndexError(
647                             "Unlabeled multi-dimensional array cannot be "
648                             "used for indexing: {}".format(k)
649                         )
650                 if k.dtype.kind == "b":
651                     if self.shape[self.get_axis_num(dim)] != len(k):
652                         raise IndexError(
653                             "Boolean array size {:d} is used to index array "
654                             "with shape {:s}.".format(len(k), str(self.shape))
655                         )
656                     if k.ndim > 1:
657                         raise IndexError(
658                             "{}-dimensional boolean indexing is "
659                             "not supported. ".format(k.ndim)
660                         )
661                     if getattr(k, "dims", (dim,)) != (dim,):
662                         raise IndexError(
663                             "Boolean indexer should be unlabeled or on the "
664                             "same dimension to the indexed array. Indexer is "
665                             "on {:s} but the target dimension is {:s}.".format(
666                                 str(k.dims), dim
667                             )
668                         )
669 
670     def _broadcast_indexes_outer(self, key):
671         dims = tuple(
672             k.dims[0] if isinstance(k, Variable) else dim
673             for k, dim in zip(key, self.dims)
674             if not isinstance(k, integer_types)
675         )
676 
677         new_key = []
678         for k in key:
679             if isinstance(k, Variable):
680                 k = k.data
681             if not isinstance(k, BASIC_INDEXING_TYPES):
682                 k = np.asarray(k)
683                 if k.size == 0:
684                     # Slice by empty list; numpy could not infer the dtype
685                     k = k.astype(int)
686                 elif k.dtype.kind == "b":
687                     (k,) = np.nonzero(k)
688             new_key.append(k)
689 
690         return dims, OuterIndexer(tuple(new_key)), None
691 
692     def _nonzero(self):
693         """ Equivalent numpy's nonzero but returns a tuple of Varibles. """
694         # TODO we should replace dask's native nonzero
695         # after https://github.com/dask/dask/issues/1076 is implemented.
696         nonzeros = np.nonzero(self.data)
697         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))
698 
699     def _broadcast_indexes_vectorized(self, key):
700         variables = []
701         out_dims_set = OrderedSet()
702         for dim, value in zip(self.dims, key):
703             if isinstance(value, slice):
704                 out_dims_set.add(dim)
705             else:
706                 variable = (
707                     value
708                     if isinstance(value, Variable)
709                     else as_variable(value, name=dim)
710                 )
711                 if variable.dtype.kind == "b":  # boolean indexing case
712                     (variable,) = variable._nonzero()
713 
714                 variables.append(variable)
715                 out_dims_set.update(variable.dims)
716 
717         variable_dims = set()
718         for variable in variables:
719             variable_dims.update(variable.dims)
720 
721         slices = []
722         for i, (dim, value) in enumerate(zip(self.dims, key)):
723             if isinstance(value, slice):
724                 if dim in variable_dims:
725                     # We only convert slice objects to variables if they share
726                     # a dimension with at least one other variable. Otherwise,
727                     # we can equivalently leave them as slices aknd transpose
728                     # the result. This is significantly faster/more efficient
729                     # for most array backends.
730                     values = np.arange(*value.indices(self.sizes[dim]))
731                     variables.insert(i - len(slices), Variable((dim,), values))
732                 else:
733                     slices.append((i, value))
734 
735         try:
736             variables = _broadcast_compat_variables(*variables)
737         except ValueError:
738             raise IndexError(f"Dimensions of indexers mismatch: {key}")
739 
740         out_key = [variable.data for variable in variables]
741         out_dims = tuple(out_dims_set)
742         slice_positions = set()
743         for i, value in slices:
744             out_key.insert(i, value)
745             new_position = out_dims.index(self.dims[i])
746             slice_positions.add(new_position)
747 
748         if slice_positions:
749             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]
750         else:
751             new_order = None
752 
753         return out_dims, VectorizedIndexer(tuple(out_key)), new_order
754 
755     def __getitem__(self: VariableType, key) -> VariableType:
756         """Return a new Variable object whose contents are consistent with
757         getting the provided key from the underlying data.
758 
759         NB. __getitem__ and __setitem__ implement xarray-style indexing,
760         where if keys are unlabeled arrays, we index the array orthogonally
761         with them. If keys are labeled array (such as Variables), they are
762         broadcasted with our usual scheme and then the array is indexed with
763         the broadcasted key, like numpy's fancy indexing.
764 
765         If you really want to do indexing like `x[x > 0]`, manipulate the numpy
766         array `x.values` directly.
767         """
768         dims, indexer, new_order = self._broadcast_indexes(key)
769         data = as_indexable(self._data)[indexer]
770         if new_order:
771             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
772         return self._finalize_indexing_result(dims, data)
773 
774     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:
775         """Used by IndexVariable to return IndexVariable objects when possible."""
776         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
777 
778     def _getitem_with_mask(self, key, fill_value=dtypes.NA):
779         """Index this Variable with -1 remapped to fill_value."""
780         # TODO(shoyer): expose this method in public API somewhere (isel?) and
781         # use it for reindex.
782         # TODO(shoyer): add a sanity check that all other integers are
783         # non-negative
784         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value
785         # that is actually indexed rather than mapping it to the last value
786         # along each axis.
787 
788         if fill_value is dtypes.NA:
789             fill_value = dtypes.get_fill_value(self.dtype)
790 
791         dims, indexer, new_order = self._broadcast_indexes(key)
792 
793         if self.size:
794             if is_duck_dask_array(self._data):
795                 # dask's indexing is faster this way; also vindex does not
796                 # support negative indices yet:
797                 # https://github.com/dask/dask/pull/2967
798                 actual_indexer = indexing.posify_mask_indexer(indexer)
799             else:
800                 actual_indexer = indexer
801 
802             data = as_indexable(self._data)[actual_indexer]
803             mask = indexing.create_mask(indexer, self.shape, data)
804             # we need to invert the mask in order to pass data first. This helps
805             # pint to choose the correct unit
806             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed
807             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)
808         else:
809             # array cannot be indexed along dimensions of size 0, so just
810             # build the mask directly instead.
811             mask = indexing.create_mask(indexer, self.shape)
812             data = np.broadcast_to(fill_value, getattr(mask, "shape", ()))
813 
814         if new_order:
815             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)
816         return self._finalize_indexing_result(dims, data)
817 
818     def __setitem__(self, key, value):
819         """__setitem__ is overloaded to access the underlying numpy values with
820         orthogonal indexing.
821 
822         See __getitem__ for more details.
823         """
824         dims, index_tuple, new_order = self._broadcast_indexes(key)
825 
826         if not isinstance(value, Variable):
827             value = as_compatible_data(value)
828             if value.ndim > len(dims):
829                 raise ValueError(
830                     "shape mismatch: value array of shape %s could not be "
831                     "broadcast to indexing result with %s dimensions"
832                     % (value.shape, len(dims))
833                 )
834             if value.ndim == 0:
835                 value = Variable((), value)
836             else:
837                 value = Variable(dims[-value.ndim :], value)
838         # broadcast to become assignable
839         value = value.set_dims(dims).data
840 
841         if new_order:
842             value = duck_array_ops.asarray(value)
843             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]
844             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))
845 
846         indexable = as_indexable(self._data)
847         indexable[index_tuple] = value
848 
849     @property
850     def attrs(self) -> Dict[Hashable, Any]:
851         """Dictionary of local attributes on this variable."""
852         if self._attrs is None:
853             self._attrs = {}
854         return self._attrs
855 
856     @attrs.setter
857     def attrs(self, value: Mapping[Hashable, Any]) -> None:
858         self._attrs = dict(value)
859 
860     @property
861     def encoding(self):
862         """Dictionary of encodings on this variable."""
863         if self._encoding is None:
864             self._encoding = {}
865         return self._encoding
866 
867     @encoding.setter
868     def encoding(self, value):
869         try:
870             self._encoding = dict(value)
871         except ValueError:
872             raise ValueError("encoding must be castable to a dictionary")
873 
874     def copy(self, deep=True, data=None):
875         """Returns a copy of this object.
876 
877         If `deep=True`, the data array is loaded into memory and copied onto
878         the new object. Dimensions, attributes and encodings are always copied.
879 
880         Use `data` to create a new object with the same structure as
881         original but entirely new data.
882 
883         Parameters
884         ----------
885         deep : bool, optional
886             Whether the data array is loaded into memory and copied onto
887             the new object. Default is True.
888         data : array_like, optional
889             Data to use in the new object. Must have same shape as original.
890             When `data` is used, `deep` is ignored.
891 
892         Returns
893         -------
894         object : Variable
895             New object with dimensions, attributes, encodings, and optionally
896             data copied from original.
897 
898         Examples
899         --------
900 
901         Shallow copy versus deep copy
902 
903         >>> var = xr.Variable(data=[1, 2, 3], dims="x")
904         >>> var.copy()
905         <xarray.Variable (x: 3)>
906         array([1, 2, 3])
907         >>> var_0 = var.copy(deep=False)
908         >>> var_0[0] = 7
909         >>> var_0
910         <xarray.Variable (x: 3)>
911         array([7, 2, 3])
912         >>> var
913         <xarray.Variable (x: 3)>
914         array([7, 2, 3])
915 
916         Changing the data using the ``data`` argument maintains the
917         structure of the original object, but with the new data. Original
918         object is unaffected.
919 
920         >>> var.copy(data=[0.1, 0.2, 0.3])
921         <xarray.Variable (x: 3)>
922         array([0.1, 0.2, 0.3])
923         >>> var
924         <xarray.Variable (x: 3)>
925         array([7, 2, 3])
926 
927         See Also
928         --------
929         pandas.DataFrame.copy
930         """
931         if data is None:
932             data = self._data
933 
934             if isinstance(data, indexing.MemoryCachedArray):
935                 # don't share caching between copies
936                 data = indexing.MemoryCachedArray(data.array)
937 
938             if deep:
939                 data = copy.deepcopy(data)
940 
941         else:
942             data = as_compatible_data(data)
943             if self.shape != data.shape:
944                 raise ValueError(
945                     "Data shape {} must match shape of object {}".format(
946                         data.shape, self.shape
947                     )
948                 )
949 
950         # note:
951         # dims is already an immutable tuple
952         # attributes and encoding will be copied when the new Array is created
953         return self._replace(data=data)
954 
955     def _replace(
956         self, dims=_default, data=_default, attrs=_default, encoding=_default
957     ) -> "Variable":
958         if dims is _default:
959             dims = copy.copy(self._dims)
960         if data is _default:
961             data = copy.copy(self.data)
962         if attrs is _default:
963             attrs = copy.copy(self._attrs)
964         if encoding is _default:
965             encoding = copy.copy(self._encoding)
966         return type(self)(dims, data, attrs, encoding, fastpath=True)
967 
968     def __copy__(self):
969         return self.copy(deep=False)
970 
971     def __deepcopy__(self, memo=None):
972         # memo does nothing but is required for compatibility with
973         # copy.deepcopy
974         return self.copy(deep=True)
975 
976     # mutable objects should not be hashable
977     # https://github.com/python/mypy/issues/4266
978     __hash__ = None  # type: ignore
979 
980     @property
981     def chunks(self):
982         """Block dimensions for this array's data or None if it's not a dask
983         array.
984         """
985         return getattr(self._data, "chunks", None)
986 
987     _array_counter = itertools.count()
988 
989     def chunk(self, chunks={}, name=None, lock=False):
990         """Coerce this array's data into a dask arrays with the given chunks.
991 
992         If this variable is a non-dask array, it will be converted to dask
993         array. If it's a dask array, it will be rechunked to the given chunk
994         sizes.
995 
996         If neither chunks is not provided for one or more dimensions, chunk
997         sizes along that dimension will not be updated; non-dask arrays will be
998         converted into dask arrays with a single block.
999 
1000         Parameters
1001         ----------
1002         chunks : int, tuple or dict, optional
1003             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or
1004             ``{'x': 5, 'y': 5}``.
1005         name : str, optional
1006             Used to generate the name for this array in the internal dask
1007             graph. Does not need not be unique.
1008         lock : optional
1009             Passed on to :py:func:`dask.array.from_array`, if the array is not
1010             already as dask array.
1011 
1012         Returns
1013         -------
1014         chunked : xarray.Variable
1015         """
1016         import dask
1017         import dask.array as da
1018 
1019         if chunks is None:
1020             warnings.warn(
1021                 "None value for 'chunks' is deprecated. "
1022                 "It will raise an error in the future. Use instead '{}'",
1023                 category=FutureWarning,
1024             )
1025             chunks = {}
1026 
1027         if utils.is_dict_like(chunks):
1028             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}
1029 
1030         data = self._data
1031         if is_duck_dask_array(data):
1032             data = data.rechunk(chunks)
1033         else:
1034             if isinstance(data, indexing.ExplicitlyIndexed):
1035                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)
1036                 # that can't handle general array indexing. For example, in netCDF4 you
1037                 # can do "outer" indexing along two dimensions independent, which works
1038                 # differently from how NumPy handles it.
1039                 # da.from_array works by using lazy indexing with a tuple of slices.
1040                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle
1041                 # different indexing types in an explicit way:
1042                 # https://github.com/dask/dask/issues/2883
1043                 data = indexing.ImplicitToExplicitIndexingAdapter(
1044                     data, indexing.OuterIndexer
1045                 )
1046                 if LooseVersion(dask.__version__) < "2.0.0":
1047                     kwargs = {}
1048                 else:
1049                     # All of our lazily loaded backend array classes should use NumPy
1050                     # array operations.
1051                     kwargs = {"meta": np.ndarray}
1052             else:
1053                 kwargs = {}
1054 
1055             if utils.is_dict_like(chunks):
1056                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))
1057 
1058             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)
1059 
1060         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)
1061 
1062     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):
1063         """
1064         use sparse-array as backend.
1065         """
1066         import sparse
1067 
1068         # TODO: what to do if dask-backended?
1069         if fill_value is dtypes.NA:
1070             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1071         else:
1072             dtype = dtypes.result_type(self.dtype, fill_value)
1073 
1074         if sparse_format is _default:
1075             sparse_format = "coo"
1076         try:
1077             as_sparse = getattr(sparse, f"as_{sparse_format.lower()}")
1078         except AttributeError:
1079             raise ValueError(f"{sparse_format} is not a valid sparse format")
1080 
1081         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)
1082         return self._replace(data=data)
1083 
1084     def _to_dense(self):
1085         """
1086         Change backend from sparse to np.array
1087         """
1088         if hasattr(self._data, "todense"):
1089             return self._replace(data=self._data.todense())
1090         return self.copy(deep=False)
1091 
1092     def isel(
1093         self: VariableType,
1094         indexers: Mapping[Hashable, Any] = None,
1095         missing_dims: str = "raise",
1096         **indexers_kwargs: Any,
1097     ) -> VariableType:
1098         """Return a new array indexed along the specified dimension(s).
1099 
1100         Parameters
1101         ----------
1102         **indexers : {dim: indexer, ...}
1103             Keyword arguments with names matching dimensions and values given
1104             by integers, slice objects or arrays.
1105         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1106             What to do if dimensions that should be selected from are not present in the
1107             DataArray:
1108             - "raise": raise an exception
1109             - "warning": raise a warning, and ignore the missing dimensions
1110             - "ignore": ignore the missing dimensions
1111 
1112         Returns
1113         -------
1114         obj : Array object
1115             A new Array with the selected data and dimensions. In general,
1116             the new variable's data will be a view of this variable's data,
1117             unless numpy fancy indexing was triggered by using an array
1118             indexer, in which case the data will be a copy.
1119         """
1120         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1121 
1122         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1123 
1124         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)
1125         return self[key]
1126 
1127     def squeeze(self, dim=None):
1128         """Return a new object with squeezed data.
1129 
1130         Parameters
1131         ----------
1132         dim : None or str or tuple of str, optional
1133             Selects a subset of the length one dimensions. If a dimension is
1134             selected with length greater than one, an error is raised. If
1135             None, all length one dimensions are squeezed.
1136 
1137         Returns
1138         -------
1139         squeezed : same type as caller
1140             This object, but with with all or a subset of the dimensions of
1141             length 1 removed.
1142 
1143         See Also
1144         --------
1145         numpy.squeeze
1146         """
1147         dims = common.get_squeeze_dims(self, dim)
1148         return self.isel({d: 0 for d in dims})
1149 
1150     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):
1151         axis = self.get_axis_num(dim)
1152 
1153         if count > 0:
1154             keep = slice(None, -count)
1155         elif count < 0:
1156             keep = slice(-count, None)
1157         else:
1158             keep = slice(None)
1159 
1160         trimmed_data = self[(slice(None),) * axis + (keep,)].data
1161 
1162         if fill_value is dtypes.NA:
1163             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1164         else:
1165             dtype = self.dtype
1166 
1167         width = min(abs(count), self.shape[axis])
1168         dim_pad = (width, 0) if count >= 0 else (0, width)
1169         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]
1170 
1171         data = duck_array_ops.pad(
1172             trimmed_data.astype(dtype),
1173             pads,
1174             mode="constant",
1175             constant_values=fill_value,
1176         )
1177 
1178         if is_duck_dask_array(data):
1179             # chunked data should come out with the same chunks; this makes
1180             # it feasible to combine shifted and unshifted data
1181             # TODO: remove this once dask.array automatically aligns chunks
1182             data = data.rechunk(self.data.chunks)
1183 
1184         return type(self)(self.dims, data, self._attrs, fastpath=True)
1185 
1186     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
1187         """
1188         Return a new Variable with shifted data.
1189 
1190         Parameters
1191         ----------
1192         shifts : mapping of the form {dim: offset}
1193             Integer offset to shift along each of the given dimensions.
1194             Positive offsets shift to the right; negative offsets shift to the
1195             left.
1196         fill_value: scalar, optional
1197             Value to use for newly missing values
1198         **shifts_kwargs
1199             The keyword arguments form of ``shifts``.
1200             One of shifts or shifts_kwargs must be provided.
1201 
1202         Returns
1203         -------
1204         shifted : Variable
1205             Variable with the same dimensions and attributes but shifted data.
1206         """
1207         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
1208         result = self
1209         for dim, count in shifts.items():
1210             result = result._shift_one_dim(dim, count, fill_value=fill_value)
1211         return result
1212 
1213     def _pad_options_dim_to_index(
1214         self,
1215         pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],
1216         fill_with_shape=False,
1217     ):
1218         if fill_with_shape:
1219             return [
1220                 (n, n) if d not in pad_option else pad_option[d]
1221                 for d, n in zip(self.dims, self.data.shape)
1222             ]
1223         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]
1224 
1225     def pad(
1226         self,
1227         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
1228         mode: str = "constant",
1229         stat_length: Union[
1230             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
1231         ] = None,
1232         constant_values: Union[
1233             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
1234         ] = None,
1235         end_values: Union[
1236             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
1237         ] = None,
1238         reflect_type: str = None,
1239         **pad_width_kwargs: Any,
1240     ):
1241         """
1242         Return a new Variable with padded data.
1243 
1244         Parameters
1245         ----------
1246         pad_width : mapping of hashable to tuple of int
1247             Mapping with the form of {dim: (pad_before, pad_after)}
1248             describing the number of values padded along each dimension.
1249             {dim: pad} is a shortcut for pad_before = pad_after = pad
1250         mode : str, default: "constant"
1251             See numpy / Dask docs
1252         stat_length : int, tuple or mapping of hashable to tuple
1253             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
1254             values at edge of each axis used to calculate the statistic value.
1255         constant_values : scalar, tuple or mapping of hashable to tuple
1256             Used in 'constant'.  The values to set the padded values for each
1257             axis.
1258         end_values : scalar, tuple or mapping of hashable to tuple
1259             Used in 'linear_ramp'.  The values used for the ending value of the
1260             linear_ramp and that will form the edge of the padded array.
1261         reflect_type : {"even", "odd"}, optional
1262             Used in "reflect", and "symmetric".  The "even" style is the
1263             default with an unaltered reflection around the edge value.  For
1264             the "odd" style, the extended part of the array is created by
1265             subtracting the reflected values from two times the edge value.
1266         **pad_width_kwargs
1267             One of pad_width or pad_width_kwargs must be provided.
1268 
1269         Returns
1270         -------
1271         padded : Variable
1272             Variable with the same dimensions and attributes but padded data.
1273         """
1274         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
1275 
1276         # change default behaviour of pad with mode constant
1277         if mode == "constant" and (
1278             constant_values is None or constant_values is dtypes.NA
1279         ):
1280             dtype, constant_values = dtypes.maybe_promote(self.dtype)
1281         else:
1282             dtype = self.dtype
1283 
1284         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty
1285         if isinstance(stat_length, dict):
1286             stat_length = self._pad_options_dim_to_index(
1287                 stat_length, fill_with_shape=True
1288             )
1289         if isinstance(constant_values, dict):
1290             constant_values = self._pad_options_dim_to_index(constant_values)
1291         if isinstance(end_values, dict):
1292             end_values = self._pad_options_dim_to_index(end_values)
1293 
1294         # workaround for bug in Dask's default value of stat_length https://github.com/dask/dask/issues/5303
1295         if stat_length is None and mode in ["maximum", "mean", "median", "minimum"]:
1296             stat_length = [(n, n) for n in self.data.shape]  # type: ignore
1297 
1298         # change integer values to a tuple of two of those values and change pad_width to index
1299         for k, v in pad_width.items():
1300             if isinstance(v, numbers.Number):
1301                 pad_width[k] = (v, v)
1302         pad_width_by_index = self._pad_options_dim_to_index(pad_width)
1303 
1304         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty
1305         pad_option_kwargs = {}
1306         if stat_length is not None:
1307             pad_option_kwargs["stat_length"] = stat_length
1308         if constant_values is not None:
1309             pad_option_kwargs["constant_values"] = constant_values
1310         if end_values is not None:
1311             pad_option_kwargs["end_values"] = end_values
1312         if reflect_type is not None:
1313             pad_option_kwargs["reflect_type"] = reflect_type  # type: ignore
1314 
1315         array = duck_array_ops.pad(
1316             self.data.astype(dtype, copy=False),
1317             pad_width_by_index,
1318             mode=mode,
1319             **pad_option_kwargs,
1320         )
1321 
1322         return type(self)(self.dims, array)
1323 
1324     def _roll_one_dim(self, dim, count):
1325         axis = self.get_axis_num(dim)
1326 
1327         count %= self.shape[axis]
1328         if count != 0:
1329             indices = [slice(-count, None), slice(None, -count)]
1330         else:
1331             indices = [slice(None)]
1332 
1333         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]
1334 
1335         data = duck_array_ops.concatenate(arrays, axis)
1336 
1337         if is_duck_dask_array(data):
1338             # chunked data should come out with the same chunks; this makes
1339             # it feasible to combine shifted and unshifted data
1340             # TODO: remove this once dask.array automatically aligns chunks
1341             data = data.rechunk(self.data.chunks)
1342 
1343         return type(self)(self.dims, data, self._attrs, fastpath=True)
1344 
1345     def roll(self, shifts=None, **shifts_kwargs):
1346         """
1347         Return a new Variable with rolld data.
1348 
1349         Parameters
1350         ----------
1351         shifts : mapping of hashable to int
1352             Integer offset to roll along each of the given dimensions.
1353             Positive offsets roll to the right; negative offsets roll to the
1354             left.
1355         **shifts_kwargs
1356             The keyword arguments form of ``shifts``.
1357             One of shifts or shifts_kwargs must be provided.
1358 
1359         Returns
1360         -------
1361         shifted : Variable
1362             Variable with the same dimensions and attributes but rolled data.
1363         """
1364         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
1365 
1366         result = self
1367         for dim, count in shifts.items():
1368             result = result._roll_one_dim(dim, count)
1369         return result
1370 
1371     def transpose(self, *dims) -> "Variable":
1372         """Return a new Variable object with transposed dimensions.
1373 
1374         Parameters
1375         ----------
1376         *dims : str, optional
1377             By default, reverse the dimensions. Otherwise, reorder the
1378             dimensions to this order.
1379 
1380         Returns
1381         -------
1382         transposed : Variable
1383             The returned object has transposed data and dimensions with the
1384             same attributes as the original.
1385 
1386         Notes
1387         -----
1388         This operation returns a view of this variable's data. It is
1389         lazy for dask-backed Variables but not for numpy-backed Variables.
1390 
1391         See Also
1392         --------
1393         numpy.transpose
1394         """
1395         if len(dims) == 0:
1396             dims = self.dims[::-1]
1397         dims = tuple(infix_dims(dims, self.dims))
1398         axes = self.get_axis_num(dims)
1399         if len(dims) < 2 or dims == self.dims:
1400             # no need to transpose if only one dimension
1401             # or dims are in same order
1402             return self.copy(deep=False)
1403 
1404         data = as_indexable(self._data).transpose(axes)
1405         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)
1406 
1407     @property
1408     def T(self) -> "Variable":
1409         return self.transpose()
1410 
1411     def set_dims(self, dims, shape=None):
1412         """Return a new variable with given set of dimensions.
1413         This method might be used to attach new dimension(s) to variable.
1414 
1415         When possible, this operation does not copy this variable's data.
1416 
1417         Parameters
1418         ----------
1419         dims : str or sequence of str or dict
1420             Dimensions to include on the new variable. If a dict, values are
1421             used to provide the sizes of new dimensions; otherwise, new
1422             dimensions are inserted with length 1.
1423 
1424         Returns
1425         -------
1426         Variable
1427         """
1428         if isinstance(dims, str):
1429             dims = [dims]
1430 
1431         if shape is None and utils.is_dict_like(dims):
1432             shape = dims.values()
1433 
1434         missing_dims = set(self.dims) - set(dims)
1435         if missing_dims:
1436             raise ValueError(
1437                 "new dimensions %r must be a superset of "
1438                 "existing dimensions %r" % (dims, self.dims)
1439             )
1440 
1441         self_dims = set(self.dims)
1442         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims
1443 
1444         if self.dims == expanded_dims:
1445             # don't use broadcast_to unless necessary so the result remains
1446             # writeable if possible
1447             expanded_data = self.data
1448         elif shape is not None:
1449             dims_map = dict(zip(dims, shape))
1450             tmp_shape = tuple(dims_map[d] for d in expanded_dims)
1451             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)
1452         else:
1453             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]
1454 
1455         expanded_var = Variable(
1456             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True
1457         )
1458         return expanded_var.transpose(*dims)
1459 
1460     def _stack_once(self, dims, new_dim):
1461         if not set(dims) <= set(self.dims):
1462             raise ValueError("invalid existing dimensions: %s" % dims)
1463 
1464         if new_dim in self.dims:
1465             raise ValueError(
1466                 "cannot create a new dimension with the same "
1467                 "name as an existing dimension"
1468             )
1469 
1470         if len(dims) == 0:
1471             # don't stack
1472             return self.copy(deep=False)
1473 
1474         other_dims = [d for d in self.dims if d not in dims]
1475         dim_order = other_dims + list(dims)
1476         reordered = self.transpose(*dim_order)
1477 
1478         new_shape = reordered.shape[: len(other_dims)] + (-1,)
1479         new_data = reordered.data.reshape(new_shape)
1480         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)
1481 
1482         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1483 
1484     def stack(self, dimensions=None, **dimensions_kwargs):
1485         """
1486         Stack any number of existing dimensions into a single new dimension.
1487 
1488         New dimensions will be added at the end, and the order of the data
1489         along each new dimension will be in contiguous (C) order.
1490 
1491         Parameters
1492         ----------
1493         dimensions : mapping of hashable to tuple of hashable
1494             Mapping of form new_name=(dim1, dim2, ...) describing the
1495             names of new dimensions, and the existing dimensions that
1496             they replace.
1497         **dimensions_kwargs
1498             The keyword arguments form of ``dimensions``.
1499             One of dimensions or dimensions_kwargs must be provided.
1500 
1501         Returns
1502         -------
1503         stacked : Variable
1504             Variable with the same attributes but stacked data.
1505 
1506         See also
1507         --------
1508         Variable.unstack
1509         """
1510         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
1511         result = self
1512         for new_dim, dims in dimensions.items():
1513             result = result._stack_once(dims, new_dim)
1514         return result
1515 
1516     def _unstack_once(self, dims, old_dim):
1517         new_dim_names = tuple(dims.keys())
1518         new_dim_sizes = tuple(dims.values())
1519 
1520         if old_dim not in self.dims:
1521             raise ValueError("invalid existing dimension: %s" % old_dim)
1522 
1523         if set(new_dim_names).intersection(self.dims):
1524             raise ValueError(
1525                 "cannot create a new dimension with the same "
1526                 "name as an existing dimension"
1527             )
1528 
1529         if np.prod(new_dim_sizes) != self.sizes[old_dim]:
1530             raise ValueError(
1531                 "the product of the new dimension sizes must "
1532                 "equal the size of the old dimension"
1533             )
1534 
1535         other_dims = [d for d in self.dims if d != old_dim]
1536         dim_order = other_dims + [old_dim]
1537         reordered = self.transpose(*dim_order)
1538 
1539         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes
1540         new_data = reordered.data.reshape(new_shape)
1541         new_dims = reordered.dims[: len(other_dims)] + new_dim_names
1542 
1543         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)
1544 
1545     def unstack(self, dimensions=None, **dimensions_kwargs):
1546         """
1547         Unstack an existing dimension into multiple new dimensions.
1548 
1549         New dimensions will be added at the end, and the order of the data
1550         along each new dimension will be in contiguous (C) order.
1551 
1552         Parameters
1553         ----------
1554         dimensions : mapping of hashable to mapping of hashable to int
1555             Mapping of the form old_dim={dim1: size1, ...} describing the
1556             names of existing dimensions, and the new dimensions and sizes
1557             that they map to.
1558         **dimensions_kwargs
1559             The keyword arguments form of ``dimensions``.
1560             One of dimensions or dimensions_kwargs must be provided.
1561 
1562         Returns
1563         -------
1564         unstacked : Variable
1565             Variable with the same attributes but unstacked data.
1566 
1567         See also
1568         --------
1569         Variable.stack
1570         """
1571         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "unstack")
1572         result = self
1573         for old_dim, dims in dimensions.items():
1574             result = result._unstack_once(dims, old_dim)
1575         return result
1576 
1577     def fillna(self, value):
1578         return ops.fillna(self, value)
1579 
1580     def where(self, cond, other=dtypes.NA):
1581         return ops.where_method(self, cond, other)
1582 
1583     def reduce(
1584         self,
1585         func,
1586         dim=None,
1587         axis=None,
1588         keep_attrs=None,
1589         keepdims=False,
1590         **kwargs,
1591     ):
1592         """Reduce this array by applying `func` along some dimension(s).
1593 
1594         Parameters
1595         ----------
1596         func : callable
1597             Function which can be called in the form
1598             `func(x, axis=axis, **kwargs)` to return the result of reducing an
1599             np.ndarray over an integer valued axis.
1600         dim : str or sequence of str, optional
1601             Dimension(s) over which to apply `func`.
1602         axis : int or sequence of int, optional
1603             Axis(es) over which to apply `func`. Only one of the 'dim'
1604             and 'axis' arguments can be supplied. If neither are supplied, then
1605             the reduction is calculated over the flattened array (by calling
1606             `func(x)` without an axis argument).
1607         keep_attrs : bool, optional
1608             If True, the variable's attributes (`attrs`) will be copied from
1609             the original object to the new one.  If False (default), the new
1610             object will be returned without attributes.
1611         keepdims : bool, default: False
1612             If True, the dimensions which are reduced are left in the result
1613             as dimensions of size one
1614         **kwargs : dict
1615             Additional keyword arguments passed on to `func`.
1616 
1617         Returns
1618         -------
1619         reduced : Array
1620             Array with summarized data and the indicated dimension(s)
1621             removed.
1622         """
1623         if dim == ...:
1624             dim = None
1625         if dim is not None and axis is not None:
1626             raise ValueError("cannot supply both 'axis' and 'dim' arguments")
1627 
1628         if dim is not None:
1629             axis = self.get_axis_num(dim)
1630 
1631         with warnings.catch_warnings():
1632             warnings.filterwarnings(
1633                 "ignore", r"Mean of empty slice", category=RuntimeWarning
1634             )
1635             if axis is not None:
1636                 data = func(self.data, axis=axis, **kwargs)
1637             else:
1638                 data = func(self.data, **kwargs)
1639 
1640         if getattr(data, "shape", ()) == self.shape:
1641             dims = self.dims
1642         else:
1643             removed_axes = (
1644                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim
1645             )
1646             if keepdims:
1647                 # Insert np.newaxis for removed dims
1648                 slices = tuple(
1649                     np.newaxis if i in removed_axes else slice(None, None)
1650                     for i in range(self.ndim)
1651                 )
1652                 if getattr(data, "shape", None) is None:
1653                     # Reduce has produced a scalar value, not an array-like
1654                     data = np.asanyarray(data)[slices]
1655                 else:
1656                     data = data[slices]
1657                 dims = self.dims
1658             else:
1659                 dims = [
1660                     adim for n, adim in enumerate(self.dims) if n not in removed_axes
1661                 ]
1662 
1663         if keep_attrs is None:
1664             keep_attrs = _get_keep_attrs(default=False)
1665         attrs = self._attrs if keep_attrs else None
1666 
1667         return Variable(dims, data, attrs=attrs)
1668 
1669     @classmethod
1670     def concat(cls, variables, dim="concat_dim", positions=None, shortcut=False):
1671         """Concatenate variables along a new or existing dimension.
1672 
1673         Parameters
1674         ----------
1675         variables : iterable of Variable
1676             Arrays to stack together. Each variable is expected to have
1677             matching dimensions and shape except for along the stacked
1678             dimension.
1679         dim : str or DataArray, optional
1680             Name of the dimension to stack along. This can either be a new
1681             dimension name, in which case it is added along axis=0, or an
1682             existing dimension name, in which case the location of the
1683             dimension is unchanged. Where to insert the new dimension is
1684             determined by the first variable.
1685         positions : None or list of array-like, optional
1686             List of integer arrays which specifies the integer positions to
1687             which to assign each dataset along the concatenated dimension.
1688             If not supplied, objects are concatenated in the provided order.
1689         shortcut : bool, optional
1690             This option is used internally to speed-up groupby operations.
1691             If `shortcut` is True, some checks of internal consistency between
1692             arrays to concatenate are skipped.
1693 
1694         Returns
1695         -------
1696         stacked : Variable
1697             Concatenated Variable formed by stacking all the supplied variables
1698             along the given dimension.
1699         """
1700         if not isinstance(dim, str):
1701             (dim,) = dim.dims
1702 
1703         # can't do this lazily: we need to loop through variables at least
1704         # twice
1705         variables = list(variables)
1706         first_var = variables[0]
1707 
1708         arrays = [v.data for v in variables]
1709 
1710         if dim in first_var.dims:
1711             axis = first_var.get_axis_num(dim)
1712             dims = first_var.dims
1713             data = duck_array_ops.concatenate(arrays, axis=axis)
1714             if positions is not None:
1715                 # TODO: deprecate this option -- we don't need it for groupby
1716                 # any more.
1717                 indices = nputils.inverse_permutation(np.concatenate(positions))
1718                 data = duck_array_ops.take(data, indices, axis=axis)
1719         else:
1720             axis = 0
1721             dims = (dim,) + first_var.dims
1722             data = duck_array_ops.stack(arrays, axis=axis)
1723 
1724         attrs = dict(first_var.attrs)
1725         encoding = dict(first_var.encoding)
1726         if not shortcut:
1727             for var in variables:
1728                 if var.dims != first_var.dims:
1729                     raise ValueError(
1730                         f"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}"
1731                     )
1732 
1733         return cls(dims, data, attrs, encoding)
1734 
1735     def equals(self, other, equiv=duck_array_ops.array_equiv):
1736         """True if two Variables have the same dimensions and values;
1737         otherwise False.
1738 
1739         Variables can still be equal (like pandas objects) if they have NaN
1740         values in the same locations.
1741 
1742         This method is necessary because `v1 == v2` for Variables
1743         does element-wise comparisons (like numpy.ndarrays).
1744         """
1745         other = getattr(other, "variable", other)
1746         try:
1747             return self.dims == other.dims and (
1748                 self._data is other._data or equiv(self.data, other.data)
1749             )
1750         except (TypeError, AttributeError):
1751             return False
1752 
1753     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):
1754         """True if two Variables have the values after being broadcast against
1755         each other; otherwise False.
1756 
1757         Variables can still be equal (like pandas objects) if they have NaN
1758         values in the same locations.
1759         """
1760         try:
1761             self, other = broadcast_variables(self, other)
1762         except (ValueError, AttributeError):
1763             return False
1764         return self.equals(other, equiv=equiv)
1765 
1766     def identical(self, other, equiv=duck_array_ops.array_equiv):
1767         """Like equals, but also checks attributes."""
1768         try:
1769             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(
1770                 other, equiv=equiv
1771             )
1772         except (TypeError, AttributeError):
1773             return False
1774 
1775     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):
1776         """True if the intersection of two Variable's non-null data is
1777         equal; otherwise false.
1778 
1779         Variables can thus still be equal if there are locations where either,
1780         or both, contain NaN values.
1781         """
1782         return self.broadcast_equals(other, equiv=equiv)
1783 
1784     def quantile(
1785         self, q, dim=None, interpolation="linear", keep_attrs=None, skipna=True
1786     ):
1787         """Compute the qth quantile of the data along the specified dimension.
1788 
1789         Returns the qth quantiles(s) of the array elements.
1790 
1791         Parameters
1792         ----------
1793         q : float or sequence of float
1794             Quantile to compute, which must be between 0 and 1
1795             inclusive.
1796         dim : str or sequence of str, optional
1797             Dimension(s) over which to apply quantile.
1798         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
1799             This optional parameter specifies the interpolation method to
1800             use when the desired quantile lies between two data points
1801             ``i < j``:
1802 
1803                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
1804                   the fractional part of the index surrounded by ``i`` and
1805                   ``j``.
1806                 * lower: ``i``.
1807                 * higher: ``j``.
1808                 * nearest: ``i`` or ``j``, whichever is nearest.
1809                 * midpoint: ``(i + j) / 2``.
1810 
1811         keep_attrs : bool, optional
1812             If True, the variable's attributes (`attrs`) will be copied from
1813             the original object to the new one.  If False (default), the new
1814             object will be returned without attributes.
1815 
1816         Returns
1817         -------
1818         quantiles : Variable
1819             If `q` is a single quantile, then the result
1820             is a scalar. If multiple percentiles are given, first axis of
1821             the result corresponds to the quantile and a quantile dimension
1822             is added to the return array. The other dimensions are the
1823             dimensions that remain after the reduction of the array.
1824 
1825         See Also
1826         --------
1827         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,
1828         DataArray.quantile
1829         """
1830 
1831         from .computation import apply_ufunc
1832 
1833         _quantile_func = np.nanquantile if skipna else np.quantile
1834 
1835         if keep_attrs is None:
1836             keep_attrs = _get_keep_attrs(default=False)
1837 
1838         scalar = utils.is_scalar(q)
1839         q = np.atleast_1d(np.asarray(q, dtype=np.float64))
1840 
1841         if dim is None:
1842             dim = self.dims
1843 
1844         if utils.is_scalar(dim):
1845             dim = [dim]
1846 
1847         def _wrapper(npa, **kwargs):
1848             # move quantile axis to end. required for apply_ufunc
1849             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)
1850 
1851         axis = np.arange(-1, -1 * len(dim) - 1, -1)
1852         result = apply_ufunc(
1853             _wrapper,
1854             self,
1855             input_core_dims=[dim],
1856             exclude_dims=set(dim),
1857             output_core_dims=[["quantile"]],
1858             output_dtypes=[np.float64],
1859             dask_gufunc_kwargs=dict(output_sizes={"quantile": len(q)}),
1860             dask="parallelized",
1861             kwargs={"q": q, "axis": axis, "interpolation": interpolation},
1862         )
1863 
1864         # for backward compatibility
1865         result = result.transpose("quantile", ...)
1866         if scalar:
1867             result = result.squeeze("quantile")
1868         if keep_attrs:
1869             result.attrs = self._attrs
1870         return result
1871 
1872     def rank(self, dim, pct=False):
1873         """Ranks the data.
1874 
1875         Equal values are assigned a rank that is the average of the ranks that
1876         would have been otherwise assigned to all of the values within that
1877         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.
1878 
1879         NaNs in the input array are returned as NaNs.
1880 
1881         The `bottleneck` library is required.
1882 
1883         Parameters
1884         ----------
1885         dim : str
1886             Dimension over which to compute rank.
1887         pct : bool, optional
1888             If True, compute percentage ranks, otherwise compute integer ranks.
1889 
1890         Returns
1891         -------
1892         ranked : Variable
1893 
1894         See Also
1895         --------
1896         Dataset.rank, DataArray.rank
1897         """
1898         import bottleneck as bn
1899 
1900         data = self.data
1901 
1902         if is_duck_dask_array(data):
1903             raise TypeError(
1904                 "rank does not work for arrays stored as dask "
1905                 "arrays. Load the data via .compute() or .load() "
1906                 "prior to calling this method."
1907             )
1908         elif not isinstance(data, np.ndarray):
1909             raise TypeError(
1910                 "rank is not implemented for {} objects.".format(type(data))
1911             )
1912 
1913         axis = self.get_axis_num(dim)
1914         func = bn.nanrankdata if self.dtype.kind == "f" else bn.rankdata
1915         ranked = func(data, axis=axis)
1916         if pct:
1917             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)
1918             ranked /= count
1919         return Variable(self.dims, ranked)
1920 
1921     def rolling_window(
1922         self, dim, window, window_dim, center=False, fill_value=dtypes.NA
1923     ):
1924         """
1925         Make a rolling_window along dim and add a new_dim to the last place.
1926 
1927         Parameters
1928         ----------
1929         dim : str
1930             Dimension over which to compute rolling_window.
1931             For nd-rolling, should be list of dimensions.
1932         window : int
1933             Window size of the rolling
1934             For nd-rolling, should be list of integers.
1935         window_dim : str
1936             New name of the window dimension.
1937             For nd-rolling, should be list of integers.
1938         center : bool, default: False
1939             If True, pad fill_value for both ends. Otherwise, pad in the head
1940             of the axis.
1941         fill_value
1942             value to be filled.
1943 
1944         Returns
1945         -------
1946         Variable that is a view of the original array with a added dimension of
1947         size w.
1948         The return dim: self.dims + (window_dim, )
1949         The return shape: self.shape + (window, )
1950 
1951         Examples
1952         --------
1953         >>> v = Variable(("a", "b"), np.arange(8).reshape((2, 4)))
1954         >>> v.rolling_window("b", 3, "window_dim")
1955         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1956         array([[[nan, nan,  0.],
1957                 [nan,  0.,  1.],
1958                 [ 0.,  1.,  2.],
1959                 [ 1.,  2.,  3.]],
1960         <BLANKLINE>
1961                [[nan, nan,  4.],
1962                 [nan,  4.,  5.],
1963                 [ 4.,  5.,  6.],
1964                 [ 5.,  6.,  7.]]])
1965 
1966         >>> v.rolling_window("b", 3, "window_dim", center=True)
1967         <xarray.Variable (a: 2, b: 4, window_dim: 3)>
1968         array([[[nan,  0.,  1.],
1969                 [ 0.,  1.,  2.],
1970                 [ 1.,  2.,  3.],
1971                 [ 2.,  3., nan]],
1972         <BLANKLINE>
1973                [[nan,  4.,  5.],
1974                 [ 4.,  5.,  6.],
1975                 [ 5.,  6.,  7.],
1976                 [ 6.,  7., nan]]])
1977         """
1978         if fill_value is dtypes.NA:  # np.nan is passed
1979             dtype, fill_value = dtypes.maybe_promote(self.dtype)
1980             array = self.astype(dtype, copy=False).data
1981         else:
1982             dtype = self.dtype
1983             array = self.data
1984 
1985         if isinstance(dim, list):
1986             assert len(dim) == len(window)
1987             assert len(dim) == len(window_dim)
1988             assert len(dim) == len(center)
1989         else:
1990             dim = [dim]
1991             window = [window]
1992             window_dim = [window_dim]
1993             center = [center]
1994         axis = [self.get_axis_num(d) for d in dim]
1995         new_dims = self.dims + tuple(window_dim)
1996         return Variable(
1997             new_dims,
1998             duck_array_ops.rolling_window(
1999                 array, axis=axis, window=window, center=center, fill_value=fill_value
2000             ),
2001         )
2002 
2003     def coarsen(
2004         self, windows, func, boundary="exact", side="left", keep_attrs=None, **kwargs
2005     ):
2006         """
2007         Apply reduction function.
2008         """
2009         windows = {k: v for k, v in windows.items() if k in self.dims}
2010         if not windows:
2011             return self.copy()
2012 
2013         if keep_attrs is None:
2014             keep_attrs = _get_keep_attrs(default=False)
2015 
2016         if keep_attrs:
2017             _attrs = self.attrs
2018         else:
2019             _attrs = None
2020 
2021         reshaped, axes = self._coarsen_reshape(windows, boundary, side)
2022         if isinstance(func, str):
2023             name = func
2024             func = getattr(duck_array_ops, name, None)
2025             if func is None:
2026                 raise NameError(f"{name} is not a valid method.")
2027 
2028         return self._replace(data=func(reshaped, axis=axes, **kwargs), attrs=_attrs)
2029 
2030     def _coarsen_reshape(self, windows, boundary, side):
2031         """
2032         Construct a reshaped-array for coarsen
2033         """
2034         if not utils.is_dict_like(boundary):
2035             boundary = {d: boundary for d in windows.keys()}
2036 
2037         if not utils.is_dict_like(side):
2038             side = {d: side for d in windows.keys()}
2039 
2040         # remove unrelated dimensions
2041         boundary = {k: v for k, v in boundary.items() if k in windows}
2042         side = {k: v for k, v in side.items() if k in windows}
2043 
2044         for d, window in windows.items():
2045             if window <= 0:
2046                 raise ValueError(f"window must be > 0. Given {window}")
2047 
2048         variable = self
2049         for d, window in windows.items():
2050             # trim or pad the object
2051             size = variable.shape[self._get_axis_num(d)]
2052             n = int(size / window)
2053             if boundary[d] == "exact":
2054                 if n * window != size:
2055                     raise ValueError(
2056                         "Could not coarsen a dimension of size {} with "
2057                         "window {}".format(size, window)
2058                     )
2059             elif boundary[d] == "trim":
2060                 if side[d] == "left":
2061                     variable = variable.isel({d: slice(0, window * n)})
2062                 else:
2063                     excess = size - window * n
2064                     variable = variable.isel({d: slice(excess, None)})
2065             elif boundary[d] == "pad":  # pad
2066                 pad = window * n - size
2067                 if pad < 0:
2068                     pad += window
2069                 if side[d] == "left":
2070                     pad_width = {d: (0, pad)}
2071                 else:
2072                     pad_width = {d: (pad, 0)}
2073                 variable = variable.pad(pad_width, mode="constant")
2074             else:
2075                 raise TypeError(
2076                     "{} is invalid for boundary. Valid option is 'exact', "
2077                     "'trim' and 'pad'".format(boundary[d])
2078                 )
2079 
2080         shape = []
2081         axes = []
2082         axis_count = 0
2083         for i, d in enumerate(variable.dims):
2084             if d in windows:
2085                 size = variable.shape[i]
2086                 shape.append(int(size / windows[d]))
2087                 shape.append(windows[d])
2088                 axis_count += 1
2089                 axes.append(i + axis_count)
2090             else:
2091                 shape.append(variable.shape[i])
2092 
2093         return variable.data.reshape(shape), tuple(axes)
2094 
2095     def isnull(self, keep_attrs: bool = None):
2096         """Test each value in the array for whether it is a missing value.
2097 
2098         Returns
2099         -------
2100         isnull : Variable
2101             Same type and shape as object, but the dtype of the data is bool.
2102 
2103         See Also
2104         --------
2105         pandas.isnull
2106 
2107         Examples
2108         --------
2109         >>> var = xr.Variable("x", [1, np.nan, 3])
2110         >>> var
2111         <xarray.Variable (x: 3)>
2112         array([ 1., nan,  3.])
2113         >>> var.isnull()
2114         <xarray.Variable (x: 3)>
2115         array([False,  True, False])
2116         """
2117         from .computation import apply_ufunc
2118 
2119         if keep_attrs is None:
2120             keep_attrs = _get_keep_attrs(default=False)
2121 
2122         return apply_ufunc(
2123             duck_array_ops.isnull,
2124             self,
2125             dask="allowed",
2126             keep_attrs=keep_attrs,
2127         )
2128 
2129     def notnull(self, keep_attrs: bool = None):
2130         """Test each value in the array for whether it is not a missing value.
2131 
2132         Returns
2133         -------
2134         notnull : Variable
2135             Same type and shape as object, but the dtype of the data is bool.
2136 
2137         See Also
2138         --------
2139         pandas.notnull
2140 
2141         Examples
2142         --------
2143         >>> var = xr.Variable("x", [1, np.nan, 3])
2144         >>> var
2145         <xarray.Variable (x: 3)>
2146         array([ 1., nan,  3.])
2147         >>> var.notnull()
2148         <xarray.Variable (x: 3)>
2149         array([ True, False,  True])
2150         """
2151         from .computation import apply_ufunc
2152 
2153         if keep_attrs is None:
2154             keep_attrs = _get_keep_attrs(default=False)
2155 
2156         return apply_ufunc(
2157             duck_array_ops.notnull,
2158             self,
2159             dask="allowed",
2160             keep_attrs=keep_attrs,
2161         )
2162 
2163     @property
2164     def real(self):
2165         return type(self)(self.dims, self.data.real, self._attrs)
2166 
2167     @property
2168     def imag(self):
2169         return type(self)(self.dims, self.data.imag, self._attrs)
2170 
2171     def __array_wrap__(self, obj, context=None):
2172         return Variable(self.dims, obj)
2173 
2174     @staticmethod
2175     def _unary_op(f):
2176         @functools.wraps(f)
2177         def func(self, *args, **kwargs):
2178             keep_attrs = kwargs.pop("keep_attrs", None)
2179             if keep_attrs is None:
2180                 keep_attrs = _get_keep_attrs(default=True)
2181             with np.errstate(all="ignore"):
2182                 result = self.__array_wrap__(f(self.data, *args, **kwargs))
2183                 if keep_attrs:
2184                     result.attrs = self.attrs
2185                 return result
2186 
2187         return func
2188 
2189     @staticmethod
2190     def _binary_op(f, reflexive=False, **ignored_kwargs):
2191         @functools.wraps(f)
2192         def func(self, other):
2193             if isinstance(other, (xr.DataArray, xr.Dataset)):
2194                 return NotImplemented
2195             self_data, other_data, dims = _broadcast_compat_data(self, other)
2196             keep_attrs = _get_keep_attrs(default=False)
2197             attrs = self._attrs if keep_attrs else None
2198             with np.errstate(all="ignore"):
2199                 new_data = (
2200                     f(self_data, other_data)
2201                     if not reflexive
2202                     else f(other_data, self_data)
2203                 )
2204             result = Variable(dims, new_data, attrs=attrs)
2205             return result
2206 
2207         return func
2208 
2209     @staticmethod
2210     def _inplace_binary_op(f):
2211         @functools.wraps(f)
2212         def func(self, other):
2213             if isinstance(other, xr.Dataset):
2214                 raise TypeError("cannot add a Dataset to a Variable in-place")
2215             self_data, other_data, dims = _broadcast_compat_data(self, other)
2216             if dims != self.dims:
2217                 raise ValueError("dimensions cannot change for in-place operations")
2218             with np.errstate(all="ignore"):
2219                 self.values = f(self_data, other_data)
2220             return self
2221 
2222         return func
2223 
2224     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):
2225         """A (private) method to convert datetime array to numeric dtype
2226         See duck_array_ops.datetime_to_numeric
2227         """
2228         numeric_array = duck_array_ops.datetime_to_numeric(
2229             self.data, offset, datetime_unit, dtype
2230         )
2231         return type(self)(self.dims, numeric_array, self._attrs)
2232 
2233     def _unravel_argminmax(
2234         self,
2235         argminmax: str,
2236         dim: Union[Hashable, Sequence[Hashable], None],
2237         axis: Union[int, None],
2238         keep_attrs: Optional[bool],
2239         skipna: Optional[bool],
2240     ) -> Union["Variable", Dict[Hashable, "Variable"]]:
2241         """Apply argmin or argmax over one or more dimensions, returning the result as a
2242         dict of DataArray that can be passed directly to isel.
2243         """
2244         if dim is None and axis is None:
2245             warnings.warn(
2246                 "Behaviour of argmin/argmax with neither dim nor axis argument will "
2247                 "change to return a dict of indices of each dimension. To get a "
2248                 "single, flat index, please use np.argmin(da.data) or "
2249                 "np.argmax(da.data) instead of da.argmin() or da.argmax().",
2250                 DeprecationWarning,
2251                 stacklevel=3,
2252             )
2253 
2254         argminmax_func = getattr(duck_array_ops, argminmax)
2255 
2256         if dim is ...:
2257             # In future, should do this also when (dim is None and axis is None)
2258             dim = self.dims
2259         if (
2260             dim is None
2261             or axis is not None
2262             or not isinstance(dim, Sequence)
2263             or isinstance(dim, str)
2264         ):
2265             # Return int index if single dimension is passed, and is not part of a
2266             # sequence
2267             return self.reduce(
2268                 argminmax_func, dim=dim, axis=axis, keep_attrs=keep_attrs, skipna=skipna
2269             )
2270 
2271         # Get a name for the new dimension that does not conflict with any existing
2272         # dimension
2273         newdimname = "_unravel_argminmax_dim_0"
2274         count = 1
2275         while newdimname in self.dims:
2276             newdimname = f"_unravel_argminmax_dim_{count}"
2277             count += 1
2278 
2279         stacked = self.stack({newdimname: dim})
2280 
2281         result_dims = stacked.dims[:-1]
2282         reduce_shape = tuple(self.sizes[d] for d in dim)
2283 
2284         result_flat_indices = stacked.reduce(argminmax_func, axis=-1, skipna=skipna)
2285 
2286         result_unravelled_indices = duck_array_ops.unravel_index(
2287             result_flat_indices.data, reduce_shape
2288         )
2289 
2290         result = {
2291             d: Variable(dims=result_dims, data=i)
2292             for d, i in zip(dim, result_unravelled_indices)
2293         }
2294 
2295         if keep_attrs is None:
2296             keep_attrs = _get_keep_attrs(default=False)
2297         if keep_attrs:
2298             for v in result.values():
2299                 v.attrs = self.attrs
2300 
2301         return result
2302 
2303     def argmin(
2304         self,
2305         dim: Union[Hashable, Sequence[Hashable]] = None,
2306         axis: int = None,
2307         keep_attrs: bool = None,
2308         skipna: bool = None,
2309     ) -> Union["Variable", Dict[Hashable, "Variable"]]:
2310         """Index or indices of the minimum of the Variable over one or more dimensions.
2311         If a sequence is passed to 'dim', then result returned as dict of Variables,
2312         which can be passed directly to isel(). If a single str is passed to 'dim' then
2313         returns a Variable with dtype int.
2314 
2315         If there are multiple minima, the indices of the first one found will be
2316         returned.
2317 
2318         Parameters
2319         ----------
2320         dim : hashable, sequence of hashable or ..., optional
2321             The dimensions over which to find the minimum. By default, finds minimum over
2322             all dimensions - for now returning an int for backward compatibility, but
2323             this is deprecated, in future will return a dict with indices for all
2324             dimensions; to return a dict with all dimensions now, pass '...'.
2325         axis : int, optional
2326             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2327             can be supplied.
2328         keep_attrs : bool, optional
2329             If True, the attributes (`attrs`) will be copied from the original
2330             object to the new one.  If False (default), the new object will be
2331             returned without attributes.
2332         skipna : bool, optional
2333             If True, skip missing values (as marked by NaN). By default, only
2334             skips missing values for float dtypes; other dtypes either do not
2335             have a sentinel missing value (int) or skipna=True has not been
2336             implemented (object, datetime64 or timedelta64).
2337 
2338         Returns
2339         -------
2340         result : Variable or dict of Variable
2341 
2342         See also
2343         --------
2344         DataArray.argmin, DataArray.idxmin
2345         """
2346         return self._unravel_argminmax("argmin", dim, axis, keep_attrs, skipna)
2347 
2348     def argmax(
2349         self,
2350         dim: Union[Hashable, Sequence[Hashable]] = None,
2351         axis: int = None,
2352         keep_attrs: bool = None,
2353         skipna: bool = None,
2354     ) -> Union["Variable", Dict[Hashable, "Variable"]]:
2355         """Index or indices of the maximum of the Variable over one or more dimensions.
2356         If a sequence is passed to 'dim', then result returned as dict of Variables,
2357         which can be passed directly to isel(). If a single str is passed to 'dim' then
2358         returns a Variable with dtype int.
2359 
2360         If there are multiple maxima, the indices of the first one found will be
2361         returned.
2362 
2363         Parameters
2364         ----------
2365         dim : hashable, sequence of hashable or ..., optional
2366             The dimensions over which to find the maximum. By default, finds maximum over
2367             all dimensions - for now returning an int for backward compatibility, but
2368             this is deprecated, in future will return a dict with indices for all
2369             dimensions; to return a dict with all dimensions now, pass '...'.
2370         axis : int, optional
2371             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
2372             can be supplied.
2373         keep_attrs : bool, optional
2374             If True, the attributes (`attrs`) will be copied from the original
2375             object to the new one.  If False (default), the new object will be
2376             returned without attributes.
2377         skipna : bool, optional
2378             If True, skip missing values (as marked by NaN). By default, only
2379             skips missing values for float dtypes; other dtypes either do not
2380             have a sentinel missing value (int) or skipna=True has not been
2381             implemented (object, datetime64 or timedelta64).
2382 
2383         Returns
2384         -------
2385         result : Variable or dict of Variable
2386 
2387         See also
2388         --------
2389         DataArray.argmax, DataArray.idxmax
2390         """
2391         return self._unravel_argminmax("argmax", dim, axis, keep_attrs, skipna)

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/common.py</file>
<class>DataWithCoords</class>
<method>astype</method>
<code>
1394     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
1395         """
1396         Copy of the xarray object, with data cast to a specified type.
1397         Leaves coordinate dtype unchanged.
1398 
1399         Parameters
1400         ----------
1401         dtype : str or dtype
1402             Typecode or data-type to which the array is cast.
1403         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
1404             Controls what kind of data casting may occur. Defaults to 'unsafe'
1405             for backwards compatibility.
1406 
1407             * 'no' means the data types should not be cast at all.
1408             * 'equiv' means only byte-order changes are allowed.
1409             * 'safe' means only casts which can preserve values are allowed.
1410             * 'same_kind' means only safe casts or casts within a kind,
1411                 like float64 to float32, are allowed.
1412             * 'unsafe' means any data conversions may be done.
1413         copy : bool, optional
1414             By default, astype always returns a newly allocated array. If this
1415             is set to False and the `dtype` requirement is satisfied, the input
1416             array is returned instead of a copy.
1417         keep_attrs : bool, optional
1418             By default, astype keeps attributes. Set to False to remove
1419             attributes in the returned object.
1420 
1421         Returns
1422         -------
1423         out : same as object
1424             New object with data cast to the specified type.
1425 
1426         See also
1427         --------
1428         np.ndarray.astype
1429         dask.array.Array.astype
1430         """
1431         from .computation import apply_ufunc
1432 
1433         return apply_ufunc(
1434             duck_array_ops.astype,
1435             self,
1436             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
1437             keep_attrs=keep_attrs,
1438             dask="allowed",
1439         )

</code><intended_behavior>Similar to `Variable.astype`, any `astype` method shared across xarray objects which does not currently support the `order` parameter should be modified to include and appropriately handle this parameter. The implementation should ensure that the `order` parameter meaningfully alters the memory layout of the array, respecting the underlying data handling and representation logic in xarray.</intended_behavior>, <file>xarray/core/common.py</file>
<class>DataWithCoords</class>
<method>None</method>
<code>
330 class DataWithCoords(SupportsArithmetic, AttrAccessMixin):
331     """Shared base class for Dataset and DataArray."""
332 
333     __slots__ = ()
334 
335     _rolling_exp_cls = RollingExp
336 
337     def squeeze(
338         self,
339         dim: Union[Hashable, Iterable[Hashable], None] = None,
340         drop: bool = False,
341         axis: Union[int, Iterable[int], None] = None,
342     ):
343         """Return a new object with squeezed data.
344 
345         Parameters
346         ----------
347         dim : None or Hashable or iterable of Hashable, optional
348             Selects a subset of the length one dimensions. If a dimension is
349             selected with length greater than one, an error is raised. If
350             None, all length one dimensions are squeezed.
351         drop : bool, optional
352             If ``drop=True``, drop squeezed coordinates instead of making them
353             scalar.
354         axis : None or int or iterable of int, optional
355             Like dim, but positional.
356 
357         Returns
358         -------
359         squeezed : same type as caller
360             This object, but with with all or a subset of the dimensions of
361             length 1 removed.
362 
363         See Also
364         --------
365         numpy.squeeze
366         """
367         dims = get_squeeze_dims(self, dim, axis)
368         return self.isel(drop=drop, **{d: 0 for d in dims})
369 
370     def get_index(self, key: Hashable) -> pd.Index:
371         """Get an index for a dimension, with fall-back to a default RangeIndex"""
372         if key not in self.dims:
373             raise KeyError(key)
374 
375         try:
376             return self.indexes[key]
377         except KeyError:
378             # need to ensure dtype=int64 in case range is empty on Python 2
379             return pd.Index(range(self.sizes[key]), name=key, dtype=np.int64)
380 
381     def _calc_assign_results(
382         self: C, kwargs: Mapping[Hashable, Union[T, Callable[[C], T]]]
383     ) -> Dict[Hashable, T]:
384         return {k: v(self) if callable(v) else v for k, v in kwargs.items()}
385 
386     def assign_coords(self, coords=None, **coords_kwargs):
387         """Assign new coordinates to this object.
388 
389         Returns a new object with all the original data in addition to the new
390         coordinates.
391 
392         Parameters
393         ----------
394         coords : dict, optional
395             A dict where the keys are the names of the coordinates
396             with the new values to assign. If the values are callable, they are
397             computed on this object and assigned to new coordinate variables.
398             If the values are not callable, (e.g. a ``DataArray``, scalar, or
399             array), they are simply assigned. A new coordinate can also be
400             defined and attached to an existing dimension using a tuple with
401             the first element the dimension name and the second element the
402             values for this new coordinate.
403 
404         **coords_kwargs : optional
405             The keyword arguments form of ``coords``.
406             One of ``coords`` or ``coords_kwargs`` must be provided.
407 
408         Returns
409         -------
410         assigned : same type as caller
411             A new object with the new coordinates in addition to the existing
412             data.
413 
414         Examples
415         --------
416         Convert longitude coordinates from 0-359 to -180-179:
417 
418         >>> da = xr.DataArray(
419         ...     np.random.rand(4),
420         ...     coords=[np.array([358, 359, 0, 1])],
421         ...     dims="lon",
422         ... )
423         >>> da
424         <xarray.DataArray (lon: 4)>
425         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
426         Coordinates:
427           * lon      (lon) int64 358 359 0 1
428         >>> da.assign_coords(lon=(((da.lon + 180) % 360) - 180))
429         <xarray.DataArray (lon: 4)>
430         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
431         Coordinates:
432           * lon      (lon) int64 -2 -1 0 1
433 
434         The function also accepts dictionary arguments:
435 
436         >>> da.assign_coords({"lon": (((da.lon + 180) % 360) - 180)})
437         <xarray.DataArray (lon: 4)>
438         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
439         Coordinates:
440           * lon      (lon) int64 -2 -1 0 1
441 
442         New coordinate can also be attached to an existing dimension:
443 
444         >>> lon_2 = np.array([300, 289, 0, 1])
445         >>> da.assign_coords(lon_2=("lon", lon_2))
446         <xarray.DataArray (lon: 4)>
447         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318])
448         Coordinates:
449           * lon      (lon) int64 358 359 0 1
450             lon_2    (lon) int64 300 289 0 1
451 
452         Note that the same result can also be obtained with a dict e.g.
453 
454         >>> _ = da.assign_coords({"lon_2": ("lon", lon_2)})
455 
456         Notes
457         -----
458         Since ``coords_kwargs`` is a dictionary, the order of your arguments
459         may not be preserved, and so the order of the new variables is not well
460         defined. Assigning multiple variables within the same ``assign_coords``
461         is possible, but you cannot reference other variables created within
462         the same ``assign_coords`` call.
463 
464         See also
465         --------
466         Dataset.assign
467         Dataset.swap_dims
468         """
469         coords_kwargs = either_dict_or_kwargs(coords, coords_kwargs, "assign_coords")
470         data = self.copy(deep=False)
471         results = self._calc_assign_results(coords_kwargs)
472         data.coords.update(results)
473         return data
474 
475     def assign_attrs(self, *args, **kwargs):
476         """Assign new attrs to this object.
477 
478         Returns a new object equivalent to ``self.attrs.update(*args, **kwargs)``.
479 
480         Parameters
481         ----------
482         args
483             positional arguments passed into ``attrs.update``.
484         kwargs
485             keyword arguments passed into ``attrs.update``.
486 
487         Returns
488         -------
489         assigned : same type as caller
490             A new object with the new attrs in addition to the existing data.
491 
492         See also
493         --------
494         Dataset.assign
495         """
496         out = self.copy(deep=False)
497         out.attrs.update(*args, **kwargs)
498         return out
499 
500     def pipe(
501         self,
502         func: Union[Callable[..., T], Tuple[Callable[..., T], str]],
503         *args,
504         **kwargs,
505     ) -> T:
506         """
507         Apply ``func(self, *args, **kwargs)``
508 
509         This method replicates the pandas method of the same name.
510 
511         Parameters
512         ----------
513         func : callable
514             function to apply to this xarray object (Dataset/DataArray).
515             ``args``, and ``kwargs`` are passed into ``func``.
516             Alternatively a ``(callable, data_keyword)`` tuple where
517             ``data_keyword`` is a string indicating the keyword of
518             ``callable`` that expects the xarray object.
519         args
520             positional arguments passed into ``func``.
521         kwargs
522             a dictionary of keyword arguments passed into ``func``.
523 
524         Returns
525         -------
526         object : Any
527             the return type of ``func``.
528 
529         Notes
530         -----
531 
532         Use ``.pipe`` when chaining together functions that expect
533         xarray or pandas objects, e.g., instead of writing
534 
535         .. code:: python
536 
537             f(g(h(ds), arg1=a), arg2=b, arg3=c)
538 
539         You can write
540 
541         .. code:: python
542 
543             (ds.pipe(h).pipe(g, arg1=a).pipe(f, arg2=b, arg3=c))
544 
545         If you have a function that takes the data as (say) the second
546         argument, pass a tuple indicating which keyword expects the
547         data. For example, suppose ``f`` takes its data as ``arg2``:
548 
549         .. code:: python
550 
551             (ds.pipe(h).pipe(g, arg1=a).pipe((f, "arg2"), arg1=a, arg3=c))
552 
553         Examples
554         --------
555 
556         >>> import numpy as np
557         >>> import xarray as xr
558         >>> x = xr.Dataset(
559         ...     {
560         ...         "temperature_c": (
561         ...             ("lat", "lon"),
562         ...             20 * np.random.rand(4).reshape(2, 2),
563         ...         ),
564         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
565         ...     },
566         ...     coords={"lat": [10, 20], "lon": [150, 160]},
567         ... )
568         >>> x
569         <xarray.Dataset>
570         Dimensions:        (lat: 2, lon: 2)
571         Coordinates:
572           * lat            (lat) int64 10 20
573           * lon            (lon) int64 150 160
574         Data variables:
575             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
576             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
577 
578         >>> def adder(data, arg):
579         ...     return data + arg
580         ...
581         >>> def div(data, arg):
582         ...     return data / arg
583         ...
584         >>> def sub_mult(data, sub_arg, mult_arg):
585         ...     return (data * mult_arg) - sub_arg
586         ...
587         >>> x.pipe(adder, 2)
588         <xarray.Dataset>
589         Dimensions:        (lat: 2, lon: 2)
590         Coordinates:
591           * lat            (lat) int64 10 20
592           * lon            (lon) int64 150 160
593         Data variables:
594             temperature_c  (lat, lon) float64 12.98 16.3 14.06 12.9
595             precipitation  (lat, lon) float64 2.424 2.646 2.438 2.892
596 
597         >>> x.pipe(adder, arg=2)
598         <xarray.Dataset>
599         Dimensions:        (lat: 2, lon: 2)
600         Coordinates:
601           * lat            (lat) int64 10 20
602           * lon            (lon) int64 150 160
603         Data variables:
604             temperature_c  (lat, lon) float64 12.98 16.3 14.06 12.9
605             precipitation  (lat, lon) float64 2.424 2.646 2.438 2.892
606 
607         >>> (
608         ...     x.pipe(adder, arg=2)
609         ...     .pipe(div, arg=2)
610         ...     .pipe(sub_mult, sub_arg=2, mult_arg=2)
611         ... )
612         <xarray.Dataset>
613         Dimensions:        (lat: 2, lon: 2)
614         Coordinates:
615           * lat            (lat) int64 10 20
616           * lon            (lon) int64 150 160
617         Data variables:
618             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
619             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
620 
621         See Also
622         --------
623         pandas.DataFrame.pipe
624         """
625         if isinstance(func, tuple):
626             func, target = func
627             if target in kwargs:
628                 raise ValueError(
629                     "%s is both the pipe target and a keyword argument" % target
630                 )
631             kwargs[target] = self
632             return func(*args, **kwargs)
633         else:
634             return func(self, *args, **kwargs)
635 
636     def groupby(self, group, squeeze: bool = True, restore_coord_dims: bool = None):
637         """Returns a GroupBy object for performing grouped operations.
638 
639         Parameters
640         ----------
641         group : str, DataArray or IndexVariable
642             Array whose unique values should be used to group this array. If a
643             string, must be the name of a variable contained in this dataset.
644         squeeze : bool, optional
645             If "group" is a dimension of any arrays in this dataset, `squeeze`
646             controls whether the subarrays have a dimension of length 1 along
647             that dimension or if the dimension is squeezed out.
648         restore_coord_dims : bool, optional
649             If True, also restore the dimension order of multi-dimensional
650             coordinates.
651 
652         Returns
653         -------
654         grouped
655             A `GroupBy` object patterned after `pandas.GroupBy` that can be
656             iterated over in the form of `(unique_value, grouped_array)` pairs.
657 
658         Examples
659         --------
660         Calculate daily anomalies for daily data:
661 
662         >>> da = xr.DataArray(
663         ...     np.linspace(0, 1826, num=1827),
664         ...     coords=[pd.date_range("1/1/2000", "31/12/2004", freq="D")],
665         ...     dims="time",
666         ... )
667         >>> da
668         <xarray.DataArray (time: 1827)>
669         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
670                1.826e+03])
671         Coordinates:
672           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
673         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
674         <xarray.DataArray (time: 1827)>
675         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
676         Coordinates:
677           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
678             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
679 
680         See Also
681         --------
682         core.groupby.DataArrayGroupBy
683         core.groupby.DatasetGroupBy
684         """
685         # While we don't generally check the type of every arg, passing
686         # multiple dimensions as multiple arguments is common enough, and the
687         # consequences hidden enough (strings evaluate as true) to warrant
688         # checking here.
689         # A future version could make squeeze kwarg only, but would face
690         # backward-compat issues.
691         if not isinstance(squeeze, bool):
692             raise TypeError(
693                 f"`squeeze` must be True or False, but {squeeze} was supplied"
694             )
695 
696         return self._groupby_cls(
697             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
698         )
699 
700     def groupby_bins(
701         self,
702         group,
703         bins,
704         right: bool = True,
705         labels=None,
706         precision: int = 3,
707         include_lowest: bool = False,
708         squeeze: bool = True,
709         restore_coord_dims: bool = None,
710     ):
711         """Returns a GroupBy object for performing grouped operations.
712 
713         Rather than using all unique values of `group`, the values are discretized
714         first by applying `pandas.cut` [1]_ to `group`.
715 
716         Parameters
717         ----------
718         group : str, DataArray or IndexVariable
719             Array whose binned values should be used to group this array. If a
720             string, must be the name of a variable contained in this dataset.
721         bins : int or array-like
722             If bins is an int, it defines the number of equal-width bins in the
723             range of x. However, in this case, the range of x is extended by .1%
724             on each side to include the min or max values of x. If bins is a
725             sequence it defines the bin edges allowing for non-uniform bin
726             width. No extension of the range of x is done in this case.
727         right : bool, default: True
728             Indicates whether the bins include the rightmost edge or not. If
729             right == True (the default), then the bins [1,2,3,4] indicate
730             (1,2], (2,3], (3,4].
731         labels : array-like or bool, default: None
732             Used as labels for the resulting bins. Must be of the same length as
733             the resulting bins. If False, string bin labels are assigned by
734             `pandas.cut`.
735         precision : int
736             The precision at which to store and display the bins labels.
737         include_lowest : bool
738             Whether the first interval should be left-inclusive or not.
739         squeeze : bool, default: True
740             If "group" is a dimension of any arrays in this dataset, `squeeze`
741             controls whether the subarrays have a dimension of length 1 along
742             that dimension or if the dimension is squeezed out.
743         restore_coord_dims : bool, optional
744             If True, also restore the dimension order of multi-dimensional
745             coordinates.
746 
747         Returns
748         -------
749         grouped
750             A `GroupBy` object patterned after `pandas.GroupBy` that can be
751             iterated over in the form of `(unique_value, grouped_array)` pairs.
752             The name of the group has the added suffix `_bins` in order to
753             distinguish it from the original variable.
754 
755         References
756         ----------
757         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
758         """
759         return self._groupby_cls(
760             self,
761             group,
762             squeeze=squeeze,
763             bins=bins,
764             restore_coord_dims=restore_coord_dims,
765             cut_kwargs={
766                 "right": right,
767                 "labels": labels,
768                 "precision": precision,
769                 "include_lowest": include_lowest,
770             },
771         )
772 
773     def weighted(self, weights):
774         """
775         Weighted operations.
776 
777         Parameters
778         ----------
779         weights : DataArray
780             An array of weights associated with the values in this Dataset.
781             Each value in the data contributes to the reduction operation
782             according to its associated weight.
783 
784         Notes
785         -----
786         ``weights`` must be a DataArray and cannot contain missing values.
787         Missing values can be replaced by ``weights.fillna(0)``.
788         """
789 
790         return self._weighted_cls(self, weights)
791 
792     def rolling(
793         self,
794         dim: Mapping[Hashable, int] = None,
795         min_periods: int = None,
796         center: Union[bool, Mapping[Hashable, bool]] = False,
797         keep_attrs: bool = None,
798         **window_kwargs: int,
799     ):
800         """
801         Rolling window object.
802 
803         Parameters
804         ----------
805         dim: dict, optional
806             Mapping from the dimension name to create the rolling iterator
807             along (e.g. `time`) to its moving window size.
808         min_periods : int, default: None
809             Minimum number of observations in window required to have a value
810             (otherwise result is NA). The default, None, is equivalent to
811             setting min_periods equal to the size of the window.
812         center : bool or mapping, default: False
813             Set the labels at the center of the window.
814         **window_kwargs : optional
815             The keyword arguments form of ``dim``.
816             One of dim or window_kwargs must be provided.
817 
818         Returns
819         -------
820         Rolling object (core.rolling.DataArrayRolling for DataArray,
821         core.rolling.DatasetRolling for Dataset.)
822 
823         Examples
824         --------
825         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
826 
827         >>> da = xr.DataArray(
828         ...     np.linspace(0, 11, num=12),
829         ...     coords=[
830         ...         pd.date_range(
831         ...             "15/12/1999",
832         ...             periods=12,
833         ...             freq=pd.DateOffset(months=1),
834         ...         )
835         ...     ],
836         ...     dims="time",
837         ... )
838         >>> da
839         <xarray.DataArray (time: 12)>
840         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
841         Coordinates:
842           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
843         >>> da.rolling(time=3, center=True).mean()
844         <xarray.DataArray (time: 12)>
845         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
846         Coordinates:
847           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
848 
849         Remove the NaNs using ``dropna()``:
850 
851         >>> da.rolling(time=3, center=True).mean().dropna("time")
852         <xarray.DataArray (time: 10)>
853         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
854         Coordinates:
855           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
856 
857         See Also
858         --------
859         core.rolling.DataArrayRolling
860         core.rolling.DatasetRolling
861         """
862 
863         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
864         return self._rolling_cls(
865             self, dim, min_periods=min_periods, center=center, keep_attrs=keep_attrs
866         )
867 
868     def rolling_exp(
869         self,
870         window: Mapping[Hashable, int] = None,
871         window_type: str = "span",
872         **window_kwargs,
873     ):
874         """
875         Exponentially-weighted moving window.
876         Similar to EWM in pandas
877 
878         Requires the optional Numbagg dependency.
879 
880         Parameters
881         ----------
882         window : mapping of hashable to int, optional
883             A mapping from the name of the dimension to create the rolling
884             exponential window along (e.g. `time`) to the size of the moving window.
885         window_type : {"span", "com", "halflife", "alpha"}, default: "span"
886             The format of the previously supplied window. Each is a simple
887             numerical transformation of the others. Described in detail:
888             https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.ewm.html
889         **window_kwargs : optional
890             The keyword arguments form of ``window``.
891             One of window or window_kwargs must be provided.
892 
893         See Also
894         --------
895         core.rolling_exp.RollingExp
896         """
897         window = either_dict_or_kwargs(window, window_kwargs, "rolling_exp")
898 
899         return self._rolling_exp_cls(self, window, window_type)
900 
901     def coarsen(
902         self,
903         dim: Mapping[Hashable, int] = None,
904         boundary: str = "exact",
905         side: Union[str, Mapping[Hashable, str]] = "left",
906         coord_func: str = "mean",
907         keep_attrs: bool = None,
908         **window_kwargs: int,
909     ):
910         """
911         Coarsen object.
912 
913         Parameters
914         ----------
915         dim : mapping of hashable to int, optional
916             Mapping from the dimension name to the window size.
917         boundary : {"exact", "trim", "pad"}, default: "exact"
918             If 'exact', a ValueError will be raised if dimension size is not a
919             multiple of the window size. If 'trim', the excess entries are
920             dropped. If 'pad', NA will be padded.
921         side : {"left", "right"} or mapping of str to {"left", "right"}
922         coord_func : str or mapping of hashable to str, default: "mean"
923             function (name) that is applied to the coordinates,
924             or a mapping from coordinate name to function (name).
925         keep_attrs : bool, optional
926             If True, the object's attributes (`attrs`) will be copied from
927             the original object to the new one.  If False (default), the new
928             object will be returned without attributes.
929 
930         Returns
931         -------
932         Coarsen object (core.rolling.DataArrayCoarsen for DataArray,
933         core.rolling.DatasetCoarsen for Dataset.)
934 
935         Examples
936         --------
937         Coarsen the long time series by averaging over every four days.
938 
939         >>> da = xr.DataArray(
940         ...     np.linspace(0, 364, num=364),
941         ...     dims="time",
942         ...     coords={"time": pd.date_range("15/12/1999", periods=364)},
943         ... )
944         >>> da  # +doctest: ELLIPSIS
945         <xarray.DataArray (time: 364)>
946         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
947                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
948                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
949         ...
950                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
951                360.99173554, 361.99449036, 362.99724518, 364.        ])
952         Coordinates:
953           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
954         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
955         <xarray.DataArray (time: 121)>
956         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
957                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
958                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
959         ...
960                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
961                361.99449036])
962         Coordinates:
963           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
964         >>>
965 
966         See Also
967         --------
968         core.rolling.DataArrayCoarsen
969         core.rolling.DatasetCoarsen
970         """
971         if keep_attrs is None:
972             keep_attrs = _get_keep_attrs(default=False)
973 
974         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
975         return self._coarsen_cls(
976             self,
977             dim,
978             boundary=boundary,
979             side=side,
980             coord_func=coord_func,
981             keep_attrs=keep_attrs,
982         )
983 
984     def resample(
985         self,
986         indexer: Mapping[Hashable, str] = None,
987         skipna=None,
988         closed: str = None,
989         label: str = None,
990         base: int = 0,
991         keep_attrs: bool = None,
992         loffset=None,
993         restore_coord_dims: bool = None,
994         **indexer_kwargs: str,
995     ):
996         """Returns a Resample object for performing resampling operations.
997 
998         Handles both downsampling and upsampling. The resampled
999         dimension must be a datetime-like coordinate. If any intervals
1000         contain no values from the original object, they will be given
1001         the value ``NaN``.
1002 
1003         Parameters
1004         ----------
1005         indexer : {dim: freq}, optional
1006             Mapping from the dimension name to resample frequency [1]_. The
1007             dimension must be datetime-like.
1008         skipna : bool, optional
1009             Whether to skip missing values when aggregating in downsampling.
1010         closed : {"left", "right"}, optional
1011             Side of each interval to treat as closed.
1012         label : {"left", "right"}, optional
1013             Side of each interval to use for labeling.
1014         base : int, optional
1015             For frequencies that evenly subdivide 1 day, the "origin" of the
1016             aggregated intervals. For example, for "24H" frequency, base could
1017             range from 0 through 23.
1018         loffset : timedelta or str, optional
1019             Offset used to adjust the resampled time labels. Some pandas date
1020             offset strings are supported.
1021         keep_attrs : bool, optional
1022             If True, the object's attributes (`attrs`) will be copied from
1023             the original object to the new one.  If False (default), the new
1024             object will be returned without attributes.
1025         restore_coord_dims : bool, optional
1026             If True, also restore the dimension order of multi-dimensional
1027             coordinates.
1028         **indexer_kwargs : {dim: freq}
1029             The keyword arguments form of ``indexer``.
1030             One of indexer or indexer_kwargs must be provided.
1031 
1032         Returns
1033         -------
1034         resampled : same type as caller
1035             This object resampled.
1036 
1037         Examples
1038         --------
1039         Downsample monthly time-series data to seasonal data:
1040 
1041         >>> da = xr.DataArray(
1042         ...     np.linspace(0, 11, num=12),
1043         ...     coords=[
1044         ...         pd.date_range(
1045         ...             "15/12/1999",
1046         ...             periods=12,
1047         ...             freq=pd.DateOffset(months=1),
1048         ...         )
1049         ...     ],
1050         ...     dims="time",
1051         ... )
1052         >>> da
1053         <xarray.DataArray (time: 12)>
1054         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
1055         Coordinates:
1056           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
1057         >>> da.resample(time="QS-DEC").mean()
1058         <xarray.DataArray (time: 4)>
1059         array([ 1.,  4.,  7., 10.])
1060         Coordinates:
1061           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
1062 
1063         Upsample monthly time-series data to daily data:
1064 
1065         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
1066         <xarray.DataArray (time: 337)>
1067         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
1068                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
1069                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
1070         ...
1071                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
1072                10.96774194, 11.        ])
1073         Coordinates:
1074           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
1075 
1076         Limit scope of upsampling method
1077 
1078         >>> da.resample(time="1D").nearest(tolerance="1D")
1079         <xarray.DataArray (time: 337)>
1080         array([ 0.,  0., nan, ..., nan, 11., 11.])
1081         Coordinates:
1082           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
1083 
1084         See Also
1085         --------
1086         pandas.Series.resample
1087         pandas.DataFrame.resample
1088 
1089         References
1090         ----------
1091 
1092         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
1093         """
1094         # TODO support non-string indexer after removing the old API.
1095 
1096         from ..coding.cftimeindex import CFTimeIndex
1097         from .dataarray import DataArray
1098         from .resample import RESAMPLE_DIM
1099 
1100         if keep_attrs is None:
1101             keep_attrs = _get_keep_attrs(default=False)
1102 
1103         # note: the second argument (now 'skipna') use to be 'dim'
1104         if (
1105             (skipna is not None and not isinstance(skipna, bool))
1106             or ("how" in indexer_kwargs and "how" not in self.dims)
1107             or ("dim" in indexer_kwargs and "dim" not in self.dims)
1108         ):
1109             raise TypeError(
1110                 "resample() no longer supports the `how` or "
1111                 "`dim` arguments. Instead call methods on resample "
1112                 "objects, e.g., data.resample(time='1D').mean()"
1113             )
1114 
1115         indexer = either_dict_or_kwargs(indexer, indexer_kwargs, "resample")
1116         if len(indexer) != 1:
1117             raise ValueError("Resampling only supported along single dimensions.")
1118         dim, freq = next(iter(indexer.items()))
1119 
1120         dim_name = dim
1121         dim_coord = self[dim]
1122 
1123         # TODO: remove once pandas=1.1 is the minimum required version
1124         with warnings.catch_warnings():
1125             warnings.filterwarnings(
1126                 "ignore",
1127                 r"'(base|loffset)' in .resample\(\) and in Grouper\(\) is deprecated.",
1128                 category=FutureWarning,
1129             )
1130 
1131             if isinstance(self.indexes[dim_name], CFTimeIndex):
1132                 from .resample_cftime import CFTimeGrouper
1133 
1134                 grouper = CFTimeGrouper(freq, closed, label, base, loffset)
1135             else:
1136                 grouper = pd.Grouper(
1137                     freq=freq, closed=closed, label=label, base=base, loffset=loffset
1138                 )
1139         group = DataArray(
1140             dim_coord, coords=dim_coord.coords, dims=dim_coord.dims, name=RESAMPLE_DIM
1141         )
1142         resampler = self._resample_cls(
1143             self,
1144             group=group,
1145             dim=dim_name,
1146             grouper=grouper,
1147             resample_dim=RESAMPLE_DIM,
1148             restore_coord_dims=restore_coord_dims,
1149         )
1150 
1151         return resampler
1152 
1153     def where(self, cond, other=dtypes.NA, drop: bool = False):
1154         """Filter elements from this object according to a condition.
1155 
1156         This operation follows the normal broadcasting and alignment rules that
1157         xarray uses for binary arithmetic.
1158 
1159         Parameters
1160         ----------
1161         cond : DataArray, Dataset, or callable
1162             Locations at which to preserve this object's values. dtype must be `bool`.
1163             If a callable, it must expect this object as its only parameter.
1164         other : scalar, DataArray or Dataset, optional
1165             Value to use for locations in this object where ``cond`` is False.
1166             By default, these locations filled with NA.
1167         drop : bool, optional
1168             If True, coordinate labels that only correspond to False values of
1169             the condition are dropped from the result. Mutually exclusive with
1170             ``other``.
1171 
1172         Returns
1173         -------
1174         DataArray or Dataset
1175             Same xarray type as caller, with dtype float64.
1176 
1177         Examples
1178         --------
1179 
1180         >>> import numpy as np
1181         >>> a = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1182         >>> a
1183         <xarray.DataArray (x: 5, y: 5)>
1184         array([[ 0,  1,  2,  3,  4],
1185                [ 5,  6,  7,  8,  9],
1186                [10, 11, 12, 13, 14],
1187                [15, 16, 17, 18, 19],
1188                [20, 21, 22, 23, 24]])
1189         Dimensions without coordinates: x, y
1190 
1191         >>> a.where(a.x + a.y < 4)
1192         <xarray.DataArray (x: 5, y: 5)>
1193         array([[ 0.,  1.,  2.,  3., nan],
1194                [ 5.,  6.,  7., nan, nan],
1195                [10., 11., nan, nan, nan],
1196                [15., nan, nan, nan, nan],
1197                [nan, nan, nan, nan, nan]])
1198         Dimensions without coordinates: x, y
1199 
1200         >>> a.where(a.x + a.y < 5, -1)
1201         <xarray.DataArray (x: 5, y: 5)>
1202         array([[ 0,  1,  2,  3,  4],
1203                [ 5,  6,  7,  8, -1],
1204                [10, 11, 12, -1, -1],
1205                [15, 16, -1, -1, -1],
1206                [20, -1, -1, -1, -1]])
1207         Dimensions without coordinates: x, y
1208 
1209         >>> a.where(a.x + a.y < 4, drop=True)
1210         <xarray.DataArray (x: 4, y: 4)>
1211         array([[ 0.,  1.,  2.,  3.],
1212                [ 5.,  6.,  7., nan],
1213                [10., 11., nan, nan],
1214                [15., nan, nan, nan]])
1215         Dimensions without coordinates: x, y
1216 
1217         >>> a.where(lambda x: x.x + x.y < 4, drop=True)
1218         <xarray.DataArray (x: 4, y: 4)>
1219         array([[ 0.,  1.,  2.,  3.],
1220                [ 5.,  6.,  7., nan],
1221                [10., 11., nan, nan],
1222                [15., nan, nan, nan]])
1223         Dimensions without coordinates: x, y
1224 
1225         See also
1226         --------
1227         numpy.where : corresponding numpy function
1228         where : equivalent function
1229         """
1230         from .alignment import align
1231         from .dataarray import DataArray
1232         from .dataset import Dataset
1233 
1234         if callable(cond):
1235             cond = cond(self)
1236 
1237         if drop:
1238             if other is not dtypes.NA:
1239                 raise ValueError("cannot set `other` if drop=True")
1240 
1241             if not isinstance(cond, (Dataset, DataArray)):
1242                 raise TypeError(
1243                     "cond argument is %r but must be a %r or %r"
1244                     % (cond, Dataset, DataArray)
1245                 )
1246 
1247             # align so we can use integer indexing
1248             self, cond = align(self, cond)
1249 
1250             # get cond with the minimal size needed for the Dataset
1251             if isinstance(cond, Dataset):
1252                 clipcond = cond.to_array().any("variable")
1253             else:
1254                 clipcond = cond
1255 
1256             # clip the data corresponding to coordinate dims that are not used
1257             nonzeros = zip(clipcond.dims, np.nonzero(clipcond.values))
1258             indexers = {k: np.unique(v) for k, v in nonzeros}
1259 
1260             self = self.isel(**indexers)
1261             cond = cond.isel(**indexers)
1262 
1263         return ops.where_method(self, cond, other)
1264 
1265     def close(self: Any) -> None:
1266         """Close any files linked to this object"""
1267         if self._file_obj is not None:
1268             self._file_obj.close()
1269         self._file_obj = None
1270 
1271     def isnull(self, keep_attrs: bool = None):
1272         """Test each value in the array for whether it is a missing value.
1273 
1274         Returns
1275         -------
1276         isnull : DataArray or Dataset
1277             Same type and shape as object, but the dtype of the data is bool.
1278 
1279         See Also
1280         --------
1281         pandas.isnull
1282 
1283         Examples
1284         --------
1285         >>> array = xr.DataArray([1, np.nan, 3], dims="x")
1286         >>> array
1287         <xarray.DataArray (x: 3)>
1288         array([ 1., nan,  3.])
1289         Dimensions without coordinates: x
1290         >>> array.isnull()
1291         <xarray.DataArray (x: 3)>
1292         array([False,  True, False])
1293         Dimensions without coordinates: x
1294         """
1295         from .computation import apply_ufunc
1296 
1297         if keep_attrs is None:
1298             keep_attrs = _get_keep_attrs(default=False)
1299 
1300         return apply_ufunc(
1301             duck_array_ops.isnull,
1302             self,
1303             dask="allowed",
1304             keep_attrs=keep_attrs,
1305         )
1306 
1307     def notnull(self, keep_attrs: bool = None):
1308         """Test each value in the array for whether it is not a missing value.
1309 
1310         Returns
1311         -------
1312         notnull : DataArray or Dataset
1313             Same type and shape as object, but the dtype of the data is bool.
1314 
1315         See Also
1316         --------
1317         pandas.notnull
1318 
1319         Examples
1320         --------
1321         >>> array = xr.DataArray([1, np.nan, 3], dims="x")
1322         >>> array
1323         <xarray.DataArray (x: 3)>
1324         array([ 1., nan,  3.])
1325         Dimensions without coordinates: x
1326         >>> array.notnull()
1327         <xarray.DataArray (x: 3)>
1328         array([ True, False,  True])
1329         Dimensions without coordinates: x
1330         """
1331         from .computation import apply_ufunc
1332 
1333         if keep_attrs is None:
1334             keep_attrs = _get_keep_attrs(default=False)
1335 
1336         return apply_ufunc(
1337             duck_array_ops.notnull,
1338             self,
1339             dask="allowed",
1340             keep_attrs=keep_attrs,
1341         )
1342 
1343     def isin(self, test_elements):
1344         """Tests each value in the array for whether it is in test elements.
1345 
1346         Parameters
1347         ----------
1348         test_elements : array_like
1349             The values against which to test each value of `element`.
1350             This argument is flattened if an array or array_like.
1351             See numpy notes for behavior with non-array-like parameters.
1352 
1353         Returns
1354         -------
1355         isin : DataArray or Dataset
1356             Has the same type and shape as this object, but with a bool dtype.
1357 
1358         Examples
1359         --------
1360 
1361         >>> array = xr.DataArray([1, 2, 3], dims="x")
1362         >>> array.isin([1, 3])
1363         <xarray.DataArray (x: 3)>
1364         array([ True, False,  True])
1365         Dimensions without coordinates: x
1366 
1367         See also
1368         --------
1369         numpy.isin
1370         """
1371         from .computation import apply_ufunc
1372         from .dataarray import DataArray
1373         from .dataset import Dataset
1374         from .variable import Variable
1375 
1376         if isinstance(test_elements, Dataset):
1377             raise TypeError(
1378                 "isin() argument must be convertible to an array: {}".format(
1379                     test_elements
1380                 )
1381             )
1382         elif isinstance(test_elements, (Variable, DataArray)):
1383             # need to explicitly pull out data to support dask arrays as the
1384             # second argument
1385             test_elements = test_elements.data
1386 
1387         return apply_ufunc(
1388             duck_array_ops.isin,
1389             self,
1390             kwargs=dict(test_elements=test_elements),
1391             dask="allowed",
1392         )
1393 
1394     def astype(self, dtype, casting="unsafe", copy=True, keep_attrs=True):
1395         """
1396         Copy of the xarray object, with data cast to a specified type.
1397         Leaves coordinate dtype unchanged.
1398 
1399         Parameters
1400         ----------
1401         dtype : str or dtype
1402             Typecode or data-type to which the array is cast.
1403         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional
1404             Controls what kind of data casting may occur. Defaults to 'unsafe'
1405             for backwards compatibility.
1406 
1407             * 'no' means the data types should not be cast at all.
1408             * 'equiv' means only byte-order changes are allowed.
1409             * 'safe' means only casts which can preserve values are allowed.
1410             * 'same_kind' means only safe casts or casts within a kind,
1411                 like float64 to float32, are allowed.
1412             * 'unsafe' means any data conversions may be done.
1413         copy : bool, optional
1414             By default, astype always returns a newly allocated array. If this
1415             is set to False and the `dtype` requirement is satisfied, the input
1416             array is returned instead of a copy.
1417         keep_attrs : bool, optional
1418             By default, astype keeps attributes. Set to False to remove
1419             attributes in the returned object.
1420 
1421         Returns
1422         -------
1423         out : same as object
1424             New object with data cast to the specified type.
1425 
1426         See also
1427         --------
1428         np.ndarray.astype
1429         dask.array.Array.astype
1430         """
1431         from .computation import apply_ufunc
1432 
1433         return apply_ufunc(
1434             duck_array_ops.astype,
1435             self,
1436             kwargs=dict(dtype=dtype, casting=casting, copy=copy),
1437             keep_attrs=keep_attrs,
1438             dask="allowed",
1439         )
1440 
1441     def __enter__(self: T) -> T:
1442         return self
1443 
1444     def __exit__(self, exc_type, exc_value, traceback) -> None:
1445         self.close()
1446 
1447     def __getitem__(self, value):
1448         # implementations of this class should implement this method
1449         raise NotImplementedError()

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 08:44:51.307 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 08:44:51.308 | INFO     | Search-only: skipping patch selection
2025-10-15 08:44:51.390 | INFO     | Task pydata__xarray-4683 completed successfully.
2025-10-15 08:44:51.391 | INFO     | No patch generated. You can try running ACR again.
