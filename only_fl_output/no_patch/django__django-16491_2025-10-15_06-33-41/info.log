2025-10-15 06:33:41.338 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 06:33:44.316 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1345, output_tokens=12, cost=0.013810
2025-10-15 06:34:05.239 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1525, output_tokens=747, cost=0.037660
2025-10-15 06:34:06.552 | INFO     | registered non reproducing test 0
2025-10-15 06:34:34.457 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3831, output_tokens=932, cost=0.066270
2025-10-15 06:34:35.602 | INFO     | registered non reproducing test 1
2025-10-15 06:34:55.210 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2945, output_tokens=775, cost=0.052700
2025-10-15 06:34:56.280 | INFO     | registered non reproducing test 2
2025-10-15 06:34:56.281 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 06:34:56.287 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 06:35:05.707 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1653, output_tokens=333, cost=0.026520
2025-10-15 06:35:05.714 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 06:35:08.775 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=687, output_tokens=55, cost=0.008520
2025-10-15 06:35:08.777 | DEBUG    | Extracted a valid json.
2025-10-15 06:35:08.777 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Exists\")",
        "search_method(\"annotate\")",
        "search_code(\"EmptyQuerySet\")",
        "search_code(\"output_field = BooleanField()\")"
    ],
    "bug_locations": []
}
2025-10-15 06:35:08.872 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 06:35:08.873 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 06:35:31.535 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2567, output_tokens=897, cost=0.052580
2025-10-15 06:35:31.543 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 06:35:42.443 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3680, output_tokens=431, cost=0.049730
2025-10-15 06:35:42.452 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 06:35:51.609 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=785, output_tokens=334, cost=0.017870
2025-10-15 06:35:51.610 | DEBUG    | Extracted a valid json.
2025-10-15 06:35:51.610 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/expressions.py",
            "class": "Exists",
            "method": "",
            "intended_behavior": "The `Exists` class should ensure that when constructed with an `EmptyQuerySet`, it always evaluates to a boolean value (`False` in the case of `EmptyQuerySet`), instead of resulting in `None`. The logic might need to explicitly handle `EmptyQuerySet` cases, ensuring that the SQL generated translates to a boolean expression that evaluates to `False` when the subquery is empty. This maintains the expected functionality of the `Exists` annotation, aligning with the assumption that an empty subquery should not exist, and thus, return `False`."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "The `annotate` method, or underlying methods it calls (like `_annotate`), should ensure that annotations using `Exists` with `EmptyQuerySet` are correctly processed to return boolean values. While the method itself may not directly handle the logic of `Exists`, ensuring that the query construction process respects the `BooleanField` output requirement, even in the case of empty results, is essential. It might involve adding checks or transformations at the query construction stage where `EmptyQuerySet` influences the annotation result, specifically ensuring that an `Exists(EmptyQuerySet)` translates to a boolean `False` in SQL output, rather than `NULL`."
        }
    ]
}
2025-10-15 06:35:51.627 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1546 class Exists(Subquery):
1547     template = "EXISTS(%(subquery)s)"
1548     output_field = fields.BooleanField()
1549 
1550     def __init__(self, queryset, **kwargs):
1551         super().__init__(queryset, **kwargs)
1552         self.query = self.query.exists()
1553 
1554     def select_format(self, compiler, sql, params):
1555         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1556         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1557         # BY list.
1558         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1559             sql = "CASE WHEN {} THEN 1 ELSE 0 END".format(sql)
1560         return sql, params

</code><intended_behavior>The `Exists` class should ensure that when constructed with an `EmptyQuerySet`, it always evaluates to a boolean value (`False` in the case of `EmptyQuerySet`), instead of resulting in `None`. The logic might need to explicitly handle `EmptyQuerySet` cases, ensuring that the SQL generated translates to a boolean expression that evaluates to `False` when the subquery is empty. This maintains the expected functionality of the `Exists` annotation, aligning with the assumption that an empty subquery should not exist, and thus, return `False`.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1571     def annotate(self, *args, **kwargs):
1572         """
1573         Return a query set in which the returned objects have been annotated
1574         with extra data or aggregations.
1575         """
1576         self._not_support_combined_queries("annotate")
1577         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>The `annotate` method, or underlying methods it calls (like `_annotate`), should ensure that annotations using `Exists` with `EmptyQuerySet` are correctly processed to return boolean values. While the method itself may not directly handle the logic of `Exists`, ensuring that the query construction process respects the `BooleanField` output requirement, even in the case of empty results, is essential. It might involve adding checks or transformations at the query construction stage where `EmptyQuerySet` influences the annotation result, specifically ensuring that an `Exists(EmptyQuerySet)` translates to a boolean `False` in SQL output, rather than `NULL`.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
290 class QuerySet(AltersData):
291     """Represent a lazy database lookup for a set of objects."""
292 
293     def __init__(self, model=None, query=None, using=None, hints=None):
294         self.model = model
295         self._db = using
296         self._hints = hints or {}
297         self._query = query or sql.Query(self.model)
298         self._result_cache = None
299         self._sticky_filter = False
300         self._for_write = False
301         self._prefetch_related_lookups = ()
302         self._prefetch_done = False
303         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
304         self._iterable_class = ModelIterable
305         self._fields = None
306         self._defer_next_filter = False
307         self._deferred_filter = None
308 
309     @property
310     def query(self):
311         if self._deferred_filter:
312             negate, args, kwargs = self._deferred_filter
313             self._filter_or_exclude_inplace(negate, args, kwargs)
314             self._deferred_filter = None
315         return self._query
316 
317     @query.setter
318     def query(self, value):
319         if value.values_select:
320             self._iterable_class = ValuesIterable
321         self._query = value
322 
323     def as_manager(cls):
324         # Address the circular dependency between `Queryset` and `Manager`.
325         from django.db.models.manager import Manager
326 
327         manager = Manager.from_queryset(cls)()
328         manager._built_with_as_manager = True
329         return manager
330 
331     as_manager.queryset_only = True
332     as_manager = classmethod(as_manager)
333 
334     ########################
335     # PYTHON MAGIC METHODS #
336     ########################
337 
338     def __deepcopy__(self, memo):
339         """Don't populate the QuerySet's cache."""
340         obj = self.__class__()
341         for k, v in self.__dict__.items():
342             if k == "_result_cache":
343                 obj.__dict__[k] = None
344             else:
345                 obj.__dict__[k] = copy.deepcopy(v, memo)
346         return obj
347 
348     def __getstate__(self):
349         # Force the cache to be fully populated.
350         self._fetch_all()
351         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
352 
353     def __setstate__(self, state):
354         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
355         if pickled_version:
356             if pickled_version != django.__version__:
357                 warnings.warn(
358                     "Pickled queryset instance's Django version %s does not "
359                     "match the current version %s."
360                     % (pickled_version, django.__version__),
361                     RuntimeWarning,
362                     stacklevel=2,
363                 )
364         else:
365             warnings.warn(
366                 "Pickled queryset instance's Django version is not specified.",
367                 RuntimeWarning,
368                 stacklevel=2,
369             )
370         self.__dict__.update(state)
371 
372     def __repr__(self):
373         data = list(self[: REPR_OUTPUT_SIZE + 1])
374         if len(data) > REPR_OUTPUT_SIZE:
375             data[-1] = "...(remaining elements truncated)..."
376         return "<%s %r>" % (self.__class__.__name__, data)
377 
378     def __len__(self):
379         self._fetch_all()
380         return len(self._result_cache)
381 
382     def __iter__(self):
383         """
384         The queryset iterator protocol uses three nested iterators in the
385         default case:
386             1. sql.compiler.execute_sql()
387                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
388                  using cursor.fetchmany(). This part is responsible for
389                  doing some column masking, and returning the rows in chunks.
390             2. sql.compiler.results_iter()
391                - Returns one row at time. At this point the rows are still just
392                  tuples. In some cases the return values are converted to
393                  Python values at this location.
394             3. self.iterator()
395                - Responsible for turning the rows into model objects.
396         """
397         self._fetch_all()
398         return iter(self._result_cache)
399 
400     def __aiter__(self):
401         # Remember, __aiter__ itself is synchronous, it's the thing it returns
402         # that is async!
403         async def generator():
404             await sync_to_async(self._fetch_all)()
405             for item in self._result_cache:
406                 yield item
407 
408         return generator()
409 
410     def __bool__(self):
411         self._fetch_all()
412         return bool(self._result_cache)
413 
414     def __getitem__(self, k):
415         """Retrieve an item or slice from the set of results."""
416         if not isinstance(k, (int, slice)):
417             raise TypeError(
418                 "QuerySet indices must be integers or slices, not %s."
419                 % type(k).__name__
420             )
421         if (isinstance(k, int) and k < 0) or (
422             isinstance(k, slice)
423             and (
424                 (k.start is not None and k.start < 0)
425                 or (k.stop is not None and k.stop < 0)
426             )
427         ):
428             raise ValueError("Negative indexing is not supported.")
429 
430         if self._result_cache is not None:
431             return self._result_cache[k]
432 
433         if isinstance(k, slice):
434             qs = self._chain()
435             if k.start is not None:
436                 start = int(k.start)
437             else:
438                 start = None
439             if k.stop is not None:
440                 stop = int(k.stop)
441             else:
442                 stop = None
443             qs.query.set_limits(start, stop)
444             return list(qs)[:: k.step] if k.step else qs
445 
446         qs = self._chain()
447         qs.query.set_limits(k, k + 1)
448         qs._fetch_all()
449         return qs._result_cache[0]
450 
451     def __class_getitem__(cls, *args, **kwargs):
452         return cls
453 
454     def __and__(self, other):
455         self._check_operator_queryset(other, "&")
456         self._merge_sanity_check(other)
457         if isinstance(other, EmptyQuerySet):
458             return other
459         if isinstance(self, EmptyQuerySet):
460             return self
461         combined = self._chain()
462         combined._merge_known_related_objects(other)
463         combined.query.combine(other.query, sql.AND)
464         return combined
465 
466     def __or__(self, other):
467         self._check_operator_queryset(other, "|")
468         self._merge_sanity_check(other)
469         if isinstance(self, EmptyQuerySet):
470             return other
471         if isinstance(other, EmptyQuerySet):
472             return self
473         query = (
474             self
475             if self.query.can_filter()
476             else self.model._base_manager.filter(pk__in=self.values("pk"))
477         )
478         combined = query._chain()
479         combined._merge_known_related_objects(other)
480         if not other.query.can_filter():
481             other = other.model._base_manager.filter(pk__in=other.values("pk"))
482         combined.query.combine(other.query, sql.OR)
483         return combined
484 
485     def __xor__(self, other):
486         self._check_operator_queryset(other, "^")
487         self._merge_sanity_check(other)
488         if isinstance(self, EmptyQuerySet):
489             return other
490         if isinstance(other, EmptyQuerySet):
491             return self
492         query = (
493             self
494             if self.query.can_filter()
495             else self.model._base_manager.filter(pk__in=self.values("pk"))
496         )
497         combined = query._chain()
498         combined._merge_known_related_objects(other)
499         if not other.query.can_filter():
500             other = other.model._base_manager.filter(pk__in=other.values("pk"))
501         combined.query.combine(other.query, sql.XOR)
502         return combined
503 
504     ####################################
505     # METHODS THAT DO DATABASE QUERIES #
506     ####################################
507 
508     def _iterator(self, use_chunked_fetch, chunk_size):
509         iterable = self._iterable_class(
510             self,
511             chunked_fetch=use_chunked_fetch,
512             chunk_size=chunk_size or 2000,
513         )
514         if not self._prefetch_related_lookups or chunk_size is None:
515             yield from iterable
516             return
517 
518         iterator = iter(iterable)
519         while results := list(islice(iterator, chunk_size)):
520             prefetch_related_objects(results, *self._prefetch_related_lookups)
521             yield from results
522 
523     def iterator(self, chunk_size=None):
524         """
525         An iterator over the results from applying this QuerySet to the
526         database. chunk_size must be provided for QuerySets that prefetch
527         related objects. Otherwise, a default chunk_size of 2000 is supplied.
528         """
529         if chunk_size is None:
530             if self._prefetch_related_lookups:
531                 raise ValueError(
532                     "chunk_size must be provided when using QuerySet.iterator() after "
533                     "prefetch_related()."
534                 )
535         elif chunk_size <= 0:
536             raise ValueError("Chunk size must be strictly positive.")
537         use_chunked_fetch = not connections[self.db].settings_dict.get(
538             "DISABLE_SERVER_SIDE_CURSORS"
539         )
540         return self._iterator(use_chunked_fetch, chunk_size)
541 
542     async def aiterator(self, chunk_size=2000):
543         """
544         An asynchronous iterator over the results from applying this QuerySet
545         to the database.
546         """
547         if self._prefetch_related_lookups:
548             raise NotSupportedError(
549                 "Using QuerySet.aiterator() after prefetch_related() is not supported."
550             )
551         if chunk_size <= 0:
552             raise ValueError("Chunk size must be strictly positive.")
553         use_chunked_fetch = not connections[self.db].settings_dict.get(
554             "DISABLE_SERVER_SIDE_CURSORS"
555         )
556         async for item in self._iterable_class(
557             self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size
558         ):
559             yield item
560 
561     def aggregate(self, *args, **kwargs):
562         """
563         Return a dictionary containing the calculations (aggregation)
564         over the current queryset.
565 
566         If args is present the expression is passed as a kwarg using
567         the Aggregate object's default alias.
568         """
569         if self.query.distinct_fields:
570             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
571         self._validate_values_are_expressions(
572             (*args, *kwargs.values()), method_name="aggregate"
573         )
574         for arg in args:
575             # The default_alias property raises TypeError if default_alias
576             # can't be set automatically or AttributeError if it isn't an
577             # attribute.
578             try:
579                 arg.default_alias
580             except (AttributeError, TypeError):
581                 raise TypeError("Complex aggregates require an alias")
582             kwargs[arg.default_alias] = arg
583 
584         return self.query.chain().get_aggregation(self.db, kwargs)
585 
586     async def aaggregate(self, *args, **kwargs):
587         return await sync_to_async(self.aggregate)(*args, **kwargs)
588 
589     def count(self):
590         """
591         Perform a SELECT COUNT() and return the number of records as an
592         integer.
593 
594         If the QuerySet is already fully cached, return the length of the
595         cached results set to avoid multiple SELECT COUNT(*) calls.
596         """
597         if self._result_cache is not None:
598             return len(self._result_cache)
599 
600         return self.query.get_count(using=self.db)
601 
602     async def acount(self):
603         return await sync_to_async(self.count)()
604 
605     def get(self, *args, **kwargs):
606         """
607         Perform the query and return a single object matching the given
608         keyword arguments.
609         """
610         if self.query.combinator and (args or kwargs):
611             raise NotSupportedError(
612                 "Calling QuerySet.get(...) with filters after %s() is not "
613                 "supported." % self.query.combinator
614             )
615         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
616         if self.query.can_filter() and not self.query.distinct_fields:
617             clone = clone.order_by()
618         limit = None
619         if (
620             not clone.query.select_for_update
621             or connections[clone.db].features.supports_select_for_update_with_limit
622         ):
623             limit = MAX_GET_RESULTS
624             clone.query.set_limits(high=limit)
625         num = len(clone)
626         if num == 1:
627             return clone._result_cache[0]
628         if not num:
629             raise self.model.DoesNotExist(
630                 "%s matching query does not exist." % self.model._meta.object_name
631             )
632         raise self.model.MultipleObjectsReturned(
633             "get() returned more than one %s -- it returned %s!"
634             % (
635                 self.model._meta.object_name,
636                 num if not limit or num < limit else "more than %s" % (limit - 1),
637             )
638         )
639 
640     async def aget(self, *args, **kwargs):
641         return await sync_to_async(self.get)(*args, **kwargs)
642 
643     def create(self, **kwargs):
644         """
645         Create a new object with the given kwargs, saving it to the database
646         and returning the created object.
647         """
648         obj = self.model(**kwargs)
649         self._for_write = True
650         obj.save(force_insert=True, using=self.db)
651         return obj
652 
653     async def acreate(self, **kwargs):
654         return await sync_to_async(self.create)(**kwargs)
655 
656     def _prepare_for_bulk_create(self, objs):
657         for obj in objs:
658             if obj.pk is None:
659                 # Populate new PK values.
660                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
661             obj._prepare_related_fields_for_save(operation_name="bulk_create")
662 
663     def _check_bulk_create_options(
664         self, ignore_conflicts, update_conflicts, update_fields, unique_fields
665     ):
666         if ignore_conflicts and update_conflicts:
667             raise ValueError(
668                 "ignore_conflicts and update_conflicts are mutually exclusive."
669             )
670         db_features = connections[self.db].features
671         if ignore_conflicts:
672             if not db_features.supports_ignore_conflicts:
673                 raise NotSupportedError(
674                     "This database backend does not support ignoring conflicts."
675                 )
676             return OnConflict.IGNORE
677         elif update_conflicts:
678             if not db_features.supports_update_conflicts:
679                 raise NotSupportedError(
680                     "This database backend does not support updating conflicts."
681                 )
682             if not update_fields:
683                 raise ValueError(
684                     "Fields that will be updated when a row insertion fails "
685                     "on conflicts must be provided."
686                 )
687             if unique_fields and not db_features.supports_update_conflicts_with_target:
688                 raise NotSupportedError(
689                     "This database backend does not support updating "
690                     "conflicts with specifying unique fields that can trigger "
691                     "the upsert."
692                 )
693             if not unique_fields and db_features.supports_update_conflicts_with_target:
694                 raise ValueError(
695                     "Unique fields that can trigger the upsert must be provided."
696                 )
697             # Updating primary keys and non-concrete fields is forbidden.
698             if any(not f.concrete or f.many_to_many for f in update_fields):
699                 raise ValueError(
700                     "bulk_create() can only be used with concrete fields in "
701                     "update_fields."
702                 )
703             if any(f.primary_key for f in update_fields):
704                 raise ValueError(
705                     "bulk_create() cannot be used with primary keys in "
706                     "update_fields."
707                 )
708             if unique_fields:
709                 if any(not f.concrete or f.many_to_many for f in unique_fields):
710                     raise ValueError(
711                         "bulk_create() can only be used with concrete fields "
712                         "in unique_fields."
713                     )
714             return OnConflict.UPDATE
715         return None
716 
717     def bulk_create(
718         self,
719         objs,
720         batch_size=None,
721         ignore_conflicts=False,
722         update_conflicts=False,
723         update_fields=None,
724         unique_fields=None,
725     ):
726         """
727         Insert each of the instances into the database. Do *not* call
728         save() on each of the instances, do not send any pre/post_save
729         signals, and do not set the primary key attribute if it is an
730         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
731         Multi-table models are not supported.
732         """
733         # When you bulk insert you don't get the primary keys back (if it's an
734         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
735         # you can't insert into the child tables which references this. There
736         # are two workarounds:
737         # 1) This could be implemented if you didn't have an autoincrement pk
738         # 2) You could do it by doing O(n) normal inserts into the parent
739         #    tables to get the primary keys back and then doing a single bulk
740         #    insert into the childmost table.
741         # We currently set the primary keys on the objects when using
742         # PostgreSQL via the RETURNING ID clause. It should be possible for
743         # Oracle as well, but the semantics for extracting the primary keys is
744         # trickier so it's not done yet.
745         if batch_size is not None and batch_size <= 0:
746             raise ValueError("Batch size must be a positive integer.")
747         # Check that the parents share the same concrete model with the our
748         # model to detect the inheritance pattern ConcreteGrandParent ->
749         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
750         # would not identify that case as involving multiple tables.
751         for parent in self.model._meta.get_parent_list():
752             if parent._meta.concrete_model is not self.model._meta.concrete_model:
753                 raise ValueError("Can't bulk create a multi-table inherited model")
754         if not objs:
755             return objs
756         opts = self.model._meta
757         if unique_fields:
758             # Primary key is allowed in unique_fields.
759             unique_fields = [
760                 self.model._meta.get_field(opts.pk.name if name == "pk" else name)
761                 for name in unique_fields
762             ]
763         if update_fields:
764             update_fields = [self.model._meta.get_field(name) for name in update_fields]
765         on_conflict = self._check_bulk_create_options(
766             ignore_conflicts,
767             update_conflicts,
768             update_fields,
769             unique_fields,
770         )
771         self._for_write = True
772         fields = opts.concrete_fields
773         objs = list(objs)
774         self._prepare_for_bulk_create(objs)
775         with transaction.atomic(using=self.db, savepoint=False):
776             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
777             if objs_with_pk:
778                 returned_columns = self._batched_insert(
779                     objs_with_pk,
780                     fields,
781                     batch_size,
782                     on_conflict=on_conflict,
783                     update_fields=update_fields,
784                     unique_fields=unique_fields,
785                 )
786                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
787                     for result, field in zip(results, opts.db_returning_fields):
788                         if field != opts.pk:
789                             setattr(obj_with_pk, field.attname, result)
790                 for obj_with_pk in objs_with_pk:
791                     obj_with_pk._state.adding = False
792                     obj_with_pk._state.db = self.db
793             if objs_without_pk:
794                 fields = [f for f in fields if not isinstance(f, AutoField)]
795                 returned_columns = self._batched_insert(
796                     objs_without_pk,
797                     fields,
798                     batch_size,
799                     on_conflict=on_conflict,
800                     update_fields=update_fields,
801                     unique_fields=unique_fields,
802                 )
803                 connection = connections[self.db]
804                 if (
805                     connection.features.can_return_rows_from_bulk_insert
806                     and on_conflict is None
807                 ):
808                     assert len(returned_columns) == len(objs_without_pk)
809                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
810                     for result, field in zip(results, opts.db_returning_fields):
811                         setattr(obj_without_pk, field.attname, result)
812                     obj_without_pk._state.adding = False
813                     obj_without_pk._state.db = self.db
814 
815         return objs
816 
817     async def abulk_create(
818         self,
819         objs,
820         batch_size=None,
821         ignore_conflicts=False,
822         update_conflicts=False,
823         update_fields=None,
824         unique_fields=None,
825     ):
826         return await sync_to_async(self.bulk_create)(
827             objs=objs,
828             batch_size=batch_size,
829             ignore_conflicts=ignore_conflicts,
830             update_conflicts=update_conflicts,
831             update_fields=update_fields,
832             unique_fields=unique_fields,
833         )
834 
835     def bulk_update(self, objs, fields, batch_size=None):
836         """
837         Update the given fields in each of the given objects in the database.
838         """
839         if batch_size is not None and batch_size <= 0:
840             raise ValueError("Batch size must be a positive integer.")
841         if not fields:
842             raise ValueError("Field names must be given to bulk_update().")
843         objs = tuple(objs)
844         if any(obj.pk is None for obj in objs):
845             raise ValueError("All bulk_update() objects must have a primary key set.")
846         fields = [self.model._meta.get_field(name) for name in fields]
847         if any(not f.concrete or f.many_to_many for f in fields):
848             raise ValueError("bulk_update() can only be used with concrete fields.")
849         if any(f.primary_key for f in fields):
850             raise ValueError("bulk_update() cannot be used with primary key fields.")
851         if not objs:
852             return 0
853         for obj in objs:
854             obj._prepare_related_fields_for_save(
855                 operation_name="bulk_update", fields=fields
856             )
857         # PK is used twice in the resulting update query, once in the filter
858         # and once in the WHEN. Each field will also have one CAST.
859         self._for_write = True
860         connection = connections[self.db]
861         max_batch_size = connection.ops.bulk_batch_size(["pk", "pk"] + fields, objs)
862         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
863         requires_casting = connection.features.requires_casted_case_in_updates
864         batches = (objs[i : i + batch_size] for i in range(0, len(objs), batch_size))
865         updates = []
866         for batch_objs in batches:
867             update_kwargs = {}
868             for field in fields:
869                 when_statements = []
870                 for obj in batch_objs:
871                     attr = getattr(obj, field.attname)
872                     if not hasattr(attr, "resolve_expression"):
873                         attr = Value(attr, output_field=field)
874                     when_statements.append(When(pk=obj.pk, then=attr))
875                 case_statement = Case(*when_statements, output_field=field)
876                 if requires_casting:
877                     case_statement = Cast(case_statement, output_field=field)
878                 update_kwargs[field.attname] = case_statement
879             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
880         rows_updated = 0
881         queryset = self.using(self.db)
882         with transaction.atomic(using=self.db, savepoint=False):
883             for pks, update_kwargs in updates:
884                 rows_updated += queryset.filter(pk__in=pks).update(**update_kwargs)
885         return rows_updated
886 
887     bulk_update.alters_data = True
888 
889     async def abulk_update(self, objs, fields, batch_size=None):
890         return await sync_to_async(self.bulk_update)(
891             objs=objs,
892             fields=fields,
893             batch_size=batch_size,
894         )
895 
896     abulk_update.alters_data = True
897 
898     def get_or_create(self, defaults=None, **kwargs):
899         """
900         Look up an object with the given kwargs, creating one if necessary.
901         Return a tuple of (object, created), where created is a boolean
902         specifying whether an object was created.
903         """
904         # The get() needs to be targeted at the write database in order
905         # to avoid potential transaction consistency problems.
906         self._for_write = True
907         try:
908             return self.get(**kwargs), False
909         except self.model.DoesNotExist:
910             params = self._extract_model_params(defaults, **kwargs)
911             # Try to create an object using passed params.
912             try:
913                 with transaction.atomic(using=self.db):
914                     params = dict(resolve_callables(params))
915                     return self.create(**params), True
916             except IntegrityError:
917                 try:
918                     return self.get(**kwargs), False
919                 except self.model.DoesNotExist:
920                     pass
921                 raise
922 
923     async def aget_or_create(self, defaults=None, **kwargs):
924         return await sync_to_async(self.get_or_create)(
925             defaults=defaults,
926             **kwargs,
927         )
928 
929     def update_or_create(self, defaults=None, **kwargs):
930         """
931         Look up an object with the given kwargs, updating one with defaults
932         if it exists, otherwise create a new one.
933         Return a tuple (object, created), where created is a boolean
934         specifying whether an object was created.
935         """
936         defaults = defaults or {}
937         self._for_write = True
938         with transaction.atomic(using=self.db):
939             # Lock the row so that a concurrent update is blocked until
940             # update_or_create() has performed its save.
941             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
942             if created:
943                 return obj, created
944             for k, v in resolve_callables(defaults):
945                 setattr(obj, k, v)
946 
947             update_fields = set(defaults)
948             concrete_field_names = self.model._meta._non_pk_concrete_field_names
949             # update_fields does not support non-concrete fields.
950             if concrete_field_names.issuperset(update_fields):
951                 # Add fields which are set on pre_save(), e.g. auto_now fields.
952                 # This is to maintain backward compatibility as these fields
953                 # are not updated unless explicitly specified in the
954                 # update_fields list.
955                 for field in self.model._meta.local_concrete_fields:
956                     if not (
957                         field.primary_key or field.__class__.pre_save is Field.pre_save
958                     ):
959                         update_fields.add(field.name)
960                         if field.name != field.attname:
961                             update_fields.add(field.attname)
962                 obj.save(using=self.db, update_fields=update_fields)
963             else:
964                 obj.save(using=self.db)
965         return obj, False
966 
967     async def aupdate_or_create(self, defaults=None, **kwargs):
968         return await sync_to_async(self.update_or_create)(
969             defaults=defaults,
970             **kwargs,
971         )
972 
973     def _extract_model_params(self, defaults, **kwargs):
974         """
975         Prepare `params` for creating a model instance based on the given
976         kwargs; for use by get_or_create().
977         """
978         defaults = defaults or {}
979         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
980         params.update(defaults)
981         property_names = self.model._meta._property_names
982         invalid_params = []
983         for param in params:
984             try:
985                 self.model._meta.get_field(param)
986             except exceptions.FieldDoesNotExist:
987                 # It's okay to use a model's property if it has a setter.
988                 if not (param in property_names and getattr(self.model, param).fset):
989                     invalid_params.append(param)
990         if invalid_params:
991             raise exceptions.FieldError(
992                 "Invalid field name(s) for model %s: '%s'."
993                 % (
994                     self.model._meta.object_name,
995                     "', '".join(sorted(invalid_params)),
996                 )
997             )
998         return params
999 
1000     def _earliest(self, *fields):
1001         """
1002         Return the earliest object according to fields (if given) or by the
1003         model's Meta.get_latest_by.
1004         """
1005         if fields:
1006             order_by = fields
1007         else:
1008             order_by = getattr(self.model._meta, "get_latest_by")
1009             if order_by and not isinstance(order_by, (tuple, list)):
1010                 order_by = (order_by,)
1011         if order_by is None:
1012             raise ValueError(
1013                 "earliest() and latest() require either fields as positional "
1014                 "arguments or 'get_latest_by' in the model's Meta."
1015             )
1016         obj = self._chain()
1017         obj.query.set_limits(high=1)
1018         obj.query.clear_ordering(force=True)
1019         obj.query.add_ordering(*order_by)
1020         return obj.get()
1021 
1022     def earliest(self, *fields):
1023         if self.query.is_sliced:
1024             raise TypeError("Cannot change a query once a slice has been taken.")
1025         return self._earliest(*fields)
1026 
1027     async def aearliest(self, *fields):
1028         return await sync_to_async(self.earliest)(*fields)
1029 
1030     def latest(self, *fields):
1031         """
1032         Return the latest object according to fields (if given) or by the
1033         model's Meta.get_latest_by.
1034         """
1035         if self.query.is_sliced:
1036             raise TypeError("Cannot change a query once a slice has been taken.")
1037         return self.reverse()._earliest(*fields)
1038 
1039     async def alatest(self, *fields):
1040         return await sync_to_async(self.latest)(*fields)
1041 
1042     def first(self):
1043         """Return the first object of a query or None if no match is found."""
1044         if self.ordered:
1045             queryset = self
1046         else:
1047             self._check_ordering_first_last_queryset_aggregation(method="first")
1048             queryset = self.order_by("pk")
1049         for obj in queryset[:1]:
1050             return obj
1051 
1052     async def afirst(self):
1053         return await sync_to_async(self.first)()
1054 
1055     def last(self):
1056         """Return the last object of a query or None if no match is found."""
1057         if self.ordered:
1058             queryset = self.reverse()
1059         else:
1060             self._check_ordering_first_last_queryset_aggregation(method="last")
1061             queryset = self.order_by("-pk")
1062         for obj in queryset[:1]:
1063             return obj
1064 
1065     async def alast(self):
1066         return await sync_to_async(self.last)()
1067 
1068     def in_bulk(self, id_list=None, *, field_name="pk"):
1069         """
1070         Return a dictionary mapping each of the given IDs to the object with
1071         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
1072         """
1073         if self.query.is_sliced:
1074             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
1075         opts = self.model._meta
1076         unique_fields = [
1077             constraint.fields[0]
1078             for constraint in opts.total_unique_constraints
1079             if len(constraint.fields) == 1
1080         ]
1081         if (
1082             field_name != "pk"
1083             and not opts.get_field(field_name).unique
1084             and field_name not in unique_fields
1085             and self.query.distinct_fields != (field_name,)
1086         ):
1087             raise ValueError(
1088                 "in_bulk()'s field_name must be a unique field but %r isn't."
1089                 % field_name
1090             )
1091         if id_list is not None:
1092             if not id_list:
1093                 return {}
1094             filter_key = "{}__in".format(field_name)
1095             batch_size = connections[self.db].features.max_query_params
1096             id_list = tuple(id_list)
1097             # If the database has a limit on the number of query parameters
1098             # (e.g. SQLite), retrieve objects in batches if necessary.
1099             if batch_size and batch_size < len(id_list):
1100                 qs = ()
1101                 for offset in range(0, len(id_list), batch_size):
1102                     batch = id_list[offset : offset + batch_size]
1103                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
1104             else:
1105                 qs = self.filter(**{filter_key: id_list}).order_by()
1106         else:
1107             qs = self._chain()
1108         return {getattr(obj, field_name): obj for obj in qs}
1109 
1110     async def ain_bulk(self, id_list=None, *, field_name="pk"):
1111         return await sync_to_async(self.in_bulk)(
1112             id_list=id_list,
1113             field_name=field_name,
1114         )
1115 
1116     def delete(self):
1117         """Delete the records in the current QuerySet."""
1118         self._not_support_combined_queries("delete")
1119         if self.query.is_sliced:
1120             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
1121         if self.query.distinct or self.query.distinct_fields:
1122             raise TypeError("Cannot call delete() after .distinct().")
1123         if self._fields is not None:
1124             raise TypeError("Cannot call delete() after .values() or .values_list()")
1125 
1126         del_query = self._chain()
1127 
1128         # The delete is actually 2 queries - one to find related objects,
1129         # and one to delete. Make sure that the discovery of related
1130         # objects is performed on the same database as the deletion.
1131         del_query._for_write = True
1132 
1133         # Disable non-supported fields.
1134         del_query.query.select_for_update = False
1135         del_query.query.select_related = False
1136         del_query.query.clear_ordering(force=True)
1137 
1138         collector = Collector(using=del_query.db, origin=self)
1139         collector.collect(del_query)
1140         deleted, _rows_count = collector.delete()
1141 
1142         # Clear the result cache, in case this QuerySet gets reused.
1143         self._result_cache = None
1144         return deleted, _rows_count
1145 
1146     delete.alters_data = True
1147     delete.queryset_only = True
1148 
1149     async def adelete(self):
1150         return await sync_to_async(self.delete)()
1151 
1152     adelete.alters_data = True
1153     adelete.queryset_only = True
1154 
1155     def _raw_delete(self, using):
1156         """
1157         Delete objects found from the given queryset in single direct SQL
1158         query. No signals are sent and there is no protection for cascades.
1159         """
1160         query = self.query.clone()
1161         query.__class__ = sql.DeleteQuery
1162         cursor = query.get_compiler(using).execute_sql(CURSOR)
1163         if cursor:
1164             with cursor:
1165                 return cursor.rowcount
1166         return 0
1167 
1168     _raw_delete.alters_data = True
1169 
1170     def update(self, **kwargs):
1171         """
1172         Update all elements in the current QuerySet, setting all the given
1173         fields to the appropriate values.
1174         """
1175         self._not_support_combined_queries("update")
1176         if self.query.is_sliced:
1177             raise TypeError("Cannot update a query once a slice has been taken.")
1178         self._for_write = True
1179         query = self.query.chain(sql.UpdateQuery)
1180         query.add_update_values(kwargs)
1181 
1182         # Inline annotations in order_by(), if possible.
1183         new_order_by = []
1184         for col in query.order_by:
1185             if annotation := query.annotations.get(col):
1186                 if getattr(annotation, "contains_aggregate", False):
1187                     raise exceptions.FieldError(
1188                         f"Cannot update when ordering by an aggregate: {annotation}"
1189                     )
1190                 new_order_by.append(annotation)
1191             else:
1192                 new_order_by.append(col)
1193         query.order_by = tuple(new_order_by)
1194 
1195         # Clear any annotations so that they won't be present in subqueries.
1196         query.annotations = {}
1197         with transaction.mark_for_rollback_on_error(using=self.db):
1198             rows = query.get_compiler(self.db).execute_sql(CURSOR)
1199         self._result_cache = None
1200         return rows
1201 
1202     update.alters_data = True
1203 
1204     async def aupdate(self, **kwargs):
1205         return await sync_to_async(self.update)(**kwargs)
1206 
1207     aupdate.alters_data = True
1208 
1209     def _update(self, values):
1210         """
1211         A version of update() that accepts field objects instead of field names.
1212         Used primarily for model saving and not intended for use by general
1213         code (it requires too much poking around at model internals to be
1214         useful at that level).
1215         """
1216         if self.query.is_sliced:
1217             raise TypeError("Cannot update a query once a slice has been taken.")
1218         query = self.query.chain(sql.UpdateQuery)
1219         query.add_update_fields(values)
1220         # Clear any annotations so that they won't be present in subqueries.
1221         query.annotations = {}
1222         self._result_cache = None
1223         return query.get_compiler(self.db).execute_sql(CURSOR)
1224 
1225     _update.alters_data = True
1226     _update.queryset_only = False
1227 
1228     def exists(self):
1229         """
1230         Return True if the QuerySet would have any results, False otherwise.
1231         """
1232         if self._result_cache is None:
1233             return self.query.has_results(using=self.db)
1234         return bool(self._result_cache)
1235 
1236     async def aexists(self):
1237         return await sync_to_async(self.exists)()
1238 
1239     def contains(self, obj):
1240         """
1241         Return True if the QuerySet contains the provided obj,
1242         False otherwise.
1243         """
1244         self._not_support_combined_queries("contains")
1245         if self._fields is not None:
1246             raise TypeError(
1247                 "Cannot call QuerySet.contains() after .values() or .values_list()."
1248             )
1249         try:
1250             if obj._meta.concrete_model != self.model._meta.concrete_model:
1251                 return False
1252         except AttributeError:
1253             raise TypeError("'obj' must be a model instance.")
1254         if obj.pk is None:
1255             raise ValueError("QuerySet.contains() cannot be used on unsaved objects.")
1256         if self._result_cache is not None:
1257             return obj in self._result_cache
1258         return self.filter(pk=obj.pk).exists()
1259 
1260     async def acontains(self, obj):
1261         return await sync_to_async(self.contains)(obj=obj)
1262 
1263     def _prefetch_related_objects(self):
1264         # This method can only be called once the result cache has been filled.
1265         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1266         self._prefetch_done = True
1267 
1268     def explain(self, *, format=None, **options):
1269         """
1270         Runs an EXPLAIN on the SQL query this QuerySet would perform, and
1271         returns the results.
1272         """
1273         return self.query.explain(using=self.db, format=format, **options)
1274 
1275     async def aexplain(self, *, format=None, **options):
1276         return await sync_to_async(self.explain)(format=format, **options)
1277 
1278     ##################################################
1279     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
1280     ##################################################
1281 
1282     def raw(self, raw_query, params=(), translations=None, using=None):
1283         if using is None:
1284             using = self.db
1285         qs = RawQuerySet(
1286             raw_query,
1287             model=self.model,
1288             params=params,
1289             translations=translations,
1290             using=using,
1291         )
1292         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
1293         return qs
1294 
1295     def _values(self, *fields, **expressions):
1296         clone = self._chain()
1297         if expressions:
1298             clone = clone.annotate(**expressions)
1299         clone._fields = fields
1300         clone.query.set_values(fields)
1301         return clone
1302 
1303     def values(self, *fields, **expressions):
1304         fields += tuple(expressions)
1305         clone = self._values(*fields, **expressions)
1306         clone._iterable_class = ValuesIterable
1307         return clone
1308 
1309     def values_list(self, *fields, flat=False, named=False):
1310         if flat and named:
1311             raise TypeError("'flat' and 'named' can't be used together.")
1312         if flat and len(fields) > 1:
1313             raise TypeError(
1314                 "'flat' is not valid when values_list is called with more than one "
1315                 "field."
1316             )
1317 
1318         field_names = {f for f in fields if not hasattr(f, "resolve_expression")}
1319         _fields = []
1320         expressions = {}
1321         counter = 1
1322         for field in fields:
1323             if hasattr(field, "resolve_expression"):
1324                 field_id_prefix = getattr(
1325                     field, "default_alias", field.__class__.__name__.lower()
1326                 )
1327                 while True:
1328                     field_id = field_id_prefix + str(counter)
1329                     counter += 1
1330                     if field_id not in field_names:
1331                         break
1332                 expressions[field_id] = field
1333                 _fields.append(field_id)
1334             else:
1335                 _fields.append(field)
1336 
1337         clone = self._values(*_fields, **expressions)
1338         clone._iterable_class = (
1339             NamedValuesListIterable
1340             if named
1341             else FlatValuesListIterable
1342             if flat
1343             else ValuesListIterable
1344         )
1345         return clone
1346 
1347     def dates(self, field_name, kind, order="ASC"):
1348         """
1349         Return a list of date objects representing all available dates for
1350         the given field_name, scoped to 'kind'.
1351         """
1352         if kind not in ("year", "month", "week", "day"):
1353             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1354         if order not in ("ASC", "DESC"):
1355             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1356         return (
1357             self.annotate(
1358                 datefield=Trunc(field_name, kind, output_field=DateField()),
1359                 plain_field=F(field_name),
1360             )
1361             .values_list("datefield", flat=True)
1362             .distinct()
1363             .filter(plain_field__isnull=False)
1364             .order_by(("-" if order == "DESC" else "") + "datefield")
1365         )
1366 
1367     def datetimes(self, field_name, kind, order="ASC", tzinfo=None):
1368         """
1369         Return a list of datetime objects representing all available
1370         datetimes for the given field_name, scoped to 'kind'.
1371         """
1372         if kind not in ("year", "month", "week", "day", "hour", "minute", "second"):
1373             raise ValueError(
1374                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1375                 "'hour', 'minute', or 'second'."
1376             )
1377         if order not in ("ASC", "DESC"):
1378             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1379         if settings.USE_TZ:
1380             if tzinfo is None:
1381                 tzinfo = timezone.get_current_timezone()
1382         else:
1383             tzinfo = None
1384         return (
1385             self.annotate(
1386                 datetimefield=Trunc(
1387                     field_name,
1388                     kind,
1389                     output_field=DateTimeField(),
1390                     tzinfo=tzinfo,
1391                 ),
1392                 plain_field=F(field_name),
1393             )
1394             .values_list("datetimefield", flat=True)
1395             .distinct()
1396             .filter(plain_field__isnull=False)
1397             .order_by(("-" if order == "DESC" else "") + "datetimefield")
1398         )
1399 
1400     def none(self):
1401         """Return an empty QuerySet."""
1402         clone = self._chain()
1403         clone.query.set_empty()
1404         return clone
1405 
1406     ##################################################################
1407     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1408     ##################################################################
1409 
1410     def all(self):
1411         """
1412         Return a new QuerySet that is a copy of the current one. This allows a
1413         QuerySet to proxy for a model manager in some cases.
1414         """
1415         return self._chain()
1416 
1417     def filter(self, *args, **kwargs):
1418         """
1419         Return a new QuerySet instance with the args ANDed to the existing
1420         set.
1421         """
1422         self._not_support_combined_queries("filter")
1423         return self._filter_or_exclude(False, args, kwargs)
1424 
1425     def exclude(self, *args, **kwargs):
1426         """
1427         Return a new QuerySet instance with NOT (args) ANDed to the existing
1428         set.
1429         """
1430         self._not_support_combined_queries("exclude")
1431         return self._filter_or_exclude(True, args, kwargs)
1432 
1433     def _filter_or_exclude(self, negate, args, kwargs):
1434         if (args or kwargs) and self.query.is_sliced:
1435             raise TypeError("Cannot filter a query once a slice has been taken.")
1436         clone = self._chain()
1437         if self._defer_next_filter:
1438             self._defer_next_filter = False
1439             clone._deferred_filter = negate, args, kwargs
1440         else:
1441             clone._filter_or_exclude_inplace(negate, args, kwargs)
1442         return clone
1443 
1444     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1445         if negate:
1446             self._query.add_q(~Q(*args, **kwargs))
1447         else:
1448             self._query.add_q(Q(*args, **kwargs))
1449 
1450     def complex_filter(self, filter_obj):
1451         """
1452         Return a new QuerySet instance with filter_obj added to the filters.
1453 
1454         filter_obj can be a Q object or a dictionary of keyword lookup
1455         arguments.
1456 
1457         This exists to support framework features such as 'limit_choices_to',
1458         and usually it will be more natural to use other methods.
1459         """
1460         if isinstance(filter_obj, Q):
1461             clone = self._chain()
1462             clone.query.add_q(filter_obj)
1463             return clone
1464         else:
1465             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1466 
1467     def _combinator_query(self, combinator, *other_qs, all=False):
1468         # Clone the query to inherit the select list and everything
1469         clone = self._chain()
1470         # Clear limits and ordering so they can be reapplied
1471         clone.query.clear_ordering(force=True)
1472         clone.query.clear_limits()
1473         clone.query.combined_queries = (self.query,) + tuple(
1474             qs.query for qs in other_qs
1475         )
1476         clone.query.combinator = combinator
1477         clone.query.combinator_all = all
1478         return clone
1479 
1480     def union(self, *other_qs, all=False):
1481         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1482         if isinstance(self, EmptyQuerySet):
1483             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1484             if not qs:
1485                 return self
1486             if len(qs) == 1:
1487                 return qs[0]
1488             return qs[0]._combinator_query("union", *qs[1:], all=all)
1489         return self._combinator_query("union", *other_qs, all=all)
1490 
1491     def intersection(self, *other_qs):
1492         # If any query is an EmptyQuerySet, return it.
1493         if isinstance(self, EmptyQuerySet):
1494             return self
1495         for other in other_qs:
1496             if isinstance(other, EmptyQuerySet):
1497                 return other
1498         return self._combinator_query("intersection", *other_qs)
1499 
1500     def difference(self, *other_qs):
1501         # If the query is an EmptyQuerySet, return it.
1502         if isinstance(self, EmptyQuerySet):
1503             return self
1504         return self._combinator_query("difference", *other_qs)
1505 
1506     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1507         """
1508         Return a new QuerySet instance that will select objects with a
1509         FOR UPDATE lock.
1510         """
1511         if nowait and skip_locked:
1512             raise ValueError("The nowait option cannot be used with skip_locked.")
1513         obj = self._chain()
1514         obj._for_write = True
1515         obj.query.select_for_update = True
1516         obj.query.select_for_update_nowait = nowait
1517         obj.query.select_for_update_skip_locked = skip_locked
1518         obj.query.select_for_update_of = of
1519         obj.query.select_for_no_key_update = no_key
1520         return obj
1521 
1522     def select_related(self, *fields):
1523         """
1524         Return a new QuerySet instance that will select related objects.
1525 
1526         If fields are specified, they must be ForeignKey fields and only those
1527         related objects are included in the selection.
1528 
1529         If select_related(None) is called, clear the list.
1530         """
1531         self._not_support_combined_queries("select_related")
1532         if self._fields is not None:
1533             raise TypeError(
1534                 "Cannot call select_related() after .values() or .values_list()"
1535             )
1536 
1537         obj = self._chain()
1538         if fields == (None,):
1539             obj.query.select_related = False
1540         elif fields:
1541             obj.query.add_select_related(fields)
1542         else:
1543             obj.query.select_related = True
1544         return obj
1545 
1546     def prefetch_related(self, *lookups):
1547         """
1548         Return a new QuerySet instance that will prefetch the specified
1549         Many-To-One and Many-To-Many related objects when the QuerySet is
1550         evaluated.
1551 
1552         When prefetch_related() is called more than once, append to the list of
1553         prefetch lookups. If prefetch_related(None) is called, clear the list.
1554         """
1555         self._not_support_combined_queries("prefetch_related")
1556         clone = self._chain()
1557         if lookups == (None,):
1558             clone._prefetch_related_lookups = ()
1559         else:
1560             for lookup in lookups:
1561                 if isinstance(lookup, Prefetch):
1562                     lookup = lookup.prefetch_to
1563                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1564                 if lookup in self.query._filtered_relations:
1565                     raise ValueError(
1566                         "prefetch_related() is not supported with FilteredRelation."
1567                     )
1568             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1569         return clone
1570 
1571     def annotate(self, *args, **kwargs):
1572         """
1573         Return a query set in which the returned objects have been annotated
1574         with extra data or aggregations.
1575         """
1576         self._not_support_combined_queries("annotate")
1577         return self._annotate(args, kwargs, select=True)
1578 
1579     def alias(self, *args, **kwargs):
1580         """
1581         Return a query set with added aliases for extra data or aggregations.
1582         """
1583         self._not_support_combined_queries("alias")
1584         return self._annotate(args, kwargs, select=False)
1585 
1586     def _annotate(self, args, kwargs, select=True):
1587         self._validate_values_are_expressions(
1588             args + tuple(kwargs.values()), method_name="annotate"
1589         )
1590         annotations = {}
1591         for arg in args:
1592             # The default_alias property may raise a TypeError.
1593             try:
1594                 if arg.default_alias in kwargs:
1595                     raise ValueError(
1596                         "The named annotation '%s' conflicts with the "
1597                         "default name for another annotation." % arg.default_alias
1598                     )
1599             except TypeError:
1600                 raise TypeError("Complex annotations require an alias")
1601             annotations[arg.default_alias] = arg
1602         annotations.update(kwargs)
1603 
1604         clone = self._chain()
1605         names = self._fields
1606         if names is None:
1607             names = set(
1608                 chain.from_iterable(
1609                     (field.name, field.attname)
1610                     if hasattr(field, "attname")
1611                     else (field.name,)
1612                     for field in self.model._meta.get_fields()
1613                 )
1614             )
1615 
1616         for alias, annotation in annotations.items():
1617             if alias in names:
1618                 raise ValueError(
1619                     "The annotation '%s' conflicts with a field on "
1620                     "the model." % alias
1621                 )
1622             if isinstance(annotation, FilteredRelation):
1623                 clone.query.add_filtered_relation(annotation, alias)
1624             else:
1625                 clone.query.add_annotation(
1626                     annotation,
1627                     alias,
1628                     select=select,
1629                 )
1630         for alias, annotation in clone.query.annotations.items():
1631             if alias in annotations and annotation.contains_aggregate:
1632                 if clone._fields is None:
1633                     clone.query.group_by = True
1634                 else:
1635                     clone.query.set_group_by()
1636                 break
1637 
1638         return clone
1639 
1640     def order_by(self, *field_names):
1641         """Return a new QuerySet instance with the ordering changed."""
1642         if self.query.is_sliced:
1643             raise TypeError("Cannot reorder a query once a slice has been taken.")
1644         obj = self._chain()
1645         obj.query.clear_ordering(force=True, clear_default=False)
1646         obj.query.add_ordering(*field_names)
1647         return obj
1648 
1649     def distinct(self, *field_names):
1650         """
1651         Return a new QuerySet instance that will select only distinct results.
1652         """
1653         self._not_support_combined_queries("distinct")
1654         if self.query.is_sliced:
1655             raise TypeError(
1656                 "Cannot create distinct fields once a slice has been taken."
1657             )
1658         obj = self._chain()
1659         obj.query.add_distinct_fields(*field_names)
1660         return obj
1661 
1662     def extra(
1663         self,
1664         select=None,
1665         where=None,
1666         params=None,
1667         tables=None,
1668         order_by=None,
1669         select_params=None,
1670     ):
1671         """Add extra SQL fragments to the query."""
1672         self._not_support_combined_queries("extra")
1673         if self.query.is_sliced:
1674             raise TypeError("Cannot change a query once a slice has been taken.")
1675         clone = self._chain()
1676         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1677         return clone
1678 
1679     def reverse(self):
1680         """Reverse the ordering of the QuerySet."""
1681         if self.query.is_sliced:
1682             raise TypeError("Cannot reverse a query once a slice has been taken.")
1683         clone = self._chain()
1684         clone.query.standard_ordering = not clone.query.standard_ordering
1685         return clone
1686 
1687     def defer(self, *fields):
1688         """
1689         Defer the loading of data for certain fields until they are accessed.
1690         Add the set of deferred fields to any existing set of deferred fields.
1691         The only exception to this is if None is passed in as the only
1692         parameter, in which case removal all deferrals.
1693         """
1694         self._not_support_combined_queries("defer")
1695         if self._fields is not None:
1696             raise TypeError("Cannot call defer() after .values() or .values_list()")
1697         clone = self._chain()
1698         if fields == (None,):
1699             clone.query.clear_deferred_loading()
1700         else:
1701             clone.query.add_deferred_loading(fields)
1702         return clone
1703 
1704     def only(self, *fields):
1705         """
1706         Essentially, the opposite of defer(). Only the fields passed into this
1707         method and that are not already specified as deferred are loaded
1708         immediately when the queryset is evaluated.
1709         """
1710         self._not_support_combined_queries("only")
1711         if self._fields is not None:
1712             raise TypeError("Cannot call only() after .values() or .values_list()")
1713         if fields == (None,):
1714             # Can only pass None to defer(), not only(), as the rest option.
1715             # That won't stop people trying to do this, so let's be explicit.
1716             raise TypeError("Cannot pass None as an argument to only().")
1717         for field in fields:
1718             field = field.split(LOOKUP_SEP, 1)[0]
1719             if field in self.query._filtered_relations:
1720                 raise ValueError("only() is not supported with FilteredRelation.")
1721         clone = self._chain()
1722         clone.query.add_immediate_loading(fields)
1723         return clone
1724 
1725     def using(self, alias):
1726         """Select which database this QuerySet should execute against."""
1727         clone = self._chain()
1728         clone._db = alias
1729         return clone
1730 
1731     ###################################
1732     # PUBLIC INTROSPECTION ATTRIBUTES #
1733     ###################################
1734 
1735     @property
1736     def ordered(self):
1737         """
1738         Return True if the QuerySet is ordered -- i.e. has an order_by()
1739         clause or a default ordering on the model (or is empty).
1740         """
1741         if isinstance(self, EmptyQuerySet):
1742             return True
1743         if self.query.extra_order_by or self.query.order_by:
1744             return True
1745         elif (
1746             self.query.default_ordering
1747             and self.query.get_meta().ordering
1748             and
1749             # A default ordering doesn't affect GROUP BY queries.
1750             not self.query.group_by
1751         ):
1752             return True
1753         else:
1754             return False
1755 
1756     @property
1757     def db(self):
1758         """Return the database used if this query is executed now."""
1759         if self._for_write:
1760             return self._db or router.db_for_write(self.model, **self._hints)
1761         return self._db or router.db_for_read(self.model, **self._hints)
1762 
1763     ###################
1764     # PRIVATE METHODS #
1765     ###################
1766 
1767     def _insert(
1768         self,
1769         objs,
1770         fields,
1771         returning_fields=None,
1772         raw=False,
1773         using=None,
1774         on_conflict=None,
1775         update_fields=None,
1776         unique_fields=None,
1777     ):
1778         """
1779         Insert a new record for the given model. This provides an interface to
1780         the InsertQuery class and is how Model.save() is implemented.
1781         """
1782         self._for_write = True
1783         if using is None:
1784             using = self.db
1785         query = sql.InsertQuery(
1786             self.model,
1787             on_conflict=on_conflict,
1788             update_fields=update_fields,
1789             unique_fields=unique_fields,
1790         )
1791         query.insert_values(fields, objs, raw=raw)
1792         return query.get_compiler(using=using).execute_sql(returning_fields)
1793 
1794     _insert.alters_data = True
1795     _insert.queryset_only = False
1796 
1797     def _batched_insert(
1798         self,
1799         objs,
1800         fields,
1801         batch_size,
1802         on_conflict=None,
1803         update_fields=None,
1804         unique_fields=None,
1805     ):
1806         """
1807         Helper method for bulk_create() to insert objs one batch at a time.
1808         """
1809         connection = connections[self.db]
1810         ops = connection.ops
1811         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1812         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1813         inserted_rows = []
1814         bulk_return = connection.features.can_return_rows_from_bulk_insert
1815         for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:
1816             if bulk_return and on_conflict is None:
1817                 inserted_rows.extend(
1818                     self._insert(
1819                         item,
1820                         fields=fields,
1821                         using=self.db,
1822                         returning_fields=self.model._meta.db_returning_fields,
1823                     )
1824                 )
1825             else:
1826                 self._insert(
1827                     item,
1828                     fields=fields,
1829                     using=self.db,
1830                     on_conflict=on_conflict,
1831                     update_fields=update_fields,
1832                     unique_fields=unique_fields,
1833                 )
1834         return inserted_rows
1835 
1836     def _chain(self):
1837         """
1838         Return a copy of the current QuerySet that's ready for another
1839         operation.
1840         """
1841         obj = self._clone()
1842         if obj._sticky_filter:
1843             obj.query.filter_is_sticky = True
1844             obj._sticky_filter = False
1845         return obj
1846 
1847     def _clone(self):
1848         """
1849         Return a copy of the current QuerySet. A lightweight alternative
1850         to deepcopy().
1851         """
1852         c = self.__class__(
1853             model=self.model,
1854             query=self.query.chain(),
1855             using=self._db,
1856             hints=self._hints,
1857         )
1858         c._sticky_filter = self._sticky_filter
1859         c._for_write = self._for_write
1860         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1861         c._known_related_objects = self._known_related_objects
1862         c._iterable_class = self._iterable_class
1863         c._fields = self._fields
1864         return c
1865 
1866     def _fetch_all(self):
1867         if self._result_cache is None:
1868             self._result_cache = list(self._iterable_class(self))
1869         if self._prefetch_related_lookups and not self._prefetch_done:
1870             self._prefetch_related_objects()
1871 
1872     def _next_is_sticky(self):
1873         """
1874         Indicate that the next filter call and the one following that should
1875         be treated as a single filter. This is only important when it comes to
1876         determining when to reuse tables for many-to-many filters. Required so
1877         that we can filter naturally on the results of related managers.
1878 
1879         This doesn't return a clone of the current QuerySet (it returns
1880         "self"). The method is only used internally and should be immediately
1881         followed by a filter() that does create a clone.
1882         """
1883         self._sticky_filter = True
1884         return self
1885 
1886     def _merge_sanity_check(self, other):
1887         """Check that two QuerySet classes may be merged."""
1888         if self._fields is not None and (
1889             set(self.query.values_select) != set(other.query.values_select)
1890             or set(self.query.extra_select) != set(other.query.extra_select)
1891             or set(self.query.annotation_select) != set(other.query.annotation_select)
1892         ):
1893             raise TypeError(
1894                 "Merging '%s' classes must involve the same values in each case."
1895                 % self.__class__.__name__
1896             )
1897 
1898     def _merge_known_related_objects(self, other):
1899         """
1900         Keep track of all known related objects from either QuerySet instance.
1901         """
1902         for field, objects in other._known_related_objects.items():
1903             self._known_related_objects.setdefault(field, {}).update(objects)
1904 
1905     def resolve_expression(self, *args, **kwargs):
1906         if self._fields and len(self._fields) > 1:
1907             # values() queryset can only be used as nested queries
1908             # if they are set up to select only a single field.
1909             raise TypeError("Cannot use multi-field values as a filter value.")
1910         query = self.query.resolve_expression(*args, **kwargs)
1911         query._db = self._db
1912         return query
1913 
1914     resolve_expression.queryset_only = True
1915 
1916     def _add_hints(self, **hints):
1917         """
1918         Update hinting information for use by routers. Add new key/values or
1919         overwrite existing key/values.
1920         """
1921         self._hints.update(hints)
1922 
1923     def _has_filters(self):
1924         """
1925         Check if this QuerySet has any filtering going on. This isn't
1926         equivalent with checking if all objects are present in results, for
1927         example, qs[1:]._has_filters() -> False.
1928         """
1929         return self.query.has_filters()
1930 
1931     @staticmethod
1932     def _validate_values_are_expressions(values, method_name):
1933         invalid_args = sorted(
1934             str(arg) for arg in values if not hasattr(arg, "resolve_expression")
1935         )
1936         if invalid_args:
1937             raise TypeError(
1938                 "QuerySet.%s() received non-expression(s): %s."
1939                 % (
1940                     method_name,
1941                     ", ".join(invalid_args),
1942                 )
1943             )
1944 
1945     def _not_support_combined_queries(self, operation_name):
1946         if self.query.combinator:
1947             raise NotSupportedError(
1948                 "Calling QuerySet.%s() after %s() is not supported."
1949                 % (operation_name, self.query.combinator)
1950             )
1951 
1952     def _check_operator_queryset(self, other, operator_):
1953         if self.query.combinator or other.query.combinator:
1954             raise TypeError(f"Cannot use {operator_} operator with combined queryset.")
1955 
1956     def _check_ordering_first_last_queryset_aggregation(self, method):
1957         if isinstance(self.query.group_by, tuple) and not any(
1958             col.output_field is self.model._meta.pk for col in self.query.group_by
1959         ):
1960             raise TypeError(
1961                 f"Cannot use QuerySet.{method}() on an unordered queryset performing "
1962                 f"aggregation. Add an ordering with order_by()."
1963             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 06:35:51.627 | INFO     | Search completed. Bug locations: [<file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1546 class Exists(Subquery):
1547     template = "EXISTS(%(subquery)s)"
1548     output_field = fields.BooleanField()
1549 
1550     def __init__(self, queryset, **kwargs):
1551         super().__init__(queryset, **kwargs)
1552         self.query = self.query.exists()
1553 
1554     def select_format(self, compiler, sql, params):
1555         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1556         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1557         # BY list.
1558         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1559             sql = "CASE WHEN {} THEN 1 ELSE 0 END".format(sql)
1560         return sql, params

</code><intended_behavior>The `Exists` class should ensure that when constructed with an `EmptyQuerySet`, it always evaluates to a boolean value (`False` in the case of `EmptyQuerySet`), instead of resulting in `None`. The logic might need to explicitly handle `EmptyQuerySet` cases, ensuring that the SQL generated translates to a boolean expression that evaluates to `False` when the subquery is empty. This maintains the expected functionality of the `Exists` annotation, aligning with the assumption that an empty subquery should not exist, and thus, return `False`.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1571     def annotate(self, *args, **kwargs):
1572         """
1573         Return a query set in which the returned objects have been annotated
1574         with extra data or aggregations.
1575         """
1576         self._not_support_combined_queries("annotate")
1577         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>The `annotate` method, or underlying methods it calls (like `_annotate`), should ensure that annotations using `Exists` with `EmptyQuerySet` are correctly processed to return boolean values. While the method itself may not directly handle the logic of `Exists`, ensuring that the query construction process respects the `BooleanField` output requirement, even in the case of empty results, is essential. It might involve adding checks or transformations at the query construction stage where `EmptyQuerySet` influences the annotation result, specifically ensuring that an `Exists(EmptyQuerySet)` translates to a boolean `False` in SQL output, rather than `NULL`.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
290 class QuerySet(AltersData):
291     """Represent a lazy database lookup for a set of objects."""
292 
293     def __init__(self, model=None, query=None, using=None, hints=None):
294         self.model = model
295         self._db = using
296         self._hints = hints or {}
297         self._query = query or sql.Query(self.model)
298         self._result_cache = None
299         self._sticky_filter = False
300         self._for_write = False
301         self._prefetch_related_lookups = ()
302         self._prefetch_done = False
303         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
304         self._iterable_class = ModelIterable
305         self._fields = None
306         self._defer_next_filter = False
307         self._deferred_filter = None
308 
309     @property
310     def query(self):
311         if self._deferred_filter:
312             negate, args, kwargs = self._deferred_filter
313             self._filter_or_exclude_inplace(negate, args, kwargs)
314             self._deferred_filter = None
315         return self._query
316 
317     @query.setter
318     def query(self, value):
319         if value.values_select:
320             self._iterable_class = ValuesIterable
321         self._query = value
322 
323     def as_manager(cls):
324         # Address the circular dependency between `Queryset` and `Manager`.
325         from django.db.models.manager import Manager
326 
327         manager = Manager.from_queryset(cls)()
328         manager._built_with_as_manager = True
329         return manager
330 
331     as_manager.queryset_only = True
332     as_manager = classmethod(as_manager)
333 
334     ########################
335     # PYTHON MAGIC METHODS #
336     ########################
337 
338     def __deepcopy__(self, memo):
339         """Don't populate the QuerySet's cache."""
340         obj = self.__class__()
341         for k, v in self.__dict__.items():
342             if k == "_result_cache":
343                 obj.__dict__[k] = None
344             else:
345                 obj.__dict__[k] = copy.deepcopy(v, memo)
346         return obj
347 
348     def __getstate__(self):
349         # Force the cache to be fully populated.
350         self._fetch_all()
351         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
352 
353     def __setstate__(self, state):
354         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
355         if pickled_version:
356             if pickled_version != django.__version__:
357                 warnings.warn(
358                     "Pickled queryset instance's Django version %s does not "
359                     "match the current version %s."
360                     % (pickled_version, django.__version__),
361                     RuntimeWarning,
362                     stacklevel=2,
363                 )
364         else:
365             warnings.warn(
366                 "Pickled queryset instance's Django version is not specified.",
367                 RuntimeWarning,
368                 stacklevel=2,
369             )
370         self.__dict__.update(state)
371 
372     def __repr__(self):
373         data = list(self[: REPR_OUTPUT_SIZE + 1])
374         if len(data) > REPR_OUTPUT_SIZE:
375             data[-1] = "...(remaining elements truncated)..."
376         return "<%s %r>" % (self.__class__.__name__, data)
377 
378     def __len__(self):
379         self._fetch_all()
380         return len(self._result_cache)
381 
382     def __iter__(self):
383         """
384         The queryset iterator protocol uses three nested iterators in the
385         default case:
386             1. sql.compiler.execute_sql()
387                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
388                  using cursor.fetchmany(). This part is responsible for
389                  doing some column masking, and returning the rows in chunks.
390             2. sql.compiler.results_iter()
391                - Returns one row at time. At this point the rows are still just
392                  tuples. In some cases the return values are converted to
393                  Python values at this location.
394             3. self.iterator()
395                - Responsible for turning the rows into model objects.
396         """
397         self._fetch_all()
398         return iter(self._result_cache)
399 
400     def __aiter__(self):
401         # Remember, __aiter__ itself is synchronous, it's the thing it returns
402         # that is async!
403         async def generator():
404             await sync_to_async(self._fetch_all)()
405             for item in self._result_cache:
406                 yield item
407 
408         return generator()
409 
410     def __bool__(self):
411         self._fetch_all()
412         return bool(self._result_cache)
413 
414     def __getitem__(self, k):
415         """Retrieve an item or slice from the set of results."""
416         if not isinstance(k, (int, slice)):
417             raise TypeError(
418                 "QuerySet indices must be integers or slices, not %s."
419                 % type(k).__name__
420             )
421         if (isinstance(k, int) and k < 0) or (
422             isinstance(k, slice)
423             and (
424                 (k.start is not None and k.start < 0)
425                 or (k.stop is not None and k.stop < 0)
426             )
427         ):
428             raise ValueError("Negative indexing is not supported.")
429 
430         if self._result_cache is not None:
431             return self._result_cache[k]
432 
433         if isinstance(k, slice):
434             qs = self._chain()
435             if k.start is not None:
436                 start = int(k.start)
437             else:
438                 start = None
439             if k.stop is not None:
440                 stop = int(k.stop)
441             else:
442                 stop = None
443             qs.query.set_limits(start, stop)
444             return list(qs)[:: k.step] if k.step else qs
445 
446         qs = self._chain()
447         qs.query.set_limits(k, k + 1)
448         qs._fetch_all()
449         return qs._result_cache[0]
450 
451     def __class_getitem__(cls, *args, **kwargs):
452         return cls
453 
454     def __and__(self, other):
455         self._check_operator_queryset(other, "&")
456         self._merge_sanity_check(other)
457         if isinstance(other, EmptyQuerySet):
458             return other
459         if isinstance(self, EmptyQuerySet):
460             return self
461         combined = self._chain()
462         combined._merge_known_related_objects(other)
463         combined.query.combine(other.query, sql.AND)
464         return combined
465 
466     def __or__(self, other):
467         self._check_operator_queryset(other, "|")
468         self._merge_sanity_check(other)
469         if isinstance(self, EmptyQuerySet):
470             return other
471         if isinstance(other, EmptyQuerySet):
472             return self
473         query = (
474             self
475             if self.query.can_filter()
476             else self.model._base_manager.filter(pk__in=self.values("pk"))
477         )
478         combined = query._chain()
479         combined._merge_known_related_objects(other)
480         if not other.query.can_filter():
481             other = other.model._base_manager.filter(pk__in=other.values("pk"))
482         combined.query.combine(other.query, sql.OR)
483         return combined
484 
485     def __xor__(self, other):
486         self._check_operator_queryset(other, "^")
487         self._merge_sanity_check(other)
488         if isinstance(self, EmptyQuerySet):
489             return other
490         if isinstance(other, EmptyQuerySet):
491             return self
492         query = (
493             self
494             if self.query.can_filter()
495             else self.model._base_manager.filter(pk__in=self.values("pk"))
496         )
497         combined = query._chain()
498         combined._merge_known_related_objects(other)
499         if not other.query.can_filter():
500             other = other.model._base_manager.filter(pk__in=other.values("pk"))
501         combined.query.combine(other.query, sql.XOR)
502         return combined
503 
504     ####################################
505     # METHODS THAT DO DATABASE QUERIES #
506     ####################################
507 
508     def _iterator(self, use_chunked_fetch, chunk_size):
509         iterable = self._iterable_class(
510             self,
511             chunked_fetch=use_chunked_fetch,
512             chunk_size=chunk_size or 2000,
513         )
514         if not self._prefetch_related_lookups or chunk_size is None:
515             yield from iterable
516             return
517 
518         iterator = iter(iterable)
519         while results := list(islice(iterator, chunk_size)):
520             prefetch_related_objects(results, *self._prefetch_related_lookups)
521             yield from results
522 
523     def iterator(self, chunk_size=None):
524         """
525         An iterator over the results from applying this QuerySet to the
526         database. chunk_size must be provided for QuerySets that prefetch
527         related objects. Otherwise, a default chunk_size of 2000 is supplied.
528         """
529         if chunk_size is None:
530             if self._prefetch_related_lookups:
531                 raise ValueError(
532                     "chunk_size must be provided when using QuerySet.iterator() after "
533                     "prefetch_related()."
534                 )
535         elif chunk_size <= 0:
536             raise ValueError("Chunk size must be strictly positive.")
537         use_chunked_fetch = not connections[self.db].settings_dict.get(
538             "DISABLE_SERVER_SIDE_CURSORS"
539         )
540         return self._iterator(use_chunked_fetch, chunk_size)
541 
542     async def aiterator(self, chunk_size=2000):
543         """
544         An asynchronous iterator over the results from applying this QuerySet
545         to the database.
546         """
547         if self._prefetch_related_lookups:
548             raise NotSupportedError(
549                 "Using QuerySet.aiterator() after prefetch_related() is not supported."
550             )
551         if chunk_size <= 0:
552             raise ValueError("Chunk size must be strictly positive.")
553         use_chunked_fetch = not connections[self.db].settings_dict.get(
554             "DISABLE_SERVER_SIDE_CURSORS"
555         )
556         async for item in self._iterable_class(
557             self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size
558         ):
559             yield item
560 
561     def aggregate(self, *args, **kwargs):
562         """
563         Return a dictionary containing the calculations (aggregation)
564         over the current queryset.
565 
566         If args is present the expression is passed as a kwarg using
567         the Aggregate object's default alias.
568         """
569         if self.query.distinct_fields:
570             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
571         self._validate_values_are_expressions(
572             (*args, *kwargs.values()), method_name="aggregate"
573         )
574         for arg in args:
575             # The default_alias property raises TypeError if default_alias
576             # can't be set automatically or AttributeError if it isn't an
577             # attribute.
578             try:
579                 arg.default_alias
580             except (AttributeError, TypeError):
581                 raise TypeError("Complex aggregates require an alias")
582             kwargs[arg.default_alias] = arg
583 
584         return self.query.chain().get_aggregation(self.db, kwargs)
585 
586     async def aaggregate(self, *args, **kwargs):
587         return await sync_to_async(self.aggregate)(*args, **kwargs)
588 
589     def count(self):
590         """
591         Perform a SELECT COUNT() and return the number of records as an
592         integer.
593 
594         If the QuerySet is already fully cached, return the length of the
595         cached results set to avoid multiple SELECT COUNT(*) calls.
596         """
597         if self._result_cache is not None:
598             return len(self._result_cache)
599 
600         return self.query.get_count(using=self.db)
601 
602     async def acount(self):
603         return await sync_to_async(self.count)()
604 
605     def get(self, *args, **kwargs):
606         """
607         Perform the query and return a single object matching the given
608         keyword arguments.
609         """
610         if self.query.combinator and (args or kwargs):
611             raise NotSupportedError(
612                 "Calling QuerySet.get(...) with filters after %s() is not "
613                 "supported." % self.query.combinator
614             )
615         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
616         if self.query.can_filter() and not self.query.distinct_fields:
617             clone = clone.order_by()
618         limit = None
619         if (
620             not clone.query.select_for_update
621             or connections[clone.db].features.supports_select_for_update_with_limit
622         ):
623             limit = MAX_GET_RESULTS
624             clone.query.set_limits(high=limit)
625         num = len(clone)
626         if num == 1:
627             return clone._result_cache[0]
628         if not num:
629             raise self.model.DoesNotExist(
630                 "%s matching query does not exist." % self.model._meta.object_name
631             )
632         raise self.model.MultipleObjectsReturned(
633             "get() returned more than one %s -- it returned %s!"
634             % (
635                 self.model._meta.object_name,
636                 num if not limit or num < limit else "more than %s" % (limit - 1),
637             )
638         )
639 
640     async def aget(self, *args, **kwargs):
641         return await sync_to_async(self.get)(*args, **kwargs)
642 
643     def create(self, **kwargs):
644         """
645         Create a new object with the given kwargs, saving it to the database
646         and returning the created object.
647         """
648         obj = self.model(**kwargs)
649         self._for_write = True
650         obj.save(force_insert=True, using=self.db)
651         return obj
652 
653     async def acreate(self, **kwargs):
654         return await sync_to_async(self.create)(**kwargs)
655 
656     def _prepare_for_bulk_create(self, objs):
657         for obj in objs:
658             if obj.pk is None:
659                 # Populate new PK values.
660                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
661             obj._prepare_related_fields_for_save(operation_name="bulk_create")
662 
663     def _check_bulk_create_options(
664         self, ignore_conflicts, update_conflicts, update_fields, unique_fields
665     ):
666         if ignore_conflicts and update_conflicts:
667             raise ValueError(
668                 "ignore_conflicts and update_conflicts are mutually exclusive."
669             )
670         db_features = connections[self.db].features
671         if ignore_conflicts:
672             if not db_features.supports_ignore_conflicts:
673                 raise NotSupportedError(
674                     "This database backend does not support ignoring conflicts."
675                 )
676             return OnConflict.IGNORE
677         elif update_conflicts:
678             if not db_features.supports_update_conflicts:
679                 raise NotSupportedError(
680                     "This database backend does not support updating conflicts."
681                 )
682             if not update_fields:
683                 raise ValueError(
684                     "Fields that will be updated when a row insertion fails "
685                     "on conflicts must be provided."
686                 )
687             if unique_fields and not db_features.supports_update_conflicts_with_target:
688                 raise NotSupportedError(
689                     "This database backend does not support updating "
690                     "conflicts with specifying unique fields that can trigger "
691                     "the upsert."
692                 )
693             if not unique_fields and db_features.supports_update_conflicts_with_target:
694                 raise ValueError(
695                     "Unique fields that can trigger the upsert must be provided."
696                 )
697             # Updating primary keys and non-concrete fields is forbidden.
698             if any(not f.concrete or f.many_to_many for f in update_fields):
699                 raise ValueError(
700                     "bulk_create() can only be used with concrete fields in "
701                     "update_fields."
702                 )
703             if any(f.primary_key for f in update_fields):
704                 raise ValueError(
705                     "bulk_create() cannot be used with primary keys in "
706                     "update_fields."
707                 )
708             if unique_fields:
709                 if any(not f.concrete or f.many_to_many for f in unique_fields):
710                     raise ValueError(
711                         "bulk_create() can only be used with concrete fields "
712                         "in unique_fields."
713                     )
714             return OnConflict.UPDATE
715         return None
716 
717     def bulk_create(
718         self,
719         objs,
720         batch_size=None,
721         ignore_conflicts=False,
722         update_conflicts=False,
723         update_fields=None,
724         unique_fields=None,
725     ):
726         """
727         Insert each of the instances into the database. Do *not* call
728         save() on each of the instances, do not send any pre/post_save
729         signals, and do not set the primary key attribute if it is an
730         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
731         Multi-table models are not supported.
732         """
733         # When you bulk insert you don't get the primary keys back (if it's an
734         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
735         # you can't insert into the child tables which references this. There
736         # are two workarounds:
737         # 1) This could be implemented if you didn't have an autoincrement pk
738         # 2) You could do it by doing O(n) normal inserts into the parent
739         #    tables to get the primary keys back and then doing a single bulk
740         #    insert into the childmost table.
741         # We currently set the primary keys on the objects when using
742         # PostgreSQL via the RETURNING ID clause. It should be possible for
743         # Oracle as well, but the semantics for extracting the primary keys is
744         # trickier so it's not done yet.
745         if batch_size is not None and batch_size <= 0:
746             raise ValueError("Batch size must be a positive integer.")
747         # Check that the parents share the same concrete model with the our
748         # model to detect the inheritance pattern ConcreteGrandParent ->
749         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
750         # would not identify that case as involving multiple tables.
751         for parent in self.model._meta.get_parent_list():
752             if parent._meta.concrete_model is not self.model._meta.concrete_model:
753                 raise ValueError("Can't bulk create a multi-table inherited model")
754         if not objs:
755             return objs
756         opts = self.model._meta
757         if unique_fields:
758             # Primary key is allowed in unique_fields.
759             unique_fields = [
760                 self.model._meta.get_field(opts.pk.name if name == "pk" else name)
761                 for name in unique_fields
762             ]
763         if update_fields:
764             update_fields = [self.model._meta.get_field(name) for name in update_fields]
765         on_conflict = self._check_bulk_create_options(
766             ignore_conflicts,
767             update_conflicts,
768             update_fields,
769             unique_fields,
770         )
771         self._for_write = True
772         fields = opts.concrete_fields
773         objs = list(objs)
774         self._prepare_for_bulk_create(objs)
775         with transaction.atomic(using=self.db, savepoint=False):
776             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
777             if objs_with_pk:
778                 returned_columns = self._batched_insert(
779                     objs_with_pk,
780                     fields,
781                     batch_size,
782                     on_conflict=on_conflict,
783                     update_fields=update_fields,
784                     unique_fields=unique_fields,
785                 )
786                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
787                     for result, field in zip(results, opts.db_returning_fields):
788                         if field != opts.pk:
789                             setattr(obj_with_pk, field.attname, result)
790                 for obj_with_pk in objs_with_pk:
791                     obj_with_pk._state.adding = False
792                     obj_with_pk._state.db = self.db
793             if objs_without_pk:
794                 fields = [f for f in fields if not isinstance(f, AutoField)]
795                 returned_columns = self._batched_insert(
796                     objs_without_pk,
797                     fields,
798                     batch_size,
799                     on_conflict=on_conflict,
800                     update_fields=update_fields,
801                     unique_fields=unique_fields,
802                 )
803                 connection = connections[self.db]
804                 if (
805                     connection.features.can_return_rows_from_bulk_insert
806                     and on_conflict is None
807                 ):
808                     assert len(returned_columns) == len(objs_without_pk)
809                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
810                     for result, field in zip(results, opts.db_returning_fields):
811                         setattr(obj_without_pk, field.attname, result)
812                     obj_without_pk._state.adding = False
813                     obj_without_pk._state.db = self.db
814 
815         return objs
816 
817     async def abulk_create(
818         self,
819         objs,
820         batch_size=None,
821         ignore_conflicts=False,
822         update_conflicts=False,
823         update_fields=None,
824         unique_fields=None,
825     ):
826         return await sync_to_async(self.bulk_create)(
827             objs=objs,
828             batch_size=batch_size,
829             ignore_conflicts=ignore_conflicts,
830             update_conflicts=update_conflicts,
831             update_fields=update_fields,
832             unique_fields=unique_fields,
833         )
834 
835     def bulk_update(self, objs, fields, batch_size=None):
836         """
837         Update the given fields in each of the given objects in the database.
838         """
839         if batch_size is not None and batch_size <= 0:
840             raise ValueError("Batch size must be a positive integer.")
841         if not fields:
842             raise ValueError("Field names must be given to bulk_update().")
843         objs = tuple(objs)
844         if any(obj.pk is None for obj in objs):
845             raise ValueError("All bulk_update() objects must have a primary key set.")
846         fields = [self.model._meta.get_field(name) for name in fields]
847         if any(not f.concrete or f.many_to_many for f in fields):
848             raise ValueError("bulk_update() can only be used with concrete fields.")
849         if any(f.primary_key for f in fields):
850             raise ValueError("bulk_update() cannot be used with primary key fields.")
851         if not objs:
852             return 0
853         for obj in objs:
854             obj._prepare_related_fields_for_save(
855                 operation_name="bulk_update", fields=fields
856             )
857         # PK is used twice in the resulting update query, once in the filter
858         # and once in the WHEN. Each field will also have one CAST.
859         self._for_write = True
860         connection = connections[self.db]
861         max_batch_size = connection.ops.bulk_batch_size(["pk", "pk"] + fields, objs)
862         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
863         requires_casting = connection.features.requires_casted_case_in_updates
864         batches = (objs[i : i + batch_size] for i in range(0, len(objs), batch_size))
865         updates = []
866         for batch_objs in batches:
867             update_kwargs = {}
868             for field in fields:
869                 when_statements = []
870                 for obj in batch_objs:
871                     attr = getattr(obj, field.attname)
872                     if not hasattr(attr, "resolve_expression"):
873                         attr = Value(attr, output_field=field)
874                     when_statements.append(When(pk=obj.pk, then=attr))
875                 case_statement = Case(*when_statements, output_field=field)
876                 if requires_casting:
877                     case_statement = Cast(case_statement, output_field=field)
878                 update_kwargs[field.attname] = case_statement
879             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
880         rows_updated = 0
881         queryset = self.using(self.db)
882         with transaction.atomic(using=self.db, savepoint=False):
883             for pks, update_kwargs in updates:
884                 rows_updated += queryset.filter(pk__in=pks).update(**update_kwargs)
885         return rows_updated
886 
887     bulk_update.alters_data = True
888 
889     async def abulk_update(self, objs, fields, batch_size=None):
890         return await sync_to_async(self.bulk_update)(
891             objs=objs,
892             fields=fields,
893             batch_size=batch_size,
894         )
895 
896     abulk_update.alters_data = True
897 
898     def get_or_create(self, defaults=None, **kwargs):
899         """
900         Look up an object with the given kwargs, creating one if necessary.
901         Return a tuple of (object, created), where created is a boolean
902         specifying whether an object was created.
903         """
904         # The get() needs to be targeted at the write database in order
905         # to avoid potential transaction consistency problems.
906         self._for_write = True
907         try:
908             return self.get(**kwargs), False
909         except self.model.DoesNotExist:
910             params = self._extract_model_params(defaults, **kwargs)
911             # Try to create an object using passed params.
912             try:
913                 with transaction.atomic(using=self.db):
914                     params = dict(resolve_callables(params))
915                     return self.create(**params), True
916             except IntegrityError:
917                 try:
918                     return self.get(**kwargs), False
919                 except self.model.DoesNotExist:
920                     pass
921                 raise
922 
923     async def aget_or_create(self, defaults=None, **kwargs):
924         return await sync_to_async(self.get_or_create)(
925             defaults=defaults,
926             **kwargs,
927         )
928 
929     def update_or_create(self, defaults=None, **kwargs):
930         """
931         Look up an object with the given kwargs, updating one with defaults
932         if it exists, otherwise create a new one.
933         Return a tuple (object, created), where created is a boolean
934         specifying whether an object was created.
935         """
936         defaults = defaults or {}
937         self._for_write = True
938         with transaction.atomic(using=self.db):
939             # Lock the row so that a concurrent update is blocked until
940             # update_or_create() has performed its save.
941             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
942             if created:
943                 return obj, created
944             for k, v in resolve_callables(defaults):
945                 setattr(obj, k, v)
946 
947             update_fields = set(defaults)
948             concrete_field_names = self.model._meta._non_pk_concrete_field_names
949             # update_fields does not support non-concrete fields.
950             if concrete_field_names.issuperset(update_fields):
951                 # Add fields which are set on pre_save(), e.g. auto_now fields.
952                 # This is to maintain backward compatibility as these fields
953                 # are not updated unless explicitly specified in the
954                 # update_fields list.
955                 for field in self.model._meta.local_concrete_fields:
956                     if not (
957                         field.primary_key or field.__class__.pre_save is Field.pre_save
958                     ):
959                         update_fields.add(field.name)
960                         if field.name != field.attname:
961                             update_fields.add(field.attname)
962                 obj.save(using=self.db, update_fields=update_fields)
963             else:
964                 obj.save(using=self.db)
965         return obj, False
966 
967     async def aupdate_or_create(self, defaults=None, **kwargs):
968         return await sync_to_async(self.update_or_create)(
969             defaults=defaults,
970             **kwargs,
971         )
972 
973     def _extract_model_params(self, defaults, **kwargs):
974         """
975         Prepare `params` for creating a model instance based on the given
976         kwargs; for use by get_or_create().
977         """
978         defaults = defaults or {}
979         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
980         params.update(defaults)
981         property_names = self.model._meta._property_names
982         invalid_params = []
983         for param in params:
984             try:
985                 self.model._meta.get_field(param)
986             except exceptions.FieldDoesNotExist:
987                 # It's okay to use a model's property if it has a setter.
988                 if not (param in property_names and getattr(self.model, param).fset):
989                     invalid_params.append(param)
990         if invalid_params:
991             raise exceptions.FieldError(
992                 "Invalid field name(s) for model %s: '%s'."
993                 % (
994                     self.model._meta.object_name,
995                     "', '".join(sorted(invalid_params)),
996                 )
997             )
998         return params
999 
1000     def _earliest(self, *fields):
1001         """
1002         Return the earliest object according to fields (if given) or by the
1003         model's Meta.get_latest_by.
1004         """
1005         if fields:
1006             order_by = fields
1007         else:
1008             order_by = getattr(self.model._meta, "get_latest_by")
1009             if order_by and not isinstance(order_by, (tuple, list)):
1010                 order_by = (order_by,)
1011         if order_by is None:
1012             raise ValueError(
1013                 "earliest() and latest() require either fields as positional "
1014                 "arguments or 'get_latest_by' in the model's Meta."
1015             )
1016         obj = self._chain()
1017         obj.query.set_limits(high=1)
1018         obj.query.clear_ordering(force=True)
1019         obj.query.add_ordering(*order_by)
1020         return obj.get()
1021 
1022     def earliest(self, *fields):
1023         if self.query.is_sliced:
1024             raise TypeError("Cannot change a query once a slice has been taken.")
1025         return self._earliest(*fields)
1026 
1027     async def aearliest(self, *fields):
1028         return await sync_to_async(self.earliest)(*fields)
1029 
1030     def latest(self, *fields):
1031         """
1032         Return the latest object according to fields (if given) or by the
1033         model's Meta.get_latest_by.
1034         """
1035         if self.query.is_sliced:
1036             raise TypeError("Cannot change a query once a slice has been taken.")
1037         return self.reverse()._earliest(*fields)
1038 
1039     async def alatest(self, *fields):
1040         return await sync_to_async(self.latest)(*fields)
1041 
1042     def first(self):
1043         """Return the first object of a query or None if no match is found."""
1044         if self.ordered:
1045             queryset = self
1046         else:
1047             self._check_ordering_first_last_queryset_aggregation(method="first")
1048             queryset = self.order_by("pk")
1049         for obj in queryset[:1]:
1050             return obj
1051 
1052     async def afirst(self):
1053         return await sync_to_async(self.first)()
1054 
1055     def last(self):
1056         """Return the last object of a query or None if no match is found."""
1057         if self.ordered:
1058             queryset = self.reverse()
1059         else:
1060             self._check_ordering_first_last_queryset_aggregation(method="last")
1061             queryset = self.order_by("-pk")
1062         for obj in queryset[:1]:
1063             return obj
1064 
1065     async def alast(self):
1066         return await sync_to_async(self.last)()
1067 
1068     def in_bulk(self, id_list=None, *, field_name="pk"):
1069         """
1070         Return a dictionary mapping each of the given IDs to the object with
1071         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
1072         """
1073         if self.query.is_sliced:
1074             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
1075         opts = self.model._meta
1076         unique_fields = [
1077             constraint.fields[0]
1078             for constraint in opts.total_unique_constraints
1079             if len(constraint.fields) == 1
1080         ]
1081         if (
1082             field_name != "pk"
1083             and not opts.get_field(field_name).unique
1084             and field_name not in unique_fields
1085             and self.query.distinct_fields != (field_name,)
1086         ):
1087             raise ValueError(
1088                 "in_bulk()'s field_name must be a unique field but %r isn't."
1089                 % field_name
1090             )
1091         if id_list is not None:
1092             if not id_list:
1093                 return {}
1094             filter_key = "{}__in".format(field_name)
1095             batch_size = connections[self.db].features.max_query_params
1096             id_list = tuple(id_list)
1097             # If the database has a limit on the number of query parameters
1098             # (e.g. SQLite), retrieve objects in batches if necessary.
1099             if batch_size and batch_size < len(id_list):
1100                 qs = ()
1101                 for offset in range(0, len(id_list), batch_size):
1102                     batch = id_list[offset : offset + batch_size]
1103                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
1104             else:
1105                 qs = self.filter(**{filter_key: id_list}).order_by()
1106         else:
1107             qs = self._chain()
1108         return {getattr(obj, field_name): obj for obj in qs}
1109 
1110     async def ain_bulk(self, id_list=None, *, field_name="pk"):
1111         return await sync_to_async(self.in_bulk)(
1112             id_list=id_list,
1113             field_name=field_name,
1114         )
1115 
1116     def delete(self):
1117         """Delete the records in the current QuerySet."""
1118         self._not_support_combined_queries("delete")
1119         if self.query.is_sliced:
1120             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
1121         if self.query.distinct or self.query.distinct_fields:
1122             raise TypeError("Cannot call delete() after .distinct().")
1123         if self._fields is not None:
1124             raise TypeError("Cannot call delete() after .values() or .values_list()")
1125 
1126         del_query = self._chain()
1127 
1128         # The delete is actually 2 queries - one to find related objects,
1129         # and one to delete. Make sure that the discovery of related
1130         # objects is performed on the same database as the deletion.
1131         del_query._for_write = True
1132 
1133         # Disable non-supported fields.
1134         del_query.query.select_for_update = False
1135         del_query.query.select_related = False
1136         del_query.query.clear_ordering(force=True)
1137 
1138         collector = Collector(using=del_query.db, origin=self)
1139         collector.collect(del_query)
1140         deleted, _rows_count = collector.delete()
1141 
1142         # Clear the result cache, in case this QuerySet gets reused.
1143         self._result_cache = None
1144         return deleted, _rows_count
1145 
1146     delete.alters_data = True
1147     delete.queryset_only = True
1148 
1149     async def adelete(self):
1150         return await sync_to_async(self.delete)()
1151 
1152     adelete.alters_data = True
1153     adelete.queryset_only = True
1154 
1155     def _raw_delete(self, using):
1156         """
1157         Delete objects found from the given queryset in single direct SQL
1158         query. No signals are sent and there is no protection for cascades.
1159         """
1160         query = self.query.clone()
1161         query.__class__ = sql.DeleteQuery
1162         cursor = query.get_compiler(using).execute_sql(CURSOR)
1163         if cursor:
1164             with cursor:
1165                 return cursor.rowcount
1166         return 0
1167 
1168     _raw_delete.alters_data = True
1169 
1170     def update(self, **kwargs):
1171         """
1172         Update all elements in the current QuerySet, setting all the given
1173         fields to the appropriate values.
1174         """
1175         self._not_support_combined_queries("update")
1176         if self.query.is_sliced:
1177             raise TypeError("Cannot update a query once a slice has been taken.")
1178         self._for_write = True
1179         query = self.query.chain(sql.UpdateQuery)
1180         query.add_update_values(kwargs)
1181 
1182         # Inline annotations in order_by(), if possible.
1183         new_order_by = []
1184         for col in query.order_by:
1185             if annotation := query.annotations.get(col):
1186                 if getattr(annotation, "contains_aggregate", False):
1187                     raise exceptions.FieldError(
1188                         f"Cannot update when ordering by an aggregate: {annotation}"
1189                     )
1190                 new_order_by.append(annotation)
1191             else:
1192                 new_order_by.append(col)
1193         query.order_by = tuple(new_order_by)
1194 
1195         # Clear any annotations so that they won't be present in subqueries.
1196         query.annotations = {}
1197         with transaction.mark_for_rollback_on_error(using=self.db):
1198             rows = query.get_compiler(self.db).execute_sql(CURSOR)
1199         self._result_cache = None
1200         return rows
1201 
1202     update.alters_data = True
1203 
1204     async def aupdate(self, **kwargs):
1205         return await sync_to_async(self.update)(**kwargs)
1206 
1207     aupdate.alters_data = True
1208 
1209     def _update(self, values):
1210         """
1211         A version of update() that accepts field objects instead of field names.
1212         Used primarily for model saving and not intended for use by general
1213         code (it requires too much poking around at model internals to be
1214         useful at that level).
1215         """
1216         if self.query.is_sliced:
1217             raise TypeError("Cannot update a query once a slice has been taken.")
1218         query = self.query.chain(sql.UpdateQuery)
1219         query.add_update_fields(values)
1220         # Clear any annotations so that they won't be present in subqueries.
1221         query.annotations = {}
1222         self._result_cache = None
1223         return query.get_compiler(self.db).execute_sql(CURSOR)
1224 
1225     _update.alters_data = True
1226     _update.queryset_only = False
1227 
1228     def exists(self):
1229         """
1230         Return True if the QuerySet would have any results, False otherwise.
1231         """
1232         if self._result_cache is None:
1233             return self.query.has_results(using=self.db)
1234         return bool(self._result_cache)
1235 
1236     async def aexists(self):
1237         return await sync_to_async(self.exists)()
1238 
1239     def contains(self, obj):
1240         """
1241         Return True if the QuerySet contains the provided obj,
1242         False otherwise.
1243         """
1244         self._not_support_combined_queries("contains")
1245         if self._fields is not None:
1246             raise TypeError(
1247                 "Cannot call QuerySet.contains() after .values() or .values_list()."
1248             )
1249         try:
1250             if obj._meta.concrete_model != self.model._meta.concrete_model:
1251                 return False
1252         except AttributeError:
1253             raise TypeError("'obj' must be a model instance.")
1254         if obj.pk is None:
1255             raise ValueError("QuerySet.contains() cannot be used on unsaved objects.")
1256         if self._result_cache is not None:
1257             return obj in self._result_cache
1258         return self.filter(pk=obj.pk).exists()
1259 
1260     async def acontains(self, obj):
1261         return await sync_to_async(self.contains)(obj=obj)
1262 
1263     def _prefetch_related_objects(self):
1264         # This method can only be called once the result cache has been filled.
1265         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1266         self._prefetch_done = True
1267 
1268     def explain(self, *, format=None, **options):
1269         """
1270         Runs an EXPLAIN on the SQL query this QuerySet would perform, and
1271         returns the results.
1272         """
1273         return self.query.explain(using=self.db, format=format, **options)
1274 
1275     async def aexplain(self, *, format=None, **options):
1276         return await sync_to_async(self.explain)(format=format, **options)
1277 
1278     ##################################################
1279     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
1280     ##################################################
1281 
1282     def raw(self, raw_query, params=(), translations=None, using=None):
1283         if using is None:
1284             using = self.db
1285         qs = RawQuerySet(
1286             raw_query,
1287             model=self.model,
1288             params=params,
1289             translations=translations,
1290             using=using,
1291         )
1292         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
1293         return qs
1294 
1295     def _values(self, *fields, **expressions):
1296         clone = self._chain()
1297         if expressions:
1298             clone = clone.annotate(**expressions)
1299         clone._fields = fields
1300         clone.query.set_values(fields)
1301         return clone
1302 
1303     def values(self, *fields, **expressions):
1304         fields += tuple(expressions)
1305         clone = self._values(*fields, **expressions)
1306         clone._iterable_class = ValuesIterable
1307         return clone
1308 
1309     def values_list(self, *fields, flat=False, named=False):
1310         if flat and named:
1311             raise TypeError("'flat' and 'named' can't be used together.")
1312         if flat and len(fields) > 1:
1313             raise TypeError(
1314                 "'flat' is not valid when values_list is called with more than one "
1315                 "field."
1316             )
1317 
1318         field_names = {f for f in fields if not hasattr(f, "resolve_expression")}
1319         _fields = []
1320         expressions = {}
1321         counter = 1
1322         for field in fields:
1323             if hasattr(field, "resolve_expression"):
1324                 field_id_prefix = getattr(
1325                     field, "default_alias", field.__class__.__name__.lower()
1326                 )
1327                 while True:
1328                     field_id = field_id_prefix + str(counter)
1329                     counter += 1
1330                     if field_id not in field_names:
1331                         break
1332                 expressions[field_id] = field
1333                 _fields.append(field_id)
1334             else:
1335                 _fields.append(field)
1336 
1337         clone = self._values(*_fields, **expressions)
1338         clone._iterable_class = (
1339             NamedValuesListIterable
1340             if named
1341             else FlatValuesListIterable
1342             if flat
1343             else ValuesListIterable
1344         )
1345         return clone
1346 
1347     def dates(self, field_name, kind, order="ASC"):
1348         """
1349         Return a list of date objects representing all available dates for
1350         the given field_name, scoped to 'kind'.
1351         """
1352         if kind not in ("year", "month", "week", "day"):
1353             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1354         if order not in ("ASC", "DESC"):
1355             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1356         return (
1357             self.annotate(
1358                 datefield=Trunc(field_name, kind, output_field=DateField()),
1359                 plain_field=F(field_name),
1360             )
1361             .values_list("datefield", flat=True)
1362             .distinct()
1363             .filter(plain_field__isnull=False)
1364             .order_by(("-" if order == "DESC" else "") + "datefield")
1365         )
1366 
1367     def datetimes(self, field_name, kind, order="ASC", tzinfo=None):
1368         """
1369         Return a list of datetime objects representing all available
1370         datetimes for the given field_name, scoped to 'kind'.
1371         """
1372         if kind not in ("year", "month", "week", "day", "hour", "minute", "second"):
1373             raise ValueError(
1374                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1375                 "'hour', 'minute', or 'second'."
1376             )
1377         if order not in ("ASC", "DESC"):
1378             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1379         if settings.USE_TZ:
1380             if tzinfo is None:
1381                 tzinfo = timezone.get_current_timezone()
1382         else:
1383             tzinfo = None
1384         return (
1385             self.annotate(
1386                 datetimefield=Trunc(
1387                     field_name,
1388                     kind,
1389                     output_field=DateTimeField(),
1390                     tzinfo=tzinfo,
1391                 ),
1392                 plain_field=F(field_name),
1393             )
1394             .values_list("datetimefield", flat=True)
1395             .distinct()
1396             .filter(plain_field__isnull=False)
1397             .order_by(("-" if order == "DESC" else "") + "datetimefield")
1398         )
1399 
1400     def none(self):
1401         """Return an empty QuerySet."""
1402         clone = self._chain()
1403         clone.query.set_empty()
1404         return clone
1405 
1406     ##################################################################
1407     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1408     ##################################################################
1409 
1410     def all(self):
1411         """
1412         Return a new QuerySet that is a copy of the current one. This allows a
1413         QuerySet to proxy for a model manager in some cases.
1414         """
1415         return self._chain()
1416 
1417     def filter(self, *args, **kwargs):
1418         """
1419         Return a new QuerySet instance with the args ANDed to the existing
1420         set.
1421         """
1422         self._not_support_combined_queries("filter")
1423         return self._filter_or_exclude(False, args, kwargs)
1424 
1425     def exclude(self, *args, **kwargs):
1426         """
1427         Return a new QuerySet instance with NOT (args) ANDed to the existing
1428         set.
1429         """
1430         self._not_support_combined_queries("exclude")
1431         return self._filter_or_exclude(True, args, kwargs)
1432 
1433     def _filter_or_exclude(self, negate, args, kwargs):
1434         if (args or kwargs) and self.query.is_sliced:
1435             raise TypeError("Cannot filter a query once a slice has been taken.")
1436         clone = self._chain()
1437         if self._defer_next_filter:
1438             self._defer_next_filter = False
1439             clone._deferred_filter = negate, args, kwargs
1440         else:
1441             clone._filter_or_exclude_inplace(negate, args, kwargs)
1442         return clone
1443 
1444     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1445         if negate:
1446             self._query.add_q(~Q(*args, **kwargs))
1447         else:
1448             self._query.add_q(Q(*args, **kwargs))
1449 
1450     def complex_filter(self, filter_obj):
1451         """
1452         Return a new QuerySet instance with filter_obj added to the filters.
1453 
1454         filter_obj can be a Q object or a dictionary of keyword lookup
1455         arguments.
1456 
1457         This exists to support framework features such as 'limit_choices_to',
1458         and usually it will be more natural to use other methods.
1459         """
1460         if isinstance(filter_obj, Q):
1461             clone = self._chain()
1462             clone.query.add_q(filter_obj)
1463             return clone
1464         else:
1465             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1466 
1467     def _combinator_query(self, combinator, *other_qs, all=False):
1468         # Clone the query to inherit the select list and everything
1469         clone = self._chain()
1470         # Clear limits and ordering so they can be reapplied
1471         clone.query.clear_ordering(force=True)
1472         clone.query.clear_limits()
1473         clone.query.combined_queries = (self.query,) + tuple(
1474             qs.query for qs in other_qs
1475         )
1476         clone.query.combinator = combinator
1477         clone.query.combinator_all = all
1478         return clone
1479 
1480     def union(self, *other_qs, all=False):
1481         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1482         if isinstance(self, EmptyQuerySet):
1483             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1484             if not qs:
1485                 return self
1486             if len(qs) == 1:
1487                 return qs[0]
1488             return qs[0]._combinator_query("union", *qs[1:], all=all)
1489         return self._combinator_query("union", *other_qs, all=all)
1490 
1491     def intersection(self, *other_qs):
1492         # If any query is an EmptyQuerySet, return it.
1493         if isinstance(self, EmptyQuerySet):
1494             return self
1495         for other in other_qs:
1496             if isinstance(other, EmptyQuerySet):
1497                 return other
1498         return self._combinator_query("intersection", *other_qs)
1499 
1500     def difference(self, *other_qs):
1501         # If the query is an EmptyQuerySet, return it.
1502         if isinstance(self, EmptyQuerySet):
1503             return self
1504         return self._combinator_query("difference", *other_qs)
1505 
1506     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1507         """
1508         Return a new QuerySet instance that will select objects with a
1509         FOR UPDATE lock.
1510         """
1511         if nowait and skip_locked:
1512             raise ValueError("The nowait option cannot be used with skip_locked.")
1513         obj = self._chain()
1514         obj._for_write = True
1515         obj.query.select_for_update = True
1516         obj.query.select_for_update_nowait = nowait
1517         obj.query.select_for_update_skip_locked = skip_locked
1518         obj.query.select_for_update_of = of
1519         obj.query.select_for_no_key_update = no_key
1520         return obj
1521 
1522     def select_related(self, *fields):
1523         """
1524         Return a new QuerySet instance that will select related objects.
1525 
1526         If fields are specified, they must be ForeignKey fields and only those
1527         related objects are included in the selection.
1528 
1529         If select_related(None) is called, clear the list.
1530         """
1531         self._not_support_combined_queries("select_related")
1532         if self._fields is not None:
1533             raise TypeError(
1534                 "Cannot call select_related() after .values() or .values_list()"
1535             )
1536 
1537         obj = self._chain()
1538         if fields == (None,):
1539             obj.query.select_related = False
1540         elif fields:
1541             obj.query.add_select_related(fields)
1542         else:
1543             obj.query.select_related = True
1544         return obj
1545 
1546     def prefetch_related(self, *lookups):
1547         """
1548         Return a new QuerySet instance that will prefetch the specified
1549         Many-To-One and Many-To-Many related objects when the QuerySet is
1550         evaluated.
1551 
1552         When prefetch_related() is called more than once, append to the list of
1553         prefetch lookups. If prefetch_related(None) is called, clear the list.
1554         """
1555         self._not_support_combined_queries("prefetch_related")
1556         clone = self._chain()
1557         if lookups == (None,):
1558             clone._prefetch_related_lookups = ()
1559         else:
1560             for lookup in lookups:
1561                 if isinstance(lookup, Prefetch):
1562                     lookup = lookup.prefetch_to
1563                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1564                 if lookup in self.query._filtered_relations:
1565                     raise ValueError(
1566                         "prefetch_related() is not supported with FilteredRelation."
1567                     )
1568             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1569         return clone
1570 
1571     def annotate(self, *args, **kwargs):
1572         """
1573         Return a query set in which the returned objects have been annotated
1574         with extra data or aggregations.
1575         """
1576         self._not_support_combined_queries("annotate")
1577         return self._annotate(args, kwargs, select=True)
1578 
1579     def alias(self, *args, **kwargs):
1580         """
1581         Return a query set with added aliases for extra data or aggregations.
1582         """
1583         self._not_support_combined_queries("alias")
1584         return self._annotate(args, kwargs, select=False)
1585 
1586     def _annotate(self, args, kwargs, select=True):
1587         self._validate_values_are_expressions(
1588             args + tuple(kwargs.values()), method_name="annotate"
1589         )
1590         annotations = {}
1591         for arg in args:
1592             # The default_alias property may raise a TypeError.
1593             try:
1594                 if arg.default_alias in kwargs:
1595                     raise ValueError(
1596                         "The named annotation '%s' conflicts with the "
1597                         "default name for another annotation." % arg.default_alias
1598                     )
1599             except TypeError:
1600                 raise TypeError("Complex annotations require an alias")
1601             annotations[arg.default_alias] = arg
1602         annotations.update(kwargs)
1603 
1604         clone = self._chain()
1605         names = self._fields
1606         if names is None:
1607             names = set(
1608                 chain.from_iterable(
1609                     (field.name, field.attname)
1610                     if hasattr(field, "attname")
1611                     else (field.name,)
1612                     for field in self.model._meta.get_fields()
1613                 )
1614             )
1615 
1616         for alias, annotation in annotations.items():
1617             if alias in names:
1618                 raise ValueError(
1619                     "The annotation '%s' conflicts with a field on "
1620                     "the model." % alias
1621                 )
1622             if isinstance(annotation, FilteredRelation):
1623                 clone.query.add_filtered_relation(annotation, alias)
1624             else:
1625                 clone.query.add_annotation(
1626                     annotation,
1627                     alias,
1628                     select=select,
1629                 )
1630         for alias, annotation in clone.query.annotations.items():
1631             if alias in annotations and annotation.contains_aggregate:
1632                 if clone._fields is None:
1633                     clone.query.group_by = True
1634                 else:
1635                     clone.query.set_group_by()
1636                 break
1637 
1638         return clone
1639 
1640     def order_by(self, *field_names):
1641         """Return a new QuerySet instance with the ordering changed."""
1642         if self.query.is_sliced:
1643             raise TypeError("Cannot reorder a query once a slice has been taken.")
1644         obj = self._chain()
1645         obj.query.clear_ordering(force=True, clear_default=False)
1646         obj.query.add_ordering(*field_names)
1647         return obj
1648 
1649     def distinct(self, *field_names):
1650         """
1651         Return a new QuerySet instance that will select only distinct results.
1652         """
1653         self._not_support_combined_queries("distinct")
1654         if self.query.is_sliced:
1655             raise TypeError(
1656                 "Cannot create distinct fields once a slice has been taken."
1657             )
1658         obj = self._chain()
1659         obj.query.add_distinct_fields(*field_names)
1660         return obj
1661 
1662     def extra(
1663         self,
1664         select=None,
1665         where=None,
1666         params=None,
1667         tables=None,
1668         order_by=None,
1669         select_params=None,
1670     ):
1671         """Add extra SQL fragments to the query."""
1672         self._not_support_combined_queries("extra")
1673         if self.query.is_sliced:
1674             raise TypeError("Cannot change a query once a slice has been taken.")
1675         clone = self._chain()
1676         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1677         return clone
1678 
1679     def reverse(self):
1680         """Reverse the ordering of the QuerySet."""
1681         if self.query.is_sliced:
1682             raise TypeError("Cannot reverse a query once a slice has been taken.")
1683         clone = self._chain()
1684         clone.query.standard_ordering = not clone.query.standard_ordering
1685         return clone
1686 
1687     def defer(self, *fields):
1688         """
1689         Defer the loading of data for certain fields until they are accessed.
1690         Add the set of deferred fields to any existing set of deferred fields.
1691         The only exception to this is if None is passed in as the only
1692         parameter, in which case removal all deferrals.
1693         """
1694         self._not_support_combined_queries("defer")
1695         if self._fields is not None:
1696             raise TypeError("Cannot call defer() after .values() or .values_list()")
1697         clone = self._chain()
1698         if fields == (None,):
1699             clone.query.clear_deferred_loading()
1700         else:
1701             clone.query.add_deferred_loading(fields)
1702         return clone
1703 
1704     def only(self, *fields):
1705         """
1706         Essentially, the opposite of defer(). Only the fields passed into this
1707         method and that are not already specified as deferred are loaded
1708         immediately when the queryset is evaluated.
1709         """
1710         self._not_support_combined_queries("only")
1711         if self._fields is not None:
1712             raise TypeError("Cannot call only() after .values() or .values_list()")
1713         if fields == (None,):
1714             # Can only pass None to defer(), not only(), as the rest option.
1715             # That won't stop people trying to do this, so let's be explicit.
1716             raise TypeError("Cannot pass None as an argument to only().")
1717         for field in fields:
1718             field = field.split(LOOKUP_SEP, 1)[0]
1719             if field in self.query._filtered_relations:
1720                 raise ValueError("only() is not supported with FilteredRelation.")
1721         clone = self._chain()
1722         clone.query.add_immediate_loading(fields)
1723         return clone
1724 
1725     def using(self, alias):
1726         """Select which database this QuerySet should execute against."""
1727         clone = self._chain()
1728         clone._db = alias
1729         return clone
1730 
1731     ###################################
1732     # PUBLIC INTROSPECTION ATTRIBUTES #
1733     ###################################
1734 
1735     @property
1736     def ordered(self):
1737         """
1738         Return True if the QuerySet is ordered -- i.e. has an order_by()
1739         clause or a default ordering on the model (or is empty).
1740         """
1741         if isinstance(self, EmptyQuerySet):
1742             return True
1743         if self.query.extra_order_by or self.query.order_by:
1744             return True
1745         elif (
1746             self.query.default_ordering
1747             and self.query.get_meta().ordering
1748             and
1749             # A default ordering doesn't affect GROUP BY queries.
1750             not self.query.group_by
1751         ):
1752             return True
1753         else:
1754             return False
1755 
1756     @property
1757     def db(self):
1758         """Return the database used if this query is executed now."""
1759         if self._for_write:
1760             return self._db or router.db_for_write(self.model, **self._hints)
1761         return self._db or router.db_for_read(self.model, **self._hints)
1762 
1763     ###################
1764     # PRIVATE METHODS #
1765     ###################
1766 
1767     def _insert(
1768         self,
1769         objs,
1770         fields,
1771         returning_fields=None,
1772         raw=False,
1773         using=None,
1774         on_conflict=None,
1775         update_fields=None,
1776         unique_fields=None,
1777     ):
1778         """
1779         Insert a new record for the given model. This provides an interface to
1780         the InsertQuery class and is how Model.save() is implemented.
1781         """
1782         self._for_write = True
1783         if using is None:
1784             using = self.db
1785         query = sql.InsertQuery(
1786             self.model,
1787             on_conflict=on_conflict,
1788             update_fields=update_fields,
1789             unique_fields=unique_fields,
1790         )
1791         query.insert_values(fields, objs, raw=raw)
1792         return query.get_compiler(using=using).execute_sql(returning_fields)
1793 
1794     _insert.alters_data = True
1795     _insert.queryset_only = False
1796 
1797     def _batched_insert(
1798         self,
1799         objs,
1800         fields,
1801         batch_size,
1802         on_conflict=None,
1803         update_fields=None,
1804         unique_fields=None,
1805     ):
1806         """
1807         Helper method for bulk_create() to insert objs one batch at a time.
1808         """
1809         connection = connections[self.db]
1810         ops = connection.ops
1811         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1812         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1813         inserted_rows = []
1814         bulk_return = connection.features.can_return_rows_from_bulk_insert
1815         for item in [objs[i : i + batch_size] for i in range(0, len(objs), batch_size)]:
1816             if bulk_return and on_conflict is None:
1817                 inserted_rows.extend(
1818                     self._insert(
1819                         item,
1820                         fields=fields,
1821                         using=self.db,
1822                         returning_fields=self.model._meta.db_returning_fields,
1823                     )
1824                 )
1825             else:
1826                 self._insert(
1827                     item,
1828                     fields=fields,
1829                     using=self.db,
1830                     on_conflict=on_conflict,
1831                     update_fields=update_fields,
1832                     unique_fields=unique_fields,
1833                 )
1834         return inserted_rows
1835 
1836     def _chain(self):
1837         """
1838         Return a copy of the current QuerySet that's ready for another
1839         operation.
1840         """
1841         obj = self._clone()
1842         if obj._sticky_filter:
1843             obj.query.filter_is_sticky = True
1844             obj._sticky_filter = False
1845         return obj
1846 
1847     def _clone(self):
1848         """
1849         Return a copy of the current QuerySet. A lightweight alternative
1850         to deepcopy().
1851         """
1852         c = self.__class__(
1853             model=self.model,
1854             query=self.query.chain(),
1855             using=self._db,
1856             hints=self._hints,
1857         )
1858         c._sticky_filter = self._sticky_filter
1859         c._for_write = self._for_write
1860         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1861         c._known_related_objects = self._known_related_objects
1862         c._iterable_class = self._iterable_class
1863         c._fields = self._fields
1864         return c
1865 
1866     def _fetch_all(self):
1867         if self._result_cache is None:
1868             self._result_cache = list(self._iterable_class(self))
1869         if self._prefetch_related_lookups and not self._prefetch_done:
1870             self._prefetch_related_objects()
1871 
1872     def _next_is_sticky(self):
1873         """
1874         Indicate that the next filter call and the one following that should
1875         be treated as a single filter. This is only important when it comes to
1876         determining when to reuse tables for many-to-many filters. Required so
1877         that we can filter naturally on the results of related managers.
1878 
1879         This doesn't return a clone of the current QuerySet (it returns
1880         "self"). The method is only used internally and should be immediately
1881         followed by a filter() that does create a clone.
1882         """
1883         self._sticky_filter = True
1884         return self
1885 
1886     def _merge_sanity_check(self, other):
1887         """Check that two QuerySet classes may be merged."""
1888         if self._fields is not None and (
1889             set(self.query.values_select) != set(other.query.values_select)
1890             or set(self.query.extra_select) != set(other.query.extra_select)
1891             or set(self.query.annotation_select) != set(other.query.annotation_select)
1892         ):
1893             raise TypeError(
1894                 "Merging '%s' classes must involve the same values in each case."
1895                 % self.__class__.__name__
1896             )
1897 
1898     def _merge_known_related_objects(self, other):
1899         """
1900         Keep track of all known related objects from either QuerySet instance.
1901         """
1902         for field, objects in other._known_related_objects.items():
1903             self._known_related_objects.setdefault(field, {}).update(objects)
1904 
1905     def resolve_expression(self, *args, **kwargs):
1906         if self._fields and len(self._fields) > 1:
1907             # values() queryset can only be used as nested queries
1908             # if they are set up to select only a single field.
1909             raise TypeError("Cannot use multi-field values as a filter value.")
1910         query = self.query.resolve_expression(*args, **kwargs)
1911         query._db = self._db
1912         return query
1913 
1914     resolve_expression.queryset_only = True
1915 
1916     def _add_hints(self, **hints):
1917         """
1918         Update hinting information for use by routers. Add new key/values or
1919         overwrite existing key/values.
1920         """
1921         self._hints.update(hints)
1922 
1923     def _has_filters(self):
1924         """
1925         Check if this QuerySet has any filtering going on. This isn't
1926         equivalent with checking if all objects are present in results, for
1927         example, qs[1:]._has_filters() -> False.
1928         """
1929         return self.query.has_filters()
1930 
1931     @staticmethod
1932     def _validate_values_are_expressions(values, method_name):
1933         invalid_args = sorted(
1934             str(arg) for arg in values if not hasattr(arg, "resolve_expression")
1935         )
1936         if invalid_args:
1937             raise TypeError(
1938                 "QuerySet.%s() received non-expression(s): %s."
1939                 % (
1940                     method_name,
1941                     ", ".join(invalid_args),
1942                 )
1943             )
1944 
1945     def _not_support_combined_queries(self, operation_name):
1946         if self.query.combinator:
1947             raise NotSupportedError(
1948                 "Calling QuerySet.%s() after %s() is not supported."
1949                 % (operation_name, self.query.combinator)
1950             )
1951 
1952     def _check_operator_queryset(self, other, operator_):
1953         if self.query.combinator or other.query.combinator:
1954             raise TypeError(f"Cannot use {operator_} operator with combined queryset.")
1955 
1956     def _check_ordering_first_last_queryset_aggregation(self, method):
1957         if isinstance(self.query.group_by, tuple) and not any(
1958             col.output_field is self.model._meta.pk for col in self.query.group_by
1959         ):
1960             raise TypeError(
1961                 f"Cannot use QuerySet.{method}() on an unordered queryset performing "
1962                 f"aggregation. Add an ordering with order_by()."
1963             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 06:35:51.628 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 06:35:51.628 | INFO     | Search-only: skipping patch selection
2025-10-15 06:35:51.768 | INFO     | Task django__django-16491 completed successfully.
2025-10-15 06:35:51.769 | INFO     | No patch generated. You can try running ACR again.
