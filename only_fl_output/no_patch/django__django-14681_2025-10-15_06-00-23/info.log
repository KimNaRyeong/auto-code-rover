2025-10-15 06:00:23.434 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 06:00:26.604 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=391, output_tokens=13, cost=0.004300
2025-10-15 06:00:47.105 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=571, output_tokens=840, cost=0.030910
2025-10-15 06:00:48.198 | INFO     | registered non reproducing test 0
2025-10-15 06:01:04.191 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1757, output_tokens=698, cost=0.038510
2025-10-15 06:01:05.236 | INFO     | registered non reproducing test 1
2025-10-15 06:01:28.707 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1672, output_tokens=802, cost=0.040780
2025-10-15 06:01:29.910 | INFO     | registered non reproducing test 2
2025-10-15 06:01:29.910 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 06:01:29.913 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 06:01:38.533 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=698, output_tokens=316, cost=0.016460
2025-10-15 06:01:38.540 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 06:01:42.216 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=670, output_tokens=66, cost=0.008680
2025-10-15 06:01:42.218 | DEBUG    | Extracted a valid json.
2025-10-15 06:01:42.218 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"File\")",
        "search_class_in_file(\"FileAdmin\", \"app/admin.py\")",
        "search_method(\"save\")",
        "search_method(\"handle_uploaded_file\")",
        "search_code(\"except\")"
    ],
    "bug_locations": []
}
2025-10-15 06:01:43.605 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 06:01:43.605 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 06:02:05.460 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=8049, output_tokens=746, cost=0.102870
2025-10-15 06:02:05.468 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 06:02:18.191 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=9011, output_tokens=461, cost=0.103940
2025-10-15 06:02:18.196 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 06:02:26.008 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=815, output_tokens=200, cost=0.014150
2025-10-15 06:02:26.009 | DEBUG    | Extracted a valid json.
2025-10-15 06:02:26.009 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/core/files/uploadhandler.py",
            "class": "TemporaryFileUploadHandler",
            "method": "",
            "intended_behavior": "Ensure that exceptions raised due to file storage issues are correctly identified and reported, instead of being misinterpreted as CSRF errors."
        },
        {
            "file": "django/http/multipartparser.py",
            "class": "",
            "method": "",
            "intended_behavior": "Implement additional error handling for file storage-related issues during file uploads, to accurately report errors related to file storage or permissions."
        },
        {
            "file": "django/middleware/csrf.py",
            "class": "",
            "method": "Middleware processing methods",
            "intended_behavior": "Introduce logic to differentiate between CSRF token issues and other exceptions, particularly those caused by file storage problems, to improve error classification and reporting."
        }
    ]
}
2025-10-15 06:02:26.018 | DEBUG    | Bug location extracted successfully: [<file>django/core/files/uploadhandler.py</file>
<class>TemporaryFileUploadHandler</class>
<method>None</method>
<code>
138 class TemporaryFileUploadHandler(FileUploadHandler):
139     """
140     Upload handler that streams data into a temporary file.
141     """
142     def new_file(self, *args, **kwargs):
143         """
144         Create the file object to append to as data is coming in.
145         """
146         super().new_file(*args, **kwargs)
147         self.file = TemporaryUploadedFile(self.file_name, self.content_type, 0, self.charset, self.content_type_extra)
148 
149     def receive_data_chunk(self, raw_data, start):
150         self.file.write(raw_data)
151 
152     def file_complete(self, file_size):
153         self.file.seek(0)
154         self.file.size = file_size
155         return self.file
156 
157     def upload_interrupted(self):
158         if hasattr(self, 'file'):
159             temp_location = self.file.temporary_file_path()
160             try:
161                 self.file.close()
162                 os.remove(temp_location)
163             except FileNotFoundError:
164                 pass

</code><intended_behavior>Ensure that exceptions raised due to file storage issues are correctly identified and reported, instead of being misinterpreted as CSRF errors.</intended_behavior>, <file>django/http/multipartparser.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Multi-part parsing for file uploads.
3 
4 Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to
5 file upload handlers for processing.
6 """
7 import base64
8 import binascii
9 import cgi
10 import collections
11 import html
12 from urllib.parse import unquote
13 
14 from django.conf import settings
15 from django.core.exceptions import (
16     RequestDataTooBig, SuspiciousMultipartForm, TooManyFieldsSent,
17 )
18 from django.core.files.uploadhandler import (
19     SkipFile, StopFutureHandlers, StopUpload,
20 )
21 from django.utils.datastructures import MultiValueDict
22 from django.utils.encoding import force_str
23 
24 __all__ = ('MultiPartParser', 'MultiPartParserError', 'InputStreamExhausted')
25 
26 
27 class MultiPartParserError(Exception):
28     pass
29 
30 
31 class InputStreamExhausted(Exception):
32     """
33     No more reads are allowed from this device.
34     """
35     pass
36 
37 
38 RAW = "raw"
39 FILE = "file"
40 FIELD = "field"
41 
42 
43 class MultiPartParser:
44     """
45     A rfc2388 multipart/form-data parser.
46 
47     ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks
48     and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.
49     """
50     def __init__(self, META, input_data, upload_handlers, encoding=None):
51         """
52         Initialize the MultiPartParser object.
53 
54         :META:
55             The standard ``META`` dictionary in Django request objects.
56         :input_data:
57             The raw post data, as a file-like object.
58         :upload_handlers:
59             A list of UploadHandler instances that perform operations on the
60             uploaded data.
61         :encoding:
62             The encoding with which to treat the incoming data.
63         """
64         # Content-Type should contain multipart and the boundary information.
65         content_type = META.get('CONTENT_TYPE', '')
66         if not content_type.startswith('multipart/'):
67             raise MultiPartParserError('Invalid Content-Type: %s' % content_type)
68 
69         # Parse the header to get the boundary to split the parts.
70         try:
71             ctypes, opts = parse_header(content_type.encode('ascii'))
72         except UnicodeEncodeError:
73             raise MultiPartParserError('Invalid non-ASCII Content-Type in multipart: %s' % force_str(content_type))
74         boundary = opts.get('boundary')
75         if not boundary or not cgi.valid_boundary(boundary):
76             raise MultiPartParserError('Invalid boundary in multipart: %s' % force_str(boundary))
77 
78         # Content-Length should contain the length of the body we are about
79         # to receive.
80         try:
81             content_length = int(META.get('CONTENT_LENGTH', 0))
82         except (ValueError, TypeError):
83             content_length = 0
84 
85         if content_length < 0:
86             # This means we shouldn't continue...raise an error.
87             raise MultiPartParserError("Invalid content length: %r" % content_length)
88 
89         if isinstance(boundary, str):
90             boundary = boundary.encode('ascii')
91         self._boundary = boundary
92         self._input_data = input_data
93 
94         # For compatibility with low-level network APIs (with 32-bit integers),
95         # the chunk size should be < 2^31, but still divisible by 4.
96         possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]
97         self._chunk_size = min([2 ** 31 - 4] + possible_sizes)
98 
99         self._meta = META
100         self._encoding = encoding or settings.DEFAULT_CHARSET
101         self._content_length = content_length
102         self._upload_handlers = upload_handlers
103 
104     def parse(self):
105         """
106         Parse the POST data and break it into a FILES MultiValueDict and a POST
107         MultiValueDict.
108 
109         Return a tuple containing the POST and FILES dictionary, respectively.
110         """
111         from django.http import QueryDict
112 
113         encoding = self._encoding
114         handlers = self._upload_handlers
115 
116         # HTTP spec says that Content-Length >= 0 is valid
117         # handling content-length == 0 before continuing
118         if self._content_length == 0:
119             return QueryDict(encoding=self._encoding), MultiValueDict()
120 
121         # See if any of the handlers take care of the parsing.
122         # This allows overriding everything if need be.
123         for handler in handlers:
124             result = handler.handle_raw_input(
125                 self._input_data,
126                 self._meta,
127                 self._content_length,
128                 self._boundary,
129                 encoding,
130             )
131             # Check to see if it was handled
132             if result is not None:
133                 return result[0], result[1]
134 
135         # Create the data structures to be used later.
136         self._post = QueryDict(mutable=True)
137         self._files = MultiValueDict()
138 
139         # Instantiate the parser and stream:
140         stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))
141 
142         # Whether or not to signal a file-completion at the beginning of the loop.
143         old_field_name = None
144         counters = [0] * len(handlers)
145 
146         # Number of bytes that have been read.
147         num_bytes_read = 0
148         # To count the number of keys in the request.
149         num_post_keys = 0
150         # To limit the amount of data read from the request.
151         read_size = None
152         # Whether a file upload is finished.
153         uploaded_file = True
154 
155         try:
156             for item_type, meta_data, field_stream in Parser(stream, self._boundary):
157                 if old_field_name:
158                     # We run this at the beginning of the next loop
159                     # since we cannot be sure a file is complete until
160                     # we hit the next boundary/part of the multipart content.
161                     self.handle_file_complete(old_field_name, counters)
162                     old_field_name = None
163                     uploaded_file = True
164 
165                 try:
166                     disposition = meta_data['content-disposition'][1]
167                     field_name = disposition['name'].strip()
168                 except (KeyError, IndexError, AttributeError):
169                     continue
170 
171                 transfer_encoding = meta_data.get('content-transfer-encoding')
172                 if transfer_encoding is not None:
173                     transfer_encoding = transfer_encoding[0].strip()
174                 field_name = force_str(field_name, encoding, errors='replace')
175 
176                 if item_type == FIELD:
177                     # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
178                     num_post_keys += 1
179                     if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and
180                             settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):
181                         raise TooManyFieldsSent(
182                             'The number of GET/POST parameters exceeded '
183                             'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
184                         )
185 
186                     # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
187                     if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:
188                         read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read
189 
190                     # This is a post field, we can just set it in the post
191                     if transfer_encoding == 'base64':
192                         raw_data = field_stream.read(size=read_size)
193                         num_bytes_read += len(raw_data)
194                         try:
195                             data = base64.b64decode(raw_data)
196                         except binascii.Error:
197                             data = raw_data
198                     else:
199                         data = field_stream.read(size=read_size)
200                         num_bytes_read += len(data)
201 
202                     # Add two here to make the check consistent with the
203                     # x-www-form-urlencoded check that includes '&='.
204                     num_bytes_read += len(field_name) + 2
205                     if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and
206                             num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):
207                         raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')
208 
209                     self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))
210                 elif item_type == FILE:
211                     # This is a file, use the handler...
212                     file_name = disposition.get('filename')
213                     if file_name:
214                         file_name = force_str(file_name, encoding, errors='replace')
215                         file_name = self.sanitize_file_name(file_name)
216                     if not file_name:
217                         continue
218 
219                     content_type, content_type_extra = meta_data.get('content-type', ('', {}))
220                     content_type = content_type.strip()
221                     charset = content_type_extra.get('charset')
222 
223                     try:
224                         content_length = int(meta_data.get('content-length')[0])
225                     except (IndexError, TypeError, ValueError):
226                         content_length = None
227 
228                     counters = [0] * len(handlers)
229                     uploaded_file = False
230                     try:
231                         for handler in handlers:
232                             try:
233                                 handler.new_file(
234                                     field_name, file_name, content_type,
235                                     content_length, charset, content_type_extra,
236                                 )
237                             except StopFutureHandlers:
238                                 break
239 
240                         for chunk in field_stream:
241                             if transfer_encoding == 'base64':
242                                 # We only special-case base64 transfer encoding
243                                 # We should always decode base64 chunks by multiple of 4,
244                                 # ignoring whitespace.
245 
246                                 stripped_chunk = b"".join(chunk.split())
247 
248                                 remaining = len(stripped_chunk) % 4
249                                 while remaining != 0:
250                                     over_chunk = field_stream.read(4 - remaining)
251                                     stripped_chunk += b"".join(over_chunk.split())
252                                     remaining = len(stripped_chunk) % 4
253 
254                                 try:
255                                     chunk = base64.b64decode(stripped_chunk)
256                                 except Exception as exc:
257                                     # Since this is only a chunk, any error is an unfixable error.
258                                     raise MultiPartParserError("Could not decode base64 data.") from exc
259 
260                             for i, handler in enumerate(handlers):
261                                 chunk_length = len(chunk)
262                                 chunk = handler.receive_data_chunk(chunk, counters[i])
263                                 counters[i] += chunk_length
264                                 if chunk is None:
265                                     # Don't continue if the chunk received by
266                                     # the handler is None.
267                                     break
268 
269                     except SkipFile:
270                         self._close_files()
271                         # Just use up the rest of this file...
272                         exhaust(field_stream)
273                     else:
274                         # Handle file upload completions on next iteration.
275                         old_field_name = field_name
276                 else:
277                     # If this is neither a FIELD or a FILE, just exhaust the stream.
278                     exhaust(stream)
279         except StopUpload as e:
280             self._close_files()
281             if not e.connection_reset:
282                 exhaust(self._input_data)
283         else:
284             if not uploaded_file:
285                 for handler in handlers:
286                     handler.upload_interrupted()
287             # Make sure that the request data is all fed
288             exhaust(self._input_data)
289 
290         # Signal that the upload has completed.
291         # any() shortcircuits if a handler's upload_complete() returns a value.
292         any(handler.upload_complete() for handler in handlers)
293         self._post._mutable = False
294         return self._post, self._files
295 
296     def handle_file_complete(self, old_field_name, counters):
297         """
298         Handle all the signaling that takes place when a file is complete.
299         """
300         for i, handler in enumerate(self._upload_handlers):
301             file_obj = handler.file_complete(counters[i])
302             if file_obj:
303                 # If it returns a file object, then set the files dict.
304                 self._files.appendlist(force_str(old_field_name, self._encoding, errors='replace'), file_obj)
305                 break
306 
307     def sanitize_file_name(self, file_name):
308         """
309         Sanitize the filename of an upload.
310 
311         Remove all possible path separators, even though that might remove more
312         than actually required by the target system. Filenames that could
313         potentially cause problems (current/parent dir) are also discarded.
314 
315         It should be noted that this function could still return a "filepath"
316         like "C:some_file.txt" which is handled later on by the storage layer.
317         So while this function does sanitize filenames to some extent, the
318         resulting filename should still be considered as untrusted user input.
319         """
320         file_name = html.unescape(file_name)
321         file_name = file_name.rsplit('/')[-1]
322         file_name = file_name.rsplit('\\')[-1]
323 
324         if file_name in {'', '.', '..'}:
325             return None
326         return file_name
327 
328     IE_sanitize = sanitize_file_name
329 
330     def _close_files(self):
331         # Free up all file handles.
332         # FIXME: this currently assumes that upload handlers store the file as 'file'
333         # We should document that... (Maybe add handler.free_file to complement new_file)
334         for handler in self._upload_handlers:
335             if hasattr(handler, 'file'):
336                 handler.file.close()
337 
338 
339 class LazyStream:
340     """
341     The LazyStream wrapper allows one to get and "unget" bytes from a stream.
342 
343     Given a producer object (an iterator that yields bytestrings), the
344     LazyStream object will support iteration, reading, and keeping a "look-back"
345     variable in case you need to "unget" some bytes.
346     """
347     def __init__(self, producer, length=None):
348         """
349         Every LazyStream must have a producer when instantiated.
350 
351         A producer is an iterable that returns a string each time it
352         is called.
353         """
354         self._producer = producer
355         self._empty = False
356         self._leftover = b''
357         self.length = length
358         self.position = 0
359         self._remaining = length
360         self._unget_history = []
361 
362     def tell(self):
363         return self.position
364 
365     def read(self, size=None):
366         def parts():
367             remaining = self._remaining if size is None else size
368             # do the whole thing in one shot if no limit was provided.
369             if remaining is None:
370                 yield b''.join(self)
371                 return
372 
373             # otherwise do some bookkeeping to return exactly enough
374             # of the stream and stashing any extra content we get from
375             # the producer
376             while remaining != 0:
377                 assert remaining > 0, 'remaining bytes to read should never go negative'
378 
379                 try:
380                     chunk = next(self)
381                 except StopIteration:
382                     return
383                 else:
384                     emitting = chunk[:remaining]
385                     self.unget(chunk[remaining:])
386                     remaining -= len(emitting)
387                     yield emitting
388 
389         return b''.join(parts())
390 
391     def __next__(self):
392         """
393         Used when the exact number of bytes to read is unimportant.
394 
395         Return whatever chunk is conveniently returned from the iterator.
396         Useful to avoid unnecessary bookkeeping if performance is an issue.
397         """
398         if self._leftover:
399             output = self._leftover
400             self._leftover = b''
401         else:
402             output = next(self._producer)
403             self._unget_history = []
404         self.position += len(output)
405         return output
406 
407     def close(self):
408         """
409         Used to invalidate/disable this lazy stream.
410 
411         Replace the producer with an empty list. Any leftover bytes that have
412         already been read will still be reported upon read() and/or next().
413         """
414         self._producer = []
415 
416     def __iter__(self):
417         return self
418 
419     def unget(self, bytes):
420         """
421         Place bytes back onto the front of the lazy stream.
422 
423         Future calls to read() will return those bytes first. The
424         stream position and thus tell() will be rewound.
425         """
426         if not bytes:
427             return
428         self._update_unget_history(len(bytes))
429         self.position -= len(bytes)
430         self._leftover = bytes + self._leftover
431 
432     def _update_unget_history(self, num_bytes):
433         """
434         Update the unget history as a sanity check to see if we've pushed
435         back the same number of bytes in one chunk. If we keep ungetting the
436         same number of bytes many times (here, 50), we're mostly likely in an
437         infinite loop of some sort. This is usually caused by a
438         maliciously-malformed MIME request.
439         """
440         self._unget_history = [num_bytes] + self._unget_history[:49]
441         number_equal = len([
442             current_number for current_number in self._unget_history
443             if current_number == num_bytes
444         ])
445 
446         if number_equal > 40:
447             raise SuspiciousMultipartForm(
448                 "The multipart parser got stuck, which shouldn't happen with"
449                 " normal uploaded files. Check for malicious upload activity;"
450                 " if there is none, report this to the Django developers."
451             )
452 
453 
454 class ChunkIter:
455     """
456     An iterable that will yield chunks of data. Given a file-like object as the
457     constructor, yield chunks of read operations from that object.
458     """
459     def __init__(self, flo, chunk_size=64 * 1024):
460         self.flo = flo
461         self.chunk_size = chunk_size
462 
463     def __next__(self):
464         try:
465             data = self.flo.read(self.chunk_size)
466         except InputStreamExhausted:
467             raise StopIteration()
468         if data:
469             return data
470         else:
471             raise StopIteration()
472 
473     def __iter__(self):
474         return self
475 
476 
477 class InterBoundaryIter:
478     """
479     A Producer that will iterate over boundaries.
480     """
481     def __init__(self, stream, boundary):
482         self._stream = stream
483         self._boundary = boundary
484 
485     def __iter__(self):
486         return self
487 
488     def __next__(self):
489         try:
490             return LazyStream(BoundaryIter(self._stream, self._boundary))
491         except InputStreamExhausted:
492             raise StopIteration()
493 
494 
495 class BoundaryIter:
496     """
497     A Producer that is sensitive to boundaries.
498 
499     Will happily yield bytes until a boundary is found. Will yield the bytes
500     before the boundary, throw away the boundary bytes themselves, and push the
501     post-boundary bytes back on the stream.
502 
503     The future calls to next() after locating the boundary will raise a
504     StopIteration exception.
505     """
506 
507     def __init__(self, stream, boundary):
508         self._stream = stream
509         self._boundary = boundary
510         self._done = False
511         # rollback an additional six bytes because the format is like
512         # this: CRLF<boundary>[--CRLF]
513         self._rollback = len(boundary) + 6
514 
515         # Try to use mx fast string search if available. Otherwise
516         # use Python find. Wrap the latter for consistency.
517         unused_char = self._stream.read(1)
518         if not unused_char:
519             raise InputStreamExhausted()
520         self._stream.unget(unused_char)
521 
522     def __iter__(self):
523         return self
524 
525     def __next__(self):
526         if self._done:
527             raise StopIteration()
528 
529         stream = self._stream
530         rollback = self._rollback
531 
532         bytes_read = 0
533         chunks = []
534         for bytes in stream:
535             bytes_read += len(bytes)
536             chunks.append(bytes)
537             if bytes_read > rollback:
538                 break
539             if not bytes:
540                 break
541         else:
542             self._done = True
543 
544         if not chunks:
545             raise StopIteration()
546 
547         chunk = b''.join(chunks)
548         boundary = self._find_boundary(chunk)
549 
550         if boundary:
551             end, next = boundary
552             stream.unget(chunk[next:])
553             self._done = True
554             return chunk[:end]
555         else:
556             # make sure we don't treat a partial boundary (and
557             # its separators) as data
558             if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):
559                 # There's nothing left, we should just return and mark as done.
560                 self._done = True
561                 return chunk
562             else:
563                 stream.unget(chunk[-rollback:])
564                 return chunk[:-rollback]
565 
566     def _find_boundary(self, data):
567         """
568         Find a multipart boundary in data.
569 
570         Should no boundary exist in the data, return None. Otherwise, return
571         a tuple containing the indices of the following:
572          * the end of current encapsulation
573          * the start of the next encapsulation
574         """
575         index = data.find(self._boundary)
576         if index < 0:
577             return None
578         else:
579             end = index
580             next = index + len(self._boundary)
581             # backup over CRLF
582             last = max(0, end - 1)
583             if data[last:last + 1] == b'\n':
584                 end -= 1
585             last = max(0, end - 1)
586             if data[last:last + 1] == b'\r':
587                 end -= 1
588             return end, next
589 
590 
591 def exhaust(stream_or_iterable):
592     """Exhaust an iterator or stream."""
593     try:
594         iterator = iter(stream_or_iterable)
595     except TypeError:
596         iterator = ChunkIter(stream_or_iterable, 16384)
597     collections.deque(iterator, maxlen=0)  # consume iterator quickly.
598 
599 
600 def parse_boundary_stream(stream, max_header_size):
601     """
602     Parse one and exactly one stream that encapsulates a boundary.
603     """
604     # Stream at beginning of header, look for end of header
605     # and parse it if found. The header must fit within one
606     # chunk.
607     chunk = stream.read(max_header_size)
608 
609     # 'find' returns the top of these four bytes, so we'll
610     # need to munch them later to prevent them from polluting
611     # the payload.
612     header_end = chunk.find(b'\r\n\r\n')
613 
614     def _parse_header(line):
615         main_value_pair, params = parse_header(line)
616         try:
617             name, value = main_value_pair.split(':', 1)
618         except ValueError:
619             raise ValueError("Invalid header: %r" % line)
620         return name, (value, params)
621 
622     if header_end == -1:
623         # we find no header, so we just mark this fact and pass on
624         # the stream verbatim
625         stream.unget(chunk)
626         return (RAW, {}, stream)
627 
628     header = chunk[:header_end]
629 
630     # here we place any excess chunk back onto the stream, as
631     # well as throwing away the CRLFCRLF bytes from above.
632     stream.unget(chunk[header_end + 4:])
633 
634     TYPE = RAW
635     outdict = {}
636 
637     # Eliminate blank lines
638     for line in header.split(b'\r\n'):
639         # This terminology ("main value" and "dictionary of
640         # parameters") is from the Python docs.
641         try:
642             name, (value, params) = _parse_header(line)
643         except ValueError:
644             continue
645 
646         if name == 'content-disposition':
647             TYPE = FIELD
648             if params.get('filename'):
649                 TYPE = FILE
650 
651         outdict[name] = value, params
652 
653     if TYPE == RAW:
654         stream.unget(chunk)
655 
656     return (TYPE, outdict, stream)
657 
658 
659 class Parser:
660     def __init__(self, stream, boundary):
661         self._stream = stream
662         self._separator = b'--' + boundary
663 
664     def __iter__(self):
665         boundarystream = InterBoundaryIter(self._stream, self._separator)
666         for sub_stream in boundarystream:
667             # Iterate over each part
668             yield parse_boundary_stream(sub_stream, 1024)
669 
670 
671 def parse_header(line):
672     """
673     Parse the header into a key-value.
674 
675     Input (line): bytes, output: str for key/name, bytes for values which
676     will be decoded later.
677     """
678     plist = _parse_header_params(b';' + line)
679     key = plist.pop(0).lower().decode('ascii')
680     pdict = {}
681     for p in plist:
682         i = p.find(b'=')
683         if i >= 0:
684             has_encoding = False
685             name = p[:i].strip().lower().decode('ascii')
686             if name.endswith('*'):
687                 # Lang/encoding embedded in the value (like "filename*=UTF-8''file.ext")
688                 # http://tools.ietf.org/html/rfc2231#section-4
689                 name = name[:-1]
690                 if p.count(b"'") == 2:
691                     has_encoding = True
692             value = p[i + 1:].strip()
693             if len(value) >= 2 and value[:1] == value[-1:] == b'"':
694                 value = value[1:-1]
695                 value = value.replace(b'\\\\', b'\\').replace(b'\\"', b'"')
696             if has_encoding:
697                 encoding, lang, value = value.split(b"'")
698                 value = unquote(value.decode(), encoding=encoding.decode())
699             pdict[name] = value
700     return key, pdict
701 
702 
703 def _parse_header_params(s):
704     plist = []
705     while s[:1] == b';':
706         s = s[1:]
707         end = s.find(b';')
708         while end > 0 and s.count(b'"', 0, end) % 2:
709             end = s.find(b';', end + 1)
710         if end < 0:
711             end = len(s)
712         f = s[:end]
713         plist.append(f.strip())
714         s = s[end:]
715     return plist

</code><intended_behavior>Implement additional error handling for file storage-related issues during file uploads, to accurately report errors related to file storage or permissions.</intended_behavior>, <file>django/middleware/csrf.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Cross Site Request Forgery Middleware.
3 
4 This module provides a middleware that implements protection
5 against request forgeries from other sites.
6 """
7 import logging
8 import string
9 from collections import defaultdict
10 from urllib.parse import urlparse
11 
12 from django.conf import settings
13 from django.core.exceptions import DisallowedHost, ImproperlyConfigured
14 from django.http.request import HttpHeaders
15 from django.urls import get_callable
16 from django.utils.cache import patch_vary_headers
17 from django.utils.crypto import constant_time_compare, get_random_string
18 from django.utils.deprecation import MiddlewareMixin
19 from django.utils.functional import cached_property
20 from django.utils.http import is_same_domain
21 from django.utils.log import log_response
22 from django.utils.regex_helper import _lazy_re_compile
23 
24 logger = logging.getLogger('django.security.csrf')
25 # This matches if any character is not in CSRF_ALLOWED_CHARS.
26 invalid_token_chars_re = _lazy_re_compile('[^a-zA-Z0-9]')
27 
28 REASON_BAD_ORIGIN = "Origin checking failed - %s does not match any trusted origins."
29 REASON_NO_REFERER = "Referer checking failed - no Referer."
30 REASON_BAD_REFERER = "Referer checking failed - %s does not match any trusted origins."
31 REASON_NO_CSRF_COOKIE = "CSRF cookie not set."
32 REASON_CSRF_TOKEN_MISSING = 'CSRF token missing.'
33 REASON_MALFORMED_REFERER = "Referer checking failed - Referer is malformed."
34 REASON_INSECURE_REFERER = "Referer checking failed - Referer is insecure while host is secure."
35 # The reason strings below are for passing to InvalidTokenFormat. They are
36 # phrases without a subject because they can be in reference to either the CSRF
37 # cookie or non-cookie token.
38 REASON_INCORRECT_LENGTH = 'has incorrect length'
39 REASON_INVALID_CHARACTERS = 'has invalid characters'
40 
41 CSRF_SECRET_LENGTH = 32
42 CSRF_TOKEN_LENGTH = 2 * CSRF_SECRET_LENGTH
43 CSRF_ALLOWED_CHARS = string.ascii_letters + string.digits
44 CSRF_SESSION_KEY = '_csrftoken'
45 
46 
47 def _get_failure_view():
48     """Return the view to be used for CSRF rejections."""
49     return get_callable(settings.CSRF_FAILURE_VIEW)
50 
51 
52 def _get_new_csrf_string():
53     return get_random_string(CSRF_SECRET_LENGTH, allowed_chars=CSRF_ALLOWED_CHARS)
54 
55 
56 def _mask_cipher_secret(secret):
57     """
58     Given a secret (assumed to be a string of CSRF_ALLOWED_CHARS), generate a
59     token by adding a mask and applying it to the secret.
60     """
61     mask = _get_new_csrf_string()
62     chars = CSRF_ALLOWED_CHARS
63     pairs = zip((chars.index(x) for x in secret), (chars.index(x) for x in mask))
64     cipher = ''.join(chars[(x + y) % len(chars)] for x, y in pairs)
65     return mask + cipher
66 
67 
68 def _unmask_cipher_token(token):
69     """
70     Given a token (assumed to be a string of CSRF_ALLOWED_CHARS, of length
71     CSRF_TOKEN_LENGTH, and that its first half is a mask), use it to decrypt
72     the second half to produce the original secret.
73     """
74     mask = token[:CSRF_SECRET_LENGTH]
75     token = token[CSRF_SECRET_LENGTH:]
76     chars = CSRF_ALLOWED_CHARS
77     pairs = zip((chars.index(x) for x in token), (chars.index(x) for x in mask))
78     return ''.join(chars[x - y] for x, y in pairs)  # Note negative values are ok
79 
80 
81 def _get_new_csrf_token():
82     return _mask_cipher_secret(_get_new_csrf_string())
83 
84 
85 def get_token(request):
86     """
87     Return the CSRF token required for a POST form. The token is an
88     alphanumeric value. A new token is created if one is not already set.
89 
90     A side effect of calling this function is to make the csrf_protect
91     decorator and the CsrfViewMiddleware add a CSRF cookie and a 'Vary: Cookie'
92     header to the outgoing response.  For this reason, you may need to use this
93     function lazily, as is done by the csrf context processor.
94     """
95     if "CSRF_COOKIE" not in request.META:
96         csrf_secret = _get_new_csrf_string()
97         request.META["CSRF_COOKIE"] = _mask_cipher_secret(csrf_secret)
98     else:
99         csrf_secret = _unmask_cipher_token(request.META["CSRF_COOKIE"])
100     request.META["CSRF_COOKIE_USED"] = True
101     return _mask_cipher_secret(csrf_secret)
102 
103 
104 def rotate_token(request):
105     """
106     Change the CSRF token in use for a request - should be done on login
107     for security purposes.
108     """
109     request.META.update({
110         "CSRF_COOKIE_USED": True,
111         "CSRF_COOKIE": _get_new_csrf_token(),
112     })
113     request.csrf_cookie_needs_reset = True
114 
115 
116 class InvalidTokenFormat(Exception):
117     def __init__(self, reason):
118         self.reason = reason
119 
120 
121 def _sanitize_token(token):
122     if len(token) not in (CSRF_TOKEN_LENGTH, CSRF_SECRET_LENGTH):
123         raise InvalidTokenFormat(REASON_INCORRECT_LENGTH)
124     # Make sure all characters are in CSRF_ALLOWED_CHARS.
125     if invalid_token_chars_re.search(token):
126         raise InvalidTokenFormat(REASON_INVALID_CHARACTERS)
127     if len(token) == CSRF_SECRET_LENGTH:
128         # Older Django versions set cookies to values of CSRF_SECRET_LENGTH
129         # alphanumeric characters. For backwards compatibility, accept
130         # such values as unmasked secrets.
131         # It's easier to mask here and be consistent later, rather than add
132         # different code paths in the checks, although that might be a tad more
133         # efficient.
134         return _mask_cipher_secret(token)
135     return token
136 
137 
138 def _compare_masked_tokens(request_csrf_token, csrf_token):
139     # Assume both arguments are sanitized -- that is, strings of
140     # length CSRF_TOKEN_LENGTH, all CSRF_ALLOWED_CHARS.
141     return constant_time_compare(
142         _unmask_cipher_token(request_csrf_token),
143         _unmask_cipher_token(csrf_token),
144     )
145 
146 
147 class RejectRequest(Exception):
148     def __init__(self, reason):
149         self.reason = reason
150 
151 
152 class CsrfViewMiddleware(MiddlewareMixin):
153     """
154     Require a present and correct csrfmiddlewaretoken for POST requests that
155     have a CSRF cookie, and set an outgoing CSRF cookie.
156 
157     This middleware should be used in conjunction with the {% csrf_token %}
158     template tag.
159     """
160     @cached_property
161     def csrf_trusted_origins_hosts(self):
162         return [
163             urlparse(origin).netloc.lstrip('*')
164             for origin in settings.CSRF_TRUSTED_ORIGINS
165         ]
166 
167     @cached_property
168     def allowed_origins_exact(self):
169         return {
170             origin for origin in settings.CSRF_TRUSTED_ORIGINS
171             if '*' not in origin
172         }
173 
174     @cached_property
175     def allowed_origin_subdomains(self):
176         """
177         A mapping of allowed schemes to list of allowed netlocs, where all
178         subdomains of the netloc are allowed.
179         """
180         allowed_origin_subdomains = defaultdict(list)
181         for parsed in (urlparse(origin) for origin in settings.CSRF_TRUSTED_ORIGINS if '*' in origin):
182             allowed_origin_subdomains[parsed.scheme].append(parsed.netloc.lstrip('*'))
183         return allowed_origin_subdomains
184 
185     # The _accept and _reject methods currently only exist for the sake of the
186     # requires_csrf_token decorator.
187     def _accept(self, request):
188         # Avoid checking the request twice by adding a custom attribute to
189         # request.  This will be relevant when both decorator and middleware
190         # are used.
191         request.csrf_processing_done = True
192         return None
193 
194     def _reject(self, request, reason):
195         response = _get_failure_view()(request, reason=reason)
196         log_response(
197             'Forbidden (%s): %s', reason, request.path,
198             response=response,
199             request=request,
200             logger=logger,
201         )
202         return response
203 
204     def _get_token(self, request):
205         if settings.CSRF_USE_SESSIONS:
206             try:
207                 return request.session.get(CSRF_SESSION_KEY)
208             except AttributeError:
209                 raise ImproperlyConfigured(
210                     'CSRF_USE_SESSIONS is enabled, but request.session is not '
211                     'set. SessionMiddleware must appear before CsrfViewMiddleware '
212                     'in MIDDLEWARE.'
213                 )
214         else:
215             try:
216                 cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
217             except KeyError:
218                 return None
219 
220             # This can raise InvalidTokenFormat.
221             csrf_token = _sanitize_token(cookie_token)
222 
223             if csrf_token != cookie_token:
224                 # Then the cookie token had length CSRF_SECRET_LENGTH, so flag
225                 # to replace it with the masked version.
226                 request.csrf_cookie_needs_reset = True
227             return csrf_token
228 
229     def _set_token(self, request, response):
230         if settings.CSRF_USE_SESSIONS:
231             if request.session.get(CSRF_SESSION_KEY) != request.META['CSRF_COOKIE']:
232                 request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
233         else:
234             response.set_cookie(
235                 settings.CSRF_COOKIE_NAME,
236                 request.META['CSRF_COOKIE'],
237                 max_age=settings.CSRF_COOKIE_AGE,
238                 domain=settings.CSRF_COOKIE_DOMAIN,
239                 path=settings.CSRF_COOKIE_PATH,
240                 secure=settings.CSRF_COOKIE_SECURE,
241                 httponly=settings.CSRF_COOKIE_HTTPONLY,
242                 samesite=settings.CSRF_COOKIE_SAMESITE,
243             )
244             # Set the Vary header since content varies with the CSRF cookie.
245             patch_vary_headers(response, ('Cookie',))
246 
247     def _origin_verified(self, request):
248         request_origin = request.META['HTTP_ORIGIN']
249         try:
250             good_host = request.get_host()
251         except DisallowedHost:
252             pass
253         else:
254             good_origin = '%s://%s' % (
255                 'https' if request.is_secure() else 'http',
256                 good_host,
257             )
258             if request_origin == good_origin:
259                 return True
260         if request_origin in self.allowed_origins_exact:
261             return True
262         try:
263             parsed_origin = urlparse(request_origin)
264         except ValueError:
265             return False
266         request_scheme = parsed_origin.scheme
267         request_netloc = parsed_origin.netloc
268         return any(
269             is_same_domain(request_netloc, host)
270             for host in self.allowed_origin_subdomains.get(request_scheme, ())
271         )
272 
273     def _check_referer(self, request):
274         referer = request.META.get('HTTP_REFERER')
275         if referer is None:
276             raise RejectRequest(REASON_NO_REFERER)
277 
278         try:
279             referer = urlparse(referer)
280         except ValueError:
281             raise RejectRequest(REASON_MALFORMED_REFERER)
282 
283         # Make sure we have a valid URL for Referer.
284         if '' in (referer.scheme, referer.netloc):
285             raise RejectRequest(REASON_MALFORMED_REFERER)
286 
287         # Ensure that our Referer is also secure.
288         if referer.scheme != 'https':
289             raise RejectRequest(REASON_INSECURE_REFERER)
290 
291         if any(
292             is_same_domain(referer.netloc, host)
293             for host in self.csrf_trusted_origins_hosts
294         ):
295             return
296         # Allow matching the configured cookie domain.
297         good_referer = (
298             settings.SESSION_COOKIE_DOMAIN
299             if settings.CSRF_USE_SESSIONS
300             else settings.CSRF_COOKIE_DOMAIN
301         )
302         if good_referer is None:
303             # If no cookie domain is configured, allow matching the current
304             # host:port exactly if it's permitted by ALLOWED_HOSTS.
305             try:
306                 # request.get_host() includes the port.
307                 good_referer = request.get_host()
308             except DisallowedHost:
309                 raise RejectRequest(REASON_BAD_REFERER % referer.geturl())
310         else:
311             server_port = request.get_port()
312             if server_port not in ('443', '80'):
313                 good_referer = '%s:%s' % (good_referer, server_port)
314 
315         if not is_same_domain(referer.netloc, good_referer):
316             raise RejectRequest(REASON_BAD_REFERER % referer.geturl())
317 
318     def _bad_token_message(self, reason, token_source):
319         if token_source != 'POST':
320             # Assume it is a settings.CSRF_HEADER_NAME value.
321             header_name = HttpHeaders.parse_header_name(token_source)
322             token_source = f'the {header_name!r} HTTP header'
323         return f'CSRF token from {token_source} {reason}.'
324 
325     def _check_token(self, request):
326         # Access csrf_token via self._get_token() as rotate_token() may have
327         # been called by an authentication middleware during the
328         # process_request() phase.
329         try:
330             csrf_token = self._get_token(request)
331         except InvalidTokenFormat as exc:
332             raise RejectRequest(f'CSRF cookie {exc.reason}.')
333 
334         if csrf_token is None:
335             # No CSRF cookie. For POST requests, we insist on a CSRF cookie,
336             # and in this way we can avoid all CSRF attacks, including login
337             # CSRF.
338             raise RejectRequest(REASON_NO_CSRF_COOKIE)
339 
340         # Check non-cookie token for match.
341         request_csrf_token = ''
342         if request.method == 'POST':
343             try:
344                 request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
345             except OSError:
346                 # Handle a broken connection before we've completed reading the
347                 # POST data. process_view shouldn't raise any exceptions, so
348                 # we'll ignore and serve the user a 403 (assuming they're still
349                 # listening, which they probably aren't because of the error).
350                 pass
351 
352         if request_csrf_token == '':
353             # Fall back to X-CSRFToken, to make things easier for AJAX, and
354             # possible for PUT/DELETE.
355             try:
356                 request_csrf_token = request.META[settings.CSRF_HEADER_NAME]
357             except KeyError:
358                 raise RejectRequest(REASON_CSRF_TOKEN_MISSING)
359             token_source = settings.CSRF_HEADER_NAME
360         else:
361             token_source = 'POST'
362 
363         try:
364             request_csrf_token = _sanitize_token(request_csrf_token)
365         except InvalidTokenFormat as exc:
366             reason = self._bad_token_message(exc.reason, token_source)
367             raise RejectRequest(reason)
368 
369         if not _compare_masked_tokens(request_csrf_token, csrf_token):
370             reason = self._bad_token_message('incorrect', token_source)
371             raise RejectRequest(reason)
372 
373     def process_request(self, request):
374         try:
375             csrf_token = self._get_token(request)
376         except InvalidTokenFormat:
377             csrf_token = _get_new_csrf_token()
378             request.csrf_cookie_needs_reset = True
379 
380         if csrf_token is not None:
381             # Use same token next time.
382             request.META['CSRF_COOKIE'] = csrf_token
383 
384     def process_view(self, request, callback, callback_args, callback_kwargs):
385         if getattr(request, 'csrf_processing_done', False):
386             return None
387 
388         # Wait until request.META["CSRF_COOKIE"] has been manipulated before
389         # bailing out, so that get_token still works
390         if getattr(callback, 'csrf_exempt', False):
391             return None
392 
393         # Assume that anything not defined as 'safe' by RFC7231 needs protection
394         if request.method in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
395             return self._accept(request)
396 
397         if getattr(request, '_dont_enforce_csrf_checks', False):
398             # Mechanism to turn off CSRF checks for test suite. It comes after
399             # the creation of CSRF cookies, so that everything else continues
400             # to work exactly the same (e.g. cookies are sent, etc.), but
401             # before any branches that call the _reject method.
402             return self._accept(request)
403 
404         # Reject the request if the Origin header doesn't match an allowed
405         # value.
406         if 'HTTP_ORIGIN' in request.META:
407             if not self._origin_verified(request):
408                 return self._reject(request, REASON_BAD_ORIGIN % request.META['HTTP_ORIGIN'])
409         elif request.is_secure():
410             # If the Origin header wasn't provided, reject HTTPS requests if
411             # the Referer header doesn't match an allowed value.
412             #
413             # Suppose user visits http://example.com/
414             # An active network attacker (man-in-the-middle, MITM) sends a
415             # POST form that targets https://example.com/detonate-bomb/ and
416             # submits it via JavaScript.
417             #
418             # The attacker will need to provide a CSRF cookie and token, but
419             # that's no problem for a MITM and the session-independent secret
420             # we're using. So the MITM can circumvent the CSRF protection. This
421             # is true for any HTTP connection, but anyone using HTTPS expects
422             # better! For this reason, for https://example.com/ we need
423             # additional protection that treats http://example.com/ as
424             # completely untrusted. Under HTTPS, Barth et al. found that the
425             # Referer header is missing for same-domain requests in only about
426             # 0.2% of cases or less, so we can use strict Referer checking.
427             try:
428                 self._check_referer(request)
429             except RejectRequest as exc:
430                 return self._reject(request, exc.reason)
431 
432         try:
433             self._check_token(request)
434         except RejectRequest as exc:
435             return self._reject(request, exc.reason)
436 
437         return self._accept(request)
438 
439     def process_response(self, request, response):
440         # Send the CSRF cookie whenever the cookie is being used (even if the
441         # client already has it) in order to renew the expiry timer, but only
442         # if it hasn't already been sent during this request-response cycle.
443         # Also, send the cookie no matter what if a reset was requested.
444         if (
445             getattr(request, 'csrf_cookie_needs_reset', False) or (
446                 request.META.get('CSRF_COOKIE_USED') and
447                 not getattr(response, 'csrf_cookie_set', False)
448             )
449         ):
450             self._set_token(request, response)
451             # Update state to prevent _set_token() from being unnecessarily
452             # called again in process_response() by other instances of
453             # CsrfViewMiddleware. This can happen e.g. when both a decorator
454             # and middleware are used. However, the csrf_cookie_needs_reset
455             # attribute is still respected in subsequent calls e.g. in case
456             # rotate_token() is called in process_response() later by custom
457             # middleware but before those subsequent calls.
458             response.csrf_cookie_set = True
459             request.csrf_cookie_needs_reset = False
460 
461         return response

</code><intended_behavior>Introduce logic to differentiate between CSRF token issues and other exceptions, particularly those caused by file storage problems, to improve error classification and reporting.</intended_behavior>]
2025-10-15 06:02:26.018 | INFO     | Search completed. Bug locations: [<file>django/core/files/uploadhandler.py</file>
<class>TemporaryFileUploadHandler</class>
<method>None</method>
<code>
138 class TemporaryFileUploadHandler(FileUploadHandler):
139     """
140     Upload handler that streams data into a temporary file.
141     """
142     def new_file(self, *args, **kwargs):
143         """
144         Create the file object to append to as data is coming in.
145         """
146         super().new_file(*args, **kwargs)
147         self.file = TemporaryUploadedFile(self.file_name, self.content_type, 0, self.charset, self.content_type_extra)
148 
149     def receive_data_chunk(self, raw_data, start):
150         self.file.write(raw_data)
151 
152     def file_complete(self, file_size):
153         self.file.seek(0)
154         self.file.size = file_size
155         return self.file
156 
157     def upload_interrupted(self):
158         if hasattr(self, 'file'):
159             temp_location = self.file.temporary_file_path()
160             try:
161                 self.file.close()
162                 os.remove(temp_location)
163             except FileNotFoundError:
164                 pass

</code><intended_behavior>Ensure that exceptions raised due to file storage issues are correctly identified and reported, instead of being misinterpreted as CSRF errors.</intended_behavior>, <file>django/http/multipartparser.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Multi-part parsing for file uploads.
3 
4 Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to
5 file upload handlers for processing.
6 """
7 import base64
8 import binascii
9 import cgi
10 import collections
11 import html
12 from urllib.parse import unquote
13 
14 from django.conf import settings
15 from django.core.exceptions import (
16     RequestDataTooBig, SuspiciousMultipartForm, TooManyFieldsSent,
17 )
18 from django.core.files.uploadhandler import (
19     SkipFile, StopFutureHandlers, StopUpload,
20 )
21 from django.utils.datastructures import MultiValueDict
22 from django.utils.encoding import force_str
23 
24 __all__ = ('MultiPartParser', 'MultiPartParserError', 'InputStreamExhausted')
25 
26 
27 class MultiPartParserError(Exception):
28     pass
29 
30 
31 class InputStreamExhausted(Exception):
32     """
33     No more reads are allowed from this device.
34     """
35     pass
36 
37 
38 RAW = "raw"
39 FILE = "file"
40 FIELD = "field"
41 
42 
43 class MultiPartParser:
44     """
45     A rfc2388 multipart/form-data parser.
46 
47     ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks
48     and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.
49     """
50     def __init__(self, META, input_data, upload_handlers, encoding=None):
51         """
52         Initialize the MultiPartParser object.
53 
54         :META:
55             The standard ``META`` dictionary in Django request objects.
56         :input_data:
57             The raw post data, as a file-like object.
58         :upload_handlers:
59             A list of UploadHandler instances that perform operations on the
60             uploaded data.
61         :encoding:
62             The encoding with which to treat the incoming data.
63         """
64         # Content-Type should contain multipart and the boundary information.
65         content_type = META.get('CONTENT_TYPE', '')
66         if not content_type.startswith('multipart/'):
67             raise MultiPartParserError('Invalid Content-Type: %s' % content_type)
68 
69         # Parse the header to get the boundary to split the parts.
70         try:
71             ctypes, opts = parse_header(content_type.encode('ascii'))
72         except UnicodeEncodeError:
73             raise MultiPartParserError('Invalid non-ASCII Content-Type in multipart: %s' % force_str(content_type))
74         boundary = opts.get('boundary')
75         if not boundary or not cgi.valid_boundary(boundary):
76             raise MultiPartParserError('Invalid boundary in multipart: %s' % force_str(boundary))
77 
78         # Content-Length should contain the length of the body we are about
79         # to receive.
80         try:
81             content_length = int(META.get('CONTENT_LENGTH', 0))
82         except (ValueError, TypeError):
83             content_length = 0
84 
85         if content_length < 0:
86             # This means we shouldn't continue...raise an error.
87             raise MultiPartParserError("Invalid content length: %r" % content_length)
88 
89         if isinstance(boundary, str):
90             boundary = boundary.encode('ascii')
91         self._boundary = boundary
92         self._input_data = input_data
93 
94         # For compatibility with low-level network APIs (with 32-bit integers),
95         # the chunk size should be < 2^31, but still divisible by 4.
96         possible_sizes = [x.chunk_size for x in upload_handlers if x.chunk_size]
97         self._chunk_size = min([2 ** 31 - 4] + possible_sizes)
98 
99         self._meta = META
100         self._encoding = encoding or settings.DEFAULT_CHARSET
101         self._content_length = content_length
102         self._upload_handlers = upload_handlers
103 
104     def parse(self):
105         """
106         Parse the POST data and break it into a FILES MultiValueDict and a POST
107         MultiValueDict.
108 
109         Return a tuple containing the POST and FILES dictionary, respectively.
110         """
111         from django.http import QueryDict
112 
113         encoding = self._encoding
114         handlers = self._upload_handlers
115 
116         # HTTP spec says that Content-Length >= 0 is valid
117         # handling content-length == 0 before continuing
118         if self._content_length == 0:
119             return QueryDict(encoding=self._encoding), MultiValueDict()
120 
121         # See if any of the handlers take care of the parsing.
122         # This allows overriding everything if need be.
123         for handler in handlers:
124             result = handler.handle_raw_input(
125                 self._input_data,
126                 self._meta,
127                 self._content_length,
128                 self._boundary,
129                 encoding,
130             )
131             # Check to see if it was handled
132             if result is not None:
133                 return result[0], result[1]
134 
135         # Create the data structures to be used later.
136         self._post = QueryDict(mutable=True)
137         self._files = MultiValueDict()
138 
139         # Instantiate the parser and stream:
140         stream = LazyStream(ChunkIter(self._input_data, self._chunk_size))
141 
142         # Whether or not to signal a file-completion at the beginning of the loop.
143         old_field_name = None
144         counters = [0] * len(handlers)
145 
146         # Number of bytes that have been read.
147         num_bytes_read = 0
148         # To count the number of keys in the request.
149         num_post_keys = 0
150         # To limit the amount of data read from the request.
151         read_size = None
152         # Whether a file upload is finished.
153         uploaded_file = True
154 
155         try:
156             for item_type, meta_data, field_stream in Parser(stream, self._boundary):
157                 if old_field_name:
158                     # We run this at the beginning of the next loop
159                     # since we cannot be sure a file is complete until
160                     # we hit the next boundary/part of the multipart content.
161                     self.handle_file_complete(old_field_name, counters)
162                     old_field_name = None
163                     uploaded_file = True
164 
165                 try:
166                     disposition = meta_data['content-disposition'][1]
167                     field_name = disposition['name'].strip()
168                 except (KeyError, IndexError, AttributeError):
169                     continue
170 
171                 transfer_encoding = meta_data.get('content-transfer-encoding')
172                 if transfer_encoding is not None:
173                     transfer_encoding = transfer_encoding[0].strip()
174                 field_name = force_str(field_name, encoding, errors='replace')
175 
176                 if item_type == FIELD:
177                     # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
178                     num_post_keys += 1
179                     if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and
180                             settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):
181                         raise TooManyFieldsSent(
182                             'The number of GET/POST parameters exceeded '
183                             'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
184                         )
185 
186                     # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
187                     if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:
188                         read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read
189 
190                     # This is a post field, we can just set it in the post
191                     if transfer_encoding == 'base64':
192                         raw_data = field_stream.read(size=read_size)
193                         num_bytes_read += len(raw_data)
194                         try:
195                             data = base64.b64decode(raw_data)
196                         except binascii.Error:
197                             data = raw_data
198                     else:
199                         data = field_stream.read(size=read_size)
200                         num_bytes_read += len(data)
201 
202                     # Add two here to make the check consistent with the
203                     # x-www-form-urlencoded check that includes '&='.
204                     num_bytes_read += len(field_name) + 2
205                     if (settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None and
206                             num_bytes_read > settings.DATA_UPLOAD_MAX_MEMORY_SIZE):
207                         raise RequestDataTooBig('Request body exceeded settings.DATA_UPLOAD_MAX_MEMORY_SIZE.')
208 
209                     self._post.appendlist(field_name, force_str(data, encoding, errors='replace'))
210                 elif item_type == FILE:
211                     # This is a file, use the handler...
212                     file_name = disposition.get('filename')
213                     if file_name:
214                         file_name = force_str(file_name, encoding, errors='replace')
215                         file_name = self.sanitize_file_name(file_name)
216                     if not file_name:
217                         continue
218 
219                     content_type, content_type_extra = meta_data.get('content-type', ('', {}))
220                     content_type = content_type.strip()
221                     charset = content_type_extra.get('charset')
222 
223                     try:
224                         content_length = int(meta_data.get('content-length')[0])
225                     except (IndexError, TypeError, ValueError):
226                         content_length = None
227 
228                     counters = [0] * len(handlers)
229                     uploaded_file = False
230                     try:
231                         for handler in handlers:
232                             try:
233                                 handler.new_file(
234                                     field_name, file_name, content_type,
235                                     content_length, charset, content_type_extra,
236                                 )
237                             except StopFutureHandlers:
238                                 break
239 
240                         for chunk in field_stream:
241                             if transfer_encoding == 'base64':
242                                 # We only special-case base64 transfer encoding
243                                 # We should always decode base64 chunks by multiple of 4,
244                                 # ignoring whitespace.
245 
246                                 stripped_chunk = b"".join(chunk.split())
247 
248                                 remaining = len(stripped_chunk) % 4
249                                 while remaining != 0:
250                                     over_chunk = field_stream.read(4 - remaining)
251                                     stripped_chunk += b"".join(over_chunk.split())
252                                     remaining = len(stripped_chunk) % 4
253 
254                                 try:
255                                     chunk = base64.b64decode(stripped_chunk)
256                                 except Exception as exc:
257                                     # Since this is only a chunk, any error is an unfixable error.
258                                     raise MultiPartParserError("Could not decode base64 data.") from exc
259 
260                             for i, handler in enumerate(handlers):
261                                 chunk_length = len(chunk)
262                                 chunk = handler.receive_data_chunk(chunk, counters[i])
263                                 counters[i] += chunk_length
264                                 if chunk is None:
265                                     # Don't continue if the chunk received by
266                                     # the handler is None.
267                                     break
268 
269                     except SkipFile:
270                         self._close_files()
271                         # Just use up the rest of this file...
272                         exhaust(field_stream)
273                     else:
274                         # Handle file upload completions on next iteration.
275                         old_field_name = field_name
276                 else:
277                     # If this is neither a FIELD or a FILE, just exhaust the stream.
278                     exhaust(stream)
279         except StopUpload as e:
280             self._close_files()
281             if not e.connection_reset:
282                 exhaust(self._input_data)
283         else:
284             if not uploaded_file:
285                 for handler in handlers:
286                     handler.upload_interrupted()
287             # Make sure that the request data is all fed
288             exhaust(self._input_data)
289 
290         # Signal that the upload has completed.
291         # any() shortcircuits if a handler's upload_complete() returns a value.
292         any(handler.upload_complete() for handler in handlers)
293         self._post._mutable = False
294         return self._post, self._files
295 
296     def handle_file_complete(self, old_field_name, counters):
297         """
298         Handle all the signaling that takes place when a file is complete.
299         """
300         for i, handler in enumerate(self._upload_handlers):
301             file_obj = handler.file_complete(counters[i])
302             if file_obj:
303                 # If it returns a file object, then set the files dict.
304                 self._files.appendlist(force_str(old_field_name, self._encoding, errors='replace'), file_obj)
305                 break
306 
307     def sanitize_file_name(self, file_name):
308         """
309         Sanitize the filename of an upload.
310 
311         Remove all possible path separators, even though that might remove more
312         than actually required by the target system. Filenames that could
313         potentially cause problems (current/parent dir) are also discarded.
314 
315         It should be noted that this function could still return a "filepath"
316         like "C:some_file.txt" which is handled later on by the storage layer.
317         So while this function does sanitize filenames to some extent, the
318         resulting filename should still be considered as untrusted user input.
319         """
320         file_name = html.unescape(file_name)
321         file_name = file_name.rsplit('/')[-1]
322         file_name = file_name.rsplit('\\')[-1]
323 
324         if file_name in {'', '.', '..'}:
325             return None
326         return file_name
327 
328     IE_sanitize = sanitize_file_name
329 
330     def _close_files(self):
331         # Free up all file handles.
332         # FIXME: this currently assumes that upload handlers store the file as 'file'
333         # We should document that... (Maybe add handler.free_file to complement new_file)
334         for handler in self._upload_handlers:
335             if hasattr(handler, 'file'):
336                 handler.file.close()
337 
338 
339 class LazyStream:
340     """
341     The LazyStream wrapper allows one to get and "unget" bytes from a stream.
342 
343     Given a producer object (an iterator that yields bytestrings), the
344     LazyStream object will support iteration, reading, and keeping a "look-back"
345     variable in case you need to "unget" some bytes.
346     """
347     def __init__(self, producer, length=None):
348         """
349         Every LazyStream must have a producer when instantiated.
350 
351         A producer is an iterable that returns a string each time it
352         is called.
353         """
354         self._producer = producer
355         self._empty = False
356         self._leftover = b''
357         self.length = length
358         self.position = 0
359         self._remaining = length
360         self._unget_history = []
361 
362     def tell(self):
363         return self.position
364 
365     def read(self, size=None):
366         def parts():
367             remaining = self._remaining if size is None else size
368             # do the whole thing in one shot if no limit was provided.
369             if remaining is None:
370                 yield b''.join(self)
371                 return
372 
373             # otherwise do some bookkeeping to return exactly enough
374             # of the stream and stashing any extra content we get from
375             # the producer
376             while remaining != 0:
377                 assert remaining > 0, 'remaining bytes to read should never go negative'
378 
379                 try:
380                     chunk = next(self)
381                 except StopIteration:
382                     return
383                 else:
384                     emitting = chunk[:remaining]
385                     self.unget(chunk[remaining:])
386                     remaining -= len(emitting)
387                     yield emitting
388 
389         return b''.join(parts())
390 
391     def __next__(self):
392         """
393         Used when the exact number of bytes to read is unimportant.
394 
395         Return whatever chunk is conveniently returned from the iterator.
396         Useful to avoid unnecessary bookkeeping if performance is an issue.
397         """
398         if self._leftover:
399             output = self._leftover
400             self._leftover = b''
401         else:
402             output = next(self._producer)
403             self._unget_history = []
404         self.position += len(output)
405         return output
406 
407     def close(self):
408         """
409         Used to invalidate/disable this lazy stream.
410 
411         Replace the producer with an empty list. Any leftover bytes that have
412         already been read will still be reported upon read() and/or next().
413         """
414         self._producer = []
415 
416     def __iter__(self):
417         return self
418 
419     def unget(self, bytes):
420         """
421         Place bytes back onto the front of the lazy stream.
422 
423         Future calls to read() will return those bytes first. The
424         stream position and thus tell() will be rewound.
425         """
426         if not bytes:
427             return
428         self._update_unget_history(len(bytes))
429         self.position -= len(bytes)
430         self._leftover = bytes + self._leftover
431 
432     def _update_unget_history(self, num_bytes):
433         """
434         Update the unget history as a sanity check to see if we've pushed
435         back the same number of bytes in one chunk. If we keep ungetting the
436         same number of bytes many times (here, 50), we're mostly likely in an
437         infinite loop of some sort. This is usually caused by a
438         maliciously-malformed MIME request.
439         """
440         self._unget_history = [num_bytes] + self._unget_history[:49]
441         number_equal = len([
442             current_number for current_number in self._unget_history
443             if current_number == num_bytes
444         ])
445 
446         if number_equal > 40:
447             raise SuspiciousMultipartForm(
448                 "The multipart parser got stuck, which shouldn't happen with"
449                 " normal uploaded files. Check for malicious upload activity;"
450                 " if there is none, report this to the Django developers."
451             )
452 
453 
454 class ChunkIter:
455     """
456     An iterable that will yield chunks of data. Given a file-like object as the
457     constructor, yield chunks of read operations from that object.
458     """
459     def __init__(self, flo, chunk_size=64 * 1024):
460         self.flo = flo
461         self.chunk_size = chunk_size
462 
463     def __next__(self):
464         try:
465             data = self.flo.read(self.chunk_size)
466         except InputStreamExhausted:
467             raise StopIteration()
468         if data:
469             return data
470         else:
471             raise StopIteration()
472 
473     def __iter__(self):
474         return self
475 
476 
477 class InterBoundaryIter:
478     """
479     A Producer that will iterate over boundaries.
480     """
481     def __init__(self, stream, boundary):
482         self._stream = stream
483         self._boundary = boundary
484 
485     def __iter__(self):
486         return self
487 
488     def __next__(self):
489         try:
490             return LazyStream(BoundaryIter(self._stream, self._boundary))
491         except InputStreamExhausted:
492             raise StopIteration()
493 
494 
495 class BoundaryIter:
496     """
497     A Producer that is sensitive to boundaries.
498 
499     Will happily yield bytes until a boundary is found. Will yield the bytes
500     before the boundary, throw away the boundary bytes themselves, and push the
501     post-boundary bytes back on the stream.
502 
503     The future calls to next() after locating the boundary will raise a
504     StopIteration exception.
505     """
506 
507     def __init__(self, stream, boundary):
508         self._stream = stream
509         self._boundary = boundary
510         self._done = False
511         # rollback an additional six bytes because the format is like
512         # this: CRLF<boundary>[--CRLF]
513         self._rollback = len(boundary) + 6
514 
515         # Try to use mx fast string search if available. Otherwise
516         # use Python find. Wrap the latter for consistency.
517         unused_char = self._stream.read(1)
518         if not unused_char:
519             raise InputStreamExhausted()
520         self._stream.unget(unused_char)
521 
522     def __iter__(self):
523         return self
524 
525     def __next__(self):
526         if self._done:
527             raise StopIteration()
528 
529         stream = self._stream
530         rollback = self._rollback
531 
532         bytes_read = 0
533         chunks = []
534         for bytes in stream:
535             bytes_read += len(bytes)
536             chunks.append(bytes)
537             if bytes_read > rollback:
538                 break
539             if not bytes:
540                 break
541         else:
542             self._done = True
543 
544         if not chunks:
545             raise StopIteration()
546 
547         chunk = b''.join(chunks)
548         boundary = self._find_boundary(chunk)
549 
550         if boundary:
551             end, next = boundary
552             stream.unget(chunk[next:])
553             self._done = True
554             return chunk[:end]
555         else:
556             # make sure we don't treat a partial boundary (and
557             # its separators) as data
558             if not chunk[:-rollback]:  # and len(chunk) >= (len(self._boundary) + 6):
559                 # There's nothing left, we should just return and mark as done.
560                 self._done = True
561                 return chunk
562             else:
563                 stream.unget(chunk[-rollback:])
564                 return chunk[:-rollback]
565 
566     def _find_boundary(self, data):
567         """
568         Find a multipart boundary in data.
569 
570         Should no boundary exist in the data, return None. Otherwise, return
571         a tuple containing the indices of the following:
572          * the end of current encapsulation
573          * the start of the next encapsulation
574         """
575         index = data.find(self._boundary)
576         if index < 0:
577             return None
578         else:
579             end = index
580             next = index + len(self._boundary)
581             # backup over CRLF
582             last = max(0, end - 1)
583             if data[last:last + 1] == b'\n':
584                 end -= 1
585             last = max(0, end - 1)
586             if data[last:last + 1] == b'\r':
587                 end -= 1
588             return end, next
589 
590 
591 def exhaust(stream_or_iterable):
592     """Exhaust an iterator or stream."""
593     try:
594         iterator = iter(stream_or_iterable)
595     except TypeError:
596         iterator = ChunkIter(stream_or_iterable, 16384)
597     collections.deque(iterator, maxlen=0)  # consume iterator quickly.
598 
599 
600 def parse_boundary_stream(stream, max_header_size):
601     """
602     Parse one and exactly one stream that encapsulates a boundary.
603     """
604     # Stream at beginning of header, look for end of header
605     # and parse it if found. The header must fit within one
606     # chunk.
607     chunk = stream.read(max_header_size)
608 
609     # 'find' returns the top of these four bytes, so we'll
610     # need to munch them later to prevent them from polluting
611     # the payload.
612     header_end = chunk.find(b'\r\n\r\n')
613 
614     def _parse_header(line):
615         main_value_pair, params = parse_header(line)
616         try:
617             name, value = main_value_pair.split(':', 1)
618         except ValueError:
619             raise ValueError("Invalid header: %r" % line)
620         return name, (value, params)
621 
622     if header_end == -1:
623         # we find no header, so we just mark this fact and pass on
624         # the stream verbatim
625         stream.unget(chunk)
626         return (RAW, {}, stream)
627 
628     header = chunk[:header_end]
629 
630     # here we place any excess chunk back onto the stream, as
631     # well as throwing away the CRLFCRLF bytes from above.
632     stream.unget(chunk[header_end + 4:])
633 
634     TYPE = RAW
635     outdict = {}
636 
637     # Eliminate blank lines
638     for line in header.split(b'\r\n'):
639         # This terminology ("main value" and "dictionary of
640         # parameters") is from the Python docs.
641         try:
642             name, (value, params) = _parse_header(line)
643         except ValueError:
644             continue
645 
646         if name == 'content-disposition':
647             TYPE = FIELD
648             if params.get('filename'):
649                 TYPE = FILE
650 
651         outdict[name] = value, params
652 
653     if TYPE == RAW:
654         stream.unget(chunk)
655 
656     return (TYPE, outdict, stream)
657 
658 
659 class Parser:
660     def __init__(self, stream, boundary):
661         self._stream = stream
662         self._separator = b'--' + boundary
663 
664     def __iter__(self):
665         boundarystream = InterBoundaryIter(self._stream, self._separator)
666         for sub_stream in boundarystream:
667             # Iterate over each part
668             yield parse_boundary_stream(sub_stream, 1024)
669 
670 
671 def parse_header(line):
672     """
673     Parse the header into a key-value.
674 
675     Input (line): bytes, output: str for key/name, bytes for values which
676     will be decoded later.
677     """
678     plist = _parse_header_params(b';' + line)
679     key = plist.pop(0).lower().decode('ascii')
680     pdict = {}
681     for p in plist:
682         i = p.find(b'=')
683         if i >= 0:
684             has_encoding = False
685             name = p[:i].strip().lower().decode('ascii')
686             if name.endswith('*'):
687                 # Lang/encoding embedded in the value (like "filename*=UTF-8''file.ext")
688                 # http://tools.ietf.org/html/rfc2231#section-4
689                 name = name[:-1]
690                 if p.count(b"'") == 2:
691                     has_encoding = True
692             value = p[i + 1:].strip()
693             if len(value) >= 2 and value[:1] == value[-1:] == b'"':
694                 value = value[1:-1]
695                 value = value.replace(b'\\\\', b'\\').replace(b'\\"', b'"')
696             if has_encoding:
697                 encoding, lang, value = value.split(b"'")
698                 value = unquote(value.decode(), encoding=encoding.decode())
699             pdict[name] = value
700     return key, pdict
701 
702 
703 def _parse_header_params(s):
704     plist = []
705     while s[:1] == b';':
706         s = s[1:]
707         end = s.find(b';')
708         while end > 0 and s.count(b'"', 0, end) % 2:
709             end = s.find(b';', end + 1)
710         if end < 0:
711             end = len(s)
712         f = s[:end]
713         plist.append(f.strip())
714         s = s[end:]
715     return plist

</code><intended_behavior>Implement additional error handling for file storage-related issues during file uploads, to accurately report errors related to file storage or permissions.</intended_behavior>, <file>django/middleware/csrf.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Cross Site Request Forgery Middleware.
3 
4 This module provides a middleware that implements protection
5 against request forgeries from other sites.
6 """
7 import logging
8 import string
9 from collections import defaultdict
10 from urllib.parse import urlparse
11 
12 from django.conf import settings
13 from django.core.exceptions import DisallowedHost, ImproperlyConfigured
14 from django.http.request import HttpHeaders
15 from django.urls import get_callable
16 from django.utils.cache import patch_vary_headers
17 from django.utils.crypto import constant_time_compare, get_random_string
18 from django.utils.deprecation import MiddlewareMixin
19 from django.utils.functional import cached_property
20 from django.utils.http import is_same_domain
21 from django.utils.log import log_response
22 from django.utils.regex_helper import _lazy_re_compile
23 
24 logger = logging.getLogger('django.security.csrf')
25 # This matches if any character is not in CSRF_ALLOWED_CHARS.
26 invalid_token_chars_re = _lazy_re_compile('[^a-zA-Z0-9]')
27 
28 REASON_BAD_ORIGIN = "Origin checking failed - %s does not match any trusted origins."
29 REASON_NO_REFERER = "Referer checking failed - no Referer."
30 REASON_BAD_REFERER = "Referer checking failed - %s does not match any trusted origins."
31 REASON_NO_CSRF_COOKIE = "CSRF cookie not set."
32 REASON_CSRF_TOKEN_MISSING = 'CSRF token missing.'
33 REASON_MALFORMED_REFERER = "Referer checking failed - Referer is malformed."
34 REASON_INSECURE_REFERER = "Referer checking failed - Referer is insecure while host is secure."
35 # The reason strings below are for passing to InvalidTokenFormat. They are
36 # phrases without a subject because they can be in reference to either the CSRF
37 # cookie or non-cookie token.
38 REASON_INCORRECT_LENGTH = 'has incorrect length'
39 REASON_INVALID_CHARACTERS = 'has invalid characters'
40 
41 CSRF_SECRET_LENGTH = 32
42 CSRF_TOKEN_LENGTH = 2 * CSRF_SECRET_LENGTH
43 CSRF_ALLOWED_CHARS = string.ascii_letters + string.digits
44 CSRF_SESSION_KEY = '_csrftoken'
45 
46 
47 def _get_failure_view():
48     """Return the view to be used for CSRF rejections."""
49     return get_callable(settings.CSRF_FAILURE_VIEW)
50 
51 
52 def _get_new_csrf_string():
53     return get_random_string(CSRF_SECRET_LENGTH, allowed_chars=CSRF_ALLOWED_CHARS)
54 
55 
56 def _mask_cipher_secret(secret):
57     """
58     Given a secret (assumed to be a string of CSRF_ALLOWED_CHARS), generate a
59     token by adding a mask and applying it to the secret.
60     """
61     mask = _get_new_csrf_string()
62     chars = CSRF_ALLOWED_CHARS
63     pairs = zip((chars.index(x) for x in secret), (chars.index(x) for x in mask))
64     cipher = ''.join(chars[(x + y) % len(chars)] for x, y in pairs)
65     return mask + cipher
66 
67 
68 def _unmask_cipher_token(token):
69     """
70     Given a token (assumed to be a string of CSRF_ALLOWED_CHARS, of length
71     CSRF_TOKEN_LENGTH, and that its first half is a mask), use it to decrypt
72     the second half to produce the original secret.
73     """
74     mask = token[:CSRF_SECRET_LENGTH]
75     token = token[CSRF_SECRET_LENGTH:]
76     chars = CSRF_ALLOWED_CHARS
77     pairs = zip((chars.index(x) for x in token), (chars.index(x) for x in mask))
78     return ''.join(chars[x - y] for x, y in pairs)  # Note negative values are ok
79 
80 
81 def _get_new_csrf_token():
82     return _mask_cipher_secret(_get_new_csrf_string())
83 
84 
85 def get_token(request):
86     """
87     Return the CSRF token required for a POST form. The token is an
88     alphanumeric value. A new token is created if one is not already set.
89 
90     A side effect of calling this function is to make the csrf_protect
91     decorator and the CsrfViewMiddleware add a CSRF cookie and a 'Vary: Cookie'
92     header to the outgoing response.  For this reason, you may need to use this
93     function lazily, as is done by the csrf context processor.
94     """
95     if "CSRF_COOKIE" not in request.META:
96         csrf_secret = _get_new_csrf_string()
97         request.META["CSRF_COOKIE"] = _mask_cipher_secret(csrf_secret)
98     else:
99         csrf_secret = _unmask_cipher_token(request.META["CSRF_COOKIE"])
100     request.META["CSRF_COOKIE_USED"] = True
101     return _mask_cipher_secret(csrf_secret)
102 
103 
104 def rotate_token(request):
105     """
106     Change the CSRF token in use for a request - should be done on login
107     for security purposes.
108     """
109     request.META.update({
110         "CSRF_COOKIE_USED": True,
111         "CSRF_COOKIE": _get_new_csrf_token(),
112     })
113     request.csrf_cookie_needs_reset = True
114 
115 
116 class InvalidTokenFormat(Exception):
117     def __init__(self, reason):
118         self.reason = reason
119 
120 
121 def _sanitize_token(token):
122     if len(token) not in (CSRF_TOKEN_LENGTH, CSRF_SECRET_LENGTH):
123         raise InvalidTokenFormat(REASON_INCORRECT_LENGTH)
124     # Make sure all characters are in CSRF_ALLOWED_CHARS.
125     if invalid_token_chars_re.search(token):
126         raise InvalidTokenFormat(REASON_INVALID_CHARACTERS)
127     if len(token) == CSRF_SECRET_LENGTH:
128         # Older Django versions set cookies to values of CSRF_SECRET_LENGTH
129         # alphanumeric characters. For backwards compatibility, accept
130         # such values as unmasked secrets.
131         # It's easier to mask here and be consistent later, rather than add
132         # different code paths in the checks, although that might be a tad more
133         # efficient.
134         return _mask_cipher_secret(token)
135     return token
136 
137 
138 def _compare_masked_tokens(request_csrf_token, csrf_token):
139     # Assume both arguments are sanitized -- that is, strings of
140     # length CSRF_TOKEN_LENGTH, all CSRF_ALLOWED_CHARS.
141     return constant_time_compare(
142         _unmask_cipher_token(request_csrf_token),
143         _unmask_cipher_token(csrf_token),
144     )
145 
146 
147 class RejectRequest(Exception):
148     def __init__(self, reason):
149         self.reason = reason
150 
151 
152 class CsrfViewMiddleware(MiddlewareMixin):
153     """
154     Require a present and correct csrfmiddlewaretoken for POST requests that
155     have a CSRF cookie, and set an outgoing CSRF cookie.
156 
157     This middleware should be used in conjunction with the {% csrf_token %}
158     template tag.
159     """
160     @cached_property
161     def csrf_trusted_origins_hosts(self):
162         return [
163             urlparse(origin).netloc.lstrip('*')
164             for origin in settings.CSRF_TRUSTED_ORIGINS
165         ]
166 
167     @cached_property
168     def allowed_origins_exact(self):
169         return {
170             origin for origin in settings.CSRF_TRUSTED_ORIGINS
171             if '*' not in origin
172         }
173 
174     @cached_property
175     def allowed_origin_subdomains(self):
176         """
177         A mapping of allowed schemes to list of allowed netlocs, where all
178         subdomains of the netloc are allowed.
179         """
180         allowed_origin_subdomains = defaultdict(list)
181         for parsed in (urlparse(origin) for origin in settings.CSRF_TRUSTED_ORIGINS if '*' in origin):
182             allowed_origin_subdomains[parsed.scheme].append(parsed.netloc.lstrip('*'))
183         return allowed_origin_subdomains
184 
185     # The _accept and _reject methods currently only exist for the sake of the
186     # requires_csrf_token decorator.
187     def _accept(self, request):
188         # Avoid checking the request twice by adding a custom attribute to
189         # request.  This will be relevant when both decorator and middleware
190         # are used.
191         request.csrf_processing_done = True
192         return None
193 
194     def _reject(self, request, reason):
195         response = _get_failure_view()(request, reason=reason)
196         log_response(
197             'Forbidden (%s): %s', reason, request.path,
198             response=response,
199             request=request,
200             logger=logger,
201         )
202         return response
203 
204     def _get_token(self, request):
205         if settings.CSRF_USE_SESSIONS:
206             try:
207                 return request.session.get(CSRF_SESSION_KEY)
208             except AttributeError:
209                 raise ImproperlyConfigured(
210                     'CSRF_USE_SESSIONS is enabled, but request.session is not '
211                     'set. SessionMiddleware must appear before CsrfViewMiddleware '
212                     'in MIDDLEWARE.'
213                 )
214         else:
215             try:
216                 cookie_token = request.COOKIES[settings.CSRF_COOKIE_NAME]
217             except KeyError:
218                 return None
219 
220             # This can raise InvalidTokenFormat.
221             csrf_token = _sanitize_token(cookie_token)
222 
223             if csrf_token != cookie_token:
224                 # Then the cookie token had length CSRF_SECRET_LENGTH, so flag
225                 # to replace it with the masked version.
226                 request.csrf_cookie_needs_reset = True
227             return csrf_token
228 
229     def _set_token(self, request, response):
230         if settings.CSRF_USE_SESSIONS:
231             if request.session.get(CSRF_SESSION_KEY) != request.META['CSRF_COOKIE']:
232                 request.session[CSRF_SESSION_KEY] = request.META['CSRF_COOKIE']
233         else:
234             response.set_cookie(
235                 settings.CSRF_COOKIE_NAME,
236                 request.META['CSRF_COOKIE'],
237                 max_age=settings.CSRF_COOKIE_AGE,
238                 domain=settings.CSRF_COOKIE_DOMAIN,
239                 path=settings.CSRF_COOKIE_PATH,
240                 secure=settings.CSRF_COOKIE_SECURE,
241                 httponly=settings.CSRF_COOKIE_HTTPONLY,
242                 samesite=settings.CSRF_COOKIE_SAMESITE,
243             )
244             # Set the Vary header since content varies with the CSRF cookie.
245             patch_vary_headers(response, ('Cookie',))
246 
247     def _origin_verified(self, request):
248         request_origin = request.META['HTTP_ORIGIN']
249         try:
250             good_host = request.get_host()
251         except DisallowedHost:
252             pass
253         else:
254             good_origin = '%s://%s' % (
255                 'https' if request.is_secure() else 'http',
256                 good_host,
257             )
258             if request_origin == good_origin:
259                 return True
260         if request_origin in self.allowed_origins_exact:
261             return True
262         try:
263             parsed_origin = urlparse(request_origin)
264         except ValueError:
265             return False
266         request_scheme = parsed_origin.scheme
267         request_netloc = parsed_origin.netloc
268         return any(
269             is_same_domain(request_netloc, host)
270             for host in self.allowed_origin_subdomains.get(request_scheme, ())
271         )
272 
273     def _check_referer(self, request):
274         referer = request.META.get('HTTP_REFERER')
275         if referer is None:
276             raise RejectRequest(REASON_NO_REFERER)
277 
278         try:
279             referer = urlparse(referer)
280         except ValueError:
281             raise RejectRequest(REASON_MALFORMED_REFERER)
282 
283         # Make sure we have a valid URL for Referer.
284         if '' in (referer.scheme, referer.netloc):
285             raise RejectRequest(REASON_MALFORMED_REFERER)
286 
287         # Ensure that our Referer is also secure.
288         if referer.scheme != 'https':
289             raise RejectRequest(REASON_INSECURE_REFERER)
290 
291         if any(
292             is_same_domain(referer.netloc, host)
293             for host in self.csrf_trusted_origins_hosts
294         ):
295             return
296         # Allow matching the configured cookie domain.
297         good_referer = (
298             settings.SESSION_COOKIE_DOMAIN
299             if settings.CSRF_USE_SESSIONS
300             else settings.CSRF_COOKIE_DOMAIN
301         )
302         if good_referer is None:
303             # If no cookie domain is configured, allow matching the current
304             # host:port exactly if it's permitted by ALLOWED_HOSTS.
305             try:
306                 # request.get_host() includes the port.
307                 good_referer = request.get_host()
308             except DisallowedHost:
309                 raise RejectRequest(REASON_BAD_REFERER % referer.geturl())
310         else:
311             server_port = request.get_port()
312             if server_port not in ('443', '80'):
313                 good_referer = '%s:%s' % (good_referer, server_port)
314 
315         if not is_same_domain(referer.netloc, good_referer):
316             raise RejectRequest(REASON_BAD_REFERER % referer.geturl())
317 
318     def _bad_token_message(self, reason, token_source):
319         if token_source != 'POST':
320             # Assume it is a settings.CSRF_HEADER_NAME value.
321             header_name = HttpHeaders.parse_header_name(token_source)
322             token_source = f'the {header_name!r} HTTP header'
323         return f'CSRF token from {token_source} {reason}.'
324 
325     def _check_token(self, request):
326         # Access csrf_token via self._get_token() as rotate_token() may have
327         # been called by an authentication middleware during the
328         # process_request() phase.
329         try:
330             csrf_token = self._get_token(request)
331         except InvalidTokenFormat as exc:
332             raise RejectRequest(f'CSRF cookie {exc.reason}.')
333 
334         if csrf_token is None:
335             # No CSRF cookie. For POST requests, we insist on a CSRF cookie,
336             # and in this way we can avoid all CSRF attacks, including login
337             # CSRF.
338             raise RejectRequest(REASON_NO_CSRF_COOKIE)
339 
340         # Check non-cookie token for match.
341         request_csrf_token = ''
342         if request.method == 'POST':
343             try:
344                 request_csrf_token = request.POST.get('csrfmiddlewaretoken', '')
345             except OSError:
346                 # Handle a broken connection before we've completed reading the
347                 # POST data. process_view shouldn't raise any exceptions, so
348                 # we'll ignore and serve the user a 403 (assuming they're still
349                 # listening, which they probably aren't because of the error).
350                 pass
351 
352         if request_csrf_token == '':
353             # Fall back to X-CSRFToken, to make things easier for AJAX, and
354             # possible for PUT/DELETE.
355             try:
356                 request_csrf_token = request.META[settings.CSRF_HEADER_NAME]
357             except KeyError:
358                 raise RejectRequest(REASON_CSRF_TOKEN_MISSING)
359             token_source = settings.CSRF_HEADER_NAME
360         else:
361             token_source = 'POST'
362 
363         try:
364             request_csrf_token = _sanitize_token(request_csrf_token)
365         except InvalidTokenFormat as exc:
366             reason = self._bad_token_message(exc.reason, token_source)
367             raise RejectRequest(reason)
368 
369         if not _compare_masked_tokens(request_csrf_token, csrf_token):
370             reason = self._bad_token_message('incorrect', token_source)
371             raise RejectRequest(reason)
372 
373     def process_request(self, request):
374         try:
375             csrf_token = self._get_token(request)
376         except InvalidTokenFormat:
377             csrf_token = _get_new_csrf_token()
378             request.csrf_cookie_needs_reset = True
379 
380         if csrf_token is not None:
381             # Use same token next time.
382             request.META['CSRF_COOKIE'] = csrf_token
383 
384     def process_view(self, request, callback, callback_args, callback_kwargs):
385         if getattr(request, 'csrf_processing_done', False):
386             return None
387 
388         # Wait until request.META["CSRF_COOKIE"] has been manipulated before
389         # bailing out, so that get_token still works
390         if getattr(callback, 'csrf_exempt', False):
391             return None
392 
393         # Assume that anything not defined as 'safe' by RFC7231 needs protection
394         if request.method in ('GET', 'HEAD', 'OPTIONS', 'TRACE'):
395             return self._accept(request)
396 
397         if getattr(request, '_dont_enforce_csrf_checks', False):
398             # Mechanism to turn off CSRF checks for test suite. It comes after
399             # the creation of CSRF cookies, so that everything else continues
400             # to work exactly the same (e.g. cookies are sent, etc.), but
401             # before any branches that call the _reject method.
402             return self._accept(request)
403 
404         # Reject the request if the Origin header doesn't match an allowed
405         # value.
406         if 'HTTP_ORIGIN' in request.META:
407             if not self._origin_verified(request):
408                 return self._reject(request, REASON_BAD_ORIGIN % request.META['HTTP_ORIGIN'])
409         elif request.is_secure():
410             # If the Origin header wasn't provided, reject HTTPS requests if
411             # the Referer header doesn't match an allowed value.
412             #
413             # Suppose user visits http://example.com/
414             # An active network attacker (man-in-the-middle, MITM) sends a
415             # POST form that targets https://example.com/detonate-bomb/ and
416             # submits it via JavaScript.
417             #
418             # The attacker will need to provide a CSRF cookie and token, but
419             # that's no problem for a MITM and the session-independent secret
420             # we're using. So the MITM can circumvent the CSRF protection. This
421             # is true for any HTTP connection, but anyone using HTTPS expects
422             # better! For this reason, for https://example.com/ we need
423             # additional protection that treats http://example.com/ as
424             # completely untrusted. Under HTTPS, Barth et al. found that the
425             # Referer header is missing for same-domain requests in only about
426             # 0.2% of cases or less, so we can use strict Referer checking.
427             try:
428                 self._check_referer(request)
429             except RejectRequest as exc:
430                 return self._reject(request, exc.reason)
431 
432         try:
433             self._check_token(request)
434         except RejectRequest as exc:
435             return self._reject(request, exc.reason)
436 
437         return self._accept(request)
438 
439     def process_response(self, request, response):
440         # Send the CSRF cookie whenever the cookie is being used (even if the
441         # client already has it) in order to renew the expiry timer, but only
442         # if it hasn't already been sent during this request-response cycle.
443         # Also, send the cookie no matter what if a reset was requested.
444         if (
445             getattr(request, 'csrf_cookie_needs_reset', False) or (
446                 request.META.get('CSRF_COOKIE_USED') and
447                 not getattr(response, 'csrf_cookie_set', False)
448             )
449         ):
450             self._set_token(request, response)
451             # Update state to prevent _set_token() from being unnecessarily
452             # called again in process_response() by other instances of
453             # CsrfViewMiddleware. This can happen e.g. when both a decorator
454             # and middleware are used. However, the csrf_cookie_needs_reset
455             # attribute is still respected in subsequent calls e.g. in case
456             # rotate_token() is called in process_response() later by custom
457             # middleware but before those subsequent calls.
458             response.csrf_cookie_set = True
459             request.csrf_cookie_needs_reset = False
460 
461         return response

</code><intended_behavior>Introduce logic to differentiate between CSRF token issues and other exceptions, particularly those caused by file storage problems, to improve error classification and reporting.</intended_behavior>]
2025-10-15 06:02:26.018 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 06:02:26.019 | INFO     | Search-only: skipping patch selection
2025-10-15 06:02:26.182 | INFO     | Task django__django-14681 completed successfully.
2025-10-15 06:02:26.183 | INFO     | No patch generated. You can try running ACR again.
