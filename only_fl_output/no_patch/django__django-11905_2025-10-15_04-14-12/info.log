2025-10-15 04:14:12.904 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 04:14:16.200 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=179, output_tokens=13, cost=0.002180
2025-10-15 04:14:16.201 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 04:14:16.205 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 04:14:24.433 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=484, output_tokens=294, cost=0.013660
2025-10-15 04:14:24.439 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 04:14:26.061 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=648, output_tokens=31, cost=0.007410
2025-10-15 04:14:26.063 | DEBUG    | Extracted a valid json.
2025-10-15 04:14:26.063 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method(\"__isnull\")", "search_code(\"__isnull\")"],
    "bug_locations": []
}
2025-10-15 04:14:26.115 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 04:14:26.116 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 04:14:41.757 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1009, output_tokens=663, cost=0.029980
2025-10-15 04:14:41.772 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 04:14:48.129 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1888, output_tokens=263, cost=0.026770
2025-10-15 04:14:48.135 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 04:14:54.495 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=617, output_tokens=247, cost=0.013580
2025-10-15 04:14:54.496 | DEBUG    | Extracted a valid json.
2025-10-15 04:14:54.496 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "",
            "method": "",
            "intended_behavior": "Within the query construction or filter application sections that handle the `__isnull` lookup, implement a validation step to ensure that the value provided for `__isnull` is strictly a boolean. If a non-boolean value is provided, the code should raise a ValueError with a clear message indicating that only boolean values are acceptable for `__isnull` lookups. This behavior will prevent misuse and maintain consistency across different usage scenarios."
        },
        {
            "file": "django/db/models/sql/query.py",
            "class": "",
            "method": "",
            "intended_behavior": "In the part of the code that translates Django query expressions (including `__isnull`) into SQL statements, ensure that a validation for `__isnull` values is performed early in the process. If a value other than a boolean is detected, raise an error before the SQL statement is constructed. This will preserve the integrity of the SQL queries generated and prevent potential database errors or unexpected behavior."
        }
    ]
}
2025-10-15 04:14:54.516 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The main QuerySet implementation. This provides the public API for the ORM.
3 """
4 
5 import copy
6 import operator
7 import warnings
8 from collections import namedtuple
9 from functools import lru_cache
10 from itertools import chain
11 
12 from django.conf import settings
13 from django.core import exceptions
14 from django.db import (
15     DJANGO_VERSION_PICKLE_KEY, IntegrityError, connections, router,
16     transaction,
17 )
18 from django.db.models import DateField, DateTimeField, sql
19 from django.db.models.constants import LOOKUP_SEP
20 from django.db.models.deletion import Collector
21 from django.db.models.expressions import Case, Expression, F, Value, When
22 from django.db.models.fields import AutoField
23 from django.db.models.functions import Cast, Trunc
24 from django.db.models.query_utils import FilteredRelation, InvalidQuery, Q
25 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE
26 from django.db.utils import NotSupportedError
27 from django.utils import timezone
28 from django.utils.functional import cached_property, partition
29 from django.utils.version import get_version
30 
31 # The maximum number of results to fetch in a get() query.
32 MAX_GET_RESULTS = 21
33 
34 # The maximum number of items to display in a QuerySet.__repr__
35 REPR_OUTPUT_SIZE = 20
36 
37 
38 class BaseIterable:
39     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
40         self.queryset = queryset
41         self.chunked_fetch = chunked_fetch
42         self.chunk_size = chunk_size
43 
44 
45 class ModelIterable(BaseIterable):
46     """Iterable that yields a model instance for each row."""
47 
48     def __iter__(self):
49         queryset = self.queryset
50         db = queryset.db
51         compiler = queryset.query.get_compiler(using=db)
52         # Execute the query. This will also fill compiler.select, klass_info,
53         # and annotations.
54         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
55         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,
56                                                   compiler.annotation_col_map)
57         model_cls = klass_info['model']
58         select_fields = klass_info['select_fields']
59         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1
60         init_list = [f[0].target.attname
61                      for f in select[model_fields_start:model_fields_end]]
62         related_populators = get_related_populators(klass_info, select, db)
63         known_related_objects = [
64             (field, related_objs, operator.attrgetter(*[
65                 field.attname
66                 if from_field == 'self' else
67                 queryset.model._meta.get_field(from_field).attname
68                 for from_field in field.from_fields
69             ])) for field, related_objs in queryset._known_related_objects.items()
70         ]
71         for row in compiler.results_iter(results):
72             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])
73             for rel_populator in related_populators:
74                 rel_populator.populate(row, obj)
75             if annotation_col_map:
76                 for attr_name, col_pos in annotation_col_map.items():
77                     setattr(obj, attr_name, row[col_pos])
78 
79             # Add the known related objects to the model.
80             for field, rel_objs, rel_getter in known_related_objects:
81                 # Avoid overwriting objects loaded by, e.g., select_related().
82                 if field.is_cached(obj):
83                     continue
84                 rel_obj_id = rel_getter(obj)
85                 try:
86                     rel_obj = rel_objs[rel_obj_id]
87                 except KeyError:
88                     pass  # May happen in qs1 | qs2 scenarios.
89                 else:
90                     setattr(obj, field.name, rel_obj)
91 
92             yield obj
93 
94 
95 class ValuesIterable(BaseIterable):
96     """
97     Iterable returned by QuerySet.values() that yields a dict for each row.
98     """
99 
100     def __iter__(self):
101         queryset = self.queryset
102         query = queryset.query
103         compiler = query.get_compiler(queryset.db)
104 
105         # extra(select=...) cols are always at the start of the row.
106         names = [
107             *query.extra_select,
108             *query.values_select,
109             *query.annotation_select,
110         ]
111         indexes = range(len(names))
112         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
113             yield {names[i]: row[i] for i in indexes}
114 
115 
116 class ValuesListIterable(BaseIterable):
117     """
118     Iterable returned by QuerySet.values_list(flat=False) that yields a tuple
119     for each row.
120     """
121 
122     def __iter__(self):
123         queryset = self.queryset
124         query = queryset.query
125         compiler = query.get_compiler(queryset.db)
126 
127         if queryset._fields:
128             # extra(select=...) cols are always at the start of the row.
129             names = [
130                 *query.extra_select,
131                 *query.values_select,
132                 *query.annotation_select,
133             ]
134             fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]
135             if fields != names:
136                 # Reorder according to fields.
137                 index_map = {name: idx for idx, name in enumerate(names)}
138                 rowfactory = operator.itemgetter(*[index_map[f] for f in fields])
139                 return map(
140                     rowfactory,
141                     compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
142                 )
143         return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
144 
145 
146 class NamedValuesListIterable(ValuesListIterable):
147     """
148     Iterable returned by QuerySet.values_list(named=True) that yields a
149     namedtuple for each row.
150     """
151 
152     @staticmethod
153     @lru_cache()
154     def create_namedtuple_class(*names):
155         # Cache namedtuple() with @lru_cache() since it's too slow to be
156         # called for every QuerySet evaluation.
157         return namedtuple('Row', names)
158 
159     def __iter__(self):
160         queryset = self.queryset
161         if queryset._fields:
162             names = queryset._fields
163         else:
164             query = queryset.query
165             names = [*query.extra_select, *query.values_select, *query.annotation_select]
166         tuple_class = self.create_namedtuple_class(*names)
167         new = tuple.__new__
168         for row in super().__iter__():
169             yield new(tuple_class, row)
170 
171 
172 class FlatValuesListIterable(BaseIterable):
173     """
174     Iterable returned by QuerySet.values_list(flat=True) that yields single
175     values.
176     """
177 
178     def __iter__(self):
179         queryset = self.queryset
180         compiler = queryset.query.get_compiler(queryset.db)
181         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
182             yield row[0]
183 
184 
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "...(remaining elements truncated)..."
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError(
284                 'QuerySet indices must be integers or slices, not %s.'
285                 % type(k).__name__
286             )
287         assert ((not isinstance(k, slice) and (k >= 0)) or
288                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
289                  (k.stop is None or k.stop >= 0))), \
290             "Negative indexing is not supported."
291 
292         if self._result_cache is not None:
293             return self._result_cache[k]
294 
295         if isinstance(k, slice):
296             qs = self._chain()
297             if k.start is not None:
298                 start = int(k.start)
299             else:
300                 start = None
301             if k.stop is not None:
302                 stop = int(k.stop)
303             else:
304                 stop = None
305             qs.query.set_limits(start, stop)
306             return list(qs)[::k.step] if k.step else qs
307 
308         qs = self._chain()
309         qs.query.set_limits(k, k + 1)
310         qs._fetch_all()
311         return qs._result_cache[0]
312 
313     def __and__(self, other):
314         self._merge_sanity_check(other)
315         if isinstance(other, EmptyQuerySet):
316             return other
317         if isinstance(self, EmptyQuerySet):
318             return self
319         combined = self._chain()
320         combined._merge_known_related_objects(other)
321         combined.query.combine(other.query, sql.AND)
322         return combined
323 
324     def __or__(self, other):
325         self._merge_sanity_check(other)
326         if isinstance(self, EmptyQuerySet):
327             return other
328         if isinstance(other, EmptyQuerySet):
329             return self
330         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
331         combined = query._chain()
332         combined._merge_known_related_objects(other)
333         if not other.query.can_filter():
334             other = other.model._base_manager.filter(pk__in=other.values('pk'))
335         combined.query.combine(other.query, sql.OR)
336         return combined
337 
338     ####################################
339     # METHODS THAT DO DATABASE QUERIES #
340     ####################################
341 
342     def _iterator(self, use_chunked_fetch, chunk_size):
343         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
344 
345     def iterator(self, chunk_size=2000):
346         """
347         An iterator over the results from applying this QuerySet to the
348         database.
349         """
350         if chunk_size <= 0:
351             raise ValueError('Chunk size must be strictly positive.')
352         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
353         return self._iterator(use_chunked_fetch, chunk_size)
354 
355     def aggregate(self, *args, **kwargs):
356         """
357         Return a dictionary containing the calculations (aggregation)
358         over the current queryset.
359 
360         If args is present the expression is passed as a kwarg using
361         the Aggregate object's default alias.
362         """
363         if self.query.distinct_fields:
364             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
365         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
366         for arg in args:
367             # The default_alias property raises TypeError if default_alias
368             # can't be set automatically or AttributeError if it isn't an
369             # attribute.
370             try:
371                 arg.default_alias
372             except (AttributeError, TypeError):
373                 raise TypeError("Complex aggregates require an alias")
374             kwargs[arg.default_alias] = arg
375 
376         query = self.query.chain()
377         for (alias, aggregate_expr) in kwargs.items():
378             query.add_annotation(aggregate_expr, alias, is_summary=True)
379             if not query.annotations[alias].contains_aggregate:
380                 raise TypeError("%s is not an aggregate expression" % alias)
381         return query.get_aggregation(self.db, kwargs)
382 
383     def count(self):
384         """
385         Perform a SELECT COUNT() and return the number of records as an
386         integer.
387 
388         If the QuerySet is already fully cached, return the length of the
389         cached results set to avoid multiple SELECT COUNT(*) calls.
390         """
391         if self._result_cache is not None:
392             return len(self._result_cache)
393 
394         return self.query.get_count(using=self.db)
395 
396     def get(self, *args, **kwargs):
397         """
398         Perform the query and return a single object matching the given
399         keyword arguments.
400         """
401         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
402         if self.query.can_filter() and not self.query.distinct_fields:
403             clone = clone.order_by()
404         limit = None
405         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
406             limit = MAX_GET_RESULTS
407             clone.query.set_limits(high=limit)
408         num = len(clone)
409         if num == 1:
410             return clone._result_cache[0]
411         if not num:
412             raise self.model.DoesNotExist(
413                 "%s matching query does not exist." %
414                 self.model._meta.object_name
415             )
416         raise self.model.MultipleObjectsReturned(
417             'get() returned more than one %s -- it returned %s!' % (
418                 self.model._meta.object_name,
419                 num if not limit or num < limit else 'more than %s' % (limit - 1),
420             )
421         )
422 
423     def create(self, **kwargs):
424         """
425         Create a new object with the given kwargs, saving it to the database
426         and returning the created object.
427         """
428         obj = self.model(**kwargs)
429         self._for_write = True
430         obj.save(force_insert=True, using=self.db)
431         return obj
432 
433     def _populate_pk_values(self, objs):
434         for obj in objs:
435             if obj.pk is None:
436                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
437 
438     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
439         """
440         Insert each of the instances into the database. Do *not* call
441         save() on each of the instances, do not send any pre/post_save
442         signals, and do not set the primary key attribute if it is an
443         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
444         Multi-table models are not supported.
445         """
446         # When you bulk insert you don't get the primary keys back (if it's an
447         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
448         # you can't insert into the child tables which references this. There
449         # are two workarounds:
450         # 1) This could be implemented if you didn't have an autoincrement pk
451         # 2) You could do it by doing O(n) normal inserts into the parent
452         #    tables to get the primary keys back and then doing a single bulk
453         #    insert into the childmost table.
454         # We currently set the primary keys on the objects when using
455         # PostgreSQL via the RETURNING ID clause. It should be possible for
456         # Oracle as well, but the semantics for extracting the primary keys is
457         # trickier so it's not done yet.
458         assert batch_size is None or batch_size > 0
459         # Check that the parents share the same concrete model with the our
460         # model to detect the inheritance pattern ConcreteGrandParent ->
461         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
462         # would not identify that case as involving multiple tables.
463         for parent in self.model._meta.get_parent_list():
464             if parent._meta.concrete_model is not self.model._meta.concrete_model:
465                 raise ValueError("Can't bulk create a multi-table inherited model")
466         if not objs:
467             return objs
468         self._for_write = True
469         connection = connections[self.db]
470         opts = self.model._meta
471         fields = opts.concrete_fields
472         objs = list(objs)
473         self._populate_pk_values(objs)
474         with transaction.atomic(using=self.db, savepoint=False):
475             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
476             if objs_with_pk:
477                 returned_columns = self._batched_insert(
478                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
479                 )
480                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
481                     for result, field in zip(results, opts.db_returning_fields):
482                         if field != opts.pk:
483                             setattr(obj_with_pk, field.attname, result)
484                 for obj_with_pk in objs_with_pk:
485                     obj_with_pk._state.adding = False
486                     obj_with_pk._state.db = self.db
487             if objs_without_pk:
488                 fields = [f for f in fields if not isinstance(f, AutoField)]
489                 returned_columns = self._batched_insert(
490                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
491                 )
492                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
493                     assert len(returned_columns) == len(objs_without_pk)
494                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
495                     for result, field in zip(results, opts.db_returning_fields):
496                         setattr(obj_without_pk, field.attname, result)
497                     obj_without_pk._state.adding = False
498                     obj_without_pk._state.db = self.db
499 
500         return objs
501 
502     def bulk_update(self, objs, fields, batch_size=None):
503         """
504         Update the given fields in each of the given objects in the database.
505         """
506         if batch_size is not None and batch_size < 0:
507             raise ValueError('Batch size must be a positive integer.')
508         if not fields:
509             raise ValueError('Field names must be given to bulk_update().')
510         objs = tuple(objs)
511         if any(obj.pk is None for obj in objs):
512             raise ValueError('All bulk_update() objects must have a primary key set.')
513         fields = [self.model._meta.get_field(name) for name in fields]
514         if any(not f.concrete or f.many_to_many for f in fields):
515             raise ValueError('bulk_update() can only be used with concrete fields.')
516         if any(f.primary_key for f in fields):
517             raise ValueError('bulk_update() cannot be used with primary key fields.')
518         if not objs:
519             return
520         # PK is used twice in the resulting update query, once in the filter
521         # and once in the WHEN. Each field will also have one CAST.
522         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
523         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
524         requires_casting = connections[self.db].features.requires_casted_case_in_updates
525         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
526         updates = []
527         for batch_objs in batches:
528             update_kwargs = {}
529             for field in fields:
530                 when_statements = []
531                 for obj in batch_objs:
532                     attr = getattr(obj, field.attname)
533                     if not isinstance(attr, Expression):
534                         attr = Value(attr, output_field=field)
535                     when_statements.append(When(pk=obj.pk, then=attr))
536                 case_statement = Case(*when_statements, output_field=field)
537                 if requires_casting:
538                     case_statement = Cast(case_statement, output_field=field)
539                 update_kwargs[field.attname] = case_statement
540             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
541         with transaction.atomic(using=self.db, savepoint=False):
542             for pks, update_kwargs in updates:
543                 self.filter(pk__in=pks).update(**update_kwargs)
544     bulk_update.alters_data = True
545 
546     def get_or_create(self, defaults=None, **kwargs):
547         """
548         Look up an object with the given kwargs, creating one if necessary.
549         Return a tuple of (object, created), where created is a boolean
550         specifying whether an object was created.
551         """
552         # The get() needs to be targeted at the write database in order
553         # to avoid potential transaction consistency problems.
554         self._for_write = True
555         try:
556             return self.get(**kwargs), False
557         except self.model.DoesNotExist:
558             params = self._extract_model_params(defaults, **kwargs)
559             return self._create_object_from_params(kwargs, params)
560 
561     def update_or_create(self, defaults=None, **kwargs):
562         """
563         Look up an object with the given kwargs, updating one with defaults
564         if it exists, otherwise create a new one.
565         Return a tuple (object, created), where created is a boolean
566         specifying whether an object was created.
567         """
568         defaults = defaults or {}
569         self._for_write = True
570         with transaction.atomic(using=self.db):
571             try:
572                 obj = self.select_for_update().get(**kwargs)
573             except self.model.DoesNotExist:
574                 params = self._extract_model_params(defaults, **kwargs)
575                 # Lock the row so that a concurrent update is blocked until
576                 # after update_or_create() has performed its save.
577                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
578                 if created:
579                     return obj, created
580             for k, v in defaults.items():
581                 setattr(obj, k, v() if callable(v) else v)
582             obj.save(using=self.db)
583         return obj, False
584 
585     def _create_object_from_params(self, lookup, params, lock=False):
586         """
587         Try to create an object using passed params. Used by get_or_create()
588         and update_or_create().
589         """
590         try:
591             with transaction.atomic(using=self.db):
592                 params = {k: v() if callable(v) else v for k, v in params.items()}
593                 obj = self.create(**params)
594             return obj, True
595         except IntegrityError as e:
596             try:
597                 qs = self.select_for_update() if lock else self
598                 return qs.get(**lookup), False
599             except self.model.DoesNotExist:
600                 pass
601             raise e
602 
603     def _extract_model_params(self, defaults, **kwargs):
604         """
605         Prepare `params` for creating a model instance based on the given
606         kwargs; for use by get_or_create() and update_or_create().
607         """
608         defaults = defaults or {}
609         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
610         params.update(defaults)
611         property_names = self.model._meta._property_names
612         invalid_params = []
613         for param in params:
614             try:
615                 self.model._meta.get_field(param)
616             except exceptions.FieldDoesNotExist:
617                 # It's okay to use a model's property if it has a setter.
618                 if not (param in property_names and getattr(self.model, param).fset):
619                     invalid_params.append(param)
620         if invalid_params:
621             raise exceptions.FieldError(
622                 "Invalid field name(s) for model %s: '%s'." % (
623                     self.model._meta.object_name,
624                     "', '".join(sorted(invalid_params)),
625                 ))
626         return params
627 
628     def _earliest(self, *fields):
629         """
630         Return the earliest object according to fields (if given) or by the
631         model's Meta.get_latest_by.
632         """
633         if fields:
634             order_by = fields
635         else:
636             order_by = getattr(self.model._meta, 'get_latest_by')
637             if order_by and not isinstance(order_by, (tuple, list)):
638                 order_by = (order_by,)
639         if order_by is None:
640             raise ValueError(
641                 "earliest() and latest() require either fields as positional "
642                 "arguments or 'get_latest_by' in the model's Meta."
643             )
644 
645         assert not self.query.is_sliced, \
646             "Cannot change a query once a slice has been taken."
647         obj = self._chain()
648         obj.query.set_limits(high=1)
649         obj.query.clear_ordering(force_empty=True)
650         obj.query.add_ordering(*order_by)
651         return obj.get()
652 
653     def earliest(self, *fields):
654         return self._earliest(*fields)
655 
656     def latest(self, *fields):
657         return self.reverse()._earliest(*fields)
658 
659     def first(self):
660         """Return the first object of a query or None if no match is found."""
661         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
662             return obj
663 
664     def last(self):
665         """Return the last object of a query or None if no match is found."""
666         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
667             return obj
668 
669     def in_bulk(self, id_list=None, *, field_name='pk'):
670         """
671         Return a dictionary mapping each of the given IDs to the object with
672         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
673         """
674         assert not self.query.is_sliced, \
675             "Cannot use 'limit' or 'offset' with in_bulk"
676         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
677             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
678         if id_list is not None:
679             if not id_list:
680                 return {}
681             filter_key = '{}__in'.format(field_name)
682             batch_size = connections[self.db].features.max_query_params
683             id_list = tuple(id_list)
684             # If the database has a limit on the number of query parameters
685             # (e.g. SQLite), retrieve objects in batches if necessary.
686             if batch_size and batch_size < len(id_list):
687                 qs = ()
688                 for offset in range(0, len(id_list), batch_size):
689                     batch = id_list[offset:offset + batch_size]
690                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
691             else:
692                 qs = self.filter(**{filter_key: id_list}).order_by()
693         else:
694             qs = self._chain()
695         return {getattr(obj, field_name): obj for obj in qs}
696 
697     def delete(self):
698         """Delete the records in the current QuerySet."""
699         assert not self.query.is_sliced, \
700             "Cannot use 'limit' or 'offset' with delete."
701 
702         if self._fields is not None:
703             raise TypeError("Cannot call delete() after .values() or .values_list()")
704 
705         del_query = self._chain()
706 
707         # The delete is actually 2 queries - one to find related objects,
708         # and one to delete. Make sure that the discovery of related
709         # objects is performed on the same database as the deletion.
710         del_query._for_write = True
711 
712         # Disable non-supported fields.
713         del_query.query.select_for_update = False
714         del_query.query.select_related = False
715         del_query.query.clear_ordering(force_empty=True)
716 
717         collector = Collector(using=del_query.db)
718         collector.collect(del_query)
719         deleted, _rows_count = collector.delete()
720 
721         # Clear the result cache, in case this QuerySet gets reused.
722         self._result_cache = None
723         return deleted, _rows_count
724 
725     delete.alters_data = True
726     delete.queryset_only = True
727 
728     def _raw_delete(self, using):
729         """
730         Delete objects found from the given queryset in single direct SQL
731         query. No signals are sent and there is no protection for cascades.
732         """
733         return sql.DeleteQuery(self.model).delete_qs(self, using)
734     _raw_delete.alters_data = True
735 
736     def update(self, **kwargs):
737         """
738         Update all elements in the current QuerySet, setting all the given
739         fields to the appropriate values.
740         """
741         assert not self.query.is_sliced, \
742             "Cannot update a query once a slice has been taken."
743         self._for_write = True
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_values(kwargs)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         with transaction.mark_for_rollback_on_error(using=self.db):
749             rows = query.get_compiler(self.db).execute_sql(CURSOR)
750         self._result_cache = None
751         return rows
752     update.alters_data = True
753 
754     def _update(self, values):
755         """
756         A version of update() that accepts field objects instead of field names.
757         Used primarily for model saving and not intended for use by general
758         code (it requires too much poking around at model internals to be
759         useful at that level).
760         """
761         assert not self.query.is_sliced, \
762             "Cannot update a query once a slice has been taken."
763         query = self.query.chain(sql.UpdateQuery)
764         query.add_update_fields(values)
765         # Clear any annotations so that they won't be present in subqueries.
766         query.annotations = {}
767         self._result_cache = None
768         return query.get_compiler(self.db).execute_sql(CURSOR)
769     _update.alters_data = True
770     _update.queryset_only = False
771 
772     def exists(self):
773         if self._result_cache is None:
774             return self.query.has_results(using=self.db)
775         return bool(self._result_cache)
776 
777     def _prefetch_related_objects(self):
778         # This method can only be called once the result cache has been filled.
779         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
780         self._prefetch_done = True
781 
782     def explain(self, *, format=None, **options):
783         return self.query.explain(using=self.db, format=format, **options)
784 
785     ##################################################
786     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
787     ##################################################
788 
789     def raw(self, raw_query, params=None, translations=None, using=None):
790         if using is None:
791             using = self.db
792         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
793         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
794         return qs
795 
796     def _values(self, *fields, **expressions):
797         clone = self._chain()
798         if expressions:
799             clone = clone.annotate(**expressions)
800         clone._fields = fields
801         clone.query.set_values(fields)
802         return clone
803 
804     def values(self, *fields, **expressions):
805         fields += tuple(expressions)
806         clone = self._values(*fields, **expressions)
807         clone._iterable_class = ValuesIterable
808         return clone
809 
810     def values_list(self, *fields, flat=False, named=False):
811         if flat and named:
812             raise TypeError("'flat' and 'named' can't be used together.")
813         if flat and len(fields) > 1:
814             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
815 
816         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
817         _fields = []
818         expressions = {}
819         counter = 1
820         for field in fields:
821             if hasattr(field, 'resolve_expression'):
822                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
823                 while True:
824                     field_id = field_id_prefix + str(counter)
825                     counter += 1
826                     if field_id not in field_names:
827                         break
828                 expressions[field_id] = field
829                 _fields.append(field_id)
830             else:
831                 _fields.append(field)
832 
833         clone = self._values(*_fields, **expressions)
834         clone._iterable_class = (
835             NamedValuesListIterable if named
836             else FlatValuesListIterable if flat
837             else ValuesListIterable
838         )
839         return clone
840 
841     def dates(self, field_name, kind, order='ASC'):
842         """
843         Return a list of date objects representing all available dates for
844         the given field_name, scoped to 'kind'.
845         """
846         assert kind in ('year', 'month', 'week', 'day'), \
847             "'kind' must be one of 'year', 'month', 'week', or 'day'."
848         assert order in ('ASC', 'DESC'), \
849             "'order' must be either 'ASC' or 'DESC'."
850         return self.annotate(
851             datefield=Trunc(field_name, kind, output_field=DateField()),
852             plain_field=F(field_name)
853         ).values_list(
854             'datefield', flat=True
855         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
856 
857     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
858         """
859         Return a list of datetime objects representing all available
860         datetimes for the given field_name, scoped to 'kind'.
861         """
862         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
863             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
864         assert order in ('ASC', 'DESC'), \
865             "'order' must be either 'ASC' or 'DESC'."
866         if settings.USE_TZ:
867             if tzinfo is None:
868                 tzinfo = timezone.get_current_timezone()
869         else:
870             tzinfo = None
871         return self.annotate(
872             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
873             plain_field=F(field_name)
874         ).values_list(
875             'datetimefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
877 
878     def none(self):
879         """Return an empty QuerySet."""
880         clone = self._chain()
881         clone.query.set_empty()
882         return clone
883 
884     ##################################################################
885     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
886     ##################################################################
887 
888     def all(self):
889         """
890         Return a new QuerySet that is a copy of the current one. This allows a
891         QuerySet to proxy for a model manager in some cases.
892         """
893         return self._chain()
894 
895     def filter(self, *args, **kwargs):
896         """
897         Return a new QuerySet instance with the args ANDed to the existing
898         set.
899         """
900         self._not_support_combined_queries('filter')
901         return self._filter_or_exclude(False, *args, **kwargs)
902 
903     def exclude(self, *args, **kwargs):
904         """
905         Return a new QuerySet instance with NOT (args) ANDed to the existing
906         set.
907         """
908         self._not_support_combined_queries('exclude')
909         return self._filter_or_exclude(True, *args, **kwargs)
910 
911     def _filter_or_exclude(self, negate, *args, **kwargs):
912         if args or kwargs:
913             assert not self.query.is_sliced, \
914                 "Cannot filter a query once a slice has been taken."
915 
916         clone = self._chain()
917         if negate:
918             clone.query.add_q(~Q(*args, **kwargs))
919         else:
920             clone.query.add_q(Q(*args, **kwargs))
921         return clone
922 
923     def complex_filter(self, filter_obj):
924         """
925         Return a new QuerySet instance with filter_obj added to the filters.
926 
927         filter_obj can be a Q object or a dictionary of keyword lookup
928         arguments.
929 
930         This exists to support framework features such as 'limit_choices_to',
931         and usually it will be more natural to use other methods.
932         """
933         if isinstance(filter_obj, Q):
934             clone = self._chain()
935             clone.query.add_q(filter_obj)
936             return clone
937         else:
938             return self._filter_or_exclude(None, **filter_obj)
939 
940     def _combinator_query(self, combinator, *other_qs, all=False):
941         # Clone the query to inherit the select list and everything
942         clone = self._chain()
943         # Clear limits and ordering so they can be reapplied
944         clone.query.clear_ordering(True)
945         clone.query.clear_limits()
946         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
947         clone.query.combinator = combinator
948         clone.query.combinator_all = all
949         return clone
950 
951     def union(self, *other_qs, all=False):
952         # If the query is an EmptyQuerySet, combine all nonempty querysets.
953         if isinstance(self, EmptyQuerySet):
954             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
955             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
956         return self._combinator_query('union', *other_qs, all=all)
957 
958     def intersection(self, *other_qs):
959         # If any query is an EmptyQuerySet, return it.
960         if isinstance(self, EmptyQuerySet):
961             return self
962         for other in other_qs:
963             if isinstance(other, EmptyQuerySet):
964                 return other
965         return self._combinator_query('intersection', *other_qs)
966 
967     def difference(self, *other_qs):
968         # If the query is an EmptyQuerySet, return it.
969         if isinstance(self, EmptyQuerySet):
970             return self
971         return self._combinator_query('difference', *other_qs)
972 
973     def select_for_update(self, nowait=False, skip_locked=False, of=()):
974         """
975         Return a new QuerySet instance that will select objects with a
976         FOR UPDATE lock.
977         """
978         if nowait and skip_locked:
979             raise ValueError('The nowait option cannot be used with skip_locked.')
980         obj = self._chain()
981         obj._for_write = True
982         obj.query.select_for_update = True
983         obj.query.select_for_update_nowait = nowait
984         obj.query.select_for_update_skip_locked = skip_locked
985         obj.query.select_for_update_of = of
986         return obj
987 
988     def select_related(self, *fields):
989         """
990         Return a new QuerySet instance that will select related objects.
991 
992         If fields are specified, they must be ForeignKey fields and only those
993         related objects are included in the selection.
994 
995         If select_related(None) is called, clear the list.
996         """
997         self._not_support_combined_queries('select_related')
998         if self._fields is not None:
999             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1000 
1001         obj = self._chain()
1002         if fields == (None,):
1003             obj.query.select_related = False
1004         elif fields:
1005             obj.query.add_select_related(fields)
1006         else:
1007             obj.query.select_related = True
1008         return obj
1009 
1010     def prefetch_related(self, *lookups):
1011         """
1012         Return a new QuerySet instance that will prefetch the specified
1013         Many-To-One and Many-To-Many related objects when the QuerySet is
1014         evaluated.
1015 
1016         When prefetch_related() is called more than once, append to the list of
1017         prefetch lookups. If prefetch_related(None) is called, clear the list.
1018         """
1019         self._not_support_combined_queries('prefetch_related')
1020         clone = self._chain()
1021         if lookups == (None,):
1022             clone._prefetch_related_lookups = ()
1023         else:
1024             for lookup in lookups:
1025                 if isinstance(lookup, Prefetch):
1026                     lookup = lookup.prefetch_to
1027                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1028                 if lookup in self.query._filtered_relations:
1029                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1030             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1031         return clone
1032 
1033     def annotate(self, *args, **kwargs):
1034         """
1035         Return a query set in which the returned objects have been annotated
1036         with extra data or aggregations.
1037         """
1038         self._not_support_combined_queries('annotate')
1039         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1040         annotations = {}
1041         for arg in args:
1042             # The default_alias property may raise a TypeError.
1043             try:
1044                 if arg.default_alias in kwargs:
1045                     raise ValueError("The named annotation '%s' conflicts with the "
1046                                      "default name for another annotation."
1047                                      % arg.default_alias)
1048             except TypeError:
1049                 raise TypeError("Complex annotations require an alias")
1050             annotations[arg.default_alias] = arg
1051         annotations.update(kwargs)
1052 
1053         clone = self._chain()
1054         names = self._fields
1055         if names is None:
1056             names = set(chain.from_iterable(
1057                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1058                 for field in self.model._meta.get_fields()
1059             ))
1060 
1061         for alias, annotation in annotations.items():
1062             if alias in names:
1063                 raise ValueError("The annotation '%s' conflicts with a field on "
1064                                  "the model." % alias)
1065             if isinstance(annotation, FilteredRelation):
1066                 clone.query.add_filtered_relation(annotation, alias)
1067             else:
1068                 clone.query.add_annotation(annotation, alias, is_summary=False)
1069 
1070         for alias, annotation in clone.query.annotations.items():
1071             if alias in annotations and annotation.contains_aggregate:
1072                 if clone._fields is None:
1073                     clone.query.group_by = True
1074                 else:
1075                     clone.query.set_group_by()
1076                 break
1077 
1078         return clone
1079 
1080     def order_by(self, *field_names):
1081         """Return a new QuerySet instance with the ordering changed."""
1082         assert not self.query.is_sliced, \
1083             "Cannot reorder a query once a slice has been taken."
1084         obj = self._chain()
1085         obj.query.clear_ordering(force_empty=False)
1086         obj.query.add_ordering(*field_names)
1087         return obj
1088 
1089     def distinct(self, *field_names):
1090         """
1091         Return a new QuerySet instance that will select only distinct results.
1092         """
1093         assert not self.query.is_sliced, \
1094             "Cannot create distinct fields once a slice has been taken."
1095         obj = self._chain()
1096         obj.query.add_distinct_fields(*field_names)
1097         return obj
1098 
1099     def extra(self, select=None, where=None, params=None, tables=None,
1100               order_by=None, select_params=None):
1101         """Add extra SQL fragments to the query."""
1102         self._not_support_combined_queries('extra')
1103         assert not self.query.is_sliced, \
1104             "Cannot change a query once a slice has been taken"
1105         clone = self._chain()
1106         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1107         return clone
1108 
1109     def reverse(self):
1110         """Reverse the ordering of the QuerySet."""
1111         if self.query.is_sliced:
1112             raise TypeError('Cannot reverse a query once a slice has been taken.')
1113         clone = self._chain()
1114         clone.query.standard_ordering = not clone.query.standard_ordering
1115         return clone
1116 
1117     def defer(self, *fields):
1118         """
1119         Defer the loading of data for certain fields until they are accessed.
1120         Add the set of deferred fields to any existing set of deferred fields.
1121         The only exception to this is if None is passed in as the only
1122         parameter, in which case removal all deferrals.
1123         """
1124         self._not_support_combined_queries('defer')
1125         if self._fields is not None:
1126             raise TypeError("Cannot call defer() after .values() or .values_list()")
1127         clone = self._chain()
1128         if fields == (None,):
1129             clone.query.clear_deferred_loading()
1130         else:
1131             clone.query.add_deferred_loading(fields)
1132         return clone
1133 
1134     def only(self, *fields):
1135         """
1136         Essentially, the opposite of defer(). Only the fields passed into this
1137         method and that are not already specified as deferred are loaded
1138         immediately when the queryset is evaluated.
1139         """
1140         self._not_support_combined_queries('only')
1141         if self._fields is not None:
1142             raise TypeError("Cannot call only() after .values() or .values_list()")
1143         if fields == (None,):
1144             # Can only pass None to defer(), not only(), as the rest option.
1145             # That won't stop people trying to do this, so let's be explicit.
1146             raise TypeError("Cannot pass None as an argument to only().")
1147         for field in fields:
1148             field = field.split(LOOKUP_SEP, 1)[0]
1149             if field in self.query._filtered_relations:
1150                 raise ValueError('only() is not supported with FilteredRelation.')
1151         clone = self._chain()
1152         clone.query.add_immediate_loading(fields)
1153         return clone
1154 
1155     def using(self, alias):
1156         """Select which database this QuerySet should execute against."""
1157         clone = self._chain()
1158         clone._db = alias
1159         return clone
1160 
1161     ###################################
1162     # PUBLIC INTROSPECTION ATTRIBUTES #
1163     ###################################
1164 
1165     @property
1166     def ordered(self):
1167         """
1168         Return True if the QuerySet is ordered -- i.e. has an order_by()
1169         clause or a default ordering on the model (or is empty).
1170         """
1171         if isinstance(self, EmptyQuerySet):
1172             return True
1173         if self.query.extra_order_by or self.query.order_by:
1174             return True
1175         elif self.query.default_ordering and self.query.get_meta().ordering:
1176             return True
1177         else:
1178             return False
1179 
1180     @property
1181     def db(self):
1182         """Return the database used if this query is executed now."""
1183         if self._for_write:
1184             return self._db or router.db_for_write(self.model, **self._hints)
1185         return self._db or router.db_for_read(self.model, **self._hints)
1186 
1187     ###################
1188     # PRIVATE METHODS #
1189     ###################
1190 
1191     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1192         """
1193         Insert a new record for the given model. This provides an interface to
1194         the InsertQuery class and is how Model.save() is implemented.
1195         """
1196         self._for_write = True
1197         if using is None:
1198             using = self.db
1199         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1200         query.insert_values(fields, objs, raw=raw)
1201         return query.get_compiler(using=using).execute_sql(returning_fields)
1202     _insert.alters_data = True
1203     _insert.queryset_only = False
1204 
1205     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1206         """
1207         Helper method for bulk_create() to insert objs one batch at a time.
1208         """
1209         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1210             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1211         ops = connections[self.db].ops
1212         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1213         inserted_rows = []
1214         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1215         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1216             if bulk_return and not ignore_conflicts:
1217                 inserted_columns = self._insert(
1218                     item, fields=fields, using=self.db,
1219                     returning_fields=self.model._meta.db_returning_fields,
1220                     ignore_conflicts=ignore_conflicts,
1221                 )
1222                 if isinstance(inserted_columns, list):
1223                     inserted_rows.extend(inserted_columns)
1224                 else:
1225                     inserted_rows.append(inserted_columns)
1226             else:
1227                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1228         return inserted_rows
1229 
1230     def _chain(self, **kwargs):
1231         """
1232         Return a copy of the current QuerySet that's ready for another
1233         operation.
1234         """
1235         obj = self._clone()
1236         if obj._sticky_filter:
1237             obj.query.filter_is_sticky = True
1238             obj._sticky_filter = False
1239         obj.__dict__.update(kwargs)
1240         return obj
1241 
1242     def _clone(self):
1243         """
1244         Return a copy of the current QuerySet. A lightweight alternative
1245         to deepcopy().
1246         """
1247         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1248         c._sticky_filter = self._sticky_filter
1249         c._for_write = self._for_write
1250         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1251         c._known_related_objects = self._known_related_objects
1252         c._iterable_class = self._iterable_class
1253         c._fields = self._fields
1254         return c
1255 
1256     def _fetch_all(self):
1257         if self._result_cache is None:
1258             self._result_cache = list(self._iterable_class(self))
1259         if self._prefetch_related_lookups and not self._prefetch_done:
1260             self._prefetch_related_objects()
1261 
1262     def _next_is_sticky(self):
1263         """
1264         Indicate that the next filter call and the one following that should
1265         be treated as a single filter. This is only important when it comes to
1266         determining when to reuse tables for many-to-many filters. Required so
1267         that we can filter naturally on the results of related managers.
1268 
1269         This doesn't return a clone of the current QuerySet (it returns
1270         "self"). The method is only used internally and should be immediately
1271         followed by a filter() that does create a clone.
1272         """
1273         self._sticky_filter = True
1274         return self
1275 
1276     def _merge_sanity_check(self, other):
1277         """Check that two QuerySet classes may be merged."""
1278         if self._fields is not None and (
1279                 set(self.query.values_select) != set(other.query.values_select) or
1280                 set(self.query.extra_select) != set(other.query.extra_select) or
1281                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1282             raise TypeError(
1283                 "Merging '%s' classes must involve the same values in each case."
1284                 % self.__class__.__name__
1285             )
1286 
1287     def _merge_known_related_objects(self, other):
1288         """
1289         Keep track of all known related objects from either QuerySet instance.
1290         """
1291         for field, objects in other._known_related_objects.items():
1292             self._known_related_objects.setdefault(field, {}).update(objects)
1293 
1294     def resolve_expression(self, *args, **kwargs):
1295         if self._fields and len(self._fields) > 1:
1296             # values() queryset can only be used as nested queries
1297             # if they are set up to select only a single field.
1298             raise TypeError('Cannot use multi-field values as a filter value.')
1299         query = self.query.resolve_expression(*args, **kwargs)
1300         query._db = self._db
1301         return query
1302     resolve_expression.queryset_only = True
1303 
1304     def _add_hints(self, **hints):
1305         """
1306         Update hinting information for use by routers. Add new key/values or
1307         overwrite existing key/values.
1308         """
1309         self._hints.update(hints)
1310 
1311     def _has_filters(self):
1312         """
1313         Check if this QuerySet has any filtering going on. This isn't
1314         equivalent with checking if all objects are present in results, for
1315         example, qs[1:]._has_filters() -> False.
1316         """
1317         return self.query.has_filters()
1318 
1319     @staticmethod
1320     def _validate_values_are_expressions(values, method_name):
1321         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1322         if invalid_args:
1323             raise TypeError(
1324                 'QuerySet.%s() received non-expression(s): %s.' % (
1325                     method_name,
1326                     ', '.join(invalid_args),
1327                 )
1328             )
1329 
1330     def _not_support_combined_queries(self, operation_name):
1331         if self.query.combinator:
1332             raise NotSupportedError(
1333                 'Calling QuerySet.%s() after %s() is not supported.'
1334                 % (operation_name, self.query.combinator)
1335             )
1336 
1337 
1338 class InstanceCheckMeta(type):
1339     def __instancecheck__(self, instance):
1340         return isinstance(instance, QuerySet) and instance.query.is_empty()
1341 
1342 
1343 class EmptyQuerySet(metaclass=InstanceCheckMeta):
1344     """
1345     Marker class to checking if a queryset is empty by .none():
1346         isinstance(qs.none(), EmptyQuerySet) -> True
1347     """
1348 
1349     def __init__(self, *args, **kwargs):
1350         raise TypeError("EmptyQuerySet can't be instantiated")
1351 
1352 
1353 class RawQuerySet:
1354     """
1355     Provide an iterator which converts the results of raw SQL queries into
1356     annotated model instances.
1357     """
1358     def __init__(self, raw_query, model=None, query=None, params=None,
1359                  translations=None, using=None, hints=None):
1360         self.raw_query = raw_query
1361         self.model = model
1362         self._db = using
1363         self._hints = hints or {}
1364         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)
1365         self.params = params or ()
1366         self.translations = translations or {}
1367         self._result_cache = None
1368         self._prefetch_related_lookups = ()
1369         self._prefetch_done = False
1370 
1371     def resolve_model_init_order(self):
1372         """Resolve the init field names and value positions."""
1373         converter = connections[self.db].introspection.identifier_converter
1374         model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]
1375         annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)
1376                              if column not in self.model_fields]
1377         model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]
1378         model_init_names = [f.attname for f in model_init_fields]
1379         return model_init_names, model_init_order, annotation_fields
1380 
1381     def prefetch_related(self, *lookups):
1382         """Same as QuerySet.prefetch_related()"""
1383         clone = self._clone()
1384         if lookups == (None,):
1385             clone._prefetch_related_lookups = ()
1386         else:
1387             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1388         return clone
1389 
1390     def _prefetch_related_objects(self):
1391         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1392         self._prefetch_done = True
1393 
1394     def _clone(self):
1395         """Same as QuerySet._clone()"""
1396         c = self.__class__(
1397             self.raw_query, model=self.model, query=self.query, params=self.params,
1398             translations=self.translations, using=self._db, hints=self._hints
1399         )
1400         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1401         return c
1402 
1403     def _fetch_all(self):
1404         if self._result_cache is None:
1405             self._result_cache = list(self.iterator())
1406         if self._prefetch_related_lookups and not self._prefetch_done:
1407             self._prefetch_related_objects()
1408 
1409     def __len__(self):
1410         self._fetch_all()
1411         return len(self._result_cache)
1412 
1413     def __bool__(self):
1414         self._fetch_all()
1415         return bool(self._result_cache)
1416 
1417     def __iter__(self):
1418         self._fetch_all()
1419         return iter(self._result_cache)
1420 
1421     def iterator(self):
1422         # Cache some things for performance reasons outside the loop.
1423         db = self.db
1424         compiler = connections[db].ops.compiler('SQLCompiler')(
1425             self.query, connections[db], db
1426         )
1427 
1428         query = iter(self.query)
1429 
1430         try:
1431             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()
1432             if self.model._meta.pk.attname not in model_init_names:
1433                 raise InvalidQuery('Raw query must include the primary key')
1434             model_cls = self.model
1435             fields = [self.model_fields.get(c) for c in self.columns]
1436             converters = compiler.get_converters([
1437                 f.get_col(f.model._meta.db_table) if f else None for f in fields
1438             ])
1439             if converters:
1440                 query = compiler.apply_converters(query, converters)
1441             for values in query:
1442                 # Associate fields to values
1443                 model_init_values = [values[pos] for pos in model_init_pos]
1444                 instance = model_cls.from_db(db, model_init_names, model_init_values)
1445                 if annotation_fields:
1446                     for column, pos in annotation_fields:
1447                         setattr(instance, column, values[pos])
1448                 yield instance
1449         finally:
1450             # Done iterating the Query. If it has its own cursor, close it.
1451             if hasattr(self.query, 'cursor') and self.query.cursor:
1452                 self.query.cursor.close()
1453 
1454     def __repr__(self):
1455         return "<%s: %s>" % (self.__class__.__name__, self.query)
1456 
1457     def __getitem__(self, k):
1458         return list(self)[k]
1459 
1460     @property
1461     def db(self):
1462         """Return the database used if this query is executed now."""
1463         return self._db or router.db_for_read(self.model, **self._hints)
1464 
1465     def using(self, alias):
1466         """Select the database this RawQuerySet should execute against."""
1467         return RawQuerySet(
1468             self.raw_query, model=self.model,
1469             query=self.query.chain(using=alias),
1470             params=self.params, translations=self.translations,
1471             using=alias,
1472         )
1473 
1474     @cached_property
1475     def columns(self):
1476         """
1477         A list of model field names in the order they'll appear in the
1478         query results.
1479         """
1480         columns = self.query.get_columns()
1481         # Adjust any column names which don't match field names
1482         for (query_name, model_name) in self.translations.items():
1483             # Ignore translations for nonexistent column names
1484             try:
1485                 index = columns.index(query_name)
1486             except ValueError:
1487                 pass
1488             else:
1489                 columns[index] = model_name
1490         return columns
1491 
1492     @cached_property
1493     def model_fields(self):
1494         """A dict mapping column names to model field names."""
1495         converter = connections[self.db].introspection.identifier_converter
1496         model_fields = {}
1497         for field in self.model._meta.fields:
1498             name, column = field.get_attname_column()
1499             model_fields[converter(column)] = field
1500         return model_fields
1501 
1502 
1503 class Prefetch:
1504     def __init__(self, lookup, queryset=None, to_attr=None):
1505         # `prefetch_through` is the path we traverse to perform the prefetch.
1506         self.prefetch_through = lookup
1507         # `prefetch_to` is the path to the attribute that stores the result.
1508         self.prefetch_to = lookup
1509         if queryset is not None and not issubclass(queryset._iterable_class, ModelIterable):
1510             raise ValueError('Prefetch querysets cannot use values().')
1511         if to_attr:
1512             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])
1513 
1514         self.queryset = queryset
1515         self.to_attr = to_attr
1516 
1517     def __getstate__(self):
1518         obj_dict = self.__dict__.copy()
1519         if self.queryset is not None:
1520             # Prevent the QuerySet from being evaluated
1521             obj_dict['queryset'] = self.queryset._chain(
1522                 _result_cache=[],
1523                 _prefetch_done=True,
1524             )
1525         return obj_dict
1526 
1527     def add_prefix(self, prefix):
1528         self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through
1529         self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to
1530 
1531     def get_current_prefetch_to(self, level):
1532         return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])
1533 
1534     def get_current_to_attr(self, level):
1535         parts = self.prefetch_to.split(LOOKUP_SEP)
1536         to_attr = parts[level]
1537         as_attr = self.to_attr and level == len(parts) - 1
1538         return to_attr, as_attr
1539 
1540     def get_current_queryset(self, level):
1541         if self.get_current_prefetch_to(level) == self.prefetch_to:
1542             return self.queryset
1543         return None
1544 
1545     def __eq__(self, other):
1546         if not isinstance(other, Prefetch):
1547             return NotImplemented
1548         return self.prefetch_to == other.prefetch_to
1549 
1550     def __hash__(self):
1551         return hash((self.__class__, self.prefetch_to))
1552 
1553 
1554 def normalize_prefetch_lookups(lookups, prefix=None):
1555     """Normalize lookups into Prefetch objects."""
1556     ret = []
1557     for lookup in lookups:
1558         if not isinstance(lookup, Prefetch):
1559             lookup = Prefetch(lookup)
1560         if prefix:
1561             lookup.add_prefix(prefix)
1562         ret.append(lookup)
1563     return ret
1564 
1565 
1566 def prefetch_related_objects(model_instances, *related_lookups):
1567     """
1568     Populate prefetched object caches for a list of model instances based on
1569     the lookups/Prefetch instances given.
1570     """
1571     if not model_instances:
1572         return  # nothing to do
1573 
1574     # We need to be able to dynamically add to the list of prefetch_related
1575     # lookups that we look up (see below).  So we need some book keeping to
1576     # ensure we don't do duplicate work.
1577     done_queries = {}    # dictionary of things like 'foo__bar': [results]
1578 
1579     auto_lookups = set()  # we add to this as we go through.
1580     followed_descriptors = set()  # recursion protection
1581 
1582     all_lookups = normalize_prefetch_lookups(reversed(related_lookups))
1583     while all_lookups:
1584         lookup = all_lookups.pop()
1585         if lookup.prefetch_to in done_queries:
1586             if lookup.queryset is not None:
1587                 raise ValueError("'%s' lookup was already seen with a different queryset. "
1588                                  "You may need to adjust the ordering of your lookups." % lookup.prefetch_to)
1589 
1590             continue
1591 
1592         # Top level, the list of objects to decorate is the result cache
1593         # from the primary QuerySet. It won't be for deeper levels.
1594         obj_list = model_instances
1595 
1596         through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)
1597         for level, through_attr in enumerate(through_attrs):
1598             # Prepare main instances
1599             if not obj_list:
1600                 break
1601 
1602             prefetch_to = lookup.get_current_prefetch_to(level)
1603             if prefetch_to in done_queries:
1604                 # Skip any prefetching, and any object preparation
1605                 obj_list = done_queries[prefetch_to]
1606                 continue
1607 
1608             # Prepare objects:
1609             good_objects = True
1610             for obj in obj_list:
1611                 # Since prefetching can re-use instances, it is possible to have
1612                 # the same instance multiple times in obj_list, so obj might
1613                 # already be prepared.
1614                 if not hasattr(obj, '_prefetched_objects_cache'):
1615                     try:
1616                         obj._prefetched_objects_cache = {}
1617                     except (AttributeError, TypeError):
1618                         # Must be an immutable object from
1619                         # values_list(flat=True), for example (TypeError) or
1620                         # a QuerySet subclass that isn't returning Model
1621                         # instances (AttributeError), either in Django or a 3rd
1622                         # party. prefetch_related() doesn't make sense, so quit.
1623                         good_objects = False
1624                         break
1625             if not good_objects:
1626                 break
1627 
1628             # Descend down tree
1629 
1630             # We assume that objects retrieved are homogeneous (which is the premise
1631             # of prefetch_related), so what applies to first object applies to all.
1632             first_obj = obj_list[0]
1633             to_attr = lookup.get_current_to_attr(level)[0]
1634             prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)
1635 
1636             if not attr_found:
1637                 raise AttributeError("Cannot find '%s' on %s object, '%s' is an invalid "
1638                                      "parameter to prefetch_related()" %
1639                                      (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))
1640 
1641             if level == len(through_attrs) - 1 and prefetcher is None:
1642                 # Last one, this *must* resolve to something that supports
1643                 # prefetching, otherwise there is no point adding it and the
1644                 # developer asking for it has made a mistake.
1645                 raise ValueError("'%s' does not resolve to an item that supports "
1646                                  "prefetching - this is an invalid parameter to "
1647                                  "prefetch_related()." % lookup.prefetch_through)
1648 
1649             if prefetcher is not None and not is_fetched:
1650                 obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)
1651                 # We need to ensure we don't keep adding lookups from the
1652                 # same relationships to stop infinite recursion. So, if we
1653                 # are already on an automatically added lookup, don't add
1654                 # the new lookups from relationships we've seen already.
1655                 if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):
1656                     done_queries[prefetch_to] = obj_list
1657                     new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)
1658                     auto_lookups.update(new_lookups)
1659                     all_lookups.extend(new_lookups)
1660                 followed_descriptors.add(descriptor)
1661             else:
1662                 # Either a singly related object that has already been fetched
1663                 # (e.g. via select_related), or hopefully some other property
1664                 # that doesn't support prefetching but needs to be traversed.
1665 
1666                 # We replace the current list of parent objects with the list
1667                 # of related objects, filtering out empty or missing values so
1668                 # that we can continue with nullable or reverse relations.
1669                 new_obj_list = []
1670                 for obj in obj_list:
1671                     if through_attr in getattr(obj, '_prefetched_objects_cache', ()):
1672                         # If related objects have been prefetched, use the
1673                         # cache rather than the object's through_attr.
1674                         new_obj = list(obj._prefetched_objects_cache.get(through_attr))
1675                     else:
1676                         try:
1677                             new_obj = getattr(obj, through_attr)
1678                         except exceptions.ObjectDoesNotExist:
1679                             continue
1680                     if new_obj is None:
1681                         continue
1682                     # We special-case `list` rather than something more generic
1683                     # like `Iterable` because we don't want to accidentally match
1684                     # user models that define __iter__.
1685                     if isinstance(new_obj, list):
1686                         new_obj_list.extend(new_obj)
1687                     else:
1688                         new_obj_list.append(new_obj)
1689                 obj_list = new_obj_list
1690 
1691 
1692 def get_prefetcher(instance, through_attr, to_attr):
1693     """
1694     For the attribute 'through_attr' on the given instance, find
1695     an object that has a get_prefetch_queryset().
1696     Return a 4 tuple containing:
1697     (the object with get_prefetch_queryset (or None),
1698      the descriptor object representing this relationship (or None),
1699      a boolean that is False if the attribute was not found at all,
1700      a boolean that is True if the attribute has already been fetched)
1701     """
1702     prefetcher = None
1703     is_fetched = False
1704 
1705     # For singly related objects, we have to avoid getting the attribute
1706     # from the object, as this will trigger the query. So we first try
1707     # on the class, in order to get the descriptor object.
1708     rel_obj_descriptor = getattr(instance.__class__, through_attr, None)
1709     if rel_obj_descriptor is None:
1710         attr_found = hasattr(instance, through_attr)
1711     else:
1712         attr_found = True
1713         if rel_obj_descriptor:
1714             # singly related object, descriptor object has the
1715             # get_prefetch_queryset() method.
1716             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):
1717                 prefetcher = rel_obj_descriptor
1718                 if rel_obj_descriptor.is_cached(instance):
1719                     is_fetched = True
1720             else:
1721                 # descriptor doesn't support prefetching, so we go ahead and get
1722                 # the attribute on the instance rather than the class to
1723                 # support many related managers
1724                 rel_obj = getattr(instance, through_attr)
1725                 if hasattr(rel_obj, 'get_prefetch_queryset'):
1726                     prefetcher = rel_obj
1727                 if through_attr != to_attr:
1728                     # Special case cached_property instances because hasattr
1729                     # triggers attribute computation and assignment.
1730                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):
1731                         is_fetched = to_attr in instance.__dict__
1732                     else:
1733                         is_fetched = hasattr(instance, to_attr)
1734                 else:
1735                     is_fetched = through_attr in instance._prefetched_objects_cache
1736     return prefetcher, rel_obj_descriptor, attr_found, is_fetched
1737 
1738 
1739 def prefetch_one_level(instances, prefetcher, lookup, level):
1740     """
1741     Helper function for prefetch_related_objects().
1742 
1743     Run prefetches on all instances using the prefetcher object,
1744     assigning results to relevant caches in instance.
1745 
1746     Return the prefetched objects along with any additional prefetches that
1747     must be done due to prefetch_related lookups found from default managers.
1748     """
1749     # prefetcher must have a method get_prefetch_queryset() which takes a list
1750     # of instances, and returns a tuple:
1751 
1752     # (queryset of instances of self.model that are related to passed in instances,
1753     #  callable that gets value to be matched for returned instances,
1754     #  callable that gets value to be matched for passed in instances,
1755     #  boolean that is True for singly related objects,
1756     #  cache or field name to assign to,
1757     #  boolean that is True when the previous argument is a cache name vs a field name).
1758 
1759     # The 'values to be matched' must be hashable as they will be used
1760     # in a dictionary.
1761 
1762     rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (
1763         prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))
1764     # We have to handle the possibility that the QuerySet we just got back
1765     # contains some prefetch_related lookups. We don't want to trigger the
1766     # prefetch_related functionality by evaluating the query. Rather, we need
1767     # to merge in the prefetch_related lookups.
1768     # Copy the lookups in case it is a Prefetch object which could be reused
1769     # later (happens in nested prefetch_related).
1770     additional_lookups = [
1771         copy.copy(additional_lookup) for additional_lookup
1772         in getattr(rel_qs, '_prefetch_related_lookups', ())
1773     ]
1774     if additional_lookups:
1775         # Don't need to clone because the manager should have given us a fresh
1776         # instance, so we access an internal instead of using public interface
1777         # for performance reasons.
1778         rel_qs._prefetch_related_lookups = ()
1779 
1780     all_related_objects = list(rel_qs)
1781 
1782     rel_obj_cache = {}
1783     for rel_obj in all_related_objects:
1784         rel_attr_val = rel_obj_attr(rel_obj)
1785         rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)
1786 
1787     to_attr, as_attr = lookup.get_current_to_attr(level)
1788     # Make sure `to_attr` does not conflict with a field.
1789     if as_attr and instances:
1790         # We assume that objects retrieved are homogeneous (which is the premise
1791         # of prefetch_related), so what applies to first object applies to all.
1792         model = instances[0].__class__
1793         try:
1794             model._meta.get_field(to_attr)
1795         except exceptions.FieldDoesNotExist:
1796             pass
1797         else:
1798             msg = 'to_attr={} conflicts with a field on the {} model.'
1799             raise ValueError(msg.format(to_attr, model.__name__))
1800 
1801     # Whether or not we're prefetching the last part of the lookup.
1802     leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level
1803 
1804     for obj in instances:
1805         instance_attr_val = instance_attr(obj)
1806         vals = rel_obj_cache.get(instance_attr_val, [])
1807 
1808         if single:
1809             val = vals[0] if vals else None
1810             if as_attr:
1811                 # A to_attr has been given for the prefetch.
1812                 setattr(obj, to_attr, val)
1813             elif is_descriptor:
1814                 # cache_name points to a field name in obj.
1815                 # This field is a descriptor for a related object.
1816                 setattr(obj, cache_name, val)
1817             else:
1818                 # No to_attr has been given for this prefetch operation and the
1819                 # cache_name does not point to a descriptor. Store the value of
1820                 # the field in the object's field cache.
1821                 obj._state.fields_cache[cache_name] = val
1822         else:
1823             if as_attr:
1824                 setattr(obj, to_attr, vals)
1825             else:
1826                 manager = getattr(obj, to_attr)
1827                 if leaf and lookup.queryset is not None:
1828                     qs = manager._apply_rel_filters(lookup.queryset)
1829                 else:
1830                     qs = manager.get_queryset()
1831                 qs._result_cache = vals
1832                 # We don't want the individual qs doing prefetch_related now,
1833                 # since we have merged this into the current work.
1834                 qs._prefetch_done = True
1835                 obj._prefetched_objects_cache[cache_name] = qs
1836     return all_related_objects, additional_lookups
1837 
1838 
1839 class RelatedPopulator:
1840     """
1841     RelatedPopulator is used for select_related() object instantiation.
1842 
1843     The idea is that each select_related() model will be populated by a
1844     different RelatedPopulator instance. The RelatedPopulator instances get
1845     klass_info and select (computed in SQLCompiler) plus the used db as
1846     input for initialization. That data is used to compute which columns
1847     to use, how to instantiate the model, and how to populate the links
1848     between the objects.
1849 
1850     The actual creation of the objects is done in populate() method. This
1851     method gets row and from_obj as input and populates the select_related()
1852     model instance.
1853     """
1854     def __init__(self, klass_info, select, db):
1855         self.db = db
1856         # Pre-compute needed attributes. The attributes are:
1857         #  - model_cls: the possibly deferred model class to instantiate
1858         #  - either:
1859         #    - cols_start, cols_end: usually the columns in the row are
1860         #      in the same order model_cls.__init__ expects them, so we
1861         #      can instantiate by model_cls(*row[cols_start:cols_end])
1862         #    - reorder_for_init: When select_related descends to a child
1863         #      class, then we want to reuse the already selected parent
1864         #      data. However, in this case the parent data isn't necessarily
1865         #      in the same order that Model.__init__ expects it to be, so
1866         #      we have to reorder the parent data. The reorder_for_init
1867         #      attribute contains a function used to reorder the field data
1868         #      in the order __init__ expects it.
1869         #  - pk_idx: the index of the primary key field in the reordered
1870         #    model data. Used to check if a related object exists at all.
1871         #  - init_list: the field attnames fetched from the database. For
1872         #    deferred models this isn't the same as all attnames of the
1873         #    model's fields.
1874         #  - related_populators: a list of RelatedPopulator instances if
1875         #    select_related() descends to related models from this model.
1876         #  - local_setter, remote_setter: Methods to set cached values on
1877         #    the object being populated and on the remote object. Usually
1878         #    these are Field.set_cached_value() methods.
1879         select_fields = klass_info['select_fields']
1880         from_parent = klass_info['from_parent']
1881         if not from_parent:
1882             self.cols_start = select_fields[0]
1883             self.cols_end = select_fields[-1] + 1
1884             self.init_list = [
1885                 f[0].target.attname for f in select[self.cols_start:self.cols_end]
1886             ]
1887             self.reorder_for_init = None
1888         else:
1889             attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}
1890             model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)
1891             self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]
1892             self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])
1893 
1894         self.model_cls = klass_info['model']
1895         self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)
1896         self.related_populators = get_related_populators(klass_info, select, self.db)
1897         self.local_setter = klass_info['local_setter']
1898         self.remote_setter = klass_info['remote_setter']
1899 
1900     def populate(self, row, from_obj):
1901         if self.reorder_for_init:
1902             obj_data = self.reorder_for_init(row)
1903         else:
1904             obj_data = row[self.cols_start:self.cols_end]
1905         if obj_data[self.pk_idx] is None:
1906             obj = None
1907         else:
1908             obj = self.model_cls.from_db(self.db, self.init_list, obj_data)
1909             for rel_iter in self.related_populators:
1910                 rel_iter.populate(row, obj)
1911         self.local_setter(from_obj, obj)
1912         if obj is not None:
1913             self.remote_setter(obj, from_obj)
1914 
1915 
1916 def get_related_populators(klass_info, select, db):
1917     iterators = []
1918     related_klass_infos = klass_info.get('related_klass_infos', [])
1919     for rel_klass_info in related_klass_infos:
1920         rel_cls = RelatedPopulator(rel_klass_info, select, db)
1921         iterators.append(rel_cls)
1922     return iterators

</code><intended_behavior>Within the query construction or filter application sections that handle the `__isnull` lookup, implement a validation step to ensure that the value provided for `__isnull` is strictly a boolean. If a non-boolean value is provided, the code should raise a ValueError with a clear message indicating that only boolean values are acceptable for `__isnull` lookups. This behavior will prevent misuse and maintain consistency across different usage scenarios.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Create SQL statements for QuerySets.
3 
4 The code in here encapsulates all of the SQL construction so that QuerySets
5 themselves do not have to (and could be backed by things other than SQL
6 databases). The abstraction barrier only works one way: this module has to know
7 all about the internals of models in order to get the information it needs.
8 """
9 import copy
10 import difflib
11 import functools
12 import inspect
13 import sys
14 import warnings
15 from collections import Counter, namedtuple
16 from collections.abc import Iterator, Mapping
17 from itertools import chain, count, product
18 from string import ascii_uppercase
19 
20 from django.core.exceptions import (
21     EmptyResultSet, FieldDoesNotExist, FieldError,
22 )
23 from django.db import DEFAULT_DB_ALIAS, NotSupportedError, connections
24 from django.db.models.aggregates import Count
25 from django.db.models.constants import LOOKUP_SEP
26 from django.db.models.expressions import (
27     BaseExpression, Col, F, OuterRef, Ref, SimpleCol,
28 )
29 from django.db.models.fields import Field
30 from django.db.models.fields.related_lookups import MultiColSource
31 from django.db.models.lookups import Lookup
32 from django.db.models.query_utils import (
33     Q, check_rel_lookup_compatibility, refs_expression,
34 )
35 from django.db.models.sql.constants import (
36     INNER, LOUTER, ORDER_DIR, ORDER_PATTERN, SINGLE,
37 )
38 from django.db.models.sql.datastructures import (
39     BaseTable, Empty, Join, MultiJoin,
40 )
41 from django.db.models.sql.where import (
42     AND, OR, ExtraWhere, NothingNode, WhereNode,
43 )
44 from django.utils.deprecation import RemovedInDjango40Warning
45 from django.utils.functional import cached_property
46 from django.utils.tree import Node
47 
48 __all__ = ['Query', 'RawQuery']
49 
50 
51 def get_field_names_from_opts(opts):
52     return set(chain.from_iterable(
53         (f.name, f.attname) if f.concrete else (f.name,)
54         for f in opts.get_fields()
55     ))
56 
57 
58 def get_children_from_q(q):
59     for child in q.children:
60         if isinstance(child, Node):
61             yield from get_children_from_q(child)
62         else:
63             yield child
64 
65 
66 JoinInfo = namedtuple(
67     'JoinInfo',
68     ('final_field', 'targets', 'opts', 'joins', 'path', 'transform_function')
69 )
70 
71 
72 def _get_col(target, field, alias, simple_col):
73     if simple_col:
74         return SimpleCol(target, field)
75     return target.get_col(alias, field)
76 
77 
78 class RawQuery:
79     """A single raw SQL query."""
80 
81     def __init__(self, sql, using, params=None):
82         self.params = params or ()
83         self.sql = sql
84         self.using = using
85         self.cursor = None
86 
87         # Mirror some properties of a normal query so that
88         # the compiler can be used to process results.
89         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
90         self.extra_select = {}
91         self.annotation_select = {}
92 
93     def chain(self, using):
94         return self.clone(using)
95 
96     def clone(self, using):
97         return RawQuery(self.sql, using, params=self.params)
98 
99     def get_columns(self):
100         if self.cursor is None:
101             self._execute_query()
102         converter = connections[self.using].introspection.identifier_converter
103         return [converter(column_meta[0])
104                 for column_meta in self.cursor.description]
105 
106     def __iter__(self):
107         # Always execute a new query for a new iterator.
108         # This could be optimized with a cache at the expense of RAM.
109         self._execute_query()
110         if not connections[self.using].features.can_use_chunked_reads:
111             # If the database can't use chunked reads we need to make sure we
112             # evaluate the entire query up front.
113             result = list(self.cursor)
114         else:
115             result = self.cursor
116         return iter(result)
117 
118     def __repr__(self):
119         return "<%s: %s>" % (self.__class__.__name__, self)
120 
121     @property
122     def params_type(self):
123         return dict if isinstance(self.params, Mapping) else tuple
124 
125     def __str__(self):
126         return self.sql % self.params_type(self.params)
127 
128     def _execute_query(self):
129         connection = connections[self.using]
130 
131         # Adapt parameters to the database, as much as possible considering
132         # that the target type isn't known. See #17755.
133         params_type = self.params_type
134         adapter = connection.ops.adapt_unknown_value
135         if params_type is tuple:
136             params = tuple(adapter(val) for val in self.params)
137         elif params_type is dict:
138             params = {key: adapter(val) for key, val in self.params.items()}
139         else:
140             raise RuntimeError("Unexpected params type: %s" % params_type)
141 
142         self.cursor = connection.cursor()
143         self.cursor.execute(self.sql, params)
144 
145 
146 class Query(BaseExpression):
147     """A single SQL query."""
148 
149     alias_prefix = 'T'
150     subq_aliases = frozenset([alias_prefix])
151 
152     compiler = 'SQLCompiler'
153 
154     def __init__(self, model, where=WhereNode):
155         self.model = model
156         self.alias_refcount = {}
157         # alias_map is the most important data structure regarding joins.
158         # It's used for recording which joins exist in the query and what
159         # types they are. The key is the alias of the joined table (possibly
160         # the table name) and the value is a Join-like object (see
161         # sql.datastructures.Join for more information).
162         self.alias_map = {}
163         # Sometimes the query contains references to aliases in outer queries (as
164         # a result of split_exclude). Correct alias quoting needs to know these
165         # aliases too.
166         self.external_aliases = set()
167         self.table_map = {}     # Maps table names to list of aliases.
168         self.default_cols = True
169         self.default_ordering = True
170         self.standard_ordering = True
171         self.used_aliases = set()
172         self.filter_is_sticky = False
173         self.subquery = False
174 
175         # SQL-related attributes
176         # Select and related select clauses are expressions to use in the
177         # SELECT clause of the query.
178         # The select is used for cases where we want to set up the select
179         # clause to contain other than default fields (values(), subqueries...)
180         # Note that annotations go to annotations dictionary.
181         self.select = ()
182         self.where = where()
183         self.where_class = where
184         # The group_by attribute can have one of the following forms:
185         #  - None: no group by at all in the query
186         #  - A tuple of expressions: group by (at least) those expressions.
187         #    String refs are also allowed for now.
188         #  - True: group by all select fields of the model
189         # See compiler.get_group_by() for details.
190         self.group_by = None
191         self.order_by = ()
192         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
193         self.distinct = False
194         self.distinct_fields = ()
195         self.select_for_update = False
196         self.select_for_update_nowait = False
197         self.select_for_update_skip_locked = False
198         self.select_for_update_of = ()
199 
200         self.select_related = False
201         # Arbitrary limit for select_related to prevents infinite recursion.
202         self.max_depth = 5
203 
204         # Holds the selects defined by a call to values() or values_list()
205         # excluding annotation_select and extra_select.
206         self.values_select = ()
207 
208         # SQL annotation-related attributes
209         self.annotations = {}  # Maps alias -> Annotation Expression
210         self.annotation_select_mask = None
211         self._annotation_select_cache = None
212 
213         # Set combination attributes
214         self.combinator = None
215         self.combinator_all = False
216         self.combined_queries = ()
217 
218         # These are for extensions. The contents are more or less appended
219         # verbatim to the appropriate clause.
220         self.extra = {}  # Maps col_alias -> (col_sql, params).
221         self.extra_select_mask = None
222         self._extra_select_cache = None
223 
224         self.extra_tables = ()
225         self.extra_order_by = ()
226 
227         # A tuple that is a set of model field names and either True, if these
228         # are the fields to defer, or False if these are the only fields to
229         # load.
230         self.deferred_loading = (frozenset(), True)
231 
232         self._filtered_relations = {}
233 
234         self.explain_query = False
235         self.explain_format = None
236         self.explain_options = {}
237 
238     @property
239     def output_field(self):
240         if len(self.select) == 1:
241             return self.select[0].field
242         elif len(self.annotation_select) == 1:
243             return next(iter(self.annotation_select.values())).output_field
244 
245     @property
246     def has_select_fields(self):
247         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)
248 
249     @cached_property
250     def base_table(self):
251         for alias in self.alias_map:
252             return alias
253 
254     def __str__(self):
255         """
256         Return the query as a string of SQL with the parameter values
257         substituted in (use sql_with_params() to see the unsubstituted string).
258 
259         Parameter values won't necessarily be quoted correctly, since that is
260         done by the database interface at execution time.
261         """
262         sql, params = self.sql_with_params()
263         return sql % params
264 
265     def sql_with_params(self):
266         """
267         Return the query as an SQL string and the parameters that will be
268         substituted into the query.
269         """
270         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
271 
272     def __deepcopy__(self, memo):
273         """Limit the amount of work when a Query is deepcopied."""
274         result = self.clone()
275         memo[id(self)] = result
276         return result
277 
278     def get_compiler(self, using=None, connection=None):
279         if using is None and connection is None:
280             raise ValueError("Need either using or connection")
281         if using:
282             connection = connections[using]
283         return connection.ops.compiler(self.compiler)(self, connection, using)
284 
285     def get_meta(self):
286         """
287         Return the Options instance (the model._meta) from which to start
288         processing. Normally, this is self.model._meta, but it can be changed
289         by subclasses.
290         """
291         return self.model._meta
292 
293     def clone(self):
294         """
295         Return a copy of the current Query. A lightweight alternative to
296         to deepcopy().
297         """
298         obj = Empty()
299         obj.__class__ = self.__class__
300         # Copy references to everything.
301         obj.__dict__ = self.__dict__.copy()
302         # Clone attributes that can't use shallow copy.
303         obj.alias_refcount = self.alias_refcount.copy()
304         obj.alias_map = self.alias_map.copy()
305         obj.external_aliases = self.external_aliases.copy()
306         obj.table_map = self.table_map.copy()
307         obj.where = self.where.clone()
308         obj.annotations = self.annotations.copy()
309         if self.annotation_select_mask is None:
310             obj.annotation_select_mask = None
311         else:
312             obj.annotation_select_mask = self.annotation_select_mask.copy()
313         # _annotation_select_cache cannot be copied, as doing so breaks the
314         # (necessary) state in which both annotations and
315         # _annotation_select_cache point to the same underlying objects.
316         # It will get re-populated in the cloned queryset the next time it's
317         # used.
318         obj._annotation_select_cache = None
319         obj.extra = self.extra.copy()
320         if self.extra_select_mask is None:
321             obj.extra_select_mask = None
322         else:
323             obj.extra_select_mask = self.extra_select_mask.copy()
324         if self._extra_select_cache is None:
325             obj._extra_select_cache = None
326         else:
327             obj._extra_select_cache = self._extra_select_cache.copy()
328         if self.select_related is not False:
329             # Use deepcopy because select_related stores fields in nested
330             # dicts.
331             obj.select_related = copy.deepcopy(obj.select_related)
332         if 'subq_aliases' in self.__dict__:
333             obj.subq_aliases = self.subq_aliases.copy()
334         obj.used_aliases = self.used_aliases.copy()
335         obj._filtered_relations = self._filtered_relations.copy()
336         # Clear the cached_property
337         try:
338             del obj.base_table
339         except AttributeError:
340             pass
341         return obj
342 
343     def chain(self, klass=None):
344         """
345         Return a copy of the current Query that's ready for another operation.
346         The klass argument changes the type of the Query, e.g. UpdateQuery.
347         """
348         obj = self.clone()
349         if klass and obj.__class__ != klass:
350             obj.__class__ = klass
351         if not obj.filter_is_sticky:
352             obj.used_aliases = set()
353         obj.filter_is_sticky = False
354         if hasattr(obj, '_setup_query'):
355             obj._setup_query()
356         return obj
357 
358     def relabeled_clone(self, change_map):
359         clone = self.clone()
360         clone.change_aliases(change_map)
361         return clone
362 
363     def rewrite_cols(self, annotation, col_cnt):
364         # We must make sure the inner query has the referred columns in it.
365         # If we are aggregating over an annotation, then Django uses Ref()
366         # instances to note this. However, if we are annotating over a column
367         # of a related model, then it might be that column isn't part of the
368         # SELECT clause of the inner query, and we must manually make sure
369         # the column is selected. An example case is:
370         #    .aggregate(Sum('author__awards'))
371         # Resolving this expression results in a join to author, but there
372         # is no guarantee the awards column of author is in the select clause
373         # of the query. Thus we must manually add the column to the inner
374         # query.
375         orig_exprs = annotation.get_source_expressions()
376         new_exprs = []
377         for expr in orig_exprs:
378             # FIXME: These conditions are fairly arbitrary. Identify a better
379             # method of having expressions decide which code path they should
380             # take.
381             if isinstance(expr, Ref):
382                 # Its already a Ref to subquery (see resolve_ref() for
383                 # details)
384                 new_exprs.append(expr)
385             elif isinstance(expr, (WhereNode, Lookup)):
386                 # Decompose the subexpressions further. The code here is
387                 # copied from the else clause, but this condition must appear
388                 # before the contains_aggregate/is_summary condition below.
389                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
390                 new_exprs.append(new_expr)
391             else:
392                 # Reuse aliases of expressions already selected in subquery.
393                 for col_alias, selected_annotation in self.annotation_select.items():
394                     if selected_annotation == expr:
395                         new_expr = Ref(col_alias, expr)
396                         break
397                 else:
398                     # An expression that is not selected the subquery.
399                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
400                         # Reference column or another aggregate. Select it
401                         # under a non-conflicting alias.
402                         col_cnt += 1
403                         col_alias = '__col%d' % col_cnt
404                         self.annotations[col_alias] = expr
405                         self.append_annotation_mask([col_alias])
406                         new_expr = Ref(col_alias, expr)
407                     else:
408                         # Some other expression not referencing database values
409                         # directly. Its subexpression might contain Cols.
410                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
411                 new_exprs.append(new_expr)
412         annotation.set_source_expressions(new_exprs)
413         return annotation, col_cnt
414 
415     def get_aggregation(self, using, added_aggregate_names):
416         """
417         Return the dictionary with the values of the existing aggregations.
418         """
419         if not self.annotation_select:
420             return {}
421         existing_annotations = [
422             annotation for alias, annotation
423             in self.annotations.items()
424             if alias not in added_aggregate_names
425         ]
426         # Decide if we need to use a subquery.
427         #
428         # Existing annotations would cause incorrect results as get_aggregation()
429         # must produce just one result and thus must not use GROUP BY. But we
430         # aren't smart enough to remove the existing annotations from the
431         # query, so those would force us to use GROUP BY.
432         #
433         # If the query has limit or distinct, or uses set operations, then
434         # those operations must be done in a subquery so that the query
435         # aggregates on the limit and/or distinct results instead of applying
436         # the distinct and limit after the aggregation.
437         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or
438                 self.distinct or self.combinator):
439             from django.db.models.sql.subqueries import AggregateQuery
440             outer_query = AggregateQuery(self.model)
441             inner_query = self.clone()
442             inner_query.select_for_update = False
443             inner_query.select_related = False
444             inner_query.set_annotation_mask(self.annotation_select)
445             if not self.is_sliced and not self.distinct_fields:
446                 # Queries with distinct_fields need ordering and when a limit
447                 # is applied we must take the slice from the ordered query.
448                 # Otherwise no need for ordering.
449                 inner_query.clear_ordering(True)
450             if not inner_query.distinct:
451                 # If the inner query uses default select and it has some
452                 # aggregate annotations, then we must make sure the inner
453                 # query is grouped by the main model's primary key. However,
454                 # clearing the select clause can alter results if distinct is
455                 # used.
456                 has_existing_aggregate_annotations = any(
457                     annotation for annotation in existing_annotations
458                     if getattr(annotation, 'contains_aggregate', True)
459                 )
460                 if inner_query.default_cols and has_existing_aggregate_annotations:
461                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
462                 inner_query.default_cols = False
463 
464             relabels = {t: 'subquery' for t in inner_query.alias_map}
465             relabels[None] = 'subquery'
466             # Remove any aggregates marked for reduction from the subquery
467             # and move them to the outer AggregateQuery.
468             col_cnt = 0
469             for alias, expression in list(inner_query.annotation_select.items()):
470                 annotation_select_mask = inner_query.annotation_select_mask
471                 if expression.is_summary:
472                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
473                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
474                     del inner_query.annotations[alias]
475                     annotation_select_mask.remove(alias)
476                 # Make sure the annotation_select wont use cached results.
477                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
478             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:
479                 # In case of Model.objects[0:3].count(), there would be no
480                 # field selected in the inner query, yet we must use a subquery.
481                 # So, make sure at least one field is selected.
482                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
483             try:
484                 outer_query.add_subquery(inner_query, using)
485             except EmptyResultSet:
486                 return {
487                     alias: None
488                     for alias in outer_query.annotation_select
489                 }
490         else:
491             outer_query = self
492             self.select = ()
493             self.default_cols = False
494             self.extra = {}
495 
496         outer_query.clear_ordering(True)
497         outer_query.clear_limits()
498         outer_query.select_for_update = False
499         outer_query.select_related = False
500         compiler = outer_query.get_compiler(using)
501         result = compiler.execute_sql(SINGLE)
502         if result is None:
503             result = [None] * len(outer_query.annotation_select)
504 
505         converters = compiler.get_converters(outer_query.annotation_select.values())
506         result = next(compiler.apply_converters((result,), converters))
507 
508         return dict(zip(outer_query.annotation_select, result))
509 
510     def get_count(self, using):
511         """
512         Perform a COUNT() query using the current filter constraints.
513         """
514         obj = self.clone()
515         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
516         number = obj.get_aggregation(using, ['__count'])['__count']
517         if number is None:
518             number = 0
519         return number
520 
521     def has_filters(self):
522         return self.where
523 
524     def has_results(self, using):
525         q = self.clone()
526         if not q.distinct:
527             if q.group_by is True:
528                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
529                 q.set_group_by()
530             q.clear_select_clause()
531         q.clear_ordering(True)
532         q.set_limits(high=1)
533         compiler = q.get_compiler(using=using)
534         return compiler.has_results()
535 
536     def explain(self, using, format=None, **options):
537         q = self.clone()
538         q.explain_query = True
539         q.explain_format = format
540         q.explain_options = options
541         compiler = q.get_compiler(using=using)
542         return '\n'.join(compiler.explain_query())
543 
544     def combine(self, rhs, connector):
545         """
546         Merge the 'rhs' query into the current one (with any 'rhs' effects
547         being applied *after* (that is, "to the right of") anything in the
548         current query. 'rhs' is not modified during a call to this function.
549 
550         The 'connector' parameter describes how to connect filters from the
551         'rhs' query.
552         """
553         assert self.model == rhs.model, \
554             "Cannot combine queries on two different base models."
555         assert not self.is_sliced, \
556             "Cannot combine queries once a slice has been taken."
557         assert self.distinct == rhs.distinct, \
558             "Cannot combine a unique query with a non-unique query."
559         assert self.distinct_fields == rhs.distinct_fields, \
560             "Cannot combine queries with different distinct fields."
561 
562         # Work out how to relabel the rhs aliases, if necessary.
563         change_map = {}
564         conjunction = (connector == AND)
565 
566         # Determine which existing joins can be reused. When combining the
567         # query with AND we must recreate all joins for m2m filters. When
568         # combining with OR we can reuse joins. The reason is that in AND
569         # case a single row can't fulfill a condition like:
570         #     revrel__col=1 & revrel__col=2
571         # But, there might be two different related rows matching this
572         # condition. In OR case a single True is enough, so single row is
573         # enough, too.
574         #
575         # Note that we will be creating duplicate joins for non-m2m joins in
576         # the AND case. The results will be correct but this creates too many
577         # joins. This is something that could be fixed later on.
578         reuse = set() if conjunction else set(self.alias_map)
579         # Base table must be present in the query - this is the same
580         # table on both sides.
581         self.get_initial_alias()
582         joinpromoter = JoinPromoter(connector, 2, False)
583         joinpromoter.add_votes(
584             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
585         rhs_votes = set()
586         # Now, add the joins from rhs query into the new query (skipping base
587         # table).
588         rhs_tables = list(rhs.alias_map)[1:]
589         for alias in rhs_tables:
590             join = rhs.alias_map[alias]
591             # If the left side of the join was already relabeled, use the
592             # updated alias.
593             join = join.relabeled_clone(change_map)
594             new_alias = self.join(join, reuse=reuse)
595             if join.join_type == INNER:
596                 rhs_votes.add(new_alias)
597             # We can't reuse the same join again in the query. If we have two
598             # distinct joins for the same connection in rhs query, then the
599             # combined query must have two joins, too.
600             reuse.discard(new_alias)
601             if alias != new_alias:
602                 change_map[alias] = new_alias
603             if not rhs.alias_refcount[alias]:
604                 # The alias was unused in the rhs query. Unref it so that it
605                 # will be unused in the new query, too. We have to add and
606                 # unref the alias so that join promotion has information of
607                 # the join type for the unused alias.
608                 self.unref_alias(new_alias)
609         joinpromoter.add_votes(rhs_votes)
610         joinpromoter.update_join_types(self)
611 
612         # Now relabel a copy of the rhs where-clause and add it to the current
613         # one.
614         w = rhs.where.clone()
615         w.relabel_aliases(change_map)
616         self.where.add(w, connector)
617 
618         # Selection columns and extra extensions are those provided by 'rhs'.
619         if rhs.select:
620             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])
621         else:
622             self.select = ()
623 
624         if connector == OR:
625             # It would be nice to be able to handle this, but the queries don't
626             # really make sense (or return consistent value sets). Not worth
627             # the extra complexity when you can write a real query instead.
628             if self.extra and rhs.extra:
629                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
630         self.extra.update(rhs.extra)
631         extra_select_mask = set()
632         if self.extra_select_mask is not None:
633             extra_select_mask.update(self.extra_select_mask)
634         if rhs.extra_select_mask is not None:
635             extra_select_mask.update(rhs.extra_select_mask)
636         if extra_select_mask:
637             self.set_extra_mask(extra_select_mask)
638         self.extra_tables += rhs.extra_tables
639 
640         # Ordering uses the 'rhs' ordering, unless it has none, in which case
641         # the current ordering is used.
642         self.order_by = rhs.order_by or self.order_by
643         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
644 
645     def deferred_to_data(self, target, callback):
646         """
647         Convert the self.deferred_loading data structure to an alternate data
648         structure, describing the field that *will* be loaded. This is used to
649         compute the columns to select from the database and also by the
650         QuerySet class to work out which fields are being initialized on each
651         model. Models that have all their fields included aren't mentioned in
652         the result, only those that have field restrictions in place.
653 
654         The "target" parameter is the instance that is populated (in place).
655         The "callback" is a function that is called whenever a (model, field)
656         pair need to be added to "target". It accepts three parameters:
657         "target", and the model and list of fields being added for that model.
658         """
659         field_names, defer = self.deferred_loading
660         if not field_names:
661             return
662         orig_opts = self.get_meta()
663         seen = {}
664         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
665         for field_name in field_names:
666             parts = field_name.split(LOOKUP_SEP)
667             cur_model = self.model._meta.concrete_model
668             opts = orig_opts
669             for name in parts[:-1]:
670                 old_model = cur_model
671                 if name in self._filtered_relations:
672                     name = self._filtered_relations[name].relation_name
673                 source = opts.get_field(name)
674                 if is_reverse_o2o(source):
675                     cur_model = source.related_model
676                 else:
677                     cur_model = source.remote_field.model
678                 opts = cur_model._meta
679                 # Even if we're "just passing through" this model, we must add
680                 # both the current model's pk and the related reference field
681                 # (if it's not a reverse relation) to the things we select.
682                 if not is_reverse_o2o(source):
683                     must_include[old_model].add(source)
684                 add_to_dict(must_include, cur_model, opts.pk)
685             field = opts.get_field(parts[-1])
686             is_reverse_object = field.auto_created and not field.concrete
687             model = field.related_model if is_reverse_object else field.model
688             model = model._meta.concrete_model
689             if model == opts.model:
690                 model = cur_model
691             if not is_reverse_o2o(field):
692                 add_to_dict(seen, model, field)
693 
694         if defer:
695             # We need to load all fields for each model, except those that
696             # appear in "seen" (for all models that appear in "seen"). The only
697             # slight complexity here is handling fields that exist on parent
698             # models.
699             workset = {}
700             for model, values in seen.items():
701                 for field in model._meta.local_fields:
702                     if field not in values:
703                         m = field.model._meta.concrete_model
704                         add_to_dict(workset, m, field)
705             for model, values in must_include.items():
706                 # If we haven't included a model in workset, we don't add the
707                 # corresponding must_include fields for that model, since an
708                 # empty set means "include all fields". That's why there's no
709                 # "else" branch here.
710                 if model in workset:
711                     workset[model].update(values)
712             for model, values in workset.items():
713                 callback(target, model, values)
714         else:
715             for model, values in must_include.items():
716                 if model in seen:
717                     seen[model].update(values)
718                 else:
719                     # As we've passed through this model, but not explicitly
720                     # included any fields, we have to make sure it's mentioned
721                     # so that only the "must include" fields are pulled in.
722                     seen[model] = values
723             # Now ensure that every model in the inheritance chain is mentioned
724             # in the parent list. Again, it must be mentioned to ensure that
725             # only "must include" fields are pulled in.
726             for model in orig_opts.get_parent_list():
727                 seen.setdefault(model, set())
728             for model, values in seen.items():
729                 callback(target, model, values)
730 
731     def table_alias(self, table_name, create=False, filtered_relation=None):
732         """
733         Return a table alias for the given table_name and whether this is a
734         new alias or not.
735 
736         If 'create' is true, a new alias is always created. Otherwise, the
737         most recently created alias for the table (if one exists) is reused.
738         """
739         alias_list = self.table_map.get(table_name)
740         if not create and alias_list:
741             alias = alias_list[0]
742             self.alias_refcount[alias] += 1
743             return alias, False
744 
745         # Create a new alias for this table.
746         if alias_list:
747             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
748             alias_list.append(alias)
749         else:
750             # The first occurrence of a table uses the table name directly.
751             alias = filtered_relation.alias if filtered_relation is not None else table_name
752             self.table_map[table_name] = [alias]
753         self.alias_refcount[alias] = 1
754         return alias, True
755 
756     def ref_alias(self, alias):
757         """Increases the reference count for this alias."""
758         self.alias_refcount[alias] += 1
759 
760     def unref_alias(self, alias, amount=1):
761         """Decreases the reference count for this alias."""
762         self.alias_refcount[alias] -= amount
763 
764     def promote_joins(self, aliases):
765         """
766         Promote recursively the join type of given aliases and its children to
767         an outer join. If 'unconditional' is False, only promote the join if
768         it is nullable or the parent join is an outer join.
769 
770         The children promotion is done to avoid join chains that contain a LOUTER
771         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
772         then we must also promote b->c automatically, or otherwise the promotion
773         of a->b doesn't actually change anything in the query results.
774         """
775         aliases = list(aliases)
776         while aliases:
777             alias = aliases.pop(0)
778             if self.alias_map[alias].join_type is None:
779                 # This is the base table (first FROM entry) - this table
780                 # isn't really joined at all in the query, so we should not
781                 # alter its join type.
782                 continue
783             # Only the first alias (skipped above) should have None join_type
784             assert self.alias_map[alias].join_type is not None
785             parent_alias = self.alias_map[alias].parent_alias
786             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
787             already_louter = self.alias_map[alias].join_type == LOUTER
788             if ((self.alias_map[alias].nullable or parent_louter) and
789                     not already_louter):
790                 self.alias_map[alias] = self.alias_map[alias].promote()
791                 # Join type of 'alias' changed, so re-examine all aliases that
792                 # refer to this one.
793                 aliases.extend(
794                     join for join in self.alias_map
795                     if self.alias_map[join].parent_alias == alias and join not in aliases
796                 )
797 
798     def demote_joins(self, aliases):
799         """
800         Change join type from LOUTER to INNER for all joins in aliases.
801 
802         Similarly to promote_joins(), this method must ensure no join chains
803         containing first an outer, then an inner join are generated. If we
804         are demoting b->c join in chain a LOUTER b LOUTER c then we must
805         demote a->b automatically, or otherwise the demotion of b->c doesn't
806         actually change anything in the query results. .
807         """
808         aliases = list(aliases)
809         while aliases:
810             alias = aliases.pop(0)
811             if self.alias_map[alias].join_type == LOUTER:
812                 self.alias_map[alias] = self.alias_map[alias].demote()
813                 parent_alias = self.alias_map[alias].parent_alias
814                 if self.alias_map[parent_alias].join_type == INNER:
815                     aliases.append(parent_alias)
816 
817     def reset_refcounts(self, to_counts):
818         """
819         Reset reference counts for aliases so that they match the value passed
820         in `to_counts`.
821         """
822         for alias, cur_refcount in self.alias_refcount.copy().items():
823             unref_amount = cur_refcount - to_counts.get(alias, 0)
824             self.unref_alias(alias, unref_amount)
825 
826     def change_aliases(self, change_map):
827         """
828         Change the aliases in change_map (which maps old-alias -> new-alias),
829         relabelling any references to them in select columns and the where
830         clause.
831         """
832         assert set(change_map).isdisjoint(change_map.values())
833 
834         # 1. Update references in "select" (normal columns plus aliases),
835         # "group by" and "where".
836         self.where.relabel_aliases(change_map)
837         if isinstance(self.group_by, tuple):
838             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])
839         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])
840         self.annotations = self.annotations and {
841             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()
842         }
843 
844         # 2. Rename the alias in the internal table/alias datastructures.
845         for old_alias, new_alias in change_map.items():
846             if old_alias not in self.alias_map:
847                 continue
848             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
849             self.alias_map[new_alias] = alias_data
850             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
851             del self.alias_refcount[old_alias]
852             del self.alias_map[old_alias]
853 
854             table_aliases = self.table_map[alias_data.table_name]
855             for pos, alias in enumerate(table_aliases):
856                 if alias == old_alias:
857                     table_aliases[pos] = new_alias
858                     break
859         self.external_aliases = {change_map.get(alias, alias)
860                                  for alias in self.external_aliases}
861 
862     def bump_prefix(self, outer_query):
863         """
864         Change the alias prefix to the next letter in the alphabet in a way
865         that the outer query's aliases and this query's aliases will not
866         conflict. Even tables that previously had no alias will get an alias
867         after this call.
868         """
869         def prefix_gen():
870             """
871             Generate a sequence of characters in alphabetical order:
872                 -> 'A', 'B', 'C', ...
873 
874             When the alphabet is finished, the sequence will continue with the
875             Cartesian product:
876                 -> 'AA', 'AB', 'AC', ...
877             """
878             alphabet = ascii_uppercase
879             prefix = chr(ord(self.alias_prefix) + 1)
880             yield prefix
881             for n in count(1):
882                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
883                 for s in product(seq, repeat=n):
884                     yield ''.join(s)
885                 prefix = None
886 
887         if self.alias_prefix != outer_query.alias_prefix:
888             # No clashes between self and outer query should be possible.
889             return
890 
891         # Explicitly avoid infinite loop. The constant divider is based on how
892         # much depth recursive subquery references add to the stack. This value
893         # might need to be adjusted when adding or removing function calls from
894         # the code path in charge of performing these operations.
895         local_recursion_limit = sys.getrecursionlimit() // 16
896         for pos, prefix in enumerate(prefix_gen()):
897             if prefix not in self.subq_aliases:
898                 self.alias_prefix = prefix
899                 break
900             if pos > local_recursion_limit:
901                 raise RecursionError(
902                     'Maximum recursion depth exceeded: too many subqueries.'
903                 )
904         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
905         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
906         self.change_aliases({
907             alias: '%s%d' % (self.alias_prefix, pos)
908             for pos, alias in enumerate(self.alias_map)
909         })
910 
911     def get_initial_alias(self):
912         """
913         Return the first alias for this query, after increasing its reference
914         count.
915         """
916         if self.alias_map:
917             alias = self.base_table
918             self.ref_alias(alias)
919         else:
920             alias = self.join(BaseTable(self.get_meta().db_table, None))
921         return alias
922 
923     def count_active_tables(self):
924         """
925         Return the number of tables in this query with a non-zero reference
926         count. After execution, the reference counts are zeroed, so tables
927         added in compiler will not be seen by this method.
928         """
929         return len([1 for count in self.alias_refcount.values() if count])
930 
931     def join(self, join, reuse=None, reuse_with_filtered_relation=False):
932         """
933         Return an alias for the 'join', either reusing an existing alias for
934         that join or creating a new one. 'join' is either a
935         sql.datastructures.BaseTable or Join.
936 
937         The 'reuse' parameter can be either None which means all joins are
938         reusable, or it can be a set containing the aliases that can be reused.
939 
940         The 'reuse_with_filtered_relation' parameter is used when computing
941         FilteredRelation instances.
942 
943         A join is always created as LOUTER if the lhs alias is LOUTER to make
944         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
945         joins are created as LOUTER if the join is nullable.
946         """
947         if reuse_with_filtered_relation and reuse:
948             reuse_aliases = [
949                 a for a, j in self.alias_map.items()
950                 if a in reuse and j.equals(join, with_filtered_relation=False)
951             ]
952         else:
953             reuse_aliases = [
954                 a for a, j in self.alias_map.items()
955                 if (reuse is None or a in reuse) and j == join
956             ]
957         if reuse_aliases:
958             if join.table_alias in reuse_aliases:
959                 reuse_alias = join.table_alias
960             else:
961                 # Reuse the most recent alias of the joined table
962                 # (a many-to-many relation may be joined multiple times).
963                 reuse_alias = reuse_aliases[-1]
964             self.ref_alias(reuse_alias)
965             return reuse_alias
966 
967         # No reuse is possible, so we need a new alias.
968         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)
969         if join.join_type:
970             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
971                 join_type = LOUTER
972             else:
973                 join_type = INNER
974             join.join_type = join_type
975         join.table_alias = alias
976         self.alias_map[alias] = join
977         return alias
978 
979     def join_parent_model(self, opts, model, alias, seen):
980         """
981         Make sure the given 'model' is joined in the query. If 'model' isn't
982         a parent of 'opts' or if it is None this method is a no-op.
983 
984         The 'alias' is the root alias for starting the join, 'seen' is a dict
985         of model -> alias of existing joins. It must also contain a mapping
986         of None -> some alias. This will be returned in the no-op case.
987         """
988         if model in seen:
989             return seen[model]
990         chain = opts.get_base_chain(model)
991         if not chain:
992             return alias
993         curr_opts = opts
994         for int_model in chain:
995             if int_model in seen:
996                 curr_opts = int_model._meta
997                 alias = seen[int_model]
998                 continue
999             # Proxy model have elements in base chain
1000             # with no parents, assign the new options
1001             # object and skip to the next base in that
1002             # case
1003             if not curr_opts.parents[int_model]:
1004                 curr_opts = int_model._meta
1005                 continue
1006             link_field = curr_opts.get_ancestor_link(int_model)
1007             join_info = self.setup_joins([link_field.name], curr_opts, alias)
1008             curr_opts = int_model._meta
1009             alias = seen[int_model] = join_info.joins[-1]
1010         return alias or seen[None]
1011 
1012     def add_annotation(self, annotation, alias, is_summary=False):
1013         """Add a single annotation expression to the Query."""
1014         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
1015                                                    summarize=is_summary)
1016         self.append_annotation_mask([alias])
1017         self.annotations[alias] = annotation
1018 
1019     def resolve_expression(self, query, *args, **kwargs):
1020         clone = self.clone()
1021         # Subqueries need to use a different set of aliases than the outer query.
1022         clone.bump_prefix(query)
1023         clone.subquery = True
1024         # It's safe to drop ordering if the queryset isn't using slicing,
1025         # distinct(*fields) or select_for_update().
1026         if (self.low_mark == 0 and self.high_mark is None and
1027                 not self.distinct_fields and
1028                 not self.select_for_update):
1029             clone.clear_ordering(True)
1030         clone.where.resolve_expression(query, *args, **kwargs)
1031         for key, value in clone.annotations.items():
1032             resolved = value.resolve_expression(query, *args, **kwargs)
1033             if hasattr(resolved, 'external_aliases'):
1034                 resolved.external_aliases.update(clone.alias_map)
1035             clone.annotations[key] = resolved
1036         # Outer query's aliases are considered external.
1037         clone.external_aliases.update(
1038             alias for alias, table in query.alias_map.items()
1039             if (
1040                 isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias
1041             ) or (
1042                 isinstance(table, BaseTable) and table.table_name != table.table_alias
1043             )
1044         )
1045         return clone
1046 
1047     def as_sql(self, compiler, connection):
1048         sql, params = self.get_compiler(connection=connection).as_sql()
1049         if self.subquery:
1050             sql = '(%s)' % sql
1051         return sql, params
1052 
1053     def resolve_lookup_value(self, value, can_reuse, allow_joins, simple_col):
1054         if hasattr(value, 'resolve_expression'):
1055             kwargs = {'reuse': can_reuse, 'allow_joins': allow_joins}
1056             if isinstance(value, F):
1057                 kwargs['simple_col'] = simple_col
1058             value = value.resolve_expression(self, **kwargs)
1059         elif isinstance(value, (list, tuple)):
1060             # The items of the iterable may be expressions and therefore need
1061             # to be resolved independently.
1062             resolved_values = []
1063             for sub_value in value:
1064                 if hasattr(sub_value, 'resolve_expression'):
1065                     if isinstance(sub_value, F):
1066                         resolved_values.append(sub_value.resolve_expression(
1067                             self, reuse=can_reuse, allow_joins=allow_joins,
1068                             simple_col=simple_col,
1069                         ))
1070                     else:
1071                         resolved_values.append(sub_value.resolve_expression(
1072                             self, reuse=can_reuse, allow_joins=allow_joins,
1073                         ))
1074                 else:
1075                     resolved_values.append(sub_value)
1076             value = tuple(resolved_values)
1077         return value
1078 
1079     def solve_lookup_type(self, lookup):
1080         """
1081         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').
1082         """
1083         lookup_splitted = lookup.split(LOOKUP_SEP)
1084         if self.annotations:
1085             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1086             if expression:
1087                 return expression_lookups, (), expression
1088         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1089         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1090         if len(lookup_parts) > 1 and not field_parts:
1091             raise FieldError(
1092                 'Invalid lookup "%s" for model %s".' %
1093                 (lookup, self.get_meta().model.__name__)
1094             )
1095         return lookup_parts, field_parts, False
1096 
1097     def check_query_object_type(self, value, opts, field):
1098         """
1099         Check whether the object passed while querying is of the correct type.
1100         If not, raise a ValueError specifying the wrong object.
1101         """
1102         if hasattr(value, '_meta'):
1103             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1104                 raise ValueError(
1105                     'Cannot query "%s": Must be "%s" instance.' %
1106                     (value, opts.object_name))
1107 
1108     def check_related_objects(self, field, value, opts):
1109         """Check the type of object passed to query relations."""
1110         if field.is_relation:
1111             # Check that the field and the queryset use the same model in a
1112             # query like .filter(author=Author.objects.all()). For example, the
1113             # opts would be Author's (from the author field) and value.model
1114             # would be Author.objects.all() queryset's .model (Author also).
1115             # The field is the related field on the lhs side.
1116             if (isinstance(value, Query) and not value.has_select_fields and
1117                     not check_rel_lookup_compatibility(value.model, opts, field)):
1118                 raise ValueError(
1119                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1120                     (value.model._meta.object_name, opts.object_name)
1121                 )
1122             elif hasattr(value, '_meta'):
1123                 self.check_query_object_type(value, opts, field)
1124             elif hasattr(value, '__iter__'):
1125                 for v in value:
1126                     self.check_query_object_type(v, opts, field)
1127 
1128     def check_filterable(self, expression):
1129         """Raise an error if expression cannot be used in a WHERE clause."""
1130         if not getattr(expression, 'filterable', 'True'):
1131             raise NotSupportedError(
1132                 expression.__class__.__name__ + ' is disallowed in the filter '
1133                 'clause.'
1134             )
1135         if hasattr(expression, 'get_source_expressions'):
1136             for expr in expression.get_source_expressions():
1137                 self.check_filterable(expr)
1138 
1139     def build_lookup(self, lookups, lhs, rhs):
1140         """
1141         Try to extract transforms and lookup from given lhs.
1142 
1143         The lhs value is something that works like SQLExpression.
1144         The rhs value is what the lookup is going to compare against.
1145         The lookups is a list of names to extract using get_lookup()
1146         and get_transform().
1147         """
1148         # __exact is the default lookup if one isn't given.
1149         *transforms, lookup_name = lookups or ['exact']
1150         for name in transforms:
1151             lhs = self.try_transform(lhs, name)
1152         # First try get_lookup() so that the lookup takes precedence if the lhs
1153         # supports both transform and lookup for the name.
1154         lookup_class = lhs.get_lookup(lookup_name)
1155         if not lookup_class:
1156             if lhs.field.is_relation:
1157                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))
1158             # A lookup wasn't found. Try to interpret the name as a transform
1159             # and do an Exact lookup against it.
1160             lhs = self.try_transform(lhs, lookup_name)
1161             lookup_name = 'exact'
1162             lookup_class = lhs.get_lookup(lookup_name)
1163             if not lookup_class:
1164                 return
1165 
1166         lookup = lookup_class(lhs, rhs)
1167         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
1168         # uses of None as a query value unless the lookup supports it.
1169         if lookup.rhs is None and not lookup.can_use_none_as_rhs:
1170             if lookup_name not in ('exact', 'iexact'):
1171                 raise ValueError("Cannot use None as a query value")
1172             return lhs.get_lookup('isnull')(lhs, True)
1173 
1174         # For Oracle '' is equivalent to null. The check must be done at this
1175         # stage because join promotion can't be done in the compiler. Using
1176         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.
1177         # A similar thing is done in is_nullable(), too.
1178         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1179                 lookup_name == 'exact' and lookup.rhs == ''):
1180             return lhs.get_lookup('isnull')(lhs, True)
1181 
1182         return lookup
1183 
1184     def try_transform(self, lhs, name):
1185         """
1186         Helper method for build_lookup(). Try to fetch and initialize
1187         a transform for name parameter from lhs.
1188         """
1189         transform_class = lhs.get_transform(name)
1190         if transform_class:
1191             return transform_class(lhs)
1192         else:
1193             output_field = lhs.output_field.__class__
1194             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())
1195             if suggested_lookups:
1196                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)
1197             else:
1198                 suggestion = '.'
1199             raise FieldError(
1200                 "Unsupported lookup '%s' for %s or join on the field not "
1201                 "permitted%s" % (name, output_field.__name__, suggestion)
1202             )
1203 
1204     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1205                      can_reuse=None, allow_joins=True, split_subq=True,
1206                      reuse_with_filtered_relation=False, simple_col=False):
1207         """
1208         Build a WhereNode for a single filter clause but don't add it
1209         to this Query. Query.add_q() will then add this filter to the where
1210         Node.
1211 
1212         The 'branch_negated' tells us if the current branch contains any
1213         negations. This will be used to determine if subqueries are needed.
1214 
1215         The 'current_negated' is used to determine if the current filter is
1216         negated or not and this will be used to determine if IS NULL filtering
1217         is needed.
1218 
1219         The difference between current_negated and branch_negated is that
1220         branch_negated is set on first negation, but current_negated is
1221         flipped for each negation.
1222 
1223         Note that add_filter will not do any negating itself, that is done
1224         upper in the code by add_q().
1225 
1226         The 'can_reuse' is a set of reusable joins for multijoins.
1227 
1228         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse
1229         will be reused.
1230 
1231         The method will create a filter clause that can be added to the current
1232         query. However, if the filter isn't added to the query then the caller
1233         is responsible for unreffing the joins used.
1234         """
1235         if isinstance(filter_expr, dict):
1236             raise FieldError("Cannot parse keyword query as dict")
1237         if hasattr(filter_expr, 'resolve_expression') and getattr(filter_expr, 'conditional', False):
1238             if connections[DEFAULT_DB_ALIAS].ops.conditional_expression_supported_in_where_clause(filter_expr):
1239                 condition = filter_expr.resolve_expression(self)
1240             else:
1241                 # Expression is not supported in the WHERE clause, add
1242                 # comparison with True.
1243                 condition = self.build_lookup(['exact'], filter_expr.resolve_expression(self), True)
1244             clause = self.where_class()
1245             clause.add(condition, AND)
1246             return clause, []
1247         arg, value = filter_expr
1248         if not arg:
1249             raise FieldError("Cannot parse keyword query %r" % arg)
1250         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1251 
1252         self.check_filterable(reffed_expression)
1253 
1254         if not allow_joins and len(parts) > 1:
1255             raise FieldError("Joined field references are not permitted in this query")
1256 
1257         pre_joins = self.alias_refcount.copy()
1258         value = self.resolve_lookup_value(value, can_reuse, allow_joins, simple_col)
1259         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1260 
1261         self.check_filterable(value)
1262 
1263         clause = self.where_class()
1264         if reffed_expression:
1265             condition = self.build_lookup(lookups, reffed_expression, value)
1266             clause.add(condition, AND)
1267             return clause, []
1268 
1269         opts = self.get_meta()
1270         alias = self.get_initial_alias()
1271         allow_many = not branch_negated or not split_subq
1272 
1273         try:
1274             join_info = self.setup_joins(
1275                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1276                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1277             )
1278 
1279             # Prevent iterator from being consumed by check_related_objects()
1280             if isinstance(value, Iterator):
1281                 value = list(value)
1282             self.check_related_objects(join_info.final_field, value, join_info.opts)
1283 
1284             # split_exclude() needs to know which joins were generated for the
1285             # lookup parts
1286             self._lookup_joins = join_info.joins
1287         except MultiJoin as e:
1288             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1289 
1290         # Update used_joins before trimming since they are reused to determine
1291         # which joins could be later promoted to INNER.
1292         used_joins.update(join_info.joins)
1293         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1294         if can_reuse is not None:
1295             can_reuse.update(join_list)
1296 
1297         if join_info.final_field.is_relation:
1298             # No support for transforms for relational fields
1299             num_lookups = len(lookups)
1300             if num_lookups > 1:
1301                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1302             if len(targets) == 1:
1303                 col = _get_col(targets[0], join_info.final_field, alias, simple_col)
1304             else:
1305                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1306         else:
1307             col = _get_col(targets[0], join_info.final_field, alias, simple_col)
1308 
1309         condition = self.build_lookup(lookups, col, value)
1310         lookup_type = condition.lookup_name
1311         clause.add(condition, AND)
1312 
1313         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1314         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1315             require_outer = True
1316             if (lookup_type != 'isnull' and (
1317                     self.is_nullable(targets[0]) or
1318                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1319                 # The condition added here will be SQL like this:
1320                 # NOT (col IS NOT NULL), where the first NOT is added in
1321                 # upper layers of code. The reason for addition is that if col
1322                 # is null, then col != someval will result in SQL "unknown"
1323                 # which isn't the same as in Python. The Python None handling
1324                 # is wanted, and it can be gotten by
1325                 # (col IS NULL OR col != someval)
1326                 #   <=>
1327                 # NOT (col IS NOT NULL AND col = someval).
1328                 lookup_class = targets[0].get_lookup('isnull')
1329                 col = _get_col(targets[0], join_info.targets[0], alias, simple_col)
1330                 clause.add(lookup_class(col, False), AND)
1331         return clause, used_joins if not require_outer else ()
1332 
1333     def add_filter(self, filter_clause):
1334         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1335 
1336     def add_q(self, q_object):
1337         """
1338         A preprocessor for the internal _add_q(). Responsible for doing final
1339         join promotion.
1340         """
1341         # For join promotion this case is doing an AND for the added q_object
1342         # and existing conditions. So, any existing inner join forces the join
1343         # type to remain inner. Existing outer joins can however be demoted.
1344         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1345         # rel_a doesn't produce any rows, then the whole condition must fail.
1346         # So, demotion is OK.
1347         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}
1348         clause, _ = self._add_q(q_object, self.used_aliases)
1349         if clause:
1350             self.where.add(clause, AND)
1351         self.demote_joins(existing_inner)
1352 
1353     def build_where(self, q_object):
1354         return self._add_q(q_object, used_aliases=set(), allow_joins=False, simple_col=True)[0]
1355 
1356     def _add_q(self, q_object, used_aliases, branch_negated=False,
1357                current_negated=False, allow_joins=True, split_subq=True,
1358                simple_col=False):
1359         """Add a Q-object to the current filter."""
1360         connector = q_object.connector
1361         current_negated = current_negated ^ q_object.negated
1362         branch_negated = branch_negated or q_object.negated
1363         target_clause = self.where_class(connector=connector,
1364                                          negated=q_object.negated)
1365         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1366         for child in q_object.children:
1367             if isinstance(child, Node):
1368                 child_clause, needed_inner = self._add_q(
1369                     child, used_aliases, branch_negated,
1370                     current_negated, allow_joins, split_subq, simple_col)
1371                 joinpromoter.add_votes(needed_inner)
1372             else:
1373                 child_clause, needed_inner = self.build_filter(
1374                     child, can_reuse=used_aliases, branch_negated=branch_negated,
1375                     current_negated=current_negated, allow_joins=allow_joins,
1376                     split_subq=split_subq, simple_col=simple_col,
1377                 )
1378                 joinpromoter.add_votes(needed_inner)
1379             if child_clause:
1380                 target_clause.add(child_clause, connector)
1381         needed_inner = joinpromoter.update_join_types(self)
1382         return target_clause, needed_inner
1383 
1384     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):
1385         """Add a FilteredRelation object to the current filter."""
1386         connector = q_object.connector
1387         current_negated ^= q_object.negated
1388         branch_negated = branch_negated or q_object.negated
1389         target_clause = self.where_class(connector=connector, negated=q_object.negated)
1390         for child in q_object.children:
1391             if isinstance(child, Node):
1392                 child_clause = self.build_filtered_relation_q(
1393                     child, reuse=reuse, branch_negated=branch_negated,
1394                     current_negated=current_negated,
1395                 )
1396             else:
1397                 child_clause, _ = self.build_filter(
1398                     child, can_reuse=reuse, branch_negated=branch_negated,
1399                     current_negated=current_negated,
1400                     allow_joins=True, split_subq=False,
1401                     reuse_with_filtered_relation=True,
1402                 )
1403             target_clause.add(child_clause, connector)
1404         return target_clause
1405 
1406     def add_filtered_relation(self, filtered_relation, alias):
1407         filtered_relation.alias = alias
1408         lookups = dict(get_children_from_q(filtered_relation.condition))
1409         for lookup in chain((filtered_relation.relation_name,), lookups):
1410             lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)
1411             shift = 2 if not lookup_parts else 1
1412             if len(field_parts) > (shift + len(lookup_parts)):
1413                 raise ValueError(
1414                     "FilteredRelation's condition doesn't support nested "
1415                     "relations (got %r)." % lookup
1416                 )
1417         self._filtered_relations[filtered_relation.alias] = filtered_relation
1418 
1419     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1420         """
1421         Walk the list of names and turns them into PathInfo tuples. A single
1422         name in 'names' can generate multiple PathInfos (m2m, for example).
1423 
1424         'names' is the path of names to travel, 'opts' is the model Options we
1425         start the name resolving from, 'allow_many' is as for setup_joins().
1426         If fail_on_missing is set to True, then a name that can't be resolved
1427         will generate a FieldError.
1428 
1429         Return a list of PathInfo tuples. In addition return the final field
1430         (the last used join field) and target (which is a field guaranteed to
1431         contain the same value as the final field). Finally, return those names
1432         that weren't found (which are likely transforms and the final lookup).
1433         """
1434         path, names_with_path = [], []
1435         for pos, name in enumerate(names):
1436             cur_names_with_path = (name, [])
1437             if name == 'pk':
1438                 name = opts.pk.name
1439 
1440             field = None
1441             filtered_relation = None
1442             try:
1443                 field = opts.get_field(name)
1444             except FieldDoesNotExist:
1445                 if name in self.annotation_select:
1446                     field = self.annotation_select[name].output_field
1447                 elif name in self._filtered_relations and pos == 0:
1448                     filtered_relation = self._filtered_relations[name]
1449                     field = opts.get_field(filtered_relation.relation_name)
1450             if field is not None:
1451                 # Fields that contain one-to-many relations with a generic
1452                 # model (like a GenericForeignKey) cannot generate reverse
1453                 # relations and therefore cannot be used for reverse querying.
1454                 if field.is_relation and not field.related_model:
1455                     raise FieldError(
1456                         "Field %r does not generate an automatic reverse "
1457                         "relation and therefore cannot be used for reverse "
1458                         "querying. If it is a GenericForeignKey, consider "
1459                         "adding a GenericRelation." % name
1460                     )
1461                 try:
1462                     model = field.model._meta.concrete_model
1463                 except AttributeError:
1464                     # QuerySet.annotate() may introduce fields that aren't
1465                     # attached to a model.
1466                     model = None
1467             else:
1468                 # We didn't find the current field, so move position back
1469                 # one step.
1470                 pos -= 1
1471                 if pos == -1 or fail_on_missing:
1472                     available = sorted([
1473                         *get_field_names_from_opts(opts),
1474                         *self.annotation_select,
1475                         *self._filtered_relations,
1476                     ])
1477                     raise FieldError("Cannot resolve keyword '%s' into field. "
1478                                      "Choices are: %s" % (name, ", ".join(available)))
1479                 break
1480             # Check if we need any joins for concrete inheritance cases (the
1481             # field lives in parent, but we are currently in one of its
1482             # children)
1483             if model is not opts.model:
1484                 path_to_parent = opts.get_path_to_parent(model)
1485                 if path_to_parent:
1486                     path.extend(path_to_parent)
1487                     cur_names_with_path[1].extend(path_to_parent)
1488                     opts = path_to_parent[-1].to_opts
1489             if hasattr(field, 'get_path_info'):
1490                 pathinfos = field.get_path_info(filtered_relation)
1491                 if not allow_many:
1492                     for inner_pos, p in enumerate(pathinfos):
1493                         if p.m2m:
1494                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1495                             names_with_path.append(cur_names_with_path)
1496                             raise MultiJoin(pos + 1, names_with_path)
1497                 last = pathinfos[-1]
1498                 path.extend(pathinfos)
1499                 final_field = last.join_field
1500                 opts = last.to_opts
1501                 targets = last.target_fields
1502                 cur_names_with_path[1].extend(pathinfos)
1503                 names_with_path.append(cur_names_with_path)
1504             else:
1505                 # Local non-relational field.
1506                 final_field = field
1507                 targets = (field,)
1508                 if fail_on_missing and pos + 1 != len(names):
1509                     raise FieldError(
1510                         "Cannot resolve keyword %r into field. Join on '%s'"
1511                         " not permitted." % (names[pos + 1], name))
1512                 break
1513         return path, final_field, targets, names[pos + 1:]
1514 
1515     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,
1516                     reuse_with_filtered_relation=False):
1517         """
1518         Compute the necessary table joins for the passage through the fields
1519         given in 'names'. 'opts' is the Options class for the current model
1520         (which gives the table we are starting from), 'alias' is the alias for
1521         the table to start the joining from.
1522 
1523         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1524         can be None in which case all joins are reusable or a set of aliases
1525         that can be reused. Note that non-reverse foreign keys are always
1526         reusable when using setup_joins().
1527 
1528         The 'reuse_with_filtered_relation' can be used to force 'can_reuse'
1529         parameter and force the relation on the given connections.
1530 
1531         If 'allow_many' is False, then any reverse foreign key seen will
1532         generate a MultiJoin exception.
1533 
1534         Return the final field involved in the joins, the target field (used
1535         for any 'where' constraint), the final 'opts' value, the joins, the
1536         field path traveled to generate the joins, and a transform function
1537         that takes a field and alias and is equivalent to `field.get_col(alias)`
1538         in the simple case but wraps field transforms if they were included in
1539         names.
1540 
1541         The target field is the field containing the concrete value. Final
1542         field can be something different, for example foreign key pointing to
1543         that value. Final field is needed for example in some value
1544         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1545         key field for example).
1546         """
1547         joins = [alias]
1548         # The transform can't be applied yet, as joins must be trimmed later.
1549         # To avoid making every caller of this method look up transforms
1550         # directly, compute transforms here and create a partial that converts
1551         # fields to the appropriate wrapped version.
1552 
1553         def final_transformer(field, alias):
1554             return field.get_col(alias)
1555 
1556         # Try resolving all the names as fields first. If there's an error,
1557         # treat trailing names as lookups until a field can be resolved.
1558         last_field_exception = None
1559         for pivot in range(len(names), 0, -1):
1560             try:
1561                 path, final_field, targets, rest = self.names_to_path(
1562                     names[:pivot], opts, allow_many, fail_on_missing=True,
1563                 )
1564             except FieldError as exc:
1565                 if pivot == 1:
1566                     # The first item cannot be a lookup, so it's safe
1567                     # to raise the field error here.
1568                     raise
1569                 else:
1570                     last_field_exception = exc
1571             else:
1572                 # The transforms are the remaining items that couldn't be
1573                 # resolved into fields.
1574                 transforms = names[pivot:]
1575                 break
1576         for name in transforms:
1577             def transform(field, alias, *, name, previous):
1578                 try:
1579                     wrapped = previous(field, alias)
1580                     return self.try_transform(wrapped, name)
1581                 except FieldError:
1582                     # FieldError is raised if the transform doesn't exist.
1583                     if isinstance(final_field, Field) and last_field_exception:
1584                         raise last_field_exception
1585                     else:
1586                         raise
1587             final_transformer = functools.partial(transform, name=name, previous=final_transformer)
1588         # Then, add the path to the query's joins. Note that we can't trim
1589         # joins at this stage - we will need the information about join type
1590         # of the trimmed joins.
1591         for join in path:
1592             if join.filtered_relation:
1593                 filtered_relation = join.filtered_relation.clone()
1594                 table_alias = filtered_relation.alias
1595             else:
1596                 filtered_relation = None
1597                 table_alias = None
1598             opts = join.to_opts
1599             if join.direct:
1600                 nullable = self.is_nullable(join.join_field)
1601             else:
1602                 nullable = True
1603             connection = Join(
1604                 opts.db_table, alias, table_alias, INNER, join.join_field,
1605                 nullable, filtered_relation=filtered_relation,
1606             )
1607             reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None
1608             alias = self.join(
1609                 connection, reuse=reuse,
1610                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1611             )
1612             joins.append(alias)
1613             if filtered_relation:
1614                 filtered_relation.path = joins[:]
1615         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)
1616 
1617     def trim_joins(self, targets, joins, path):
1618         """
1619         The 'target' parameter is the final field being joined to, 'joins'
1620         is the full list of join aliases. The 'path' contain the PathInfos
1621         used to create the joins.
1622 
1623         Return the final target field and table alias and the new active
1624         joins.
1625 
1626         Always trim any direct join if the target column is already in the
1627         previous table. Can't trim reverse joins as it's unknown if there's
1628         anything on the other side of the join.
1629         """
1630         joins = joins[:]
1631         for pos, info in enumerate(reversed(path)):
1632             if len(joins) == 1 or not info.direct:
1633                 break
1634             if info.filtered_relation:
1635                 break
1636             join_targets = {t.column for t in info.join_field.foreign_related_fields}
1637             cur_targets = {t.column for t in targets}
1638             if not cur_targets.issubset(join_targets):
1639                 break
1640             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1641             targets = tuple(targets_dict[t.column] for t in targets)
1642             self.unref_alias(joins.pop())
1643         return targets, joins[-1], joins
1644 
1645     @classmethod
1646     def _gen_col_aliases(cls, exprs):
1647         for expr in exprs:
1648             if isinstance(expr, Col):
1649                 yield expr.alias
1650             else:
1651                 yield from cls._gen_col_aliases(expr.get_source_expressions())
1652 
1653     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False, simple_col=False):
1654         if not allow_joins and LOOKUP_SEP in name:
1655             raise FieldError("Joined field references are not permitted in this query")
1656         annotation = self.annotations.get(name)
1657         if annotation is not None:
1658             if not allow_joins:
1659                 for alias in self._gen_col_aliases([annotation]):
1660                     if isinstance(self.alias_map[alias], Join):
1661                         raise FieldError(
1662                             'Joined field references are not permitted in '
1663                             'this query'
1664                         )
1665             if summarize:
1666                 # Summarize currently means we are doing an aggregate() query
1667                 # which is executed as a wrapped subquery if any of the
1668                 # aggregate() elements reference an existing annotation. In
1669                 # that case we need to return a Ref to the subquery's annotation.
1670                 return Ref(name, self.annotation_select[name])
1671             else:
1672                 return annotation
1673         else:
1674             field_list = name.split(LOOKUP_SEP)
1675             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)
1676             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1677             if not allow_joins and len(join_list) > 1:
1678                 raise FieldError('Joined field references are not permitted in this query')
1679             if len(targets) > 1:
1680                 raise FieldError("Referencing multicolumn fields with F() objects "
1681                                  "isn't supported")
1682             # Verify that the last lookup in name is a field or a transform:
1683             # transform_function() raises FieldError if not.
1684             join_info.transform_function(targets[0], final_alias)
1685             if reuse is not None:
1686                 reuse.update(join_list)
1687             col = _get_col(targets[0], join_info.targets[0], join_list[-1], simple_col)
1688             return col
1689 
1690     def split_exclude(self, filter_expr, can_reuse, names_with_path):
1691         """
1692         When doing an exclude against any kind of N-to-many relation, we need
1693         to use a subquery. This method constructs the nested query, given the
1694         original exclude filter (filter_expr) and the portion up to the first
1695         N-to-many relation field.
1696 
1697         For example, if the origin filter is ~Q(child__name='foo'), filter_expr
1698         is ('child__name', 'foo') and can_reuse is a set of joins usable for
1699         filters in the original query.
1700 
1701         We will turn this into equivalent of:
1702             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1703                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1704 
1705         It might be worth it to consider using WHERE NOT EXISTS as that has
1706         saner null handling, and is easier for the backend's optimizer to
1707         handle.
1708         """
1709         filter_lhs, filter_rhs = filter_expr
1710         if isinstance(filter_rhs, OuterRef):
1711             filter_expr = (filter_lhs, OuterRef(filter_rhs))
1712         elif isinstance(filter_rhs, F):
1713             filter_expr = (filter_lhs, OuterRef(filter_rhs.name))
1714         # Generate the inner query.
1715         query = Query(self.model)
1716         query._filtered_relations = self._filtered_relations
1717         query.add_filter(filter_expr)
1718         query.clear_ordering(True)
1719         # Try to have as simple as possible subquery -> trim leading joins from
1720         # the subquery.
1721         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1722 
1723         # Add extra check to make sure the selected field will not be null
1724         # since we are adding an IN <subquery> clause. This prevents the
1725         # database from tripping over IN (...,NULL,...) selects and returning
1726         # nothing
1727         col = query.select[0]
1728         select_field = col.target
1729         alias = col.alias
1730         if self.is_nullable(select_field):
1731             lookup_class = select_field.get_lookup('isnull')
1732             lookup = lookup_class(select_field.get_col(alias), False)
1733             query.where.add(lookup, AND)
1734         if alias in can_reuse:
1735             pk = select_field.model._meta.pk
1736             # Need to add a restriction so that outer query's filters are in effect for
1737             # the subquery, too.
1738             query.bump_prefix(self)
1739             lookup_class = select_field.get_lookup('exact')
1740             # Note that the query.select[0].alias is different from alias
1741             # due to bump_prefix above.
1742             lookup = lookup_class(pk.get_col(query.select[0].alias),
1743                                   pk.get_col(alias))
1744             query.where.add(lookup, AND)
1745             query.external_aliases.add(alias)
1746 
1747         condition, needed_inner = self.build_filter(
1748             ('%s__in' % trimmed_prefix, query),
1749             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1750         if contains_louter:
1751             or_null_condition, _ = self.build_filter(
1752                 ('%s__isnull' % trimmed_prefix, True),
1753                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1754             condition.add(or_null_condition, OR)
1755             # Note that the end result will be:
1756             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1757             # This might look crazy but due to how IN works, this seems to be
1758             # correct. If the IS NOT NULL check is removed then outercol NOT
1759             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1760             # outercol IS NULL we will not match the row.
1761         return condition, needed_inner
1762 
1763     def set_empty(self):
1764         self.where.add(NothingNode(), AND)
1765 
1766     def is_empty(self):
1767         return any(isinstance(c, NothingNode) for c in self.where.children)
1768 
1769     def set_limits(self, low=None, high=None):
1770         """
1771         Adjust the limits on the rows retrieved. Use low/high to set these,
1772         as it makes it more Pythonic to read and write. When the SQL query is
1773         created, convert them to the appropriate offset and limit values.
1774 
1775         Apply any limits passed in here to the existing constraints. Add low
1776         to the current low value and clamp both to any existing high value.
1777         """
1778         if high is not None:
1779             if self.high_mark is not None:
1780                 self.high_mark = min(self.high_mark, self.low_mark + high)
1781             else:
1782                 self.high_mark = self.low_mark + high
1783         if low is not None:
1784             if self.high_mark is not None:
1785                 self.low_mark = min(self.high_mark, self.low_mark + low)
1786             else:
1787                 self.low_mark = self.low_mark + low
1788 
1789         if self.low_mark == self.high_mark:
1790             self.set_empty()
1791 
1792     def clear_limits(self):
1793         """Clear any existing limits."""
1794         self.low_mark, self.high_mark = 0, None
1795 
1796     @property
1797     def is_sliced(self):
1798         return self.low_mark != 0 or self.high_mark is not None
1799 
1800     def has_limit_one(self):
1801         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1
1802 
1803     def can_filter(self):
1804         """
1805         Return True if adding filters to this instance is still possible.
1806 
1807         Typically, this means no limits or offsets have been put on the results.
1808         """
1809         return not self.is_sliced
1810 
1811     def clear_select_clause(self):
1812         """Remove all fields from SELECT clause."""
1813         self.select = ()
1814         self.default_cols = False
1815         self.select_related = False
1816         self.set_extra_mask(())
1817         self.set_annotation_mask(())
1818 
1819     def clear_select_fields(self):
1820         """
1821         Clear the list of fields to select (but not extra_select columns).
1822         Some queryset types completely replace any existing list of select
1823         columns.
1824         """
1825         self.select = ()
1826         self.values_select = ()
1827 
1828     def add_select_col(self, col):
1829         self.select += col,
1830         self.values_select += col.output_field.name,
1831 
1832     def set_select(self, cols):
1833         self.default_cols = False
1834         self.select = tuple(cols)
1835 
1836     def add_distinct_fields(self, *field_names):
1837         """
1838         Add and resolve the given fields to the query's "distinct on" clause.
1839         """
1840         self.distinct_fields = field_names
1841         self.distinct = True
1842 
1843     def add_fields(self, field_names, allow_m2m=True):
1844         """
1845         Add the given (model) fields to the select set. Add the field names in
1846         the order specified.
1847         """
1848         alias = self.get_initial_alias()
1849         opts = self.get_meta()
1850 
1851         try:
1852             cols = []
1853             for name in field_names:
1854                 # Join promotion note - we must not remove any rows here, so
1855                 # if there is no existing joins, use outer join.
1856                 join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1857                 targets, final_alias, joins = self.trim_joins(
1858                     join_info.targets,
1859                     join_info.joins,
1860                     join_info.path,
1861                 )
1862                 for target in targets:
1863                     cols.append(join_info.transform_function(target, final_alias))
1864             if cols:
1865                 self.set_select(cols)
1866         except MultiJoin:
1867             raise FieldError("Invalid field name: '%s'" % name)
1868         except FieldError:
1869             if LOOKUP_SEP in name:
1870                 # For lookups spanning over relationships, show the error
1871                 # from the model on which the lookup failed.
1872                 raise
1873             else:
1874                 names = sorted([
1875                     *get_field_names_from_opts(opts), *self.extra,
1876                     *self.annotation_select, *self._filtered_relations
1877                 ])
1878                 raise FieldError("Cannot resolve keyword %r into field. "
1879                                  "Choices are: %s" % (name, ", ".join(names)))
1880 
1881     def add_ordering(self, *ordering):
1882         """
1883         Add items from the 'ordering' sequence to the query's "order by"
1884         clause. These items are either field names (not column names) --
1885         possibly with a direction prefix ('-' or '?') -- or OrderBy
1886         expressions.
1887 
1888         If 'ordering' is empty, clear all ordering from the query.
1889         """
1890         errors = []
1891         for item in ordering:
1892             if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):
1893                 errors.append(item)
1894             if getattr(item, 'contains_aggregate', False):
1895                 raise FieldError(
1896                     'Using an aggregate in order_by() without also including '
1897                     'it in annotate() is not allowed: %s' % item
1898                 )
1899         if errors:
1900             raise FieldError('Invalid order_by arguments: %s' % errors)
1901         if ordering:
1902             self.order_by += ordering
1903         else:
1904             self.default_ordering = False
1905 
1906     def clear_ordering(self, force_empty):
1907         """
1908         Remove any ordering settings. If 'force_empty' is True, there will be
1909         no ordering in the resulting query (not even the model's default).
1910         """
1911         self.order_by = ()
1912         self.extra_order_by = ()
1913         if force_empty:
1914             self.default_ordering = False
1915 
1916     def set_group_by(self):
1917         """
1918         Expand the GROUP BY clause required by the query.
1919 
1920         This will usually be the set of all non-aggregate fields in the
1921         return data. If the database backend supports grouping by the
1922         primary key, and the query would be equivalent, the optimization
1923         will be made automatically.
1924         """
1925         group_by = list(self.select)
1926         if self.annotation_select:
1927             for alias, annotation in self.annotation_select.items():
1928                 try:
1929                     inspect.getcallargs(annotation.get_group_by_cols, alias=alias)
1930                 except TypeError:
1931                     annotation_class = annotation.__class__
1932                     msg = (
1933                         '`alias=None` must be added to the signature of '
1934                         '%s.%s.get_group_by_cols().'
1935                     ) % (annotation_class.__module__, annotation_class.__qualname__)
1936                     warnings.warn(msg, category=RemovedInDjango40Warning)
1937                     group_by_cols = annotation.get_group_by_cols()
1938                 else:
1939                     group_by_cols = annotation.get_group_by_cols(alias=alias)
1940                 group_by.extend(group_by_cols)
1941         self.group_by = tuple(group_by)
1942 
1943     def add_select_related(self, fields):
1944         """
1945         Set up the select_related data structure so that we only select
1946         certain related models (as opposed to all models, when
1947         self.select_related=True).
1948         """
1949         if isinstance(self.select_related, bool):
1950             field_dict = {}
1951         else:
1952             field_dict = self.select_related
1953         for field in fields:
1954             d = field_dict
1955             for part in field.split(LOOKUP_SEP):
1956                 d = d.setdefault(part, {})
1957         self.select_related = field_dict
1958 
1959     def add_extra(self, select, select_params, where, params, tables, order_by):
1960         """
1961         Add data to the various extra_* attributes for user-created additions
1962         to the query.
1963         """
1964         if select:
1965             # We need to pair any placeholder markers in the 'select'
1966             # dictionary with their parameters in 'select_params' so that
1967             # subsequent updates to the select dictionary also adjust the
1968             # parameters appropriately.
1969             select_pairs = {}
1970             if select_params:
1971                 param_iter = iter(select_params)
1972             else:
1973                 param_iter = iter([])
1974             for name, entry in select.items():
1975                 entry = str(entry)
1976                 entry_params = []
1977                 pos = entry.find("%s")
1978                 while pos != -1:
1979                     if pos == 0 or entry[pos - 1] != '%':
1980                         entry_params.append(next(param_iter))
1981                     pos = entry.find("%s", pos + 2)
1982                 select_pairs[name] = (entry, entry_params)
1983             self.extra.update(select_pairs)
1984         if where or params:
1985             self.where.add(ExtraWhere(where, params), AND)
1986         if tables:
1987             self.extra_tables += tuple(tables)
1988         if order_by:
1989             self.extra_order_by = order_by
1990 
1991     def clear_deferred_loading(self):
1992         """Remove any fields from the deferred loading set."""
1993         self.deferred_loading = (frozenset(), True)
1994 
1995     def add_deferred_loading(self, field_names):
1996         """
1997         Add the given list of model field names to the set of fields to
1998         exclude from loading from the database when automatic column selection
1999         is done. Add the new field names to any existing field names that
2000         are deferred (or removed from any existing field names that are marked
2001         as the only ones for immediate loading).
2002         """
2003         # Fields on related models are stored in the literal double-underscore
2004         # format, so that we can use a set datastructure. We do the foo__bar
2005         # splitting and handling when computing the SQL column names (as part of
2006         # get_columns()).
2007         existing, defer = self.deferred_loading
2008         if defer:
2009             # Add to existing deferred names.
2010             self.deferred_loading = existing.union(field_names), True
2011         else:
2012             # Remove names from the set of any existing "immediate load" names.
2013             self.deferred_loading = existing.difference(field_names), False
2014 
2015     def add_immediate_loading(self, field_names):
2016         """
2017         Add the given list of model field names to the set of fields to
2018         retrieve when the SQL is executed ("immediate loading" fields). The
2019         field names replace any existing immediate loading field names. If
2020         there are field names already specified for deferred loading, remove
2021         those names from the new field_names before storing the new names
2022         for immediate loading. (That is, immediate loading overrides any
2023         existing immediate values, but respects existing deferrals.)
2024         """
2025         existing, defer = self.deferred_loading
2026         field_names = set(field_names)
2027         if 'pk' in field_names:
2028             field_names.remove('pk')
2029             field_names.add(self.get_meta().pk.name)
2030 
2031         if defer:
2032             # Remove any existing deferred names from the current set before
2033             # setting the new names.
2034             self.deferred_loading = field_names.difference(existing), False
2035         else:
2036             # Replace any existing "immediate load" field names.
2037             self.deferred_loading = frozenset(field_names), False
2038 
2039     def get_loaded_field_names(self):
2040         """
2041         If any fields are marked to be deferred, return a dictionary mapping
2042         models to a set of names in those fields that will be loaded. If a
2043         model is not in the returned dictionary, none of its fields are
2044         deferred.
2045 
2046         If no fields are marked for deferral, return an empty dictionary.
2047         """
2048         # We cache this because we call this function multiple times
2049         # (compiler.fill_related_selections, query.iterator)
2050         try:
2051             return self._loaded_field_names_cache
2052         except AttributeError:
2053             collection = {}
2054             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
2055             self._loaded_field_names_cache = collection
2056             return collection
2057 
2058     def get_loaded_field_names_cb(self, target, model, fields):
2059         """Callback used by get_deferred_field_names()."""
2060         target[model] = {f.attname for f in fields}
2061 
2062     def set_annotation_mask(self, names):
2063         """Set the mask of annotations that will be returned by the SELECT."""
2064         if names is None:
2065             self.annotation_select_mask = None
2066         else:
2067             self.annotation_select_mask = set(names)
2068         self._annotation_select_cache = None
2069 
2070     def append_annotation_mask(self, names):
2071         if self.annotation_select_mask is not None:
2072             self.set_annotation_mask(self.annotation_select_mask.union(names))
2073 
2074     def set_extra_mask(self, names):
2075         """
2076         Set the mask of extra select items that will be returned by SELECT.
2077         Don't remove them from the Query since they might be used later.
2078         """
2079         if names is None:
2080             self.extra_select_mask = None
2081         else:
2082             self.extra_select_mask = set(names)
2083         self._extra_select_cache = None
2084 
2085     def set_values(self, fields):
2086         self.select_related = False
2087         self.clear_deferred_loading()
2088         self.clear_select_fields()
2089 
2090         if self.group_by is True:
2091             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
2092             self.set_group_by()
2093             self.clear_select_fields()
2094 
2095         if fields:
2096             field_names = []
2097             extra_names = []
2098             annotation_names = []
2099             if not self.extra and not self.annotations:
2100                 # Shortcut - if there are no extra or annotations, then
2101                 # the values() clause must be just field names.
2102                 field_names = list(fields)
2103             else:
2104                 self.default_cols = False
2105                 for f in fields:
2106                     if f in self.extra_select:
2107                         extra_names.append(f)
2108                     elif f in self.annotation_select:
2109                         annotation_names.append(f)
2110                     else:
2111                         field_names.append(f)
2112             self.set_extra_mask(extra_names)
2113             self.set_annotation_mask(annotation_names)
2114         else:
2115             field_names = [f.attname for f in self.model._meta.concrete_fields]
2116 
2117         self.values_select = tuple(field_names)
2118         self.add_fields(field_names, True)
2119 
2120     @property
2121     def annotation_select(self):
2122         """
2123         Return the dictionary of aggregate columns that are not masked and
2124         should be used in the SELECT clause. Cache this result for performance.
2125         """
2126         if self._annotation_select_cache is not None:
2127             return self._annotation_select_cache
2128         elif not self.annotations:
2129             return {}
2130         elif self.annotation_select_mask is not None:
2131             self._annotation_select_cache = {
2132                 k: v for k, v in self.annotations.items()
2133                 if k in self.annotation_select_mask
2134             }
2135             return self._annotation_select_cache
2136         else:
2137             return self.annotations
2138 
2139     @property
2140     def extra_select(self):
2141         if self._extra_select_cache is not None:
2142             return self._extra_select_cache
2143         if not self.extra:
2144             return {}
2145         elif self.extra_select_mask is not None:
2146             self._extra_select_cache = {
2147                 k: v for k, v in self.extra.items()
2148                 if k in self.extra_select_mask
2149             }
2150             return self._extra_select_cache
2151         else:
2152             return self.extra
2153 
2154     def trim_start(self, names_with_path):
2155         """
2156         Trim joins from the start of the join path. The candidates for trim
2157         are the PathInfos in names_with_path structure that are m2m joins.
2158 
2159         Also set the select column so the start matches the join.
2160 
2161         This method is meant to be used for generating the subquery joins &
2162         cols in split_exclude().
2163 
2164         Return a lookup usable for doing outerq.filter(lookup=self) and a
2165         boolean indicating if the joins in the prefix contain a LEFT OUTER join.
2166         _"""
2167         all_paths = []
2168         for _, paths in names_with_path:
2169             all_paths.extend(paths)
2170         contains_louter = False
2171         # Trim and operate only on tables that were generated for
2172         # the lookup part of the query. That is, avoid trimming
2173         # joins generated for F() expressions.
2174         lookup_tables = [
2175             t for t in self.alias_map
2176             if t in self._lookup_joins or t == self.base_table
2177         ]
2178         for trimmed_paths, path in enumerate(all_paths):
2179             if path.m2m:
2180                 break
2181             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
2182                 contains_louter = True
2183             alias = lookup_tables[trimmed_paths]
2184             self.unref_alias(alias)
2185         # The path.join_field is a Rel, lets get the other side's field
2186         join_field = path.join_field.field
2187         # Build the filter prefix.
2188         paths_in_prefix = trimmed_paths
2189         trimmed_prefix = []
2190         for name, path in names_with_path:
2191             if paths_in_prefix - len(path) < 0:
2192                 break
2193             trimmed_prefix.append(name)
2194             paths_in_prefix -= len(path)
2195         trimmed_prefix.append(
2196             join_field.foreign_related_fields[0].name)
2197         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
2198         # Lets still see if we can trim the first join from the inner query
2199         # (that is, self). We can't do this for:
2200         # - LEFT JOINs because we would miss those rows that have nothing on
2201         #   the outer side,
2202         # - INNER JOINs from filtered relations because we would miss their
2203         #   filters.
2204         first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]
2205         if first_join.join_type != LOUTER and not first_join.filtered_relation:
2206             select_fields = [r[0] for r in join_field.related_fields]
2207             select_alias = lookup_tables[trimmed_paths + 1]
2208             self.unref_alias(lookup_tables[trimmed_paths])
2209             extra_restriction = join_field.get_extra_restriction(
2210                 self.where_class, None, lookup_tables[trimmed_paths + 1])
2211             if extra_restriction:
2212                 self.where.add(extra_restriction, AND)
2213         else:
2214             # TODO: It might be possible to trim more joins from the start of the
2215             # inner query if it happens to have a longer join chain containing the
2216             # values in select_fields. Lets punt this one for now.
2217             select_fields = [r[1] for r in join_field.related_fields]
2218             select_alias = lookup_tables[trimmed_paths]
2219         # The found starting point is likely a Join instead of a BaseTable reference.
2220         # But the first entry in the query's FROM clause must not be a JOIN.
2221         for table in self.alias_map:
2222             if self.alias_refcount[table] > 0:
2223                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
2224                 break
2225         self.set_select([f.get_col(select_alias) for f in select_fields])
2226         return trimmed_prefix, contains_louter
2227 
2228     def is_nullable(self, field):
2229         """
2230         Check if the given field should be treated as nullable.
2231 
2232         Some backends treat '' as null and Django treats such fields as
2233         nullable for those backends. In such situations field.null can be
2234         False even if we should treat the field as nullable.
2235         """
2236         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2237         # (nor should it have) knowledge of which connection is going to be
2238         # used. The proper fix would be to defer all decisions where
2239         # is_nullable() is needed to the compiler stage, but that is not easy
2240         # to do currently.
2241         return (
2242             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
2243             field.empty_strings_allowed
2244         ) or field.null
2245 
2246 
2247 def get_order_dir(field, default='ASC'):
2248     """
2249     Return the field name and direction for an order specification. For
2250     example, '-foo' is returned as ('foo', 'DESC').
2251 
2252     The 'default' param is used to indicate which way no prefix (or a '+'
2253     prefix) should sort. The '-' prefix always sorts the opposite way.
2254     """
2255     dirn = ORDER_DIR[default]
2256     if field[0] == '-':
2257         return field[1:], dirn[1]
2258     return field, dirn[0]
2259 
2260 
2261 def add_to_dict(data, key, value):
2262     """
2263     Add "value" to the set of values for "key", whether or not "key" already
2264     exists.
2265     """
2266     if key in data:
2267         data[key].add(value)
2268     else:
2269         data[key] = {value}
2270 
2271 
2272 def is_reverse_o2o(field):
2273     """
2274     Check if the given field is reverse-o2o. The field is expected to be some
2275     sort of relation field or related object.
2276     """
2277     return field.is_relation and field.one_to_one and not field.concrete
2278 
2279 
2280 class JoinPromoter:
2281     """
2282     A class to abstract away join promotion problems for complex filter
2283     conditions.
2284     """
2285 
2286     def __init__(self, connector, num_children, negated):
2287         self.connector = connector
2288         self.negated = negated
2289         if self.negated:
2290             if connector == AND:
2291                 self.effective_connector = OR
2292             else:
2293                 self.effective_connector = AND
2294         else:
2295             self.effective_connector = self.connector
2296         self.num_children = num_children
2297         # Maps of table alias to how many times it is seen as required for
2298         # inner and/or outer joins.
2299         self.votes = Counter()
2300 
2301     def add_votes(self, votes):
2302         """
2303         Add single vote per item to self.votes. Parameter can be any
2304         iterable.
2305         """
2306         self.votes.update(votes)
2307 
2308     def update_join_types(self, query):
2309         """
2310         Change join types so that the generated query is as efficient as
2311         possible, but still correct. So, change as many joins as possible
2312         to INNER, but don't make OUTER joins INNER if that could remove
2313         results from the query.
2314         """
2315         to_promote = set()
2316         to_demote = set()
2317         # The effective_connector is used so that NOT (a AND b) is treated
2318         # similarly to (a OR b) for join promotion.
2319         for table, votes in self.votes.items():
2320             # We must use outer joins in OR case when the join isn't contained
2321             # in all of the joins. Otherwise the INNER JOIN itself could remove
2322             # valid results. Consider the case where a model with rel_a and
2323             # rel_b relations is queried with rel_a__col=1 | rel_b__col=2. Now,
2324             # if rel_a join doesn't produce any results is null (for example
2325             # reverse foreign key or null value in direct foreign key), and
2326             # there is a matching row in rel_b with col=2, then an INNER join
2327             # to rel_a would remove a valid match from the query. So, we need
2328             # to promote any existing INNER to LOUTER (it is possible this
2329             # promotion in turn will be demoted later on).
2330             if self.effective_connector == 'OR' and votes < self.num_children:
2331                 to_promote.add(table)
2332             # If connector is AND and there is a filter that can match only
2333             # when there is a joinable row, then use INNER. For example, in
2334             # rel_a__col=1 & rel_b__col=2, if either of the rels produce NULL
2335             # as join output, then the col=1 or col=2 can't match (as
2336             # NULL=anything is always false).
2337             # For the OR case, if all children voted for a join to be inner,
2338             # then we can use INNER for the join. For example:
2339             #     (rel_a__col__icontains=Alex | rel_a__col__icontains=Russell)
2340             # then if rel_a doesn't produce any rows, the whole condition
2341             # can't match. Hence we can safely use INNER join.
2342             if self.effective_connector == 'AND' or (
2343                     self.effective_connector == 'OR' and votes == self.num_children):
2344                 to_demote.add(table)
2345             # Finally, what happens in cases where we have:
2346             #    (rel_a__col=1|rel_b__col=2) & rel_a__col__gte=0
2347             # Now, we first generate the OR clause, and promote joins for it
2348             # in the first if branch above. Both rel_a and rel_b are promoted
2349             # to LOUTER joins. After that we do the AND case. The OR case
2350             # voted no inner joins but the rel_a__col__gte=0 votes inner join
2351             # for rel_a. We demote it back to INNER join (in AND case a single
2352             # vote is enough). The demotion is OK, if rel_a doesn't produce
2353             # rows, then the rel_a__col__gte=0 clause can't be true, and thus
2354             # the whole clause must be false. So, it is safe to use INNER
2355             # join.
2356             # Note that in this example we could just as well have the __gte
2357             # clause and the OR clause swapped. Or we could replace the __gte
2358             # clause with an OR clause containing rel_a__col=1|rel_a__col=2,
2359             # and again we could safely demote to INNER.
2360         query.promote_joins(to_promote)
2361         query.demote_joins(to_demote)
2362         return to_demote

</code><intended_behavior>In the part of the code that translates Django query expressions (including `__isnull`) into SQL statements, ensure that a validation for `__isnull` values is performed early in the process. If a value other than a boolean is detected, raise an error before the SQL statement is constructed. This will preserve the integrity of the SQL queries generated and prevent potential database errors or unexpected behavior.</intended_behavior>]
2025-10-15 04:14:54.517 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 The main QuerySet implementation. This provides the public API for the ORM.
3 """
4 
5 import copy
6 import operator
7 import warnings
8 from collections import namedtuple
9 from functools import lru_cache
10 from itertools import chain
11 
12 from django.conf import settings
13 from django.core import exceptions
14 from django.db import (
15     DJANGO_VERSION_PICKLE_KEY, IntegrityError, connections, router,
16     transaction,
17 )
18 from django.db.models import DateField, DateTimeField, sql
19 from django.db.models.constants import LOOKUP_SEP
20 from django.db.models.deletion import Collector
21 from django.db.models.expressions import Case, Expression, F, Value, When
22 from django.db.models.fields import AutoField
23 from django.db.models.functions import Cast, Trunc
24 from django.db.models.query_utils import FilteredRelation, InvalidQuery, Q
25 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE
26 from django.db.utils import NotSupportedError
27 from django.utils import timezone
28 from django.utils.functional import cached_property, partition
29 from django.utils.version import get_version
30 
31 # The maximum number of results to fetch in a get() query.
32 MAX_GET_RESULTS = 21
33 
34 # The maximum number of items to display in a QuerySet.__repr__
35 REPR_OUTPUT_SIZE = 20
36 
37 
38 class BaseIterable:
39     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
40         self.queryset = queryset
41         self.chunked_fetch = chunked_fetch
42         self.chunk_size = chunk_size
43 
44 
45 class ModelIterable(BaseIterable):
46     """Iterable that yields a model instance for each row."""
47 
48     def __iter__(self):
49         queryset = self.queryset
50         db = queryset.db
51         compiler = queryset.query.get_compiler(using=db)
52         # Execute the query. This will also fill compiler.select, klass_info,
53         # and annotations.
54         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
55         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,
56                                                   compiler.annotation_col_map)
57         model_cls = klass_info['model']
58         select_fields = klass_info['select_fields']
59         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1
60         init_list = [f[0].target.attname
61                      for f in select[model_fields_start:model_fields_end]]
62         related_populators = get_related_populators(klass_info, select, db)
63         known_related_objects = [
64             (field, related_objs, operator.attrgetter(*[
65                 field.attname
66                 if from_field == 'self' else
67                 queryset.model._meta.get_field(from_field).attname
68                 for from_field in field.from_fields
69             ])) for field, related_objs in queryset._known_related_objects.items()
70         ]
71         for row in compiler.results_iter(results):
72             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])
73             for rel_populator in related_populators:
74                 rel_populator.populate(row, obj)
75             if annotation_col_map:
76                 for attr_name, col_pos in annotation_col_map.items():
77                     setattr(obj, attr_name, row[col_pos])
78 
79             # Add the known related objects to the model.
80             for field, rel_objs, rel_getter in known_related_objects:
81                 # Avoid overwriting objects loaded by, e.g., select_related().
82                 if field.is_cached(obj):
83                     continue
84                 rel_obj_id = rel_getter(obj)
85                 try:
86                     rel_obj = rel_objs[rel_obj_id]
87                 except KeyError:
88                     pass  # May happen in qs1 | qs2 scenarios.
89                 else:
90                     setattr(obj, field.name, rel_obj)
91 
92             yield obj
93 
94 
95 class ValuesIterable(BaseIterable):
96     """
97     Iterable returned by QuerySet.values() that yields a dict for each row.
98     """
99 
100     def __iter__(self):
101         queryset = self.queryset
102         query = queryset.query
103         compiler = query.get_compiler(queryset.db)
104 
105         # extra(select=...) cols are always at the start of the row.
106         names = [
107             *query.extra_select,
108             *query.values_select,
109             *query.annotation_select,
110         ]
111         indexes = range(len(names))
112         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
113             yield {names[i]: row[i] for i in indexes}
114 
115 
116 class ValuesListIterable(BaseIterable):
117     """
118     Iterable returned by QuerySet.values_list(flat=False) that yields a tuple
119     for each row.
120     """
121 
122     def __iter__(self):
123         queryset = self.queryset
124         query = queryset.query
125         compiler = query.get_compiler(queryset.db)
126 
127         if queryset._fields:
128             # extra(select=...) cols are always at the start of the row.
129             names = [
130                 *query.extra_select,
131                 *query.values_select,
132                 *query.annotation_select,
133             ]
134             fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]
135             if fields != names:
136                 # Reorder according to fields.
137                 index_map = {name: idx for idx, name in enumerate(names)}
138                 rowfactory = operator.itemgetter(*[index_map[f] for f in fields])
139                 return map(
140                     rowfactory,
141                     compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
142                 )
143         return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)
144 
145 
146 class NamedValuesListIterable(ValuesListIterable):
147     """
148     Iterable returned by QuerySet.values_list(named=True) that yields a
149     namedtuple for each row.
150     """
151 
152     @staticmethod
153     @lru_cache()
154     def create_namedtuple_class(*names):
155         # Cache namedtuple() with @lru_cache() since it's too slow to be
156         # called for every QuerySet evaluation.
157         return namedtuple('Row', names)
158 
159     def __iter__(self):
160         queryset = self.queryset
161         if queryset._fields:
162             names = queryset._fields
163         else:
164             query = queryset.query
165             names = [*query.extra_select, *query.values_select, *query.annotation_select]
166         tuple_class = self.create_namedtuple_class(*names)
167         new = tuple.__new__
168         for row in super().__iter__():
169             yield new(tuple_class, row)
170 
171 
172 class FlatValuesListIterable(BaseIterable):
173     """
174     Iterable returned by QuerySet.values_list(flat=True) that yields single
175     values.
176     """
177 
178     def __iter__(self):
179         queryset = self.queryset
180         compiler = queryset.query.get_compiler(queryset.db)
181         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):
182             yield row[0]
183 
184 
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "...(remaining elements truncated)..."
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError(
284                 'QuerySet indices must be integers or slices, not %s.'
285                 % type(k).__name__
286             )
287         assert ((not isinstance(k, slice) and (k >= 0)) or
288                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
289                  (k.stop is None or k.stop >= 0))), \
290             "Negative indexing is not supported."
291 
292         if self._result_cache is not None:
293             return self._result_cache[k]
294 
295         if isinstance(k, slice):
296             qs = self._chain()
297             if k.start is not None:
298                 start = int(k.start)
299             else:
300                 start = None
301             if k.stop is not None:
302                 stop = int(k.stop)
303             else:
304                 stop = None
305             qs.query.set_limits(start, stop)
306             return list(qs)[::k.step] if k.step else qs
307 
308         qs = self._chain()
309         qs.query.set_limits(k, k + 1)
310         qs._fetch_all()
311         return qs._result_cache[0]
312 
313     def __and__(self, other):
314         self._merge_sanity_check(other)
315         if isinstance(other, EmptyQuerySet):
316             return other
317         if isinstance(self, EmptyQuerySet):
318             return self
319         combined = self._chain()
320         combined._merge_known_related_objects(other)
321         combined.query.combine(other.query, sql.AND)
322         return combined
323 
324     def __or__(self, other):
325         self._merge_sanity_check(other)
326         if isinstance(self, EmptyQuerySet):
327             return other
328         if isinstance(other, EmptyQuerySet):
329             return self
330         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
331         combined = query._chain()
332         combined._merge_known_related_objects(other)
333         if not other.query.can_filter():
334             other = other.model._base_manager.filter(pk__in=other.values('pk'))
335         combined.query.combine(other.query, sql.OR)
336         return combined
337 
338     ####################################
339     # METHODS THAT DO DATABASE QUERIES #
340     ####################################
341 
342     def _iterator(self, use_chunked_fetch, chunk_size):
343         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
344 
345     def iterator(self, chunk_size=2000):
346         """
347         An iterator over the results from applying this QuerySet to the
348         database.
349         """
350         if chunk_size <= 0:
351             raise ValueError('Chunk size must be strictly positive.')
352         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
353         return self._iterator(use_chunked_fetch, chunk_size)
354 
355     def aggregate(self, *args, **kwargs):
356         """
357         Return a dictionary containing the calculations (aggregation)
358         over the current queryset.
359 
360         If args is present the expression is passed as a kwarg using
361         the Aggregate object's default alias.
362         """
363         if self.query.distinct_fields:
364             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
365         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
366         for arg in args:
367             # The default_alias property raises TypeError if default_alias
368             # can't be set automatically or AttributeError if it isn't an
369             # attribute.
370             try:
371                 arg.default_alias
372             except (AttributeError, TypeError):
373                 raise TypeError("Complex aggregates require an alias")
374             kwargs[arg.default_alias] = arg
375 
376         query = self.query.chain()
377         for (alias, aggregate_expr) in kwargs.items():
378             query.add_annotation(aggregate_expr, alias, is_summary=True)
379             if not query.annotations[alias].contains_aggregate:
380                 raise TypeError("%s is not an aggregate expression" % alias)
381         return query.get_aggregation(self.db, kwargs)
382 
383     def count(self):
384         """
385         Perform a SELECT COUNT() and return the number of records as an
386         integer.
387 
388         If the QuerySet is already fully cached, return the length of the
389         cached results set to avoid multiple SELECT COUNT(*) calls.
390         """
391         if self._result_cache is not None:
392             return len(self._result_cache)
393 
394         return self.query.get_count(using=self.db)
395 
396     def get(self, *args, **kwargs):
397         """
398         Perform the query and return a single object matching the given
399         keyword arguments.
400         """
401         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
402         if self.query.can_filter() and not self.query.distinct_fields:
403             clone = clone.order_by()
404         limit = None
405         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
406             limit = MAX_GET_RESULTS
407             clone.query.set_limits(high=limit)
408         num = len(clone)
409         if num == 1:
410             return clone._result_cache[0]
411         if not num:
412             raise self.model.DoesNotExist(
413                 "%s matching query does not exist." %
414                 self.model._meta.object_name
415             )
416         raise self.model.MultipleObjectsReturned(
417             'get() returned more than one %s -- it returned %s!' % (
418                 self.model._meta.object_name,
419                 num if not limit or num < limit else 'more than %s' % (limit - 1),
420             )
421         )
422 
423     def create(self, **kwargs):
424         """
425         Create a new object with the given kwargs, saving it to the database
426         and returning the created object.
427         """
428         obj = self.model(**kwargs)
429         self._for_write = True
430         obj.save(force_insert=True, using=self.db)
431         return obj
432 
433     def _populate_pk_values(self, objs):
434         for obj in objs:
435             if obj.pk is None:
436                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
437 
438     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
439         """
440         Insert each of the instances into the database. Do *not* call
441         save() on each of the instances, do not send any pre/post_save
442         signals, and do not set the primary key attribute if it is an
443         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
444         Multi-table models are not supported.
445         """
446         # When you bulk insert you don't get the primary keys back (if it's an
447         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
448         # you can't insert into the child tables which references this. There
449         # are two workarounds:
450         # 1) This could be implemented if you didn't have an autoincrement pk
451         # 2) You could do it by doing O(n) normal inserts into the parent
452         #    tables to get the primary keys back and then doing a single bulk
453         #    insert into the childmost table.
454         # We currently set the primary keys on the objects when using
455         # PostgreSQL via the RETURNING ID clause. It should be possible for
456         # Oracle as well, but the semantics for extracting the primary keys is
457         # trickier so it's not done yet.
458         assert batch_size is None or batch_size > 0
459         # Check that the parents share the same concrete model with the our
460         # model to detect the inheritance pattern ConcreteGrandParent ->
461         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
462         # would not identify that case as involving multiple tables.
463         for parent in self.model._meta.get_parent_list():
464             if parent._meta.concrete_model is not self.model._meta.concrete_model:
465                 raise ValueError("Can't bulk create a multi-table inherited model")
466         if not objs:
467             return objs
468         self._for_write = True
469         connection = connections[self.db]
470         opts = self.model._meta
471         fields = opts.concrete_fields
472         objs = list(objs)
473         self._populate_pk_values(objs)
474         with transaction.atomic(using=self.db, savepoint=False):
475             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
476             if objs_with_pk:
477                 returned_columns = self._batched_insert(
478                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
479                 )
480                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
481                     for result, field in zip(results, opts.db_returning_fields):
482                         if field != opts.pk:
483                             setattr(obj_with_pk, field.attname, result)
484                 for obj_with_pk in objs_with_pk:
485                     obj_with_pk._state.adding = False
486                     obj_with_pk._state.db = self.db
487             if objs_without_pk:
488                 fields = [f for f in fields if not isinstance(f, AutoField)]
489                 returned_columns = self._batched_insert(
490                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
491                 )
492                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
493                     assert len(returned_columns) == len(objs_without_pk)
494                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
495                     for result, field in zip(results, opts.db_returning_fields):
496                         setattr(obj_without_pk, field.attname, result)
497                     obj_without_pk._state.adding = False
498                     obj_without_pk._state.db = self.db
499 
500         return objs
501 
502     def bulk_update(self, objs, fields, batch_size=None):
503         """
504         Update the given fields in each of the given objects in the database.
505         """
506         if batch_size is not None and batch_size < 0:
507             raise ValueError('Batch size must be a positive integer.')
508         if not fields:
509             raise ValueError('Field names must be given to bulk_update().')
510         objs = tuple(objs)
511         if any(obj.pk is None for obj in objs):
512             raise ValueError('All bulk_update() objects must have a primary key set.')
513         fields = [self.model._meta.get_field(name) for name in fields]
514         if any(not f.concrete or f.many_to_many for f in fields):
515             raise ValueError('bulk_update() can only be used with concrete fields.')
516         if any(f.primary_key for f in fields):
517             raise ValueError('bulk_update() cannot be used with primary key fields.')
518         if not objs:
519             return
520         # PK is used twice in the resulting update query, once in the filter
521         # and once in the WHEN. Each field will also have one CAST.
522         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
523         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
524         requires_casting = connections[self.db].features.requires_casted_case_in_updates
525         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
526         updates = []
527         for batch_objs in batches:
528             update_kwargs = {}
529             for field in fields:
530                 when_statements = []
531                 for obj in batch_objs:
532                     attr = getattr(obj, field.attname)
533                     if not isinstance(attr, Expression):
534                         attr = Value(attr, output_field=field)
535                     when_statements.append(When(pk=obj.pk, then=attr))
536                 case_statement = Case(*when_statements, output_field=field)
537                 if requires_casting:
538                     case_statement = Cast(case_statement, output_field=field)
539                 update_kwargs[field.attname] = case_statement
540             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
541         with transaction.atomic(using=self.db, savepoint=False):
542             for pks, update_kwargs in updates:
543                 self.filter(pk__in=pks).update(**update_kwargs)
544     bulk_update.alters_data = True
545 
546     def get_or_create(self, defaults=None, **kwargs):
547         """
548         Look up an object with the given kwargs, creating one if necessary.
549         Return a tuple of (object, created), where created is a boolean
550         specifying whether an object was created.
551         """
552         # The get() needs to be targeted at the write database in order
553         # to avoid potential transaction consistency problems.
554         self._for_write = True
555         try:
556             return self.get(**kwargs), False
557         except self.model.DoesNotExist:
558             params = self._extract_model_params(defaults, **kwargs)
559             return self._create_object_from_params(kwargs, params)
560 
561     def update_or_create(self, defaults=None, **kwargs):
562         """
563         Look up an object with the given kwargs, updating one with defaults
564         if it exists, otherwise create a new one.
565         Return a tuple (object, created), where created is a boolean
566         specifying whether an object was created.
567         """
568         defaults = defaults or {}
569         self._for_write = True
570         with transaction.atomic(using=self.db):
571             try:
572                 obj = self.select_for_update().get(**kwargs)
573             except self.model.DoesNotExist:
574                 params = self._extract_model_params(defaults, **kwargs)
575                 # Lock the row so that a concurrent update is blocked until
576                 # after update_or_create() has performed its save.
577                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
578                 if created:
579                     return obj, created
580             for k, v in defaults.items():
581                 setattr(obj, k, v() if callable(v) else v)
582             obj.save(using=self.db)
583         return obj, False
584 
585     def _create_object_from_params(self, lookup, params, lock=False):
586         """
587         Try to create an object using passed params. Used by get_or_create()
588         and update_or_create().
589         """
590         try:
591             with transaction.atomic(using=self.db):
592                 params = {k: v() if callable(v) else v for k, v in params.items()}
593                 obj = self.create(**params)
594             return obj, True
595         except IntegrityError as e:
596             try:
597                 qs = self.select_for_update() if lock else self
598                 return qs.get(**lookup), False
599             except self.model.DoesNotExist:
600                 pass
601             raise e
602 
603     def _extract_model_params(self, defaults, **kwargs):
604         """
605         Prepare `params` for creating a model instance based on the given
606         kwargs; for use by get_or_create() and update_or_create().
607         """
608         defaults = defaults or {}
609         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
610         params.update(defaults)
611         property_names = self.model._meta._property_names
612         invalid_params = []
613         for param in params:
614             try:
615                 self.model._meta.get_field(param)
616             except exceptions.FieldDoesNotExist:
617                 # It's okay to use a model's property if it has a setter.
618                 if not (param in property_names and getattr(self.model, param).fset):
619                     invalid_params.append(param)
620         if invalid_params:
621             raise exceptions.FieldError(
622                 "Invalid field name(s) for model %s: '%s'." % (
623                     self.model._meta.object_name,
624                     "', '".join(sorted(invalid_params)),
625                 ))
626         return params
627 
628     def _earliest(self, *fields):
629         """
630         Return the earliest object according to fields (if given) or by the
631         model's Meta.get_latest_by.
632         """
633         if fields:
634             order_by = fields
635         else:
636             order_by = getattr(self.model._meta, 'get_latest_by')
637             if order_by and not isinstance(order_by, (tuple, list)):
638                 order_by = (order_by,)
639         if order_by is None:
640             raise ValueError(
641                 "earliest() and latest() require either fields as positional "
642                 "arguments or 'get_latest_by' in the model's Meta."
643             )
644 
645         assert not self.query.is_sliced, \
646             "Cannot change a query once a slice has been taken."
647         obj = self._chain()
648         obj.query.set_limits(high=1)
649         obj.query.clear_ordering(force_empty=True)
650         obj.query.add_ordering(*order_by)
651         return obj.get()
652 
653     def earliest(self, *fields):
654         return self._earliest(*fields)
655 
656     def latest(self, *fields):
657         return self.reverse()._earliest(*fields)
658 
659     def first(self):
660         """Return the first object of a query or None if no match is found."""
661         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
662             return obj
663 
664     def last(self):
665         """Return the last object of a query or None if no match is found."""
666         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
667             return obj
668 
669     def in_bulk(self, id_list=None, *, field_name='pk'):
670         """
671         Return a dictionary mapping each of the given IDs to the object with
672         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
673         """
674         assert not self.query.is_sliced, \
675             "Cannot use 'limit' or 'offset' with in_bulk"
676         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
677             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
678         if id_list is not None:
679             if not id_list:
680                 return {}
681             filter_key = '{}__in'.format(field_name)
682             batch_size = connections[self.db].features.max_query_params
683             id_list = tuple(id_list)
684             # If the database has a limit on the number of query parameters
685             # (e.g. SQLite), retrieve objects in batches if necessary.
686             if batch_size and batch_size < len(id_list):
687                 qs = ()
688                 for offset in range(0, len(id_list), batch_size):
689                     batch = id_list[offset:offset + batch_size]
690                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
691             else:
692                 qs = self.filter(**{filter_key: id_list}).order_by()
693         else:
694             qs = self._chain()
695         return {getattr(obj, field_name): obj for obj in qs}
696 
697     def delete(self):
698         """Delete the records in the current QuerySet."""
699         assert not self.query.is_sliced, \
700             "Cannot use 'limit' or 'offset' with delete."
701 
702         if self._fields is not None:
703             raise TypeError("Cannot call delete() after .values() or .values_list()")
704 
705         del_query = self._chain()
706 
707         # The delete is actually 2 queries - one to find related objects,
708         # and one to delete. Make sure that the discovery of related
709         # objects is performed on the same database as the deletion.
710         del_query._for_write = True
711 
712         # Disable non-supported fields.
713         del_query.query.select_for_update = False
714         del_query.query.select_related = False
715         del_query.query.clear_ordering(force_empty=True)
716 
717         collector = Collector(using=del_query.db)
718         collector.collect(del_query)
719         deleted, _rows_count = collector.delete()
720 
721         # Clear the result cache, in case this QuerySet gets reused.
722         self._result_cache = None
723         return deleted, _rows_count
724 
725     delete.alters_data = True
726     delete.queryset_only = True
727 
728     def _raw_delete(self, using):
729         """
730         Delete objects found from the given queryset in single direct SQL
731         query. No signals are sent and there is no protection for cascades.
732         """
733         return sql.DeleteQuery(self.model).delete_qs(self, using)
734     _raw_delete.alters_data = True
735 
736     def update(self, **kwargs):
737         """
738         Update all elements in the current QuerySet, setting all the given
739         fields to the appropriate values.
740         """
741         assert not self.query.is_sliced, \
742             "Cannot update a query once a slice has been taken."
743         self._for_write = True
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_values(kwargs)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         with transaction.mark_for_rollback_on_error(using=self.db):
749             rows = query.get_compiler(self.db).execute_sql(CURSOR)
750         self._result_cache = None
751         return rows
752     update.alters_data = True
753 
754     def _update(self, values):
755         """
756         A version of update() that accepts field objects instead of field names.
757         Used primarily for model saving and not intended for use by general
758         code (it requires too much poking around at model internals to be
759         useful at that level).
760         """
761         assert not self.query.is_sliced, \
762             "Cannot update a query once a slice has been taken."
763         query = self.query.chain(sql.UpdateQuery)
764         query.add_update_fields(values)
765         # Clear any annotations so that they won't be present in subqueries.
766         query.annotations = {}
767         self._result_cache = None
768         return query.get_compiler(self.db).execute_sql(CURSOR)
769     _update.alters_data = True
770     _update.queryset_only = False
771 
772     def exists(self):
773         if self._result_cache is None:
774             return self.query.has_results(using=self.db)
775         return bool(self._result_cache)
776 
777     def _prefetch_related_objects(self):
778         # This method can only be called once the result cache has been filled.
779         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
780         self._prefetch_done = True
781 
782     def explain(self, *, format=None, **options):
783         return self.query.explain(using=self.db, format=format, **options)
784 
785     ##################################################
786     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
787     ##################################################
788 
789     def raw(self, raw_query, params=None, translations=None, using=None):
790         if using is None:
791             using = self.db
792         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
793         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
794         return qs
795 
796     def _values(self, *fields, **expressions):
797         clone = self._chain()
798         if expressions:
799             clone = clone.annotate(**expressions)
800         clone._fields = fields
801         clone.query.set_values(fields)
802         return clone
803 
804     def values(self, *fields, **expressions):
805         fields += tuple(expressions)
806         clone = self._values(*fields, **expressions)
807         clone._iterable_class = ValuesIterable
808         return clone
809 
810     def values_list(self, *fields, flat=False, named=False):
811         if flat and named:
812             raise TypeError("'flat' and 'named' can't be used together.")
813         if flat and len(fields) > 1:
814             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
815 
816         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
817         _fields = []
818         expressions = {}
819         counter = 1
820         for field in fields:
821             if hasattr(field, 'resolve_expression'):
822                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
823                 while True:
824                     field_id = field_id_prefix + str(counter)
825                     counter += 1
826                     if field_id not in field_names:
827                         break
828                 expressions[field_id] = field
829                 _fields.append(field_id)
830             else:
831                 _fields.append(field)
832 
833         clone = self._values(*_fields, **expressions)
834         clone._iterable_class = (
835             NamedValuesListIterable if named
836             else FlatValuesListIterable if flat
837             else ValuesListIterable
838         )
839         return clone
840 
841     def dates(self, field_name, kind, order='ASC'):
842         """
843         Return a list of date objects representing all available dates for
844         the given field_name, scoped to 'kind'.
845         """
846         assert kind in ('year', 'month', 'week', 'day'), \
847             "'kind' must be one of 'year', 'month', 'week', or 'day'."
848         assert order in ('ASC', 'DESC'), \
849             "'order' must be either 'ASC' or 'DESC'."
850         return self.annotate(
851             datefield=Trunc(field_name, kind, output_field=DateField()),
852             plain_field=F(field_name)
853         ).values_list(
854             'datefield', flat=True
855         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
856 
857     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
858         """
859         Return a list of datetime objects representing all available
860         datetimes for the given field_name, scoped to 'kind'.
861         """
862         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
863             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
864         assert order in ('ASC', 'DESC'), \
865             "'order' must be either 'ASC' or 'DESC'."
866         if settings.USE_TZ:
867             if tzinfo is None:
868                 tzinfo = timezone.get_current_timezone()
869         else:
870             tzinfo = None
871         return self.annotate(
872             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
873             plain_field=F(field_name)
874         ).values_list(
875             'datetimefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
877 
878     def none(self):
879         """Return an empty QuerySet."""
880         clone = self._chain()
881         clone.query.set_empty()
882         return clone
883 
884     ##################################################################
885     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
886     ##################################################################
887 
888     def all(self):
889         """
890         Return a new QuerySet that is a copy of the current one. This allows a
891         QuerySet to proxy for a model manager in some cases.
892         """
893         return self._chain()
894 
895     def filter(self, *args, **kwargs):
896         """
897         Return a new QuerySet instance with the args ANDed to the existing
898         set.
899         """
900         self._not_support_combined_queries('filter')
901         return self._filter_or_exclude(False, *args, **kwargs)
902 
903     def exclude(self, *args, **kwargs):
904         """
905         Return a new QuerySet instance with NOT (args) ANDed to the existing
906         set.
907         """
908         self._not_support_combined_queries('exclude')
909         return self._filter_or_exclude(True, *args, **kwargs)
910 
911     def _filter_or_exclude(self, negate, *args, **kwargs):
912         if args or kwargs:
913             assert not self.query.is_sliced, \
914                 "Cannot filter a query once a slice has been taken."
915 
916         clone = self._chain()
917         if negate:
918             clone.query.add_q(~Q(*args, **kwargs))
919         else:
920             clone.query.add_q(Q(*args, **kwargs))
921         return clone
922 
923     def complex_filter(self, filter_obj):
924         """
925         Return a new QuerySet instance with filter_obj added to the filters.
926 
927         filter_obj can be a Q object or a dictionary of keyword lookup
928         arguments.
929 
930         This exists to support framework features such as 'limit_choices_to',
931         and usually it will be more natural to use other methods.
932         """
933         if isinstance(filter_obj, Q):
934             clone = self._chain()
935             clone.query.add_q(filter_obj)
936             return clone
937         else:
938             return self._filter_or_exclude(None, **filter_obj)
939 
940     def _combinator_query(self, combinator, *other_qs, all=False):
941         # Clone the query to inherit the select list and everything
942         clone = self._chain()
943         # Clear limits and ordering so they can be reapplied
944         clone.query.clear_ordering(True)
945         clone.query.clear_limits()
946         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
947         clone.query.combinator = combinator
948         clone.query.combinator_all = all
949         return clone
950 
951     def union(self, *other_qs, all=False):
952         # If the query is an EmptyQuerySet, combine all nonempty querysets.
953         if isinstance(self, EmptyQuerySet):
954             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
955             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
956         return self._combinator_query('union', *other_qs, all=all)
957 
958     def intersection(self, *other_qs):
959         # If any query is an EmptyQuerySet, return it.
960         if isinstance(self, EmptyQuerySet):
961             return self
962         for other in other_qs:
963             if isinstance(other, EmptyQuerySet):
964                 return other
965         return self._combinator_query('intersection', *other_qs)
966 
967     def difference(self, *other_qs):
968         # If the query is an EmptyQuerySet, return it.
969         if isinstance(self, EmptyQuerySet):
970             return self
971         return self._combinator_query('difference', *other_qs)
972 
973     def select_for_update(self, nowait=False, skip_locked=False, of=()):
974         """
975         Return a new QuerySet instance that will select objects with a
976         FOR UPDATE lock.
977         """
978         if nowait and skip_locked:
979             raise ValueError('The nowait option cannot be used with skip_locked.')
980         obj = self._chain()
981         obj._for_write = True
982         obj.query.select_for_update = True
983         obj.query.select_for_update_nowait = nowait
984         obj.query.select_for_update_skip_locked = skip_locked
985         obj.query.select_for_update_of = of
986         return obj
987 
988     def select_related(self, *fields):
989         """
990         Return a new QuerySet instance that will select related objects.
991 
992         If fields are specified, they must be ForeignKey fields and only those
993         related objects are included in the selection.
994 
995         If select_related(None) is called, clear the list.
996         """
997         self._not_support_combined_queries('select_related')
998         if self._fields is not None:
999             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1000 
1001         obj = self._chain()
1002         if fields == (None,):
1003             obj.query.select_related = False
1004         elif fields:
1005             obj.query.add_select_related(fields)
1006         else:
1007             obj.query.select_related = True
1008         return obj
1009 
1010     def prefetch_related(self, *lookups):
1011         """
1012         Return a new QuerySet instance that will prefetch the specified
1013         Many-To-One and Many-To-Many related objects when the QuerySet is
1014         evaluated.
1015 
1016         When prefetch_related() is called more than once, append to the list of
1017         prefetch lookups. If prefetch_related(None) is called, clear the list.
1018         """
1019         self._not_support_combined_queries('prefetch_related')
1020         clone = self._chain()
1021         if lookups == (None,):
1022             clone._prefetch_related_lookups = ()
1023         else:
1024             for lookup in lookups:
1025                 if isinstance(lookup, Prefetch):
1026                     lookup = lookup.prefetch_to
1027                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1028                 if lookup in self.query._filtered_relations:
1029                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1030             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1031         return clone
1032 
1033     def annotate(self, *args, **kwargs):
1034         """
1035         Return a query set in which the returned objects have been annotated
1036         with extra data or aggregations.
1037         """
1038         self._not_support_combined_queries('annotate')
1039         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1040         annotations = {}
1041         for arg in args:
1042             # The default_alias property may raise a TypeError.
1043             try:
1044                 if arg.default_alias in kwargs:
1045                     raise ValueError("The named annotation '%s' conflicts with the "
1046                                      "default name for another annotation."
1047                                      % arg.default_alias)
1048             except TypeError:
1049                 raise TypeError("Complex annotations require an alias")
1050             annotations[arg.default_alias] = arg
1051         annotations.update(kwargs)
1052 
1053         clone = self._chain()
1054         names = self._fields
1055         if names is None:
1056             names = set(chain.from_iterable(
1057                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1058                 for field in self.model._meta.get_fields()
1059             ))
1060 
1061         for alias, annotation in annotations.items():
1062             if alias in names:
1063                 raise ValueError("The annotation '%s' conflicts with a field on "
1064                                  "the model." % alias)
1065             if isinstance(annotation, FilteredRelation):
1066                 clone.query.add_filtered_relation(annotation, alias)
1067             else:
1068                 clone.query.add_annotation(annotation, alias, is_summary=False)
1069 
1070         for alias, annotation in clone.query.annotations.items():
1071             if alias in annotations and annotation.contains_aggregate:
1072                 if clone._fields is None:
1073                     clone.query.group_by = True
1074                 else:
1075                     clone.query.set_group_by()
1076                 break
1077 
1078         return clone
1079 
1080     def order_by(self, *field_names):
1081         """Return a new QuerySet instance with the ordering changed."""
1082         assert not self.query.is_sliced, \
1083             "Cannot reorder a query once a slice has been taken."
1084         obj = self._chain()
1085         obj.query.clear_ordering(force_empty=False)
1086         obj.query.add_ordering(*field_names)
1087         return obj
1088 
1089     def distinct(self, *field_names):
1090         """
1091         Return a new QuerySet instance that will select only distinct results.
1092         """
1093         assert not self.query.is_sliced, \
1094             "Cannot create distinct fields once a slice has been taken."
1095         obj = self._chain()
1096         obj.query.add_distinct_fields(*field_names)
1097         return obj
1098 
1099     def extra(self, select=None, where=None, params=None, tables=None,
1100               order_by=None, select_params=None):
1101         """Add extra SQL fragments to the query."""
1102         self._not_support_combined_queries('extra')
1103         assert not self.query.is_sliced, \
1104             "Cannot change a query once a slice has been taken"
1105         clone = self._chain()
1106         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1107         return clone
1108 
1109     def reverse(self):
1110         """Reverse the ordering of the QuerySet."""
1111         if self.query.is_sliced:
1112             raise TypeError('Cannot reverse a query once a slice has been taken.')
1113         clone = self._chain()
1114         clone.query.standard_ordering = not clone.query.standard_ordering
1115         return clone
1116 
1117     def defer(self, *fields):
1118         """
1119         Defer the loading of data for certain fields until they are accessed.
1120         Add the set of deferred fields to any existing set of deferred fields.
1121         The only exception to this is if None is passed in as the only
1122         parameter, in which case removal all deferrals.
1123         """
1124         self._not_support_combined_queries('defer')
1125         if self._fields is not None:
1126             raise TypeError("Cannot call defer() after .values() or .values_list()")
1127         clone = self._chain()
1128         if fields == (None,):
1129             clone.query.clear_deferred_loading()
1130         else:
1131             clone.query.add_deferred_loading(fields)
1132         return clone
1133 
1134     def only(self, *fields):
1135         """
1136         Essentially, the opposite of defer(). Only the fields passed into this
1137         method and that are not already specified as deferred are loaded
1138         immediately when the queryset is evaluated.
1139         """
1140         self._not_support_combined_queries('only')
1141         if self._fields is not None:
1142             raise TypeError("Cannot call only() after .values() or .values_list()")
1143         if fields == (None,):
1144             # Can only pass None to defer(), not only(), as the rest option.
1145             # That won't stop people trying to do this, so let's be explicit.
1146             raise TypeError("Cannot pass None as an argument to only().")
1147         for field in fields:
1148             field = field.split(LOOKUP_SEP, 1)[0]
1149             if field in self.query._filtered_relations:
1150                 raise ValueError('only() is not supported with FilteredRelation.')
1151         clone = self._chain()
1152         clone.query.add_immediate_loading(fields)
1153         return clone
1154 
1155     def using(self, alias):
1156         """Select which database this QuerySet should execute against."""
1157         clone = self._chain()
1158         clone._db = alias
1159         return clone
1160 
1161     ###################################
1162     # PUBLIC INTROSPECTION ATTRIBUTES #
1163     ###################################
1164 
1165     @property
1166     def ordered(self):
1167         """
1168         Return True if the QuerySet is ordered -- i.e. has an order_by()
1169         clause or a default ordering on the model (or is empty).
1170         """
1171         if isinstance(self, EmptyQuerySet):
1172             return True
1173         if self.query.extra_order_by or self.query.order_by:
1174             return True
1175         elif self.query.default_ordering and self.query.get_meta().ordering:
1176             return True
1177         else:
1178             return False
1179 
1180     @property
1181     def db(self):
1182         """Return the database used if this query is executed now."""
1183         if self._for_write:
1184             return self._db or router.db_for_write(self.model, **self._hints)
1185         return self._db or router.db_for_read(self.model, **self._hints)
1186 
1187     ###################
1188     # PRIVATE METHODS #
1189     ###################
1190 
1191     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1192         """
1193         Insert a new record for the given model. This provides an interface to
1194         the InsertQuery class and is how Model.save() is implemented.
1195         """
1196         self._for_write = True
1197         if using is None:
1198             using = self.db
1199         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1200         query.insert_values(fields, objs, raw=raw)
1201         return query.get_compiler(using=using).execute_sql(returning_fields)
1202     _insert.alters_data = True
1203     _insert.queryset_only = False
1204 
1205     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1206         """
1207         Helper method for bulk_create() to insert objs one batch at a time.
1208         """
1209         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1210             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1211         ops = connections[self.db].ops
1212         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1213         inserted_rows = []
1214         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1215         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1216             if bulk_return and not ignore_conflicts:
1217                 inserted_columns = self._insert(
1218                     item, fields=fields, using=self.db,
1219                     returning_fields=self.model._meta.db_returning_fields,
1220                     ignore_conflicts=ignore_conflicts,
1221                 )
1222                 if isinstance(inserted_columns, list):
1223                     inserted_rows.extend(inserted_columns)
1224                 else:
1225                     inserted_rows.append(inserted_columns)
1226             else:
1227                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1228         return inserted_rows
1229 
1230     def _chain(self, **kwargs):
1231         """
1232         Return a copy of the current QuerySet that's ready for another
1233         operation.
1234         """
1235         obj = self._clone()
1236         if obj._sticky_filter:
1237             obj.query.filter_is_sticky = True
1238             obj._sticky_filter = False
1239         obj.__dict__.update(kwargs)
1240         return obj
1241 
1242     def _clone(self):
1243         """
1244         Return a copy of the current QuerySet. A lightweight alternative
1245         to deepcopy().
1246         """
1247         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1248         c._sticky_filter = self._sticky_filter
1249         c._for_write = self._for_write
1250         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1251         c._known_related_objects = self._known_related_objects
1252         c._iterable_class = self._iterable_class
1253         c._fields = self._fields
1254         return c
1255 
1256     def _fetch_all(self):
1257         if self._result_cache is None:
1258             self._result_cache = list(self._iterable_class(self))
1259         if self._prefetch_related_lookups and not self._prefetch_done:
1260             self._prefetch_related_objects()
1261 
1262     def _next_is_sticky(self):
1263         """
1264         Indicate that the next filter call and the one following that should
1265         be treated as a single filter. This is only important when it comes to
1266         determining when to reuse tables for many-to-many filters. Required so
1267         that we can filter naturally on the results of related managers.
1268 
1269         This doesn't return a clone of the current QuerySet (it returns
1270         "self"). The method is only used internally and should be immediately
1271         followed by a filter() that does create a clone.
1272         """
1273         self._sticky_filter = True
1274         return self
1275 
1276     def _merge_sanity_check(self, other):
1277         """Check that two QuerySet classes may be merged."""
1278         if self._fields is not None and (
1279                 set(self.query.values_select) != set(other.query.values_select) or
1280                 set(self.query.extra_select) != set(other.query.extra_select) or
1281                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1282             raise TypeError(
1283                 "Merging '%s' classes must involve the same values in each case."
1284                 % self.__class__.__name__
1285             )
1286 
1287     def _merge_known_related_objects(self, other):
1288         """
1289         Keep track of all known related objects from either QuerySet instance.
1290         """
1291         for field, objects in other._known_related_objects.items():
1292             self._known_related_objects.setdefault(field, {}).update(objects)
1293 
1294     def resolve_expression(self, *args, **kwargs):
1295         if self._fields and len(self._fields) > 1:
1296             # values() queryset can only be used as nested queries
1297             # if they are set up to select only a single field.
1298             raise TypeError('Cannot use multi-field values as a filter value.')
1299         query = self.query.resolve_expression(*args, **kwargs)
1300         query._db = self._db
1301         return query
1302     resolve_expression.queryset_only = True
1303 
1304     def _add_hints(self, **hints):
1305         """
1306         Update hinting information for use by routers. Add new key/values or
1307         overwrite existing key/values.
1308         """
1309         self._hints.update(hints)
1310 
1311     def _has_filters(self):
1312         """
1313         Check if this QuerySet has any filtering going on. This isn't
1314         equivalent with checking if all objects are present in results, for
1315         example, qs[1:]._has_filters() -> False.
1316         """
1317         return self.query.has_filters()
1318 
1319     @staticmethod
1320     def _validate_values_are_expressions(values, method_name):
1321         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1322         if invalid_args:
1323             raise TypeError(
1324                 'QuerySet.%s() received non-expression(s): %s.' % (
1325                     method_name,
1326                     ', '.join(invalid_args),
1327                 )
1328             )
1329 
1330     def _not_support_combined_queries(self, operation_name):
1331         if self.query.combinator:
1332             raise NotSupportedError(
1333                 'Calling QuerySet.%s() after %s() is not supported.'
1334                 % (operation_name, self.query.combinator)
1335             )
1336 
1337 
1338 class InstanceCheckMeta(type):
1339     def __instancecheck__(self, instance):
1340         return isinstance(instance, QuerySet) and instance.query.is_empty()
1341 
1342 
1343 class EmptyQuerySet(metaclass=InstanceCheckMeta):
1344     """
1345     Marker class to checking if a queryset is empty by .none():
1346         isinstance(qs.none(), EmptyQuerySet) -> True
1347     """
1348 
1349     def __init__(self, *args, **kwargs):
1350         raise TypeError("EmptyQuerySet can't be instantiated")
1351 
1352 
1353 class RawQuerySet:
1354     """
1355     Provide an iterator which converts the results of raw SQL queries into
1356     annotated model instances.
1357     """
1358     def __init__(self, raw_query, model=None, query=None, params=None,
1359                  translations=None, using=None, hints=None):
1360         self.raw_query = raw_query
1361         self.model = model
1362         self._db = using
1363         self._hints = hints or {}
1364         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)
1365         self.params = params or ()
1366         self.translations = translations or {}
1367         self._result_cache = None
1368         self._prefetch_related_lookups = ()
1369         self._prefetch_done = False
1370 
1371     def resolve_model_init_order(self):
1372         """Resolve the init field names and value positions."""
1373         converter = connections[self.db].introspection.identifier_converter
1374         model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]
1375         annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)
1376                              if column not in self.model_fields]
1377         model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]
1378         model_init_names = [f.attname for f in model_init_fields]
1379         return model_init_names, model_init_order, annotation_fields
1380 
1381     def prefetch_related(self, *lookups):
1382         """Same as QuerySet.prefetch_related()"""
1383         clone = self._clone()
1384         if lookups == (None,):
1385             clone._prefetch_related_lookups = ()
1386         else:
1387             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1388         return clone
1389 
1390     def _prefetch_related_objects(self):
1391         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
1392         self._prefetch_done = True
1393 
1394     def _clone(self):
1395         """Same as QuerySet._clone()"""
1396         c = self.__class__(
1397             self.raw_query, model=self.model, query=self.query, params=self.params,
1398             translations=self.translations, using=self._db, hints=self._hints
1399         )
1400         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1401         return c
1402 
1403     def _fetch_all(self):
1404         if self._result_cache is None:
1405             self._result_cache = list(self.iterator())
1406         if self._prefetch_related_lookups and not self._prefetch_done:
1407             self._prefetch_related_objects()
1408 
1409     def __len__(self):
1410         self._fetch_all()
1411         return len(self._result_cache)
1412 
1413     def __bool__(self):
1414         self._fetch_all()
1415         return bool(self._result_cache)
1416 
1417     def __iter__(self):
1418         self._fetch_all()
1419         return iter(self._result_cache)
1420 
1421     def iterator(self):
1422         # Cache some things for performance reasons outside the loop.
1423         db = self.db
1424         compiler = connections[db].ops.compiler('SQLCompiler')(
1425             self.query, connections[db], db
1426         )
1427 
1428         query = iter(self.query)
1429 
1430         try:
1431             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()
1432             if self.model._meta.pk.attname not in model_init_names:
1433                 raise InvalidQuery('Raw query must include the primary key')
1434             model_cls = self.model
1435             fields = [self.model_fields.get(c) for c in self.columns]
1436             converters = compiler.get_converters([
1437                 f.get_col(f.model._meta.db_table) if f else None for f in fields
1438             ])
1439             if converters:
1440                 query = compiler.apply_converters(query, converters)
1441             for values in query:
1442                 # Associate fields to values
1443                 model_init_values = [values[pos] for pos in model_init_pos]
1444                 instance = model_cls.from_db(db, model_init_names, model_init_values)
1445                 if annotation_fields:
1446                     for column, pos in annotation_fields:
1447                         setattr(instance, column, values[pos])
1448                 yield instance
1449         finally:
1450             # Done iterating the Query. If it has its own cursor, close it.
1451             if hasattr(self.query, 'cursor') and self.query.cursor:
1452                 self.query.cursor.close()
1453 
1454     def __repr__(self):
1455         return "<%s: %s>" % (self.__class__.__name__, self.query)
1456 
1457     def __getitem__(self, k):
1458         return list(self)[k]
1459 
1460     @property
1461     def db(self):
1462         """Return the database used if this query is executed now."""
1463         return self._db or router.db_for_read(self.model, **self._hints)
1464 
1465     def using(self, alias):
1466         """Select the database this RawQuerySet should execute against."""
1467         return RawQuerySet(
1468             self.raw_query, model=self.model,
1469             query=self.query.chain(using=alias),
1470             params=self.params, translations=self.translations,
1471             using=alias,
1472         )
1473 
1474     @cached_property
1475     def columns(self):
1476         """
1477         A list of model field names in the order they'll appear in the
1478         query results.
1479         """
1480         columns = self.query.get_columns()
1481         # Adjust any column names which don't match field names
1482         for (query_name, model_name) in self.translations.items():
1483             # Ignore translations for nonexistent column names
1484             try:
1485                 index = columns.index(query_name)
1486             except ValueError:
1487                 pass
1488             else:
1489                 columns[index] = model_name
1490         return columns
1491 
1492     @cached_property
1493     def model_fields(self):
1494         """A dict mapping column names to model field names."""
1495         converter = connections[self.db].introspection.identifier_converter
1496         model_fields = {}
1497         for field in self.model._meta.fields:
1498             name, column = field.get_attname_column()
1499             model_fields[converter(column)] = field
1500         return model_fields
1501 
1502 
1503 class Prefetch:
1504     def __init__(self, lookup, queryset=None, to_attr=None):
1505         # `prefetch_through` is the path we traverse to perform the prefetch.
1506         self.prefetch_through = lookup
1507         # `prefetch_to` is the path to the attribute that stores the result.
1508         self.prefetch_to = lookup
1509         if queryset is not None and not issubclass(queryset._iterable_class, ModelIterable):
1510             raise ValueError('Prefetch querysets cannot use values().')
1511         if to_attr:
1512             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])
1513 
1514         self.queryset = queryset
1515         self.to_attr = to_attr
1516 
1517     def __getstate__(self):
1518         obj_dict = self.__dict__.copy()
1519         if self.queryset is not None:
1520             # Prevent the QuerySet from being evaluated
1521             obj_dict['queryset'] = self.queryset._chain(
1522                 _result_cache=[],
1523                 _prefetch_done=True,
1524             )
1525         return obj_dict
1526 
1527     def add_prefix(self, prefix):
1528         self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through
1529         self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to
1530 
1531     def get_current_prefetch_to(self, level):
1532         return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])
1533 
1534     def get_current_to_attr(self, level):
1535         parts = self.prefetch_to.split(LOOKUP_SEP)
1536         to_attr = parts[level]
1537         as_attr = self.to_attr and level == len(parts) - 1
1538         return to_attr, as_attr
1539 
1540     def get_current_queryset(self, level):
1541         if self.get_current_prefetch_to(level) == self.prefetch_to:
1542             return self.queryset
1543         return None
1544 
1545     def __eq__(self, other):
1546         if not isinstance(other, Prefetch):
1547             return NotImplemented
1548         return self.prefetch_to == other.prefetch_to
1549 
1550     def __hash__(self):
1551         return hash((self.__class__, self.prefetch_to))
1552 
1553 
1554 def normalize_prefetch_lookups(lookups, prefix=None):
1555     """Normalize lookups into Prefetch objects."""
1556     ret = []
1557     for lookup in lookups:
1558         if not isinstance(lookup, Prefetch):
1559             lookup = Prefetch(lookup)
1560         if prefix:
1561             lookup.add_prefix(prefix)
1562         ret.append(lookup)
1563     return ret
1564 
1565 
1566 def prefetch_related_objects(model_instances, *related_lookups):
1567     """
1568     Populate prefetched object caches for a list of model instances based on
1569     the lookups/Prefetch instances given.
1570     """
1571     if not model_instances:
1572         return  # nothing to do
1573 
1574     # We need to be able to dynamically add to the list of prefetch_related
1575     # lookups that we look up (see below).  So we need some book keeping to
1576     # ensure we don't do duplicate work.
1577     done_queries = {}    # dictionary of things like 'foo__bar': [results]
1578 
1579     auto_lookups = set()  # we add to this as we go through.
1580     followed_descriptors = set()  # recursion protection
1581 
1582     all_lookups = normalize_prefetch_lookups(reversed(related_lookups))
1583     while all_lookups:
1584         lookup = all_lookups.pop()
1585         if lookup.prefetch_to in done_queries:
1586             if lookup.queryset is not None:
1587                 raise ValueError("'%s' lookup was already seen with a different queryset. "
1588                                  "You may need to adjust the ordering of your lookups." % lookup.prefetch_to)
1589 
1590             continue
1591 
1592         # Top level, the list of objects to decorate is the result cache
1593         # from the primary QuerySet. It won't be for deeper levels.
1594         obj_list = model_instances
1595 
1596         through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)
1597         for level, through_attr in enumerate(through_attrs):
1598             # Prepare main instances
1599             if not obj_list:
1600                 break
1601 
1602             prefetch_to = lookup.get_current_prefetch_to(level)
1603             if prefetch_to in done_queries:
1604                 # Skip any prefetching, and any object preparation
1605                 obj_list = done_queries[prefetch_to]
1606                 continue
1607 
1608             # Prepare objects:
1609             good_objects = True
1610             for obj in obj_list:
1611                 # Since prefetching can re-use instances, it is possible to have
1612                 # the same instance multiple times in obj_list, so obj might
1613                 # already be prepared.
1614                 if not hasattr(obj, '_prefetched_objects_cache'):
1615                     try:
1616                         obj._prefetched_objects_cache = {}
1617                     except (AttributeError, TypeError):
1618                         # Must be an immutable object from
1619                         # values_list(flat=True), for example (TypeError) or
1620                         # a QuerySet subclass that isn't returning Model
1621                         # instances (AttributeError), either in Django or a 3rd
1622                         # party. prefetch_related() doesn't make sense, so quit.
1623                         good_objects = False
1624                         break
1625             if not good_objects:
1626                 break
1627 
1628             # Descend down tree
1629 
1630             # We assume that objects retrieved are homogeneous (which is the premise
1631             # of prefetch_related), so what applies to first object applies to all.
1632             first_obj = obj_list[0]
1633             to_attr = lookup.get_current_to_attr(level)[0]
1634             prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)
1635 
1636             if not attr_found:
1637                 raise AttributeError("Cannot find '%s' on %s object, '%s' is an invalid "
1638                                      "parameter to prefetch_related()" %
1639                                      (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))
1640 
1641             if level == len(through_attrs) - 1 and prefetcher is None:
1642                 # Last one, this *must* resolve to something that supports
1643                 # prefetching, otherwise there is no point adding it and the
1644                 # developer asking for it has made a mistake.
1645                 raise ValueError("'%s' does not resolve to an item that supports "
1646                                  "prefetching - this is an invalid parameter to "
1647                                  "prefetch_related()." % lookup.prefetch_through)
1648 
1649             if prefetcher is not None and not is_fetched:
1650                 obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)
1651                 # We need to ensure we don't keep adding lookups from the
1652                 # same relationships to stop infinite recursion. So, if we
1653                 # are already on an automatically added lookup, don't add
1654                 # the new lookups from relationships we've seen already.
1655                 if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):
1656                     done_queries[prefetch_to] = obj_list
1657                     new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)
1658                     auto_lookups.update(new_lookups)
1659                     all_lookups.extend(new_lookups)
1660                 followed_descriptors.add(descriptor)
1661             else:
1662                 # Either a singly related object that has already been fetched
1663                 # (e.g. via select_related), or hopefully some other property
1664                 # that doesn't support prefetching but needs to be traversed.
1665 
1666                 # We replace the current list of parent objects with the list
1667                 # of related objects, filtering out empty or missing values so
1668                 # that we can continue with nullable or reverse relations.
1669                 new_obj_list = []
1670                 for obj in obj_list:
1671                     if through_attr in getattr(obj, '_prefetched_objects_cache', ()):
1672                         # If related objects have been prefetched, use the
1673                         # cache rather than the object's through_attr.
1674                         new_obj = list(obj._prefetched_objects_cache.get(through_attr))
1675                     else:
1676                         try:
1677                             new_obj = getattr(obj, through_attr)
1678                         except exceptions.ObjectDoesNotExist:
1679                             continue
1680                     if new_obj is None:
1681                         continue
1682                     # We special-case `list` rather than something more generic
1683                     # like `Iterable` because we don't want to accidentally match
1684                     # user models that define __iter__.
1685                     if isinstance(new_obj, list):
1686                         new_obj_list.extend(new_obj)
1687                     else:
1688                         new_obj_list.append(new_obj)
1689                 obj_list = new_obj_list
1690 
1691 
1692 def get_prefetcher(instance, through_attr, to_attr):
1693     """
1694     For the attribute 'through_attr' on the given instance, find
1695     an object that has a get_prefetch_queryset().
1696     Return a 4 tuple containing:
1697     (the object with get_prefetch_queryset (or None),
1698      the descriptor object representing this relationship (or None),
1699      a boolean that is False if the attribute was not found at all,
1700      a boolean that is True if the attribute has already been fetched)
1701     """
1702     prefetcher = None
1703     is_fetched = False
1704 
1705     # For singly related objects, we have to avoid getting the attribute
1706     # from the object, as this will trigger the query. So we first try
1707     # on the class, in order to get the descriptor object.
1708     rel_obj_descriptor = getattr(instance.__class__, through_attr, None)
1709     if rel_obj_descriptor is None:
1710         attr_found = hasattr(instance, through_attr)
1711     else:
1712         attr_found = True
1713         if rel_obj_descriptor:
1714             # singly related object, descriptor object has the
1715             # get_prefetch_queryset() method.
1716             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):
1717                 prefetcher = rel_obj_descriptor
1718                 if rel_obj_descriptor.is_cached(instance):
1719                     is_fetched = True
1720             else:
1721                 # descriptor doesn't support prefetching, so we go ahead and get
1722                 # the attribute on the instance rather than the class to
1723                 # support many related managers
1724                 rel_obj = getattr(instance, through_attr)
1725                 if hasattr(rel_obj, 'get_prefetch_queryset'):
1726                     prefetcher = rel_obj
1727                 if through_attr != to_attr:
1728                     # Special case cached_property instances because hasattr
1729                     # triggers attribute computation and assignment.
1730                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):
1731                         is_fetched = to_attr in instance.__dict__
1732                     else:
1733                         is_fetched = hasattr(instance, to_attr)
1734                 else:
1735                     is_fetched = through_attr in instance._prefetched_objects_cache
1736     return prefetcher, rel_obj_descriptor, attr_found, is_fetched
1737 
1738 
1739 def prefetch_one_level(instances, prefetcher, lookup, level):
1740     """
1741     Helper function for prefetch_related_objects().
1742 
1743     Run prefetches on all instances using the prefetcher object,
1744     assigning results to relevant caches in instance.
1745 
1746     Return the prefetched objects along with any additional prefetches that
1747     must be done due to prefetch_related lookups found from default managers.
1748     """
1749     # prefetcher must have a method get_prefetch_queryset() which takes a list
1750     # of instances, and returns a tuple:
1751 
1752     # (queryset of instances of self.model that are related to passed in instances,
1753     #  callable that gets value to be matched for returned instances,
1754     #  callable that gets value to be matched for passed in instances,
1755     #  boolean that is True for singly related objects,
1756     #  cache or field name to assign to,
1757     #  boolean that is True when the previous argument is a cache name vs a field name).
1758 
1759     # The 'values to be matched' must be hashable as they will be used
1760     # in a dictionary.
1761 
1762     rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (
1763         prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))
1764     # We have to handle the possibility that the QuerySet we just got back
1765     # contains some prefetch_related lookups. We don't want to trigger the
1766     # prefetch_related functionality by evaluating the query. Rather, we need
1767     # to merge in the prefetch_related lookups.
1768     # Copy the lookups in case it is a Prefetch object which could be reused
1769     # later (happens in nested prefetch_related).
1770     additional_lookups = [
1771         copy.copy(additional_lookup) for additional_lookup
1772         in getattr(rel_qs, '_prefetch_related_lookups', ())
1773     ]
1774     if additional_lookups:
1775         # Don't need to clone because the manager should have given us a fresh
1776         # instance, so we access an internal instead of using public interface
1777         # for performance reasons.
1778         rel_qs._prefetch_related_lookups = ()
1779 
1780     all_related_objects = list(rel_qs)
1781 
1782     rel_obj_cache = {}
1783     for rel_obj in all_related_objects:
1784         rel_attr_val = rel_obj_attr(rel_obj)
1785         rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)
1786 
1787     to_attr, as_attr = lookup.get_current_to_attr(level)
1788     # Make sure `to_attr` does not conflict with a field.
1789     if as_attr and instances:
1790         # We assume that objects retrieved are homogeneous (which is the premise
1791         # of prefetch_related), so what applies to first object applies to all.
1792         model = instances[0].__class__
1793         try:
1794             model._meta.get_field(to_attr)
1795         except exceptions.FieldDoesNotExist:
1796             pass
1797         else:
1798             msg = 'to_attr={} conflicts with a field on the {} model.'
1799             raise ValueError(msg.format(to_attr, model.__name__))
1800 
1801     # Whether or not we're prefetching the last part of the lookup.
1802     leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level
1803 
1804     for obj in instances:
1805         instance_attr_val = instance_attr(obj)
1806         vals = rel_obj_cache.get(instance_attr_val, [])
1807 
1808         if single:
1809             val = vals[0] if vals else None
1810             if as_attr:
1811                 # A to_attr has been given for the prefetch.
1812                 setattr(obj, to_attr, val)
1813             elif is_descriptor:
1814                 # cache_name points to a field name in obj.
1815                 # This field is a descriptor for a related object.
1816                 setattr(obj, cache_name, val)
1817             else:
1818                 # No to_attr has been given for this prefetch operation and the
1819                 # cache_name does not point to a descriptor. Store the value of
1820                 # the field in the object's field cache.
1821                 obj._state.fields_cache[cache_name] = val
1822         else:
1823             if as_attr:
1824                 setattr(obj, to_attr, vals)
1825             else:
1826                 manager = getattr(obj, to_attr)
1827                 if leaf and lookup.queryset is not None:
1828                     qs = manager._apply_rel_filters(lookup.queryset)
1829                 else:
1830                     qs = manager.get_queryset()
1831                 qs._result_cache = vals
1832                 # We don't want the individual qs doing prefetch_related now,
1833                 # since we have merged this into the current work.
1834                 qs._prefetch_done = True
1835                 obj._prefetched_objects_cache[cache_name] = qs
1836     return all_related_objects, additional_lookups
1837 
1838 
1839 class RelatedPopulator:
1840     """
1841     RelatedPopulator is used for select_related() object instantiation.
1842 
1843     The idea is that each select_related() model will be populated by a
1844     different RelatedPopulator instance. The RelatedPopulator instances get
1845     klass_info and select (computed in SQLCompiler) plus the used db as
1846     input for initialization. That data is used to compute which columns
1847     to use, how to instantiate the model, and how to populate the links
1848     between the objects.
1849 
1850     The actual creation of the objects is done in populate() method. This
1851     method gets row and from_obj as input and populates the select_related()
1852     model instance.
1853     """
1854     def __init__(self, klass_info, select, db):
1855         self.db = db
1856         # Pre-compute needed attributes. The attributes are:
1857         #  - model_cls: the possibly deferred model class to instantiate
1858         #  - either:
1859         #    - cols_start, cols_end: usually the columns in the row are
1860         #      in the same order model_cls.__init__ expects them, so we
1861         #      can instantiate by model_cls(*row[cols_start:cols_end])
1862         #    - reorder_for_init: When select_related descends to a child
1863         #      class, then we want to reuse the already selected parent
1864         #      data. However, in this case the parent data isn't necessarily
1865         #      in the same order that Model.__init__ expects it to be, so
1866         #      we have to reorder the parent data. The reorder_for_init
1867         #      attribute contains a function used to reorder the field data
1868         #      in the order __init__ expects it.
1869         #  - pk_idx: the index of the primary key field in the reordered
1870         #    model data. Used to check if a related object exists at all.
1871         #  - init_list: the field attnames fetched from the database. For
1872         #    deferred models this isn't the same as all attnames of the
1873         #    model's fields.
1874         #  - related_populators: a list of RelatedPopulator instances if
1875         #    select_related() descends to related models from this model.
1876         #  - local_setter, remote_setter: Methods to set cached values on
1877         #    the object being populated and on the remote object. Usually
1878         #    these are Field.set_cached_value() methods.
1879         select_fields = klass_info['select_fields']
1880         from_parent = klass_info['from_parent']
1881         if not from_parent:
1882             self.cols_start = select_fields[0]
1883             self.cols_end = select_fields[-1] + 1
1884             self.init_list = [
1885                 f[0].target.attname for f in select[self.cols_start:self.cols_end]
1886             ]
1887             self.reorder_for_init = None
1888         else:
1889             attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}
1890             model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)
1891             self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]
1892             self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])
1893 
1894         self.model_cls = klass_info['model']
1895         self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)
1896         self.related_populators = get_related_populators(klass_info, select, self.db)
1897         self.local_setter = klass_info['local_setter']
1898         self.remote_setter = klass_info['remote_setter']
1899 
1900     def populate(self, row, from_obj):
1901         if self.reorder_for_init:
1902             obj_data = self.reorder_for_init(row)
1903         else:
1904             obj_data = row[self.cols_start:self.cols_end]
1905         if obj_data[self.pk_idx] is None:
1906             obj = None
1907         else:
1908             obj = self.model_cls.from_db(self.db, self.init_list, obj_data)
1909             for rel_iter in self.related_populators:
1910                 rel_iter.populate(row, obj)
1911         self.local_setter(from_obj, obj)
1912         if obj is not None:
1913             self.remote_setter(obj, from_obj)
1914 
1915 
1916 def get_related_populators(klass_info, select, db):
1917     iterators = []
1918     related_klass_infos = klass_info.get('related_klass_infos', [])
1919     for rel_klass_info in related_klass_infos:
1920         rel_cls = RelatedPopulator(rel_klass_info, select, db)
1921         iterators.append(rel_cls)
1922     return iterators

</code><intended_behavior>Within the query construction or filter application sections that handle the `__isnull` lookup, implement a validation step to ensure that the value provided for `__isnull` is strictly a boolean. If a non-boolean value is provided, the code should raise a ValueError with a clear message indicating that only boolean values are acceptable for `__isnull` lookups. This behavior will prevent misuse and maintain consistency across different usage scenarios.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 Create SQL statements for QuerySets.
3 
4 The code in here encapsulates all of the SQL construction so that QuerySets
5 themselves do not have to (and could be backed by things other than SQL
6 databases). The abstraction barrier only works one way: this module has to know
7 all about the internals of models in order to get the information it needs.
8 """
9 import copy
10 import difflib
11 import functools
12 import inspect
13 import sys
14 import warnings
15 from collections import Counter, namedtuple
16 from collections.abc import Iterator, Mapping
17 from itertools import chain, count, product
18 from string import ascii_uppercase
19 
20 from django.core.exceptions import (
21     EmptyResultSet, FieldDoesNotExist, FieldError,
22 )
23 from django.db import DEFAULT_DB_ALIAS, NotSupportedError, connections
24 from django.db.models.aggregates import Count
25 from django.db.models.constants import LOOKUP_SEP
26 from django.db.models.expressions import (
27     BaseExpression, Col, F, OuterRef, Ref, SimpleCol,
28 )
29 from django.db.models.fields import Field
30 from django.db.models.fields.related_lookups import MultiColSource
31 from django.db.models.lookups import Lookup
32 from django.db.models.query_utils import (
33     Q, check_rel_lookup_compatibility, refs_expression,
34 )
35 from django.db.models.sql.constants import (
36     INNER, LOUTER, ORDER_DIR, ORDER_PATTERN, SINGLE,
37 )
38 from django.db.models.sql.datastructures import (
39     BaseTable, Empty, Join, MultiJoin,
40 )
41 from django.db.models.sql.where import (
42     AND, OR, ExtraWhere, NothingNode, WhereNode,
43 )
44 from django.utils.deprecation import RemovedInDjango40Warning
45 from django.utils.functional import cached_property
46 from django.utils.tree import Node
47 
48 __all__ = ['Query', 'RawQuery']
49 
50 
51 def get_field_names_from_opts(opts):
52     return set(chain.from_iterable(
53         (f.name, f.attname) if f.concrete else (f.name,)
54         for f in opts.get_fields()
55     ))
56 
57 
58 def get_children_from_q(q):
59     for child in q.children:
60         if isinstance(child, Node):
61             yield from get_children_from_q(child)
62         else:
63             yield child
64 
65 
66 JoinInfo = namedtuple(
67     'JoinInfo',
68     ('final_field', 'targets', 'opts', 'joins', 'path', 'transform_function')
69 )
70 
71 
72 def _get_col(target, field, alias, simple_col):
73     if simple_col:
74         return SimpleCol(target, field)
75     return target.get_col(alias, field)
76 
77 
78 class RawQuery:
79     """A single raw SQL query."""
80 
81     def __init__(self, sql, using, params=None):
82         self.params = params or ()
83         self.sql = sql
84         self.using = using
85         self.cursor = None
86 
87         # Mirror some properties of a normal query so that
88         # the compiler can be used to process results.
89         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
90         self.extra_select = {}
91         self.annotation_select = {}
92 
93     def chain(self, using):
94         return self.clone(using)
95 
96     def clone(self, using):
97         return RawQuery(self.sql, using, params=self.params)
98 
99     def get_columns(self):
100         if self.cursor is None:
101             self._execute_query()
102         converter = connections[self.using].introspection.identifier_converter
103         return [converter(column_meta[0])
104                 for column_meta in self.cursor.description]
105 
106     def __iter__(self):
107         # Always execute a new query for a new iterator.
108         # This could be optimized with a cache at the expense of RAM.
109         self._execute_query()
110         if not connections[self.using].features.can_use_chunked_reads:
111             # If the database can't use chunked reads we need to make sure we
112             # evaluate the entire query up front.
113             result = list(self.cursor)
114         else:
115             result = self.cursor
116         return iter(result)
117 
118     def __repr__(self):
119         return "<%s: %s>" % (self.__class__.__name__, self)
120 
121     @property
122     def params_type(self):
123         return dict if isinstance(self.params, Mapping) else tuple
124 
125     def __str__(self):
126         return self.sql % self.params_type(self.params)
127 
128     def _execute_query(self):
129         connection = connections[self.using]
130 
131         # Adapt parameters to the database, as much as possible considering
132         # that the target type isn't known. See #17755.
133         params_type = self.params_type
134         adapter = connection.ops.adapt_unknown_value
135         if params_type is tuple:
136             params = tuple(adapter(val) for val in self.params)
137         elif params_type is dict:
138             params = {key: adapter(val) for key, val in self.params.items()}
139         else:
140             raise RuntimeError("Unexpected params type: %s" % params_type)
141 
142         self.cursor = connection.cursor()
143         self.cursor.execute(self.sql, params)
144 
145 
146 class Query(BaseExpression):
147     """A single SQL query."""
148 
149     alias_prefix = 'T'
150     subq_aliases = frozenset([alias_prefix])
151 
152     compiler = 'SQLCompiler'
153 
154     def __init__(self, model, where=WhereNode):
155         self.model = model
156         self.alias_refcount = {}
157         # alias_map is the most important data structure regarding joins.
158         # It's used for recording which joins exist in the query and what
159         # types they are. The key is the alias of the joined table (possibly
160         # the table name) and the value is a Join-like object (see
161         # sql.datastructures.Join for more information).
162         self.alias_map = {}
163         # Sometimes the query contains references to aliases in outer queries (as
164         # a result of split_exclude). Correct alias quoting needs to know these
165         # aliases too.
166         self.external_aliases = set()
167         self.table_map = {}     # Maps table names to list of aliases.
168         self.default_cols = True
169         self.default_ordering = True
170         self.standard_ordering = True
171         self.used_aliases = set()
172         self.filter_is_sticky = False
173         self.subquery = False
174 
175         # SQL-related attributes
176         # Select and related select clauses are expressions to use in the
177         # SELECT clause of the query.
178         # The select is used for cases where we want to set up the select
179         # clause to contain other than default fields (values(), subqueries...)
180         # Note that annotations go to annotations dictionary.
181         self.select = ()
182         self.where = where()
183         self.where_class = where
184         # The group_by attribute can have one of the following forms:
185         #  - None: no group by at all in the query
186         #  - A tuple of expressions: group by (at least) those expressions.
187         #    String refs are also allowed for now.
188         #  - True: group by all select fields of the model
189         # See compiler.get_group_by() for details.
190         self.group_by = None
191         self.order_by = ()
192         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
193         self.distinct = False
194         self.distinct_fields = ()
195         self.select_for_update = False
196         self.select_for_update_nowait = False
197         self.select_for_update_skip_locked = False
198         self.select_for_update_of = ()
199 
200         self.select_related = False
201         # Arbitrary limit for select_related to prevents infinite recursion.
202         self.max_depth = 5
203 
204         # Holds the selects defined by a call to values() or values_list()
205         # excluding annotation_select and extra_select.
206         self.values_select = ()
207 
208         # SQL annotation-related attributes
209         self.annotations = {}  # Maps alias -> Annotation Expression
210         self.annotation_select_mask = None
211         self._annotation_select_cache = None
212 
213         # Set combination attributes
214         self.combinator = None
215         self.combinator_all = False
216         self.combined_queries = ()
217 
218         # These are for extensions. The contents are more or less appended
219         # verbatim to the appropriate clause.
220         self.extra = {}  # Maps col_alias -> (col_sql, params).
221         self.extra_select_mask = None
222         self._extra_select_cache = None
223 
224         self.extra_tables = ()
225         self.extra_order_by = ()
226 
227         # A tuple that is a set of model field names and either True, if these
228         # are the fields to defer, or False if these are the only fields to
229         # load.
230         self.deferred_loading = (frozenset(), True)
231 
232         self._filtered_relations = {}
233 
234         self.explain_query = False
235         self.explain_format = None
236         self.explain_options = {}
237 
238     @property
239     def output_field(self):
240         if len(self.select) == 1:
241             return self.select[0].field
242         elif len(self.annotation_select) == 1:
243             return next(iter(self.annotation_select.values())).output_field
244 
245     @property
246     def has_select_fields(self):
247         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)
248 
249     @cached_property
250     def base_table(self):
251         for alias in self.alias_map:
252             return alias
253 
254     def __str__(self):
255         """
256         Return the query as a string of SQL with the parameter values
257         substituted in (use sql_with_params() to see the unsubstituted string).
258 
259         Parameter values won't necessarily be quoted correctly, since that is
260         done by the database interface at execution time.
261         """
262         sql, params = self.sql_with_params()
263         return sql % params
264 
265     def sql_with_params(self):
266         """
267         Return the query as an SQL string and the parameters that will be
268         substituted into the query.
269         """
270         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
271 
272     def __deepcopy__(self, memo):
273         """Limit the amount of work when a Query is deepcopied."""
274         result = self.clone()
275         memo[id(self)] = result
276         return result
277 
278     def get_compiler(self, using=None, connection=None):
279         if using is None and connection is None:
280             raise ValueError("Need either using or connection")
281         if using:
282             connection = connections[using]
283         return connection.ops.compiler(self.compiler)(self, connection, using)
284 
285     def get_meta(self):
286         """
287         Return the Options instance (the model._meta) from which to start
288         processing. Normally, this is self.model._meta, but it can be changed
289         by subclasses.
290         """
291         return self.model._meta
292 
293     def clone(self):
294         """
295         Return a copy of the current Query. A lightweight alternative to
296         to deepcopy().
297         """
298         obj = Empty()
299         obj.__class__ = self.__class__
300         # Copy references to everything.
301         obj.__dict__ = self.__dict__.copy()
302         # Clone attributes that can't use shallow copy.
303         obj.alias_refcount = self.alias_refcount.copy()
304         obj.alias_map = self.alias_map.copy()
305         obj.external_aliases = self.external_aliases.copy()
306         obj.table_map = self.table_map.copy()
307         obj.where = self.where.clone()
308         obj.annotations = self.annotations.copy()
309         if self.annotation_select_mask is None:
310             obj.annotation_select_mask = None
311         else:
312             obj.annotation_select_mask = self.annotation_select_mask.copy()
313         # _annotation_select_cache cannot be copied, as doing so breaks the
314         # (necessary) state in which both annotations and
315         # _annotation_select_cache point to the same underlying objects.
316         # It will get re-populated in the cloned queryset the next time it's
317         # used.
318         obj._annotation_select_cache = None
319         obj.extra = self.extra.copy()
320         if self.extra_select_mask is None:
321             obj.extra_select_mask = None
322         else:
323             obj.extra_select_mask = self.extra_select_mask.copy()
324         if self._extra_select_cache is None:
325             obj._extra_select_cache = None
326         else:
327             obj._extra_select_cache = self._extra_select_cache.copy()
328         if self.select_related is not False:
329             # Use deepcopy because select_related stores fields in nested
330             # dicts.
331             obj.select_related = copy.deepcopy(obj.select_related)
332         if 'subq_aliases' in self.__dict__:
333             obj.subq_aliases = self.subq_aliases.copy()
334         obj.used_aliases = self.used_aliases.copy()
335         obj._filtered_relations = self._filtered_relations.copy()
336         # Clear the cached_property
337         try:
338             del obj.base_table
339         except AttributeError:
340             pass
341         return obj
342 
343     def chain(self, klass=None):
344         """
345         Return a copy of the current Query that's ready for another operation.
346         The klass argument changes the type of the Query, e.g. UpdateQuery.
347         """
348         obj = self.clone()
349         if klass and obj.__class__ != klass:
350             obj.__class__ = klass
351         if not obj.filter_is_sticky:
352             obj.used_aliases = set()
353         obj.filter_is_sticky = False
354         if hasattr(obj, '_setup_query'):
355             obj._setup_query()
356         return obj
357 
358     def relabeled_clone(self, change_map):
359         clone = self.clone()
360         clone.change_aliases(change_map)
361         return clone
362 
363     def rewrite_cols(self, annotation, col_cnt):
364         # We must make sure the inner query has the referred columns in it.
365         # If we are aggregating over an annotation, then Django uses Ref()
366         # instances to note this. However, if we are annotating over a column
367         # of a related model, then it might be that column isn't part of the
368         # SELECT clause of the inner query, and we must manually make sure
369         # the column is selected. An example case is:
370         #    .aggregate(Sum('author__awards'))
371         # Resolving this expression results in a join to author, but there
372         # is no guarantee the awards column of author is in the select clause
373         # of the query. Thus we must manually add the column to the inner
374         # query.
375         orig_exprs = annotation.get_source_expressions()
376         new_exprs = []
377         for expr in orig_exprs:
378             # FIXME: These conditions are fairly arbitrary. Identify a better
379             # method of having expressions decide which code path they should
380             # take.
381             if isinstance(expr, Ref):
382                 # Its already a Ref to subquery (see resolve_ref() for
383                 # details)
384                 new_exprs.append(expr)
385             elif isinstance(expr, (WhereNode, Lookup)):
386                 # Decompose the subexpressions further. The code here is
387                 # copied from the else clause, but this condition must appear
388                 # before the contains_aggregate/is_summary condition below.
389                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
390                 new_exprs.append(new_expr)
391             else:
392                 # Reuse aliases of expressions already selected in subquery.
393                 for col_alias, selected_annotation in self.annotation_select.items():
394                     if selected_annotation == expr:
395                         new_expr = Ref(col_alias, expr)
396                         break
397                 else:
398                     # An expression that is not selected the subquery.
399                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
400                         # Reference column or another aggregate. Select it
401                         # under a non-conflicting alias.
402                         col_cnt += 1
403                         col_alias = '__col%d' % col_cnt
404                         self.annotations[col_alias] = expr
405                         self.append_annotation_mask([col_alias])
406                         new_expr = Ref(col_alias, expr)
407                     else:
408                         # Some other expression not referencing database values
409                         # directly. Its subexpression might contain Cols.
410                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
411                 new_exprs.append(new_expr)
412         annotation.set_source_expressions(new_exprs)
413         return annotation, col_cnt
414 
415     def get_aggregation(self, using, added_aggregate_names):
416         """
417         Return the dictionary with the values of the existing aggregations.
418         """
419         if not self.annotation_select:
420             return {}
421         existing_annotations = [
422             annotation for alias, annotation
423             in self.annotations.items()
424             if alias not in added_aggregate_names
425         ]
426         # Decide if we need to use a subquery.
427         #
428         # Existing annotations would cause incorrect results as get_aggregation()
429         # must produce just one result and thus must not use GROUP BY. But we
430         # aren't smart enough to remove the existing annotations from the
431         # query, so those would force us to use GROUP BY.
432         #
433         # If the query has limit or distinct, or uses set operations, then
434         # those operations must be done in a subquery so that the query
435         # aggregates on the limit and/or distinct results instead of applying
436         # the distinct and limit after the aggregation.
437         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or
438                 self.distinct or self.combinator):
439             from django.db.models.sql.subqueries import AggregateQuery
440             outer_query = AggregateQuery(self.model)
441             inner_query = self.clone()
442             inner_query.select_for_update = False
443             inner_query.select_related = False
444             inner_query.set_annotation_mask(self.annotation_select)
445             if not self.is_sliced and not self.distinct_fields:
446                 # Queries with distinct_fields need ordering and when a limit
447                 # is applied we must take the slice from the ordered query.
448                 # Otherwise no need for ordering.
449                 inner_query.clear_ordering(True)
450             if not inner_query.distinct:
451                 # If the inner query uses default select and it has some
452                 # aggregate annotations, then we must make sure the inner
453                 # query is grouped by the main model's primary key. However,
454                 # clearing the select clause can alter results if distinct is
455                 # used.
456                 has_existing_aggregate_annotations = any(
457                     annotation for annotation in existing_annotations
458                     if getattr(annotation, 'contains_aggregate', True)
459                 )
460                 if inner_query.default_cols and has_existing_aggregate_annotations:
461                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
462                 inner_query.default_cols = False
463 
464             relabels = {t: 'subquery' for t in inner_query.alias_map}
465             relabels[None] = 'subquery'
466             # Remove any aggregates marked for reduction from the subquery
467             # and move them to the outer AggregateQuery.
468             col_cnt = 0
469             for alias, expression in list(inner_query.annotation_select.items()):
470                 annotation_select_mask = inner_query.annotation_select_mask
471                 if expression.is_summary:
472                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
473                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
474                     del inner_query.annotations[alias]
475                     annotation_select_mask.remove(alias)
476                 # Make sure the annotation_select wont use cached results.
477                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
478             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:
479                 # In case of Model.objects[0:3].count(), there would be no
480                 # field selected in the inner query, yet we must use a subquery.
481                 # So, make sure at least one field is selected.
482                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
483             try:
484                 outer_query.add_subquery(inner_query, using)
485             except EmptyResultSet:
486                 return {
487                     alias: None
488                     for alias in outer_query.annotation_select
489                 }
490         else:
491             outer_query = self
492             self.select = ()
493             self.default_cols = False
494             self.extra = {}
495 
496         outer_query.clear_ordering(True)
497         outer_query.clear_limits()
498         outer_query.select_for_update = False
499         outer_query.select_related = False
500         compiler = outer_query.get_compiler(using)
501         result = compiler.execute_sql(SINGLE)
502         if result is None:
503             result = [None] * len(outer_query.annotation_select)
504 
505         converters = compiler.get_converters(outer_query.annotation_select.values())
506         result = next(compiler.apply_converters((result,), converters))
507 
508         return dict(zip(outer_query.annotation_select, result))
509 
510     def get_count(self, using):
511         """
512         Perform a COUNT() query using the current filter constraints.
513         """
514         obj = self.clone()
515         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
516         number = obj.get_aggregation(using, ['__count'])['__count']
517         if number is None:
518             number = 0
519         return number
520 
521     def has_filters(self):
522         return self.where
523 
524     def has_results(self, using):
525         q = self.clone()
526         if not q.distinct:
527             if q.group_by is True:
528                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
529                 q.set_group_by()
530             q.clear_select_clause()
531         q.clear_ordering(True)
532         q.set_limits(high=1)
533         compiler = q.get_compiler(using=using)
534         return compiler.has_results()
535 
536     def explain(self, using, format=None, **options):
537         q = self.clone()
538         q.explain_query = True
539         q.explain_format = format
540         q.explain_options = options
541         compiler = q.get_compiler(using=using)
542         return '\n'.join(compiler.explain_query())
543 
544     def combine(self, rhs, connector):
545         """
546         Merge the 'rhs' query into the current one (with any 'rhs' effects
547         being applied *after* (that is, "to the right of") anything in the
548         current query. 'rhs' is not modified during a call to this function.
549 
550         The 'connector' parameter describes how to connect filters from the
551         'rhs' query.
552         """
553         assert self.model == rhs.model, \
554             "Cannot combine queries on two different base models."
555         assert not self.is_sliced, \
556             "Cannot combine queries once a slice has been taken."
557         assert self.distinct == rhs.distinct, \
558             "Cannot combine a unique query with a non-unique query."
559         assert self.distinct_fields == rhs.distinct_fields, \
560             "Cannot combine queries with different distinct fields."
561 
562         # Work out how to relabel the rhs aliases, if necessary.
563         change_map = {}
564         conjunction = (connector == AND)
565 
566         # Determine which existing joins can be reused. When combining the
567         # query with AND we must recreate all joins for m2m filters. When
568         # combining with OR we can reuse joins. The reason is that in AND
569         # case a single row can't fulfill a condition like:
570         #     revrel__col=1 & revrel__col=2
571         # But, there might be two different related rows matching this
572         # condition. In OR case a single True is enough, so single row is
573         # enough, too.
574         #
575         # Note that we will be creating duplicate joins for non-m2m joins in
576         # the AND case. The results will be correct but this creates too many
577         # joins. This is something that could be fixed later on.
578         reuse = set() if conjunction else set(self.alias_map)
579         # Base table must be present in the query - this is the same
580         # table on both sides.
581         self.get_initial_alias()
582         joinpromoter = JoinPromoter(connector, 2, False)
583         joinpromoter.add_votes(
584             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
585         rhs_votes = set()
586         # Now, add the joins from rhs query into the new query (skipping base
587         # table).
588         rhs_tables = list(rhs.alias_map)[1:]
589         for alias in rhs_tables:
590             join = rhs.alias_map[alias]
591             # If the left side of the join was already relabeled, use the
592             # updated alias.
593             join = join.relabeled_clone(change_map)
594             new_alias = self.join(join, reuse=reuse)
595             if join.join_type == INNER:
596                 rhs_votes.add(new_alias)
597             # We can't reuse the same join again in the query. If we have two
598             # distinct joins for the same connection in rhs query, then the
599             # combined query must have two joins, too.
600             reuse.discard(new_alias)
601             if alias != new_alias:
602                 change_map[alias] = new_alias
603             if not rhs.alias_refcount[alias]:
604                 # The alias was unused in the rhs query. Unref it so that it
605                 # will be unused in the new query, too. We have to add and
606                 # unref the alias so that join promotion has information of
607                 # the join type for the unused alias.
608                 self.unref_alias(new_alias)
609         joinpromoter.add_votes(rhs_votes)
610         joinpromoter.update_join_types(self)
611 
612         # Now relabel a copy of the rhs where-clause and add it to the current
613         # one.
614         w = rhs.where.clone()
615         w.relabel_aliases(change_map)
616         self.where.add(w, connector)
617 
618         # Selection columns and extra extensions are those provided by 'rhs'.
619         if rhs.select:
620             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])
621         else:
622             self.select = ()
623 
624         if connector == OR:
625             # It would be nice to be able to handle this, but the queries don't
626             # really make sense (or return consistent value sets). Not worth
627             # the extra complexity when you can write a real query instead.
628             if self.extra and rhs.extra:
629                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
630         self.extra.update(rhs.extra)
631         extra_select_mask = set()
632         if self.extra_select_mask is not None:
633             extra_select_mask.update(self.extra_select_mask)
634         if rhs.extra_select_mask is not None:
635             extra_select_mask.update(rhs.extra_select_mask)
636         if extra_select_mask:
637             self.set_extra_mask(extra_select_mask)
638         self.extra_tables += rhs.extra_tables
639 
640         # Ordering uses the 'rhs' ordering, unless it has none, in which case
641         # the current ordering is used.
642         self.order_by = rhs.order_by or self.order_by
643         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
644 
645     def deferred_to_data(self, target, callback):
646         """
647         Convert the self.deferred_loading data structure to an alternate data
648         structure, describing the field that *will* be loaded. This is used to
649         compute the columns to select from the database and also by the
650         QuerySet class to work out which fields are being initialized on each
651         model. Models that have all their fields included aren't mentioned in
652         the result, only those that have field restrictions in place.
653 
654         The "target" parameter is the instance that is populated (in place).
655         The "callback" is a function that is called whenever a (model, field)
656         pair need to be added to "target". It accepts three parameters:
657         "target", and the model and list of fields being added for that model.
658         """
659         field_names, defer = self.deferred_loading
660         if not field_names:
661             return
662         orig_opts = self.get_meta()
663         seen = {}
664         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
665         for field_name in field_names:
666             parts = field_name.split(LOOKUP_SEP)
667             cur_model = self.model._meta.concrete_model
668             opts = orig_opts
669             for name in parts[:-1]:
670                 old_model = cur_model
671                 if name in self._filtered_relations:
672                     name = self._filtered_relations[name].relation_name
673                 source = opts.get_field(name)
674                 if is_reverse_o2o(source):
675                     cur_model = source.related_model
676                 else:
677                     cur_model = source.remote_field.model
678                 opts = cur_model._meta
679                 # Even if we're "just passing through" this model, we must add
680                 # both the current model's pk and the related reference field
681                 # (if it's not a reverse relation) to the things we select.
682                 if not is_reverse_o2o(source):
683                     must_include[old_model].add(source)
684                 add_to_dict(must_include, cur_model, opts.pk)
685             field = opts.get_field(parts[-1])
686             is_reverse_object = field.auto_created and not field.concrete
687             model = field.related_model if is_reverse_object else field.model
688             model = model._meta.concrete_model
689             if model == opts.model:
690                 model = cur_model
691             if not is_reverse_o2o(field):
692                 add_to_dict(seen, model, field)
693 
694         if defer:
695             # We need to load all fields for each model, except those that
696             # appear in "seen" (for all models that appear in "seen"). The only
697             # slight complexity here is handling fields that exist on parent
698             # models.
699             workset = {}
700             for model, values in seen.items():
701                 for field in model._meta.local_fields:
702                     if field not in values:
703                         m = field.model._meta.concrete_model
704                         add_to_dict(workset, m, field)
705             for model, values in must_include.items():
706                 # If we haven't included a model in workset, we don't add the
707                 # corresponding must_include fields for that model, since an
708                 # empty set means "include all fields". That's why there's no
709                 # "else" branch here.
710                 if model in workset:
711                     workset[model].update(values)
712             for model, values in workset.items():
713                 callback(target, model, values)
714         else:
715             for model, values in must_include.items():
716                 if model in seen:
717                     seen[model].update(values)
718                 else:
719                     # As we've passed through this model, but not explicitly
720                     # included any fields, we have to make sure it's mentioned
721                     # so that only the "must include" fields are pulled in.
722                     seen[model] = values
723             # Now ensure that every model in the inheritance chain is mentioned
724             # in the parent list. Again, it must be mentioned to ensure that
725             # only "must include" fields are pulled in.
726             for model in orig_opts.get_parent_list():
727                 seen.setdefault(model, set())
728             for model, values in seen.items():
729                 callback(target, model, values)
730 
731     def table_alias(self, table_name, create=False, filtered_relation=None):
732         """
733         Return a table alias for the given table_name and whether this is a
734         new alias or not.
735 
736         If 'create' is true, a new alias is always created. Otherwise, the
737         most recently created alias for the table (if one exists) is reused.
738         """
739         alias_list = self.table_map.get(table_name)
740         if not create and alias_list:
741             alias = alias_list[0]
742             self.alias_refcount[alias] += 1
743             return alias, False
744 
745         # Create a new alias for this table.
746         if alias_list:
747             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
748             alias_list.append(alias)
749         else:
750             # The first occurrence of a table uses the table name directly.
751             alias = filtered_relation.alias if filtered_relation is not None else table_name
752             self.table_map[table_name] = [alias]
753         self.alias_refcount[alias] = 1
754         return alias, True
755 
756     def ref_alias(self, alias):
757         """Increases the reference count for this alias."""
758         self.alias_refcount[alias] += 1
759 
760     def unref_alias(self, alias, amount=1):
761         """Decreases the reference count for this alias."""
762         self.alias_refcount[alias] -= amount
763 
764     def promote_joins(self, aliases):
765         """
766         Promote recursively the join type of given aliases and its children to
767         an outer join. If 'unconditional' is False, only promote the join if
768         it is nullable or the parent join is an outer join.
769 
770         The children promotion is done to avoid join chains that contain a LOUTER
771         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
772         then we must also promote b->c automatically, or otherwise the promotion
773         of a->b doesn't actually change anything in the query results.
774         """
775         aliases = list(aliases)
776         while aliases:
777             alias = aliases.pop(0)
778             if self.alias_map[alias].join_type is None:
779                 # This is the base table (first FROM entry) - this table
780                 # isn't really joined at all in the query, so we should not
781                 # alter its join type.
782                 continue
783             # Only the first alias (skipped above) should have None join_type
784             assert self.alias_map[alias].join_type is not None
785             parent_alias = self.alias_map[alias].parent_alias
786             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
787             already_louter = self.alias_map[alias].join_type == LOUTER
788             if ((self.alias_map[alias].nullable or parent_louter) and
789                     not already_louter):
790                 self.alias_map[alias] = self.alias_map[alias].promote()
791                 # Join type of 'alias' changed, so re-examine all aliases that
792                 # refer to this one.
793                 aliases.extend(
794                     join for join in self.alias_map
795                     if self.alias_map[join].parent_alias == alias and join not in aliases
796                 )
797 
798     def demote_joins(self, aliases):
799         """
800         Change join type from LOUTER to INNER for all joins in aliases.
801 
802         Similarly to promote_joins(), this method must ensure no join chains
803         containing first an outer, then an inner join are generated. If we
804         are demoting b->c join in chain a LOUTER b LOUTER c then we must
805         demote a->b automatically, or otherwise the demotion of b->c doesn't
806         actually change anything in the query results. .
807         """
808         aliases = list(aliases)
809         while aliases:
810             alias = aliases.pop(0)
811             if self.alias_map[alias].join_type == LOUTER:
812                 self.alias_map[alias] = self.alias_map[alias].demote()
813                 parent_alias = self.alias_map[alias].parent_alias
814                 if self.alias_map[parent_alias].join_type == INNER:
815                     aliases.append(parent_alias)
816 
817     def reset_refcounts(self, to_counts):
818         """
819         Reset reference counts for aliases so that they match the value passed
820         in `to_counts`.
821         """
822         for alias, cur_refcount in self.alias_refcount.copy().items():
823             unref_amount = cur_refcount - to_counts.get(alias, 0)
824             self.unref_alias(alias, unref_amount)
825 
826     def change_aliases(self, change_map):
827         """
828         Change the aliases in change_map (which maps old-alias -> new-alias),
829         relabelling any references to them in select columns and the where
830         clause.
831         """
832         assert set(change_map).isdisjoint(change_map.values())
833 
834         # 1. Update references in "select" (normal columns plus aliases),
835         # "group by" and "where".
836         self.where.relabel_aliases(change_map)
837         if isinstance(self.group_by, tuple):
838             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])
839         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])
840         self.annotations = self.annotations and {
841             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()
842         }
843 
844         # 2. Rename the alias in the internal table/alias datastructures.
845         for old_alias, new_alias in change_map.items():
846             if old_alias not in self.alias_map:
847                 continue
848             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
849             self.alias_map[new_alias] = alias_data
850             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
851             del self.alias_refcount[old_alias]
852             del self.alias_map[old_alias]
853 
854             table_aliases = self.table_map[alias_data.table_name]
855             for pos, alias in enumerate(table_aliases):
856                 if alias == old_alias:
857                     table_aliases[pos] = new_alias
858                     break
859         self.external_aliases = {change_map.get(alias, alias)
860                                  for alias in self.external_aliases}
861 
862     def bump_prefix(self, outer_query):
863         """
864         Change the alias prefix to the next letter in the alphabet in a way
865         that the outer query's aliases and this query's aliases will not
866         conflict. Even tables that previously had no alias will get an alias
867         after this call.
868         """
869         def prefix_gen():
870             """
871             Generate a sequence of characters in alphabetical order:
872                 -> 'A', 'B', 'C', ...
873 
874             When the alphabet is finished, the sequence will continue with the
875             Cartesian product:
876                 -> 'AA', 'AB', 'AC', ...
877             """
878             alphabet = ascii_uppercase
879             prefix = chr(ord(self.alias_prefix) + 1)
880             yield prefix
881             for n in count(1):
882                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
883                 for s in product(seq, repeat=n):
884                     yield ''.join(s)
885                 prefix = None
886 
887         if self.alias_prefix != outer_query.alias_prefix:
888             # No clashes between self and outer query should be possible.
889             return
890 
891         # Explicitly avoid infinite loop. The constant divider is based on how
892         # much depth recursive subquery references add to the stack. This value
893         # might need to be adjusted when adding or removing function calls from
894         # the code path in charge of performing these operations.
895         local_recursion_limit = sys.getrecursionlimit() // 16
896         for pos, prefix in enumerate(prefix_gen()):
897             if prefix not in self.subq_aliases:
898                 self.alias_prefix = prefix
899                 break
900             if pos > local_recursion_limit:
901                 raise RecursionError(
902                     'Maximum recursion depth exceeded: too many subqueries.'
903                 )
904         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
905         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
906         self.change_aliases({
907             alias: '%s%d' % (self.alias_prefix, pos)
908             for pos, alias in enumerate(self.alias_map)
909         })
910 
911     def get_initial_alias(self):
912         """
913         Return the first alias for this query, after increasing its reference
914         count.
915         """
916         if self.alias_map:
917             alias = self.base_table
918             self.ref_alias(alias)
919         else:
920             alias = self.join(BaseTable(self.get_meta().db_table, None))
921         return alias
922 
923     def count_active_tables(self):
924         """
925         Return the number of tables in this query with a non-zero reference
926         count. After execution, the reference counts are zeroed, so tables
927         added in compiler will not be seen by this method.
928         """
929         return len([1 for count in self.alias_refcount.values() if count])
930 
931     def join(self, join, reuse=None, reuse_with_filtered_relation=False):
932         """
933         Return an alias for the 'join', either reusing an existing alias for
934         that join or creating a new one. 'join' is either a
935         sql.datastructures.BaseTable or Join.
936 
937         The 'reuse' parameter can be either None which means all joins are
938         reusable, or it can be a set containing the aliases that can be reused.
939 
940         The 'reuse_with_filtered_relation' parameter is used when computing
941         FilteredRelation instances.
942 
943         A join is always created as LOUTER if the lhs alias is LOUTER to make
944         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
945         joins are created as LOUTER if the join is nullable.
946         """
947         if reuse_with_filtered_relation and reuse:
948             reuse_aliases = [
949                 a for a, j in self.alias_map.items()
950                 if a in reuse and j.equals(join, with_filtered_relation=False)
951             ]
952         else:
953             reuse_aliases = [
954                 a for a, j in self.alias_map.items()
955                 if (reuse is None or a in reuse) and j == join
956             ]
957         if reuse_aliases:
958             if join.table_alias in reuse_aliases:
959                 reuse_alias = join.table_alias
960             else:
961                 # Reuse the most recent alias of the joined table
962                 # (a many-to-many relation may be joined multiple times).
963                 reuse_alias = reuse_aliases[-1]
964             self.ref_alias(reuse_alias)
965             return reuse_alias
966 
967         # No reuse is possible, so we need a new alias.
968         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)
969         if join.join_type:
970             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
971                 join_type = LOUTER
972             else:
973                 join_type = INNER
974             join.join_type = join_type
975         join.table_alias = alias
976         self.alias_map[alias] = join
977         return alias
978 
979     def join_parent_model(self, opts, model, alias, seen):
980         """
981         Make sure the given 'model' is joined in the query. If 'model' isn't
982         a parent of 'opts' or if it is None this method is a no-op.
983 
984         The 'alias' is the root alias for starting the join, 'seen' is a dict
985         of model -> alias of existing joins. It must also contain a mapping
986         of None -> some alias. This will be returned in the no-op case.
987         """
988         if model in seen:
989             return seen[model]
990         chain = opts.get_base_chain(model)
991         if not chain:
992             return alias
993         curr_opts = opts
994         for int_model in chain:
995             if int_model in seen:
996                 curr_opts = int_model._meta
997                 alias = seen[int_model]
998                 continue
999             # Proxy model have elements in base chain
1000             # with no parents, assign the new options
1001             # object and skip to the next base in that
1002             # case
1003             if not curr_opts.parents[int_model]:
1004                 curr_opts = int_model._meta
1005                 continue
1006             link_field = curr_opts.get_ancestor_link(int_model)
1007             join_info = self.setup_joins([link_field.name], curr_opts, alias)
1008             curr_opts = int_model._meta
1009             alias = seen[int_model] = join_info.joins[-1]
1010         return alias or seen[None]
1011 
1012     def add_annotation(self, annotation, alias, is_summary=False):
1013         """Add a single annotation expression to the Query."""
1014         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
1015                                                    summarize=is_summary)
1016         self.append_annotation_mask([alias])
1017         self.annotations[alias] = annotation
1018 
1019     def resolve_expression(self, query, *args, **kwargs):
1020         clone = self.clone()
1021         # Subqueries need to use a different set of aliases than the outer query.
1022         clone.bump_prefix(query)
1023         clone.subquery = True
1024         # It's safe to drop ordering if the queryset isn't using slicing,
1025         # distinct(*fields) or select_for_update().
1026         if (self.low_mark == 0 and self.high_mark is None and
1027                 not self.distinct_fields and
1028                 not self.select_for_update):
1029             clone.clear_ordering(True)
1030         clone.where.resolve_expression(query, *args, **kwargs)
1031         for key, value in clone.annotations.items():
1032             resolved = value.resolve_expression(query, *args, **kwargs)
1033             if hasattr(resolved, 'external_aliases'):
1034                 resolved.external_aliases.update(clone.alias_map)
1035             clone.annotations[key] = resolved
1036         # Outer query's aliases are considered external.
1037         clone.external_aliases.update(
1038             alias for alias, table in query.alias_map.items()
1039             if (
1040                 isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias
1041             ) or (
1042                 isinstance(table, BaseTable) and table.table_name != table.table_alias
1043             )
1044         )
1045         return clone
1046 
1047     def as_sql(self, compiler, connection):
1048         sql, params = self.get_compiler(connection=connection).as_sql()
1049         if self.subquery:
1050             sql = '(%s)' % sql
1051         return sql, params
1052 
1053     def resolve_lookup_value(self, value, can_reuse, allow_joins, simple_col):
1054         if hasattr(value, 'resolve_expression'):
1055             kwargs = {'reuse': can_reuse, 'allow_joins': allow_joins}
1056             if isinstance(value, F):
1057                 kwargs['simple_col'] = simple_col
1058             value = value.resolve_expression(self, **kwargs)
1059         elif isinstance(value, (list, tuple)):
1060             # The items of the iterable may be expressions and therefore need
1061             # to be resolved independently.
1062             resolved_values = []
1063             for sub_value in value:
1064                 if hasattr(sub_value, 'resolve_expression'):
1065                     if isinstance(sub_value, F):
1066                         resolved_values.append(sub_value.resolve_expression(
1067                             self, reuse=can_reuse, allow_joins=allow_joins,
1068                             simple_col=simple_col,
1069                         ))
1070                     else:
1071                         resolved_values.append(sub_value.resolve_expression(
1072                             self, reuse=can_reuse, allow_joins=allow_joins,
1073                         ))
1074                 else:
1075                     resolved_values.append(sub_value)
1076             value = tuple(resolved_values)
1077         return value
1078 
1079     def solve_lookup_type(self, lookup):
1080         """
1081         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').
1082         """
1083         lookup_splitted = lookup.split(LOOKUP_SEP)
1084         if self.annotations:
1085             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1086             if expression:
1087                 return expression_lookups, (), expression
1088         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1089         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1090         if len(lookup_parts) > 1 and not field_parts:
1091             raise FieldError(
1092                 'Invalid lookup "%s" for model %s".' %
1093                 (lookup, self.get_meta().model.__name__)
1094             )
1095         return lookup_parts, field_parts, False
1096 
1097     def check_query_object_type(self, value, opts, field):
1098         """
1099         Check whether the object passed while querying is of the correct type.
1100         If not, raise a ValueError specifying the wrong object.
1101         """
1102         if hasattr(value, '_meta'):
1103             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1104                 raise ValueError(
1105                     'Cannot query "%s": Must be "%s" instance.' %
1106                     (value, opts.object_name))
1107 
1108     def check_related_objects(self, field, value, opts):
1109         """Check the type of object passed to query relations."""
1110         if field.is_relation:
1111             # Check that the field and the queryset use the same model in a
1112             # query like .filter(author=Author.objects.all()). For example, the
1113             # opts would be Author's (from the author field) and value.model
1114             # would be Author.objects.all() queryset's .model (Author also).
1115             # The field is the related field on the lhs side.
1116             if (isinstance(value, Query) and not value.has_select_fields and
1117                     not check_rel_lookup_compatibility(value.model, opts, field)):
1118                 raise ValueError(
1119                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1120                     (value.model._meta.object_name, opts.object_name)
1121                 )
1122             elif hasattr(value, '_meta'):
1123                 self.check_query_object_type(value, opts, field)
1124             elif hasattr(value, '__iter__'):
1125                 for v in value:
1126                     self.check_query_object_type(v, opts, field)
1127 
1128     def check_filterable(self, expression):
1129         """Raise an error if expression cannot be used in a WHERE clause."""
1130         if not getattr(expression, 'filterable', 'True'):
1131             raise NotSupportedError(
1132                 expression.__class__.__name__ + ' is disallowed in the filter '
1133                 'clause.'
1134             )
1135         if hasattr(expression, 'get_source_expressions'):
1136             for expr in expression.get_source_expressions():
1137                 self.check_filterable(expr)
1138 
1139     def build_lookup(self, lookups, lhs, rhs):
1140         """
1141         Try to extract transforms and lookup from given lhs.
1142 
1143         The lhs value is something that works like SQLExpression.
1144         The rhs value is what the lookup is going to compare against.
1145         The lookups is a list of names to extract using get_lookup()
1146         and get_transform().
1147         """
1148         # __exact is the default lookup if one isn't given.
1149         *transforms, lookup_name = lookups or ['exact']
1150         for name in transforms:
1151             lhs = self.try_transform(lhs, name)
1152         # First try get_lookup() so that the lookup takes precedence if the lhs
1153         # supports both transform and lookup for the name.
1154         lookup_class = lhs.get_lookup(lookup_name)
1155         if not lookup_class:
1156             if lhs.field.is_relation:
1157                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))
1158             # A lookup wasn't found. Try to interpret the name as a transform
1159             # and do an Exact lookup against it.
1160             lhs = self.try_transform(lhs, lookup_name)
1161             lookup_name = 'exact'
1162             lookup_class = lhs.get_lookup(lookup_name)
1163             if not lookup_class:
1164                 return
1165 
1166         lookup = lookup_class(lhs, rhs)
1167         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
1168         # uses of None as a query value unless the lookup supports it.
1169         if lookup.rhs is None and not lookup.can_use_none_as_rhs:
1170             if lookup_name not in ('exact', 'iexact'):
1171                 raise ValueError("Cannot use None as a query value")
1172             return lhs.get_lookup('isnull')(lhs, True)
1173 
1174         # For Oracle '' is equivalent to null. The check must be done at this
1175         # stage because join promotion can't be done in the compiler. Using
1176         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.
1177         # A similar thing is done in is_nullable(), too.
1178         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1179                 lookup_name == 'exact' and lookup.rhs == ''):
1180             return lhs.get_lookup('isnull')(lhs, True)
1181 
1182         return lookup
1183 
1184     def try_transform(self, lhs, name):
1185         """
1186         Helper method for build_lookup(). Try to fetch and initialize
1187         a transform for name parameter from lhs.
1188         """
1189         transform_class = lhs.get_transform(name)
1190         if transform_class:
1191             return transform_class(lhs)
1192         else:
1193             output_field = lhs.output_field.__class__
1194             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())
1195             if suggested_lookups:
1196                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)
1197             else:
1198                 suggestion = '.'
1199             raise FieldError(
1200                 "Unsupported lookup '%s' for %s or join on the field not "
1201                 "permitted%s" % (name, output_field.__name__, suggestion)
1202             )
1203 
1204     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1205                      can_reuse=None, allow_joins=True, split_subq=True,
1206                      reuse_with_filtered_relation=False, simple_col=False):
1207         """
1208         Build a WhereNode for a single filter clause but don't add it
1209         to this Query. Query.add_q() will then add this filter to the where
1210         Node.
1211 
1212         The 'branch_negated' tells us if the current branch contains any
1213         negations. This will be used to determine if subqueries are needed.
1214 
1215         The 'current_negated' is used to determine if the current filter is
1216         negated or not and this will be used to determine if IS NULL filtering
1217         is needed.
1218 
1219         The difference between current_negated and branch_negated is that
1220         branch_negated is set on first negation, but current_negated is
1221         flipped for each negation.
1222 
1223         Note that add_filter will not do any negating itself, that is done
1224         upper in the code by add_q().
1225 
1226         The 'can_reuse' is a set of reusable joins for multijoins.
1227 
1228         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse
1229         will be reused.
1230 
1231         The method will create a filter clause that can be added to the current
1232         query. However, if the filter isn't added to the query then the caller
1233         is responsible for unreffing the joins used.
1234         """
1235         if isinstance(filter_expr, dict):
1236             raise FieldError("Cannot parse keyword query as dict")
1237         if hasattr(filter_expr, 'resolve_expression') and getattr(filter_expr, 'conditional', False):
1238             if connections[DEFAULT_DB_ALIAS].ops.conditional_expression_supported_in_where_clause(filter_expr):
1239                 condition = filter_expr.resolve_expression(self)
1240             else:
1241                 # Expression is not supported in the WHERE clause, add
1242                 # comparison with True.
1243                 condition = self.build_lookup(['exact'], filter_expr.resolve_expression(self), True)
1244             clause = self.where_class()
1245             clause.add(condition, AND)
1246             return clause, []
1247         arg, value = filter_expr
1248         if not arg:
1249             raise FieldError("Cannot parse keyword query %r" % arg)
1250         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1251 
1252         self.check_filterable(reffed_expression)
1253 
1254         if not allow_joins and len(parts) > 1:
1255             raise FieldError("Joined field references are not permitted in this query")
1256 
1257         pre_joins = self.alias_refcount.copy()
1258         value = self.resolve_lookup_value(value, can_reuse, allow_joins, simple_col)
1259         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1260 
1261         self.check_filterable(value)
1262 
1263         clause = self.where_class()
1264         if reffed_expression:
1265             condition = self.build_lookup(lookups, reffed_expression, value)
1266             clause.add(condition, AND)
1267             return clause, []
1268 
1269         opts = self.get_meta()
1270         alias = self.get_initial_alias()
1271         allow_many = not branch_negated or not split_subq
1272 
1273         try:
1274             join_info = self.setup_joins(
1275                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1276                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1277             )
1278 
1279             # Prevent iterator from being consumed by check_related_objects()
1280             if isinstance(value, Iterator):
1281                 value = list(value)
1282             self.check_related_objects(join_info.final_field, value, join_info.opts)
1283 
1284             # split_exclude() needs to know which joins were generated for the
1285             # lookup parts
1286             self._lookup_joins = join_info.joins
1287         except MultiJoin as e:
1288             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1289 
1290         # Update used_joins before trimming since they are reused to determine
1291         # which joins could be later promoted to INNER.
1292         used_joins.update(join_info.joins)
1293         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1294         if can_reuse is not None:
1295             can_reuse.update(join_list)
1296 
1297         if join_info.final_field.is_relation:
1298             # No support for transforms for relational fields
1299             num_lookups = len(lookups)
1300             if num_lookups > 1:
1301                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1302             if len(targets) == 1:
1303                 col = _get_col(targets[0], join_info.final_field, alias, simple_col)
1304             else:
1305                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1306         else:
1307             col = _get_col(targets[0], join_info.final_field, alias, simple_col)
1308 
1309         condition = self.build_lookup(lookups, col, value)
1310         lookup_type = condition.lookup_name
1311         clause.add(condition, AND)
1312 
1313         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1314         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1315             require_outer = True
1316             if (lookup_type != 'isnull' and (
1317                     self.is_nullable(targets[0]) or
1318                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1319                 # The condition added here will be SQL like this:
1320                 # NOT (col IS NOT NULL), where the first NOT is added in
1321                 # upper layers of code. The reason for addition is that if col
1322                 # is null, then col != someval will result in SQL "unknown"
1323                 # which isn't the same as in Python. The Python None handling
1324                 # is wanted, and it can be gotten by
1325                 # (col IS NULL OR col != someval)
1326                 #   <=>
1327                 # NOT (col IS NOT NULL AND col = someval).
1328                 lookup_class = targets[0].get_lookup('isnull')
1329                 col = _get_col(targets[0], join_info.targets[0], alias, simple_col)
1330                 clause.add(lookup_class(col, False), AND)
1331         return clause, used_joins if not require_outer else ()
1332 
1333     def add_filter(self, filter_clause):
1334         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1335 
1336     def add_q(self, q_object):
1337         """
1338         A preprocessor for the internal _add_q(). Responsible for doing final
1339         join promotion.
1340         """
1341         # For join promotion this case is doing an AND for the added q_object
1342         # and existing conditions. So, any existing inner join forces the join
1343         # type to remain inner. Existing outer joins can however be demoted.
1344         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1345         # rel_a doesn't produce any rows, then the whole condition must fail.
1346         # So, demotion is OK.
1347         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}
1348         clause, _ = self._add_q(q_object, self.used_aliases)
1349         if clause:
1350             self.where.add(clause, AND)
1351         self.demote_joins(existing_inner)
1352 
1353     def build_where(self, q_object):
1354         return self._add_q(q_object, used_aliases=set(), allow_joins=False, simple_col=True)[0]
1355 
1356     def _add_q(self, q_object, used_aliases, branch_negated=False,
1357                current_negated=False, allow_joins=True, split_subq=True,
1358                simple_col=False):
1359         """Add a Q-object to the current filter."""
1360         connector = q_object.connector
1361         current_negated = current_negated ^ q_object.negated
1362         branch_negated = branch_negated or q_object.negated
1363         target_clause = self.where_class(connector=connector,
1364                                          negated=q_object.negated)
1365         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1366         for child in q_object.children:
1367             if isinstance(child, Node):
1368                 child_clause, needed_inner = self._add_q(
1369                     child, used_aliases, branch_negated,
1370                     current_negated, allow_joins, split_subq, simple_col)
1371                 joinpromoter.add_votes(needed_inner)
1372             else:
1373                 child_clause, needed_inner = self.build_filter(
1374                     child, can_reuse=used_aliases, branch_negated=branch_negated,
1375                     current_negated=current_negated, allow_joins=allow_joins,
1376                     split_subq=split_subq, simple_col=simple_col,
1377                 )
1378                 joinpromoter.add_votes(needed_inner)
1379             if child_clause:
1380                 target_clause.add(child_clause, connector)
1381         needed_inner = joinpromoter.update_join_types(self)
1382         return target_clause, needed_inner
1383 
1384     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):
1385         """Add a FilteredRelation object to the current filter."""
1386         connector = q_object.connector
1387         current_negated ^= q_object.negated
1388         branch_negated = branch_negated or q_object.negated
1389         target_clause = self.where_class(connector=connector, negated=q_object.negated)
1390         for child in q_object.children:
1391             if isinstance(child, Node):
1392                 child_clause = self.build_filtered_relation_q(
1393                     child, reuse=reuse, branch_negated=branch_negated,
1394                     current_negated=current_negated,
1395                 )
1396             else:
1397                 child_clause, _ = self.build_filter(
1398                     child, can_reuse=reuse, branch_negated=branch_negated,
1399                     current_negated=current_negated,
1400                     allow_joins=True, split_subq=False,
1401                     reuse_with_filtered_relation=True,
1402                 )
1403             target_clause.add(child_clause, connector)
1404         return target_clause
1405 
1406     def add_filtered_relation(self, filtered_relation, alias):
1407         filtered_relation.alias = alias
1408         lookups = dict(get_children_from_q(filtered_relation.condition))
1409         for lookup in chain((filtered_relation.relation_name,), lookups):
1410             lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)
1411             shift = 2 if not lookup_parts else 1
1412             if len(field_parts) > (shift + len(lookup_parts)):
1413                 raise ValueError(
1414                     "FilteredRelation's condition doesn't support nested "
1415                     "relations (got %r)." % lookup
1416                 )
1417         self._filtered_relations[filtered_relation.alias] = filtered_relation
1418 
1419     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1420         """
1421         Walk the list of names and turns them into PathInfo tuples. A single
1422         name in 'names' can generate multiple PathInfos (m2m, for example).
1423 
1424         'names' is the path of names to travel, 'opts' is the model Options we
1425         start the name resolving from, 'allow_many' is as for setup_joins().
1426         If fail_on_missing is set to True, then a name that can't be resolved
1427         will generate a FieldError.
1428 
1429         Return a list of PathInfo tuples. In addition return the final field
1430         (the last used join field) and target (which is a field guaranteed to
1431         contain the same value as the final field). Finally, return those names
1432         that weren't found (which are likely transforms and the final lookup).
1433         """
1434         path, names_with_path = [], []
1435         for pos, name in enumerate(names):
1436             cur_names_with_path = (name, [])
1437             if name == 'pk':
1438                 name = opts.pk.name
1439 
1440             field = None
1441             filtered_relation = None
1442             try:
1443                 field = opts.get_field(name)
1444             except FieldDoesNotExist:
1445                 if name in self.annotation_select:
1446                     field = self.annotation_select[name].output_field
1447                 elif name in self._filtered_relations and pos == 0:
1448                     filtered_relation = self._filtered_relations[name]
1449                     field = opts.get_field(filtered_relation.relation_name)
1450             if field is not None:
1451                 # Fields that contain one-to-many relations with a generic
1452                 # model (like a GenericForeignKey) cannot generate reverse
1453                 # relations and therefore cannot be used for reverse querying.
1454                 if field.is_relation and not field.related_model:
1455                     raise FieldError(
1456                         "Field %r does not generate an automatic reverse "
1457                         "relation and therefore cannot be used for reverse "
1458                         "querying. If it is a GenericForeignKey, consider "
1459                         "adding a GenericRelation." % name
1460                     )
1461                 try:
1462                     model = field.model._meta.concrete_model
1463                 except AttributeError:
1464                     # QuerySet.annotate() may introduce fields that aren't
1465                     # attached to a model.
1466                     model = None
1467             else:
1468                 # We didn't find the current field, so move position back
1469                 # one step.
1470                 pos -= 1
1471                 if pos == -1 or fail_on_missing:
1472                     available = sorted([
1473                         *get_field_names_from_opts(opts),
1474                         *self.annotation_select,
1475                         *self._filtered_relations,
1476                     ])
1477                     raise FieldError("Cannot resolve keyword '%s' into field. "
1478                                      "Choices are: %s" % (name, ", ".join(available)))
1479                 break
1480             # Check if we need any joins for concrete inheritance cases (the
1481             # field lives in parent, but we are currently in one of its
1482             # children)
1483             if model is not opts.model:
1484                 path_to_parent = opts.get_path_to_parent(model)
1485                 if path_to_parent:
1486                     path.extend(path_to_parent)
1487                     cur_names_with_path[1].extend(path_to_parent)
1488                     opts = path_to_parent[-1].to_opts
1489             if hasattr(field, 'get_path_info'):
1490                 pathinfos = field.get_path_info(filtered_relation)
1491                 if not allow_many:
1492                     for inner_pos, p in enumerate(pathinfos):
1493                         if p.m2m:
1494                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1495                             names_with_path.append(cur_names_with_path)
1496                             raise MultiJoin(pos + 1, names_with_path)
1497                 last = pathinfos[-1]
1498                 path.extend(pathinfos)
1499                 final_field = last.join_field
1500                 opts = last.to_opts
1501                 targets = last.target_fields
1502                 cur_names_with_path[1].extend(pathinfos)
1503                 names_with_path.append(cur_names_with_path)
1504             else:
1505                 # Local non-relational field.
1506                 final_field = field
1507                 targets = (field,)
1508                 if fail_on_missing and pos + 1 != len(names):
1509                     raise FieldError(
1510                         "Cannot resolve keyword %r into field. Join on '%s'"
1511                         " not permitted." % (names[pos + 1], name))
1512                 break
1513         return path, final_field, targets, names[pos + 1:]
1514 
1515     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,
1516                     reuse_with_filtered_relation=False):
1517         """
1518         Compute the necessary table joins for the passage through the fields
1519         given in 'names'. 'opts' is the Options class for the current model
1520         (which gives the table we are starting from), 'alias' is the alias for
1521         the table to start the joining from.
1522 
1523         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1524         can be None in which case all joins are reusable or a set of aliases
1525         that can be reused. Note that non-reverse foreign keys are always
1526         reusable when using setup_joins().
1527 
1528         The 'reuse_with_filtered_relation' can be used to force 'can_reuse'
1529         parameter and force the relation on the given connections.
1530 
1531         If 'allow_many' is False, then any reverse foreign key seen will
1532         generate a MultiJoin exception.
1533 
1534         Return the final field involved in the joins, the target field (used
1535         for any 'where' constraint), the final 'opts' value, the joins, the
1536         field path traveled to generate the joins, and a transform function
1537         that takes a field and alias and is equivalent to `field.get_col(alias)`
1538         in the simple case but wraps field transforms if they were included in
1539         names.
1540 
1541         The target field is the field containing the concrete value. Final
1542         field can be something different, for example foreign key pointing to
1543         that value. Final field is needed for example in some value
1544         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1545         key field for example).
1546         """
1547         joins = [alias]
1548         # The transform can't be applied yet, as joins must be trimmed later.
1549         # To avoid making every caller of this method look up transforms
1550         # directly, compute transforms here and create a partial that converts
1551         # fields to the appropriate wrapped version.
1552 
1553         def final_transformer(field, alias):
1554             return field.get_col(alias)
1555 
1556         # Try resolving all the names as fields first. If there's an error,
1557         # treat trailing names as lookups until a field can be resolved.
1558         last_field_exception = None
1559         for pivot in range(len(names), 0, -1):
1560             try:
1561                 path, final_field, targets, rest = self.names_to_path(
1562                     names[:pivot], opts, allow_many, fail_on_missing=True,
1563                 )
1564             except FieldError as exc:
1565                 if pivot == 1:
1566                     # The first item cannot be a lookup, so it's safe
1567                     # to raise the field error here.
1568                     raise
1569                 else:
1570                     last_field_exception = exc
1571             else:
1572                 # The transforms are the remaining items that couldn't be
1573                 # resolved into fields.
1574                 transforms = names[pivot:]
1575                 break
1576         for name in transforms:
1577             def transform(field, alias, *, name, previous):
1578                 try:
1579                     wrapped = previous(field, alias)
1580                     return self.try_transform(wrapped, name)
1581                 except FieldError:
1582                     # FieldError is raised if the transform doesn't exist.
1583                     if isinstance(final_field, Field) and last_field_exception:
1584                         raise last_field_exception
1585                     else:
1586                         raise
1587             final_transformer = functools.partial(transform, name=name, previous=final_transformer)
1588         # Then, add the path to the query's joins. Note that we can't trim
1589         # joins at this stage - we will need the information about join type
1590         # of the trimmed joins.
1591         for join in path:
1592             if join.filtered_relation:
1593                 filtered_relation = join.filtered_relation.clone()
1594                 table_alias = filtered_relation.alias
1595             else:
1596                 filtered_relation = None
1597                 table_alias = None
1598             opts = join.to_opts
1599             if join.direct:
1600                 nullable = self.is_nullable(join.join_field)
1601             else:
1602                 nullable = True
1603             connection = Join(
1604                 opts.db_table, alias, table_alias, INNER, join.join_field,
1605                 nullable, filtered_relation=filtered_relation,
1606             )
1607             reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None
1608             alias = self.join(
1609                 connection, reuse=reuse,
1610                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1611             )
1612             joins.append(alias)
1613             if filtered_relation:
1614                 filtered_relation.path = joins[:]
1615         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)
1616 
1617     def trim_joins(self, targets, joins, path):
1618         """
1619         The 'target' parameter is the final field being joined to, 'joins'
1620         is the full list of join aliases. The 'path' contain the PathInfos
1621         used to create the joins.
1622 
1623         Return the final target field and table alias and the new active
1624         joins.
1625 
1626         Always trim any direct join if the target column is already in the
1627         previous table. Can't trim reverse joins as it's unknown if there's
1628         anything on the other side of the join.
1629         """
1630         joins = joins[:]
1631         for pos, info in enumerate(reversed(path)):
1632             if len(joins) == 1 or not info.direct:
1633                 break
1634             if info.filtered_relation:
1635                 break
1636             join_targets = {t.column for t in info.join_field.foreign_related_fields}
1637             cur_targets = {t.column for t in targets}
1638             if not cur_targets.issubset(join_targets):
1639                 break
1640             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1641             targets = tuple(targets_dict[t.column] for t in targets)
1642             self.unref_alias(joins.pop())
1643         return targets, joins[-1], joins
1644 
1645     @classmethod
1646     def _gen_col_aliases(cls, exprs):
1647         for expr in exprs:
1648             if isinstance(expr, Col):
1649                 yield expr.alias
1650             else:
1651                 yield from cls._gen_col_aliases(expr.get_source_expressions())
1652 
1653     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False, simple_col=False):
1654         if not allow_joins and LOOKUP_SEP in name:
1655             raise FieldError("Joined field references are not permitted in this query")
1656         annotation = self.annotations.get(name)
1657         if annotation is not None:
1658             if not allow_joins:
1659                 for alias in self._gen_col_aliases([annotation]):
1660                     if isinstance(self.alias_map[alias], Join):
1661                         raise FieldError(
1662                             'Joined field references are not permitted in '
1663                             'this query'
1664                         )
1665             if summarize:
1666                 # Summarize currently means we are doing an aggregate() query
1667                 # which is executed as a wrapped subquery if any of the
1668                 # aggregate() elements reference an existing annotation. In
1669                 # that case we need to return a Ref to the subquery's annotation.
1670                 return Ref(name, self.annotation_select[name])
1671             else:
1672                 return annotation
1673         else:
1674             field_list = name.split(LOOKUP_SEP)
1675             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)
1676             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1677             if not allow_joins and len(join_list) > 1:
1678                 raise FieldError('Joined field references are not permitted in this query')
1679             if len(targets) > 1:
1680                 raise FieldError("Referencing multicolumn fields with F() objects "
1681                                  "isn't supported")
1682             # Verify that the last lookup in name is a field or a transform:
1683             # transform_function() raises FieldError if not.
1684             join_info.transform_function(targets[0], final_alias)
1685             if reuse is not None:
1686                 reuse.update(join_list)
1687             col = _get_col(targets[0], join_info.targets[0], join_list[-1], simple_col)
1688             return col
1689 
1690     def split_exclude(self, filter_expr, can_reuse, names_with_path):
1691         """
1692         When doing an exclude against any kind of N-to-many relation, we need
1693         to use a subquery. This method constructs the nested query, given the
1694         original exclude filter (filter_expr) and the portion up to the first
1695         N-to-many relation field.
1696 
1697         For example, if the origin filter is ~Q(child__name='foo'), filter_expr
1698         is ('child__name', 'foo') and can_reuse is a set of joins usable for
1699         filters in the original query.
1700 
1701         We will turn this into equivalent of:
1702             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1703                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1704 
1705         It might be worth it to consider using WHERE NOT EXISTS as that has
1706         saner null handling, and is easier for the backend's optimizer to
1707         handle.
1708         """
1709         filter_lhs, filter_rhs = filter_expr
1710         if isinstance(filter_rhs, OuterRef):
1711             filter_expr = (filter_lhs, OuterRef(filter_rhs))
1712         elif isinstance(filter_rhs, F):
1713             filter_expr = (filter_lhs, OuterRef(filter_rhs.name))
1714         # Generate the inner query.
1715         query = Query(self.model)
1716         query._filtered_relations = self._filtered_relations
1717         query.add_filter(filter_expr)
1718         query.clear_ordering(True)
1719         # Try to have as simple as possible subquery -> trim leading joins from
1720         # the subquery.
1721         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1722 
1723         # Add extra check to make sure the selected field will not be null
1724         # since we are adding an IN <subquery> clause. This prevents the
1725         # database from tripping over IN (...,NULL,...) selects and returning
1726         # nothing
1727         col = query.select[0]
1728         select_field = col.target
1729         alias = col.alias
1730         if self.is_nullable(select_field):
1731             lookup_class = select_field.get_lookup('isnull')
1732             lookup = lookup_class(select_field.get_col(alias), False)
1733             query.where.add(lookup, AND)
1734         if alias in can_reuse:
1735             pk = select_field.model._meta.pk
1736             # Need to add a restriction so that outer query's filters are in effect for
1737             # the subquery, too.
1738             query.bump_prefix(self)
1739             lookup_class = select_field.get_lookup('exact')
1740             # Note that the query.select[0].alias is different from alias
1741             # due to bump_prefix above.
1742             lookup = lookup_class(pk.get_col(query.select[0].alias),
1743                                   pk.get_col(alias))
1744             query.where.add(lookup, AND)
1745             query.external_aliases.add(alias)
1746 
1747         condition, needed_inner = self.build_filter(
1748             ('%s__in' % trimmed_prefix, query),
1749             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1750         if contains_louter:
1751             or_null_condition, _ = self.build_filter(
1752                 ('%s__isnull' % trimmed_prefix, True),
1753                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1754             condition.add(or_null_condition, OR)
1755             # Note that the end result will be:
1756             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1757             # This might look crazy but due to how IN works, this seems to be
1758             # correct. If the IS NOT NULL check is removed then outercol NOT
1759             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1760             # outercol IS NULL we will not match the row.
1761         return condition, needed_inner
1762 
1763     def set_empty(self):
1764         self.where.add(NothingNode(), AND)
1765 
1766     def is_empty(self):
1767         return any(isinstance(c, NothingNode) for c in self.where.children)
1768 
1769     def set_limits(self, low=None, high=None):
1770         """
1771         Adjust the limits on the rows retrieved. Use low/high to set these,
1772         as it makes it more Pythonic to read and write. When the SQL query is
1773         created, convert them to the appropriate offset and limit values.
1774 
1775         Apply any limits passed in here to the existing constraints. Add low
1776         to the current low value and clamp both to any existing high value.
1777         """
1778         if high is not None:
1779             if self.high_mark is not None:
1780                 self.high_mark = min(self.high_mark, self.low_mark + high)
1781             else:
1782                 self.high_mark = self.low_mark + high
1783         if low is not None:
1784             if self.high_mark is not None:
1785                 self.low_mark = min(self.high_mark, self.low_mark + low)
1786             else:
1787                 self.low_mark = self.low_mark + low
1788 
1789         if self.low_mark == self.high_mark:
1790             self.set_empty()
1791 
1792     def clear_limits(self):
1793         """Clear any existing limits."""
1794         self.low_mark, self.high_mark = 0, None
1795 
1796     @property
1797     def is_sliced(self):
1798         return self.low_mark != 0 or self.high_mark is not None
1799 
1800     def has_limit_one(self):
1801         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1
1802 
1803     def can_filter(self):
1804         """
1805         Return True if adding filters to this instance is still possible.
1806 
1807         Typically, this means no limits or offsets have been put on the results.
1808         """
1809         return not self.is_sliced
1810 
1811     def clear_select_clause(self):
1812         """Remove all fields from SELECT clause."""
1813         self.select = ()
1814         self.default_cols = False
1815         self.select_related = False
1816         self.set_extra_mask(())
1817         self.set_annotation_mask(())
1818 
1819     def clear_select_fields(self):
1820         """
1821         Clear the list of fields to select (but not extra_select columns).
1822         Some queryset types completely replace any existing list of select
1823         columns.
1824         """
1825         self.select = ()
1826         self.values_select = ()
1827 
1828     def add_select_col(self, col):
1829         self.select += col,
1830         self.values_select += col.output_field.name,
1831 
1832     def set_select(self, cols):
1833         self.default_cols = False
1834         self.select = tuple(cols)
1835 
1836     def add_distinct_fields(self, *field_names):
1837         """
1838         Add and resolve the given fields to the query's "distinct on" clause.
1839         """
1840         self.distinct_fields = field_names
1841         self.distinct = True
1842 
1843     def add_fields(self, field_names, allow_m2m=True):
1844         """
1845         Add the given (model) fields to the select set. Add the field names in
1846         the order specified.
1847         """
1848         alias = self.get_initial_alias()
1849         opts = self.get_meta()
1850 
1851         try:
1852             cols = []
1853             for name in field_names:
1854                 # Join promotion note - we must not remove any rows here, so
1855                 # if there is no existing joins, use outer join.
1856                 join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1857                 targets, final_alias, joins = self.trim_joins(
1858                     join_info.targets,
1859                     join_info.joins,
1860                     join_info.path,
1861                 )
1862                 for target in targets:
1863                     cols.append(join_info.transform_function(target, final_alias))
1864             if cols:
1865                 self.set_select(cols)
1866         except MultiJoin:
1867             raise FieldError("Invalid field name: '%s'" % name)
1868         except FieldError:
1869             if LOOKUP_SEP in name:
1870                 # For lookups spanning over relationships, show the error
1871                 # from the model on which the lookup failed.
1872                 raise
1873             else:
1874                 names = sorted([
1875                     *get_field_names_from_opts(opts), *self.extra,
1876                     *self.annotation_select, *self._filtered_relations
1877                 ])
1878                 raise FieldError("Cannot resolve keyword %r into field. "
1879                                  "Choices are: %s" % (name, ", ".join(names)))
1880 
1881     def add_ordering(self, *ordering):
1882         """
1883         Add items from the 'ordering' sequence to the query's "order by"
1884         clause. These items are either field names (not column names) --
1885         possibly with a direction prefix ('-' or '?') -- or OrderBy
1886         expressions.
1887 
1888         If 'ordering' is empty, clear all ordering from the query.
1889         """
1890         errors = []
1891         for item in ordering:
1892             if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):
1893                 errors.append(item)
1894             if getattr(item, 'contains_aggregate', False):
1895                 raise FieldError(
1896                     'Using an aggregate in order_by() without also including '
1897                     'it in annotate() is not allowed: %s' % item
1898                 )
1899         if errors:
1900             raise FieldError('Invalid order_by arguments: %s' % errors)
1901         if ordering:
1902             self.order_by += ordering
1903         else:
1904             self.default_ordering = False
1905 
1906     def clear_ordering(self, force_empty):
1907         """
1908         Remove any ordering settings. If 'force_empty' is True, there will be
1909         no ordering in the resulting query (not even the model's default).
1910         """
1911         self.order_by = ()
1912         self.extra_order_by = ()
1913         if force_empty:
1914             self.default_ordering = False
1915 
1916     def set_group_by(self):
1917         """
1918         Expand the GROUP BY clause required by the query.
1919 
1920         This will usually be the set of all non-aggregate fields in the
1921         return data. If the database backend supports grouping by the
1922         primary key, and the query would be equivalent, the optimization
1923         will be made automatically.
1924         """
1925         group_by = list(self.select)
1926         if self.annotation_select:
1927             for alias, annotation in self.annotation_select.items():
1928                 try:
1929                     inspect.getcallargs(annotation.get_group_by_cols, alias=alias)
1930                 except TypeError:
1931                     annotation_class = annotation.__class__
1932                     msg = (
1933                         '`alias=None` must be added to the signature of '
1934                         '%s.%s.get_group_by_cols().'
1935                     ) % (annotation_class.__module__, annotation_class.__qualname__)
1936                     warnings.warn(msg, category=RemovedInDjango40Warning)
1937                     group_by_cols = annotation.get_group_by_cols()
1938                 else:
1939                     group_by_cols = annotation.get_group_by_cols(alias=alias)
1940                 group_by.extend(group_by_cols)
1941         self.group_by = tuple(group_by)
1942 
1943     def add_select_related(self, fields):
1944         """
1945         Set up the select_related data structure so that we only select
1946         certain related models (as opposed to all models, when
1947         self.select_related=True).
1948         """
1949         if isinstance(self.select_related, bool):
1950             field_dict = {}
1951         else:
1952             field_dict = self.select_related
1953         for field in fields:
1954             d = field_dict
1955             for part in field.split(LOOKUP_SEP):
1956                 d = d.setdefault(part, {})
1957         self.select_related = field_dict
1958 
1959     def add_extra(self, select, select_params, where, params, tables, order_by):
1960         """
1961         Add data to the various extra_* attributes for user-created additions
1962         to the query.
1963         """
1964         if select:
1965             # We need to pair any placeholder markers in the 'select'
1966             # dictionary with their parameters in 'select_params' so that
1967             # subsequent updates to the select dictionary also adjust the
1968             # parameters appropriately.
1969             select_pairs = {}
1970             if select_params:
1971                 param_iter = iter(select_params)
1972             else:
1973                 param_iter = iter([])
1974             for name, entry in select.items():
1975                 entry = str(entry)
1976                 entry_params = []
1977                 pos = entry.find("%s")
1978                 while pos != -1:
1979                     if pos == 0 or entry[pos - 1] != '%':
1980                         entry_params.append(next(param_iter))
1981                     pos = entry.find("%s", pos + 2)
1982                 select_pairs[name] = (entry, entry_params)
1983             self.extra.update(select_pairs)
1984         if where or params:
1985             self.where.add(ExtraWhere(where, params), AND)
1986         if tables:
1987             self.extra_tables += tuple(tables)
1988         if order_by:
1989             self.extra_order_by = order_by
1990 
1991     def clear_deferred_loading(self):
1992         """Remove any fields from the deferred loading set."""
1993         self.deferred_loading = (frozenset(), True)
1994 
1995     def add_deferred_loading(self, field_names):
1996         """
1997         Add the given list of model field names to the set of fields to
1998         exclude from loading from the database when automatic column selection
1999         is done. Add the new field names to any existing field names that
2000         are deferred (or removed from any existing field names that are marked
2001         as the only ones for immediate loading).
2002         """
2003         # Fields on related models are stored in the literal double-underscore
2004         # format, so that we can use a set datastructure. We do the foo__bar
2005         # splitting and handling when computing the SQL column names (as part of
2006         # get_columns()).
2007         existing, defer = self.deferred_loading
2008         if defer:
2009             # Add to existing deferred names.
2010             self.deferred_loading = existing.union(field_names), True
2011         else:
2012             # Remove names from the set of any existing "immediate load" names.
2013             self.deferred_loading = existing.difference(field_names), False
2014 
2015     def add_immediate_loading(self, field_names):
2016         """
2017         Add the given list of model field names to the set of fields to
2018         retrieve when the SQL is executed ("immediate loading" fields). The
2019         field names replace any existing immediate loading field names. If
2020         there are field names already specified for deferred loading, remove
2021         those names from the new field_names before storing the new names
2022         for immediate loading. (That is, immediate loading overrides any
2023         existing immediate values, but respects existing deferrals.)
2024         """
2025         existing, defer = self.deferred_loading
2026         field_names = set(field_names)
2027         if 'pk' in field_names:
2028             field_names.remove('pk')
2029             field_names.add(self.get_meta().pk.name)
2030 
2031         if defer:
2032             # Remove any existing deferred names from the current set before
2033             # setting the new names.
2034             self.deferred_loading = field_names.difference(existing), False
2035         else:
2036             # Replace any existing "immediate load" field names.
2037             self.deferred_loading = frozenset(field_names), False
2038 
2039     def get_loaded_field_names(self):
2040         """
2041         If any fields are marked to be deferred, return a dictionary mapping
2042         models to a set of names in those fields that will be loaded. If a
2043         model is not in the returned dictionary, none of its fields are
2044         deferred.
2045 
2046         If no fields are marked for deferral, return an empty dictionary.
2047         """
2048         # We cache this because we call this function multiple times
2049         # (compiler.fill_related_selections, query.iterator)
2050         try:
2051             return self._loaded_field_names_cache
2052         except AttributeError:
2053             collection = {}
2054             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
2055             self._loaded_field_names_cache = collection
2056             return collection
2057 
2058     def get_loaded_field_names_cb(self, target, model, fields):
2059         """Callback used by get_deferred_field_names()."""
2060         target[model] = {f.attname for f in fields}
2061 
2062     def set_annotation_mask(self, names):
2063         """Set the mask of annotations that will be returned by the SELECT."""
2064         if names is None:
2065             self.annotation_select_mask = None
2066         else:
2067             self.annotation_select_mask = set(names)
2068         self._annotation_select_cache = None
2069 
2070     def append_annotation_mask(self, names):
2071         if self.annotation_select_mask is not None:
2072             self.set_annotation_mask(self.annotation_select_mask.union(names))
2073 
2074     def set_extra_mask(self, names):
2075         """
2076         Set the mask of extra select items that will be returned by SELECT.
2077         Don't remove them from the Query since they might be used later.
2078         """
2079         if names is None:
2080             self.extra_select_mask = None
2081         else:
2082             self.extra_select_mask = set(names)
2083         self._extra_select_cache = None
2084 
2085     def set_values(self, fields):
2086         self.select_related = False
2087         self.clear_deferred_loading()
2088         self.clear_select_fields()
2089 
2090         if self.group_by is True:
2091             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
2092             self.set_group_by()
2093             self.clear_select_fields()
2094 
2095         if fields:
2096             field_names = []
2097             extra_names = []
2098             annotation_names = []
2099             if not self.extra and not self.annotations:
2100                 # Shortcut - if there are no extra or annotations, then
2101                 # the values() clause must be just field names.
2102                 field_names = list(fields)
2103             else:
2104                 self.default_cols = False
2105                 for f in fields:
2106                     if f in self.extra_select:
2107                         extra_names.append(f)
2108                     elif f in self.annotation_select:
2109                         annotation_names.append(f)
2110                     else:
2111                         field_names.append(f)
2112             self.set_extra_mask(extra_names)
2113             self.set_annotation_mask(annotation_names)
2114         else:
2115             field_names = [f.attname for f in self.model._meta.concrete_fields]
2116 
2117         self.values_select = tuple(field_names)
2118         self.add_fields(field_names, True)
2119 
2120     @property
2121     def annotation_select(self):
2122         """
2123         Return the dictionary of aggregate columns that are not masked and
2124         should be used in the SELECT clause. Cache this result for performance.
2125         """
2126         if self._annotation_select_cache is not None:
2127             return self._annotation_select_cache
2128         elif not self.annotations:
2129             return {}
2130         elif self.annotation_select_mask is not None:
2131             self._annotation_select_cache = {
2132                 k: v for k, v in self.annotations.items()
2133                 if k in self.annotation_select_mask
2134             }
2135             return self._annotation_select_cache
2136         else:
2137             return self.annotations
2138 
2139     @property
2140     def extra_select(self):
2141         if self._extra_select_cache is not None:
2142             return self._extra_select_cache
2143         if not self.extra:
2144             return {}
2145         elif self.extra_select_mask is not None:
2146             self._extra_select_cache = {
2147                 k: v for k, v in self.extra.items()
2148                 if k in self.extra_select_mask
2149             }
2150             return self._extra_select_cache
2151         else:
2152             return self.extra
2153 
2154     def trim_start(self, names_with_path):
2155         """
2156         Trim joins from the start of the join path. The candidates for trim
2157         are the PathInfos in names_with_path structure that are m2m joins.
2158 
2159         Also set the select column so the start matches the join.
2160 
2161         This method is meant to be used for generating the subquery joins &
2162         cols in split_exclude().
2163 
2164         Return a lookup usable for doing outerq.filter(lookup=self) and a
2165         boolean indicating if the joins in the prefix contain a LEFT OUTER join.
2166         _"""
2167         all_paths = []
2168         for _, paths in names_with_path:
2169             all_paths.extend(paths)
2170         contains_louter = False
2171         # Trim and operate only on tables that were generated for
2172         # the lookup part of the query. That is, avoid trimming
2173         # joins generated for F() expressions.
2174         lookup_tables = [
2175             t for t in self.alias_map
2176             if t in self._lookup_joins or t == self.base_table
2177         ]
2178         for trimmed_paths, path in enumerate(all_paths):
2179             if path.m2m:
2180                 break
2181             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
2182                 contains_louter = True
2183             alias = lookup_tables[trimmed_paths]
2184             self.unref_alias(alias)
2185         # The path.join_field is a Rel, lets get the other side's field
2186         join_field = path.join_field.field
2187         # Build the filter prefix.
2188         paths_in_prefix = trimmed_paths
2189         trimmed_prefix = []
2190         for name, path in names_with_path:
2191             if paths_in_prefix - len(path) < 0:
2192                 break
2193             trimmed_prefix.append(name)
2194             paths_in_prefix -= len(path)
2195         trimmed_prefix.append(
2196             join_field.foreign_related_fields[0].name)
2197         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
2198         # Lets still see if we can trim the first join from the inner query
2199         # (that is, self). We can't do this for:
2200         # - LEFT JOINs because we would miss those rows that have nothing on
2201         #   the outer side,
2202         # - INNER JOINs from filtered relations because we would miss their
2203         #   filters.
2204         first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]
2205         if first_join.join_type != LOUTER and not first_join.filtered_relation:
2206             select_fields = [r[0] for r in join_field.related_fields]
2207             select_alias = lookup_tables[trimmed_paths + 1]
2208             self.unref_alias(lookup_tables[trimmed_paths])
2209             extra_restriction = join_field.get_extra_restriction(
2210                 self.where_class, None, lookup_tables[trimmed_paths + 1])
2211             if extra_restriction:
2212                 self.where.add(extra_restriction, AND)
2213         else:
2214             # TODO: It might be possible to trim more joins from the start of the
2215             # inner query if it happens to have a longer join chain containing the
2216             # values in select_fields. Lets punt this one for now.
2217             select_fields = [r[1] for r in join_field.related_fields]
2218             select_alias = lookup_tables[trimmed_paths]
2219         # The found starting point is likely a Join instead of a BaseTable reference.
2220         # But the first entry in the query's FROM clause must not be a JOIN.
2221         for table in self.alias_map:
2222             if self.alias_refcount[table] > 0:
2223                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
2224                 break
2225         self.set_select([f.get_col(select_alias) for f in select_fields])
2226         return trimmed_prefix, contains_louter
2227 
2228     def is_nullable(self, field):
2229         """
2230         Check if the given field should be treated as nullable.
2231 
2232         Some backends treat '' as null and Django treats such fields as
2233         nullable for those backends. In such situations field.null can be
2234         False even if we should treat the field as nullable.
2235         """
2236         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2237         # (nor should it have) knowledge of which connection is going to be
2238         # used. The proper fix would be to defer all decisions where
2239         # is_nullable() is needed to the compiler stage, but that is not easy
2240         # to do currently.
2241         return (
2242             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
2243             field.empty_strings_allowed
2244         ) or field.null
2245 
2246 
2247 def get_order_dir(field, default='ASC'):
2248     """
2249     Return the field name and direction for an order specification. For
2250     example, '-foo' is returned as ('foo', 'DESC').
2251 
2252     The 'default' param is used to indicate which way no prefix (or a '+'
2253     prefix) should sort. The '-' prefix always sorts the opposite way.
2254     """
2255     dirn = ORDER_DIR[default]
2256     if field[0] == '-':
2257         return field[1:], dirn[1]
2258     return field, dirn[0]
2259 
2260 
2261 def add_to_dict(data, key, value):
2262     """
2263     Add "value" to the set of values for "key", whether or not "key" already
2264     exists.
2265     """
2266     if key in data:
2267         data[key].add(value)
2268     else:
2269         data[key] = {value}
2270 
2271 
2272 def is_reverse_o2o(field):
2273     """
2274     Check if the given field is reverse-o2o. The field is expected to be some
2275     sort of relation field or related object.
2276     """
2277     return field.is_relation and field.one_to_one and not field.concrete
2278 
2279 
2280 class JoinPromoter:
2281     """
2282     A class to abstract away join promotion problems for complex filter
2283     conditions.
2284     """
2285 
2286     def __init__(self, connector, num_children, negated):
2287         self.connector = connector
2288         self.negated = negated
2289         if self.negated:
2290             if connector == AND:
2291                 self.effective_connector = OR
2292             else:
2293                 self.effective_connector = AND
2294         else:
2295             self.effective_connector = self.connector
2296         self.num_children = num_children
2297         # Maps of table alias to how many times it is seen as required for
2298         # inner and/or outer joins.
2299         self.votes = Counter()
2300 
2301     def add_votes(self, votes):
2302         """
2303         Add single vote per item to self.votes. Parameter can be any
2304         iterable.
2305         """
2306         self.votes.update(votes)
2307 
2308     def update_join_types(self, query):
2309         """
2310         Change join types so that the generated query is as efficient as
2311         possible, but still correct. So, change as many joins as possible
2312         to INNER, but don't make OUTER joins INNER if that could remove
2313         results from the query.
2314         """
2315         to_promote = set()
2316         to_demote = set()
2317         # The effective_connector is used so that NOT (a AND b) is treated
2318         # similarly to (a OR b) for join promotion.
2319         for table, votes in self.votes.items():
2320             # We must use outer joins in OR case when the join isn't contained
2321             # in all of the joins. Otherwise the INNER JOIN itself could remove
2322             # valid results. Consider the case where a model with rel_a and
2323             # rel_b relations is queried with rel_a__col=1 | rel_b__col=2. Now,
2324             # if rel_a join doesn't produce any results is null (for example
2325             # reverse foreign key or null value in direct foreign key), and
2326             # there is a matching row in rel_b with col=2, then an INNER join
2327             # to rel_a would remove a valid match from the query. So, we need
2328             # to promote any existing INNER to LOUTER (it is possible this
2329             # promotion in turn will be demoted later on).
2330             if self.effective_connector == 'OR' and votes < self.num_children:
2331                 to_promote.add(table)
2332             # If connector is AND and there is a filter that can match only
2333             # when there is a joinable row, then use INNER. For example, in
2334             # rel_a__col=1 & rel_b__col=2, if either of the rels produce NULL
2335             # as join output, then the col=1 or col=2 can't match (as
2336             # NULL=anything is always false).
2337             # For the OR case, if all children voted for a join to be inner,
2338             # then we can use INNER for the join. For example:
2339             #     (rel_a__col__icontains=Alex | rel_a__col__icontains=Russell)
2340             # then if rel_a doesn't produce any rows, the whole condition
2341             # can't match. Hence we can safely use INNER join.
2342             if self.effective_connector == 'AND' or (
2343                     self.effective_connector == 'OR' and votes == self.num_children):
2344                 to_demote.add(table)
2345             # Finally, what happens in cases where we have:
2346             #    (rel_a__col=1|rel_b__col=2) & rel_a__col__gte=0
2347             # Now, we first generate the OR clause, and promote joins for it
2348             # in the first if branch above. Both rel_a and rel_b are promoted
2349             # to LOUTER joins. After that we do the AND case. The OR case
2350             # voted no inner joins but the rel_a__col__gte=0 votes inner join
2351             # for rel_a. We demote it back to INNER join (in AND case a single
2352             # vote is enough). The demotion is OK, if rel_a doesn't produce
2353             # rows, then the rel_a__col__gte=0 clause can't be true, and thus
2354             # the whole clause must be false. So, it is safe to use INNER
2355             # join.
2356             # Note that in this example we could just as well have the __gte
2357             # clause and the OR clause swapped. Or we could replace the __gte
2358             # clause with an OR clause containing rel_a__col=1|rel_a__col=2,
2359             # and again we could safely demote to INNER.
2360         query.promote_joins(to_promote)
2361         query.demote_joins(to_demote)
2362         return to_demote

</code><intended_behavior>In the part of the code that translates Django query expressions (including `__isnull`) into SQL statements, ensure that a validation for `__isnull` values is performed early in the process. If a value other than a boolean is detected, raise an error before the SQL statement is constructed. This will preserve the integrity of the SQL queries generated and prevent potential database errors or unexpected behavior.</intended_behavior>]
2025-10-15 04:14:54.517 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 04:14:54.518 | INFO     | Search-only: skipping patch selection
2025-10-15 04:14:54.669 | INFO     | Task django__django-11905 completed successfully.
2025-10-15 04:14:54.670 | INFO     | No patch generated. You can try running ACR again.
