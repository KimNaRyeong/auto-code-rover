[
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.1/django/db/models/query.py",
        "start": 1,
        "end": 1922,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 The main QuerySet implementation. This provides the public API for the ORM.\n3 \"\"\"\n4 \n5 import copy\n6 import operator\n7 import warnings\n8 from collections import namedtuple\n9 from functools import lru_cache\n10 from itertools import chain\n11 \n12 from django.conf import settings\n13 from django.core import exceptions\n14 from django.db import (\n15     DJANGO_VERSION_PICKLE_KEY, IntegrityError, connections, router,\n16     transaction,\n17 )\n18 from django.db.models import DateField, DateTimeField, sql\n19 from django.db.models.constants import LOOKUP_SEP\n20 from django.db.models.deletion import Collector\n21 from django.db.models.expressions import Case, Expression, F, Value, When\n22 from django.db.models.fields import AutoField\n23 from django.db.models.functions import Cast, Trunc\n24 from django.db.models.query_utils import FilteredRelation, InvalidQuery, Q\n25 from django.db.models.sql.constants import CURSOR, GET_ITERATOR_CHUNK_SIZE\n26 from django.db.utils import NotSupportedError\n27 from django.utils import timezone\n28 from django.utils.functional import cached_property, partition\n29 from django.utils.version import get_version\n30 \n31 # The maximum number of results to fetch in a get() query.\n32 MAX_GET_RESULTS = 21\n33 \n34 # The maximum number of items to display in a QuerySet.__repr__\n35 REPR_OUTPUT_SIZE = 20\n36 \n37 \n38 class BaseIterable:\n39     def __init__(self, queryset, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n40         self.queryset = queryset\n41         self.chunked_fetch = chunked_fetch\n42         self.chunk_size = chunk_size\n43 \n44 \n45 class ModelIterable(BaseIterable):\n46     \"\"\"Iterable that yields a model instance for each row.\"\"\"\n47 \n48     def __iter__(self):\n49         queryset = self.queryset\n50         db = queryset.db\n51         compiler = queryset.query.get_compiler(using=db)\n52         # Execute the query. This will also fill compiler.select, klass_info,\n53         # and annotations.\n54         results = compiler.execute_sql(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n55         select, klass_info, annotation_col_map = (compiler.select, compiler.klass_info,\n56                                                   compiler.annotation_col_map)\n57         model_cls = klass_info['model']\n58         select_fields = klass_info['select_fields']\n59         model_fields_start, model_fields_end = select_fields[0], select_fields[-1] + 1\n60         init_list = [f[0].target.attname\n61                      for f in select[model_fields_start:model_fields_end]]\n62         related_populators = get_related_populators(klass_info, select, db)\n63         known_related_objects = [\n64             (field, related_objs, operator.attrgetter(*[\n65                 field.attname\n66                 if from_field == 'self' else\n67                 queryset.model._meta.get_field(from_field).attname\n68                 for from_field in field.from_fields\n69             ])) for field, related_objs in queryset._known_related_objects.items()\n70         ]\n71         for row in compiler.results_iter(results):\n72             obj = model_cls.from_db(db, init_list, row[model_fields_start:model_fields_end])\n73             for rel_populator in related_populators:\n74                 rel_populator.populate(row, obj)\n75             if annotation_col_map:\n76                 for attr_name, col_pos in annotation_col_map.items():\n77                     setattr(obj, attr_name, row[col_pos])\n78 \n79             # Add the known related objects to the model.\n80             for field, rel_objs, rel_getter in known_related_objects:\n81                 # Avoid overwriting objects loaded by, e.g., select_related().\n82                 if field.is_cached(obj):\n83                     continue\n84                 rel_obj_id = rel_getter(obj)\n85                 try:\n86                     rel_obj = rel_objs[rel_obj_id]\n87                 except KeyError:\n88                     pass  # May happen in qs1 | qs2 scenarios.\n89                 else:\n90                     setattr(obj, field.name, rel_obj)\n91 \n92             yield obj\n93 \n94 \n95 class ValuesIterable(BaseIterable):\n96     \"\"\"\n97     Iterable returned by QuerySet.values() that yields a dict for each row.\n98     \"\"\"\n99 \n100     def __iter__(self):\n101         queryset = self.queryset\n102         query = queryset.query\n103         compiler = query.get_compiler(queryset.db)\n104 \n105         # extra(select=...) cols are always at the start of the row.\n106         names = [\n107             *query.extra_select,\n108             *query.values_select,\n109             *query.annotation_select,\n110         ]\n111         indexes = range(len(names))\n112         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):\n113             yield {names[i]: row[i] for i in indexes}\n114 \n115 \n116 class ValuesListIterable(BaseIterable):\n117     \"\"\"\n118     Iterable returned by QuerySet.values_list(flat=False) that yields a tuple\n119     for each row.\n120     \"\"\"\n121 \n122     def __iter__(self):\n123         queryset = self.queryset\n124         query = queryset.query\n125         compiler = query.get_compiler(queryset.db)\n126 \n127         if queryset._fields:\n128             # extra(select=...) cols are always at the start of the row.\n129             names = [\n130                 *query.extra_select,\n131                 *query.values_select,\n132                 *query.annotation_select,\n133             ]\n134             fields = [*queryset._fields, *(f for f in query.annotation_select if f not in queryset._fields)]\n135             if fields != names:\n136                 # Reorder according to fields.\n137                 index_map = {name: idx for idx, name in enumerate(names)}\n138                 rowfactory = operator.itemgetter(*[index_map[f] for f in fields])\n139                 return map(\n140                     rowfactory,\n141                     compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n142                 )\n143         return compiler.results_iter(tuple_expected=True, chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size)\n144 \n145 \n146 class NamedValuesListIterable(ValuesListIterable):\n147     \"\"\"\n148     Iterable returned by QuerySet.values_list(named=True) that yields a\n149     namedtuple for each row.\n150     \"\"\"\n151 \n152     @staticmethod\n153     @lru_cache()\n154     def create_namedtuple_class(*names):\n155         # Cache namedtuple() with @lru_cache() since it's too slow to be\n156         # called for every QuerySet evaluation.\n157         return namedtuple('Row', names)\n158 \n159     def __iter__(self):\n160         queryset = self.queryset\n161         if queryset._fields:\n162             names = queryset._fields\n163         else:\n164             query = queryset.query\n165             names = [*query.extra_select, *query.values_select, *query.annotation_select]\n166         tuple_class = self.create_namedtuple_class(*names)\n167         new = tuple.__new__\n168         for row in super().__iter__():\n169             yield new(tuple_class, row)\n170 \n171 \n172 class FlatValuesListIterable(BaseIterable):\n173     \"\"\"\n174     Iterable returned by QuerySet.values_list(flat=True) that yields single\n175     values.\n176     \"\"\"\n177 \n178     def __iter__(self):\n179         queryset = self.queryset\n180         compiler = queryset.query.get_compiler(queryset.db)\n181         for row in compiler.results_iter(chunked_fetch=self.chunked_fetch, chunk_size=self.chunk_size):\n182             yield row[0]\n183 \n184 \n185 class QuerySet:\n186     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n187 \n188     def __init__(self, model=None, query=None, using=None, hints=None):\n189         self.model = model\n190         self._db = using\n191         self._hints = hints or {}\n192         self.query = query or sql.Query(self.model)\n193         self._result_cache = None\n194         self._sticky_filter = False\n195         self._for_write = False\n196         self._prefetch_related_lookups = ()\n197         self._prefetch_done = False\n198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n199         self._iterable_class = ModelIterable\n200         self._fields = None\n201 \n202     def as_manager(cls):\n203         # Address the circular dependency between `Queryset` and `Manager`.\n204         from django.db.models.manager import Manager\n205         manager = Manager.from_queryset(cls)()\n206         manager._built_with_as_manager = True\n207         return manager\n208     as_manager.queryset_only = True\n209     as_manager = classmethod(as_manager)\n210 \n211     ########################\n212     # PYTHON MAGIC METHODS #\n213     ########################\n214 \n215     def __deepcopy__(self, memo):\n216         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n217         obj = self.__class__()\n218         for k, v in self.__dict__.items():\n219             if k == '_result_cache':\n220                 obj.__dict__[k] = None\n221             else:\n222                 obj.__dict__[k] = copy.deepcopy(v, memo)\n223         return obj\n224 \n225     def __getstate__(self):\n226         # Force the cache to be fully populated.\n227         self._fetch_all()\n228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}\n229 \n230     def __setstate__(self, state):\n231         msg = None\n232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n233         if pickled_version:\n234             current_version = get_version()\n235             if current_version != pickled_version:\n236                 msg = (\n237                     \"Pickled queryset instance's Django version %s does not \"\n238                     \"match the current version %s.\" % (pickled_version, current_version)\n239                 )\n240         else:\n241             msg = \"Pickled queryset instance's Django version is not specified.\"\n242 \n243         if msg:\n244             warnings.warn(msg, RuntimeWarning, stacklevel=2)\n245 \n246         self.__dict__.update(state)\n247 \n248     def __repr__(self):\n249         data = list(self[:REPR_OUTPUT_SIZE + 1])\n250         if len(data) > REPR_OUTPUT_SIZE:\n251             data[-1] = \"...(remaining elements truncated)...\"\n252         return '<%s %r>' % (self.__class__.__name__, data)\n253 \n254     def __len__(self):\n255         self._fetch_all()\n256         return len(self._result_cache)\n257 \n258     def __iter__(self):\n259         \"\"\"\n260         The queryset iterator protocol uses three nested iterators in the\n261         default case:\n262             1. sql.compiler.execute_sql()\n263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n264                  using cursor.fetchmany(). This part is responsible for\n265                  doing some column masking, and returning the rows in chunks.\n266             2. sql.compiler.results_iter()\n267                - Returns one row at time. At this point the rows are still just\n268                  tuples. In some cases the return values are converted to\n269                  Python values at this location.\n270             3. self.iterator()\n271                - Responsible for turning the rows into model objects.\n272         \"\"\"\n273         self._fetch_all()\n274         return iter(self._result_cache)\n275 \n276     def __bool__(self):\n277         self._fetch_all()\n278         return bool(self._result_cache)\n279 \n280     def __getitem__(self, k):\n281         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n282         if not isinstance(k, (int, slice)):\n283             raise TypeError(\n284                 'QuerySet indices must be integers or slices, not %s.'\n285                 % type(k).__name__\n286             )\n287         assert ((not isinstance(k, slice) and (k >= 0)) or\n288                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n289                  (k.stop is None or k.stop >= 0))), \\\n290             \"Negative indexing is not supported.\"\n291 \n292         if self._result_cache is not None:\n293             return self._result_cache[k]\n294 \n295         if isinstance(k, slice):\n296             qs = self._chain()\n297             if k.start is not None:\n298                 start = int(k.start)\n299             else:\n300                 start = None\n301             if k.stop is not None:\n302                 stop = int(k.stop)\n303             else:\n304                 stop = None\n305             qs.query.set_limits(start, stop)\n306             return list(qs)[::k.step] if k.step else qs\n307 \n308         qs = self._chain()\n309         qs.query.set_limits(k, k + 1)\n310         qs._fetch_all()\n311         return qs._result_cache[0]\n312 \n313     def __and__(self, other):\n314         self._merge_sanity_check(other)\n315         if isinstance(other, EmptyQuerySet):\n316             return other\n317         if isinstance(self, EmptyQuerySet):\n318             return self\n319         combined = self._chain()\n320         combined._merge_known_related_objects(other)\n321         combined.query.combine(other.query, sql.AND)\n322         return combined\n323 \n324     def __or__(self, other):\n325         self._merge_sanity_check(other)\n326         if isinstance(self, EmptyQuerySet):\n327             return other\n328         if isinstance(other, EmptyQuerySet):\n329             return self\n330         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n331         combined = query._chain()\n332         combined._merge_known_related_objects(other)\n333         if not other.query.can_filter():\n334             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n335         combined.query.combine(other.query, sql.OR)\n336         return combined\n337 \n338     ####################################\n339     # METHODS THAT DO DATABASE QUERIES #\n340     ####################################\n341 \n342     def _iterator(self, use_chunked_fetch, chunk_size):\n343         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n344 \n345     def iterator(self, chunk_size=2000):\n346         \"\"\"\n347         An iterator over the results from applying this QuerySet to the\n348         database.\n349         \"\"\"\n350         if chunk_size <= 0:\n351             raise ValueError('Chunk size must be strictly positive.')\n352         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n353         return self._iterator(use_chunked_fetch, chunk_size)\n354 \n355     def aggregate(self, *args, **kwargs):\n356         \"\"\"\n357         Return a dictionary containing the calculations (aggregation)\n358         over the current queryset.\n359 \n360         If args is present the expression is passed as a kwarg using\n361         the Aggregate object's default alias.\n362         \"\"\"\n363         if self.query.distinct_fields:\n364             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n365         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n366         for arg in args:\n367             # The default_alias property raises TypeError if default_alias\n368             # can't be set automatically or AttributeError if it isn't an\n369             # attribute.\n370             try:\n371                 arg.default_alias\n372             except (AttributeError, TypeError):\n373                 raise TypeError(\"Complex aggregates require an alias\")\n374             kwargs[arg.default_alias] = arg\n375 \n376         query = self.query.chain()\n377         for (alias, aggregate_expr) in kwargs.items():\n378             query.add_annotation(aggregate_expr, alias, is_summary=True)\n379             if not query.annotations[alias].contains_aggregate:\n380                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n381         return query.get_aggregation(self.db, kwargs)\n382 \n383     def count(self):\n384         \"\"\"\n385         Perform a SELECT COUNT() and return the number of records as an\n386         integer.\n387 \n388         If the QuerySet is already fully cached, return the length of the\n389         cached results set to avoid multiple SELECT COUNT(*) calls.\n390         \"\"\"\n391         if self._result_cache is not None:\n392             return len(self._result_cache)\n393 \n394         return self.query.get_count(using=self.db)\n395 \n396     def get(self, *args, **kwargs):\n397         \"\"\"\n398         Perform the query and return a single object matching the given\n399         keyword arguments.\n400         \"\"\"\n401         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n402         if self.query.can_filter() and not self.query.distinct_fields:\n403             clone = clone.order_by()\n404         limit = None\n405         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n406             limit = MAX_GET_RESULTS\n407             clone.query.set_limits(high=limit)\n408         num = len(clone)\n409         if num == 1:\n410             return clone._result_cache[0]\n411         if not num:\n412             raise self.model.DoesNotExist(\n413                 \"%s matching query does not exist.\" %\n414                 self.model._meta.object_name\n415             )\n416         raise self.model.MultipleObjectsReturned(\n417             'get() returned more than one %s -- it returned %s!' % (\n418                 self.model._meta.object_name,\n419                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n420             )\n421         )\n422 \n423     def create(self, **kwargs):\n424         \"\"\"\n425         Create a new object with the given kwargs, saving it to the database\n426         and returning the created object.\n427         \"\"\"\n428         obj = self.model(**kwargs)\n429         self._for_write = True\n430         obj.save(force_insert=True, using=self.db)\n431         return obj\n432 \n433     def _populate_pk_values(self, objs):\n434         for obj in objs:\n435             if obj.pk is None:\n436                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n437 \n438     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n439         \"\"\"\n440         Insert each of the instances into the database. Do *not* call\n441         save() on each of the instances, do not send any pre/post_save\n442         signals, and do not set the primary key attribute if it is an\n443         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n444         Multi-table models are not supported.\n445         \"\"\"\n446         # When you bulk insert you don't get the primary keys back (if it's an\n447         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n448         # you can't insert into the child tables which references this. There\n449         # are two workarounds:\n450         # 1) This could be implemented if you didn't have an autoincrement pk\n451         # 2) You could do it by doing O(n) normal inserts into the parent\n452         #    tables to get the primary keys back and then doing a single bulk\n453         #    insert into the childmost table.\n454         # We currently set the primary keys on the objects when using\n455         # PostgreSQL via the RETURNING ID clause. It should be possible for\n456         # Oracle as well, but the semantics for extracting the primary keys is\n457         # trickier so it's not done yet.\n458         assert batch_size is None or batch_size > 0\n459         # Check that the parents share the same concrete model with the our\n460         # model to detect the inheritance pattern ConcreteGrandParent ->\n461         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n462         # would not identify that case as involving multiple tables.\n463         for parent in self.model._meta.get_parent_list():\n464             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n465                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n466         if not objs:\n467             return objs\n468         self._for_write = True\n469         connection = connections[self.db]\n470         opts = self.model._meta\n471         fields = opts.concrete_fields\n472         objs = list(objs)\n473         self._populate_pk_values(objs)\n474         with transaction.atomic(using=self.db, savepoint=False):\n475             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n476             if objs_with_pk:\n477                 returned_columns = self._batched_insert(\n478                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n479                 )\n480                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n481                     for result, field in zip(results, opts.db_returning_fields):\n482                         if field != opts.pk:\n483                             setattr(obj_with_pk, field.attname, result)\n484                 for obj_with_pk in objs_with_pk:\n485                     obj_with_pk._state.adding = False\n486                     obj_with_pk._state.db = self.db\n487             if objs_without_pk:\n488                 fields = [f for f in fields if not isinstance(f, AutoField)]\n489                 returned_columns = self._batched_insert(\n490                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,\n491                 )\n492                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n493                     assert len(returned_columns) == len(objs_without_pk)\n494                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n495                     for result, field in zip(results, opts.db_returning_fields):\n496                         setattr(obj_without_pk, field.attname, result)\n497                     obj_without_pk._state.adding = False\n498                     obj_without_pk._state.db = self.db\n499 \n500         return objs\n501 \n502     def bulk_update(self, objs, fields, batch_size=None):\n503         \"\"\"\n504         Update the given fields in each of the given objects in the database.\n505         \"\"\"\n506         if batch_size is not None and batch_size < 0:\n507             raise ValueError('Batch size must be a positive integer.')\n508         if not fields:\n509             raise ValueError('Field names must be given to bulk_update().')\n510         objs = tuple(objs)\n511         if any(obj.pk is None for obj in objs):\n512             raise ValueError('All bulk_update() objects must have a primary key set.')\n513         fields = [self.model._meta.get_field(name) for name in fields]\n514         if any(not f.concrete or f.many_to_many for f in fields):\n515             raise ValueError('bulk_update() can only be used with concrete fields.')\n516         if any(f.primary_key for f in fields):\n517             raise ValueError('bulk_update() cannot be used with primary key fields.')\n518         if not objs:\n519             return\n520         # PK is used twice in the resulting update query, once in the filter\n521         # and once in the WHEN. Each field will also have one CAST.\n522         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n523         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n524         requires_casting = connections[self.db].features.requires_casted_case_in_updates\n525         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n526         updates = []\n527         for batch_objs in batches:\n528             update_kwargs = {}\n529             for field in fields:\n530                 when_statements = []\n531                 for obj in batch_objs:\n532                     attr = getattr(obj, field.attname)\n533                     if not isinstance(attr, Expression):\n534                         attr = Value(attr, output_field=field)\n535                     when_statements.append(When(pk=obj.pk, then=attr))\n536                 case_statement = Case(*when_statements, output_field=field)\n537                 if requires_casting:\n538                     case_statement = Cast(case_statement, output_field=field)\n539                 update_kwargs[field.attname] = case_statement\n540             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n541         with transaction.atomic(using=self.db, savepoint=False):\n542             for pks, update_kwargs in updates:\n543                 self.filter(pk__in=pks).update(**update_kwargs)\n544     bulk_update.alters_data = True\n545 \n546     def get_or_create(self, defaults=None, **kwargs):\n547         \"\"\"\n548         Look up an object with the given kwargs, creating one if necessary.\n549         Return a tuple of (object, created), where created is a boolean\n550         specifying whether an object was created.\n551         \"\"\"\n552         # The get() needs to be targeted at the write database in order\n553         # to avoid potential transaction consistency problems.\n554         self._for_write = True\n555         try:\n556             return self.get(**kwargs), False\n557         except self.model.DoesNotExist:\n558             params = self._extract_model_params(defaults, **kwargs)\n559             return self._create_object_from_params(kwargs, params)\n560 \n561     def update_or_create(self, defaults=None, **kwargs):\n562         \"\"\"\n563         Look up an object with the given kwargs, updating one with defaults\n564         if it exists, otherwise create a new one.\n565         Return a tuple (object, created), where created is a boolean\n566         specifying whether an object was created.\n567         \"\"\"\n568         defaults = defaults or {}\n569         self._for_write = True\n570         with transaction.atomic(using=self.db):\n571             try:\n572                 obj = self.select_for_update().get(**kwargs)\n573             except self.model.DoesNotExist:\n574                 params = self._extract_model_params(defaults, **kwargs)\n575                 # Lock the row so that a concurrent update is blocked until\n576                 # after update_or_create() has performed its save.\n577                 obj, created = self._create_object_from_params(kwargs, params, lock=True)\n578                 if created:\n579                     return obj, created\n580             for k, v in defaults.items():\n581                 setattr(obj, k, v() if callable(v) else v)\n582             obj.save(using=self.db)\n583         return obj, False\n584 \n585     def _create_object_from_params(self, lookup, params, lock=False):\n586         \"\"\"\n587         Try to create an object using passed params. Used by get_or_create()\n588         and update_or_create().\n589         \"\"\"\n590         try:\n591             with transaction.atomic(using=self.db):\n592                 params = {k: v() if callable(v) else v for k, v in params.items()}\n593                 obj = self.create(**params)\n594             return obj, True\n595         except IntegrityError as e:\n596             try:\n597                 qs = self.select_for_update() if lock else self\n598                 return qs.get(**lookup), False\n599             except self.model.DoesNotExist:\n600                 pass\n601             raise e\n602 \n603     def _extract_model_params(self, defaults, **kwargs):\n604         \"\"\"\n605         Prepare `params` for creating a model instance based on the given\n606         kwargs; for use by get_or_create() and update_or_create().\n607         \"\"\"\n608         defaults = defaults or {}\n609         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n610         params.update(defaults)\n611         property_names = self.model._meta._property_names\n612         invalid_params = []\n613         for param in params:\n614             try:\n615                 self.model._meta.get_field(param)\n616             except exceptions.FieldDoesNotExist:\n617                 # It's okay to use a model's property if it has a setter.\n618                 if not (param in property_names and getattr(self.model, param).fset):\n619                     invalid_params.append(param)\n620         if invalid_params:\n621             raise exceptions.FieldError(\n622                 \"Invalid field name(s) for model %s: '%s'.\" % (\n623                     self.model._meta.object_name,\n624                     \"', '\".join(sorted(invalid_params)),\n625                 ))\n626         return params\n627 \n628     def _earliest(self, *fields):\n629         \"\"\"\n630         Return the earliest object according to fields (if given) or by the\n631         model's Meta.get_latest_by.\n632         \"\"\"\n633         if fields:\n634             order_by = fields\n635         else:\n636             order_by = getattr(self.model._meta, 'get_latest_by')\n637             if order_by and not isinstance(order_by, (tuple, list)):\n638                 order_by = (order_by,)\n639         if order_by is None:\n640             raise ValueError(\n641                 \"earliest() and latest() require either fields as positional \"\n642                 \"arguments or 'get_latest_by' in the model's Meta.\"\n643             )\n644 \n645         assert not self.query.is_sliced, \\\n646             \"Cannot change a query once a slice has been taken.\"\n647         obj = self._chain()\n648         obj.query.set_limits(high=1)\n649         obj.query.clear_ordering(force_empty=True)\n650         obj.query.add_ordering(*order_by)\n651         return obj.get()\n652 \n653     def earliest(self, *fields):\n654         return self._earliest(*fields)\n655 \n656     def latest(self, *fields):\n657         return self.reverse()._earliest(*fields)\n658 \n659     def first(self):\n660         \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n661         for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n662             return obj\n663 \n664     def last(self):\n665         \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n666         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n667             return obj\n668 \n669     def in_bulk(self, id_list=None, *, field_name='pk'):\n670         \"\"\"\n671         Return a dictionary mapping each of the given IDs to the object with\n672         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n673         \"\"\"\n674         assert not self.query.is_sliced, \\\n675             \"Cannot use 'limit' or 'offset' with in_bulk\"\n676         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:\n677             raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n678         if id_list is not None:\n679             if not id_list:\n680                 return {}\n681             filter_key = '{}__in'.format(field_name)\n682             batch_size = connections[self.db].features.max_query_params\n683             id_list = tuple(id_list)\n684             # If the database has a limit on the number of query parameters\n685             # (e.g. SQLite), retrieve objects in batches if necessary.\n686             if batch_size and batch_size < len(id_list):\n687                 qs = ()\n688                 for offset in range(0, len(id_list), batch_size):\n689                     batch = id_list[offset:offset + batch_size]\n690                     qs += tuple(self.filter(**{filter_key: batch}).order_by())\n691             else:\n692                 qs = self.filter(**{filter_key: id_list}).order_by()\n693         else:\n694             qs = self._chain()\n695         return {getattr(obj, field_name): obj for obj in qs}\n696 \n697     def delete(self):\n698         \"\"\"Delete the records in the current QuerySet.\"\"\"\n699         assert not self.query.is_sliced, \\\n700             \"Cannot use 'limit' or 'offset' with delete.\"\n701 \n702         if self._fields is not None:\n703             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n704 \n705         del_query = self._chain()\n706 \n707         # The delete is actually 2 queries - one to find related objects,\n708         # and one to delete. Make sure that the discovery of related\n709         # objects is performed on the same database as the deletion.\n710         del_query._for_write = True\n711 \n712         # Disable non-supported fields.\n713         del_query.query.select_for_update = False\n714         del_query.query.select_related = False\n715         del_query.query.clear_ordering(force_empty=True)\n716 \n717         collector = Collector(using=del_query.db)\n718         collector.collect(del_query)\n719         deleted, _rows_count = collector.delete()\n720 \n721         # Clear the result cache, in case this QuerySet gets reused.\n722         self._result_cache = None\n723         return deleted, _rows_count\n724 \n725     delete.alters_data = True\n726     delete.queryset_only = True\n727 \n728     def _raw_delete(self, using):\n729         \"\"\"\n730         Delete objects found from the given queryset in single direct SQL\n731         query. No signals are sent and there is no protection for cascades.\n732         \"\"\"\n733         return sql.DeleteQuery(self.model).delete_qs(self, using)\n734     _raw_delete.alters_data = True\n735 \n736     def update(self, **kwargs):\n737         \"\"\"\n738         Update all elements in the current QuerySet, setting all the given\n739         fields to the appropriate values.\n740         \"\"\"\n741         assert not self.query.is_sliced, \\\n742             \"Cannot update a query once a slice has been taken.\"\n743         self._for_write = True\n744         query = self.query.chain(sql.UpdateQuery)\n745         query.add_update_values(kwargs)\n746         # Clear any annotations so that they won't be present in subqueries.\n747         query.annotations = {}\n748         with transaction.mark_for_rollback_on_error(using=self.db):\n749             rows = query.get_compiler(self.db).execute_sql(CURSOR)\n750         self._result_cache = None\n751         return rows\n752     update.alters_data = True\n753 \n754     def _update(self, values):\n755         \"\"\"\n756         A version of update() that accepts field objects instead of field names.\n757         Used primarily for model saving and not intended for use by general\n758         code (it requires too much poking around at model internals to be\n759         useful at that level).\n760         \"\"\"\n761         assert not self.query.is_sliced, \\\n762             \"Cannot update a query once a slice has been taken.\"\n763         query = self.query.chain(sql.UpdateQuery)\n764         query.add_update_fields(values)\n765         # Clear any annotations so that they won't be present in subqueries.\n766         query.annotations = {}\n767         self._result_cache = None\n768         return query.get_compiler(self.db).execute_sql(CURSOR)\n769     _update.alters_data = True\n770     _update.queryset_only = False\n771 \n772     def exists(self):\n773         if self._result_cache is None:\n774             return self.query.has_results(using=self.db)\n775         return bool(self._result_cache)\n776 \n777     def _prefetch_related_objects(self):\n778         # This method can only be called once the result cache has been filled.\n779         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n780         self._prefetch_done = True\n781 \n782     def explain(self, *, format=None, **options):\n783         return self.query.explain(using=self.db, format=format, **options)\n784 \n785     ##################################################\n786     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n787     ##################################################\n788 \n789     def raw(self, raw_query, params=None, translations=None, using=None):\n790         if using is None:\n791             using = self.db\n792         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)\n793         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n794         return qs\n795 \n796     def _values(self, *fields, **expressions):\n797         clone = self._chain()\n798         if expressions:\n799             clone = clone.annotate(**expressions)\n800         clone._fields = fields\n801         clone.query.set_values(fields)\n802         return clone\n803 \n804     def values(self, *fields, **expressions):\n805         fields += tuple(expressions)\n806         clone = self._values(*fields, **expressions)\n807         clone._iterable_class = ValuesIterable\n808         return clone\n809 \n810     def values_list(self, *fields, flat=False, named=False):\n811         if flat and named:\n812             raise TypeError(\"'flat' and 'named' can't be used together.\")\n813         if flat and len(fields) > 1:\n814             raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n815 \n816         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}\n817         _fields = []\n818         expressions = {}\n819         counter = 1\n820         for field in fields:\n821             if hasattr(field, 'resolve_expression'):\n822                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())\n823                 while True:\n824                     field_id = field_id_prefix + str(counter)\n825                     counter += 1\n826                     if field_id not in field_names:\n827                         break\n828                 expressions[field_id] = field\n829                 _fields.append(field_id)\n830             else:\n831                 _fields.append(field)\n832 \n833         clone = self._values(*_fields, **expressions)\n834         clone._iterable_class = (\n835             NamedValuesListIterable if named\n836             else FlatValuesListIterable if flat\n837             else ValuesListIterable\n838         )\n839         return clone\n840 \n841     def dates(self, field_name, kind, order='ASC'):\n842         \"\"\"\n843         Return a list of date objects representing all available dates for\n844         the given field_name, scoped to 'kind'.\n845         \"\"\"\n846         assert kind in ('year', 'month', 'week', 'day'), \\\n847             \"'kind' must be one of 'year', 'month', 'week', or 'day'.\"\n848         assert order in ('ASC', 'DESC'), \\\n849             \"'order' must be either 'ASC' or 'DESC'.\"\n850         return self.annotate(\n851             datefield=Trunc(field_name, kind, output_field=DateField()),\n852             plain_field=F(field_name)\n853         ).values_list(\n854             'datefield', flat=True\n855         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')\n856 \n857     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):\n858         \"\"\"\n859         Return a list of datetime objects representing all available\n860         datetimes for the given field_name, scoped to 'kind'.\n861         \"\"\"\n862         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \\\n863             \"'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'.\"\n864         assert order in ('ASC', 'DESC'), \\\n865             \"'order' must be either 'ASC' or 'DESC'.\"\n866         if settings.USE_TZ:\n867             if tzinfo is None:\n868                 tzinfo = timezone.get_current_timezone()\n869         else:\n870             tzinfo = None\n871         return self.annotate(\n872             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),\n873             plain_field=F(field_name)\n874         ).values_list(\n875             'datetimefield', flat=True\n876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')\n877 \n878     def none(self):\n879         \"\"\"Return an empty QuerySet.\"\"\"\n880         clone = self._chain()\n881         clone.query.set_empty()\n882         return clone\n883 \n884     ##################################################################\n885     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n886     ##################################################################\n887 \n888     def all(self):\n889         \"\"\"\n890         Return a new QuerySet that is a copy of the current one. This allows a\n891         QuerySet to proxy for a model manager in some cases.\n892         \"\"\"\n893         return self._chain()\n894 \n895     def filter(self, *args, **kwargs):\n896         \"\"\"\n897         Return a new QuerySet instance with the args ANDed to the existing\n898         set.\n899         \"\"\"\n900         self._not_support_combined_queries('filter')\n901         return self._filter_or_exclude(False, *args, **kwargs)\n902 \n903     def exclude(self, *args, **kwargs):\n904         \"\"\"\n905         Return a new QuerySet instance with NOT (args) ANDed to the existing\n906         set.\n907         \"\"\"\n908         self._not_support_combined_queries('exclude')\n909         return self._filter_or_exclude(True, *args, **kwargs)\n910 \n911     def _filter_or_exclude(self, negate, *args, **kwargs):\n912         if args or kwargs:\n913             assert not self.query.is_sliced, \\\n914                 \"Cannot filter a query once a slice has been taken.\"\n915 \n916         clone = self._chain()\n917         if negate:\n918             clone.query.add_q(~Q(*args, **kwargs))\n919         else:\n920             clone.query.add_q(Q(*args, **kwargs))\n921         return clone\n922 \n923     def complex_filter(self, filter_obj):\n924         \"\"\"\n925         Return a new QuerySet instance with filter_obj added to the filters.\n926 \n927         filter_obj can be a Q object or a dictionary of keyword lookup\n928         arguments.\n929 \n930         This exists to support framework features such as 'limit_choices_to',\n931         and usually it will be more natural to use other methods.\n932         \"\"\"\n933         if isinstance(filter_obj, Q):\n934             clone = self._chain()\n935             clone.query.add_q(filter_obj)\n936             return clone\n937         else:\n938             return self._filter_or_exclude(None, **filter_obj)\n939 \n940     def _combinator_query(self, combinator, *other_qs, all=False):\n941         # Clone the query to inherit the select list and everything\n942         clone = self._chain()\n943         # Clear limits and ordering so they can be reapplied\n944         clone.query.clear_ordering(True)\n945         clone.query.clear_limits()\n946         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)\n947         clone.query.combinator = combinator\n948         clone.query.combinator_all = all\n949         return clone\n950 \n951     def union(self, *other_qs, all=False):\n952         # If the query is an EmptyQuerySet, combine all nonempty querysets.\n953         if isinstance(self, EmptyQuerySet):\n954             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n955             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self\n956         return self._combinator_query('union', *other_qs, all=all)\n957 \n958     def intersection(self, *other_qs):\n959         # If any query is an EmptyQuerySet, return it.\n960         if isinstance(self, EmptyQuerySet):\n961             return self\n962         for other in other_qs:\n963             if isinstance(other, EmptyQuerySet):\n964                 return other\n965         return self._combinator_query('intersection', *other_qs)\n966 \n967     def difference(self, *other_qs):\n968         # If the query is an EmptyQuerySet, return it.\n969         if isinstance(self, EmptyQuerySet):\n970             return self\n971         return self._combinator_query('difference', *other_qs)\n972 \n973     def select_for_update(self, nowait=False, skip_locked=False, of=()):\n974         \"\"\"\n975         Return a new QuerySet instance that will select objects with a\n976         FOR UPDATE lock.\n977         \"\"\"\n978         if nowait and skip_locked:\n979             raise ValueError('The nowait option cannot be used with skip_locked.')\n980         obj = self._chain()\n981         obj._for_write = True\n982         obj.query.select_for_update = True\n983         obj.query.select_for_update_nowait = nowait\n984         obj.query.select_for_update_skip_locked = skip_locked\n985         obj.query.select_for_update_of = of\n986         return obj\n987 \n988     def select_related(self, *fields):\n989         \"\"\"\n990         Return a new QuerySet instance that will select related objects.\n991 \n992         If fields are specified, they must be ForeignKey fields and only those\n993         related objects are included in the selection.\n994 \n995         If select_related(None) is called, clear the list.\n996         \"\"\"\n997         self._not_support_combined_queries('select_related')\n998         if self._fields is not None:\n999             raise TypeError(\"Cannot call select_related() after .values() or .values_list()\")\n1000 \n1001         obj = self._chain()\n1002         if fields == (None,):\n1003             obj.query.select_related = False\n1004         elif fields:\n1005             obj.query.add_select_related(fields)\n1006         else:\n1007             obj.query.select_related = True\n1008         return obj\n1009 \n1010     def prefetch_related(self, *lookups):\n1011         \"\"\"\n1012         Return a new QuerySet instance that will prefetch the specified\n1013         Many-To-One and Many-To-Many related objects when the QuerySet is\n1014         evaluated.\n1015 \n1016         When prefetch_related() is called more than once, append to the list of\n1017         prefetch lookups. If prefetch_related(None) is called, clear the list.\n1018         \"\"\"\n1019         self._not_support_combined_queries('prefetch_related')\n1020         clone = self._chain()\n1021         if lookups == (None,):\n1022             clone._prefetch_related_lookups = ()\n1023         else:\n1024             for lookup in lookups:\n1025                 if isinstance(lookup, Prefetch):\n1026                     lookup = lookup.prefetch_to\n1027                 lookup = lookup.split(LOOKUP_SEP, 1)[0]\n1028                 if lookup in self.query._filtered_relations:\n1029                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')\n1030             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n1031         return clone\n1032 \n1033     def annotate(self, *args, **kwargs):\n1034         \"\"\"\n1035         Return a query set in which the returned objects have been annotated\n1036         with extra data or aggregations.\n1037         \"\"\"\n1038         self._not_support_combined_queries('annotate')\n1039         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n1040         annotations = {}\n1041         for arg in args:\n1042             # The default_alias property may raise a TypeError.\n1043             try:\n1044                 if arg.default_alias in kwargs:\n1045                     raise ValueError(\"The named annotation '%s' conflicts with the \"\n1046                                      \"default name for another annotation.\"\n1047                                      % arg.default_alias)\n1048             except TypeError:\n1049                 raise TypeError(\"Complex annotations require an alias\")\n1050             annotations[arg.default_alias] = arg\n1051         annotations.update(kwargs)\n1052 \n1053         clone = self._chain()\n1054         names = self._fields\n1055         if names is None:\n1056             names = set(chain.from_iterable(\n1057                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n1058                 for field in self.model._meta.get_fields()\n1059             ))\n1060 \n1061         for alias, annotation in annotations.items():\n1062             if alias in names:\n1063                 raise ValueError(\"The annotation '%s' conflicts with a field on \"\n1064                                  \"the model.\" % alias)\n1065             if isinstance(annotation, FilteredRelation):\n1066                 clone.query.add_filtered_relation(annotation, alias)\n1067             else:\n1068                 clone.query.add_annotation(annotation, alias, is_summary=False)\n1069 \n1070         for alias, annotation in clone.query.annotations.items():\n1071             if alias in annotations and annotation.contains_aggregate:\n1072                 if clone._fields is None:\n1073                     clone.query.group_by = True\n1074                 else:\n1075                     clone.query.set_group_by()\n1076                 break\n1077 \n1078         return clone\n1079 \n1080     def order_by(self, *field_names):\n1081         \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n1082         assert not self.query.is_sliced, \\\n1083             \"Cannot reorder a query once a slice has been taken.\"\n1084         obj = self._chain()\n1085         obj.query.clear_ordering(force_empty=False)\n1086         obj.query.add_ordering(*field_names)\n1087         return obj\n1088 \n1089     def distinct(self, *field_names):\n1090         \"\"\"\n1091         Return a new QuerySet instance that will select only distinct results.\n1092         \"\"\"\n1093         assert not self.query.is_sliced, \\\n1094             \"Cannot create distinct fields once a slice has been taken.\"\n1095         obj = self._chain()\n1096         obj.query.add_distinct_fields(*field_names)\n1097         return obj\n1098 \n1099     def extra(self, select=None, where=None, params=None, tables=None,\n1100               order_by=None, select_params=None):\n1101         \"\"\"Add extra SQL fragments to the query.\"\"\"\n1102         self._not_support_combined_queries('extra')\n1103         assert not self.query.is_sliced, \\\n1104             \"Cannot change a query once a slice has been taken\"\n1105         clone = self._chain()\n1106         clone.query.add_extra(select, select_params, where, params, tables, order_by)\n1107         return clone\n1108 \n1109     def reverse(self):\n1110         \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n1111         if self.query.is_sliced:\n1112             raise TypeError('Cannot reverse a query once a slice has been taken.')\n1113         clone = self._chain()\n1114         clone.query.standard_ordering = not clone.query.standard_ordering\n1115         return clone\n1116 \n1117     def defer(self, *fields):\n1118         \"\"\"\n1119         Defer the loading of data for certain fields until they are accessed.\n1120         Add the set of deferred fields to any existing set of deferred fields.\n1121         The only exception to this is if None is passed in as the only\n1122         parameter, in which case removal all deferrals.\n1123         \"\"\"\n1124         self._not_support_combined_queries('defer')\n1125         if self._fields is not None:\n1126             raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n1127         clone = self._chain()\n1128         if fields == (None,):\n1129             clone.query.clear_deferred_loading()\n1130         else:\n1131             clone.query.add_deferred_loading(fields)\n1132         return clone\n1133 \n1134     def only(self, *fields):\n1135         \"\"\"\n1136         Essentially, the opposite of defer(). Only the fields passed into this\n1137         method and that are not already specified as deferred are loaded\n1138         immediately when the queryset is evaluated.\n1139         \"\"\"\n1140         self._not_support_combined_queries('only')\n1141         if self._fields is not None:\n1142             raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n1143         if fields == (None,):\n1144             # Can only pass None to defer(), not only(), as the rest option.\n1145             # That won't stop people trying to do this, so let's be explicit.\n1146             raise TypeError(\"Cannot pass None as an argument to only().\")\n1147         for field in fields:\n1148             field = field.split(LOOKUP_SEP, 1)[0]\n1149             if field in self.query._filtered_relations:\n1150                 raise ValueError('only() is not supported with FilteredRelation.')\n1151         clone = self._chain()\n1152         clone.query.add_immediate_loading(fields)\n1153         return clone\n1154 \n1155     def using(self, alias):\n1156         \"\"\"Select which database this QuerySet should execute against.\"\"\"\n1157         clone = self._chain()\n1158         clone._db = alias\n1159         return clone\n1160 \n1161     ###################################\n1162     # PUBLIC INTROSPECTION ATTRIBUTES #\n1163     ###################################\n1164 \n1165     @property\n1166     def ordered(self):\n1167         \"\"\"\n1168         Return True if the QuerySet is ordered -- i.e. has an order_by()\n1169         clause or a default ordering on the model (or is empty).\n1170         \"\"\"\n1171         if isinstance(self, EmptyQuerySet):\n1172             return True\n1173         if self.query.extra_order_by or self.query.order_by:\n1174             return True\n1175         elif self.query.default_ordering and self.query.get_meta().ordering:\n1176             return True\n1177         else:\n1178             return False\n1179 \n1180     @property\n1181     def db(self):\n1182         \"\"\"Return the database used if this query is executed now.\"\"\"\n1183         if self._for_write:\n1184             return self._db or router.db_for_write(self.model, **self._hints)\n1185         return self._db or router.db_for_read(self.model, **self._hints)\n1186 \n1187     ###################\n1188     # PRIVATE METHODS #\n1189     ###################\n1190 \n1191     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):\n1192         \"\"\"\n1193         Insert a new record for the given model. This provides an interface to\n1194         the InsertQuery class and is how Model.save() is implemented.\n1195         \"\"\"\n1196         self._for_write = True\n1197         if using is None:\n1198             using = self.db\n1199         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)\n1200         query.insert_values(fields, objs, raw=raw)\n1201         return query.get_compiler(using=using).execute_sql(returning_fields)\n1202     _insert.alters_data = True\n1203     _insert.queryset_only = False\n1204 \n1205     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):\n1206         \"\"\"\n1207         Helper method for bulk_create() to insert objs one batch at a time.\n1208         \"\"\"\n1209         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n1210             raise NotSupportedError('This database backend does not support ignoring conflicts.')\n1211         ops = connections[self.db].ops\n1212         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))\n1213         inserted_rows = []\n1214         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n1215         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n1216             if bulk_return and not ignore_conflicts:\n1217                 inserted_columns = self._insert(\n1218                     item, fields=fields, using=self.db,\n1219                     returning_fields=self.model._meta.db_returning_fields,\n1220                     ignore_conflicts=ignore_conflicts,\n1221                 )\n1222                 if isinstance(inserted_columns, list):\n1223                     inserted_rows.extend(inserted_columns)\n1224                 else:\n1225                     inserted_rows.append(inserted_columns)\n1226             else:\n1227                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)\n1228         return inserted_rows\n1229 \n1230     def _chain(self, **kwargs):\n1231         \"\"\"\n1232         Return a copy of the current QuerySet that's ready for another\n1233         operation.\n1234         \"\"\"\n1235         obj = self._clone()\n1236         if obj._sticky_filter:\n1237             obj.query.filter_is_sticky = True\n1238             obj._sticky_filter = False\n1239         obj.__dict__.update(kwargs)\n1240         return obj\n1241 \n1242     def _clone(self):\n1243         \"\"\"\n1244         Return a copy of the current QuerySet. A lightweight alternative\n1245         to deepcopy().\n1246         \"\"\"\n1247         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)\n1248         c._sticky_filter = self._sticky_filter\n1249         c._for_write = self._for_write\n1250         c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n1251         c._known_related_objects = self._known_related_objects\n1252         c._iterable_class = self._iterable_class\n1253         c._fields = self._fields\n1254         return c\n1255 \n1256     def _fetch_all(self):\n1257         if self._result_cache is None:\n1258             self._result_cache = list(self._iterable_class(self))\n1259         if self._prefetch_related_lookups and not self._prefetch_done:\n1260             self._prefetch_related_objects()\n1261 \n1262     def _next_is_sticky(self):\n1263         \"\"\"\n1264         Indicate that the next filter call and the one following that should\n1265         be treated as a single filter. This is only important when it comes to\n1266         determining when to reuse tables for many-to-many filters. Required so\n1267         that we can filter naturally on the results of related managers.\n1268 \n1269         This doesn't return a clone of the current QuerySet (it returns\n1270         \"self\"). The method is only used internally and should be immediately\n1271         followed by a filter() that does create a clone.\n1272         \"\"\"\n1273         self._sticky_filter = True\n1274         return self\n1275 \n1276     def _merge_sanity_check(self, other):\n1277         \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n1278         if self._fields is not None and (\n1279                 set(self.query.values_select) != set(other.query.values_select) or\n1280                 set(self.query.extra_select) != set(other.query.extra_select) or\n1281                 set(self.query.annotation_select) != set(other.query.annotation_select)):\n1282             raise TypeError(\n1283                 \"Merging '%s' classes must involve the same values in each case.\"\n1284                 % self.__class__.__name__\n1285             )\n1286 \n1287     def _merge_known_related_objects(self, other):\n1288         \"\"\"\n1289         Keep track of all known related objects from either QuerySet instance.\n1290         \"\"\"\n1291         for field, objects in other._known_related_objects.items():\n1292             self._known_related_objects.setdefault(field, {}).update(objects)\n1293 \n1294     def resolve_expression(self, *args, **kwargs):\n1295         if self._fields and len(self._fields) > 1:\n1296             # values() queryset can only be used as nested queries\n1297             # if they are set up to select only a single field.\n1298             raise TypeError('Cannot use multi-field values as a filter value.')\n1299         query = self.query.resolve_expression(*args, **kwargs)\n1300         query._db = self._db\n1301         return query\n1302     resolve_expression.queryset_only = True\n1303 \n1304     def _add_hints(self, **hints):\n1305         \"\"\"\n1306         Update hinting information for use by routers. Add new key/values or\n1307         overwrite existing key/values.\n1308         \"\"\"\n1309         self._hints.update(hints)\n1310 \n1311     def _has_filters(self):\n1312         \"\"\"\n1313         Check if this QuerySet has any filtering going on. This isn't\n1314         equivalent with checking if all objects are present in results, for\n1315         example, qs[1:]._has_filters() -> False.\n1316         \"\"\"\n1317         return self.query.has_filters()\n1318 \n1319     @staticmethod\n1320     def _validate_values_are_expressions(values, method_name):\n1321         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))\n1322         if invalid_args:\n1323             raise TypeError(\n1324                 'QuerySet.%s() received non-expression(s): %s.' % (\n1325                     method_name,\n1326                     ', '.join(invalid_args),\n1327                 )\n1328             )\n1329 \n1330     def _not_support_combined_queries(self, operation_name):\n1331         if self.query.combinator:\n1332             raise NotSupportedError(\n1333                 'Calling QuerySet.%s() after %s() is not supported.'\n1334                 % (operation_name, self.query.combinator)\n1335             )\n1336 \n1337 \n1338 class InstanceCheckMeta(type):\n1339     def __instancecheck__(self, instance):\n1340         return isinstance(instance, QuerySet) and instance.query.is_empty()\n1341 \n1342 \n1343 class EmptyQuerySet(metaclass=InstanceCheckMeta):\n1344     \"\"\"\n1345     Marker class to checking if a queryset is empty by .none():\n1346         isinstance(qs.none(), EmptyQuerySet) -> True\n1347     \"\"\"\n1348 \n1349     def __init__(self, *args, **kwargs):\n1350         raise TypeError(\"EmptyQuerySet can't be instantiated\")\n1351 \n1352 \n1353 class RawQuerySet:\n1354     \"\"\"\n1355     Provide an iterator which converts the results of raw SQL queries into\n1356     annotated model instances.\n1357     \"\"\"\n1358     def __init__(self, raw_query, model=None, query=None, params=None,\n1359                  translations=None, using=None, hints=None):\n1360         self.raw_query = raw_query\n1361         self.model = model\n1362         self._db = using\n1363         self._hints = hints or {}\n1364         self.query = query or sql.RawQuery(sql=raw_query, using=self.db, params=params)\n1365         self.params = params or ()\n1366         self.translations = translations or {}\n1367         self._result_cache = None\n1368         self._prefetch_related_lookups = ()\n1369         self._prefetch_done = False\n1370 \n1371     def resolve_model_init_order(self):\n1372         \"\"\"Resolve the init field names and value positions.\"\"\"\n1373         converter = connections[self.db].introspection.identifier_converter\n1374         model_init_fields = [f for f in self.model._meta.fields if converter(f.column) in self.columns]\n1375         annotation_fields = [(column, pos) for pos, column in enumerate(self.columns)\n1376                              if column not in self.model_fields]\n1377         model_init_order = [self.columns.index(converter(f.column)) for f in model_init_fields]\n1378         model_init_names = [f.attname for f in model_init_fields]\n1379         return model_init_names, model_init_order, annotation_fields\n1380 \n1381     def prefetch_related(self, *lookups):\n1382         \"\"\"Same as QuerySet.prefetch_related()\"\"\"\n1383         clone = self._clone()\n1384         if lookups == (None,):\n1385             clone._prefetch_related_lookups = ()\n1386         else:\n1387             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n1388         return clone\n1389 \n1390     def _prefetch_related_objects(self):\n1391         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n1392         self._prefetch_done = True\n1393 \n1394     def _clone(self):\n1395         \"\"\"Same as QuerySet._clone()\"\"\"\n1396         c = self.__class__(\n1397             self.raw_query, model=self.model, query=self.query, params=self.params,\n1398             translations=self.translations, using=self._db, hints=self._hints\n1399         )\n1400         c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n1401         return c\n1402 \n1403     def _fetch_all(self):\n1404         if self._result_cache is None:\n1405             self._result_cache = list(self.iterator())\n1406         if self._prefetch_related_lookups and not self._prefetch_done:\n1407             self._prefetch_related_objects()\n1408 \n1409     def __len__(self):\n1410         self._fetch_all()\n1411         return len(self._result_cache)\n1412 \n1413     def __bool__(self):\n1414         self._fetch_all()\n1415         return bool(self._result_cache)\n1416 \n1417     def __iter__(self):\n1418         self._fetch_all()\n1419         return iter(self._result_cache)\n1420 \n1421     def iterator(self):\n1422         # Cache some things for performance reasons outside the loop.\n1423         db = self.db\n1424         compiler = connections[db].ops.compiler('SQLCompiler')(\n1425             self.query, connections[db], db\n1426         )\n1427 \n1428         query = iter(self.query)\n1429 \n1430         try:\n1431             model_init_names, model_init_pos, annotation_fields = self.resolve_model_init_order()\n1432             if self.model._meta.pk.attname not in model_init_names:\n1433                 raise InvalidQuery('Raw query must include the primary key')\n1434             model_cls = self.model\n1435             fields = [self.model_fields.get(c) for c in self.columns]\n1436             converters = compiler.get_converters([\n1437                 f.get_col(f.model._meta.db_table) if f else None for f in fields\n1438             ])\n1439             if converters:\n1440                 query = compiler.apply_converters(query, converters)\n1441             for values in query:\n1442                 # Associate fields to values\n1443                 model_init_values = [values[pos] for pos in model_init_pos]\n1444                 instance = model_cls.from_db(db, model_init_names, model_init_values)\n1445                 if annotation_fields:\n1446                     for column, pos in annotation_fields:\n1447                         setattr(instance, column, values[pos])\n1448                 yield instance\n1449         finally:\n1450             # Done iterating the Query. If it has its own cursor, close it.\n1451             if hasattr(self.query, 'cursor') and self.query.cursor:\n1452                 self.query.cursor.close()\n1453 \n1454     def __repr__(self):\n1455         return \"<%s: %s>\" % (self.__class__.__name__, self.query)\n1456 \n1457     def __getitem__(self, k):\n1458         return list(self)[k]\n1459 \n1460     @property\n1461     def db(self):\n1462         \"\"\"Return the database used if this query is executed now.\"\"\"\n1463         return self._db or router.db_for_read(self.model, **self._hints)\n1464 \n1465     def using(self, alias):\n1466         \"\"\"Select the database this RawQuerySet should execute against.\"\"\"\n1467         return RawQuerySet(\n1468             self.raw_query, model=self.model,\n1469             query=self.query.chain(using=alias),\n1470             params=self.params, translations=self.translations,\n1471             using=alias,\n1472         )\n1473 \n1474     @cached_property\n1475     def columns(self):\n1476         \"\"\"\n1477         A list of model field names in the order they'll appear in the\n1478         query results.\n1479         \"\"\"\n1480         columns = self.query.get_columns()\n1481         # Adjust any column names which don't match field names\n1482         for (query_name, model_name) in self.translations.items():\n1483             # Ignore translations for nonexistent column names\n1484             try:\n1485                 index = columns.index(query_name)\n1486             except ValueError:\n1487                 pass\n1488             else:\n1489                 columns[index] = model_name\n1490         return columns\n1491 \n1492     @cached_property\n1493     def model_fields(self):\n1494         \"\"\"A dict mapping column names to model field names.\"\"\"\n1495         converter = connections[self.db].introspection.identifier_converter\n1496         model_fields = {}\n1497         for field in self.model._meta.fields:\n1498             name, column = field.get_attname_column()\n1499             model_fields[converter(column)] = field\n1500         return model_fields\n1501 \n1502 \n1503 class Prefetch:\n1504     def __init__(self, lookup, queryset=None, to_attr=None):\n1505         # `prefetch_through` is the path we traverse to perform the prefetch.\n1506         self.prefetch_through = lookup\n1507         # `prefetch_to` is the path to the attribute that stores the result.\n1508         self.prefetch_to = lookup\n1509         if queryset is not None and not issubclass(queryset._iterable_class, ModelIterable):\n1510             raise ValueError('Prefetch querysets cannot use values().')\n1511         if to_attr:\n1512             self.prefetch_to = LOOKUP_SEP.join(lookup.split(LOOKUP_SEP)[:-1] + [to_attr])\n1513 \n1514         self.queryset = queryset\n1515         self.to_attr = to_attr\n1516 \n1517     def __getstate__(self):\n1518         obj_dict = self.__dict__.copy()\n1519         if self.queryset is not None:\n1520             # Prevent the QuerySet from being evaluated\n1521             obj_dict['queryset'] = self.queryset._chain(\n1522                 _result_cache=[],\n1523                 _prefetch_done=True,\n1524             )\n1525         return obj_dict\n1526 \n1527     def add_prefix(self, prefix):\n1528         self.prefetch_through = prefix + LOOKUP_SEP + self.prefetch_through\n1529         self.prefetch_to = prefix + LOOKUP_SEP + self.prefetch_to\n1530 \n1531     def get_current_prefetch_to(self, level):\n1532         return LOOKUP_SEP.join(self.prefetch_to.split(LOOKUP_SEP)[:level + 1])\n1533 \n1534     def get_current_to_attr(self, level):\n1535         parts = self.prefetch_to.split(LOOKUP_SEP)\n1536         to_attr = parts[level]\n1537         as_attr = self.to_attr and level == len(parts) - 1\n1538         return to_attr, as_attr\n1539 \n1540     def get_current_queryset(self, level):\n1541         if self.get_current_prefetch_to(level) == self.prefetch_to:\n1542             return self.queryset\n1543         return None\n1544 \n1545     def __eq__(self, other):\n1546         if not isinstance(other, Prefetch):\n1547             return NotImplemented\n1548         return self.prefetch_to == other.prefetch_to\n1549 \n1550     def __hash__(self):\n1551         return hash((self.__class__, self.prefetch_to))\n1552 \n1553 \n1554 def normalize_prefetch_lookups(lookups, prefix=None):\n1555     \"\"\"Normalize lookups into Prefetch objects.\"\"\"\n1556     ret = []\n1557     for lookup in lookups:\n1558         if not isinstance(lookup, Prefetch):\n1559             lookup = Prefetch(lookup)\n1560         if prefix:\n1561             lookup.add_prefix(prefix)\n1562         ret.append(lookup)\n1563     return ret\n1564 \n1565 \n1566 def prefetch_related_objects(model_instances, *related_lookups):\n1567     \"\"\"\n1568     Populate prefetched object caches for a list of model instances based on\n1569     the lookups/Prefetch instances given.\n1570     \"\"\"\n1571     if not model_instances:\n1572         return  # nothing to do\n1573 \n1574     # We need to be able to dynamically add to the list of prefetch_related\n1575     # lookups that we look up (see below).  So we need some book keeping to\n1576     # ensure we don't do duplicate work.\n1577     done_queries = {}    # dictionary of things like 'foo__bar': [results]\n1578 \n1579     auto_lookups = set()  # we add to this as we go through.\n1580     followed_descriptors = set()  # recursion protection\n1581 \n1582     all_lookups = normalize_prefetch_lookups(reversed(related_lookups))\n1583     while all_lookups:\n1584         lookup = all_lookups.pop()\n1585         if lookup.prefetch_to in done_queries:\n1586             if lookup.queryset is not None:\n1587                 raise ValueError(\"'%s' lookup was already seen with a different queryset. \"\n1588                                  \"You may need to adjust the ordering of your lookups.\" % lookup.prefetch_to)\n1589 \n1590             continue\n1591 \n1592         # Top level, the list of objects to decorate is the result cache\n1593         # from the primary QuerySet. It won't be for deeper levels.\n1594         obj_list = model_instances\n1595 \n1596         through_attrs = lookup.prefetch_through.split(LOOKUP_SEP)\n1597         for level, through_attr in enumerate(through_attrs):\n1598             # Prepare main instances\n1599             if not obj_list:\n1600                 break\n1601 \n1602             prefetch_to = lookup.get_current_prefetch_to(level)\n1603             if prefetch_to in done_queries:\n1604                 # Skip any prefetching, and any object preparation\n1605                 obj_list = done_queries[prefetch_to]\n1606                 continue\n1607 \n1608             # Prepare objects:\n1609             good_objects = True\n1610             for obj in obj_list:\n1611                 # Since prefetching can re-use instances, it is possible to have\n1612                 # the same instance multiple times in obj_list, so obj might\n1613                 # already be prepared.\n1614                 if not hasattr(obj, '_prefetched_objects_cache'):\n1615                     try:\n1616                         obj._prefetched_objects_cache = {}\n1617                     except (AttributeError, TypeError):\n1618                         # Must be an immutable object from\n1619                         # values_list(flat=True), for example (TypeError) or\n1620                         # a QuerySet subclass that isn't returning Model\n1621                         # instances (AttributeError), either in Django or a 3rd\n1622                         # party. prefetch_related() doesn't make sense, so quit.\n1623                         good_objects = False\n1624                         break\n1625             if not good_objects:\n1626                 break\n1627 \n1628             # Descend down tree\n1629 \n1630             # We assume that objects retrieved are homogeneous (which is the premise\n1631             # of prefetch_related), so what applies to first object applies to all.\n1632             first_obj = obj_list[0]\n1633             to_attr = lookup.get_current_to_attr(level)[0]\n1634             prefetcher, descriptor, attr_found, is_fetched = get_prefetcher(first_obj, through_attr, to_attr)\n1635 \n1636             if not attr_found:\n1637                 raise AttributeError(\"Cannot find '%s' on %s object, '%s' is an invalid \"\n1638                                      \"parameter to prefetch_related()\" %\n1639                                      (through_attr, first_obj.__class__.__name__, lookup.prefetch_through))\n1640 \n1641             if level == len(through_attrs) - 1 and prefetcher is None:\n1642                 # Last one, this *must* resolve to something that supports\n1643                 # prefetching, otherwise there is no point adding it and the\n1644                 # developer asking for it has made a mistake.\n1645                 raise ValueError(\"'%s' does not resolve to an item that supports \"\n1646                                  \"prefetching - this is an invalid parameter to \"\n1647                                  \"prefetch_related().\" % lookup.prefetch_through)\n1648 \n1649             if prefetcher is not None and not is_fetched:\n1650                 obj_list, additional_lookups = prefetch_one_level(obj_list, prefetcher, lookup, level)\n1651                 # We need to ensure we don't keep adding lookups from the\n1652                 # same relationships to stop infinite recursion. So, if we\n1653                 # are already on an automatically added lookup, don't add\n1654                 # the new lookups from relationships we've seen already.\n1655                 if not (prefetch_to in done_queries and lookup in auto_lookups and descriptor in followed_descriptors):\n1656                     done_queries[prefetch_to] = obj_list\n1657                     new_lookups = normalize_prefetch_lookups(reversed(additional_lookups), prefetch_to)\n1658                     auto_lookups.update(new_lookups)\n1659                     all_lookups.extend(new_lookups)\n1660                 followed_descriptors.add(descriptor)\n1661             else:\n1662                 # Either a singly related object that has already been fetched\n1663                 # (e.g. via select_related), or hopefully some other property\n1664                 # that doesn't support prefetching but needs to be traversed.\n1665 \n1666                 # We replace the current list of parent objects with the list\n1667                 # of related objects, filtering out empty or missing values so\n1668                 # that we can continue with nullable or reverse relations.\n1669                 new_obj_list = []\n1670                 for obj in obj_list:\n1671                     if through_attr in getattr(obj, '_prefetched_objects_cache', ()):\n1672                         # If related objects have been prefetched, use the\n1673                         # cache rather than the object's through_attr.\n1674                         new_obj = list(obj._prefetched_objects_cache.get(through_attr))\n1675                     else:\n1676                         try:\n1677                             new_obj = getattr(obj, through_attr)\n1678                         except exceptions.ObjectDoesNotExist:\n1679                             continue\n1680                     if new_obj is None:\n1681                         continue\n1682                     # We special-case `list` rather than something more generic\n1683                     # like `Iterable` because we don't want to accidentally match\n1684                     # user models that define __iter__.\n1685                     if isinstance(new_obj, list):\n1686                         new_obj_list.extend(new_obj)\n1687                     else:\n1688                         new_obj_list.append(new_obj)\n1689                 obj_list = new_obj_list\n1690 \n1691 \n1692 def get_prefetcher(instance, through_attr, to_attr):\n1693     \"\"\"\n1694     For the attribute 'through_attr' on the given instance, find\n1695     an object that has a get_prefetch_queryset().\n1696     Return a 4 tuple containing:\n1697     (the object with get_prefetch_queryset (or None),\n1698      the descriptor object representing this relationship (or None),\n1699      a boolean that is False if the attribute was not found at all,\n1700      a boolean that is True if the attribute has already been fetched)\n1701     \"\"\"\n1702     prefetcher = None\n1703     is_fetched = False\n1704 \n1705     # For singly related objects, we have to avoid getting the attribute\n1706     # from the object, as this will trigger the query. So we first try\n1707     # on the class, in order to get the descriptor object.\n1708     rel_obj_descriptor = getattr(instance.__class__, through_attr, None)\n1709     if rel_obj_descriptor is None:\n1710         attr_found = hasattr(instance, through_attr)\n1711     else:\n1712         attr_found = True\n1713         if rel_obj_descriptor:\n1714             # singly related object, descriptor object has the\n1715             # get_prefetch_queryset() method.\n1716             if hasattr(rel_obj_descriptor, 'get_prefetch_queryset'):\n1717                 prefetcher = rel_obj_descriptor\n1718                 if rel_obj_descriptor.is_cached(instance):\n1719                     is_fetched = True\n1720             else:\n1721                 # descriptor doesn't support prefetching, so we go ahead and get\n1722                 # the attribute on the instance rather than the class to\n1723                 # support many related managers\n1724                 rel_obj = getattr(instance, through_attr)\n1725                 if hasattr(rel_obj, 'get_prefetch_queryset'):\n1726                     prefetcher = rel_obj\n1727                 if through_attr != to_attr:\n1728                     # Special case cached_property instances because hasattr\n1729                     # triggers attribute computation and assignment.\n1730                     if isinstance(getattr(instance.__class__, to_attr, None), cached_property):\n1731                         is_fetched = to_attr in instance.__dict__\n1732                     else:\n1733                         is_fetched = hasattr(instance, to_attr)\n1734                 else:\n1735                     is_fetched = through_attr in instance._prefetched_objects_cache\n1736     return prefetcher, rel_obj_descriptor, attr_found, is_fetched\n1737 \n1738 \n1739 def prefetch_one_level(instances, prefetcher, lookup, level):\n1740     \"\"\"\n1741     Helper function for prefetch_related_objects().\n1742 \n1743     Run prefetches on all instances using the prefetcher object,\n1744     assigning results to relevant caches in instance.\n1745 \n1746     Return the prefetched objects along with any additional prefetches that\n1747     must be done due to prefetch_related lookups found from default managers.\n1748     \"\"\"\n1749     # prefetcher must have a method get_prefetch_queryset() which takes a list\n1750     # of instances, and returns a tuple:\n1751 \n1752     # (queryset of instances of self.model that are related to passed in instances,\n1753     #  callable that gets value to be matched for returned instances,\n1754     #  callable that gets value to be matched for passed in instances,\n1755     #  boolean that is True for singly related objects,\n1756     #  cache or field name to assign to,\n1757     #  boolean that is True when the previous argument is a cache name vs a field name).\n1758 \n1759     # The 'values to be matched' must be hashable as they will be used\n1760     # in a dictionary.\n1761 \n1762     rel_qs, rel_obj_attr, instance_attr, single, cache_name, is_descriptor = (\n1763         prefetcher.get_prefetch_queryset(instances, lookup.get_current_queryset(level)))\n1764     # We have to handle the possibility that the QuerySet we just got back\n1765     # contains some prefetch_related lookups. We don't want to trigger the\n1766     # prefetch_related functionality by evaluating the query. Rather, we need\n1767     # to merge in the prefetch_related lookups.\n1768     # Copy the lookups in case it is a Prefetch object which could be reused\n1769     # later (happens in nested prefetch_related).\n1770     additional_lookups = [\n1771         copy.copy(additional_lookup) for additional_lookup\n1772         in getattr(rel_qs, '_prefetch_related_lookups', ())\n1773     ]\n1774     if additional_lookups:\n1775         # Don't need to clone because the manager should have given us a fresh\n1776         # instance, so we access an internal instead of using public interface\n1777         # for performance reasons.\n1778         rel_qs._prefetch_related_lookups = ()\n1779 \n1780     all_related_objects = list(rel_qs)\n1781 \n1782     rel_obj_cache = {}\n1783     for rel_obj in all_related_objects:\n1784         rel_attr_val = rel_obj_attr(rel_obj)\n1785         rel_obj_cache.setdefault(rel_attr_val, []).append(rel_obj)\n1786 \n1787     to_attr, as_attr = lookup.get_current_to_attr(level)\n1788     # Make sure `to_attr` does not conflict with a field.\n1789     if as_attr and instances:\n1790         # We assume that objects retrieved are homogeneous (which is the premise\n1791         # of prefetch_related), so what applies to first object applies to all.\n1792         model = instances[0].__class__\n1793         try:\n1794             model._meta.get_field(to_attr)\n1795         except exceptions.FieldDoesNotExist:\n1796             pass\n1797         else:\n1798             msg = 'to_attr={} conflicts with a field on the {} model.'\n1799             raise ValueError(msg.format(to_attr, model.__name__))\n1800 \n1801     # Whether or not we're prefetching the last part of the lookup.\n1802     leaf = len(lookup.prefetch_through.split(LOOKUP_SEP)) - 1 == level\n1803 \n1804     for obj in instances:\n1805         instance_attr_val = instance_attr(obj)\n1806         vals = rel_obj_cache.get(instance_attr_val, [])\n1807 \n1808         if single:\n1809             val = vals[0] if vals else None\n1810             if as_attr:\n1811                 # A to_attr has been given for the prefetch.\n1812                 setattr(obj, to_attr, val)\n1813             elif is_descriptor:\n1814                 # cache_name points to a field name in obj.\n1815                 # This field is a descriptor for a related object.\n1816                 setattr(obj, cache_name, val)\n1817             else:\n1818                 # No to_attr has been given for this prefetch operation and the\n1819                 # cache_name does not point to a descriptor. Store the value of\n1820                 # the field in the object's field cache.\n1821                 obj._state.fields_cache[cache_name] = val\n1822         else:\n1823             if as_attr:\n1824                 setattr(obj, to_attr, vals)\n1825             else:\n1826                 manager = getattr(obj, to_attr)\n1827                 if leaf and lookup.queryset is not None:\n1828                     qs = manager._apply_rel_filters(lookup.queryset)\n1829                 else:\n1830                     qs = manager.get_queryset()\n1831                 qs._result_cache = vals\n1832                 # We don't want the individual qs doing prefetch_related now,\n1833                 # since we have merged this into the current work.\n1834                 qs._prefetch_done = True\n1835                 obj._prefetched_objects_cache[cache_name] = qs\n1836     return all_related_objects, additional_lookups\n1837 \n1838 \n1839 class RelatedPopulator:\n1840     \"\"\"\n1841     RelatedPopulator is used for select_related() object instantiation.\n1842 \n1843     The idea is that each select_related() model will be populated by a\n1844     different RelatedPopulator instance. The RelatedPopulator instances get\n1845     klass_info and select (computed in SQLCompiler) plus the used db as\n1846     input for initialization. That data is used to compute which columns\n1847     to use, how to instantiate the model, and how to populate the links\n1848     between the objects.\n1849 \n1850     The actual creation of the objects is done in populate() method. This\n1851     method gets row and from_obj as input and populates the select_related()\n1852     model instance.\n1853     \"\"\"\n1854     def __init__(self, klass_info, select, db):\n1855         self.db = db\n1856         # Pre-compute needed attributes. The attributes are:\n1857         #  - model_cls: the possibly deferred model class to instantiate\n1858         #  - either:\n1859         #    - cols_start, cols_end: usually the columns in the row are\n1860         #      in the same order model_cls.__init__ expects them, so we\n1861         #      can instantiate by model_cls(*row[cols_start:cols_end])\n1862         #    - reorder_for_init: When select_related descends to a child\n1863         #      class, then we want to reuse the already selected parent\n1864         #      data. However, in this case the parent data isn't necessarily\n1865         #      in the same order that Model.__init__ expects it to be, so\n1866         #      we have to reorder the parent data. The reorder_for_init\n1867         #      attribute contains a function used to reorder the field data\n1868         #      in the order __init__ expects it.\n1869         #  - pk_idx: the index of the primary key field in the reordered\n1870         #    model data. Used to check if a related object exists at all.\n1871         #  - init_list: the field attnames fetched from the database. For\n1872         #    deferred models this isn't the same as all attnames of the\n1873         #    model's fields.\n1874         #  - related_populators: a list of RelatedPopulator instances if\n1875         #    select_related() descends to related models from this model.\n1876         #  - local_setter, remote_setter: Methods to set cached values on\n1877         #    the object being populated and on the remote object. Usually\n1878         #    these are Field.set_cached_value() methods.\n1879         select_fields = klass_info['select_fields']\n1880         from_parent = klass_info['from_parent']\n1881         if not from_parent:\n1882             self.cols_start = select_fields[0]\n1883             self.cols_end = select_fields[-1] + 1\n1884             self.init_list = [\n1885                 f[0].target.attname for f in select[self.cols_start:self.cols_end]\n1886             ]\n1887             self.reorder_for_init = None\n1888         else:\n1889             attname_indexes = {select[idx][0].target.attname: idx for idx in select_fields}\n1890             model_init_attnames = (f.attname for f in klass_info['model']._meta.concrete_fields)\n1891             self.init_list = [attname for attname in model_init_attnames if attname in attname_indexes]\n1892             self.reorder_for_init = operator.itemgetter(*[attname_indexes[attname] for attname in self.init_list])\n1893 \n1894         self.model_cls = klass_info['model']\n1895         self.pk_idx = self.init_list.index(self.model_cls._meta.pk.attname)\n1896         self.related_populators = get_related_populators(klass_info, select, self.db)\n1897         self.local_setter = klass_info['local_setter']\n1898         self.remote_setter = klass_info['remote_setter']\n1899 \n1900     def populate(self, row, from_obj):\n1901         if self.reorder_for_init:\n1902             obj_data = self.reorder_for_init(row)\n1903         else:\n1904             obj_data = row[self.cols_start:self.cols_end]\n1905         if obj_data[self.pk_idx] is None:\n1906             obj = None\n1907         else:\n1908             obj = self.model_cls.from_db(self.db, self.init_list, obj_data)\n1909             for rel_iter in self.related_populators:\n1910                 rel_iter.populate(row, obj)\n1911         self.local_setter(from_obj, obj)\n1912         if obj is not None:\n1913             self.remote_setter(obj, from_obj)\n1914 \n1915 \n1916 def get_related_populators(klass_info, select, db):\n1917     iterators = []\n1918     related_klass_infos = klass_info.get('related_klass_infos', [])\n1919     for rel_klass_info in related_klass_infos:\n1920         rel_cls = RelatedPopulator(rel_klass_info, select, db)\n1921         iterators.append(rel_cls)\n1922     return iterators\n",
        "intended_behavior": "Within the query construction or filter application sections that handle the `__isnull` lookup, implement a validation step to ensure that the value provided for `__isnull` is strictly a boolean. If a non-boolean value is provided, the code should raise a ValueError with a clear message indicating that only boolean values are acceptable for `__isnull` lookups. This behavior will prevent misuse and maintain consistency across different usage scenarios."
    },
    {
        "rel_file_path": "django/db/models/sql/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.1/django/db/models/sql/query.py",
        "start": 1,
        "end": 2362,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 Create SQL statements for QuerySets.\n3 \n4 The code in here encapsulates all of the SQL construction so that QuerySets\n5 themselves do not have to (and could be backed by things other than SQL\n6 databases). The abstraction barrier only works one way: this module has to know\n7 all about the internals of models in order to get the information it needs.\n8 \"\"\"\n9 import copy\n10 import difflib\n11 import functools\n12 import inspect\n13 import sys\n14 import warnings\n15 from collections import Counter, namedtuple\n16 from collections.abc import Iterator, Mapping\n17 from itertools import chain, count, product\n18 from string import ascii_uppercase\n19 \n20 from django.core.exceptions import (\n21     EmptyResultSet, FieldDoesNotExist, FieldError,\n22 )\n23 from django.db import DEFAULT_DB_ALIAS, NotSupportedError, connections\n24 from django.db.models.aggregates import Count\n25 from django.db.models.constants import LOOKUP_SEP\n26 from django.db.models.expressions import (\n27     BaseExpression, Col, F, OuterRef, Ref, SimpleCol,\n28 )\n29 from django.db.models.fields import Field\n30 from django.db.models.fields.related_lookups import MultiColSource\n31 from django.db.models.lookups import Lookup\n32 from django.db.models.query_utils import (\n33     Q, check_rel_lookup_compatibility, refs_expression,\n34 )\n35 from django.db.models.sql.constants import (\n36     INNER, LOUTER, ORDER_DIR, ORDER_PATTERN, SINGLE,\n37 )\n38 from django.db.models.sql.datastructures import (\n39     BaseTable, Empty, Join, MultiJoin,\n40 )\n41 from django.db.models.sql.where import (\n42     AND, OR, ExtraWhere, NothingNode, WhereNode,\n43 )\n44 from django.utils.deprecation import RemovedInDjango40Warning\n45 from django.utils.functional import cached_property\n46 from django.utils.tree import Node\n47 \n48 __all__ = ['Query', 'RawQuery']\n49 \n50 \n51 def get_field_names_from_opts(opts):\n52     return set(chain.from_iterable(\n53         (f.name, f.attname) if f.concrete else (f.name,)\n54         for f in opts.get_fields()\n55     ))\n56 \n57 \n58 def get_children_from_q(q):\n59     for child in q.children:\n60         if isinstance(child, Node):\n61             yield from get_children_from_q(child)\n62         else:\n63             yield child\n64 \n65 \n66 JoinInfo = namedtuple(\n67     'JoinInfo',\n68     ('final_field', 'targets', 'opts', 'joins', 'path', 'transform_function')\n69 )\n70 \n71 \n72 def _get_col(target, field, alias, simple_col):\n73     if simple_col:\n74         return SimpleCol(target, field)\n75     return target.get_col(alias, field)\n76 \n77 \n78 class RawQuery:\n79     \"\"\"A single raw SQL query.\"\"\"\n80 \n81     def __init__(self, sql, using, params=None):\n82         self.params = params or ()\n83         self.sql = sql\n84         self.using = using\n85         self.cursor = None\n86 \n87         # Mirror some properties of a normal query so that\n88         # the compiler can be used to process results.\n89         self.low_mark, self.high_mark = 0, None  # Used for offset/limit\n90         self.extra_select = {}\n91         self.annotation_select = {}\n92 \n93     def chain(self, using):\n94         return self.clone(using)\n95 \n96     def clone(self, using):\n97         return RawQuery(self.sql, using, params=self.params)\n98 \n99     def get_columns(self):\n100         if self.cursor is None:\n101             self._execute_query()\n102         converter = connections[self.using].introspection.identifier_converter\n103         return [converter(column_meta[0])\n104                 for column_meta in self.cursor.description]\n105 \n106     def __iter__(self):\n107         # Always execute a new query for a new iterator.\n108         # This could be optimized with a cache at the expense of RAM.\n109         self._execute_query()\n110         if not connections[self.using].features.can_use_chunked_reads:\n111             # If the database can't use chunked reads we need to make sure we\n112             # evaluate the entire query up front.\n113             result = list(self.cursor)\n114         else:\n115             result = self.cursor\n116         return iter(result)\n117 \n118     def __repr__(self):\n119         return \"<%s: %s>\" % (self.__class__.__name__, self)\n120 \n121     @property\n122     def params_type(self):\n123         return dict if isinstance(self.params, Mapping) else tuple\n124 \n125     def __str__(self):\n126         return self.sql % self.params_type(self.params)\n127 \n128     def _execute_query(self):\n129         connection = connections[self.using]\n130 \n131         # Adapt parameters to the database, as much as possible considering\n132         # that the target type isn't known. See #17755.\n133         params_type = self.params_type\n134         adapter = connection.ops.adapt_unknown_value\n135         if params_type is tuple:\n136             params = tuple(adapter(val) for val in self.params)\n137         elif params_type is dict:\n138             params = {key: adapter(val) for key, val in self.params.items()}\n139         else:\n140             raise RuntimeError(\"Unexpected params type: %s\" % params_type)\n141 \n142         self.cursor = connection.cursor()\n143         self.cursor.execute(self.sql, params)\n144 \n145 \n146 class Query(BaseExpression):\n147     \"\"\"A single SQL query.\"\"\"\n148 \n149     alias_prefix = 'T'\n150     subq_aliases = frozenset([alias_prefix])\n151 \n152     compiler = 'SQLCompiler'\n153 \n154     def __init__(self, model, where=WhereNode):\n155         self.model = model\n156         self.alias_refcount = {}\n157         # alias_map is the most important data structure regarding joins.\n158         # It's used for recording which joins exist in the query and what\n159         # types they are. The key is the alias of the joined table (possibly\n160         # the table name) and the value is a Join-like object (see\n161         # sql.datastructures.Join for more information).\n162         self.alias_map = {}\n163         # Sometimes the query contains references to aliases in outer queries (as\n164         # a result of split_exclude). Correct alias quoting needs to know these\n165         # aliases too.\n166         self.external_aliases = set()\n167         self.table_map = {}     # Maps table names to list of aliases.\n168         self.default_cols = True\n169         self.default_ordering = True\n170         self.standard_ordering = True\n171         self.used_aliases = set()\n172         self.filter_is_sticky = False\n173         self.subquery = False\n174 \n175         # SQL-related attributes\n176         # Select and related select clauses are expressions to use in the\n177         # SELECT clause of the query.\n178         # The select is used for cases where we want to set up the select\n179         # clause to contain other than default fields (values(), subqueries...)\n180         # Note that annotations go to annotations dictionary.\n181         self.select = ()\n182         self.where = where()\n183         self.where_class = where\n184         # The group_by attribute can have one of the following forms:\n185         #  - None: no group by at all in the query\n186         #  - A tuple of expressions: group by (at least) those expressions.\n187         #    String refs are also allowed for now.\n188         #  - True: group by all select fields of the model\n189         # See compiler.get_group_by() for details.\n190         self.group_by = None\n191         self.order_by = ()\n192         self.low_mark, self.high_mark = 0, None  # Used for offset/limit\n193         self.distinct = False\n194         self.distinct_fields = ()\n195         self.select_for_update = False\n196         self.select_for_update_nowait = False\n197         self.select_for_update_skip_locked = False\n198         self.select_for_update_of = ()\n199 \n200         self.select_related = False\n201         # Arbitrary limit for select_related to prevents infinite recursion.\n202         self.max_depth = 5\n203 \n204         # Holds the selects defined by a call to values() or values_list()\n205         # excluding annotation_select and extra_select.\n206         self.values_select = ()\n207 \n208         # SQL annotation-related attributes\n209         self.annotations = {}  # Maps alias -> Annotation Expression\n210         self.annotation_select_mask = None\n211         self._annotation_select_cache = None\n212 \n213         # Set combination attributes\n214         self.combinator = None\n215         self.combinator_all = False\n216         self.combined_queries = ()\n217 \n218         # These are for extensions. The contents are more or less appended\n219         # verbatim to the appropriate clause.\n220         self.extra = {}  # Maps col_alias -> (col_sql, params).\n221         self.extra_select_mask = None\n222         self._extra_select_cache = None\n223 \n224         self.extra_tables = ()\n225         self.extra_order_by = ()\n226 \n227         # A tuple that is a set of model field names and either True, if these\n228         # are the fields to defer, or False if these are the only fields to\n229         # load.\n230         self.deferred_loading = (frozenset(), True)\n231 \n232         self._filtered_relations = {}\n233 \n234         self.explain_query = False\n235         self.explain_format = None\n236         self.explain_options = {}\n237 \n238     @property\n239     def output_field(self):\n240         if len(self.select) == 1:\n241             return self.select[0].field\n242         elif len(self.annotation_select) == 1:\n243             return next(iter(self.annotation_select.values())).output_field\n244 \n245     @property\n246     def has_select_fields(self):\n247         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)\n248 \n249     @cached_property\n250     def base_table(self):\n251         for alias in self.alias_map:\n252             return alias\n253 \n254     def __str__(self):\n255         \"\"\"\n256         Return the query as a string of SQL with the parameter values\n257         substituted in (use sql_with_params() to see the unsubstituted string).\n258 \n259         Parameter values won't necessarily be quoted correctly, since that is\n260         done by the database interface at execution time.\n261         \"\"\"\n262         sql, params = self.sql_with_params()\n263         return sql % params\n264 \n265     def sql_with_params(self):\n266         \"\"\"\n267         Return the query as an SQL string and the parameters that will be\n268         substituted into the query.\n269         \"\"\"\n270         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()\n271 \n272     def __deepcopy__(self, memo):\n273         \"\"\"Limit the amount of work when a Query is deepcopied.\"\"\"\n274         result = self.clone()\n275         memo[id(self)] = result\n276         return result\n277 \n278     def get_compiler(self, using=None, connection=None):\n279         if using is None and connection is None:\n280             raise ValueError(\"Need either using or connection\")\n281         if using:\n282             connection = connections[using]\n283         return connection.ops.compiler(self.compiler)(self, connection, using)\n284 \n285     def get_meta(self):\n286         \"\"\"\n287         Return the Options instance (the model._meta) from which to start\n288         processing. Normally, this is self.model._meta, but it can be changed\n289         by subclasses.\n290         \"\"\"\n291         return self.model._meta\n292 \n293     def clone(self):\n294         \"\"\"\n295         Return a copy of the current Query. A lightweight alternative to\n296         to deepcopy().\n297         \"\"\"\n298         obj = Empty()\n299         obj.__class__ = self.__class__\n300         # Copy references to everything.\n301         obj.__dict__ = self.__dict__.copy()\n302         # Clone attributes that can't use shallow copy.\n303         obj.alias_refcount = self.alias_refcount.copy()\n304         obj.alias_map = self.alias_map.copy()\n305         obj.external_aliases = self.external_aliases.copy()\n306         obj.table_map = self.table_map.copy()\n307         obj.where = self.where.clone()\n308         obj.annotations = self.annotations.copy()\n309         if self.annotation_select_mask is None:\n310             obj.annotation_select_mask = None\n311         else:\n312             obj.annotation_select_mask = self.annotation_select_mask.copy()\n313         # _annotation_select_cache cannot be copied, as doing so breaks the\n314         # (necessary) state in which both annotations and\n315         # _annotation_select_cache point to the same underlying objects.\n316         # It will get re-populated in the cloned queryset the next time it's\n317         # used.\n318         obj._annotation_select_cache = None\n319         obj.extra = self.extra.copy()\n320         if self.extra_select_mask is None:\n321             obj.extra_select_mask = None\n322         else:\n323             obj.extra_select_mask = self.extra_select_mask.copy()\n324         if self._extra_select_cache is None:\n325             obj._extra_select_cache = None\n326         else:\n327             obj._extra_select_cache = self._extra_select_cache.copy()\n328         if self.select_related is not False:\n329             # Use deepcopy because select_related stores fields in nested\n330             # dicts.\n331             obj.select_related = copy.deepcopy(obj.select_related)\n332         if 'subq_aliases' in self.__dict__:\n333             obj.subq_aliases = self.subq_aliases.copy()\n334         obj.used_aliases = self.used_aliases.copy()\n335         obj._filtered_relations = self._filtered_relations.copy()\n336         # Clear the cached_property\n337         try:\n338             del obj.base_table\n339         except AttributeError:\n340             pass\n341         return obj\n342 \n343     def chain(self, klass=None):\n344         \"\"\"\n345         Return a copy of the current Query that's ready for another operation.\n346         The klass argument changes the type of the Query, e.g. UpdateQuery.\n347         \"\"\"\n348         obj = self.clone()\n349         if klass and obj.__class__ != klass:\n350             obj.__class__ = klass\n351         if not obj.filter_is_sticky:\n352             obj.used_aliases = set()\n353         obj.filter_is_sticky = False\n354         if hasattr(obj, '_setup_query'):\n355             obj._setup_query()\n356         return obj\n357 \n358     def relabeled_clone(self, change_map):\n359         clone = self.clone()\n360         clone.change_aliases(change_map)\n361         return clone\n362 \n363     def rewrite_cols(self, annotation, col_cnt):\n364         # We must make sure the inner query has the referred columns in it.\n365         # If we are aggregating over an annotation, then Django uses Ref()\n366         # instances to note this. However, if we are annotating over a column\n367         # of a related model, then it might be that column isn't part of the\n368         # SELECT clause of the inner query, and we must manually make sure\n369         # the column is selected. An example case is:\n370         #    .aggregate(Sum('author__awards'))\n371         # Resolving this expression results in a join to author, but there\n372         # is no guarantee the awards column of author is in the select clause\n373         # of the query. Thus we must manually add the column to the inner\n374         # query.\n375         orig_exprs = annotation.get_source_expressions()\n376         new_exprs = []\n377         for expr in orig_exprs:\n378             # FIXME: These conditions are fairly arbitrary. Identify a better\n379             # method of having expressions decide which code path they should\n380             # take.\n381             if isinstance(expr, Ref):\n382                 # Its already a Ref to subquery (see resolve_ref() for\n383                 # details)\n384                 new_exprs.append(expr)\n385             elif isinstance(expr, (WhereNode, Lookup)):\n386                 # Decompose the subexpressions further. The code here is\n387                 # copied from the else clause, but this condition must appear\n388                 # before the contains_aggregate/is_summary condition below.\n389                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)\n390                 new_exprs.append(new_expr)\n391             else:\n392                 # Reuse aliases of expressions already selected in subquery.\n393                 for col_alias, selected_annotation in self.annotation_select.items():\n394                     if selected_annotation == expr:\n395                         new_expr = Ref(col_alias, expr)\n396                         break\n397                 else:\n398                     # An expression that is not selected the subquery.\n399                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):\n400                         # Reference column or another aggregate. Select it\n401                         # under a non-conflicting alias.\n402                         col_cnt += 1\n403                         col_alias = '__col%d' % col_cnt\n404                         self.annotations[col_alias] = expr\n405                         self.append_annotation_mask([col_alias])\n406                         new_expr = Ref(col_alias, expr)\n407                     else:\n408                         # Some other expression not referencing database values\n409                         # directly. Its subexpression might contain Cols.\n410                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)\n411                 new_exprs.append(new_expr)\n412         annotation.set_source_expressions(new_exprs)\n413         return annotation, col_cnt\n414 \n415     def get_aggregation(self, using, added_aggregate_names):\n416         \"\"\"\n417         Return the dictionary with the values of the existing aggregations.\n418         \"\"\"\n419         if not self.annotation_select:\n420             return {}\n421         existing_annotations = [\n422             annotation for alias, annotation\n423             in self.annotations.items()\n424             if alias not in added_aggregate_names\n425         ]\n426         # Decide if we need to use a subquery.\n427         #\n428         # Existing annotations would cause incorrect results as get_aggregation()\n429         # must produce just one result and thus must not use GROUP BY. But we\n430         # aren't smart enough to remove the existing annotations from the\n431         # query, so those would force us to use GROUP BY.\n432         #\n433         # If the query has limit or distinct, or uses set operations, then\n434         # those operations must be done in a subquery so that the query\n435         # aggregates on the limit and/or distinct results instead of applying\n436         # the distinct and limit after the aggregation.\n437         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or\n438                 self.distinct or self.combinator):\n439             from django.db.models.sql.subqueries import AggregateQuery\n440             outer_query = AggregateQuery(self.model)\n441             inner_query = self.clone()\n442             inner_query.select_for_update = False\n443             inner_query.select_related = False\n444             inner_query.set_annotation_mask(self.annotation_select)\n445             if not self.is_sliced and not self.distinct_fields:\n446                 # Queries with distinct_fields need ordering and when a limit\n447                 # is applied we must take the slice from the ordered query.\n448                 # Otherwise no need for ordering.\n449                 inner_query.clear_ordering(True)\n450             if not inner_query.distinct:\n451                 # If the inner query uses default select and it has some\n452                 # aggregate annotations, then we must make sure the inner\n453                 # query is grouped by the main model's primary key. However,\n454                 # clearing the select clause can alter results if distinct is\n455                 # used.\n456                 has_existing_aggregate_annotations = any(\n457                     annotation for annotation in existing_annotations\n458                     if getattr(annotation, 'contains_aggregate', True)\n459                 )\n460                 if inner_query.default_cols and has_existing_aggregate_annotations:\n461                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n462                 inner_query.default_cols = False\n463 \n464             relabels = {t: 'subquery' for t in inner_query.alias_map}\n465             relabels[None] = 'subquery'\n466             # Remove any aggregates marked for reduction from the subquery\n467             # and move them to the outer AggregateQuery.\n468             col_cnt = 0\n469             for alias, expression in list(inner_query.annotation_select.items()):\n470                 annotation_select_mask = inner_query.annotation_select_mask\n471                 if expression.is_summary:\n472                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)\n473                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)\n474                     del inner_query.annotations[alias]\n475                     annotation_select_mask.remove(alias)\n476                 # Make sure the annotation_select wont use cached results.\n477                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)\n478             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:\n479                 # In case of Model.objects[0:3].count(), there would be no\n480                 # field selected in the inner query, yet we must use a subquery.\n481                 # So, make sure at least one field is selected.\n482                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)\n483             try:\n484                 outer_query.add_subquery(inner_query, using)\n485             except EmptyResultSet:\n486                 return {\n487                     alias: None\n488                     for alias in outer_query.annotation_select\n489                 }\n490         else:\n491             outer_query = self\n492             self.select = ()\n493             self.default_cols = False\n494             self.extra = {}\n495 \n496         outer_query.clear_ordering(True)\n497         outer_query.clear_limits()\n498         outer_query.select_for_update = False\n499         outer_query.select_related = False\n500         compiler = outer_query.get_compiler(using)\n501         result = compiler.execute_sql(SINGLE)\n502         if result is None:\n503             result = [None] * len(outer_query.annotation_select)\n504 \n505         converters = compiler.get_converters(outer_query.annotation_select.values())\n506         result = next(compiler.apply_converters((result,), converters))\n507 \n508         return dict(zip(outer_query.annotation_select, result))\n509 \n510     def get_count(self, using):\n511         \"\"\"\n512         Perform a COUNT() query using the current filter constraints.\n513         \"\"\"\n514         obj = self.clone()\n515         obj.add_annotation(Count('*'), alias='__count', is_summary=True)\n516         number = obj.get_aggregation(using, ['__count'])['__count']\n517         if number is None:\n518             number = 0\n519         return number\n520 \n521     def has_filters(self):\n522         return self.where\n523 \n524     def has_results(self, using):\n525         q = self.clone()\n526         if not q.distinct:\n527             if q.group_by is True:\n528                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)\n529                 q.set_group_by()\n530             q.clear_select_clause()\n531         q.clear_ordering(True)\n532         q.set_limits(high=1)\n533         compiler = q.get_compiler(using=using)\n534         return compiler.has_results()\n535 \n536     def explain(self, using, format=None, **options):\n537         q = self.clone()\n538         q.explain_query = True\n539         q.explain_format = format\n540         q.explain_options = options\n541         compiler = q.get_compiler(using=using)\n542         return '\\n'.join(compiler.explain_query())\n543 \n544     def combine(self, rhs, connector):\n545         \"\"\"\n546         Merge the 'rhs' query into the current one (with any 'rhs' effects\n547         being applied *after* (that is, \"to the right of\") anything in the\n548         current query. 'rhs' is not modified during a call to this function.\n549 \n550         The 'connector' parameter describes how to connect filters from the\n551         'rhs' query.\n552         \"\"\"\n553         assert self.model == rhs.model, \\\n554             \"Cannot combine queries on two different base models.\"\n555         assert not self.is_sliced, \\\n556             \"Cannot combine queries once a slice has been taken.\"\n557         assert self.distinct == rhs.distinct, \\\n558             \"Cannot combine a unique query with a non-unique query.\"\n559         assert self.distinct_fields == rhs.distinct_fields, \\\n560             \"Cannot combine queries with different distinct fields.\"\n561 \n562         # Work out how to relabel the rhs aliases, if necessary.\n563         change_map = {}\n564         conjunction = (connector == AND)\n565 \n566         # Determine which existing joins can be reused. When combining the\n567         # query with AND we must recreate all joins for m2m filters. When\n568         # combining with OR we can reuse joins. The reason is that in AND\n569         # case a single row can't fulfill a condition like:\n570         #     revrel__col=1 & revrel__col=2\n571         # But, there might be two different related rows matching this\n572         # condition. In OR case a single True is enough, so single row is\n573         # enough, too.\n574         #\n575         # Note that we will be creating duplicate joins for non-m2m joins in\n576         # the AND case. The results will be correct but this creates too many\n577         # joins. This is something that could be fixed later on.\n578         reuse = set() if conjunction else set(self.alias_map)\n579         # Base table must be present in the query - this is the same\n580         # table on both sides.\n581         self.get_initial_alias()\n582         joinpromoter = JoinPromoter(connector, 2, False)\n583         joinpromoter.add_votes(\n584             j for j in self.alias_map if self.alias_map[j].join_type == INNER)\n585         rhs_votes = set()\n586         # Now, add the joins from rhs query into the new query (skipping base\n587         # table).\n588         rhs_tables = list(rhs.alias_map)[1:]\n589         for alias in rhs_tables:\n590             join = rhs.alias_map[alias]\n591             # If the left side of the join was already relabeled, use the\n592             # updated alias.\n593             join = join.relabeled_clone(change_map)\n594             new_alias = self.join(join, reuse=reuse)\n595             if join.join_type == INNER:\n596                 rhs_votes.add(new_alias)\n597             # We can't reuse the same join again in the query. If we have two\n598             # distinct joins for the same connection in rhs query, then the\n599             # combined query must have two joins, too.\n600             reuse.discard(new_alias)\n601             if alias != new_alias:\n602                 change_map[alias] = new_alias\n603             if not rhs.alias_refcount[alias]:\n604                 # The alias was unused in the rhs query. Unref it so that it\n605                 # will be unused in the new query, too. We have to add and\n606                 # unref the alias so that join promotion has information of\n607                 # the join type for the unused alias.\n608                 self.unref_alias(new_alias)\n609         joinpromoter.add_votes(rhs_votes)\n610         joinpromoter.update_join_types(self)\n611 \n612         # Now relabel a copy of the rhs where-clause and add it to the current\n613         # one.\n614         w = rhs.where.clone()\n615         w.relabel_aliases(change_map)\n616         self.where.add(w, connector)\n617 \n618         # Selection columns and extra extensions are those provided by 'rhs'.\n619         if rhs.select:\n620             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])\n621         else:\n622             self.select = ()\n623 \n624         if connector == OR:\n625             # It would be nice to be able to handle this, but the queries don't\n626             # really make sense (or return consistent value sets). Not worth\n627             # the extra complexity when you can write a real query instead.\n628             if self.extra and rhs.extra:\n629                 raise ValueError(\"When merging querysets using 'or', you cannot have extra(select=...) on both sides.\")\n630         self.extra.update(rhs.extra)\n631         extra_select_mask = set()\n632         if self.extra_select_mask is not None:\n633             extra_select_mask.update(self.extra_select_mask)\n634         if rhs.extra_select_mask is not None:\n635             extra_select_mask.update(rhs.extra_select_mask)\n636         if extra_select_mask:\n637             self.set_extra_mask(extra_select_mask)\n638         self.extra_tables += rhs.extra_tables\n639 \n640         # Ordering uses the 'rhs' ordering, unless it has none, in which case\n641         # the current ordering is used.\n642         self.order_by = rhs.order_by or self.order_by\n643         self.extra_order_by = rhs.extra_order_by or self.extra_order_by\n644 \n645     def deferred_to_data(self, target, callback):\n646         \"\"\"\n647         Convert the self.deferred_loading data structure to an alternate data\n648         structure, describing the field that *will* be loaded. This is used to\n649         compute the columns to select from the database and also by the\n650         QuerySet class to work out which fields are being initialized on each\n651         model. Models that have all their fields included aren't mentioned in\n652         the result, only those that have field restrictions in place.\n653 \n654         The \"target\" parameter is the instance that is populated (in place).\n655         The \"callback\" is a function that is called whenever a (model, field)\n656         pair need to be added to \"target\". It accepts three parameters:\n657         \"target\", and the model and list of fields being added for that model.\n658         \"\"\"\n659         field_names, defer = self.deferred_loading\n660         if not field_names:\n661             return\n662         orig_opts = self.get_meta()\n663         seen = {}\n664         must_include = {orig_opts.concrete_model: {orig_opts.pk}}\n665         for field_name in field_names:\n666             parts = field_name.split(LOOKUP_SEP)\n667             cur_model = self.model._meta.concrete_model\n668             opts = orig_opts\n669             for name in parts[:-1]:\n670                 old_model = cur_model\n671                 if name in self._filtered_relations:\n672                     name = self._filtered_relations[name].relation_name\n673                 source = opts.get_field(name)\n674                 if is_reverse_o2o(source):\n675                     cur_model = source.related_model\n676                 else:\n677                     cur_model = source.remote_field.model\n678                 opts = cur_model._meta\n679                 # Even if we're \"just passing through\" this model, we must add\n680                 # both the current model's pk and the related reference field\n681                 # (if it's not a reverse relation) to the things we select.\n682                 if not is_reverse_o2o(source):\n683                     must_include[old_model].add(source)\n684                 add_to_dict(must_include, cur_model, opts.pk)\n685             field = opts.get_field(parts[-1])\n686             is_reverse_object = field.auto_created and not field.concrete\n687             model = field.related_model if is_reverse_object else field.model\n688             model = model._meta.concrete_model\n689             if model == opts.model:\n690                 model = cur_model\n691             if not is_reverse_o2o(field):\n692                 add_to_dict(seen, model, field)\n693 \n694         if defer:\n695             # We need to load all fields for each model, except those that\n696             # appear in \"seen\" (for all models that appear in \"seen\"). The only\n697             # slight complexity here is handling fields that exist on parent\n698             # models.\n699             workset = {}\n700             for model, values in seen.items():\n701                 for field in model._meta.local_fields:\n702                     if field not in values:\n703                         m = field.model._meta.concrete_model\n704                         add_to_dict(workset, m, field)\n705             for model, values in must_include.items():\n706                 # If we haven't included a model in workset, we don't add the\n707                 # corresponding must_include fields for that model, since an\n708                 # empty set means \"include all fields\". That's why there's no\n709                 # \"else\" branch here.\n710                 if model in workset:\n711                     workset[model].update(values)\n712             for model, values in workset.items():\n713                 callback(target, model, values)\n714         else:\n715             for model, values in must_include.items():\n716                 if model in seen:\n717                     seen[model].update(values)\n718                 else:\n719                     # As we've passed through this model, but not explicitly\n720                     # included any fields, we have to make sure it's mentioned\n721                     # so that only the \"must include\" fields are pulled in.\n722                     seen[model] = values\n723             # Now ensure that every model in the inheritance chain is mentioned\n724             # in the parent list. Again, it must be mentioned to ensure that\n725             # only \"must include\" fields are pulled in.\n726             for model in orig_opts.get_parent_list():\n727                 seen.setdefault(model, set())\n728             for model, values in seen.items():\n729                 callback(target, model, values)\n730 \n731     def table_alias(self, table_name, create=False, filtered_relation=None):\n732         \"\"\"\n733         Return a table alias for the given table_name and whether this is a\n734         new alias or not.\n735 \n736         If 'create' is true, a new alias is always created. Otherwise, the\n737         most recently created alias for the table (if one exists) is reused.\n738         \"\"\"\n739         alias_list = self.table_map.get(table_name)\n740         if not create and alias_list:\n741             alias = alias_list[0]\n742             self.alias_refcount[alias] += 1\n743             return alias, False\n744 \n745         # Create a new alias for this table.\n746         if alias_list:\n747             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)\n748             alias_list.append(alias)\n749         else:\n750             # The first occurrence of a table uses the table name directly.\n751             alias = filtered_relation.alias if filtered_relation is not None else table_name\n752             self.table_map[table_name] = [alias]\n753         self.alias_refcount[alias] = 1\n754         return alias, True\n755 \n756     def ref_alias(self, alias):\n757         \"\"\"Increases the reference count for this alias.\"\"\"\n758         self.alias_refcount[alias] += 1\n759 \n760     def unref_alias(self, alias, amount=1):\n761         \"\"\"Decreases the reference count for this alias.\"\"\"\n762         self.alias_refcount[alias] -= amount\n763 \n764     def promote_joins(self, aliases):\n765         \"\"\"\n766         Promote recursively the join type of given aliases and its children to\n767         an outer join. If 'unconditional' is False, only promote the join if\n768         it is nullable or the parent join is an outer join.\n769 \n770         The children promotion is done to avoid join chains that contain a LOUTER\n771         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,\n772         then we must also promote b->c automatically, or otherwise the promotion\n773         of a->b doesn't actually change anything in the query results.\n774         \"\"\"\n775         aliases = list(aliases)\n776         while aliases:\n777             alias = aliases.pop(0)\n778             if self.alias_map[alias].join_type is None:\n779                 # This is the base table (first FROM entry) - this table\n780                 # isn't really joined at all in the query, so we should not\n781                 # alter its join type.\n782                 continue\n783             # Only the first alias (skipped above) should have None join_type\n784             assert self.alias_map[alias].join_type is not None\n785             parent_alias = self.alias_map[alias].parent_alias\n786             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER\n787             already_louter = self.alias_map[alias].join_type == LOUTER\n788             if ((self.alias_map[alias].nullable or parent_louter) and\n789                     not already_louter):\n790                 self.alias_map[alias] = self.alias_map[alias].promote()\n791                 # Join type of 'alias' changed, so re-examine all aliases that\n792                 # refer to this one.\n793                 aliases.extend(\n794                     join for join in self.alias_map\n795                     if self.alias_map[join].parent_alias == alias and join not in aliases\n796                 )\n797 \n798     def demote_joins(self, aliases):\n799         \"\"\"\n800         Change join type from LOUTER to INNER for all joins in aliases.\n801 \n802         Similarly to promote_joins(), this method must ensure no join chains\n803         containing first an outer, then an inner join are generated. If we\n804         are demoting b->c join in chain a LOUTER b LOUTER c then we must\n805         demote a->b automatically, or otherwise the demotion of b->c doesn't\n806         actually change anything in the query results. .\n807         \"\"\"\n808         aliases = list(aliases)\n809         while aliases:\n810             alias = aliases.pop(0)\n811             if self.alias_map[alias].join_type == LOUTER:\n812                 self.alias_map[alias] = self.alias_map[alias].demote()\n813                 parent_alias = self.alias_map[alias].parent_alias\n814                 if self.alias_map[parent_alias].join_type == INNER:\n815                     aliases.append(parent_alias)\n816 \n817     def reset_refcounts(self, to_counts):\n818         \"\"\"\n819         Reset reference counts for aliases so that they match the value passed\n820         in `to_counts`.\n821         \"\"\"\n822         for alias, cur_refcount in self.alias_refcount.copy().items():\n823             unref_amount = cur_refcount - to_counts.get(alias, 0)\n824             self.unref_alias(alias, unref_amount)\n825 \n826     def change_aliases(self, change_map):\n827         \"\"\"\n828         Change the aliases in change_map (which maps old-alias -> new-alias),\n829         relabelling any references to them in select columns and the where\n830         clause.\n831         \"\"\"\n832         assert set(change_map).isdisjoint(change_map.values())\n833 \n834         # 1. Update references in \"select\" (normal columns plus aliases),\n835         # \"group by\" and \"where\".\n836         self.where.relabel_aliases(change_map)\n837         if isinstance(self.group_by, tuple):\n838             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])\n839         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])\n840         self.annotations = self.annotations and {\n841             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()\n842         }\n843 \n844         # 2. Rename the alias in the internal table/alias datastructures.\n845         for old_alias, new_alias in change_map.items():\n846             if old_alias not in self.alias_map:\n847                 continue\n848             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)\n849             self.alias_map[new_alias] = alias_data\n850             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]\n851             del self.alias_refcount[old_alias]\n852             del self.alias_map[old_alias]\n853 \n854             table_aliases = self.table_map[alias_data.table_name]\n855             for pos, alias in enumerate(table_aliases):\n856                 if alias == old_alias:\n857                     table_aliases[pos] = new_alias\n858                     break\n859         self.external_aliases = {change_map.get(alias, alias)\n860                                  for alias in self.external_aliases}\n861 \n862     def bump_prefix(self, outer_query):\n863         \"\"\"\n864         Change the alias prefix to the next letter in the alphabet in a way\n865         that the outer query's aliases and this query's aliases will not\n866         conflict. Even tables that previously had no alias will get an alias\n867         after this call.\n868         \"\"\"\n869         def prefix_gen():\n870             \"\"\"\n871             Generate a sequence of characters in alphabetical order:\n872                 -> 'A', 'B', 'C', ...\n873 \n874             When the alphabet is finished, the sequence will continue with the\n875             Cartesian product:\n876                 -> 'AA', 'AB', 'AC', ...\n877             \"\"\"\n878             alphabet = ascii_uppercase\n879             prefix = chr(ord(self.alias_prefix) + 1)\n880             yield prefix\n881             for n in count(1):\n882                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet\n883                 for s in product(seq, repeat=n):\n884                     yield ''.join(s)\n885                 prefix = None\n886 \n887         if self.alias_prefix != outer_query.alias_prefix:\n888             # No clashes between self and outer query should be possible.\n889             return\n890 \n891         # Explicitly avoid infinite loop. The constant divider is based on how\n892         # much depth recursive subquery references add to the stack. This value\n893         # might need to be adjusted when adding or removing function calls from\n894         # the code path in charge of performing these operations.\n895         local_recursion_limit = sys.getrecursionlimit() // 16\n896         for pos, prefix in enumerate(prefix_gen()):\n897             if prefix not in self.subq_aliases:\n898                 self.alias_prefix = prefix\n899                 break\n900             if pos > local_recursion_limit:\n901                 raise RecursionError(\n902                     'Maximum recursion depth exceeded: too many subqueries.'\n903                 )\n904         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])\n905         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)\n906         self.change_aliases({\n907             alias: '%s%d' % (self.alias_prefix, pos)\n908             for pos, alias in enumerate(self.alias_map)\n909         })\n910 \n911     def get_initial_alias(self):\n912         \"\"\"\n913         Return the first alias for this query, after increasing its reference\n914         count.\n915         \"\"\"\n916         if self.alias_map:\n917             alias = self.base_table\n918             self.ref_alias(alias)\n919         else:\n920             alias = self.join(BaseTable(self.get_meta().db_table, None))\n921         return alias\n922 \n923     def count_active_tables(self):\n924         \"\"\"\n925         Return the number of tables in this query with a non-zero reference\n926         count. After execution, the reference counts are zeroed, so tables\n927         added in compiler will not be seen by this method.\n928         \"\"\"\n929         return len([1 for count in self.alias_refcount.values() if count])\n930 \n931     def join(self, join, reuse=None, reuse_with_filtered_relation=False):\n932         \"\"\"\n933         Return an alias for the 'join', either reusing an existing alias for\n934         that join or creating a new one. 'join' is either a\n935         sql.datastructures.BaseTable or Join.\n936 \n937         The 'reuse' parameter can be either None which means all joins are\n938         reusable, or it can be a set containing the aliases that can be reused.\n939 \n940         The 'reuse_with_filtered_relation' parameter is used when computing\n941         FilteredRelation instances.\n942 \n943         A join is always created as LOUTER if the lhs alias is LOUTER to make\n944         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new\n945         joins are created as LOUTER if the join is nullable.\n946         \"\"\"\n947         if reuse_with_filtered_relation and reuse:\n948             reuse_aliases = [\n949                 a for a, j in self.alias_map.items()\n950                 if a in reuse and j.equals(join, with_filtered_relation=False)\n951             ]\n952         else:\n953             reuse_aliases = [\n954                 a for a, j in self.alias_map.items()\n955                 if (reuse is None or a in reuse) and j == join\n956             ]\n957         if reuse_aliases:\n958             if join.table_alias in reuse_aliases:\n959                 reuse_alias = join.table_alias\n960             else:\n961                 # Reuse the most recent alias of the joined table\n962                 # (a many-to-many relation may be joined multiple times).\n963                 reuse_alias = reuse_aliases[-1]\n964             self.ref_alias(reuse_alias)\n965             return reuse_alias\n966 \n967         # No reuse is possible, so we need a new alias.\n968         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)\n969         if join.join_type:\n970             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:\n971                 join_type = LOUTER\n972             else:\n973                 join_type = INNER\n974             join.join_type = join_type\n975         join.table_alias = alias\n976         self.alias_map[alias] = join\n977         return alias\n978 \n979     def join_parent_model(self, opts, model, alias, seen):\n980         \"\"\"\n981         Make sure the given 'model' is joined in the query. If 'model' isn't\n982         a parent of 'opts' or if it is None this method is a no-op.\n983 \n984         The 'alias' is the root alias for starting the join, 'seen' is a dict\n985         of model -> alias of existing joins. It must also contain a mapping\n986         of None -> some alias. This will be returned in the no-op case.\n987         \"\"\"\n988         if model in seen:\n989             return seen[model]\n990         chain = opts.get_base_chain(model)\n991         if not chain:\n992             return alias\n993         curr_opts = opts\n994         for int_model in chain:\n995             if int_model in seen:\n996                 curr_opts = int_model._meta\n997                 alias = seen[int_model]\n998                 continue\n999             # Proxy model have elements in base chain\n1000             # with no parents, assign the new options\n1001             # object and skip to the next base in that\n1002             # case\n1003             if not curr_opts.parents[int_model]:\n1004                 curr_opts = int_model._meta\n1005                 continue\n1006             link_field = curr_opts.get_ancestor_link(int_model)\n1007             join_info = self.setup_joins([link_field.name], curr_opts, alias)\n1008             curr_opts = int_model._meta\n1009             alias = seen[int_model] = join_info.joins[-1]\n1010         return alias or seen[None]\n1011 \n1012     def add_annotation(self, annotation, alias, is_summary=False):\n1013         \"\"\"Add a single annotation expression to the Query.\"\"\"\n1014         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n1015                                                    summarize=is_summary)\n1016         self.append_annotation_mask([alias])\n1017         self.annotations[alias] = annotation\n1018 \n1019     def resolve_expression(self, query, *args, **kwargs):\n1020         clone = self.clone()\n1021         # Subqueries need to use a different set of aliases than the outer query.\n1022         clone.bump_prefix(query)\n1023         clone.subquery = True\n1024         # It's safe to drop ordering if the queryset isn't using slicing,\n1025         # distinct(*fields) or select_for_update().\n1026         if (self.low_mark == 0 and self.high_mark is None and\n1027                 not self.distinct_fields and\n1028                 not self.select_for_update):\n1029             clone.clear_ordering(True)\n1030         clone.where.resolve_expression(query, *args, **kwargs)\n1031         for key, value in clone.annotations.items():\n1032             resolved = value.resolve_expression(query, *args, **kwargs)\n1033             if hasattr(resolved, 'external_aliases'):\n1034                 resolved.external_aliases.update(clone.alias_map)\n1035             clone.annotations[key] = resolved\n1036         # Outer query's aliases are considered external.\n1037         clone.external_aliases.update(\n1038             alias for alias, table in query.alias_map.items()\n1039             if (\n1040                 isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias\n1041             ) or (\n1042                 isinstance(table, BaseTable) and table.table_name != table.table_alias\n1043             )\n1044         )\n1045         return clone\n1046 \n1047     def as_sql(self, compiler, connection):\n1048         sql, params = self.get_compiler(connection=connection).as_sql()\n1049         if self.subquery:\n1050             sql = '(%s)' % sql\n1051         return sql, params\n1052 \n1053     def resolve_lookup_value(self, value, can_reuse, allow_joins, simple_col):\n1054         if hasattr(value, 'resolve_expression'):\n1055             kwargs = {'reuse': can_reuse, 'allow_joins': allow_joins}\n1056             if isinstance(value, F):\n1057                 kwargs['simple_col'] = simple_col\n1058             value = value.resolve_expression(self, **kwargs)\n1059         elif isinstance(value, (list, tuple)):\n1060             # The items of the iterable may be expressions and therefore need\n1061             # to be resolved independently.\n1062             resolved_values = []\n1063             for sub_value in value:\n1064                 if hasattr(sub_value, 'resolve_expression'):\n1065                     if isinstance(sub_value, F):\n1066                         resolved_values.append(sub_value.resolve_expression(\n1067                             self, reuse=can_reuse, allow_joins=allow_joins,\n1068                             simple_col=simple_col,\n1069                         ))\n1070                     else:\n1071                         resolved_values.append(sub_value.resolve_expression(\n1072                             self, reuse=can_reuse, allow_joins=allow_joins,\n1073                         ))\n1074                 else:\n1075                     resolved_values.append(sub_value)\n1076             value = tuple(resolved_values)\n1077         return value\n1078 \n1079     def solve_lookup_type(self, lookup):\n1080         \"\"\"\n1081         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').\n1082         \"\"\"\n1083         lookup_splitted = lookup.split(LOOKUP_SEP)\n1084         if self.annotations:\n1085             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)\n1086             if expression:\n1087                 return expression_lookups, (), expression\n1088         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())\n1089         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]\n1090         if len(lookup_parts) > 1 and not field_parts:\n1091             raise FieldError(\n1092                 'Invalid lookup \"%s\" for model %s\".' %\n1093                 (lookup, self.get_meta().model.__name__)\n1094             )\n1095         return lookup_parts, field_parts, False\n1096 \n1097     def check_query_object_type(self, value, opts, field):\n1098         \"\"\"\n1099         Check whether the object passed while querying is of the correct type.\n1100         If not, raise a ValueError specifying the wrong object.\n1101         \"\"\"\n1102         if hasattr(value, '_meta'):\n1103             if not check_rel_lookup_compatibility(value._meta.model, opts, field):\n1104                 raise ValueError(\n1105                     'Cannot query \"%s\": Must be \"%s\" instance.' %\n1106                     (value, opts.object_name))\n1107 \n1108     def check_related_objects(self, field, value, opts):\n1109         \"\"\"Check the type of object passed to query relations.\"\"\"\n1110         if field.is_relation:\n1111             # Check that the field and the queryset use the same model in a\n1112             # query like .filter(author=Author.objects.all()). For example, the\n1113             # opts would be Author's (from the author field) and value.model\n1114             # would be Author.objects.all() queryset's .model (Author also).\n1115             # The field is the related field on the lhs side.\n1116             if (isinstance(value, Query) and not value.has_select_fields and\n1117                     not check_rel_lookup_compatibility(value.model, opts, field)):\n1118                 raise ValueError(\n1119                     'Cannot use QuerySet for \"%s\": Use a QuerySet for \"%s\".' %\n1120                     (value.model._meta.object_name, opts.object_name)\n1121                 )\n1122             elif hasattr(value, '_meta'):\n1123                 self.check_query_object_type(value, opts, field)\n1124             elif hasattr(value, '__iter__'):\n1125                 for v in value:\n1126                     self.check_query_object_type(v, opts, field)\n1127 \n1128     def check_filterable(self, expression):\n1129         \"\"\"Raise an error if expression cannot be used in a WHERE clause.\"\"\"\n1130         if not getattr(expression, 'filterable', 'True'):\n1131             raise NotSupportedError(\n1132                 expression.__class__.__name__ + ' is disallowed in the filter '\n1133                 'clause.'\n1134             )\n1135         if hasattr(expression, 'get_source_expressions'):\n1136             for expr in expression.get_source_expressions():\n1137                 self.check_filterable(expr)\n1138 \n1139     def build_lookup(self, lookups, lhs, rhs):\n1140         \"\"\"\n1141         Try to extract transforms and lookup from given lhs.\n1142 \n1143         The lhs value is something that works like SQLExpression.\n1144         The rhs value is what the lookup is going to compare against.\n1145         The lookups is a list of names to extract using get_lookup()\n1146         and get_transform().\n1147         \"\"\"\n1148         # __exact is the default lookup if one isn't given.\n1149         *transforms, lookup_name = lookups or ['exact']\n1150         for name in transforms:\n1151             lhs = self.try_transform(lhs, name)\n1152         # First try get_lookup() so that the lookup takes precedence if the lhs\n1153         # supports both transform and lookup for the name.\n1154         lookup_class = lhs.get_lookup(lookup_name)\n1155         if not lookup_class:\n1156             if lhs.field.is_relation:\n1157                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))\n1158             # A lookup wasn't found. Try to interpret the name as a transform\n1159             # and do an Exact lookup against it.\n1160             lhs = self.try_transform(lhs, lookup_name)\n1161             lookup_name = 'exact'\n1162             lookup_class = lhs.get_lookup(lookup_name)\n1163             if not lookup_class:\n1164                 return\n1165 \n1166         lookup = lookup_class(lhs, rhs)\n1167         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all\n1168         # uses of None as a query value unless the lookup supports it.\n1169         if lookup.rhs is None and not lookup.can_use_none_as_rhs:\n1170             if lookup_name not in ('exact', 'iexact'):\n1171                 raise ValueError(\"Cannot use None as a query value\")\n1172             return lhs.get_lookup('isnull')(lhs, True)\n1173 \n1174         # For Oracle '' is equivalent to null. The check must be done at this\n1175         # stage because join promotion can't be done in the compiler. Using\n1176         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.\n1177         # A similar thing is done in is_nullable(), too.\n1178         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and\n1179                 lookup_name == 'exact' and lookup.rhs == ''):\n1180             return lhs.get_lookup('isnull')(lhs, True)\n1181 \n1182         return lookup\n1183 \n1184     def try_transform(self, lhs, name):\n1185         \"\"\"\n1186         Helper method for build_lookup(). Try to fetch and initialize\n1187         a transform for name parameter from lhs.\n1188         \"\"\"\n1189         transform_class = lhs.get_transform(name)\n1190         if transform_class:\n1191             return transform_class(lhs)\n1192         else:\n1193             output_field = lhs.output_field.__class__\n1194             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())\n1195             if suggested_lookups:\n1196                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)\n1197             else:\n1198                 suggestion = '.'\n1199             raise FieldError(\n1200                 \"Unsupported lookup '%s' for %s or join on the field not \"\n1201                 \"permitted%s\" % (name, output_field.__name__, suggestion)\n1202             )\n1203 \n1204     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,\n1205                      can_reuse=None, allow_joins=True, split_subq=True,\n1206                      reuse_with_filtered_relation=False, simple_col=False):\n1207         \"\"\"\n1208         Build a WhereNode for a single filter clause but don't add it\n1209         to this Query. Query.add_q() will then add this filter to the where\n1210         Node.\n1211 \n1212         The 'branch_negated' tells us if the current branch contains any\n1213         negations. This will be used to determine if subqueries are needed.\n1214 \n1215         The 'current_negated' is used to determine if the current filter is\n1216         negated or not and this will be used to determine if IS NULL filtering\n1217         is needed.\n1218 \n1219         The difference between current_negated and branch_negated is that\n1220         branch_negated is set on first negation, but current_negated is\n1221         flipped for each negation.\n1222 \n1223         Note that add_filter will not do any negating itself, that is done\n1224         upper in the code by add_q().\n1225 \n1226         The 'can_reuse' is a set of reusable joins for multijoins.\n1227 \n1228         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse\n1229         will be reused.\n1230 \n1231         The method will create a filter clause that can be added to the current\n1232         query. However, if the filter isn't added to the query then the caller\n1233         is responsible for unreffing the joins used.\n1234         \"\"\"\n1235         if isinstance(filter_expr, dict):\n1236             raise FieldError(\"Cannot parse keyword query as dict\")\n1237         if hasattr(filter_expr, 'resolve_expression') and getattr(filter_expr, 'conditional', False):\n1238             if connections[DEFAULT_DB_ALIAS].ops.conditional_expression_supported_in_where_clause(filter_expr):\n1239                 condition = filter_expr.resolve_expression(self)\n1240             else:\n1241                 # Expression is not supported in the WHERE clause, add\n1242                 # comparison with True.\n1243                 condition = self.build_lookup(['exact'], filter_expr.resolve_expression(self), True)\n1244             clause = self.where_class()\n1245             clause.add(condition, AND)\n1246             return clause, []\n1247         arg, value = filter_expr\n1248         if not arg:\n1249             raise FieldError(\"Cannot parse keyword query %r\" % arg)\n1250         lookups, parts, reffed_expression = self.solve_lookup_type(arg)\n1251 \n1252         self.check_filterable(reffed_expression)\n1253 \n1254         if not allow_joins and len(parts) > 1:\n1255             raise FieldError(\"Joined field references are not permitted in this query\")\n1256 \n1257         pre_joins = self.alias_refcount.copy()\n1258         value = self.resolve_lookup_value(value, can_reuse, allow_joins, simple_col)\n1259         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}\n1260 \n1261         self.check_filterable(value)\n1262 \n1263         clause = self.where_class()\n1264         if reffed_expression:\n1265             condition = self.build_lookup(lookups, reffed_expression, value)\n1266             clause.add(condition, AND)\n1267             return clause, []\n1268 \n1269         opts = self.get_meta()\n1270         alias = self.get_initial_alias()\n1271         allow_many = not branch_negated or not split_subq\n1272 \n1273         try:\n1274             join_info = self.setup_joins(\n1275                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,\n1276                 reuse_with_filtered_relation=reuse_with_filtered_relation,\n1277             )\n1278 \n1279             # Prevent iterator from being consumed by check_related_objects()\n1280             if isinstance(value, Iterator):\n1281                 value = list(value)\n1282             self.check_related_objects(join_info.final_field, value, join_info.opts)\n1283 \n1284             # split_exclude() needs to know which joins were generated for the\n1285             # lookup parts\n1286             self._lookup_joins = join_info.joins\n1287         except MultiJoin as e:\n1288             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)\n1289 \n1290         # Update used_joins before trimming since they are reused to determine\n1291         # which joins could be later promoted to INNER.\n1292         used_joins.update(join_info.joins)\n1293         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n1294         if can_reuse is not None:\n1295             can_reuse.update(join_list)\n1296 \n1297         if join_info.final_field.is_relation:\n1298             # No support for transforms for relational fields\n1299             num_lookups = len(lookups)\n1300             if num_lookups > 1:\n1301                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))\n1302             if len(targets) == 1:\n1303                 col = _get_col(targets[0], join_info.final_field, alias, simple_col)\n1304             else:\n1305                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)\n1306         else:\n1307             col = _get_col(targets[0], join_info.final_field, alias, simple_col)\n1308 \n1309         condition = self.build_lookup(lookups, col, value)\n1310         lookup_type = condition.lookup_name\n1311         clause.add(condition, AND)\n1312 \n1313         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated\n1314         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:\n1315             require_outer = True\n1316             if (lookup_type != 'isnull' and (\n1317                     self.is_nullable(targets[0]) or\n1318                     self.alias_map[join_list[-1]].join_type == LOUTER)):\n1319                 # The condition added here will be SQL like this:\n1320                 # NOT (col IS NOT NULL), where the first NOT is added in\n1321                 # upper layers of code. The reason for addition is that if col\n1322                 # is null, then col != someval will result in SQL \"unknown\"\n1323                 # which isn't the same as in Python. The Python None handling\n1324                 # is wanted, and it can be gotten by\n1325                 # (col IS NULL OR col != someval)\n1326                 #   <=>\n1327                 # NOT (col IS NOT NULL AND col = someval).\n1328                 lookup_class = targets[0].get_lookup('isnull')\n1329                 col = _get_col(targets[0], join_info.targets[0], alias, simple_col)\n1330                 clause.add(lookup_class(col, False), AND)\n1331         return clause, used_joins if not require_outer else ()\n1332 \n1333     def add_filter(self, filter_clause):\n1334         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))\n1335 \n1336     def add_q(self, q_object):\n1337         \"\"\"\n1338         A preprocessor for the internal _add_q(). Responsible for doing final\n1339         join promotion.\n1340         \"\"\"\n1341         # For join promotion this case is doing an AND for the added q_object\n1342         # and existing conditions. So, any existing inner join forces the join\n1343         # type to remain inner. Existing outer joins can however be demoted.\n1344         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if\n1345         # rel_a doesn't produce any rows, then the whole condition must fail.\n1346         # So, demotion is OK.\n1347         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}\n1348         clause, _ = self._add_q(q_object, self.used_aliases)\n1349         if clause:\n1350             self.where.add(clause, AND)\n1351         self.demote_joins(existing_inner)\n1352 \n1353     def build_where(self, q_object):\n1354         return self._add_q(q_object, used_aliases=set(), allow_joins=False, simple_col=True)[0]\n1355 \n1356     def _add_q(self, q_object, used_aliases, branch_negated=False,\n1357                current_negated=False, allow_joins=True, split_subq=True,\n1358                simple_col=False):\n1359         \"\"\"Add a Q-object to the current filter.\"\"\"\n1360         connector = q_object.connector\n1361         current_negated = current_negated ^ q_object.negated\n1362         branch_negated = branch_negated or q_object.negated\n1363         target_clause = self.where_class(connector=connector,\n1364                                          negated=q_object.negated)\n1365         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)\n1366         for child in q_object.children:\n1367             if isinstance(child, Node):\n1368                 child_clause, needed_inner = self._add_q(\n1369                     child, used_aliases, branch_negated,\n1370                     current_negated, allow_joins, split_subq, simple_col)\n1371                 joinpromoter.add_votes(needed_inner)\n1372             else:\n1373                 child_clause, needed_inner = self.build_filter(\n1374                     child, can_reuse=used_aliases, branch_negated=branch_negated,\n1375                     current_negated=current_negated, allow_joins=allow_joins,\n1376                     split_subq=split_subq, simple_col=simple_col,\n1377                 )\n1378                 joinpromoter.add_votes(needed_inner)\n1379             if child_clause:\n1380                 target_clause.add(child_clause, connector)\n1381         needed_inner = joinpromoter.update_join_types(self)\n1382         return target_clause, needed_inner\n1383 \n1384     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):\n1385         \"\"\"Add a FilteredRelation object to the current filter.\"\"\"\n1386         connector = q_object.connector\n1387         current_negated ^= q_object.negated\n1388         branch_negated = branch_negated or q_object.negated\n1389         target_clause = self.where_class(connector=connector, negated=q_object.negated)\n1390         for child in q_object.children:\n1391             if isinstance(child, Node):\n1392                 child_clause = self.build_filtered_relation_q(\n1393                     child, reuse=reuse, branch_negated=branch_negated,\n1394                     current_negated=current_negated,\n1395                 )\n1396             else:\n1397                 child_clause, _ = self.build_filter(\n1398                     child, can_reuse=reuse, branch_negated=branch_negated,\n1399                     current_negated=current_negated,\n1400                     allow_joins=True, split_subq=False,\n1401                     reuse_with_filtered_relation=True,\n1402                 )\n1403             target_clause.add(child_clause, connector)\n1404         return target_clause\n1405 \n1406     def add_filtered_relation(self, filtered_relation, alias):\n1407         filtered_relation.alias = alias\n1408         lookups = dict(get_children_from_q(filtered_relation.condition))\n1409         for lookup in chain((filtered_relation.relation_name,), lookups):\n1410             lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)\n1411             shift = 2 if not lookup_parts else 1\n1412             if len(field_parts) > (shift + len(lookup_parts)):\n1413                 raise ValueError(\n1414                     \"FilteredRelation's condition doesn't support nested \"\n1415                     \"relations (got %r).\" % lookup\n1416                 )\n1417         self._filtered_relations[filtered_relation.alias] = filtered_relation\n1418 \n1419     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):\n1420         \"\"\"\n1421         Walk the list of names and turns them into PathInfo tuples. A single\n1422         name in 'names' can generate multiple PathInfos (m2m, for example).\n1423 \n1424         'names' is the path of names to travel, 'opts' is the model Options we\n1425         start the name resolving from, 'allow_many' is as for setup_joins().\n1426         If fail_on_missing is set to True, then a name that can't be resolved\n1427         will generate a FieldError.\n1428 \n1429         Return a list of PathInfo tuples. In addition return the final field\n1430         (the last used join field) and target (which is a field guaranteed to\n1431         contain the same value as the final field). Finally, return those names\n1432         that weren't found (which are likely transforms and the final lookup).\n1433         \"\"\"\n1434         path, names_with_path = [], []\n1435         for pos, name in enumerate(names):\n1436             cur_names_with_path = (name, [])\n1437             if name == 'pk':\n1438                 name = opts.pk.name\n1439 \n1440             field = None\n1441             filtered_relation = None\n1442             try:\n1443                 field = opts.get_field(name)\n1444             except FieldDoesNotExist:\n1445                 if name in self.annotation_select:\n1446                     field = self.annotation_select[name].output_field\n1447                 elif name in self._filtered_relations and pos == 0:\n1448                     filtered_relation = self._filtered_relations[name]\n1449                     field = opts.get_field(filtered_relation.relation_name)\n1450             if field is not None:\n1451                 # Fields that contain one-to-many relations with a generic\n1452                 # model (like a GenericForeignKey) cannot generate reverse\n1453                 # relations and therefore cannot be used for reverse querying.\n1454                 if field.is_relation and not field.related_model:\n1455                     raise FieldError(\n1456                         \"Field %r does not generate an automatic reverse \"\n1457                         \"relation and therefore cannot be used for reverse \"\n1458                         \"querying. If it is a GenericForeignKey, consider \"\n1459                         \"adding a GenericRelation.\" % name\n1460                     )\n1461                 try:\n1462                     model = field.model._meta.concrete_model\n1463                 except AttributeError:\n1464                     # QuerySet.annotate() may introduce fields that aren't\n1465                     # attached to a model.\n1466                     model = None\n1467             else:\n1468                 # We didn't find the current field, so move position back\n1469                 # one step.\n1470                 pos -= 1\n1471                 if pos == -1 or fail_on_missing:\n1472                     available = sorted([\n1473                         *get_field_names_from_opts(opts),\n1474                         *self.annotation_select,\n1475                         *self._filtered_relations,\n1476                     ])\n1477                     raise FieldError(\"Cannot resolve keyword '%s' into field. \"\n1478                                      \"Choices are: %s\" % (name, \", \".join(available)))\n1479                 break\n1480             # Check if we need any joins for concrete inheritance cases (the\n1481             # field lives in parent, but we are currently in one of its\n1482             # children)\n1483             if model is not opts.model:\n1484                 path_to_parent = opts.get_path_to_parent(model)\n1485                 if path_to_parent:\n1486                     path.extend(path_to_parent)\n1487                     cur_names_with_path[1].extend(path_to_parent)\n1488                     opts = path_to_parent[-1].to_opts\n1489             if hasattr(field, 'get_path_info'):\n1490                 pathinfos = field.get_path_info(filtered_relation)\n1491                 if not allow_many:\n1492                     for inner_pos, p in enumerate(pathinfos):\n1493                         if p.m2m:\n1494                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])\n1495                             names_with_path.append(cur_names_with_path)\n1496                             raise MultiJoin(pos + 1, names_with_path)\n1497                 last = pathinfos[-1]\n1498                 path.extend(pathinfos)\n1499                 final_field = last.join_field\n1500                 opts = last.to_opts\n1501                 targets = last.target_fields\n1502                 cur_names_with_path[1].extend(pathinfos)\n1503                 names_with_path.append(cur_names_with_path)\n1504             else:\n1505                 # Local non-relational field.\n1506                 final_field = field\n1507                 targets = (field,)\n1508                 if fail_on_missing and pos + 1 != len(names):\n1509                     raise FieldError(\n1510                         \"Cannot resolve keyword %r into field. Join on '%s'\"\n1511                         \" not permitted.\" % (names[pos + 1], name))\n1512                 break\n1513         return path, final_field, targets, names[pos + 1:]\n1514 \n1515     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,\n1516                     reuse_with_filtered_relation=False):\n1517         \"\"\"\n1518         Compute the necessary table joins for the passage through the fields\n1519         given in 'names'. 'opts' is the Options class for the current model\n1520         (which gives the table we are starting from), 'alias' is the alias for\n1521         the table to start the joining from.\n1522 \n1523         The 'can_reuse' defines the reverse foreign key joins we can reuse. It\n1524         can be None in which case all joins are reusable or a set of aliases\n1525         that can be reused. Note that non-reverse foreign keys are always\n1526         reusable when using setup_joins().\n1527 \n1528         The 'reuse_with_filtered_relation' can be used to force 'can_reuse'\n1529         parameter and force the relation on the given connections.\n1530 \n1531         If 'allow_many' is False, then any reverse foreign key seen will\n1532         generate a MultiJoin exception.\n1533 \n1534         Return the final field involved in the joins, the target field (used\n1535         for any 'where' constraint), the final 'opts' value, the joins, the\n1536         field path traveled to generate the joins, and a transform function\n1537         that takes a field and alias and is equivalent to `field.get_col(alias)`\n1538         in the simple case but wraps field transforms if they were included in\n1539         names.\n1540 \n1541         The target field is the field containing the concrete value. Final\n1542         field can be something different, for example foreign key pointing to\n1543         that value. Final field is needed for example in some value\n1544         conversions (convert 'obj' in fk__id=obj to pk val using the foreign\n1545         key field for example).\n1546         \"\"\"\n1547         joins = [alias]\n1548         # The transform can't be applied yet, as joins must be trimmed later.\n1549         # To avoid making every caller of this method look up transforms\n1550         # directly, compute transforms here and create a partial that converts\n1551         # fields to the appropriate wrapped version.\n1552 \n1553         def final_transformer(field, alias):\n1554             return field.get_col(alias)\n1555 \n1556         # Try resolving all the names as fields first. If there's an error,\n1557         # treat trailing names as lookups until a field can be resolved.\n1558         last_field_exception = None\n1559         for pivot in range(len(names), 0, -1):\n1560             try:\n1561                 path, final_field, targets, rest = self.names_to_path(\n1562                     names[:pivot], opts, allow_many, fail_on_missing=True,\n1563                 )\n1564             except FieldError as exc:\n1565                 if pivot == 1:\n1566                     # The first item cannot be a lookup, so it's safe\n1567                     # to raise the field error here.\n1568                     raise\n1569                 else:\n1570                     last_field_exception = exc\n1571             else:\n1572                 # The transforms are the remaining items that couldn't be\n1573                 # resolved into fields.\n1574                 transforms = names[pivot:]\n1575                 break\n1576         for name in transforms:\n1577             def transform(field, alias, *, name, previous):\n1578                 try:\n1579                     wrapped = previous(field, alias)\n1580                     return self.try_transform(wrapped, name)\n1581                 except FieldError:\n1582                     # FieldError is raised if the transform doesn't exist.\n1583                     if isinstance(final_field, Field) and last_field_exception:\n1584                         raise last_field_exception\n1585                     else:\n1586                         raise\n1587             final_transformer = functools.partial(transform, name=name, previous=final_transformer)\n1588         # Then, add the path to the query's joins. Note that we can't trim\n1589         # joins at this stage - we will need the information about join type\n1590         # of the trimmed joins.\n1591         for join in path:\n1592             if join.filtered_relation:\n1593                 filtered_relation = join.filtered_relation.clone()\n1594                 table_alias = filtered_relation.alias\n1595             else:\n1596                 filtered_relation = None\n1597                 table_alias = None\n1598             opts = join.to_opts\n1599             if join.direct:\n1600                 nullable = self.is_nullable(join.join_field)\n1601             else:\n1602                 nullable = True\n1603             connection = Join(\n1604                 opts.db_table, alias, table_alias, INNER, join.join_field,\n1605                 nullable, filtered_relation=filtered_relation,\n1606             )\n1607             reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None\n1608             alias = self.join(\n1609                 connection, reuse=reuse,\n1610                 reuse_with_filtered_relation=reuse_with_filtered_relation,\n1611             )\n1612             joins.append(alias)\n1613             if filtered_relation:\n1614                 filtered_relation.path = joins[:]\n1615         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)\n1616 \n1617     def trim_joins(self, targets, joins, path):\n1618         \"\"\"\n1619         The 'target' parameter is the final field being joined to, 'joins'\n1620         is the full list of join aliases. The 'path' contain the PathInfos\n1621         used to create the joins.\n1622 \n1623         Return the final target field and table alias and the new active\n1624         joins.\n1625 \n1626         Always trim any direct join if the target column is already in the\n1627         previous table. Can't trim reverse joins as it's unknown if there's\n1628         anything on the other side of the join.\n1629         \"\"\"\n1630         joins = joins[:]\n1631         for pos, info in enumerate(reversed(path)):\n1632             if len(joins) == 1 or not info.direct:\n1633                 break\n1634             if info.filtered_relation:\n1635                 break\n1636             join_targets = {t.column for t in info.join_field.foreign_related_fields}\n1637             cur_targets = {t.column for t in targets}\n1638             if not cur_targets.issubset(join_targets):\n1639                 break\n1640             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}\n1641             targets = tuple(targets_dict[t.column] for t in targets)\n1642             self.unref_alias(joins.pop())\n1643         return targets, joins[-1], joins\n1644 \n1645     @classmethod\n1646     def _gen_col_aliases(cls, exprs):\n1647         for expr in exprs:\n1648             if isinstance(expr, Col):\n1649                 yield expr.alias\n1650             else:\n1651                 yield from cls._gen_col_aliases(expr.get_source_expressions())\n1652 \n1653     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False, simple_col=False):\n1654         if not allow_joins and LOOKUP_SEP in name:\n1655             raise FieldError(\"Joined field references are not permitted in this query\")\n1656         annotation = self.annotations.get(name)\n1657         if annotation is not None:\n1658             if not allow_joins:\n1659                 for alias in self._gen_col_aliases([annotation]):\n1660                     if isinstance(self.alias_map[alias], Join):\n1661                         raise FieldError(\n1662                             'Joined field references are not permitted in '\n1663                             'this query'\n1664                         )\n1665             if summarize:\n1666                 # Summarize currently means we are doing an aggregate() query\n1667                 # which is executed as a wrapped subquery if any of the\n1668                 # aggregate() elements reference an existing annotation. In\n1669                 # that case we need to return a Ref to the subquery's annotation.\n1670                 return Ref(name, self.annotation_select[name])\n1671             else:\n1672                 return annotation\n1673         else:\n1674             field_list = name.split(LOOKUP_SEP)\n1675             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)\n1676             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)\n1677             if not allow_joins and len(join_list) > 1:\n1678                 raise FieldError('Joined field references are not permitted in this query')\n1679             if len(targets) > 1:\n1680                 raise FieldError(\"Referencing multicolumn fields with F() objects \"\n1681                                  \"isn't supported\")\n1682             # Verify that the last lookup in name is a field or a transform:\n1683             # transform_function() raises FieldError if not.\n1684             join_info.transform_function(targets[0], final_alias)\n1685             if reuse is not None:\n1686                 reuse.update(join_list)\n1687             col = _get_col(targets[0], join_info.targets[0], join_list[-1], simple_col)\n1688             return col\n1689 \n1690     def split_exclude(self, filter_expr, can_reuse, names_with_path):\n1691         \"\"\"\n1692         When doing an exclude against any kind of N-to-many relation, we need\n1693         to use a subquery. This method constructs the nested query, given the\n1694         original exclude filter (filter_expr) and the portion up to the first\n1695         N-to-many relation field.\n1696 \n1697         For example, if the origin filter is ~Q(child__name='foo'), filter_expr\n1698         is ('child__name', 'foo') and can_reuse is a set of joins usable for\n1699         filters in the original query.\n1700 \n1701         We will turn this into equivalent of:\n1702             WHERE NOT (pk IN (SELECT parent_id FROM thetable\n1703                               WHERE name = 'foo' AND parent_id IS NOT NULL))\n1704 \n1705         It might be worth it to consider using WHERE NOT EXISTS as that has\n1706         saner null handling, and is easier for the backend's optimizer to\n1707         handle.\n1708         \"\"\"\n1709         filter_lhs, filter_rhs = filter_expr\n1710         if isinstance(filter_rhs, OuterRef):\n1711             filter_expr = (filter_lhs, OuterRef(filter_rhs))\n1712         elif isinstance(filter_rhs, F):\n1713             filter_expr = (filter_lhs, OuterRef(filter_rhs.name))\n1714         # Generate the inner query.\n1715         query = Query(self.model)\n1716         query._filtered_relations = self._filtered_relations\n1717         query.add_filter(filter_expr)\n1718         query.clear_ordering(True)\n1719         # Try to have as simple as possible subquery -> trim leading joins from\n1720         # the subquery.\n1721         trimmed_prefix, contains_louter = query.trim_start(names_with_path)\n1722 \n1723         # Add extra check to make sure the selected field will not be null\n1724         # since we are adding an IN <subquery> clause. This prevents the\n1725         # database from tripping over IN (...,NULL,...) selects and returning\n1726         # nothing\n1727         col = query.select[0]\n1728         select_field = col.target\n1729         alias = col.alias\n1730         if self.is_nullable(select_field):\n1731             lookup_class = select_field.get_lookup('isnull')\n1732             lookup = lookup_class(select_field.get_col(alias), False)\n1733             query.where.add(lookup, AND)\n1734         if alias in can_reuse:\n1735             pk = select_field.model._meta.pk\n1736             # Need to add a restriction so that outer query's filters are in effect for\n1737             # the subquery, too.\n1738             query.bump_prefix(self)\n1739             lookup_class = select_field.get_lookup('exact')\n1740             # Note that the query.select[0].alias is different from alias\n1741             # due to bump_prefix above.\n1742             lookup = lookup_class(pk.get_col(query.select[0].alias),\n1743                                   pk.get_col(alias))\n1744             query.where.add(lookup, AND)\n1745             query.external_aliases.add(alias)\n1746 \n1747         condition, needed_inner = self.build_filter(\n1748             ('%s__in' % trimmed_prefix, query),\n1749             current_negated=True, branch_negated=True, can_reuse=can_reuse)\n1750         if contains_louter:\n1751             or_null_condition, _ = self.build_filter(\n1752                 ('%s__isnull' % trimmed_prefix, True),\n1753                 current_negated=True, branch_negated=True, can_reuse=can_reuse)\n1754             condition.add(or_null_condition, OR)\n1755             # Note that the end result will be:\n1756             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.\n1757             # This might look crazy but due to how IN works, this seems to be\n1758             # correct. If the IS NOT NULL check is removed then outercol NOT\n1759             # IN will return UNKNOWN. If the IS NULL check is removed, then if\n1760             # outercol IS NULL we will not match the row.\n1761         return condition, needed_inner\n1762 \n1763     def set_empty(self):\n1764         self.where.add(NothingNode(), AND)\n1765 \n1766     def is_empty(self):\n1767         return any(isinstance(c, NothingNode) for c in self.where.children)\n1768 \n1769     def set_limits(self, low=None, high=None):\n1770         \"\"\"\n1771         Adjust the limits on the rows retrieved. Use low/high to set these,\n1772         as it makes it more Pythonic to read and write. When the SQL query is\n1773         created, convert them to the appropriate offset and limit values.\n1774 \n1775         Apply any limits passed in here to the existing constraints. Add low\n1776         to the current low value and clamp both to any existing high value.\n1777         \"\"\"\n1778         if high is not None:\n1779             if self.high_mark is not None:\n1780                 self.high_mark = min(self.high_mark, self.low_mark + high)\n1781             else:\n1782                 self.high_mark = self.low_mark + high\n1783         if low is not None:\n1784             if self.high_mark is not None:\n1785                 self.low_mark = min(self.high_mark, self.low_mark + low)\n1786             else:\n1787                 self.low_mark = self.low_mark + low\n1788 \n1789         if self.low_mark == self.high_mark:\n1790             self.set_empty()\n1791 \n1792     def clear_limits(self):\n1793         \"\"\"Clear any existing limits.\"\"\"\n1794         self.low_mark, self.high_mark = 0, None\n1795 \n1796     @property\n1797     def is_sliced(self):\n1798         return self.low_mark != 0 or self.high_mark is not None\n1799 \n1800     def has_limit_one(self):\n1801         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1\n1802 \n1803     def can_filter(self):\n1804         \"\"\"\n1805         Return True if adding filters to this instance is still possible.\n1806 \n1807         Typically, this means no limits or offsets have been put on the results.\n1808         \"\"\"\n1809         return not self.is_sliced\n1810 \n1811     def clear_select_clause(self):\n1812         \"\"\"Remove all fields from SELECT clause.\"\"\"\n1813         self.select = ()\n1814         self.default_cols = False\n1815         self.select_related = False\n1816         self.set_extra_mask(())\n1817         self.set_annotation_mask(())\n1818 \n1819     def clear_select_fields(self):\n1820         \"\"\"\n1821         Clear the list of fields to select (but not extra_select columns).\n1822         Some queryset types completely replace any existing list of select\n1823         columns.\n1824         \"\"\"\n1825         self.select = ()\n1826         self.values_select = ()\n1827 \n1828     def add_select_col(self, col):\n1829         self.select += col,\n1830         self.values_select += col.output_field.name,\n1831 \n1832     def set_select(self, cols):\n1833         self.default_cols = False\n1834         self.select = tuple(cols)\n1835 \n1836     def add_distinct_fields(self, *field_names):\n1837         \"\"\"\n1838         Add and resolve the given fields to the query's \"distinct on\" clause.\n1839         \"\"\"\n1840         self.distinct_fields = field_names\n1841         self.distinct = True\n1842 \n1843     def add_fields(self, field_names, allow_m2m=True):\n1844         \"\"\"\n1845         Add the given (model) fields to the select set. Add the field names in\n1846         the order specified.\n1847         \"\"\"\n1848         alias = self.get_initial_alias()\n1849         opts = self.get_meta()\n1850 \n1851         try:\n1852             cols = []\n1853             for name in field_names:\n1854                 # Join promotion note - we must not remove any rows here, so\n1855                 # if there is no existing joins, use outer join.\n1856                 join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)\n1857                 targets, final_alias, joins = self.trim_joins(\n1858                     join_info.targets,\n1859                     join_info.joins,\n1860                     join_info.path,\n1861                 )\n1862                 for target in targets:\n1863                     cols.append(join_info.transform_function(target, final_alias))\n1864             if cols:\n1865                 self.set_select(cols)\n1866         except MultiJoin:\n1867             raise FieldError(\"Invalid field name: '%s'\" % name)\n1868         except FieldError:\n1869             if LOOKUP_SEP in name:\n1870                 # For lookups spanning over relationships, show the error\n1871                 # from the model on which the lookup failed.\n1872                 raise\n1873             else:\n1874                 names = sorted([\n1875                     *get_field_names_from_opts(opts), *self.extra,\n1876                     *self.annotation_select, *self._filtered_relations\n1877                 ])\n1878                 raise FieldError(\"Cannot resolve keyword %r into field. \"\n1879                                  \"Choices are: %s\" % (name, \", \".join(names)))\n1880 \n1881     def add_ordering(self, *ordering):\n1882         \"\"\"\n1883         Add items from the 'ordering' sequence to the query's \"order by\"\n1884         clause. These items are either field names (not column names) --\n1885         possibly with a direction prefix ('-' or '?') -- or OrderBy\n1886         expressions.\n1887 \n1888         If 'ordering' is empty, clear all ordering from the query.\n1889         \"\"\"\n1890         errors = []\n1891         for item in ordering:\n1892             if not hasattr(item, 'resolve_expression') and not ORDER_PATTERN.match(item):\n1893                 errors.append(item)\n1894             if getattr(item, 'contains_aggregate', False):\n1895                 raise FieldError(\n1896                     'Using an aggregate in order_by() without also including '\n1897                     'it in annotate() is not allowed: %s' % item\n1898                 )\n1899         if errors:\n1900             raise FieldError('Invalid order_by arguments: %s' % errors)\n1901         if ordering:\n1902             self.order_by += ordering\n1903         else:\n1904             self.default_ordering = False\n1905 \n1906     def clear_ordering(self, force_empty):\n1907         \"\"\"\n1908         Remove any ordering settings. If 'force_empty' is True, there will be\n1909         no ordering in the resulting query (not even the model's default).\n1910         \"\"\"\n1911         self.order_by = ()\n1912         self.extra_order_by = ()\n1913         if force_empty:\n1914             self.default_ordering = False\n1915 \n1916     def set_group_by(self):\n1917         \"\"\"\n1918         Expand the GROUP BY clause required by the query.\n1919 \n1920         This will usually be the set of all non-aggregate fields in the\n1921         return data. If the database backend supports grouping by the\n1922         primary key, and the query would be equivalent, the optimization\n1923         will be made automatically.\n1924         \"\"\"\n1925         group_by = list(self.select)\n1926         if self.annotation_select:\n1927             for alias, annotation in self.annotation_select.items():\n1928                 try:\n1929                     inspect.getcallargs(annotation.get_group_by_cols, alias=alias)\n1930                 except TypeError:\n1931                     annotation_class = annotation.__class__\n1932                     msg = (\n1933                         '`alias=None` must be added to the signature of '\n1934                         '%s.%s.get_group_by_cols().'\n1935                     ) % (annotation_class.__module__, annotation_class.__qualname__)\n1936                     warnings.warn(msg, category=RemovedInDjango40Warning)\n1937                     group_by_cols = annotation.get_group_by_cols()\n1938                 else:\n1939                     group_by_cols = annotation.get_group_by_cols(alias=alias)\n1940                 group_by.extend(group_by_cols)\n1941         self.group_by = tuple(group_by)\n1942 \n1943     def add_select_related(self, fields):\n1944         \"\"\"\n1945         Set up the select_related data structure so that we only select\n1946         certain related models (as opposed to all models, when\n1947         self.select_related=True).\n1948         \"\"\"\n1949         if isinstance(self.select_related, bool):\n1950             field_dict = {}\n1951         else:\n1952             field_dict = self.select_related\n1953         for field in fields:\n1954             d = field_dict\n1955             for part in field.split(LOOKUP_SEP):\n1956                 d = d.setdefault(part, {})\n1957         self.select_related = field_dict\n1958 \n1959     def add_extra(self, select, select_params, where, params, tables, order_by):\n1960         \"\"\"\n1961         Add data to the various extra_* attributes for user-created additions\n1962         to the query.\n1963         \"\"\"\n1964         if select:\n1965             # We need to pair any placeholder markers in the 'select'\n1966             # dictionary with their parameters in 'select_params' so that\n1967             # subsequent updates to the select dictionary also adjust the\n1968             # parameters appropriately.\n1969             select_pairs = {}\n1970             if select_params:\n1971                 param_iter = iter(select_params)\n1972             else:\n1973                 param_iter = iter([])\n1974             for name, entry in select.items():\n1975                 entry = str(entry)\n1976                 entry_params = []\n1977                 pos = entry.find(\"%s\")\n1978                 while pos != -1:\n1979                     if pos == 0 or entry[pos - 1] != '%':\n1980                         entry_params.append(next(param_iter))\n1981                     pos = entry.find(\"%s\", pos + 2)\n1982                 select_pairs[name] = (entry, entry_params)\n1983             self.extra.update(select_pairs)\n1984         if where or params:\n1985             self.where.add(ExtraWhere(where, params), AND)\n1986         if tables:\n1987             self.extra_tables += tuple(tables)\n1988         if order_by:\n1989             self.extra_order_by = order_by\n1990 \n1991     def clear_deferred_loading(self):\n1992         \"\"\"Remove any fields from the deferred loading set.\"\"\"\n1993         self.deferred_loading = (frozenset(), True)\n1994 \n1995     def add_deferred_loading(self, field_names):\n1996         \"\"\"\n1997         Add the given list of model field names to the set of fields to\n1998         exclude from loading from the database when automatic column selection\n1999         is done. Add the new field names to any existing field names that\n2000         are deferred (or removed from any existing field names that are marked\n2001         as the only ones for immediate loading).\n2002         \"\"\"\n2003         # Fields on related models are stored in the literal double-underscore\n2004         # format, so that we can use a set datastructure. We do the foo__bar\n2005         # splitting and handling when computing the SQL column names (as part of\n2006         # get_columns()).\n2007         existing, defer = self.deferred_loading\n2008         if defer:\n2009             # Add to existing deferred names.\n2010             self.deferred_loading = existing.union(field_names), True\n2011         else:\n2012             # Remove names from the set of any existing \"immediate load\" names.\n2013             self.deferred_loading = existing.difference(field_names), False\n2014 \n2015     def add_immediate_loading(self, field_names):\n2016         \"\"\"\n2017         Add the given list of model field names to the set of fields to\n2018         retrieve when the SQL is executed (\"immediate loading\" fields). The\n2019         field names replace any existing immediate loading field names. If\n2020         there are field names already specified for deferred loading, remove\n2021         those names from the new field_names before storing the new names\n2022         for immediate loading. (That is, immediate loading overrides any\n2023         existing immediate values, but respects existing deferrals.)\n2024         \"\"\"\n2025         existing, defer = self.deferred_loading\n2026         field_names = set(field_names)\n2027         if 'pk' in field_names:\n2028             field_names.remove('pk')\n2029             field_names.add(self.get_meta().pk.name)\n2030 \n2031         if defer:\n2032             # Remove any existing deferred names from the current set before\n2033             # setting the new names.\n2034             self.deferred_loading = field_names.difference(existing), False\n2035         else:\n2036             # Replace any existing \"immediate load\" field names.\n2037             self.deferred_loading = frozenset(field_names), False\n2038 \n2039     def get_loaded_field_names(self):\n2040         \"\"\"\n2041         If any fields are marked to be deferred, return a dictionary mapping\n2042         models to a set of names in those fields that will be loaded. If a\n2043         model is not in the returned dictionary, none of its fields are\n2044         deferred.\n2045 \n2046         If no fields are marked for deferral, return an empty dictionary.\n2047         \"\"\"\n2048         # We cache this because we call this function multiple times\n2049         # (compiler.fill_related_selections, query.iterator)\n2050         try:\n2051             return self._loaded_field_names_cache\n2052         except AttributeError:\n2053             collection = {}\n2054             self.deferred_to_data(collection, self.get_loaded_field_names_cb)\n2055             self._loaded_field_names_cache = collection\n2056             return collection\n2057 \n2058     def get_loaded_field_names_cb(self, target, model, fields):\n2059         \"\"\"Callback used by get_deferred_field_names().\"\"\"\n2060         target[model] = {f.attname for f in fields}\n2061 \n2062     def set_annotation_mask(self, names):\n2063         \"\"\"Set the mask of annotations that will be returned by the SELECT.\"\"\"\n2064         if names is None:\n2065             self.annotation_select_mask = None\n2066         else:\n2067             self.annotation_select_mask = set(names)\n2068         self._annotation_select_cache = None\n2069 \n2070     def append_annotation_mask(self, names):\n2071         if self.annotation_select_mask is not None:\n2072             self.set_annotation_mask(self.annotation_select_mask.union(names))\n2073 \n2074     def set_extra_mask(self, names):\n2075         \"\"\"\n2076         Set the mask of extra select items that will be returned by SELECT.\n2077         Don't remove them from the Query since they might be used later.\n2078         \"\"\"\n2079         if names is None:\n2080             self.extra_select_mask = None\n2081         else:\n2082             self.extra_select_mask = set(names)\n2083         self._extra_select_cache = None\n2084 \n2085     def set_values(self, fields):\n2086         self.select_related = False\n2087         self.clear_deferred_loading()\n2088         self.clear_select_fields()\n2089 \n2090         if self.group_by is True:\n2091             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)\n2092             self.set_group_by()\n2093             self.clear_select_fields()\n2094 \n2095         if fields:\n2096             field_names = []\n2097             extra_names = []\n2098             annotation_names = []\n2099             if not self.extra and not self.annotations:\n2100                 # Shortcut - if there are no extra or annotations, then\n2101                 # the values() clause must be just field names.\n2102                 field_names = list(fields)\n2103             else:\n2104                 self.default_cols = False\n2105                 for f in fields:\n2106                     if f in self.extra_select:\n2107                         extra_names.append(f)\n2108                     elif f in self.annotation_select:\n2109                         annotation_names.append(f)\n2110                     else:\n2111                         field_names.append(f)\n2112             self.set_extra_mask(extra_names)\n2113             self.set_annotation_mask(annotation_names)\n2114         else:\n2115             field_names = [f.attname for f in self.model._meta.concrete_fields]\n2116 \n2117         self.values_select = tuple(field_names)\n2118         self.add_fields(field_names, True)\n2119 \n2120     @property\n2121     def annotation_select(self):\n2122         \"\"\"\n2123         Return the dictionary of aggregate columns that are not masked and\n2124         should be used in the SELECT clause. Cache this result for performance.\n2125         \"\"\"\n2126         if self._annotation_select_cache is not None:\n2127             return self._annotation_select_cache\n2128         elif not self.annotations:\n2129             return {}\n2130         elif self.annotation_select_mask is not None:\n2131             self._annotation_select_cache = {\n2132                 k: v for k, v in self.annotations.items()\n2133                 if k in self.annotation_select_mask\n2134             }\n2135             return self._annotation_select_cache\n2136         else:\n2137             return self.annotations\n2138 \n2139     @property\n2140     def extra_select(self):\n2141         if self._extra_select_cache is not None:\n2142             return self._extra_select_cache\n2143         if not self.extra:\n2144             return {}\n2145         elif self.extra_select_mask is not None:\n2146             self._extra_select_cache = {\n2147                 k: v for k, v in self.extra.items()\n2148                 if k in self.extra_select_mask\n2149             }\n2150             return self._extra_select_cache\n2151         else:\n2152             return self.extra\n2153 \n2154     def trim_start(self, names_with_path):\n2155         \"\"\"\n2156         Trim joins from the start of the join path. The candidates for trim\n2157         are the PathInfos in names_with_path structure that are m2m joins.\n2158 \n2159         Also set the select column so the start matches the join.\n2160 \n2161         This method is meant to be used for generating the subquery joins &\n2162         cols in split_exclude().\n2163 \n2164         Return a lookup usable for doing outerq.filter(lookup=self) and a\n2165         boolean indicating if the joins in the prefix contain a LEFT OUTER join.\n2166         _\"\"\"\n2167         all_paths = []\n2168         for _, paths in names_with_path:\n2169             all_paths.extend(paths)\n2170         contains_louter = False\n2171         # Trim and operate only on tables that were generated for\n2172         # the lookup part of the query. That is, avoid trimming\n2173         # joins generated for F() expressions.\n2174         lookup_tables = [\n2175             t for t in self.alias_map\n2176             if t in self._lookup_joins or t == self.base_table\n2177         ]\n2178         for trimmed_paths, path in enumerate(all_paths):\n2179             if path.m2m:\n2180                 break\n2181             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:\n2182                 contains_louter = True\n2183             alias = lookup_tables[trimmed_paths]\n2184             self.unref_alias(alias)\n2185         # The path.join_field is a Rel, lets get the other side's field\n2186         join_field = path.join_field.field\n2187         # Build the filter prefix.\n2188         paths_in_prefix = trimmed_paths\n2189         trimmed_prefix = []\n2190         for name, path in names_with_path:\n2191             if paths_in_prefix - len(path) < 0:\n2192                 break\n2193             trimmed_prefix.append(name)\n2194             paths_in_prefix -= len(path)\n2195         trimmed_prefix.append(\n2196             join_field.foreign_related_fields[0].name)\n2197         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)\n2198         # Lets still see if we can trim the first join from the inner query\n2199         # (that is, self). We can't do this for:\n2200         # - LEFT JOINs because we would miss those rows that have nothing on\n2201         #   the outer side,\n2202         # - INNER JOINs from filtered relations because we would miss their\n2203         #   filters.\n2204         first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]\n2205         if first_join.join_type != LOUTER and not first_join.filtered_relation:\n2206             select_fields = [r[0] for r in join_field.related_fields]\n2207             select_alias = lookup_tables[trimmed_paths + 1]\n2208             self.unref_alias(lookup_tables[trimmed_paths])\n2209             extra_restriction = join_field.get_extra_restriction(\n2210                 self.where_class, None, lookup_tables[trimmed_paths + 1])\n2211             if extra_restriction:\n2212                 self.where.add(extra_restriction, AND)\n2213         else:\n2214             # TODO: It might be possible to trim more joins from the start of the\n2215             # inner query if it happens to have a longer join chain containing the\n2216             # values in select_fields. Lets punt this one for now.\n2217             select_fields = [r[1] for r in join_field.related_fields]\n2218             select_alias = lookup_tables[trimmed_paths]\n2219         # The found starting point is likely a Join instead of a BaseTable reference.\n2220         # But the first entry in the query's FROM clause must not be a JOIN.\n2221         for table in self.alias_map:\n2222             if self.alias_refcount[table] > 0:\n2223                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)\n2224                 break\n2225         self.set_select([f.get_col(select_alias) for f in select_fields])\n2226         return trimmed_prefix, contains_louter\n2227 \n2228     def is_nullable(self, field):\n2229         \"\"\"\n2230         Check if the given field should be treated as nullable.\n2231 \n2232         Some backends treat '' as null and Django treats such fields as\n2233         nullable for those backends. In such situations field.null can be\n2234         False even if we should treat the field as nullable.\n2235         \"\"\"\n2236         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have\n2237         # (nor should it have) knowledge of which connection is going to be\n2238         # used. The proper fix would be to defer all decisions where\n2239         # is_nullable() is needed to the compiler stage, but that is not easy\n2240         # to do currently.\n2241         return (\n2242             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and\n2243             field.empty_strings_allowed\n2244         ) or field.null\n2245 \n2246 \n2247 def get_order_dir(field, default='ASC'):\n2248     \"\"\"\n2249     Return the field name and direction for an order specification. For\n2250     example, '-foo' is returned as ('foo', 'DESC').\n2251 \n2252     The 'default' param is used to indicate which way no prefix (or a '+'\n2253     prefix) should sort. The '-' prefix always sorts the opposite way.\n2254     \"\"\"\n2255     dirn = ORDER_DIR[default]\n2256     if field[0] == '-':\n2257         return field[1:], dirn[1]\n2258     return field, dirn[0]\n2259 \n2260 \n2261 def add_to_dict(data, key, value):\n2262     \"\"\"\n2263     Add \"value\" to the set of values for \"key\", whether or not \"key\" already\n2264     exists.\n2265     \"\"\"\n2266     if key in data:\n2267         data[key].add(value)\n2268     else:\n2269         data[key] = {value}\n2270 \n2271 \n2272 def is_reverse_o2o(field):\n2273     \"\"\"\n2274     Check if the given field is reverse-o2o. The field is expected to be some\n2275     sort of relation field or related object.\n2276     \"\"\"\n2277     return field.is_relation and field.one_to_one and not field.concrete\n2278 \n2279 \n2280 class JoinPromoter:\n2281     \"\"\"\n2282     A class to abstract away join promotion problems for complex filter\n2283     conditions.\n2284     \"\"\"\n2285 \n2286     def __init__(self, connector, num_children, negated):\n2287         self.connector = connector\n2288         self.negated = negated\n2289         if self.negated:\n2290             if connector == AND:\n2291                 self.effective_connector = OR\n2292             else:\n2293                 self.effective_connector = AND\n2294         else:\n2295             self.effective_connector = self.connector\n2296         self.num_children = num_children\n2297         # Maps of table alias to how many times it is seen as required for\n2298         # inner and/or outer joins.\n2299         self.votes = Counter()\n2300 \n2301     def add_votes(self, votes):\n2302         \"\"\"\n2303         Add single vote per item to self.votes. Parameter can be any\n2304         iterable.\n2305         \"\"\"\n2306         self.votes.update(votes)\n2307 \n2308     def update_join_types(self, query):\n2309         \"\"\"\n2310         Change join types so that the generated query is as efficient as\n2311         possible, but still correct. So, change as many joins as possible\n2312         to INNER, but don't make OUTER joins INNER if that could remove\n2313         results from the query.\n2314         \"\"\"\n2315         to_promote = set()\n2316         to_demote = set()\n2317         # The effective_connector is used so that NOT (a AND b) is treated\n2318         # similarly to (a OR b) for join promotion.\n2319         for table, votes in self.votes.items():\n2320             # We must use outer joins in OR case when the join isn't contained\n2321             # in all of the joins. Otherwise the INNER JOIN itself could remove\n2322             # valid results. Consider the case where a model with rel_a and\n2323             # rel_b relations is queried with rel_a__col=1 | rel_b__col=2. Now,\n2324             # if rel_a join doesn't produce any results is null (for example\n2325             # reverse foreign key or null value in direct foreign key), and\n2326             # there is a matching row in rel_b with col=2, then an INNER join\n2327             # to rel_a would remove a valid match from the query. So, we need\n2328             # to promote any existing INNER to LOUTER (it is possible this\n2329             # promotion in turn will be demoted later on).\n2330             if self.effective_connector == 'OR' and votes < self.num_children:\n2331                 to_promote.add(table)\n2332             # If connector is AND and there is a filter that can match only\n2333             # when there is a joinable row, then use INNER. For example, in\n2334             # rel_a__col=1 & rel_b__col=2, if either of the rels produce NULL\n2335             # as join output, then the col=1 or col=2 can't match (as\n2336             # NULL=anything is always false).\n2337             # For the OR case, if all children voted for a join to be inner,\n2338             # then we can use INNER for the join. For example:\n2339             #     (rel_a__col__icontains=Alex | rel_a__col__icontains=Russell)\n2340             # then if rel_a doesn't produce any rows, the whole condition\n2341             # can't match. Hence we can safely use INNER join.\n2342             if self.effective_connector == 'AND' or (\n2343                     self.effective_connector == 'OR' and votes == self.num_children):\n2344                 to_demote.add(table)\n2345             # Finally, what happens in cases where we have:\n2346             #    (rel_a__col=1|rel_b__col=2) & rel_a__col__gte=0\n2347             # Now, we first generate the OR clause, and promote joins for it\n2348             # in the first if branch above. Both rel_a and rel_b are promoted\n2349             # to LOUTER joins. After that we do the AND case. The OR case\n2350             # voted no inner joins but the rel_a__col__gte=0 votes inner join\n2351             # for rel_a. We demote it back to INNER join (in AND case a single\n2352             # vote is enough). The demotion is OK, if rel_a doesn't produce\n2353             # rows, then the rel_a__col__gte=0 clause can't be true, and thus\n2354             # the whole clause must be false. So, it is safe to use INNER\n2355             # join.\n2356             # Note that in this example we could just as well have the __gte\n2357             # clause and the OR clause swapped. Or we could replace the __gte\n2358             # clause with an OR clause containing rel_a__col=1|rel_a__col=2,\n2359             # and again we could safely demote to INNER.\n2360         query.promote_joins(to_promote)\n2361         query.demote_joins(to_demote)\n2362         return to_demote\n",
        "intended_behavior": "In the part of the code that translates Django query expressions (including `__isnull`) into SQL statements, ensure that a validation for `__isnull` values is performed early in the process. If a value other than a boolean is detected, raise an error before the SQL statement is constructed. This will preserve the integrity of the SQL queries generated and prevent potential database errors or unexpected behavior."
    }
]