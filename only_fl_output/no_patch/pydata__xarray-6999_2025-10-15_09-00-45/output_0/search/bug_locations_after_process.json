[
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 2001,
        "end": 2043,
        "class_name": "DataArray",
        "method_name": "rename",
        "code": "2001     def rename(\n2002         self,\n2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,\n2004         **names: Hashable,\n2005     ) -> DataArray:\n2006         \"\"\"Returns a new DataArray with renamed coordinates, dimensions or a new name.\n2007 \n2008         Parameters\n2009         ----------\n2010         new_name_or_name_dict : str or dict-like, optional\n2011             If the argument is dict-like, it used as a mapping from old\n2012             names to new names for coordinates or dimensions. Otherwise,\n2013             use the argument as the new name for this array.\n2014         **names : Hashable, optional\n2015             The keyword arguments form of a mapping from old names to\n2016             new names for coordinates or dimensions.\n2017             One of new_name_or_name_dict or names must be provided.\n2018 \n2019         Returns\n2020         -------\n2021         renamed : DataArray\n2022             Renamed array or array with renamed coordinates.\n2023 \n2024         See Also\n2025         --------\n2026         Dataset.rename\n2027         DataArray.swap_dims\n2028         \"\"\"\n2029         if new_name_or_name_dict is None and not names:\n2030             # change name to None?\n2031             return self._replace(name=None)\n2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:\n2033             # change dims/coords\n2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n2035             dataset = self._to_temp_dataset().rename(name_dict)\n2036             return self._from_temp_dataset(dataset)\n2037         if utils.hashable(new_name_or_name_dict) and names:\n2038             # change name + dims/coords\n2039             dataset = self._to_temp_dataset().rename(names)\n2040             dataarray = self._from_temp_dataset(dataset)\n2041             return dataarray._replace(name=new_name_or_name_dict)\n2042         # only change name\n2043         return self._replace(name=new_name_or_name_dict)\n",
        "intended_behavior": "The `rename` method, when used to rename a coordinate to a dimension name, needs to ensure that an index is created or updated for the newly dimension coordinate. This behavior would align it more closely with the explicit indexing functionalities seen in `set_index`. The method should check if the renaming operation converts a non-dimension coordinate into a dimension coordinate and, in such cases, either automatically handle the indexing similar to `set_index` or raise a user-friendly error message advising the use of `set_index` for such cases. This modification preserves the existing functionality by still allowing simple renaming operations while ensuring that any operation turning coordinates into dimension coordinates maintains the integrity and expected behavior of DataArray indexing."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 223,
        "end": 5777,
        "class_name": "DataArray",
        "method_name": null,
        "code": "223 class DataArray(\n224     AbstractArray, DataWithCoords, DataArrayArithmetic, DataArrayReductions\n225 ):\n226     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n227 \n228     DataArray provides a wrapper around numpy ndarrays that uses\n229     labeled dimensions and coordinates to support metadata aware\n230     operations. The API is similar to that for the pandas Series or\n231     DataFrame, but DataArray objects can have any number of dimensions,\n232     and their contents have fixed data types.\n233 \n234     Additional features over raw numpy arrays:\n235 \n236     - Apply operations over dimensions by name: ``x.sum('time')``.\n237     - Select or assign values by integer location (like numpy):\n238       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or\n239       ``x.sel(time='2014-01-01')``.\n240     - Mathematical operations (e.g., ``x - y``) vectorize across\n241       multiple dimensions (known in numpy as \"broadcasting\") based on\n242       dimension names, regardless of their original order.\n243     - Keep track of arbitrary metadata in the form of a Python\n244       dictionary: ``x.attrs``\n245     - Convert to a pandas Series: ``x.to_series()``.\n246 \n247     Getting items from or doing mathematical operations with a\n248     DataArray always returns another DataArray.\n249 \n250     Parameters\n251     ----------\n252     data : array_like\n253         Values for this array. Must be an ``numpy.ndarray``, ndarray\n254         like, or castable to an ``ndarray``. If a self-described xarray\n255         or pandas object, attempts are made to use this array's\n256         metadata to fill in other unspecified arguments. A view of the\n257         array's data is used instead of a copy if possible.\n258     coords : sequence or dict of array_like, optional\n259         Coordinates (tick labels) to use for indexing along each\n260         dimension. The following notations are accepted:\n261 \n262         - mapping {dimension name: array-like}\n263         - sequence of tuples that are valid arguments for\n264           ``xarray.Variable()``\n265           - (dims, data)\n266           - (dims, data, attrs)\n267           - (dims, data, attrs, encoding)\n268 \n269         Additionally, it is possible to define a coord whose name\n270         does not match the dimension name, or a coord based on multiple\n271         dimensions, with one of the following notations:\n272 \n273         - mapping {coord name: DataArray}\n274         - mapping {coord name: Variable}\n275         - mapping {coord name: (dimension name, array-like)}\n276         - mapping {coord name: (tuple of dimension names, array-like)}\n277 \n278     dims : Hashable or sequence of Hashable, optional\n279         Name(s) of the data dimension(s). Must be either a Hashable\n280         (only for 1D data) or a sequence of Hashables with length equal\n281         to the number of dimensions. If this argument is omitted,\n282         dimension names are taken from ``coords`` (if possible) and\n283         otherwise default to ``['dim_0', ... 'dim_n']``.\n284     name : str or None, optional\n285         Name of this array.\n286     attrs : dict_like or None, optional\n287         Attributes to assign to the new instance. By default, an empty\n288         attribute dictionary is initialized.\n289 \n290     Examples\n291     --------\n292     Create data:\n293 \n294     >>> np.random.seed(0)\n295     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n296     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n297     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n298     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n299     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n300 \n301     Initialize a dataarray with multiple dimensions:\n302 \n303     >>> da = xr.DataArray(\n304     ...     data=temperature,\n305     ...     dims=[\"x\", \"y\", \"time\"],\n306     ...     coords=dict(\n307     ...         lon=([\"x\", \"y\"], lon),\n308     ...         lat=([\"x\", \"y\"], lat),\n309     ...         time=time,\n310     ...         reference_time=reference_time,\n311     ...     ),\n312     ...     attrs=dict(\n313     ...         description=\"Ambient temperature.\",\n314     ...         units=\"degC\",\n315     ...     ),\n316     ... )\n317     >>> da\n318     <xarray.DataArray (x: 2, y: 2, time: 3)>\n319     array([[[29.11241877, 18.20125767, 22.82990387],\n320             [32.92714559, 29.94046392,  7.18177696]],\n321     <BLANKLINE>\n322            [[22.60070734, 13.78914233, 14.17424919],\n323             [18.28478802, 16.15234857, 26.63418806]]])\n324     Coordinates:\n325         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n326         lat             (x, y) float64 42.25 42.21 42.63 42.59\n327       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n328         reference_time  datetime64[ns] 2014-09-05\n329     Dimensions without coordinates: x, y\n330     Attributes:\n331         description:  Ambient temperature.\n332         units:        degC\n333 \n334     Find out where the coldest temperature was:\n335 \n336     >>> da.isel(da.argmin(...))\n337     <xarray.DataArray ()>\n338     array(7.18177696)\n339     Coordinates:\n340         lon             float64 -99.32\n341         lat             float64 42.21\n342         time            datetime64[ns] 2014-09-08\n343         reference_time  datetime64[ns] 2014-09-05\n344     Attributes:\n345         description:  Ambient temperature.\n346         units:        degC\n347     \"\"\"\n348 \n349     _cache: dict[str, Any]\n350     _coords: dict[Any, Variable]\n351     _close: Callable[[], None] | None\n352     _indexes: dict[Hashable, Index]\n353     _name: Hashable | None\n354     _variable: Variable\n355 \n356     __slots__ = (\n357         \"_cache\",\n358         \"_coords\",\n359         \"_close\",\n360         \"_indexes\",\n361         \"_name\",\n362         \"_variable\",\n363         \"__weakref__\",\n364     )\n365 \n366     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor[\"DataArray\"])\n367 \n368     def __init__(\n369         self,\n370         data: Any = dtypes.NA,\n371         coords: Sequence[Sequence[Any] | pd.Index | DataArray]\n372         | Mapping[Any, Any]\n373         | None = None,\n374         dims: Hashable | Sequence[Hashable] | None = None,\n375         name: Hashable = None,\n376         attrs: Mapping = None,\n377         # internal parameters\n378         indexes: dict[Hashable, Index] = None,\n379         fastpath: bool = False,\n380     ) -> None:\n381         if fastpath:\n382             variable = data\n383             assert dims is None\n384             assert attrs is None\n385             assert indexes is not None\n386         else:\n387             # TODO: (benbovy - explicit indexes) remove\n388             # once it becomes part of the public interface\n389             if indexes is not None:\n390                 raise ValueError(\"Providing explicit indexes is not supported yet\")\n391 \n392             # try to fill in arguments from data if they weren't supplied\n393             if coords is None:\n394 \n395                 if isinstance(data, DataArray):\n396                     coords = data.coords\n397                 elif isinstance(data, pd.Series):\n398                     coords = [data.index]\n399                 elif isinstance(data, pd.DataFrame):\n400                     coords = [data.index, data.columns]\n401                 elif isinstance(data, (pd.Index, IndexVariable)):\n402                     coords = [data]\n403 \n404             if dims is None:\n405                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n406             if name is None:\n407                 name = getattr(data, \"name\", None)\n408             if attrs is None and not isinstance(data, PANDAS_TYPES):\n409                 attrs = getattr(data, \"attrs\", None)\n410 \n411             data = _check_data_shape(data, coords, dims)\n412             data = as_compatible_data(data)\n413             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n414             variable = Variable(dims, data, attrs, fastpath=True)\n415             indexes, coords = _create_indexes_from_coords(coords)\n416 \n417         # These fully describe a DataArray\n418         self._variable = variable\n419         assert isinstance(coords, dict)\n420         self._coords = coords\n421         self._name = name\n422 \n423         # TODO(shoyer): document this argument, once it becomes part of the\n424         # public interface.\n425         self._indexes = indexes  # type: ignore[assignment]\n426 \n427         self._close = None\n428 \n429     @classmethod\n430     def _construct_direct(\n431         cls: type[T_DataArray],\n432         variable: Variable,\n433         coords: dict[Any, Variable],\n434         name: Hashable,\n435         indexes: dict[Hashable, Index],\n436     ) -> T_DataArray:\n437         \"\"\"Shortcut around __init__ for internal use when we want to skip\n438         costly validation\n439         \"\"\"\n440         obj = object.__new__(cls)\n441         obj._variable = variable\n442         obj._coords = coords\n443         obj._name = name\n444         obj._indexes = indexes\n445         obj._close = None\n446         return obj\n447 \n448     def _replace(\n449         self: T_DataArray,\n450         variable: Variable = None,\n451         coords=None,\n452         name: Hashable | None | Default = _default,\n453         indexes=None,\n454     ) -> T_DataArray:\n455         if variable is None:\n456             variable = self.variable\n457         if coords is None:\n458             coords = self._coords\n459         if indexes is None:\n460             indexes = self._indexes\n461         if name is _default:\n462             name = self.name\n463         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)\n464 \n465     def _replace_maybe_drop_dims(\n466         self: T_DataArray,\n467         variable: Variable,\n468         name: Hashable | None | Default = _default,\n469     ) -> T_DataArray:\n470         if variable.dims == self.dims and variable.shape == self.shape:\n471             coords = self._coords.copy()\n472             indexes = self._indexes\n473         elif variable.dims == self.dims:\n474             # Shape has changed (e.g. from reduce(..., keepdims=True)\n475             new_sizes = dict(zip(self.dims, variable.shape))\n476             coords = {\n477                 k: v\n478                 for k, v in self._coords.items()\n479                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n480             }\n481             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n482         else:\n483             allowed_dims = set(variable.dims)\n484             coords = {\n485                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n486             }\n487             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n488         return self._replace(variable, coords, name, indexes=indexes)\n489 \n490     def _overwrite_indexes(\n491         self: T_DataArray,\n492         indexes: Mapping[Any, Index],\n493         coords: Mapping[Any, Variable] = None,\n494         drop_coords: list[Hashable] = None,\n495         rename_dims: Mapping[Any, Any] = None,\n496     ) -> T_DataArray:\n497         \"\"\"Maybe replace indexes and their corresponding coordinates.\"\"\"\n498         if not indexes:\n499             return self\n500 \n501         if coords is None:\n502             coords = {}\n503         if drop_coords is None:\n504             drop_coords = []\n505 \n506         new_variable = self.variable.copy()\n507         new_coords = self._coords.copy()\n508         new_indexes = dict(self._indexes)\n509 \n510         for name in indexes:\n511             new_coords[name] = coords[name]\n512             new_indexes[name] = indexes[name]\n513 \n514         for name in drop_coords:\n515             new_coords.pop(name)\n516             new_indexes.pop(name)\n517 \n518         if rename_dims:\n519             new_variable.dims = [rename_dims.get(d, d) for d in new_variable.dims]\n520 \n521         return self._replace(\n522             variable=new_variable, coords=new_coords, indexes=new_indexes\n523         )\n524 \n525     def _to_temp_dataset(self) -> Dataset:\n526         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n527 \n528     def _from_temp_dataset(\n529         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default\n530     ) -> T_DataArray:\n531         variable = dataset._variables.pop(_THIS_ARRAY)\n532         coords = dataset._variables\n533         indexes = dataset._indexes\n534         return self._replace(variable, coords, name, indexes=indexes)\n535 \n536     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n537         \"\"\"splits dataarray along dimension 'dim'\"\"\"\n538 \n539         def subset(dim, label):\n540             array = self.loc[{dim: label}]\n541             array.attrs = {}\n542             return as_variable(array)\n543 \n544         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n545         variables.update({k: v for k, v in self._coords.items() if k != dim})\n546         coord_names = set(self._coords) - {dim}\n547         indexes = filter_indexes_from_coords(self._indexes, coord_names)\n548         dataset = Dataset._construct_direct(\n549             variables, coord_names, indexes=indexes, attrs=self.attrs\n550         )\n551         return dataset\n552 \n553     def _to_dataset_whole(\n554         self, name: Hashable = None, shallow_copy: bool = True\n555     ) -> Dataset:\n556         if name is None:\n557             name = self.name\n558         if name is None:\n559             raise ValueError(\n560                 \"unable to convert unnamed DataArray to a \"\n561                 \"Dataset without providing an explicit name\"\n562             )\n563         if name in self.coords:\n564             raise ValueError(\n565                 \"cannot create a Dataset from a DataArray with \"\n566                 \"the same name as one of its coordinates\"\n567             )\n568         # use private APIs for speed: this is called by _to_temp_dataset(),\n569         # which is used in the guts of a lot of operations (e.g., reindex)\n570         variables = self._coords.copy()\n571         variables[name] = self.variable\n572         if shallow_copy:\n573             for k in variables:\n574                 variables[k] = variables[k].copy(deep=False)\n575         indexes = self._indexes\n576 \n577         coord_names = set(self._coords)\n578         return Dataset._construct_direct(variables, coord_names, indexes=indexes)\n579 \n580     def to_dataset(\n581         self,\n582         dim: Hashable = None,\n583         *,\n584         name: Hashable = None,\n585         promote_attrs: bool = False,\n586     ) -> Dataset:\n587         \"\"\"Convert a DataArray to a Dataset.\n588 \n589         Parameters\n590         ----------\n591         dim : Hashable, optional\n592             Name of the dimension on this array along which to split this array\n593             into separate variables. If not provided, this array is converted\n594             into a Dataset of one variable.\n595         name : Hashable, optional\n596             Name to substitute for this array's name. Only valid if ``dim`` is\n597             not provided.\n598         promote_attrs : bool, default: False\n599             Set to True to shallow copy attrs of DataArray to returned Dataset.\n600 \n601         Returns\n602         -------\n603         dataset : Dataset\n604         \"\"\"\n605         if dim is not None and dim not in self.dims:\n606             raise TypeError(\n607                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n608             )\n609 \n610         if dim is not None:\n611             if name is not None:\n612                 raise TypeError(\"cannot supply both dim and name arguments\")\n613             result = self._to_dataset_split(dim)\n614         else:\n615             result = self._to_dataset_whole(name)\n616 \n617         if promote_attrs:\n618             result.attrs = dict(self.attrs)\n619 \n620         return result\n621 \n622     @property\n623     def name(self) -> Hashable | None:\n624         \"\"\"The name of this array.\"\"\"\n625         return self._name\n626 \n627     @name.setter\n628     def name(self, value: Hashable | None) -> None:\n629         self._name = value\n630 \n631     @property\n632     def variable(self) -> Variable:\n633         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n634         return self._variable\n635 \n636     @property\n637     def dtype(self) -> np.dtype:\n638         return self.variable.dtype\n639 \n640     @property\n641     def shape(self) -> tuple[int, ...]:\n642         return self.variable.shape\n643 \n644     @property\n645     def size(self) -> int:\n646         return self.variable.size\n647 \n648     @property\n649     def nbytes(self) -> int:\n650         \"\"\"\n651         Total bytes consumed by the elements of this DataArray's data.\n652 \n653         If the backend data array does not include ``nbytes``, estimates\n654         the bytes consumed based on the ``size`` and ``dtype``.\n655         \"\"\"\n656         return self.variable.nbytes\n657 \n658     @property\n659     def ndim(self) -> int:\n660         return self.variable.ndim\n661 \n662     def __len__(self) -> int:\n663         return len(self.variable)\n664 \n665     @property\n666     def data(self) -> Any:\n667         \"\"\"\n668         The DataArray's data as an array. The underlying array type\n669         (e.g. dask, sparse, pint) is preserved.\n670 \n671         See Also\n672         --------\n673         DataArray.to_numpy\n674         DataArray.as_numpy\n675         DataArray.values\n676         \"\"\"\n677         return self.variable.data\n678 \n679     @data.setter\n680     def data(self, value: Any) -> None:\n681         self.variable.data = value\n682 \n683     @property\n684     def values(self) -> np.ndarray:\n685         \"\"\"\n686         The array's data as a numpy.ndarray.\n687 \n688         If the array's data is not a numpy.ndarray this will attempt to convert\n689         it naively using np.array(), which will raise an error if the array\n690         type does not support coercion like this (e.g. cupy).\n691         \"\"\"\n692         return self.variable.values\n693 \n694     @values.setter\n695     def values(self, value: Any) -> None:\n696         self.variable.values = value\n697 \n698     def to_numpy(self) -> np.ndarray:\n699         \"\"\"\n700         Coerces wrapped data to numpy and returns a numpy.ndarray.\n701 \n702         See Also\n703         --------\n704         DataArray.as_numpy : Same but returns the surrounding DataArray instead.\n705         Dataset.as_numpy\n706         DataArray.values\n707         DataArray.data\n708         \"\"\"\n709         return self.variable.to_numpy()\n710 \n711     def as_numpy(self: T_DataArray) -> T_DataArray:\n712         \"\"\"\n713         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.\n714 \n715         See Also\n716         --------\n717         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.\n718         Dataset.as_numpy : Converts all variables in a Dataset.\n719         DataArray.values\n720         DataArray.data\n721         \"\"\"\n722         coords = {k: v.as_numpy() for k, v in self._coords.items()}\n723         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)\n724 \n725     @property\n726     def _in_memory(self) -> bool:\n727         return self.variable._in_memory\n728 \n729     def to_index(self) -> pd.Index:\n730         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n731         arrays.\n732         \"\"\"\n733         return self.variable.to_index()\n734 \n735     @property\n736     def dims(self) -> tuple[Hashable, ...]:\n737         \"\"\"Tuple of dimension names associated with this array.\n738 \n739         Note that the type of this property is inconsistent with\n740         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n741         consistently named properties.\n742 \n743         See Also\n744         --------\n745         DataArray.sizes\n746         Dataset.dims\n747         \"\"\"\n748         return self.variable.dims\n749 \n750     @dims.setter\n751     def dims(self, value: Any) -> NoReturn:\n752         raise AttributeError(\n753             \"you cannot assign dims on a DataArray. Use \"\n754             \".rename() or .swap_dims() instead.\"\n755         )\n756 \n757     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n758         if utils.is_dict_like(key):\n759             return key\n760         key = indexing.expanded_indexer(key, self.ndim)\n761         return dict(zip(self.dims, key))\n762 \n763     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:\n764         from .dataset import _get_virtual_variable\n765 \n766         try:\n767             var = self._coords[key]\n768         except KeyError:\n769             dim_sizes = dict(zip(self.dims, self.shape))\n770             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)\n771 \n772         return self._replace_maybe_drop_dims(var, name=key)\n773 \n774     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:\n775         if isinstance(key, str):\n776             return self._getitem_coord(key)\n777         else:\n778             # xarray-style array indexing\n779             return self.isel(indexers=self._item_key_to_dict(key))\n780 \n781     def __setitem__(self, key: Any, value: Any) -> None:\n782         if isinstance(key, str):\n783             self.coords[key] = value\n784         else:\n785             # Coordinates in key, value and self[key] should be consistent.\n786             # TODO Coordinate consistency in key is checked here, but it\n787             # causes unnecessary indexing. It should be optimized.\n788             obj = self[key]\n789             if isinstance(value, DataArray):\n790                 assert_coordinate_consistent(value, obj.coords.variables)\n791             # DataArray key -> Variable key\n792             key = {\n793                 k: v.variable if isinstance(v, DataArray) else v\n794                 for k, v in self._item_key_to_dict(key).items()\n795             }\n796             self.variable[key] = value\n797 \n798     def __delitem__(self, key: Any) -> None:\n799         del self.coords[key]\n800 \n801     @property\n802     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n803         \"\"\"Places to look-up items for attribute-style access\"\"\"\n804         yield from self._item_sources\n805         yield self.attrs\n806 \n807     @property\n808     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n809         \"\"\"Places to look-up items for key-completion\"\"\"\n810         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)\n811 \n812         # virtual coordinates\n813         # uses empty dict -- everything here can already be found in self.coords.\n814         yield HybridMappingProxy(keys=self.dims, mapping={})\n815 \n816     def __contains__(self, key: Any) -> bool:\n817         return key in self.data\n818 \n819     @property\n820     def loc(self) -> _LocIndexer:\n821         \"\"\"Attribute for location based indexing like pandas.\"\"\"\n822         return _LocIndexer(self)\n823 \n824     @property\n825     # Key type needs to be `Any` because of mypy#4167\n826     def attrs(self) -> dict[Any, Any]:\n827         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n828         return self.variable.attrs\n829 \n830     @attrs.setter\n831     def attrs(self, value: Mapping[Any, Any]) -> None:\n832         # Disable type checking to work around mypy bug - see mypy#4167\n833         self.variable.attrs = value  # type: ignore[assignment]\n834 \n835     @property\n836     def encoding(self) -> dict[Hashable, Any]:\n837         \"\"\"Dictionary of format-specific settings for how this array should be\n838         serialized.\"\"\"\n839         return self.variable.encoding\n840 \n841     @encoding.setter\n842     def encoding(self, value: Mapping[Any, Any]) -> None:\n843         self.variable.encoding = value\n844 \n845     @property\n846     def indexes(self) -> Indexes:\n847         \"\"\"Mapping of pandas.Index objects used for label based indexing.\n848 \n849         Raises an error if this Dataset has indexes that cannot be coerced\n850         to pandas.Index objects.\n851 \n852         See Also\n853         --------\n854         DataArray.xindexes\n855 \n856         \"\"\"\n857         return self.xindexes.to_pandas_indexes()\n858 \n859     @property\n860     def xindexes(self) -> Indexes:\n861         \"\"\"Mapping of xarray Index objects used for label based indexing.\"\"\"\n862         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})\n863 \n864     @property\n865     def coords(self) -> DataArrayCoordinates:\n866         \"\"\"Dictionary-like container of coordinate arrays.\"\"\"\n867         return DataArrayCoordinates(self)\n868 \n869     @overload\n870     def reset_coords(\n871         self: T_DataArray,\n872         names: Hashable | Iterable[Hashable] | None = None,\n873         drop: Literal[False] = False,\n874     ) -> Dataset:\n875         ...\n876 \n877     @overload\n878     def reset_coords(\n879         self: T_DataArray,\n880         names: Hashable | Iterable[Hashable] | None = None,\n881         *,\n882         drop: Literal[True],\n883     ) -> T_DataArray:\n884         ...\n885 \n886     def reset_coords(\n887         self: T_DataArray,\n888         names: Hashable | Iterable[Hashable] | None = None,\n889         drop: bool = False,\n890     ) -> T_DataArray | Dataset:\n891         \"\"\"Given names of coordinates, reset them to become variables.\n892 \n893         Parameters\n894         ----------\n895         names : Hashable or iterable of Hashable, optional\n896             Name(s) of non-index coordinates in this dataset to reset into\n897             variables. By default, all non-index coordinates are reset.\n898         drop : bool, default: False\n899             If True, remove coordinates instead of converting them into\n900             variables.\n901 \n902         Returns\n903         -------\n904         Dataset, or DataArray if ``drop == True``\n905         \"\"\"\n906         if names is None:\n907             names = set(self.coords) - set(self._indexes)\n908         dataset = self.coords.to_dataset().reset_coords(names, drop)\n909         if drop:\n910             return self._replace(coords=dataset._variables)\n911         if self.name is None:\n912             raise ValueError(\n913                 \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n914             )\n915         dataset[self.name] = self.variable\n916         return dataset\n917 \n918     def __dask_tokenize__(self):\n919         from dask.base import normalize_token\n920 \n921         return normalize_token((type(self), self._variable, self._coords, self._name))\n922 \n923     def __dask_graph__(self):\n924         return self._to_temp_dataset().__dask_graph__()\n925 \n926     def __dask_keys__(self):\n927         return self._to_temp_dataset().__dask_keys__()\n928 \n929     def __dask_layers__(self):\n930         return self._to_temp_dataset().__dask_layers__()\n931 \n932     @property\n933     def __dask_optimize__(self):\n934         return self._to_temp_dataset().__dask_optimize__\n935 \n936     @property\n937     def __dask_scheduler__(self):\n938         return self._to_temp_dataset().__dask_scheduler__\n939 \n940     def __dask_postcompute__(self):\n941         func, args = self._to_temp_dataset().__dask_postcompute__()\n942         return self._dask_finalize, (self.name, func) + args\n943 \n944     def __dask_postpersist__(self):\n945         func, args = self._to_temp_dataset().__dask_postpersist__()\n946         return self._dask_finalize, (self.name, func) + args\n947 \n948     @staticmethod\n949     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:\n950         ds = func(results, *args, **kwargs)\n951         variable = ds._variables.pop(_THIS_ARRAY)\n952         coords = ds._variables\n953         indexes = ds._indexes\n954         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n955 \n956     def load(self: T_DataArray, **kwargs) -> T_DataArray:\n957         \"\"\"Manually trigger loading of this array's data from disk or a\n958         remote source into memory and return this array.\n959 \n960         Normally, it should not be necessary to call this method in user code,\n961         because all xarray functions should either work on deferred data or\n962         load data automatically. However, this method can be necessary when\n963         working with many file objects on disk.\n964 \n965         Parameters\n966         ----------\n967         **kwargs : dict\n968             Additional keyword arguments passed on to ``dask.compute``.\n969 \n970         See Also\n971         --------\n972         dask.compute\n973         \"\"\"\n974         ds = self._to_temp_dataset().load(**kwargs)\n975         new = self._from_temp_dataset(ds)\n976         self._variable = new._variable\n977         self._coords = new._coords\n978         return self\n979 \n980     def compute(self: T_DataArray, **kwargs) -> T_DataArray:\n981         \"\"\"Manually trigger loading of this array's data from disk or a\n982         remote source into memory and return a new array. The original is\n983         left unaltered.\n984 \n985         Normally, it should not be necessary to call this method in user code,\n986         because all xarray functions should either work on deferred data or\n987         load data automatically. However, this method can be necessary when\n988         working with many file objects on disk.\n989 \n990         Parameters\n991         ----------\n992         **kwargs : dict\n993             Additional keyword arguments passed on to ``dask.compute``.\n994 \n995         See Also\n996         --------\n997         dask.compute\n998         \"\"\"\n999         new = self.copy(deep=False)\n1000         return new.load(**kwargs)\n1001 \n1002     def persist(self: T_DataArray, **kwargs) -> T_DataArray:\n1003         \"\"\"Trigger computation in constituent dask arrays\n1004 \n1005         This keeps them as dask arrays but encourages them to keep data in\n1006         memory.  This is particularly useful when on a distributed machine.\n1007         When on a single machine consider using ``.compute()`` instead.\n1008 \n1009         Parameters\n1010         ----------\n1011         **kwargs : dict\n1012             Additional keyword arguments passed on to ``dask.persist``.\n1013 \n1014         See Also\n1015         --------\n1016         dask.persist\n1017         \"\"\"\n1018         ds = self._to_temp_dataset().persist(**kwargs)\n1019         return self._from_temp_dataset(ds)\n1020 \n1021     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:\n1022         \"\"\"Returns a copy of this array.\n1023 \n1024         If `deep=True`, a deep copy is made of the data array.\n1025         Otherwise, a shallow copy is made, and the returned data array's\n1026         values are a new view of this data array's values.\n1027 \n1028         Use `data` to create a new object with the same structure as\n1029         original but entirely new data.\n1030 \n1031         Parameters\n1032         ----------\n1033         deep : bool, optional\n1034             Whether the data array and its coordinates are loaded into memory\n1035             and copied onto the new object. Default is True.\n1036         data : array_like, optional\n1037             Data to use in the new object. Must have same shape as original.\n1038             When `data` is used, `deep` is ignored for all data variables,\n1039             and only used for coords.\n1040 \n1041         Returns\n1042         -------\n1043         copy : DataArray\n1044             New object with dimensions, attributes, coordinates, name,\n1045             encoding, and optionally data copied from original.\n1046 \n1047         Examples\n1048         --------\n1049         Shallow versus deep copy\n1050 \n1051         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n1052         >>> array.copy()\n1053         <xarray.DataArray (x: 3)>\n1054         array([1, 2, 3])\n1055         Coordinates:\n1056           * x        (x) <U1 'a' 'b' 'c'\n1057         >>> array_0 = array.copy(deep=False)\n1058         >>> array_0[0] = 7\n1059         >>> array_0\n1060         <xarray.DataArray (x: 3)>\n1061         array([7, 2, 3])\n1062         Coordinates:\n1063           * x        (x) <U1 'a' 'b' 'c'\n1064         >>> array\n1065         <xarray.DataArray (x: 3)>\n1066         array([7, 2, 3])\n1067         Coordinates:\n1068           * x        (x) <U1 'a' 'b' 'c'\n1069 \n1070         Changing the data using the ``data`` argument maintains the\n1071         structure of the original object, but with the new data. Original\n1072         object is unaffected.\n1073 \n1074         >>> array.copy(data=[0.1, 0.2, 0.3])\n1075         <xarray.DataArray (x: 3)>\n1076         array([0.1, 0.2, 0.3])\n1077         Coordinates:\n1078           * x        (x) <U1 'a' 'b' 'c'\n1079         >>> array\n1080         <xarray.DataArray (x: 3)>\n1081         array([7, 2, 3])\n1082         Coordinates:\n1083           * x        (x) <U1 'a' 'b' 'c'\n1084 \n1085         See Also\n1086         --------\n1087         pandas.DataFrame.copy\n1088         \"\"\"\n1089         variable = self.variable.copy(deep=deep, data=data)\n1090         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n1091 \n1092         coords = {}\n1093         for k, v in self._coords.items():\n1094             if k in index_vars:\n1095                 coords[k] = index_vars[k]\n1096             else:\n1097                 coords[k] = v.copy(deep=deep)\n1098 \n1099         return self._replace(variable, coords, indexes=indexes)\n1100 \n1101     def __copy__(self: T_DataArray) -> T_DataArray:\n1102         return self.copy(deep=False)\n1103 \n1104     def __deepcopy__(self: T_DataArray, memo=None) -> T_DataArray:\n1105         # memo does nothing but is required for compatibility with\n1106         # copy.deepcopy\n1107         return self.copy(deep=True)\n1108 \n1109     # mutable objects should not be Hashable\n1110     # https://github.com/python/mypy/issues/4266\n1111     __hash__ = None  # type: ignore[assignment]\n1112 \n1113     @property\n1114     def chunks(self) -> tuple[tuple[int, ...], ...] | None:\n1115         \"\"\"\n1116         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if\n1117         the underlying data is not a dask array.\n1118 \n1119         See Also\n1120         --------\n1121         DataArray.chunk\n1122         DataArray.chunksizes\n1123         xarray.unify_chunks\n1124         \"\"\"\n1125         return self.variable.chunks\n1126 \n1127     @property\n1128     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:\n1129         \"\"\"\n1130         Mapping from dimension names to block lengths for this dataarray's data, or None if\n1131         the underlying data is not a dask array.\n1132         Cannot be modified directly, but can be modified by calling .chunk().\n1133 \n1134         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes\n1135         instead of a tuple of chunk shapes.\n1136 \n1137         See Also\n1138         --------\n1139         DataArray.chunk\n1140         DataArray.chunks\n1141         xarray.unify_chunks\n1142         \"\"\"\n1143         all_variables = [self.variable] + [c.variable for c in self.coords.values()]\n1144         return get_chunksizes(all_variables)\n1145 \n1146     def chunk(\n1147         self: T_DataArray,\n1148         chunks: (\n1149             int\n1150             | Literal[\"auto\"]\n1151             | tuple[int, ...]\n1152             | tuple[tuple[int, ...], ...]\n1153             | Mapping[Any, None | int | tuple[int, ...]]\n1154         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n1155         name_prefix: str = \"xarray-\",\n1156         token: str | None = None,\n1157         lock: bool = False,\n1158         inline_array: bool = False,\n1159         **chunks_kwargs: Any,\n1160     ) -> T_DataArray:\n1161         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n1162 \n1163         If this variable is a non-dask array, it will be converted to dask\n1164         array. If it's a dask array, it will be rechunked to the given chunk\n1165         sizes.\n1166 \n1167         If neither chunks is not provided for one or more dimensions, chunk\n1168         sizes along that dimension will not be updated; non-dask arrays will be\n1169         converted into dask arrays with a single block.\n1170 \n1171         Parameters\n1172         ----------\n1173         chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n1174             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n1175             ``{\"x\": 5, \"y\": 5}``.\n1176         name_prefix : str, optional\n1177             Prefix for the name of the new dask array.\n1178         token : str, optional\n1179             Token uniquely identifying this array.\n1180         lock : optional\n1181             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1182             already as dask array.\n1183         inline_array: optional\n1184             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1185             already as dask array.\n1186         **chunks_kwargs : {dim: chunks, ...}, optional\n1187             The keyword arguments form of ``chunks``.\n1188             One of chunks or chunks_kwargs must be provided.\n1189 \n1190         Returns\n1191         -------\n1192         chunked : xarray.DataArray\n1193 \n1194         See Also\n1195         --------\n1196         DataArray.chunks\n1197         DataArray.chunksizes\n1198         xarray.unify_chunks\n1199         dask.array.from_array\n1200         \"\"\"\n1201         if chunks is None:\n1202             warnings.warn(\n1203                 \"None value for 'chunks' is deprecated. \"\n1204                 \"It will raise an error in the future. Use instead '{}'\",\n1205                 category=FutureWarning,\n1206             )\n1207             chunks = {}\n1208 \n1209         if isinstance(chunks, (float, str, int)):\n1210             # ignoring type; unclear why it won't accept a Literal into the value.\n1211             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore\n1212         elif isinstance(chunks, (tuple, list)):\n1213             chunks = dict(zip(self.dims, chunks))\n1214         else:\n1215             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n1216 \n1217         ds = self._to_temp_dataset().chunk(\n1218             chunks,\n1219             name_prefix=name_prefix,\n1220             token=token,\n1221             lock=lock,\n1222             inline_array=inline_array,\n1223         )\n1224         return self._from_temp_dataset(ds)\n1225 \n1226     def isel(\n1227         self: T_DataArray,\n1228         indexers: Mapping[Any, Any] | None = None,\n1229         drop: bool = False,\n1230         missing_dims: ErrorOptionsWithWarn = \"raise\",\n1231         **indexers_kwargs: Any,\n1232     ) -> T_DataArray:\n1233         \"\"\"Return a new DataArray whose data is given by selecting indexes\n1234         along the specified dimension(s).\n1235 \n1236         Parameters\n1237         ----------\n1238         indexers : dict, optional\n1239             A dict with keys matching dimensions and values given\n1240             by integers, slice objects or arrays.\n1241             indexer can be a integer, slice, array-like or DataArray.\n1242             If DataArrays are passed as indexers, xarray-style indexing will be\n1243             carried out. See :ref:`indexing` for the details.\n1244             One of indexers or indexers_kwargs must be provided.\n1245         drop : bool, default: False\n1246             If ``drop=True``, drop coordinates variables indexed by integers\n1247             instead of making them scalar.\n1248         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n1249             What to do if dimensions that should be selected from are not present in the\n1250             DataArray:\n1251             - \"raise\": raise an exception\n1252             - \"warn\": raise a warning, and ignore the missing dimensions\n1253             - \"ignore\": ignore the missing dimensions\n1254         **indexers_kwargs : {dim: indexer, ...}, optional\n1255             The keyword arguments form of ``indexers``.\n1256 \n1257         Returns\n1258         -------\n1259         indexed : xarray.DataArray\n1260 \n1261         See Also\n1262         --------\n1263         Dataset.isel\n1264         DataArray.sel\n1265 \n1266         Examples\n1267         --------\n1268         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=(\"x\", \"y\"))\n1269         >>> da\n1270         <xarray.DataArray (x: 5, y: 5)>\n1271         array([[ 0,  1,  2,  3,  4],\n1272                [ 5,  6,  7,  8,  9],\n1273                [10, 11, 12, 13, 14],\n1274                [15, 16, 17, 18, 19],\n1275                [20, 21, 22, 23, 24]])\n1276         Dimensions without coordinates: x, y\n1277 \n1278         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1279         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1280         >>> da = da.isel(x=tgt_x, y=tgt_y)\n1281         >>> da\n1282         <xarray.DataArray (points: 5)>\n1283         array([ 0,  6, 12, 18, 24])\n1284         Dimensions without coordinates: points\n1285         \"\"\"\n1286 \n1287         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1288 \n1289         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1290             ds = self._to_temp_dataset()._isel_fancy(\n1291                 indexers, drop=drop, missing_dims=missing_dims\n1292             )\n1293             return self._from_temp_dataset(ds)\n1294 \n1295         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1296         # lists, or zero or one-dimensional np.ndarray's\n1297 \n1298         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1299         indexes, index_variables = isel_indexes(self.xindexes, indexers)\n1300 \n1301         coords = {}\n1302         for coord_name, coord_value in self._coords.items():\n1303             if coord_name in index_variables:\n1304                 coord_value = index_variables[coord_name]\n1305             else:\n1306                 coord_indexers = {\n1307                     k: v for k, v in indexers.items() if k in coord_value.dims\n1308                 }\n1309                 if coord_indexers:\n1310                     coord_value = coord_value.isel(coord_indexers)\n1311                     if drop and coord_value.ndim == 0:\n1312                         continue\n1313             coords[coord_name] = coord_value\n1314 \n1315         return self._replace(variable=variable, coords=coords, indexes=indexes)\n1316 \n1317     def sel(\n1318         self: T_DataArray,\n1319         indexers: Mapping[Any, Any] = None,\n1320         method: str = None,\n1321         tolerance=None,\n1322         drop: bool = False,\n1323         **indexers_kwargs: Any,\n1324     ) -> T_DataArray:\n1325         \"\"\"Return a new DataArray whose data is given by selecting index\n1326         labels along the specified dimension(s).\n1327 \n1328         In contrast to `DataArray.isel`, indexers for this method should use\n1329         labels instead of integers.\n1330 \n1331         Under the hood, this method is powered by using pandas's powerful Index\n1332         objects. This makes label based indexing essentially just as fast as\n1333         using integer indexing.\n1334 \n1335         It also means this method uses pandas's (well documented) logic for\n1336         indexing. This means you can use string shortcuts for datetime indexes\n1337         (e.g., '2000-01' to select all values in January 2000). It also means\n1338         that slices are treated as inclusive of both the start and stop values,\n1339         unlike normal Python indexing.\n1340 \n1341         .. warning::\n1342 \n1343           Do not try to assign values when using any of the indexing methods\n1344           ``isel`` or ``sel``::\n1345 \n1346             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1347             # DO NOT do this\n1348             da.isel(x=[0, 1, 2])[1] = -1\n1349 \n1350           Assigning values with the chained indexing using ``.sel`` or\n1351           ``.isel`` fails silently.\n1352 \n1353         Parameters\n1354         ----------\n1355         indexers : dict, optional\n1356             A dict with keys matching dimensions and values given\n1357             by scalars, slices or arrays of tick labels. For dimensions with\n1358             multi-index, the indexer may also be a dict-like object with keys\n1359             matching index level names.\n1360             If DataArrays are passed as indexers, xarray-style indexing will be\n1361             carried out. See :ref:`indexing` for the details.\n1362             One of indexers or indexers_kwargs must be provided.\n1363         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1364             Method to use for inexact matches:\n1365 \n1366             - None (default): only exact matches\n1367             - pad / ffill: propagate last valid index value forward\n1368             - backfill / bfill: propagate next valid index value backward\n1369             - nearest: use nearest valid index value\n1370 \n1371         tolerance : optional\n1372             Maximum distance between original and new labels for inexact\n1373             matches. The values of the index at the matching locations must\n1374             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1375         drop : bool, optional\n1376             If ``drop=True``, drop coordinates variables in `indexers` instead\n1377             of making them scalar.\n1378         **indexers_kwargs : {dim: indexer, ...}, optional\n1379             The keyword arguments form of ``indexers``.\n1380             One of indexers or indexers_kwargs must be provided.\n1381 \n1382         Returns\n1383         -------\n1384         obj : DataArray\n1385             A new DataArray with the same contents as this DataArray, except the\n1386             data and each dimension is indexed by the appropriate indexers.\n1387             If indexer DataArrays have coordinates that do not conflict with\n1388             this object, then these coordinates will be attached.\n1389             In general, each array's data will be a view of the array's data\n1390             in this DataArray, unless vectorized indexing was triggered by using\n1391             an array indexer, in which case the data will be a copy.\n1392 \n1393         See Also\n1394         --------\n1395         Dataset.sel\n1396         DataArray.isel\n1397 \n1398         Examples\n1399         --------\n1400         >>> da = xr.DataArray(\n1401         ...     np.arange(25).reshape(5, 5),\n1402         ...     coords={\"x\": np.arange(5), \"y\": np.arange(5)},\n1403         ...     dims=(\"x\", \"y\"),\n1404         ... )\n1405         >>> da\n1406         <xarray.DataArray (x: 5, y: 5)>\n1407         array([[ 0,  1,  2,  3,  4],\n1408                [ 5,  6,  7,  8,  9],\n1409                [10, 11, 12, 13, 14],\n1410                [15, 16, 17, 18, 19],\n1411                [20, 21, 22, 23, 24]])\n1412         Coordinates:\n1413           * x        (x) int64 0 1 2 3 4\n1414           * y        (y) int64 0 1 2 3 4\n1415 \n1416         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1417         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1418         >>> da = da.sel(x=tgt_x, y=tgt_y, method=\"nearest\")\n1419         >>> da\n1420         <xarray.DataArray (points: 5)>\n1421         array([ 0,  6, 12, 18, 24])\n1422         Coordinates:\n1423             x        (points) int64 0 1 2 3 4\n1424             y        (points) int64 0 1 2 3 4\n1425         Dimensions without coordinates: points\n1426         \"\"\"\n1427         ds = self._to_temp_dataset().sel(\n1428             indexers=indexers,\n1429             drop=drop,\n1430             method=method,\n1431             tolerance=tolerance,\n1432             **indexers_kwargs,\n1433         )\n1434         return self._from_temp_dataset(ds)\n1435 \n1436     def head(\n1437         self: T_DataArray,\n1438         indexers: Mapping[Any, int] | int | None = None,\n1439         **indexers_kwargs: Any,\n1440     ) -> T_DataArray:\n1441         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1442         values along the specified dimension(s). Default `n` = 5\n1443 \n1444         See Also\n1445         --------\n1446         Dataset.head\n1447         DataArray.tail\n1448         DataArray.thin\n1449         \"\"\"\n1450         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1451         return self._from_temp_dataset(ds)\n1452 \n1453     def tail(\n1454         self: T_DataArray,\n1455         indexers: Mapping[Any, int] | int | None = None,\n1456         **indexers_kwargs: Any,\n1457     ) -> T_DataArray:\n1458         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1459         values along the specified dimension(s). Default `n` = 5\n1460 \n1461         See Also\n1462         --------\n1463         Dataset.tail\n1464         DataArray.head\n1465         DataArray.thin\n1466         \"\"\"\n1467         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1468         return self._from_temp_dataset(ds)\n1469 \n1470     def thin(\n1471         self: T_DataArray,\n1472         indexers: Mapping[Any, int] | int | None = None,\n1473         **indexers_kwargs: Any,\n1474     ) -> T_DataArray:\n1475         \"\"\"Return a new DataArray whose data is given by each `n` value\n1476         along the specified dimension(s).\n1477 \n1478         Examples\n1479         --------\n1480         >>> x_arr = np.arange(0, 26)\n1481         >>> x_arr\n1482         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n1483                17, 18, 19, 20, 21, 22, 23, 24, 25])\n1484         >>> x = xr.DataArray(\n1485         ...     np.reshape(x_arr, (2, 13)),\n1486         ...     dims=(\"x\", \"y\"),\n1487         ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n1488         ... )\n1489         >>> x\n1490         <xarray.DataArray (x: 2, y: 13)>\n1491         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n1492                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])\n1493         Coordinates:\n1494           * x        (x) int64 0 1\n1495           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n1496 \n1497         >>>\n1498         >>> x.thin(3)\n1499         <xarray.DataArray (x: 1, y: 5)>\n1500         array([[ 0,  3,  6,  9, 12]])\n1501         Coordinates:\n1502           * x        (x) int64 0\n1503           * y        (y) int64 0 3 6 9 12\n1504         >>> x.thin({\"x\": 2, \"y\": 5})\n1505         <xarray.DataArray (x: 1, y: 3)>\n1506         array([[ 0,  5, 10]])\n1507         Coordinates:\n1508           * x        (x) int64 0\n1509           * y        (y) int64 0 5 10\n1510 \n1511         See Also\n1512         --------\n1513         Dataset.thin\n1514         DataArray.head\n1515         DataArray.tail\n1516         \"\"\"\n1517         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1518         return self._from_temp_dataset(ds)\n1519 \n1520     def broadcast_like(\n1521         self: T_DataArray,\n1522         other: DataArray | Dataset,\n1523         exclude: Iterable[Hashable] | None = None,\n1524     ) -> T_DataArray:\n1525         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1526 \n1527         This is equivalent to xr.broadcast(other, self)[1]\n1528 \n1529         xarray objects are broadcast against each other in arithmetic\n1530         operations, so this method is not be necessary for most uses.\n1531 \n1532         If no change is needed, the input data is returned to the output\n1533         without being copied.\n1534 \n1535         If new coords are added by the broadcast, their values are\n1536         NaN filled.\n1537 \n1538         Parameters\n1539         ----------\n1540         other : Dataset or DataArray\n1541             Object against which to broadcast this array.\n1542         exclude : iterable of Hashable, optional\n1543             Dimensions that must not be broadcasted\n1544 \n1545         Returns\n1546         -------\n1547         new_da : DataArray\n1548             The caller broadcasted against ``other``.\n1549 \n1550         Examples\n1551         --------\n1552         >>> arr1 = xr.DataArray(\n1553         ...     np.random.randn(2, 3),\n1554         ...     dims=(\"x\", \"y\"),\n1555         ...     coords={\"x\": [\"a\", \"b\"], \"y\": [\"a\", \"b\", \"c\"]},\n1556         ... )\n1557         >>> arr2 = xr.DataArray(\n1558         ...     np.random.randn(3, 2),\n1559         ...     dims=(\"x\", \"y\"),\n1560         ...     coords={\"x\": [\"a\", \"b\", \"c\"], \"y\": [\"a\", \"b\"]},\n1561         ... )\n1562         >>> arr1\n1563         <xarray.DataArray (x: 2, y: 3)>\n1564         array([[ 1.76405235,  0.40015721,  0.97873798],\n1565                [ 2.2408932 ,  1.86755799, -0.97727788]])\n1566         Coordinates:\n1567           * x        (x) <U1 'a' 'b'\n1568           * y        (y) <U1 'a' 'b' 'c'\n1569         >>> arr2\n1570         <xarray.DataArray (x: 3, y: 2)>\n1571         array([[ 0.95008842, -0.15135721],\n1572                [-0.10321885,  0.4105985 ],\n1573                [ 0.14404357,  1.45427351]])\n1574         Coordinates:\n1575           * x        (x) <U1 'a' 'b' 'c'\n1576           * y        (y) <U1 'a' 'b'\n1577         >>> arr1.broadcast_like(arr2)\n1578         <xarray.DataArray (x: 3, y: 3)>\n1579         array([[ 1.76405235,  0.40015721,  0.97873798],\n1580                [ 2.2408932 ,  1.86755799, -0.97727788],\n1581                [        nan,         nan,         nan]])\n1582         Coordinates:\n1583           * x        (x) <U1 'a' 'b' 'c'\n1584           * y        (y) <U1 'a' 'b' 'c'\n1585         \"\"\"\n1586         if exclude is None:\n1587             exclude = set()\n1588         else:\n1589             exclude = set(exclude)\n1590         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1591 \n1592         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1593 \n1594         return _broadcast_helper(\n1595             cast(\"T_DataArray\", args[1]), exclude, dims_map, common_coords\n1596         )\n1597 \n1598     def _reindex_callback(\n1599         self: T_DataArray,\n1600         aligner: alignment.Aligner,\n1601         dim_pos_indexers: dict[Hashable, Any],\n1602         variables: dict[Hashable, Variable],\n1603         indexes: dict[Hashable, Index],\n1604         fill_value: Any,\n1605         exclude_dims: frozenset[Hashable],\n1606         exclude_vars: frozenset[Hashable],\n1607     ) -> T_DataArray:\n1608         \"\"\"Callback called from ``Aligner`` to create a new reindexed DataArray.\"\"\"\n1609 \n1610         if isinstance(fill_value, dict):\n1611             fill_value = fill_value.copy()\n1612             sentinel = object()\n1613             value = fill_value.pop(self.name, sentinel)\n1614             if value is not sentinel:\n1615                 fill_value[_THIS_ARRAY] = value\n1616 \n1617         ds = self._to_temp_dataset()\n1618         reindexed = ds._reindex_callback(\n1619             aligner,\n1620             dim_pos_indexers,\n1621             variables,\n1622             indexes,\n1623             fill_value,\n1624             exclude_dims,\n1625             exclude_vars,\n1626         )\n1627         return self._from_temp_dataset(reindexed)\n1628 \n1629     def reindex_like(\n1630         self: T_DataArray,\n1631         other: DataArray | Dataset,\n1632         method: ReindexMethodOptions = None,\n1633         tolerance: int | float | Iterable[int | float] | None = None,\n1634         copy: bool = True,\n1635         fill_value=dtypes.NA,\n1636     ) -> T_DataArray:\n1637         \"\"\"Conform this object onto the indexes of another object, filling in\n1638         missing values with ``fill_value``. The default fill value is NaN.\n1639 \n1640         Parameters\n1641         ----------\n1642         other : Dataset or DataArray\n1643             Object with an 'indexes' attribute giving a mapping from dimension\n1644             names to pandas.Index objects, which provides coordinates upon\n1645             which to index the variables in this dataset. The indexes on this\n1646             other object need not be the same as the indexes on this\n1647             dataset. Any mis-matched index values will be filled in with\n1648             NaN, and any mis-matched dimension names will simply be ignored.\n1649         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1650             Method to use for filling index values from other not found on this\n1651             data array:\n1652 \n1653             - None (default): don't fill gaps\n1654             - pad / ffill: propagate last valid index value forward\n1655             - backfill / bfill: propagate next valid index value backward\n1656             - nearest: use nearest valid index value\n1657 \n1658         tolerance : optional\n1659             Maximum distance between original and new labels for inexact\n1660             matches. The values of the index at the matching locations must\n1661             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1662             Tolerance may be a scalar value, which applies the same tolerance\n1663             to all values, or list-like, which applies variable tolerance per\n1664             element. List-like must be the same size as the index and its dtype\n1665             must exactly match the index\u2019s type.\n1666         copy : bool, default: True\n1667             If ``copy=True``, data in the return value is always copied. If\n1668             ``copy=False`` and reindexing is unnecessary, or can be performed\n1669             with only slice operations, then the output may share memory with\n1670             the input. In either case, a new xarray object is always returned.\n1671         fill_value : scalar or dict-like, optional\n1672             Value to use for newly missing values. If a dict-like, maps\n1673             variable names (including coordinates) to fill values. Use this\n1674             data array's name to refer to the data array's values.\n1675 \n1676         Returns\n1677         -------\n1678         reindexed : DataArray\n1679             Another dataset array, with this array's data but coordinates from\n1680             the other object.\n1681 \n1682         See Also\n1683         --------\n1684         DataArray.reindex\n1685         align\n1686         \"\"\"\n1687         return alignment.reindex_like(\n1688             self,\n1689             other=other,\n1690             method=method,\n1691             tolerance=tolerance,\n1692             copy=copy,\n1693             fill_value=fill_value,\n1694         )\n1695 \n1696     def reindex(\n1697         self: T_DataArray,\n1698         indexers: Mapping[Any, Any] = None,\n1699         method: ReindexMethodOptions = None,\n1700         tolerance: float | Iterable[float] | None = None,\n1701         copy: bool = True,\n1702         fill_value=dtypes.NA,\n1703         **indexers_kwargs: Any,\n1704     ) -> T_DataArray:\n1705         \"\"\"Conform this object onto the indexes of another object, filling in\n1706         missing values with ``fill_value``. The default fill value is NaN.\n1707 \n1708         Parameters\n1709         ----------\n1710         indexers : dict, optional\n1711             Dictionary with keys given by dimension names and values given by\n1712             arrays of coordinates tick labels. Any mis-matched coordinate\n1713             values will be filled in with NaN, and any mis-matched dimension\n1714             names will simply be ignored.\n1715             One of indexers or indexers_kwargs must be provided.\n1716         copy : bool, optional\n1717             If ``copy=True``, data in the return value is always copied. If\n1718             ``copy=False`` and reindexing is unnecessary, or can be performed\n1719             with only slice operations, then the output may share memory with\n1720             the input. In either case, a new xarray object is always returned.\n1721         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1722             Method to use for filling index values in ``indexers`` not found on\n1723             this data array:\n1724 \n1725             - None (default): don't fill gaps\n1726             - pad / ffill: propagate last valid index value forward\n1727             - backfill / bfill: propagate next valid index value backward\n1728             - nearest: use nearest valid index value\n1729 \n1730         tolerance : float | Iterable[float] | None, default: None\n1731             Maximum distance between original and new labels for inexact\n1732             matches. The values of the index at the matching locations must\n1733             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1734             Tolerance may be a scalar value, which applies the same tolerance\n1735             to all values, or list-like, which applies variable tolerance per\n1736             element. List-like must be the same size as the index and its dtype\n1737             must exactly match the index\u2019s type.\n1738         fill_value : scalar or dict-like, optional\n1739             Value to use for newly missing values. If a dict-like, maps\n1740             variable names (including coordinates) to fill values. Use this\n1741             data array's name to refer to the data array's values.\n1742         **indexers_kwargs : {dim: indexer, ...}, optional\n1743             The keyword arguments form of ``indexers``.\n1744             One of indexers or indexers_kwargs must be provided.\n1745 \n1746         Returns\n1747         -------\n1748         reindexed : DataArray\n1749             Another dataset array, with this array's data but replaced\n1750             coordinates.\n1751 \n1752         Examples\n1753         --------\n1754         Reverse latitude:\n1755 \n1756         >>> da = xr.DataArray(\n1757         ...     np.arange(4),\n1758         ...     coords=[np.array([90, 89, 88, 87])],\n1759         ...     dims=\"lat\",\n1760         ... )\n1761         >>> da\n1762         <xarray.DataArray (lat: 4)>\n1763         array([0, 1, 2, 3])\n1764         Coordinates:\n1765           * lat      (lat) int64 90 89 88 87\n1766         >>> da.reindex(lat=da.lat[::-1])\n1767         <xarray.DataArray (lat: 4)>\n1768         array([3, 2, 1, 0])\n1769         Coordinates:\n1770           * lat      (lat) int64 87 88 89 90\n1771 \n1772         See Also\n1773         --------\n1774         DataArray.reindex_like\n1775         align\n1776         \"\"\"\n1777         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n1778         return alignment.reindex(\n1779             self,\n1780             indexers=indexers,\n1781             method=method,\n1782             tolerance=tolerance,\n1783             copy=copy,\n1784             fill_value=fill_value,\n1785         )\n1786 \n1787     def interp(\n1788         self: T_DataArray,\n1789         coords: Mapping[Any, Any] | None = None,\n1790         method: InterpOptions = \"linear\",\n1791         assume_sorted: bool = False,\n1792         kwargs: Mapping[str, Any] | None = None,\n1793         **coords_kwargs: Any,\n1794     ) -> T_DataArray:\n1795         \"\"\"Interpolate a DataArray onto new coordinates\n1796 \n1797         Performs univariate or multivariate interpolation of a DataArray onto\n1798         new coordinates using scipy's interpolation routines. If interpolating\n1799         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n1800         called. When interpolating along multiple existing dimensions, an\n1801         attempt is made to decompose the interpolation into multiple\n1802         1-dimensional interpolations. If this is possible,\n1803         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n1804         :py:func:`scipy.interpolate.interpn` is called.\n1805 \n1806         Parameters\n1807         ----------\n1808         coords : dict, optional\n1809             Mapping from dimension names to the new coordinates.\n1810             New coordinate can be a scalar, array-like or DataArray.\n1811             If DataArrays are passed as new coordinates, their dimensions are\n1812             used for the broadcasting. Missing values are skipped.\n1813         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n1814             The method used to interpolate. The method should be supported by\n1815             the scipy interpolator:\n1816 \n1817             - ``interp1d``: {\"linear\", \"nearest\", \"zero\", \"slinear\",\n1818               \"quadratic\", \"cubic\", \"polynomial\"}\n1819             - ``interpn``: {\"linear\", \"nearest\"}\n1820 \n1821             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n1822             also be provided.\n1823         assume_sorted : bool, default: False\n1824             If False, values of x can be in any order and they are sorted\n1825             first. If True, x has to be an array of monotonically increasing\n1826             values.\n1827         kwargs : dict-like or None, default: None\n1828             Additional keyword arguments passed to scipy's interpolator. Valid\n1829             options and their behavior depend whether ``interp1d`` or\n1830             ``interpn`` is used.\n1831         **coords_kwargs : {dim: coordinate, ...}, optional\n1832             The keyword arguments form of ``coords``.\n1833             One of coords or coords_kwargs must be provided.\n1834 \n1835         Returns\n1836         -------\n1837         interpolated : DataArray\n1838             New dataarray on the new coordinates.\n1839 \n1840         Notes\n1841         -----\n1842         scipy is required.\n1843 \n1844         See Also\n1845         --------\n1846         scipy.interpolate.interp1d\n1847         scipy.interpolate.interpn\n1848 \n1849         Examples\n1850         --------\n1851         >>> da = xr.DataArray(\n1852         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n1853         ...     dims=(\"x\", \"y\"),\n1854         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n1855         ... )\n1856         >>> da\n1857         <xarray.DataArray (x: 3, y: 4)>\n1858         array([[ 1.,  4.,  2.,  9.],\n1859                [ 2.,  7.,  6., nan],\n1860                [ 6., nan,  5.,  8.]])\n1861         Coordinates:\n1862           * x        (x) int64 0 1 2\n1863           * y        (y) int64 10 12 14 16\n1864 \n1865         1D linear interpolation (the default):\n1866 \n1867         >>> da.interp(x=[0, 0.75, 1.25, 1.75])\n1868         <xarray.DataArray (x: 4, y: 4)>\n1869         array([[1.  , 4.  , 2.  ,  nan],\n1870                [1.75, 6.25, 5.  ,  nan],\n1871                [3.  ,  nan, 5.75,  nan],\n1872                [5.  ,  nan, 5.25,  nan]])\n1873         Coordinates:\n1874           * y        (y) int64 10 12 14 16\n1875           * x        (x) float64 0.0 0.75 1.25 1.75\n1876 \n1877         1D nearest interpolation:\n1878 \n1879         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n1880         <xarray.DataArray (x: 4, y: 4)>\n1881         array([[ 1.,  4.,  2.,  9.],\n1882                [ 2.,  7.,  6., nan],\n1883                [ 2.,  7.,  6., nan],\n1884                [ 6., nan,  5.,  8.]])\n1885         Coordinates:\n1886           * y        (y) int64 10 12 14 16\n1887           * x        (x) float64 0.0 0.75 1.25 1.75\n1888 \n1889         1D linear extrapolation:\n1890 \n1891         >>> da.interp(\n1892         ...     x=[1, 1.5, 2.5, 3.5],\n1893         ...     method=\"linear\",\n1894         ...     kwargs={\"fill_value\": \"extrapolate\"},\n1895         ... )\n1896         <xarray.DataArray (x: 4, y: 4)>\n1897         array([[ 2. ,  7. ,  6. ,  nan],\n1898                [ 4. ,  nan,  5.5,  nan],\n1899                [ 8. ,  nan,  4.5,  nan],\n1900                [12. ,  nan,  3.5,  nan]])\n1901         Coordinates:\n1902           * y        (y) int64 10 12 14 16\n1903           * x        (x) float64 1.0 1.5 2.5 3.5\n1904 \n1905         2D linear interpolation:\n1906 \n1907         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n1908         <xarray.DataArray (x: 4, y: 3)>\n1909         array([[2.5  , 3.   ,   nan],\n1910                [4.   , 5.625,   nan],\n1911                [  nan,   nan,   nan],\n1912                [  nan,   nan,   nan]])\n1913         Coordinates:\n1914           * x        (x) float64 0.0 0.75 1.25 1.75\n1915           * y        (y) int64 11 13 15\n1916         \"\"\"\n1917         if self.dtype.kind not in \"uifc\":\n1918             raise TypeError(\n1919                 \"interp only works for a numeric type array. \"\n1920                 \"Given {}.\".format(self.dtype)\n1921             )\n1922         ds = self._to_temp_dataset().interp(\n1923             coords,\n1924             method=method,\n1925             kwargs=kwargs,\n1926             assume_sorted=assume_sorted,\n1927             **coords_kwargs,\n1928         )\n1929         return self._from_temp_dataset(ds)\n1930 \n1931     def interp_like(\n1932         self: T_DataArray,\n1933         other: DataArray | Dataset,\n1934         method: InterpOptions = \"linear\",\n1935         assume_sorted: bool = False,\n1936         kwargs: Mapping[str, Any] | None = None,\n1937     ) -> T_DataArray:\n1938         \"\"\"Interpolate this object onto the coordinates of another object,\n1939         filling out of range values with NaN.\n1940 \n1941         If interpolating along a single existing dimension,\n1942         :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n1943         along multiple existing dimensions, an attempt is made to decompose the\n1944         interpolation into multiple 1-dimensional interpolations. If this is\n1945         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n1946         :py:func:`scipy.interpolate.interpn` is called.\n1947 \n1948         Parameters\n1949         ----------\n1950         other : Dataset or DataArray\n1951             Object with an 'indexes' attribute giving a mapping from dimension\n1952             names to an 1d array-like, which provides coordinates upon\n1953             which to index the variables in this dataset. Missing values are skipped.\n1954         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n1955             The method used to interpolate. The method should be supported by\n1956             the scipy interpolator:\n1957 \n1958             - {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\",\n1959               \"polynomial\"} when ``interp1d`` is called.\n1960             - {\"linear\", \"nearest\"} when ``interpn`` is called.\n1961 \n1962             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n1963             also be provided.\n1964         assume_sorted : bool, default: False\n1965             If False, values of coordinates that are interpolated over can be\n1966             in any order and they are sorted first. If True, interpolated\n1967             coordinates are assumed to be an array of monotonically increasing\n1968             values.\n1969         kwargs : dict, optional\n1970             Additional keyword passed to scipy's interpolator.\n1971 \n1972         Returns\n1973         -------\n1974         interpolated : DataArray\n1975             Another dataarray by interpolating this dataarray's data along the\n1976             coordinates of the other object.\n1977 \n1978         Notes\n1979         -----\n1980         scipy is required.\n1981         If the dataarray has object-type coordinates, reindex is used for these\n1982         coordinates instead of the interpolation.\n1983 \n1984         See Also\n1985         --------\n1986         DataArray.interp\n1987         DataArray.reindex_like\n1988         \"\"\"\n1989         if self.dtype.kind not in \"uifc\":\n1990             raise TypeError(\n1991                 \"interp only works for a numeric type array. \"\n1992                 \"Given {}.\".format(self.dtype)\n1993             )\n1994         ds = self._to_temp_dataset().interp_like(\n1995             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n1996         )\n1997         return self._from_temp_dataset(ds)\n1998 \n1999     # change type of self and return to T_DataArray once\n2000     # https://github.com/python/mypy/issues/12846 is resolved\n2001     def rename(\n2002         self,\n2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,\n2004         **names: Hashable,\n2005     ) -> DataArray:\n2006         \"\"\"Returns a new DataArray with renamed coordinates, dimensions or a new name.\n2007 \n2008         Parameters\n2009         ----------\n2010         new_name_or_name_dict : str or dict-like, optional\n2011             If the argument is dict-like, it used as a mapping from old\n2012             names to new names for coordinates or dimensions. Otherwise,\n2013             use the argument as the new name for this array.\n2014         **names : Hashable, optional\n2015             The keyword arguments form of a mapping from old names to\n2016             new names for coordinates or dimensions.\n2017             One of new_name_or_name_dict or names must be provided.\n2018 \n2019         Returns\n2020         -------\n2021         renamed : DataArray\n2022             Renamed array or array with renamed coordinates.\n2023 \n2024         See Also\n2025         --------\n2026         Dataset.rename\n2027         DataArray.swap_dims\n2028         \"\"\"\n2029         if new_name_or_name_dict is None and not names:\n2030             # change name to None?\n2031             return self._replace(name=None)\n2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:\n2033             # change dims/coords\n2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n2035             dataset = self._to_temp_dataset().rename(name_dict)\n2036             return self._from_temp_dataset(dataset)\n2037         if utils.hashable(new_name_or_name_dict) and names:\n2038             # change name + dims/coords\n2039             dataset = self._to_temp_dataset().rename(names)\n2040             dataarray = self._from_temp_dataset(dataset)\n2041             return dataarray._replace(name=new_name_or_name_dict)\n2042         # only change name\n2043         return self._replace(name=new_name_or_name_dict)\n2044 \n2045     def swap_dims(\n2046         self: T_DataArray,\n2047         dims_dict: Mapping[Any, Hashable] | None = None,\n2048         **dims_kwargs,\n2049     ) -> T_DataArray:\n2050         \"\"\"Returns a new DataArray with swapped dimensions.\n2051 \n2052         Parameters\n2053         ----------\n2054         dims_dict : dict-like\n2055             Dictionary whose keys are current dimension names and whose values\n2056             are new names.\n2057         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n2058             The keyword arguments form of ``dims_dict``.\n2059             One of dims_dict or dims_kwargs must be provided.\n2060 \n2061         Returns\n2062         -------\n2063         swapped : DataArray\n2064             DataArray with swapped dimensions.\n2065 \n2066         Examples\n2067         --------\n2068         >>> arr = xr.DataArray(\n2069         ...     data=[0, 1],\n2070         ...     dims=\"x\",\n2071         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n2072         ... )\n2073         >>> arr\n2074         <xarray.DataArray (x: 2)>\n2075         array([0, 1])\n2076         Coordinates:\n2077           * x        (x) <U1 'a' 'b'\n2078             y        (x) int64 0 1\n2079 \n2080         >>> arr.swap_dims({\"x\": \"y\"})\n2081         <xarray.DataArray (y: 2)>\n2082         array([0, 1])\n2083         Coordinates:\n2084             x        (y) <U1 'a' 'b'\n2085           * y        (y) int64 0 1\n2086 \n2087         >>> arr.swap_dims({\"x\": \"z\"})\n2088         <xarray.DataArray (z: 2)>\n2089         array([0, 1])\n2090         Coordinates:\n2091             x        (z) <U1 'a' 'b'\n2092             y        (z) int64 0 1\n2093         Dimensions without coordinates: z\n2094 \n2095         See Also\n2096         --------\n2097         DataArray.rename\n2098         Dataset.swap_dims\n2099         \"\"\"\n2100         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n2101         ds = self._to_temp_dataset().swap_dims(dims_dict)\n2102         return self._from_temp_dataset(ds)\n2103 \n2104     # change type of self and return to T_DataArray once\n2105     # https://github.com/python/mypy/issues/12846 is resolved\n2106     def expand_dims(\n2107         self,\n2108         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,\n2109         axis: None | int | Sequence[int] = None,\n2110         **dim_kwargs: Any,\n2111     ) -> DataArray:\n2112         \"\"\"Return a new object with an additional axis (or axes) inserted at\n2113         the corresponding position in the array shape. The new object is a\n2114         view into the underlying array, not a copy.\n2115 \n2116         If dim is already a scalar coordinate, it will be promoted to a 1D\n2117         coordinate consisting of a single value.\n2118 \n2119         Parameters\n2120         ----------\n2121         dim : Hashable, sequence of Hashable, dict, or None, optional\n2122             Dimensions to include on the new variable.\n2123             If provided as str or sequence of str, then dimensions are inserted\n2124             with length 1. If provided as a dict, then the keys are the new\n2125             dimensions and the values are either integers (giving the length of\n2126             the new dimensions) or sequence/ndarray (giving the coordinates of\n2127             the new dimensions).\n2128         axis : int, sequence of int, or None, default: None\n2129             Axis position(s) where new axis is to be inserted (position(s) on\n2130             the result array). If a sequence of integers is passed,\n2131             multiple axes are inserted. In this case, dim arguments should be\n2132             same length list. If axis=None is passed, all the axes will be\n2133             inserted to the start of the result array.\n2134         **dim_kwargs : int or sequence or ndarray\n2135             The keywords are arbitrary dimensions being inserted and the values\n2136             are either the lengths of the new dims (if int is given), or their\n2137             coordinates. Note, this is an alternative to passing a dict to the\n2138             dim kwarg and will only be used if dim is None.\n2139 \n2140         Returns\n2141         -------\n2142         expanded : DataArray\n2143             This object, but with additional dimension(s).\n2144 \n2145         See Also\n2146         --------\n2147         Dataset.expand_dims\n2148         \"\"\"\n2149         if isinstance(dim, int):\n2150             raise TypeError(\"dim should be Hashable or sequence/mapping of Hashables\")\n2151         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n2152             if len(dim) != len(set(dim)):\n2153                 raise ValueError(\"dims should not contain duplicate values.\")\n2154             dim = dict.fromkeys(dim, 1)\n2155         elif dim is not None and not isinstance(dim, Mapping):\n2156             dim = {cast(Hashable, dim): 1}\n2157 \n2158         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n2159         ds = self._to_temp_dataset().expand_dims(dim, axis)\n2160         return self._from_temp_dataset(ds)\n2161 \n2162     # change type of self and return to T_DataArray once\n2163     # https://github.com/python/mypy/issues/12846 is resolved\n2164     def set_index(\n2165         self,\n2166         indexes: Mapping[Any, Hashable | Sequence[Hashable]] = None,\n2167         append: bool = False,\n2168         **indexes_kwargs: Hashable | Sequence[Hashable],\n2169     ) -> DataArray:\n2170         \"\"\"Set DataArray (multi-)indexes using one or more existing\n2171         coordinates.\n2172 \n2173         Parameters\n2174         ----------\n2175         indexes : {dim: index, ...}\n2176             Mapping from names matching dimensions and values given\n2177             by (lists of) the names of existing coordinates or variables to set\n2178             as new (multi-)index.\n2179         append : bool, default: False\n2180             If True, append the supplied index(es) to the existing index(es).\n2181             Otherwise replace the existing index(es).\n2182         **indexes_kwargs : optional\n2183             The keyword arguments form of ``indexes``.\n2184             One of indexes or indexes_kwargs must be provided.\n2185 \n2186         Returns\n2187         -------\n2188         obj : DataArray\n2189             Another DataArray, with this data but replaced coordinates.\n2190 \n2191         Examples\n2192         --------\n2193         >>> arr = xr.DataArray(\n2194         ...     data=np.ones((2, 3)),\n2195         ...     dims=[\"x\", \"y\"],\n2196         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n2197         ... )\n2198         >>> arr\n2199         <xarray.DataArray (x: 2, y: 3)>\n2200         array([[1., 1., 1.],\n2201                [1., 1., 1.]])\n2202         Coordinates:\n2203           * x        (x) int64 0 1\n2204           * y        (y) int64 0 1 2\n2205             a        (x) int64 3 4\n2206         >>> arr.set_index(x=\"a\")\n2207         <xarray.DataArray (x: 2, y: 3)>\n2208         array([[1., 1., 1.],\n2209                [1., 1., 1.]])\n2210         Coordinates:\n2211           * x        (x) int64 3 4\n2212           * y        (y) int64 0 1 2\n2213 \n2214         See Also\n2215         --------\n2216         DataArray.reset_index\n2217         \"\"\"\n2218         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)\n2219         return self._from_temp_dataset(ds)\n2220 \n2221     # change type of self and return to T_DataArray once\n2222     # https://github.com/python/mypy/issues/12846 is resolved\n2223     def reset_index(\n2224         self,\n2225         dims_or_levels: Hashable | Sequence[Hashable],\n2226         drop: bool = False,\n2227     ) -> DataArray:\n2228         \"\"\"Reset the specified index(es) or multi-index level(s).\n2229 \n2230         Parameters\n2231         ----------\n2232         dims_or_levels : Hashable or sequence of Hashable\n2233             Name(s) of the dimension(s) and/or multi-index level(s) that will\n2234             be reset.\n2235         drop : bool, default: False\n2236             If True, remove the specified indexes and/or multi-index levels\n2237             instead of extracting them as new coordinates (default: False).\n2238 \n2239         Returns\n2240         -------\n2241         obj : DataArray\n2242             Another dataarray, with this dataarray's data but replaced\n2243             coordinates.\n2244 \n2245         See Also\n2246         --------\n2247         DataArray.set_index\n2248         \"\"\"\n2249         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)\n2250         return self._from_temp_dataset(ds)\n2251 \n2252     def reorder_levels(\n2253         self: T_DataArray,\n2254         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,\n2255         **dim_order_kwargs: Sequence[int | Hashable],\n2256     ) -> T_DataArray:\n2257         \"\"\"Rearrange index levels using input order.\n2258 \n2259         Parameters\n2260         ----------\n2261         dim_order dict-like of Hashable to int or Hashable: optional\n2262             Mapping from names matching dimensions and values given\n2263             by lists representing new level orders. Every given dimension\n2264             must have a multi-index.\n2265         **dim_order_kwargs : optional\n2266             The keyword arguments form of ``dim_order``.\n2267             One of dim_order or dim_order_kwargs must be provided.\n2268 \n2269         Returns\n2270         -------\n2271         obj : DataArray\n2272             Another dataarray, with this dataarray's data but replaced\n2273             coordinates.\n2274         \"\"\"\n2275         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)\n2276         return self._from_temp_dataset(ds)\n2277 \n2278     def stack(\n2279         self: T_DataArray,\n2280         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,\n2281         create_index: bool | None = True,\n2282         index_cls: type[Index] = PandasMultiIndex,\n2283         **dimensions_kwargs: Sequence[Hashable],\n2284     ) -> T_DataArray:\n2285         \"\"\"\n2286         Stack any number of existing dimensions into a single new dimension.\n2287 \n2288         New dimensions will be added at the end, and the corresponding\n2289         coordinate variables will be combined into a MultiIndex.\n2290 \n2291         Parameters\n2292         ----------\n2293         dimensions : mapping of Hashable to sequence of Hashable\n2294             Mapping of the form `new_name=(dim1, dim2, ...)`.\n2295             Names of new dimensions, and the existing dimensions that they\n2296             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n2297             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n2298             all dimensions.\n2299         create_index : bool or None, default: True\n2300             If True, create a multi-index for each of the stacked dimensions.\n2301             If False, don't create any index.\n2302             If None, create a multi-index only if exactly one single (1-d) coordinate\n2303             index is found for every dimension to stack.\n2304         index_cls: class, optional\n2305             Can be used to pass a custom multi-index type. Must be an Xarray index that\n2306             implements `.stack()`. By default, a pandas multi-index wrapper is used.\n2307         **dimensions_kwargs\n2308             The keyword arguments form of ``dimensions``.\n2309             One of dimensions or dimensions_kwargs must be provided.\n2310 \n2311         Returns\n2312         -------\n2313         stacked : DataArray\n2314             DataArray with stacked data.\n2315 \n2316         Examples\n2317         --------\n2318         >>> arr = xr.DataArray(\n2319         ...     np.arange(6).reshape(2, 3),\n2320         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2321         ... )\n2322         >>> arr\n2323         <xarray.DataArray (x: 2, y: 3)>\n2324         array([[0, 1, 2],\n2325                [3, 4, 5]])\n2326         Coordinates:\n2327           * x        (x) <U1 'a' 'b'\n2328           * y        (y) int64 0 1 2\n2329         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2330         >>> stacked.indexes[\"z\"]\n2331         MultiIndex([('a', 0),\n2332                     ('a', 1),\n2333                     ('a', 2),\n2334                     ('b', 0),\n2335                     ('b', 1),\n2336                     ('b', 2)],\n2337                    name='z')\n2338 \n2339         See Also\n2340         --------\n2341         DataArray.unstack\n2342         \"\"\"\n2343         ds = self._to_temp_dataset().stack(\n2344             dimensions,\n2345             create_index=create_index,\n2346             index_cls=index_cls,\n2347             **dimensions_kwargs,\n2348         )\n2349         return self._from_temp_dataset(ds)\n2350 \n2351     # change type of self and return to T_DataArray once\n2352     # https://github.com/python/mypy/issues/12846 is resolved\n2353     def unstack(\n2354         self,\n2355         dim: Hashable | Sequence[Hashable] | None = None,\n2356         fill_value: Any = dtypes.NA,\n2357         sparse: bool = False,\n2358     ) -> DataArray:\n2359         \"\"\"\n2360         Unstack existing dimensions corresponding to MultiIndexes into\n2361         multiple new dimensions.\n2362 \n2363         New dimensions will be added at the end.\n2364 \n2365         Parameters\n2366         ----------\n2367         dim : Hashable or sequence of Hashable, optional\n2368             Dimension(s) over which to unstack. By default unstacks all\n2369             MultiIndexes.\n2370         fill_value : scalar or dict-like, default: nan\n2371             Value to be filled. If a dict-like, maps variable names to\n2372             fill values. Use the data array's name to refer to its\n2373             name. If not provided or if the dict-like does not contain\n2374             all variables, the dtype's NA value will be used.\n2375         sparse : bool, default: False\n2376             Use sparse-array if True\n2377 \n2378         Returns\n2379         -------\n2380         unstacked : DataArray\n2381             Array with unstacked data.\n2382 \n2383         Examples\n2384         --------\n2385         >>> arr = xr.DataArray(\n2386         ...     np.arange(6).reshape(2, 3),\n2387         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2388         ... )\n2389         >>> arr\n2390         <xarray.DataArray (x: 2, y: 3)>\n2391         array([[0, 1, 2],\n2392                [3, 4, 5]])\n2393         Coordinates:\n2394           * x        (x) <U1 'a' 'b'\n2395           * y        (y) int64 0 1 2\n2396         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2397         >>> stacked.indexes[\"z\"]\n2398         MultiIndex([('a', 0),\n2399                     ('a', 1),\n2400                     ('a', 2),\n2401                     ('b', 0),\n2402                     ('b', 1),\n2403                     ('b', 2)],\n2404                    name='z')\n2405         >>> roundtripped = stacked.unstack()\n2406         >>> arr.identical(roundtripped)\n2407         True\n2408 \n2409         See Also\n2410         --------\n2411         DataArray.stack\n2412         \"\"\"\n2413         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n2414         return self._from_temp_dataset(ds)\n2415 \n2416     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:\n2417         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n2418         stacked coordinate.\n2419 \n2420         This is the inverse operation of Dataset.to_stacked_array.\n2421 \n2422         Parameters\n2423         ----------\n2424         dim : Hashable\n2425             Name of existing dimension to unstack\n2426         level : int or Hashable, default: 0\n2427             The MultiIndex level to expand to a dataset along. Can either be\n2428             the integer index of the level or its name.\n2429 \n2430         Returns\n2431         -------\n2432         unstacked: Dataset\n2433 \n2434         Examples\n2435         --------\n2436         >>> arr = xr.DataArray(\n2437         ...     np.arange(6).reshape(2, 3),\n2438         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2439         ... )\n2440         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n2441         >>> data\n2442         <xarray.Dataset>\n2443         Dimensions:  (x: 2, y: 3)\n2444         Coordinates:\n2445           * x        (x) <U1 'a' 'b'\n2446           * y        (y) int64 0 1 2\n2447         Data variables:\n2448             a        (x, y) int64 0 1 2 3 4 5\n2449             b        (x) int64 0 3\n2450         >>> stacked = data.to_stacked_array(\"z\", [\"x\"])\n2451         >>> stacked.indexes[\"z\"]\n2452         MultiIndex([('a', 0.0),\n2453                     ('a', 1.0),\n2454                     ('a', 2.0),\n2455                     ('b', nan)],\n2456                    name='z')\n2457         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n2458         >>> data.identical(roundtripped)\n2459         True\n2460 \n2461         See Also\n2462         --------\n2463         Dataset.to_stacked_array\n2464         \"\"\"\n2465         idx = self._indexes[dim].to_pandas_index()\n2466         if not isinstance(idx, pd.MultiIndex):\n2467             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n2468 \n2469         level_number = idx._get_level_number(level)\n2470         variables = idx.levels[level_number]\n2471         variable_dim = idx.names[level_number]\n2472 \n2473         # pull variables out of datarray\n2474         data_dict = {}\n2475         for k in variables:\n2476             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)\n2477 \n2478         # unstacked dataset\n2479         return Dataset(data_dict)\n2480 \n2481     def transpose(\n2482         self: T_DataArray,\n2483         *dims: Hashable,\n2484         transpose_coords: bool = True,\n2485         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2486     ) -> T_DataArray:\n2487         \"\"\"Return a new DataArray object with transposed dimensions.\n2488 \n2489         Parameters\n2490         ----------\n2491         *dims : Hashable, optional\n2492             By default, reverse the dimensions. Otherwise, reorder the\n2493             dimensions to this order.\n2494         transpose_coords : bool, default: True\n2495             If True, also transpose the coordinates of this DataArray.\n2496         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2497             What to do if dimensions that should be selected from are not present in the\n2498             DataArray:\n2499             - \"raise\": raise an exception\n2500             - \"warn\": raise a warning, and ignore the missing dimensions\n2501             - \"ignore\": ignore the missing dimensions\n2502 \n2503         Returns\n2504         -------\n2505         transposed : DataArray\n2506             The returned DataArray's array is transposed.\n2507 \n2508         Notes\n2509         -----\n2510         This operation returns a view of this array's data. It is\n2511         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n2512         -- the data will be fully loaded.\n2513 \n2514         See Also\n2515         --------\n2516         numpy.transpose\n2517         Dataset.transpose\n2518         \"\"\"\n2519         if dims:\n2520             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))\n2521         variable = self.variable.transpose(*dims)\n2522         if transpose_coords:\n2523             coords: dict[Hashable, Variable] = {}\n2524             for name, coord in self.coords.items():\n2525                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n2526                 coords[name] = coord.variable.transpose(*coord_dims)\n2527             return self._replace(variable, coords)\n2528         else:\n2529             return self._replace(variable)\n2530 \n2531     @property\n2532     def T(self: T_DataArray) -> T_DataArray:\n2533         return self.transpose()\n2534 \n2535     # change type of self and return to T_DataArray once\n2536     # https://github.com/python/mypy/issues/12846 is resolved\n2537     def drop_vars(\n2538         self,\n2539         names: Hashable | Iterable[Hashable],\n2540         *,\n2541         errors: ErrorOptions = \"raise\",\n2542     ) -> DataArray:\n2543         \"\"\"Returns an array with dropped variables.\n2544 \n2545         Parameters\n2546         ----------\n2547         names : Hashable or iterable of Hashable\n2548             Name(s) of variables to drop.\n2549         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n2550             If 'raise', raises a ValueError error if any of the variable\n2551             passed are not in the dataset. If 'ignore', any given names that are in the\n2552             DataArray are dropped and no error is raised.\n2553 \n2554         Returns\n2555         -------\n2556         dropped : Dataset\n2557             New Dataset copied from `self` with variables removed.\n2558         \"\"\"\n2559         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n2560         return self._from_temp_dataset(ds)\n2561 \n2562     def drop(\n2563         self: T_DataArray,\n2564         labels: Mapping[Any, Any] | None = None,\n2565         dim: Hashable | None = None,\n2566         *,\n2567         errors: ErrorOptions = \"raise\",\n2568         **labels_kwargs,\n2569     ) -> T_DataArray:\n2570         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n2571 \n2572         Using either `drop_vars` or `drop_sel` is encouraged\n2573 \n2574         See Also\n2575         --------\n2576         DataArray.drop_vars\n2577         DataArray.drop_sel\n2578         \"\"\"\n2579         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)\n2580         return self._from_temp_dataset(ds)\n2581 \n2582     def drop_sel(\n2583         self: T_DataArray,\n2584         labels: Mapping[Any, Any] | None = None,\n2585         *,\n2586         errors: ErrorOptions = \"raise\",\n2587         **labels_kwargs,\n2588     ) -> T_DataArray:\n2589         \"\"\"Drop index labels from this DataArray.\n2590 \n2591         Parameters\n2592         ----------\n2593         labels : mapping of Hashable to Any\n2594             Index labels to drop\n2595         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n2596             If 'raise', raises a ValueError error if\n2597             any of the index labels passed are not\n2598             in the dataset. If 'ignore', any given labels that are in the\n2599             dataset are dropped and no error is raised.\n2600         **labels_kwargs : {dim: label, ...}, optional\n2601             The keyword arguments form of ``dim`` and ``labels``\n2602 \n2603         Returns\n2604         -------\n2605         dropped : DataArray\n2606         \"\"\"\n2607         if labels_kwargs or isinstance(labels, dict):\n2608             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n2609 \n2610         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n2611         return self._from_temp_dataset(ds)\n2612 \n2613     def drop_isel(\n2614         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs\n2615     ) -> T_DataArray:\n2616         \"\"\"Drop index positions from this DataArray.\n2617 \n2618         Parameters\n2619         ----------\n2620         indexers : mapping of Hashable to Any or None, default: None\n2621             Index locations to drop\n2622         **indexers_kwargs : {dim: position, ...}, optional\n2623             The keyword arguments form of ``dim`` and ``positions``\n2624 \n2625         Returns\n2626         -------\n2627         dropped : DataArray\n2628 \n2629         Raises\n2630         ------\n2631         IndexError\n2632         \"\"\"\n2633         dataset = self._to_temp_dataset()\n2634         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)\n2635         return self._from_temp_dataset(dataset)\n2636 \n2637     def dropna(\n2638         self: T_DataArray,\n2639         dim: Hashable,\n2640         how: Literal[\"any\", \"all\"] = \"any\",\n2641         thresh: int | None = None,\n2642     ) -> T_DataArray:\n2643         \"\"\"Returns a new array with dropped labels for missing values along\n2644         the provided dimension.\n2645 \n2646         Parameters\n2647         ----------\n2648         dim : Hashable\n2649             Dimension along which to drop missing values. Dropping along\n2650             multiple dimensions simultaneously is not yet supported.\n2651         how : {\"any\", \"all\"}, default: \"any\"\n2652             - any : if any NA values are present, drop that label\n2653             - all : if all values are NA, drop that label\n2654 \n2655         thresh : int or None, default: None\n2656             If supplied, require this many non-NA values.\n2657 \n2658         Returns\n2659         -------\n2660         dropped : DataArray\n2661         \"\"\"\n2662         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n2663         return self._from_temp_dataset(ds)\n2664 \n2665     def fillna(self: T_DataArray, value: Any) -> T_DataArray:\n2666         \"\"\"Fill missing values in this object.\n2667 \n2668         This operation follows the normal broadcasting and alignment rules that\n2669         xarray uses for binary arithmetic, except the result is aligned to this\n2670         object (``join='left'``) instead of aligned to the intersection of\n2671         index coordinates (``join='inner'``).\n2672 \n2673         Parameters\n2674         ----------\n2675         value : scalar, ndarray or DataArray\n2676             Used to fill all matching missing values in this array. If the\n2677             argument is a DataArray, it is first aligned with (reindexed to)\n2678             this array.\n2679 \n2680         Returns\n2681         -------\n2682         filled : DataArray\n2683         \"\"\"\n2684         if utils.is_dict_like(value):\n2685             raise TypeError(\n2686                 \"cannot provide fill value as a dictionary with \"\n2687                 \"fillna on a DataArray\"\n2688             )\n2689         out = ops.fillna(self, value)\n2690         return out\n2691 \n2692     def interpolate_na(\n2693         self: T_DataArray,\n2694         dim: Hashable | None = None,\n2695         method: InterpOptions = \"linear\",\n2696         limit: int | None = None,\n2697         use_coordinate: bool | str = True,\n2698         max_gap: (\n2699             None\n2700             | int\n2701             | float\n2702             | str\n2703             | pd.Timedelta\n2704             | np.timedelta64\n2705             | datetime.timedelta\n2706         ) = None,\n2707         keep_attrs: bool | None = None,\n2708         **kwargs: Any,\n2709     ) -> T_DataArray:\n2710         \"\"\"Fill in NaNs by interpolating according to different methods.\n2711 \n2712         Parameters\n2713         ----------\n2714         dim : Hashable or None, optional\n2715             Specifies the dimension along which to interpolate.\n2716         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n2717             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n2718             String indicating which method to use for interpolation:\n2719 \n2720             - 'linear': linear interpolation. Additional keyword\n2721               arguments are passed to :py:func:`numpy.interp`\n2722             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n2723               are passed to :py:func:`scipy.interpolate.interp1d`. If\n2724               ``method='polynomial'``, the ``order`` keyword argument must also be\n2725               provided.\n2726             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n2727               respective :py:class:`scipy.interpolate` classes.\n2728 \n2729         use_coordinate : bool or str, default: True\n2730             Specifies which index to use as the x values in the interpolation\n2731             formulated as `y = f(x)`. If False, values are treated as if\n2732             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is\n2733             used. If ``use_coordinate`` is a string, it specifies the name of a\n2734             coordinate variariable to use as the index.\n2735         limit : int or None, default: None\n2736             Maximum number of consecutive NaNs to fill. Must be greater than 0\n2737             or None for no limit. This filling is done regardless of the size of\n2738             the gap in the data. To only interpolate over gaps less than a given length,\n2739             see ``max_gap``.\n2740         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n2741             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n2742             Use None for no limit. When interpolating along a datetime64 dimension\n2743             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n2744 \n2745             - a string that is valid input for pandas.to_timedelta\n2746             - a :py:class:`numpy.timedelta64` object\n2747             - a :py:class:`pandas.Timedelta` object\n2748             - a :py:class:`datetime.timedelta` object\n2749 \n2750             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n2751             dimensions has not been implemented yet. Gap length is defined as the difference\n2752             between coordinate values at the first data point after a gap and the last value\n2753             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n2754             between coordinate values at the first (last) valid data point and the first (last) NaN.\n2755             For example, consider::\n2756 \n2757                 <xarray.DataArray (x: 9)>\n2758                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n2759                 Coordinates:\n2760                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n2761 \n2762             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n2763         keep_attrs : bool or None, default: None\n2764             If True, the dataarray's attributes (`attrs`) will be copied from\n2765             the original object to the new one.  If False, the new\n2766             object will be returned without attributes.\n2767         **kwargs : dict, optional\n2768             parameters passed verbatim to the underlying interpolation function\n2769 \n2770         Returns\n2771         -------\n2772         interpolated: DataArray\n2773             Filled in DataArray.\n2774 \n2775         See Also\n2776         --------\n2777         numpy.interp\n2778         scipy.interpolate\n2779 \n2780         Examples\n2781         --------\n2782         >>> da = xr.DataArray(\n2783         ...     [np.nan, 2, 3, np.nan, 0], dims=\"x\", coords={\"x\": [0, 1, 2, 3, 4]}\n2784         ... )\n2785         >>> da\n2786         <xarray.DataArray (x: 5)>\n2787         array([nan,  2.,  3., nan,  0.])\n2788         Coordinates:\n2789           * x        (x) int64 0 1 2 3 4\n2790 \n2791         >>> da.interpolate_na(dim=\"x\", method=\"linear\")\n2792         <xarray.DataArray (x: 5)>\n2793         array([nan, 2. , 3. , 1.5, 0. ])\n2794         Coordinates:\n2795           * x        (x) int64 0 1 2 3 4\n2796 \n2797         >>> da.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n2798         <xarray.DataArray (x: 5)>\n2799         array([1. , 2. , 3. , 1.5, 0. ])\n2800         Coordinates:\n2801           * x        (x) int64 0 1 2 3 4\n2802         \"\"\"\n2803         from .missing import interp_na\n2804 \n2805         return interp_na(\n2806             self,\n2807             dim=dim,\n2808             method=method,\n2809             limit=limit,\n2810             use_coordinate=use_coordinate,\n2811             max_gap=max_gap,\n2812             keep_attrs=keep_attrs,\n2813             **kwargs,\n2814         )\n2815 \n2816     def ffill(\n2817         self: T_DataArray, dim: Hashable, limit: int | None = None\n2818     ) -> T_DataArray:\n2819         \"\"\"Fill NaN values by propagating values forward\n2820 \n2821         *Requires bottleneck.*\n2822 \n2823         Parameters\n2824         ----------\n2825         dim : Hashable\n2826             Specifies the dimension along which to propagate values when\n2827             filling.\n2828         limit : int or None, default: None\n2829             The maximum number of consecutive NaN values to forward fill. In\n2830             other words, if there is a gap with more than this number of\n2831             consecutive NaNs, it will only be partially filled. Must be greater\n2832             than 0 or None for no limit. Must be None or greater than or equal\n2833             to axis length if filling along chunked axes (dimensions).\n2834 \n2835         Returns\n2836         -------\n2837         filled : DataArray\n2838         \"\"\"\n2839         from .missing import ffill\n2840 \n2841         return ffill(self, dim, limit=limit)\n2842 \n2843     def bfill(\n2844         self: T_DataArray, dim: Hashable, limit: int | None = None\n2845     ) -> T_DataArray:\n2846         \"\"\"Fill NaN values by propagating values backward\n2847 \n2848         *Requires bottleneck.*\n2849 \n2850         Parameters\n2851         ----------\n2852         dim : str\n2853             Specifies the dimension along which to propagate values when\n2854             filling.\n2855         limit : int or None, default: None\n2856             The maximum number of consecutive NaN values to backward fill. In\n2857             other words, if there is a gap with more than this number of\n2858             consecutive NaNs, it will only be partially filled. Must be greater\n2859             than 0 or None for no limit. Must be None or greater than or equal\n2860             to axis length if filling along chunked axes (dimensions).\n2861 \n2862         Returns\n2863         -------\n2864         filled : DataArray\n2865         \"\"\"\n2866         from .missing import bfill\n2867 \n2868         return bfill(self, dim, limit=limit)\n2869 \n2870     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n2871         \"\"\"Combine two DataArray objects, with union of coordinates.\n2872 \n2873         This operation follows the normal broadcasting and alignment rules of\n2874         ``join='outer'``.  Default to non-null values of array calling the\n2875         method.  Use np.nan to fill in vacant cells after alignment.\n2876 \n2877         Parameters\n2878         ----------\n2879         other : DataArray\n2880             Used to fill all matching missing values in this array.\n2881 \n2882         Returns\n2883         -------\n2884         DataArray\n2885         \"\"\"\n2886         return ops.fillna(self, other, join=\"outer\")\n2887 \n2888     def reduce(\n2889         self: T_DataArray,\n2890         func: Callable[..., Any],\n2891         dim: None | Hashable | Iterable[Hashable] = None,\n2892         *,\n2893         axis: None | int | Sequence[int] = None,\n2894         keep_attrs: bool | None = None,\n2895         keepdims: bool = False,\n2896         **kwargs: Any,\n2897     ) -> T_DataArray:\n2898         \"\"\"Reduce this array by applying `func` along some dimension(s).\n2899 \n2900         Parameters\n2901         ----------\n2902         func : callable\n2903             Function which can be called in the form\n2904             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n2905             np.ndarray over an integer valued axis.\n2906         dim : Hashable or Iterable of Hashable, optional\n2907             Dimension(s) over which to apply `func`.\n2908         axis : int or sequence of int, optional\n2909             Axis(es) over which to repeatedly apply `func`. Only one of the\n2910             'dim' and 'axis' arguments can be supplied. If neither are\n2911             supplied, then the reduction is calculated over the flattened array\n2912             (by calling `f(x)` without an axis argument).\n2913         keep_attrs : bool or None, optional\n2914             If True, the variable's attributes (`attrs`) will be copied from\n2915             the original object to the new one.  If False (default), the new\n2916             object will be returned without attributes.\n2917         keepdims : bool, default: False\n2918             If True, the dimensions which are reduced are left in the result\n2919             as dimensions of size one. Coordinates that use these dimensions\n2920             are removed.\n2921         **kwargs : dict\n2922             Additional keyword arguments passed on to `func`.\n2923 \n2924         Returns\n2925         -------\n2926         reduced : DataArray\n2927             DataArray with this object's array replaced with an array with\n2928             summarized data and the indicated dimension(s) removed.\n2929         \"\"\"\n2930 \n2931         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n2932         return self._replace_maybe_drop_dims(var)\n2933 \n2934     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:\n2935         \"\"\"Convert this array into a pandas object with the same shape.\n2936 \n2937         The type of the returned object depends on the number of DataArray\n2938         dimensions:\n2939 \n2940         * 0D -> `xarray.DataArray`\n2941         * 1D -> `pandas.Series`\n2942         * 2D -> `pandas.DataFrame`\n2943 \n2944         Only works for arrays with 2 or fewer dimensions.\n2945 \n2946         The DataArray constructor performs the inverse transformation.\n2947 \n2948         Returns\n2949         -------\n2950         result : DataArray | Series | DataFrame\n2951             DataArray, pandas Series or pandas DataFrame.\n2952         \"\"\"\n2953         # TODO: consolidate the info about pandas constructors and the\n2954         # attributes that correspond to their indexes into a separate module?\n2955         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n2956         try:\n2957             constructor = constructors[self.ndim]\n2958         except KeyError:\n2959             raise ValueError(\n2960                 f\"Cannot convert arrays with {self.ndim} dimensions into \"\n2961                 \"pandas objects. Requires 2 or fewer dimensions.\"\n2962             )\n2963         indexes = [self.get_index(dim) for dim in self.dims]\n2964         return constructor(self.values, *indexes)\n2965 \n2966     def to_dataframe(\n2967         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None\n2968     ) -> pd.DataFrame:\n2969         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n2970 \n2971         The DataFrame is indexed by the Cartesian product of index coordinates\n2972         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are\n2973         included as columns in the DataFrame.\n2974 \n2975         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which\n2976         doesn't rely on a MultiIndex to build the DataFrame.\n2977 \n2978         Parameters\n2979         ----------\n2980         name: Hashable or None, optional\n2981             Name to give to this array (required if unnamed).\n2982         dim_order: Sequence of Hashable or None, optional\n2983             Hierarchical dimension order for the resulting dataframe.\n2984             Array content is transposed to this order and then written out as flat\n2985             vectors in contiguous order, so the last dimension in this list\n2986             will be contiguous in the resulting DataFrame. This has a major\n2987             influence on which operations are efficient on the resulting\n2988             dataframe.\n2989 \n2990             If provided, must include all dimensions of this DataArray. By default,\n2991             dimensions are sorted according to the DataArray dimensions order.\n2992 \n2993         Returns\n2994         -------\n2995         result: DataFrame\n2996             DataArray as a pandas DataFrame.\n2997 \n2998         See also\n2999         --------\n3000         DataArray.to_pandas\n3001         DataArray.to_series\n3002         \"\"\"\n3003         if name is None:\n3004             name = self.name\n3005         if name is None:\n3006             raise ValueError(\n3007                 \"cannot convert an unnamed DataArray to a \"\n3008                 \"DataFrame: use the ``name`` parameter\"\n3009             )\n3010         if self.ndim == 0:\n3011             raise ValueError(\"cannot convert a scalar to a DataFrame\")\n3012 \n3013         # By using a unique name, we can convert a DataArray into a DataFrame\n3014         # even if it shares a name with one of its coordinates.\n3015         # I would normally use unique_name = object() but that results in a\n3016         # dataframe with columns in the wrong order, for reasons I have not\n3017         # been able to debug (possibly a pandas bug?).\n3018         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n3019         ds = self._to_dataset_whole(name=unique_name)\n3020 \n3021         if dim_order is None:\n3022             ordered_dims = dict(zip(self.dims, self.shape))\n3023         else:\n3024             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)\n3025 \n3026         df = ds._to_dataframe(ordered_dims)\n3027         df.columns = [name if c == unique_name else c for c in df.columns]\n3028         return df\n3029 \n3030     def to_series(self) -> pd.Series:\n3031         \"\"\"Convert this array into a pandas.Series.\n3032 \n3033         The Series is indexed by the Cartesian product of index coordinates\n3034         (in the form of a :py:class:`pandas.MultiIndex`).\n3035 \n3036         Returns\n3037         -------\n3038         result : Series\n3039             DataArray as a pandas Series.\n3040 \n3041         See also\n3042         --------\n3043         DataArray.to_pandas\n3044         DataArray.to_dataframe\n3045         \"\"\"\n3046         index = self.coords.to_index()\n3047         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n3048 \n3049     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n3050         \"\"\"Convert this array into a numpy.ma.MaskedArray\n3051 \n3052         Parameters\n3053         ----------\n3054         copy : bool, default: True\n3055             If True make a copy of the array in the result. If False,\n3056             a MaskedArray view of DataArray.values is returned.\n3057 \n3058         Returns\n3059         -------\n3060         result : MaskedArray\n3061             Masked where invalid values (nan or inf) occur.\n3062         \"\"\"\n3063         values = self.to_numpy()  # only compute lazy arrays once\n3064         isnull = pd.isnull(values)\n3065         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n3066 \n3067     # path=None writes to bytes\n3068     @overload\n3069     def to_netcdf(\n3070         self,\n3071         path: None = None,\n3072         mode: Literal[\"w\", \"a\"] = \"w\",\n3073         format: T_NetcdfTypes | None = None,\n3074         group: str | None = None,\n3075         engine: T_NetcdfEngine | None = None,\n3076         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3077         unlimited_dims: Iterable[Hashable] | None = None,\n3078         compute: bool = True,\n3079         invalid_netcdf: bool = False,\n3080     ) -> bytes:\n3081         ...\n3082 \n3083     # default return None\n3084     @overload\n3085     def to_netcdf(\n3086         self,\n3087         path: str | PathLike,\n3088         mode: Literal[\"w\", \"a\"] = \"w\",\n3089         format: T_NetcdfTypes | None = None,\n3090         group: str | None = None,\n3091         engine: T_NetcdfEngine | None = None,\n3092         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3093         unlimited_dims: Iterable[Hashable] | None = None,\n3094         compute: Literal[True] = True,\n3095         invalid_netcdf: bool = False,\n3096     ) -> None:\n3097         ...\n3098 \n3099     # compute=False returns dask.Delayed\n3100     @overload\n3101     def to_netcdf(\n3102         self,\n3103         path: str | PathLike,\n3104         mode: Literal[\"w\", \"a\"] = \"w\",\n3105         format: T_NetcdfTypes | None = None,\n3106         group: str | None = None,\n3107         engine: T_NetcdfEngine | None = None,\n3108         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3109         unlimited_dims: Iterable[Hashable] | None = None,\n3110         *,\n3111         compute: Literal[False],\n3112         invalid_netcdf: bool = False,\n3113     ) -> Delayed:\n3114         ...\n3115 \n3116     def to_netcdf(\n3117         self,\n3118         path: str | PathLike | None = None,\n3119         mode: Literal[\"w\", \"a\"] = \"w\",\n3120         format: T_NetcdfTypes | None = None,\n3121         group: str | None = None,\n3122         engine: T_NetcdfEngine | None = None,\n3123         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3124         unlimited_dims: Iterable[Hashable] | None = None,\n3125         compute: bool = True,\n3126         invalid_netcdf: bool = False,\n3127     ) -> bytes | Delayed | None:\n3128         \"\"\"Write dataset contents to a netCDF file.\n3129 \n3130         Parameters\n3131         ----------\n3132         path : str, path-like or None, optional\n3133             Path to which to save this dataset. File-like objects are only\n3134             supported by the scipy engine. If no path is provided, this\n3135             function returns the resulting netCDF file as bytes; in this case,\n3136             we need to use scipy, which does not support netCDF version 4 (the\n3137             default format becomes NETCDF3_64BIT).\n3138         mode : {\"w\", \"a\"}, default: \"w\"\n3139             Write ('w') or append ('a') mode. If mode='w', any existing file at\n3140             this location will be overwritten. If mode='a', existing variables\n3141             will be overwritten.\n3142         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n3143                   \"NETCDF3_CLASSIC\"}, optional\n3144             File format for the resulting netCDF file:\n3145 \n3146             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n3147               features.\n3148             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n3149               netCDF 3 compatible API features.\n3150             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n3151               which fully supports 2+ GB files, but is only compatible with\n3152               clients linked against netCDF version 3.6.0 or later.\n3153             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n3154               handle 2+ GB files very well.\n3155 \n3156             All formats are supported by the netCDF4-python library.\n3157             scipy.io.netcdf only supports the last two formats.\n3158 \n3159             The default format is NETCDF4 if you are saving a file to disk and\n3160             have the netCDF4-python library available. Otherwise, xarray falls\n3161             back to using scipy to write netCDF files and defaults to the\n3162             NETCDF3_64BIT format (scipy does not support netCDF4).\n3163         group : str, optional\n3164             Path to the netCDF4 group in the given file to open (only works for\n3165             format='NETCDF4'). The group(s) will be created if necessary.\n3166         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n3167             Engine to use when writing netCDF files. If not provided, the\n3168             default engine is chosen based on available dependencies, with a\n3169             preference for 'netcdf4' if writing to a file on disk.\n3170         encoding : dict, optional\n3171             Nested dictionary with variable names as keys and dictionaries of\n3172             variable specific encodings as values, e.g.,\n3173             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n3174             \"zlib\": True}, ...}``\n3175 \n3176             The `h5netcdf` engine supports both the NetCDF4-style compression\n3177             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n3178             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n3179             This allows using any compression plugin installed in the HDF5\n3180             library, e.g. LZF.\n3181 \n3182         unlimited_dims : iterable of Hashable, optional\n3183             Dimension(s) that should be serialized as unlimited dimensions.\n3184             By default, no dimensions are treated as unlimited dimensions.\n3185             Note that unlimited_dims may also be set via\n3186             ``dataset.encoding[\"unlimited_dims\"]``.\n3187         compute: bool, default: True\n3188             If true compute immediately, otherwise return a\n3189             ``dask.delayed.Delayed`` object that can be computed later.\n3190         invalid_netcdf: bool, default: False\n3191             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n3192             hdf5 files which are invalid netcdf as described in\n3193             https://github.com/h5netcdf/h5netcdf.\n3194 \n3195         Returns\n3196         -------\n3197         store: bytes or Delayed or None\n3198             * ``bytes`` if path is None\n3199             * ``dask.delayed.Delayed`` if compute is False\n3200             * None otherwise\n3201 \n3202         Notes\n3203         -----\n3204         Only xarray.Dataset objects can be written to netCDF files, so\n3205         the xarray.DataArray is converted to a xarray.Dataset object\n3206         containing a single variable. If the DataArray has no name, or if the\n3207         name is the same as a coordinate name, then it is given the name\n3208         ``\"__xarray_dataarray_variable__\"``.\n3209 \n3210         See Also\n3211         --------\n3212         Dataset.to_netcdf\n3213         \"\"\"\n3214         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf\n3215 \n3216         if self.name is None:\n3217             # If no name is set then use a generic xarray name\n3218             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3219         elif self.name in self.coords or self.name in self.dims:\n3220             # The name is the same as one of the coords names, which netCDF\n3221             # doesn't support, so rename it but keep track of the old name\n3222             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3223             dataset.attrs[DATAARRAY_NAME] = self.name\n3224         else:\n3225             # No problems with the name - so we're fine!\n3226             dataset = self.to_dataset()\n3227 \n3228         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n3229             dataset,\n3230             path,\n3231             mode=mode,\n3232             format=format,\n3233             group=group,\n3234             engine=engine,\n3235             encoding=encoding,\n3236             unlimited_dims=unlimited_dims,\n3237             compute=compute,\n3238             multifile=False,\n3239             invalid_netcdf=invalid_netcdf,\n3240         )\n3241 \n3242     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:\n3243         \"\"\"\n3244         Convert this xarray.DataArray into a dictionary following xarray\n3245         naming conventions.\n3246 \n3247         Converts all variables and attributes to native Python objects.\n3248         Useful for converting to json. To avoid datetime incompatibility\n3249         use decode_times=False kwarg in xarray.open_dataset.\n3250 \n3251         Parameters\n3252         ----------\n3253         data : bool, default: True\n3254             Whether to include the actual data in the dictionary. When set to\n3255             False, returns just the schema.\n3256         encoding : bool, default: False\n3257             Whether to include the Dataset's encoding in the dictionary.\n3258 \n3259         Returns\n3260         -------\n3261         dict: dict\n3262 \n3263         See Also\n3264         --------\n3265         DataArray.from_dict\n3266         Dataset.to_dict\n3267         \"\"\"\n3268         d = self.variable.to_dict(data=data)\n3269         d.update({\"coords\": {}, \"name\": self.name})\n3270         for k in self.coords:\n3271             d[\"coords\"][k] = self.coords[k].variable.to_dict(data=data)\n3272         if encoding:\n3273             d[\"encoding\"] = dict(self.encoding)\n3274         return d\n3275 \n3276     @classmethod\n3277     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:\n3278         \"\"\"Convert a dictionary into an xarray.DataArray\n3279 \n3280         Parameters\n3281         ----------\n3282         d : dict\n3283             Mapping with a minimum structure of {\"dims\": [...], \"data\": [...]}\n3284 \n3285         Returns\n3286         -------\n3287         obj : xarray.DataArray\n3288 \n3289         See Also\n3290         --------\n3291         DataArray.to_dict\n3292         Dataset.from_dict\n3293 \n3294         Examples\n3295         --------\n3296         >>> d = {\"dims\": \"t\", \"data\": [1, 2, 3]}\n3297         >>> da = xr.DataArray.from_dict(d)\n3298         >>> da\n3299         <xarray.DataArray (t: 3)>\n3300         array([1, 2, 3])\n3301         Dimensions without coordinates: t\n3302 \n3303         >>> d = {\n3304         ...     \"coords\": {\n3305         ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n3306         ...     },\n3307         ...     \"attrs\": {\"title\": \"air temperature\"},\n3308         ...     \"dims\": \"t\",\n3309         ...     \"data\": [10, 20, 30],\n3310         ...     \"name\": \"a\",\n3311         ... }\n3312         >>> da = xr.DataArray.from_dict(d)\n3313         >>> da\n3314         <xarray.DataArray 'a' (t: 3)>\n3315         array([10, 20, 30])\n3316         Coordinates:\n3317           * t        (t) int64 0 1 2\n3318         Attributes:\n3319             title:    air temperature\n3320         \"\"\"\n3321         coords = None\n3322         if \"coords\" in d:\n3323             try:\n3324                 coords = {\n3325                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n3326                     for k, v in d[\"coords\"].items()\n3327                 }\n3328             except KeyError as e:\n3329                 raise ValueError(\n3330                     \"cannot convert dict when coords are missing the key \"\n3331                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n3332                 )\n3333         try:\n3334             data = d[\"data\"]\n3335         except KeyError:\n3336             raise ValueError(\"cannot convert dict without the key 'data''\")\n3337         else:\n3338             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n3339 \n3340         obj.encoding.update(d.get(\"encoding\", {}))\n3341 \n3342         return obj\n3343 \n3344     @classmethod\n3345     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:\n3346         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n3347 \n3348         If the series's index is a MultiIndex, it will be expanded into a\n3349         tensor product of one-dimensional coordinates (filling in missing\n3350         values with NaN). Thus this operation should be the inverse of the\n3351         `to_series` method.\n3352 \n3353         Parameters\n3354         ----------\n3355         series : Series\n3356             Pandas Series object to convert.\n3357         sparse : bool, default: False\n3358             If sparse=True, creates a sparse array instead of a dense NumPy array.\n3359             Requires the pydata/sparse package.\n3360 \n3361         See Also\n3362         --------\n3363         DataArray.to_series\n3364         Dataset.from_dataframe\n3365         \"\"\"\n3366         temp_name = \"__temporary_name\"\n3367         df = pd.DataFrame({temp_name: series})\n3368         ds = Dataset.from_dataframe(df, sparse=sparse)\n3369         result = cast(DataArray, ds[temp_name])\n3370         result.name = series.name\n3371         return result\n3372 \n3373     def to_cdms2(self) -> cdms2_Variable:\n3374         \"\"\"Convert this array into a cdms2.Variable\"\"\"\n3375         from ..convert import to_cdms2\n3376 \n3377         return to_cdms2(self)\n3378 \n3379     @classmethod\n3380     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:\n3381         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\"\"\"\n3382         from ..convert import from_cdms2\n3383 \n3384         return from_cdms2(variable)\n3385 \n3386     def to_iris(self) -> iris_Cube:\n3387         \"\"\"Convert this array into a iris.cube.Cube\"\"\"\n3388         from ..convert import to_iris\n3389 \n3390         return to_iris(self)\n3391 \n3392     @classmethod\n3393     def from_iris(cls, cube: iris_Cube) -> DataArray:\n3394         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\"\"\"\n3395         from ..convert import from_iris\n3396 \n3397         return from_iris(cube)\n3398 \n3399     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:\n3400         \"\"\"Helper function for equals, broadcast_equals, and identical\"\"\"\n3401 \n3402         def compat(x, y):\n3403             return getattr(x.variable, compat_str)(y.variable)\n3404 \n3405         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n3406             self, other\n3407         )\n3408 \n3409     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:\n3410         \"\"\"Two DataArrays are broadcast equal if they are equal after\n3411         broadcasting them against each other such that they have the same\n3412         dimensions.\n3413 \n3414         Parameters\n3415         ----------\n3416         other : DataArray\n3417             DataArray to compare to.\n3418 \n3419         Returns\n3420         ----------\n3421         equal : bool\n3422             True if the two DataArrays are broadcast equal.\n3423 \n3424         See Also\n3425         --------\n3426         DataArray.equals\n3427         DataArray.identical\n3428         \"\"\"\n3429         try:\n3430             return self._all_compat(other, \"broadcast_equals\")\n3431         except (TypeError, AttributeError):\n3432             return False\n3433 \n3434     def equals(self: T_DataArray, other: T_DataArray) -> bool:\n3435         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n3436         values; otherwise False.\n3437 \n3438         DataArrays can still be equal (like pandas objects) if they have NaN\n3439         values in the same locations.\n3440 \n3441         This method is necessary because `v1 == v2` for ``DataArray``\n3442         does element-wise comparisons (like numpy.ndarrays).\n3443 \n3444         Parameters\n3445         ----------\n3446         other : DataArray\n3447             DataArray to compare to.\n3448 \n3449         Returns\n3450         ----------\n3451         equal : bool\n3452             True if the two DataArrays are equal.\n3453 \n3454         See Also\n3455         --------\n3456         DataArray.broadcast_equals\n3457         DataArray.identical\n3458         \"\"\"\n3459         try:\n3460             return self._all_compat(other, \"equals\")\n3461         except (TypeError, AttributeError):\n3462             return False\n3463 \n3464     def identical(self: T_DataArray, other: T_DataArray) -> bool:\n3465         \"\"\"Like equals, but also checks the array name and attributes, and\n3466         attributes on all coordinates.\n3467 \n3468         Parameters\n3469         ----------\n3470         other : DataArray\n3471             DataArray to compare to.\n3472 \n3473         Returns\n3474         ----------\n3475         equal : bool\n3476             True if the two DataArrays are identical.\n3477 \n3478         See Also\n3479         --------\n3480         DataArray.broadcast_equals\n3481         DataArray.equals\n3482         \"\"\"\n3483         try:\n3484             return self.name == other.name and self._all_compat(other, \"identical\")\n3485         except (TypeError, AttributeError):\n3486             return False\n3487 \n3488     def _result_name(self, other: Any = None) -> Hashable | None:\n3489         # use the same naming heuristics as pandas:\n3490         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n3491         other_name = getattr(other, \"name\", _default)\n3492         if other_name is _default or other_name == self.name:\n3493             return self.name\n3494         else:\n3495             return None\n3496 \n3497     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:\n3498         new_var = self.variable.__array_wrap__(obj, context)\n3499         return self._replace(new_var)\n3500 \n3501     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:\n3502         return self.dot(obj)\n3503 \n3504     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n3505         # currently somewhat duplicative, as only other DataArrays are\n3506         # compatible with matmul\n3507         return computation.dot(other, self)\n3508 \n3509     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:\n3510         keep_attrs = kwargs.pop(\"keep_attrs\", None)\n3511         if keep_attrs is None:\n3512             keep_attrs = _get_keep_attrs(default=True)\n3513         with warnings.catch_warnings():\n3514             warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n3515             warnings.filterwarnings(\n3516                 \"ignore\", r\"Mean of empty slice\", category=RuntimeWarning\n3517             )\n3518             with np.errstate(all=\"ignore\"):\n3519                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n3520             if keep_attrs:\n3521                 da.attrs = self.attrs\n3522             return da\n3523 \n3524     def _binary_op(\n3525         self: T_DataArray,\n3526         other: Any,\n3527         f: Callable,\n3528         reflexive: bool = False,\n3529     ) -> T_DataArray:\n3530         from .groupby import GroupBy\n3531 \n3532         if isinstance(other, (Dataset, GroupBy)):\n3533             return NotImplemented\n3534         if isinstance(other, DataArray):\n3535             align_type = OPTIONS[\"arithmetic_join\"]\n3536             self, other = align(self, other, join=align_type, copy=False)  # type: ignore\n3537         other_variable = getattr(other, \"variable\", other)\n3538         other_coords = getattr(other, \"coords\", None)\n3539 \n3540         variable = (\n3541             f(self.variable, other_variable)\n3542             if not reflexive\n3543             else f(other_variable, self.variable)\n3544         )\n3545         coords, indexes = self.coords._merge_raw(other_coords, reflexive)\n3546         name = self._result_name(other)\n3547 \n3548         return self._replace(variable, coords, name, indexes=indexes)\n3549 \n3550     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:\n3551         from .groupby import GroupBy\n3552 \n3553         if isinstance(other, GroupBy):\n3554             raise TypeError(\n3555                 \"in-place operations between a DataArray and \"\n3556                 \"a grouped object are not permitted\"\n3557             )\n3558         # n.b. we can't align other to self (with other.reindex_like(self))\n3559         # because `other` may be converted into floats, which would cause\n3560         # in-place arithmetic to fail unpredictably. Instead, we simply\n3561         # don't support automatic alignment with in-place arithmetic.\n3562         other_coords = getattr(other, \"coords\", None)\n3563         other_variable = getattr(other, \"variable\", other)\n3564         try:\n3565             with self.coords._merge_inplace(other_coords):\n3566                 f(self.variable, other_variable)\n3567         except MergeError as exc:\n3568             raise MergeError(\n3569                 \"Automatic alignment is not supported for in-place operations.\\n\"\n3570                 \"Consider aligning the indices manually or using a not-in-place operation.\\n\"\n3571                 \"See https://github.com/pydata/xarray/issues/3910 for more explanations.\"\n3572             ) from exc\n3573         return self\n3574 \n3575     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:\n3576         self.attrs = other.attrs\n3577 \n3578     plot = utils.UncachedAccessor(_PlotMethods)\n3579 \n3580     def _title_for_slice(self, truncate: int = 50) -> str:\n3581         \"\"\"\n3582         If the dataarray has 1 dimensional coordinates or comes from a slice\n3583         we can show that info in the title\n3584 \n3585         Parameters\n3586         ----------\n3587         truncate : int, default: 50\n3588             maximum number of characters for title\n3589 \n3590         Returns\n3591         -------\n3592         title : string\n3593             Can be used for plot titles\n3594 \n3595         \"\"\"\n3596         one_dims = []\n3597         for dim, coord in self.coords.items():\n3598             if coord.size == 1:\n3599                 one_dims.append(\n3600                     \"{dim} = {v}{unit}\".format(\n3601                         dim=dim,\n3602                         v=format_item(coord.values),\n3603                         unit=_get_units_from_attrs(coord),\n3604                     )\n3605                 )\n3606 \n3607         title = \", \".join(one_dims)\n3608         if len(title) > truncate:\n3609             title = title[: (truncate - 3)] + \"...\"\n3610 \n3611         return title\n3612 \n3613     def diff(\n3614         self: T_DataArray,\n3615         dim: Hashable,\n3616         n: int = 1,\n3617         label: Literal[\"upper\", \"lower\"] = \"upper\",\n3618     ) -> T_DataArray:\n3619         \"\"\"Calculate the n-th order discrete difference along given axis.\n3620 \n3621         Parameters\n3622         ----------\n3623         dim : Hashable\n3624             Dimension over which to calculate the finite difference.\n3625         n : int, default: 1\n3626             The number of times values are differenced.\n3627         label : {\"upper\", \"lower\"}, default: \"upper\"\n3628             The new coordinate in dimension ``dim`` will have the\n3629             values of either the minuend's or subtrahend's coordinate\n3630             for values 'upper' and 'lower', respectively.\n3631 \n3632         Returns\n3633         -------\n3634         difference : DataArray\n3635             The n-th order finite difference of this object.\n3636 \n3637         Notes\n3638         -----\n3639         `n` matches numpy's behavior and is different from pandas' first argument named\n3640         `periods`.\n3641 \n3642         Examples\n3643         --------\n3644         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n3645         >>> arr.diff(\"x\")\n3646         <xarray.DataArray (x: 3)>\n3647         array([0, 1, 0])\n3648         Coordinates:\n3649           * x        (x) int64 2 3 4\n3650         >>> arr.diff(\"x\", 2)\n3651         <xarray.DataArray (x: 2)>\n3652         array([ 1, -1])\n3653         Coordinates:\n3654           * x        (x) int64 3 4\n3655 \n3656         See Also\n3657         --------\n3658         DataArray.differentiate\n3659         \"\"\"\n3660         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n3661         return self._from_temp_dataset(ds)\n3662 \n3663     def shift(\n3664         self: T_DataArray,\n3665         shifts: Mapping[Any, int] | None = None,\n3666         fill_value: Any = dtypes.NA,\n3667         **shifts_kwargs: int,\n3668     ) -> T_DataArray:\n3669         \"\"\"Shift this DataArray by an offset along one or more dimensions.\n3670 \n3671         Only the data is moved; coordinates stay in place. This is consistent\n3672         with the behavior of ``shift`` in pandas.\n3673 \n3674         Values shifted from beyond array bounds will appear at one end of\n3675         each dimension, which are filled according to `fill_value`. For periodic\n3676         offsets instead see `roll`.\n3677 \n3678         Parameters\n3679         ----------\n3680         shifts : mapping of Hashable to int or None, optional\n3681             Integer offset to shift along each of the given dimensions.\n3682             Positive offsets shift to the right; negative offsets shift to the\n3683             left.\n3684         fill_value : scalar, optional\n3685             Value to use for newly missing values\n3686         **shifts_kwargs\n3687             The keyword arguments form of ``shifts``.\n3688             One of shifts or shifts_kwargs must be provided.\n3689 \n3690         Returns\n3691         -------\n3692         shifted : DataArray\n3693             DataArray with the same coordinates and attributes but shifted\n3694             data.\n3695 \n3696         See Also\n3697         --------\n3698         roll\n3699 \n3700         Examples\n3701         --------\n3702         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3703         >>> arr.shift(x=1)\n3704         <xarray.DataArray (x: 3)>\n3705         array([nan,  5.,  6.])\n3706         Dimensions without coordinates: x\n3707         \"\"\"\n3708         variable = self.variable.shift(\n3709             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n3710         )\n3711         return self._replace(variable=variable)\n3712 \n3713     def roll(\n3714         self: T_DataArray,\n3715         shifts: Mapping[Hashable, int] | None = None,\n3716         roll_coords: bool = False,\n3717         **shifts_kwargs: int,\n3718     ) -> T_DataArray:\n3719         \"\"\"Roll this array by an offset along one or more dimensions.\n3720 \n3721         Unlike shift, roll treats the given dimensions as periodic, so will not\n3722         create any missing values to be filled.\n3723 \n3724         Unlike shift, roll may rotate all variables, including coordinates\n3725         if specified. The direction of rotation is consistent with\n3726         :py:func:`numpy.roll`.\n3727 \n3728         Parameters\n3729         ----------\n3730         shifts : mapping of Hashable to int, optional\n3731             Integer offset to rotate each of the given dimensions.\n3732             Positive offsets roll to the right; negative offsets roll to the\n3733             left.\n3734         roll_coords : bool, default: False\n3735             Indicates whether to roll the coordinates by the offset too.\n3736         **shifts_kwargs : {dim: offset, ...}, optional\n3737             The keyword arguments form of ``shifts``.\n3738             One of shifts or shifts_kwargs must be provided.\n3739 \n3740         Returns\n3741         -------\n3742         rolled : DataArray\n3743             DataArray with the same attributes but rolled data and coordinates.\n3744 \n3745         See Also\n3746         --------\n3747         shift\n3748 \n3749         Examples\n3750         --------\n3751         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n3752         >>> arr.roll(x=1)\n3753         <xarray.DataArray (x: 3)>\n3754         array([7, 5, 6])\n3755         Dimensions without coordinates: x\n3756         \"\"\"\n3757         ds = self._to_temp_dataset().roll(\n3758             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n3759         )\n3760         return self._from_temp_dataset(ds)\n3761 \n3762     @property\n3763     def real(self: T_DataArray) -> T_DataArray:\n3764         return self._replace(self.variable.real)\n3765 \n3766     @property\n3767     def imag(self: T_DataArray) -> T_DataArray:\n3768         return self._replace(self.variable.imag)\n3769 \n3770     def dot(\n3771         self: T_DataArray,\n3772         other: T_DataArray,\n3773         dims: str | Iterable[Hashable] | Ellipsis | None = None,\n3774     ) -> T_DataArray:\n3775         \"\"\"Perform dot product of two DataArrays along their shared dims.\n3776 \n3777         Equivalent to taking taking tensordot over all shared dims.\n3778 \n3779         Parameters\n3780         ----------\n3781         other : DataArray\n3782             The other array with which the dot product is performed.\n3783         dims : ..., str or Iterable of Hashable, optional\n3784             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.\n3785             If not specified, then all the common dimensions are summed over.\n3786 \n3787         Returns\n3788         -------\n3789         result : DataArray\n3790             Array resulting from the dot product over all shared dimensions.\n3791 \n3792         See Also\n3793         --------\n3794         dot\n3795         numpy.tensordot\n3796 \n3797         Examples\n3798         --------\n3799         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n3800         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n3801         >>> dm_vals = np.arange(4)\n3802         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n3803 \n3804         >>> dm.dims\n3805         ('z',)\n3806 \n3807         >>> da.dims\n3808         ('x', 'y', 'z')\n3809 \n3810         >>> dot_result = da.dot(dm)\n3811         >>> dot_result.dims\n3812         ('x', 'y')\n3813 \n3814         \"\"\"\n3815         if isinstance(other, Dataset):\n3816             raise NotImplementedError(\n3817                 \"dot products are not yet supported with Dataset objects.\"\n3818             )\n3819         if not isinstance(other, DataArray):\n3820             raise TypeError(\"dot only operates on DataArrays.\")\n3821 \n3822         return computation.dot(self, other, dims=dims)\n3823 \n3824     # change type of self and return to T_DataArray once\n3825     # https://github.com/python/mypy/issues/12846 is resolved\n3826     def sortby(\n3827         self,\n3828         variables: Hashable | DataArray | Sequence[Hashable | DataArray],\n3829         ascending: bool = True,\n3830     ) -> DataArray:\n3831         \"\"\"Sort object by labels or values (along an axis).\n3832 \n3833         Sorts the dataarray, either along specified dimensions,\n3834         or according to values of 1-D dataarrays that share dimension\n3835         with calling object.\n3836 \n3837         If the input variables are dataarrays, then the dataarrays are aligned\n3838         (via left-join) to the calling object prior to sorting by cell values.\n3839         NaNs are sorted to the end, following Numpy convention.\n3840 \n3841         If multiple sorts along the same dimension is\n3842         given, numpy's lexsort is performed along that dimension:\n3843         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n3844         and the FIRST key in the sequence is used as the primary sort key,\n3845         followed by the 2nd key, etc.\n3846 \n3847         Parameters\n3848         ----------\n3849         variables : Hashable, DataArray, or sequence of Hashable or DataArray\n3850             1D DataArray objects or name(s) of 1D variable(s) in\n3851             coords whose values are used to sort this array.\n3852         ascending : bool, default: True\n3853             Whether to sort by ascending or descending order.\n3854 \n3855         Returns\n3856         -------\n3857         sorted : DataArray\n3858             A new dataarray where all the specified dims are sorted by dim\n3859             labels.\n3860 \n3861         See Also\n3862         --------\n3863         Dataset.sortby\n3864         numpy.sort\n3865         pandas.sort_values\n3866         pandas.sort_index\n3867 \n3868         Examples\n3869         --------\n3870         >>> da = xr.DataArray(\n3871         ...     np.random.rand(5),\n3872         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n3873         ...     dims=\"time\",\n3874         ... )\n3875         >>> da\n3876         <xarray.DataArray (time: 5)>\n3877         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])\n3878         Coordinates:\n3879           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05\n3880 \n3881         >>> da.sortby(da)\n3882         <xarray.DataArray (time: 5)>\n3883         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])\n3884         Coordinates:\n3885           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02\n3886         \"\"\"\n3887         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n3888         return self._from_temp_dataset(ds)\n3889 \n3890     def quantile(\n3891         self: T_DataArray,\n3892         q: ArrayLike,\n3893         dim: str | Iterable[Hashable] | None = None,\n3894         method: QUANTILE_METHODS = \"linear\",\n3895         keep_attrs: bool | None = None,\n3896         skipna: bool | None = None,\n3897         interpolation: QUANTILE_METHODS = None,\n3898     ) -> T_DataArray:\n3899         \"\"\"Compute the qth quantile of the data along the specified dimension.\n3900 \n3901         Returns the qth quantiles(s) of the array elements.\n3902 \n3903         Parameters\n3904         ----------\n3905         q : float or array-like of float\n3906             Quantile to compute, which must be between 0 and 1 inclusive.\n3907         dim : str or Iterable of Hashable, optional\n3908             Dimension(s) over which to apply quantile.\n3909         method : str, default: \"linear\"\n3910             This optional parameter specifies the interpolation method to use when the\n3911             desired quantile lies between two data points. The options sorted by their R\n3912             type as summarized in the H&F paper [1]_ are:\n3913 \n3914                 1. \"inverted_cdf\" (*)\n3915                 2. \"averaged_inverted_cdf\" (*)\n3916                 3. \"closest_observation\" (*)\n3917                 4. \"interpolated_inverted_cdf\" (*)\n3918                 5. \"hazen\" (*)\n3919                 6. \"weibull\" (*)\n3920                 7. \"linear\"  (default)\n3921                 8. \"median_unbiased\" (*)\n3922                 9. \"normal_unbiased\" (*)\n3923 \n3924             The first three methods are discontiuous. The following discontinuous\n3925             variations of the default \"linear\" (7.) option are also available:\n3926 \n3927                 * \"lower\"\n3928                 * \"higher\"\n3929                 * \"midpoint\"\n3930                 * \"nearest\"\n3931 \n3932             See :py:func:`numpy.quantile` or [1]_ for details. The \"method\" argument\n3933             was previously called \"interpolation\", renamed in accordance with numpy\n3934             version 1.22.0.\n3935 \n3936             (*) These methods require numpy version 1.22 or newer.\n3937 \n3938         keep_attrs : bool or None, optional\n3939             If True, the dataset's attributes (`attrs`) will be copied from\n3940             the original object to the new one.  If False (default), the new\n3941             object will be returned without attributes.\n3942         skipna : bool or None, optional\n3943             If True, skip missing values (as marked by NaN). By default, only\n3944             skips missing values for float dtypes; other dtypes either do not\n3945             have a sentinel missing value (int) or skipna=True has not been\n3946             implemented (object, datetime64 or timedelta64).\n3947 \n3948         Returns\n3949         -------\n3950         quantiles : DataArray\n3951             If `q` is a single quantile, then the result\n3952             is a scalar. If multiple percentiles are given, first axis of\n3953             the result corresponds to the quantile and a quantile dimension\n3954             is added to the return array. The other dimensions are the\n3955             dimensions that remain after the reduction of the array.\n3956 \n3957         See Also\n3958         --------\n3959         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n3960 \n3961         Examples\n3962         --------\n3963         >>> da = xr.DataArray(\n3964         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n3965         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n3966         ...     dims=(\"x\", \"y\"),\n3967         ... )\n3968         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n3969         <xarray.DataArray ()>\n3970         array(0.7)\n3971         Coordinates:\n3972             quantile  float64 0.0\n3973         >>> da.quantile(0, dim=\"x\")\n3974         <xarray.DataArray (y: 4)>\n3975         array([0.7, 4.2, 2.6, 1.5])\n3976         Coordinates:\n3977           * y         (y) float64 1.0 1.5 2.0 2.5\n3978             quantile  float64 0.0\n3979         >>> da.quantile([0, 0.5, 1])\n3980         <xarray.DataArray (quantile: 3)>\n3981         array([0.7, 3.4, 9.4])\n3982         Coordinates:\n3983           * quantile  (quantile) float64 0.0 0.5 1.0\n3984         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n3985         <xarray.DataArray (quantile: 3, y: 4)>\n3986         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n3987                [3.6 , 5.75, 6.  , 1.7 ],\n3988                [6.5 , 7.3 , 9.4 , 1.9 ]])\n3989         Coordinates:\n3990           * y         (y) float64 1.0 1.5 2.0 2.5\n3991           * quantile  (quantile) float64 0.0 0.5 1.0\n3992 \n3993         References\n3994         ----------\n3995         .. [1] R. J. Hyndman and Y. Fan,\n3996            \"Sample quantiles in statistical packages,\"\n3997            The American Statistician, 50(4), pp. 361-365, 1996\n3998         \"\"\"\n3999 \n4000         ds = self._to_temp_dataset().quantile(\n4001             q,\n4002             dim=dim,\n4003             keep_attrs=keep_attrs,\n4004             method=method,\n4005             skipna=skipna,\n4006             interpolation=interpolation,\n4007         )\n4008         return self._from_temp_dataset(ds)\n4009 \n4010     def rank(\n4011         self: T_DataArray,\n4012         dim: Hashable,\n4013         pct: bool = False,\n4014         keep_attrs: bool | None = None,\n4015     ) -> T_DataArray:\n4016         \"\"\"Ranks the data.\n4017 \n4018         Equal values are assigned a rank that is the average of the ranks that\n4019         would have been otherwise assigned to all of the values within that\n4020         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n4021 \n4022         NaNs in the input array are returned as NaNs.\n4023 \n4024         The `bottleneck` library is required.\n4025 \n4026         Parameters\n4027         ----------\n4028         dim : Hashable\n4029             Dimension over which to compute rank.\n4030         pct : bool, default: False\n4031             If True, compute percentage ranks, otherwise compute integer ranks.\n4032         keep_attrs : bool or None, optional\n4033             If True, the dataset's attributes (`attrs`) will be copied from\n4034             the original object to the new one.  If False (default), the new\n4035             object will be returned without attributes.\n4036 \n4037         Returns\n4038         -------\n4039         ranked : DataArray\n4040             DataArray with the same coordinates and dtype 'float64'.\n4041 \n4042         Examples\n4043         --------\n4044         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n4045         >>> arr.rank(\"x\")\n4046         <xarray.DataArray (x: 3)>\n4047         array([1., 2., 3.])\n4048         Dimensions without coordinates: x\n4049         \"\"\"\n4050 \n4051         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n4052         return self._from_temp_dataset(ds)\n4053 \n4054     def differentiate(\n4055         self: T_DataArray,\n4056         coord: Hashable,\n4057         edge_order: Literal[1, 2] = 1,\n4058         datetime_unit: DatetimeUnitOptions = None,\n4059     ) -> T_DataArray:\n4060         \"\"\" Differentiate the array with the second order accurate central\n4061         differences.\n4062 \n4063         .. note::\n4064             This feature is limited to simple cartesian geometry, i.e. coord\n4065             must be one dimensional.\n4066 \n4067         Parameters\n4068         ----------\n4069         coord : Hashable\n4070             The coordinate to be used to compute the gradient.\n4071         edge_order : {1, 2}, default: 1\n4072             N-th order accurate differences at the boundaries.\n4073         datetime_unit : {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n4074                          \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, optional\n4075             Unit to compute gradient. Only valid for datetime coordinate.\n4076 \n4077         Returns\n4078         -------\n4079         differentiated: DataArray\n4080 \n4081         See also\n4082         --------\n4083         numpy.gradient: corresponding numpy function\n4084 \n4085         Examples\n4086         --------\n4087 \n4088         >>> da = xr.DataArray(\n4089         ...     np.arange(12).reshape(4, 3),\n4090         ...     dims=[\"x\", \"y\"],\n4091         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n4092         ... )\n4093         >>> da\n4094         <xarray.DataArray (x: 4, y: 3)>\n4095         array([[ 0,  1,  2],\n4096                [ 3,  4,  5],\n4097                [ 6,  7,  8],\n4098                [ 9, 10, 11]])\n4099         Coordinates:\n4100           * x        (x) float64 0.0 0.1 1.1 1.2\n4101         Dimensions without coordinates: y\n4102         >>>\n4103         >>> da.differentiate(\"x\")\n4104         <xarray.DataArray (x: 4, y: 3)>\n4105         array([[30.        , 30.        , 30.        ],\n4106                [27.54545455, 27.54545455, 27.54545455],\n4107                [27.54545455, 27.54545455, 27.54545455],\n4108                [30.        , 30.        , 30.        ]])\n4109         Coordinates:\n4110           * x        (x) float64 0.0 0.1 1.1 1.2\n4111         Dimensions without coordinates: y\n4112         \"\"\"\n4113         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n4114         return self._from_temp_dataset(ds)\n4115 \n4116     # change type of self and return to T_DataArray once\n4117     # https://github.com/python/mypy/issues/12846 is resolved\n4118     def integrate(\n4119         self,\n4120         coord: Hashable | Sequence[Hashable] = None,\n4121         datetime_unit: DatetimeUnitOptions = None,\n4122     ) -> DataArray:\n4123         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n4124 \n4125         .. note::\n4126             This feature is limited to simple cartesian geometry, i.e. coord\n4127             must be one dimensional.\n4128 \n4129         Parameters\n4130         ----------\n4131         coord : Hashable, or sequence of Hashable\n4132             Coordinate(s) used for the integration.\n4133         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n4134                         'ps', 'fs', 'as', None}, optional\n4135             Specify the unit if a datetime coordinate is used.\n4136 \n4137         Returns\n4138         -------\n4139         integrated : DataArray\n4140 \n4141         See also\n4142         --------\n4143         Dataset.integrate\n4144         numpy.trapz : corresponding numpy function\n4145 \n4146         Examples\n4147         --------\n4148 \n4149         >>> da = xr.DataArray(\n4150         ...     np.arange(12).reshape(4, 3),\n4151         ...     dims=[\"x\", \"y\"],\n4152         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n4153         ... )\n4154         >>> da\n4155         <xarray.DataArray (x: 4, y: 3)>\n4156         array([[ 0,  1,  2],\n4157                [ 3,  4,  5],\n4158                [ 6,  7,  8],\n4159                [ 9, 10, 11]])\n4160         Coordinates:\n4161           * x        (x) float64 0.0 0.1 1.1 1.2\n4162         Dimensions without coordinates: y\n4163         >>>\n4164         >>> da.integrate(\"x\")\n4165         <xarray.DataArray (y: 3)>\n4166         array([5.4, 6.6, 7.8])\n4167         Dimensions without coordinates: y\n4168         \"\"\"\n4169         ds = self._to_temp_dataset().integrate(coord, datetime_unit)\n4170         return self._from_temp_dataset(ds)\n4171 \n4172     # change type of self and return to T_DataArray once\n4173     # https://github.com/python/mypy/issues/12846 is resolved\n4174     def cumulative_integrate(\n4175         self,\n4176         coord: Hashable | Sequence[Hashable] = None,\n4177         datetime_unit: DatetimeUnitOptions = None,\n4178     ) -> DataArray:\n4179         \"\"\"Integrate cumulatively along the given coordinate using the trapezoidal rule.\n4180 \n4181         .. note::\n4182             This feature is limited to simple cartesian geometry, i.e. coord\n4183             must be one dimensional.\n4184 \n4185             The first entry of the cumulative integral is always 0, in order to keep the\n4186             length of the dimension unchanged between input and output.\n4187 \n4188         Parameters\n4189         ----------\n4190         coord : Hashable, or sequence of Hashable\n4191             Coordinate(s) used for the integration.\n4192         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n4193                         'ps', 'fs', 'as', None}, optional\n4194             Specify the unit if a datetime coordinate is used.\n4195 \n4196         Returns\n4197         -------\n4198         integrated : DataArray\n4199 \n4200         See also\n4201         --------\n4202         Dataset.cumulative_integrate\n4203         scipy.integrate.cumulative_trapezoid : corresponding scipy function\n4204 \n4205         Examples\n4206         --------\n4207 \n4208         >>> da = xr.DataArray(\n4209         ...     np.arange(12).reshape(4, 3),\n4210         ...     dims=[\"x\", \"y\"],\n4211         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n4212         ... )\n4213         >>> da\n4214         <xarray.DataArray (x: 4, y: 3)>\n4215         array([[ 0,  1,  2],\n4216                [ 3,  4,  5],\n4217                [ 6,  7,  8],\n4218                [ 9, 10, 11]])\n4219         Coordinates:\n4220           * x        (x) float64 0.0 0.1 1.1 1.2\n4221         Dimensions without coordinates: y\n4222         >>>\n4223         >>> da.cumulative_integrate(\"x\")\n4224         <xarray.DataArray (x: 4, y: 3)>\n4225         array([[0.  , 0.  , 0.  ],\n4226                [0.15, 0.25, 0.35],\n4227                [4.65, 5.75, 6.85],\n4228                [5.4 , 6.6 , 7.8 ]])\n4229         Coordinates:\n4230           * x        (x) float64 0.0 0.1 1.1 1.2\n4231         Dimensions without coordinates: y\n4232         \"\"\"\n4233         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)\n4234         return self._from_temp_dataset(ds)\n4235 \n4236     def unify_chunks(self) -> DataArray:\n4237         \"\"\"Unify chunk size along all chunked dimensions of this DataArray.\n4238 \n4239         Returns\n4240         -------\n4241         DataArray with consistent chunk sizes for all dask-array variables\n4242 \n4243         See Also\n4244         --------\n4245         dask.array.core.unify_chunks\n4246         \"\"\"\n4247 \n4248         return unify_chunks(self)[0]\n4249 \n4250     def map_blocks(\n4251         self,\n4252         func: Callable[..., T_Xarray],\n4253         args: Sequence[Any] = (),\n4254         kwargs: Mapping[str, Any] | None = None,\n4255         template: DataArray | Dataset | None = None,\n4256     ) -> T_Xarray:\n4257         \"\"\"\n4258         Apply a function to each block of this DataArray.\n4259 \n4260         .. warning::\n4261             This method is experimental and its signature may change.\n4262 \n4263         Parameters\n4264         ----------\n4265         func : callable\n4266             User-provided function that accepts a DataArray as its first\n4267             parameter. The function will receive a subset or 'block' of this DataArray (see below),\n4268             corresponding to one chunk along each chunked dimension. ``func`` will be\n4269             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n4270 \n4271             This function must return either a single DataArray or a single Dataset.\n4272 \n4273             This function cannot add a new chunked dimension.\n4274         args : sequence\n4275             Passed to func after unpacking and subsetting any xarray objects by blocks.\n4276             xarray objects in args must be aligned with this object, otherwise an error is raised.\n4277         kwargs : mapping\n4278             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n4279             subset to blocks. Passing dask collections in kwargs is not allowed.\n4280         template : DataArray or Dataset, optional\n4281             xarray object representing the final result after compute is called. If not provided,\n4282             the function will be first run on mocked-up data, that looks like this object but\n4283             has sizes 0, to determine properties of the returned object such as dtype,\n4284             variable names, attributes, new dimensions and new indexes (if any).\n4285             ``template`` must be provided if the function changes the size of existing dimensions.\n4286             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n4287             ``attrs`` set by ``func`` will be ignored.\n4288 \n4289         Returns\n4290         -------\n4291         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n4292         function.\n4293 \n4294         Notes\n4295         -----\n4296         This function is designed for when ``func`` needs to manipulate a whole xarray object\n4297         subset to each block. Each block is loaded into memory. In the more common case where\n4298         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n4299 \n4300         If none of the variables in this object is backed by dask arrays, calling this function is\n4301         equivalent to calling ``func(obj, *args, **kwargs)``.\n4302 \n4303         See Also\n4304         --------\n4305         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n4306         xarray.DataArray.map_blocks\n4307 \n4308         Examples\n4309         --------\n4310         Calculate an anomaly from climatology using ``.groupby()``. Using\n4311         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n4312         its indices, and its methods like ``.groupby()``.\n4313 \n4314         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n4315         ...     gb = da.groupby(groupby_type)\n4316         ...     clim = gb.mean(dim=\"time\")\n4317         ...     return gb - clim\n4318         ...\n4319         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n4320         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n4321         >>> np.random.seed(123)\n4322         >>> array = xr.DataArray(\n4323         ...     np.random.rand(len(time)),\n4324         ...     dims=[\"time\"],\n4325         ...     coords={\"time\": time, \"month\": month},\n4326         ... ).chunk()\n4327         >>> array.map_blocks(calculate_anomaly, template=array).compute()\n4328         <xarray.DataArray (time: 24)>\n4329         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n4330                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n4331                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n4332                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n4333                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n4334         Coordinates:\n4335           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n4336             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n4337 \n4338         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n4339         to the function being applied in ``xr.map_blocks()``:\n4340 \n4341         >>> array.map_blocks(\n4342         ...     calculate_anomaly, kwargs={\"groupby_type\": \"time.year\"}, template=array\n4343         ... )  # doctest: +ELLIPSIS\n4344         <xarray.DataArray (time: 24)>\n4345         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>\n4346         Coordinates:\n4347           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n4348             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n4349         \"\"\"\n4350         from .parallel import map_blocks\n4351 \n4352         return map_blocks(func, self, args, kwargs, template)\n4353 \n4354     def polyfit(\n4355         self,\n4356         dim: Hashable,\n4357         deg: int,\n4358         skipna: bool | None = None,\n4359         rcond: float | None = None,\n4360         w: Hashable | Any | None = None,\n4361         full: bool = False,\n4362         cov: bool | Literal[\"unscaled\"] = False,\n4363     ) -> Dataset:\n4364         \"\"\"\n4365         Least squares polynomial fit.\n4366 \n4367         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n4368         invalid values when `skipna = True`.\n4369 \n4370         Parameters\n4371         ----------\n4372         dim : Hashable\n4373             Coordinate along which to fit the polynomials.\n4374         deg : int\n4375             Degree of the fitting polynomial.\n4376         skipna : bool or None, optional\n4377             If True, removes all invalid values before fitting each 1D slices of the array.\n4378             Default is True if data is stored in a dask.array or if there is any\n4379             invalid values, False otherwise.\n4380         rcond : float or None, optional\n4381             Relative condition number to the fit.\n4382         w : Hashable, array-like or None, optional\n4383             Weights to apply to the y-coordinate of the sample points.\n4384             Can be an array-like object or the name of a coordinate in the dataset.\n4385         full : bool, default: False\n4386             Whether to return the residuals, matrix rank and singular values in addition\n4387             to the coefficients.\n4388         cov : bool or \"unscaled\", default: False\n4389             Whether to return to the covariance matrix in addition to the coefficients.\n4390             The matrix is not scaled if `cov='unscaled'`.\n4391 \n4392         Returns\n4393         -------\n4394         polyfit_results : Dataset\n4395             A single dataset which contains:\n4396 \n4397             polyfit_coefficients\n4398                 The coefficients of the best fit.\n4399             polyfit_residuals\n4400                 The residuals of the least-square computation (only included if `full=True`).\n4401                 When the matrix rank is deficient, np.nan is returned.\n4402             [dim]_matrix_rank\n4403                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n4404             [dim]_singular_value\n4405                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n4406             polyfit_covariance\n4407                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n4408 \n4409         See Also\n4410         --------\n4411         numpy.polyfit\n4412         numpy.polyval\n4413         xarray.polyval\n4414         \"\"\"\n4415         return self._to_temp_dataset().polyfit(\n4416             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n4417         )\n4418 \n4419     def pad(\n4420         self: T_DataArray,\n4421         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,\n4422         mode: PadModeOptions = \"constant\",\n4423         stat_length: int\n4424         | tuple[int, int]\n4425         | Mapping[Any, tuple[int, int]]\n4426         | None = None,\n4427         constant_values: float\n4428         | tuple[float, float]\n4429         | Mapping[Any, tuple[float, float]]\n4430         | None = None,\n4431         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,\n4432         reflect_type: PadReflectOptions = None,\n4433         **pad_width_kwargs: Any,\n4434     ) -> T_DataArray:\n4435         \"\"\"Pad this array along one or more dimensions.\n4436 \n4437         .. warning::\n4438             This function is experimental and its behaviour is likely to change\n4439             especially regarding padding of dimension coordinates (or IndexVariables).\n4440 \n4441         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n4442         coordinates will be padded with the same mode, otherwise coordinates\n4443         are padded using the \"constant\" mode with fill_value dtypes.NA.\n4444 \n4445         Parameters\n4446         ----------\n4447         pad_width : mapping of Hashable to tuple of int\n4448             Mapping with the form of {dim: (pad_before, pad_after)}\n4449             describing the number of values padded along each dimension.\n4450             {dim: pad} is a shortcut for pad_before = pad_after = pad\n4451         mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \\\n4452             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n4453             How to pad the DataArray (taken from numpy docs):\n4454 \n4455             - \"constant\": Pads with a constant value.\n4456             - \"edge\": Pads with the edge values of array.\n4457             - \"linear_ramp\": Pads with the linear ramp between end_value and the\n4458               array edge value.\n4459             - \"maximum\": Pads with the maximum value of all or part of the\n4460               vector along each axis.\n4461             - \"mean\": Pads with the mean value of all or part of the\n4462               vector along each axis.\n4463             - \"median\": Pads with the median value of all or part of the\n4464               vector along each axis.\n4465             - \"minimum\": Pads with the minimum value of all or part of the\n4466               vector along each axis.\n4467             - \"reflect\": Pads with the reflection of the vector mirrored on\n4468               the first and last values of the vector along each axis.\n4469             - \"symmetric\": Pads with the reflection of the vector mirrored\n4470               along the edge of the array.\n4471             - \"wrap\": Pads with the wrap of the vector along the axis.\n4472               The first values are used to pad the end and the\n4473               end values are used to pad the beginning.\n4474 \n4475         stat_length : int, tuple or mapping of Hashable to tuple, default: None\n4476             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n4477             values at edge of each axis used to calculate the statistic value.\n4478             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n4479             statistic lengths along each dimension.\n4480             ((before, after),) yields same before and after statistic lengths\n4481             for each dimension.\n4482             (stat_length,) or int is a shortcut for before = after = statistic\n4483             length for all axes.\n4484             Default is ``None``, to use the entire axis.\n4485         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n4486             Used in 'constant'.  The values to set the padded values for each\n4487             axis.\n4488             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n4489             pad constants along each dimension.\n4490             ``((before, after),)`` yields same before and after constants for each\n4491             dimension.\n4492             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n4493             all dimensions.\n4494             Default is 0.\n4495         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n4496             Used in 'linear_ramp'.  The values used for the ending value of the\n4497             linear_ramp and that will form the edge of the padded array.\n4498             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n4499             end values along each dimension.\n4500             ``((before, after),)`` yields same before and after end values for each\n4501             axis.\n4502             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n4503             all axes.\n4504             Default is 0.\n4505         reflect_type : {\"even\", \"odd\", None}, optional\n4506             Used in \"reflect\", and \"symmetric\". The \"even\" style is the\n4507             default with an unaltered reflection around the edge value. For\n4508             the \"odd\" style, the extended part of the array is created by\n4509             subtracting the reflected values from two times the edge value.\n4510         **pad_width_kwargs\n4511             The keyword arguments form of ``pad_width``.\n4512             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n4513 \n4514         Returns\n4515         -------\n4516         padded : DataArray\n4517             DataArray with the padded coordinates and data.\n4518 \n4519         See Also\n4520         --------\n4521         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n4522 \n4523         Notes\n4524         -----\n4525         For ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n4526         promoted to ``float`` and padded with ``np.nan``.\n4527 \n4528         Padding coordinates will drop their corresponding index (if any) and will reset default\n4529         indexes for dimension coordinates.\n4530 \n4531         Examples\n4532         --------\n4533         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0, 1, 2])])\n4534         >>> arr.pad(x=(1, 2), constant_values=0)\n4535         <xarray.DataArray (x: 6)>\n4536         array([0, 5, 6, 7, 0, 0])\n4537         Coordinates:\n4538           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n4539 \n4540         >>> da = xr.DataArray(\n4541         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n4542         ...     dims=[\"x\", \"y\"],\n4543         ...     coords={\"x\": [0, 1], \"y\": [10, 20, 30, 40], \"z\": (\"x\", [100, 200])},\n4544         ... )\n4545         >>> da.pad(x=1)\n4546         <xarray.DataArray (x: 4, y: 4)>\n4547         array([[nan, nan, nan, nan],\n4548                [ 0.,  1.,  2.,  3.],\n4549                [10., 11., 12., 13.],\n4550                [nan, nan, nan, nan]])\n4551         Coordinates:\n4552           * x        (x) float64 nan 0.0 1.0 nan\n4553           * y        (y) int64 10 20 30 40\n4554             z        (x) float64 nan 100.0 200.0 nan\n4555 \n4556         Careful, ``constant_values`` are coerced to the data type of the array which may\n4557         lead to a loss of precision:\n4558 \n4559         >>> da.pad(x=1, constant_values=1.23456789)\n4560         <xarray.DataArray (x: 4, y: 4)>\n4561         array([[ 1,  1,  1,  1],\n4562                [ 0,  1,  2,  3],\n4563                [10, 11, 12, 13],\n4564                [ 1,  1,  1,  1]])\n4565         Coordinates:\n4566           * x        (x) float64 nan 0.0 1.0 nan\n4567           * y        (y) int64 10 20 30 40\n4568             z        (x) float64 nan 100.0 200.0 nan\n4569         \"\"\"\n4570         ds = self._to_temp_dataset().pad(\n4571             pad_width=pad_width,\n4572             mode=mode,\n4573             stat_length=stat_length,\n4574             constant_values=constant_values,\n4575             end_values=end_values,\n4576             reflect_type=reflect_type,\n4577             **pad_width_kwargs,\n4578         )\n4579         return self._from_temp_dataset(ds)\n4580 \n4581     def idxmin(\n4582         self,\n4583         dim: Hashable | None = None,\n4584         skipna: bool | None = None,\n4585         fill_value: Any = dtypes.NA,\n4586         keep_attrs: bool | None = None,\n4587     ) -> DataArray:\n4588         \"\"\"Return the coordinate label of the minimum value along a dimension.\n4589 \n4590         Returns a new `DataArray` named after the dimension with the values of\n4591         the coordinate labels along that dimension corresponding to minimum\n4592         values along that dimension.\n4593 \n4594         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n4595         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n4596 \n4597         Parameters\n4598         ----------\n4599         dim : str, optional\n4600             Dimension over which to apply `idxmin`.  This is optional for 1D\n4601             arrays, but required for arrays with 2 or more dimensions.\n4602         skipna : bool or None, default: None\n4603             If True, skip missing values (as marked by NaN). By default, only\n4604             skips missing values for ``float``, ``complex``, and ``object``\n4605             dtypes; other dtypes either do not have a sentinel missing value\n4606             (``int``) or ``skipna=True`` has not been implemented\n4607             (``datetime64`` or ``timedelta64``).\n4608         fill_value : Any, default: NaN\n4609             Value to be filled in case all of the values along a dimension are\n4610             null.  By default this is NaN.  The fill value and result are\n4611             automatically converted to a compatible dtype if possible.\n4612             Ignored if ``skipna`` is False.\n4613         keep_attrs : bool or None, optional\n4614             If True, the attributes (``attrs``) will be copied from the\n4615             original object to the new one. If False, the new object\n4616             will be returned without attributes.\n4617 \n4618         Returns\n4619         -------\n4620         reduced : DataArray\n4621             New `DataArray` object with `idxmin` applied to its data and the\n4622             indicated dimension removed.\n4623 \n4624         See Also\n4625         --------\n4626         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n4627 \n4628         Examples\n4629         --------\n4630         >>> array = xr.DataArray(\n4631         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n4632         ... )\n4633         >>> array.min()\n4634         <xarray.DataArray ()>\n4635         array(-2)\n4636         >>> array.argmin()\n4637         <xarray.DataArray ()>\n4638         array(4)\n4639         >>> array.idxmin()\n4640         <xarray.DataArray 'x' ()>\n4641         array('e', dtype='<U1')\n4642 \n4643         >>> array = xr.DataArray(\n4644         ...     [\n4645         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n4646         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n4647         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n4648         ...     ],\n4649         ...     dims=[\"y\", \"x\"],\n4650         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n4651         ... )\n4652         >>> array.min(dim=\"x\")\n4653         <xarray.DataArray (y: 3)>\n4654         array([-2., -4.,  1.])\n4655         Coordinates:\n4656           * y        (y) int64 -1 0 1\n4657         >>> array.argmin(dim=\"x\")\n4658         <xarray.DataArray (y: 3)>\n4659         array([4, 0, 2])\n4660         Coordinates:\n4661           * y        (y) int64 -1 0 1\n4662         >>> array.idxmin(dim=\"x\")\n4663         <xarray.DataArray 'x' (y: 3)>\n4664         array([16.,  0.,  4.])\n4665         Coordinates:\n4666           * y        (y) int64 -1 0 1\n4667         \"\"\"\n4668         return computation._calc_idxminmax(\n4669             array=self,\n4670             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n4671             dim=dim,\n4672             skipna=skipna,\n4673             fill_value=fill_value,\n4674             keep_attrs=keep_attrs,\n4675         )\n4676 \n4677     def idxmax(\n4678         self,\n4679         dim: Hashable = None,\n4680         skipna: bool | None = None,\n4681         fill_value: Any = dtypes.NA,\n4682         keep_attrs: bool | None = None,\n4683     ) -> DataArray:\n4684         \"\"\"Return the coordinate label of the maximum value along a dimension.\n4685 \n4686         Returns a new `DataArray` named after the dimension with the values of\n4687         the coordinate labels along that dimension corresponding to maximum\n4688         values along that dimension.\n4689 \n4690         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n4691         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n4692 \n4693         Parameters\n4694         ----------\n4695         dim : Hashable, optional\n4696             Dimension over which to apply `idxmax`.  This is optional for 1D\n4697             arrays, but required for arrays with 2 or more dimensions.\n4698         skipna : bool or None, default: None\n4699             If True, skip missing values (as marked by NaN). By default, only\n4700             skips missing values for ``float``, ``complex``, and ``object``\n4701             dtypes; other dtypes either do not have a sentinel missing value\n4702             (``int``) or ``skipna=True`` has not been implemented\n4703             (``datetime64`` or ``timedelta64``).\n4704         fill_value : Any, default: NaN\n4705             Value to be filled in case all of the values along a dimension are\n4706             null.  By default this is NaN.  The fill value and result are\n4707             automatically converted to a compatible dtype if possible.\n4708             Ignored if ``skipna`` is False.\n4709         keep_attrs : bool or None, optional\n4710             If True, the attributes (``attrs``) will be copied from the\n4711             original object to the new one. If False, the new object\n4712             will be returned without attributes.\n4713 \n4714         Returns\n4715         -------\n4716         reduced : DataArray\n4717             New `DataArray` object with `idxmax` applied to its data and the\n4718             indicated dimension removed.\n4719 \n4720         See Also\n4721         --------\n4722         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n4723 \n4724         Examples\n4725         --------\n4726         >>> array = xr.DataArray(\n4727         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n4728         ... )\n4729         >>> array.max()\n4730         <xarray.DataArray ()>\n4731         array(2)\n4732         >>> array.argmax()\n4733         <xarray.DataArray ()>\n4734         array(1)\n4735         >>> array.idxmax()\n4736         <xarray.DataArray 'x' ()>\n4737         array('b', dtype='<U1')\n4738 \n4739         >>> array = xr.DataArray(\n4740         ...     [\n4741         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n4742         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n4743         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n4744         ...     ],\n4745         ...     dims=[\"y\", \"x\"],\n4746         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n4747         ... )\n4748         >>> array.max(dim=\"x\")\n4749         <xarray.DataArray (y: 3)>\n4750         array([2., 2., 1.])\n4751         Coordinates:\n4752           * y        (y) int64 -1 0 1\n4753         >>> array.argmax(dim=\"x\")\n4754         <xarray.DataArray (y: 3)>\n4755         array([0, 2, 2])\n4756         Coordinates:\n4757           * y        (y) int64 -1 0 1\n4758         >>> array.idxmax(dim=\"x\")\n4759         <xarray.DataArray 'x' (y: 3)>\n4760         array([0., 4., 4.])\n4761         Coordinates:\n4762           * y        (y) int64 -1 0 1\n4763         \"\"\"\n4764         return computation._calc_idxminmax(\n4765             array=self,\n4766             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n4767             dim=dim,\n4768             skipna=skipna,\n4769             fill_value=fill_value,\n4770             keep_attrs=keep_attrs,\n4771         )\n4772 \n4773     # change type of self and return to T_DataArray once\n4774     # https://github.com/python/mypy/issues/12846 is resolved\n4775     def argmin(\n4776         self,\n4777         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,\n4778         axis: int | None = None,\n4779         keep_attrs: bool | None = None,\n4780         skipna: bool | None = None,\n4781     ) -> DataArray | dict[Hashable, DataArray]:\n4782         \"\"\"Index or indices of the minimum of the DataArray over one or more dimensions.\n4783 \n4784         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n4785         which can be passed directly to isel(). If a single str is passed to 'dim' then\n4786         returns a DataArray with dtype int.\n4787 \n4788         If there are multiple minima, the indices of the first one found will be\n4789         returned.\n4790 \n4791         Parameters\n4792         ----------\n4793         dim : Hashable, sequence of Hashable, None or ..., optional\n4794             The dimensions over which to find the minimum. By default, finds minimum over\n4795             all dimensions - for now returning an int for backward compatibility, but\n4796             this is deprecated, in future will return a dict with indices for all\n4797             dimensions; to return a dict with all dimensions now, pass '...'.\n4798         axis : int or None, optional\n4799             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n4800             can be supplied.\n4801         keep_attrs : bool or None, optional\n4802             If True, the attributes (`attrs`) will be copied from the original\n4803             object to the new one. If False, the new object will be\n4804             returned without attributes.\n4805         skipna : bool or None, optional\n4806             If True, skip missing values (as marked by NaN). By default, only\n4807             skips missing values for float dtypes; other dtypes either do not\n4808             have a sentinel missing value (int) or skipna=True has not been\n4809             implemented (object, datetime64 or timedelta64).\n4810 \n4811         Returns\n4812         -------\n4813         result : DataArray or dict of DataArray\n4814 \n4815         See Also\n4816         --------\n4817         Variable.argmin, DataArray.idxmin\n4818 \n4819         Examples\n4820         --------\n4821         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n4822         >>> array.min()\n4823         <xarray.DataArray ()>\n4824         array(-1)\n4825         >>> array.argmin()\n4826         <xarray.DataArray ()>\n4827         array(2)\n4828         >>> array.argmin(...)\n4829         {'x': <xarray.DataArray ()>\n4830         array(2)}\n4831         >>> array.isel(array.argmin(...))\n4832         <xarray.DataArray ()>\n4833         array(-1)\n4834 \n4835         >>> array = xr.DataArray(\n4836         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],\n4837         ...     dims=(\"x\", \"y\", \"z\"),\n4838         ... )\n4839         >>> array.min(dim=\"x\")\n4840         <xarray.DataArray (y: 3, z: 3)>\n4841         array([[ 1,  2,  1],\n4842                [ 2, -5,  1],\n4843                [ 2,  1,  1]])\n4844         Dimensions without coordinates: y, z\n4845         >>> array.argmin(dim=\"x\")\n4846         <xarray.DataArray (y: 3, z: 3)>\n4847         array([[1, 0, 0],\n4848                [1, 1, 1],\n4849                [0, 0, 1]])\n4850         Dimensions without coordinates: y, z\n4851         >>> array.argmin(dim=[\"x\"])\n4852         {'x': <xarray.DataArray (y: 3, z: 3)>\n4853         array([[1, 0, 0],\n4854                [1, 1, 1],\n4855                [0, 0, 1]])\n4856         Dimensions without coordinates: y, z}\n4857         >>> array.min(dim=(\"x\", \"z\"))\n4858         <xarray.DataArray (y: 3)>\n4859         array([ 1, -5,  1])\n4860         Dimensions without coordinates: y\n4861         >>> array.argmin(dim=[\"x\", \"z\"])\n4862         {'x': <xarray.DataArray (y: 3)>\n4863         array([0, 1, 0])\n4864         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n4865         array([2, 1, 1])\n4866         Dimensions without coordinates: y}\n4867         >>> array.isel(array.argmin(dim=[\"x\", \"z\"]))\n4868         <xarray.DataArray (y: 3)>\n4869         array([ 1, -5,  1])\n4870         Dimensions without coordinates: y\n4871         \"\"\"\n4872         result = self.variable.argmin(dim, axis, keep_attrs, skipna)\n4873         if isinstance(result, dict):\n4874             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n4875         else:\n4876             return self._replace_maybe_drop_dims(result)\n4877 \n4878     # change type of self and return to T_DataArray once\n4879     # https://github.com/python/mypy/issues/12846 is resolved\n4880     def argmax(\n4881         self,\n4882         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,\n4883         axis: int | None = None,\n4884         keep_attrs: bool | None = None,\n4885         skipna: bool | None = None,\n4886     ) -> DataArray | dict[Hashable, DataArray]:\n4887         \"\"\"Index or indices of the maximum of the DataArray over one or more dimensions.\n4888 \n4889         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n4890         which can be passed directly to isel(). If a single str is passed to 'dim' then\n4891         returns a DataArray with dtype int.\n4892 \n4893         If there are multiple maxima, the indices of the first one found will be\n4894         returned.\n4895 \n4896         Parameters\n4897         ----------\n4898         dim : Hashable, sequence of Hashable, None or ..., optional\n4899             The dimensions over which to find the maximum. By default, finds maximum over\n4900             all dimensions - for now returning an int for backward compatibility, but\n4901             this is deprecated, in future will return a dict with indices for all\n4902             dimensions; to return a dict with all dimensions now, pass '...'.\n4903         axis : int or None, optional\n4904             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments\n4905             can be supplied.\n4906         keep_attrs : bool or None, optional\n4907             If True, the attributes (`attrs`) will be copied from the original\n4908             object to the new one. If False, the new object will be\n4909             returned without attributes.\n4910         skipna : bool or None, optional\n4911             If True, skip missing values (as marked by NaN). By default, only\n4912             skips missing values for float dtypes; other dtypes either do not\n4913             have a sentinel missing value (int) or skipna=True has not been\n4914             implemented (object, datetime64 or timedelta64).\n4915 \n4916         Returns\n4917         -------\n4918         result : DataArray or dict of DataArray\n4919 \n4920         See Also\n4921         --------\n4922         Variable.argmax, DataArray.idxmax\n4923 \n4924         Examples\n4925         --------\n4926         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n4927         >>> array.max()\n4928         <xarray.DataArray ()>\n4929         array(3)\n4930         >>> array.argmax()\n4931         <xarray.DataArray ()>\n4932         array(3)\n4933         >>> array.argmax(...)\n4934         {'x': <xarray.DataArray ()>\n4935         array(3)}\n4936         >>> array.isel(array.argmax(...))\n4937         <xarray.DataArray ()>\n4938         array(3)\n4939 \n4940         >>> array = xr.DataArray(\n4941         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],\n4942         ...     dims=(\"x\", \"y\", \"z\"),\n4943         ... )\n4944         >>> array.max(dim=\"x\")\n4945         <xarray.DataArray (y: 3, z: 3)>\n4946         array([[3, 3, 2],\n4947                [3, 5, 2],\n4948                [2, 3, 3]])\n4949         Dimensions without coordinates: y, z\n4950         >>> array.argmax(dim=\"x\")\n4951         <xarray.DataArray (y: 3, z: 3)>\n4952         array([[0, 1, 1],\n4953                [0, 1, 0],\n4954                [0, 1, 0]])\n4955         Dimensions without coordinates: y, z\n4956         >>> array.argmax(dim=[\"x\"])\n4957         {'x': <xarray.DataArray (y: 3, z: 3)>\n4958         array([[0, 1, 1],\n4959                [0, 1, 0],\n4960                [0, 1, 0]])\n4961         Dimensions without coordinates: y, z}\n4962         >>> array.max(dim=(\"x\", \"z\"))\n4963         <xarray.DataArray (y: 3)>\n4964         array([3, 5, 3])\n4965         Dimensions without coordinates: y\n4966         >>> array.argmax(dim=[\"x\", \"z\"])\n4967         {'x': <xarray.DataArray (y: 3)>\n4968         array([0, 1, 0])\n4969         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n4970         array([0, 1, 2])\n4971         Dimensions without coordinates: y}\n4972         >>> array.isel(array.argmax(dim=[\"x\", \"z\"]))\n4973         <xarray.DataArray (y: 3)>\n4974         array([3, 5, 3])\n4975         Dimensions without coordinates: y\n4976         \"\"\"\n4977         result = self.variable.argmax(dim, axis, keep_attrs, skipna)\n4978         if isinstance(result, dict):\n4979             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n4980         else:\n4981             return self._replace_maybe_drop_dims(result)\n4982 \n4983     def query(\n4984         self,\n4985         queries: Mapping[Any, Any] | None = None,\n4986         parser: QueryParserOptions = \"pandas\",\n4987         engine: QueryEngineOptions = None,\n4988         missing_dims: ErrorOptionsWithWarn = \"raise\",\n4989         **queries_kwargs: Any,\n4990     ) -> DataArray:\n4991         \"\"\"Return a new data array indexed along the specified\n4992         dimension(s), where the indexers are given as strings containing\n4993         Python expressions to be evaluated against the values in the array.\n4994 \n4995         Parameters\n4996         ----------\n4997         queries : dict-like or None, optional\n4998             A dict-like with keys matching dimensions and values given by strings\n4999             containing Python expressions to be evaluated against the data variables\n5000             in the dataset. The expressions will be evaluated using the pandas\n5001             eval() function, and can contain any valid Python expressions but cannot\n5002             contain any Python statements.\n5003         parser : {\"pandas\", \"python\"}, default: \"pandas\"\n5004             The parser to use to construct the syntax tree from the expression.\n5005             The default of 'pandas' parses code slightly different than standard\n5006             Python. Alternatively, you can parse an expression using the 'python'\n5007             parser to retain strict Python semantics.\n5008         engine : {\"python\", \"numexpr\", None}, default: None\n5009             The engine used to evaluate the expression. Supported engines are:\n5010 \n5011             - None: tries to use numexpr, falls back to python\n5012             - \"numexpr\": evaluates expressions using numexpr\n5013             - \"python\": performs operations as if you had eval\u2019d in top level python\n5014 \n5015         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n5016             What to do if dimensions that should be selected from are not present in the\n5017             DataArray:\n5018 \n5019             - \"raise\": raise an exception\n5020             - \"warn\": raise a warning, and ignore the missing dimensions\n5021             - \"ignore\": ignore the missing dimensions\n5022 \n5023         **queries_kwargs : {dim: query, ...}, optional\n5024             The keyword arguments form of ``queries``.\n5025             One of queries or queries_kwargs must be provided.\n5026 \n5027         Returns\n5028         -------\n5029         obj : DataArray\n5030             A new DataArray with the same contents as this dataset, indexed by\n5031             the results of the appropriate queries.\n5032 \n5033         See Also\n5034         --------\n5035         DataArray.isel\n5036         Dataset.query\n5037         pandas.eval\n5038 \n5039         Examples\n5040         --------\n5041         >>> da = xr.DataArray(np.arange(0, 5, 1), dims=\"x\", name=\"a\")\n5042         >>> da\n5043         <xarray.DataArray 'a' (x: 5)>\n5044         array([0, 1, 2, 3, 4])\n5045         Dimensions without coordinates: x\n5046         >>> da.query(x=\"a > 2\")\n5047         <xarray.DataArray 'a' (x: 2)>\n5048         array([3, 4])\n5049         Dimensions without coordinates: x\n5050         \"\"\"\n5051 \n5052         ds = self._to_dataset_whole(shallow_copy=True)\n5053         ds = ds.query(\n5054             queries=queries,\n5055             parser=parser,\n5056             engine=engine,\n5057             missing_dims=missing_dims,\n5058             **queries_kwargs,\n5059         )\n5060         return ds[self.name]\n5061 \n5062     def curvefit(\n5063         self,\n5064         coords: str | DataArray | Iterable[str | DataArray],\n5065         func: Callable[..., Any],\n5066         reduce_dims: Hashable | Iterable[Hashable] | None = None,\n5067         skipna: bool = True,\n5068         p0: dict[str, Any] | None = None,\n5069         bounds: dict[str, Any] | None = None,\n5070         param_names: Sequence[str] | None = None,\n5071         kwargs: dict[str, Any] | None = None,\n5072     ) -> Dataset:\n5073         \"\"\"\n5074         Curve fitting optimization for arbitrary functions.\n5075 \n5076         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n5077 \n5078         Parameters\n5079         ----------\n5080         coords : Hashable, DataArray, or sequence of DataArray or Hashable\n5081             Independent coordinate(s) over which to perform the curve fitting. Must share\n5082             at least one dimension with the calling object. When fitting multi-dimensional\n5083             functions, supply `coords` as a sequence in the same order as arguments in\n5084             `func`. To fit along existing dimensions of the calling object, `coords` can\n5085             also be specified as a str or sequence of strs.\n5086         func : callable\n5087             User specified function in the form `f(x, *params)` which returns a numpy\n5088             array of length `len(x)`. `params` are the fittable parameters which are optimized\n5089             by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n5090             coordinates, e.g. `f((x0, x1), *params)`.\n5091         reduce_dims : Hashable or sequence of Hashable\n5092             Additional dimension(s) over which to aggregate while fitting. For example,\n5093             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n5094             aggregate all lat and lon points and fit the specified function along the\n5095             time dimension.\n5096         skipna : bool, default: True\n5097             Whether to skip missing values when fitting. Default is True.\n5098         p0 : dict-like or None, optional\n5099             Optional dictionary of parameter names to initial guesses passed to the\n5100             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n5101             be assigned initial values following the default scipy behavior.\n5102         bounds : dict-like or None, optional\n5103             Optional dictionary of parameter names to bounding values passed to the\n5104             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n5105             will be unbounded following the default scipy behavior.\n5106         param_names : sequence of Hashable or None, optional\n5107             Sequence of names for the fittable parameters of `func`. If not supplied,\n5108             this will be automatically determined by arguments of `func`. `param_names`\n5109             should be manually supplied when fitting a function that takes a variable\n5110             number of parameters.\n5111         **kwargs : optional\n5112             Additional keyword arguments to passed to scipy curve_fit.\n5113 \n5114         Returns\n5115         -------\n5116         curvefit_results : Dataset\n5117             A single dataset which contains:\n5118 \n5119             [var]_curvefit_coefficients\n5120                 The coefficients of the best fit.\n5121             [var]_curvefit_covariance\n5122                 The covariance matrix of the coefficient estimates.\n5123 \n5124         See Also\n5125         --------\n5126         DataArray.polyfit\n5127         scipy.optimize.curve_fit\n5128         \"\"\"\n5129         return self._to_temp_dataset().curvefit(\n5130             coords,\n5131             func,\n5132             reduce_dims=reduce_dims,\n5133             skipna=skipna,\n5134             p0=p0,\n5135             bounds=bounds,\n5136             param_names=param_names,\n5137             kwargs=kwargs,\n5138         )\n5139 \n5140     def drop_duplicates(\n5141         self: T_DataArray,\n5142         dim: Hashable | Iterable[Hashable],\n5143         keep: Literal[\"first\", \"last\", False] = \"first\",\n5144     ) -> T_DataArray:\n5145         \"\"\"Returns a new DataArray with duplicate dimension values removed.\n5146 \n5147         Parameters\n5148         ----------\n5149         dim : dimension label or labels\n5150             Pass `...` to drop duplicates along all dimensions.\n5151         keep : {\"first\", \"last\", False}, default: \"first\"\n5152             Determines which duplicates (if any) to keep.\n5153 \n5154             - ``\"first\"`` : Drop duplicates except for the first occurrence.\n5155             - ``\"last\"`` : Drop duplicates except for the last occurrence.\n5156             - False : Drop all duplicates.\n5157 \n5158         Returns\n5159         -------\n5160         DataArray\n5161 \n5162         See Also\n5163         --------\n5164         Dataset.drop_duplicates\n5165         \"\"\"\n5166         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)\n5167         return self._from_temp_dataset(deduplicated)\n5168 \n5169     def convert_calendar(\n5170         self,\n5171         calendar: str,\n5172         dim: str = \"time\",\n5173         align_on: str | None = None,\n5174         missing: Any | None = None,\n5175         use_cftime: bool | None = None,\n5176     ) -> DataArray:\n5177         \"\"\"Convert the DataArray to another calendar.\n5178 \n5179         Only converts the individual timestamps, does not modify any data except\n5180         in dropping invalid/surplus dates or inserting missing dates.\n5181 \n5182         If the source and target calendars are either no_leap, all_leap or a\n5183         standard type, only the type of the time array is modified.\n5184         When converting to a leap year from a non-leap year, the 29th of February\n5185         is removed from the array. In the other direction the 29th of February\n5186         will be missing in the output, unless `missing` is specified,\n5187         in which case that value is inserted.\n5188 \n5189         For conversions involving `360_day` calendars, see Notes.\n5190 \n5191         This method is safe to use with sub-daily data as it doesn't touch the\n5192         time part of the timestamps.\n5193 \n5194         Parameters\n5195         ---------\n5196         calendar : str\n5197             The target calendar name.\n5198         dim : str\n5199             Name of the time coordinate.\n5200         align_on : {None, 'date', 'year'}\n5201             Must be specified when either source or target is a `360_day` calendar,\n5202            ignored otherwise. See Notes.\n5203         missing : Optional[any]\n5204             By default, i.e. if the value is None, this method will simply attempt\n5205             to convert the dates in the source calendar to the same dates in the\n5206             target calendar, and drop any of those that are not possible to\n5207             represent.  If a value is provided, a new time coordinate will be\n5208             created in the target calendar with the same frequency as the original\n5209             time coordinate; for any dates that are not present in the source, the\n5210             data will be filled with this value.  Note that using this mode requires\n5211             that the source data have an inferable frequency; for more information\n5212             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n5213             target calendar combinations, this could result in many missing values, see notes.\n5214         use_cftime : boolean, optional\n5215             Whether to use cftime objects in the output, only used if `calendar`\n5216             is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n5217             If True, the new time axis uses cftime objects.\n5218             If None (default), it uses :py:class:`numpy.datetime64` values if the\n5219             date range permits it, and :py:class:`cftime.datetime` objects if not.\n5220             If False, it uses :py:class:`numpy.datetime64`  or fails.\n5221 \n5222         Returns\n5223         -------\n5224         DataArray\n5225             Copy of the dataarray with the time coordinate converted to the\n5226             target calendar. If 'missing' was None (default), invalid dates in\n5227             the new calendar are dropped, but missing dates are not inserted.\n5228             If `missing` was given, the new data is reindexed to have a time axis\n5229             with the same frequency as the source, but in the new calendar; any\n5230             missing datapoints are filled with `missing`.\n5231 \n5232         Notes\n5233         -----\n5234         Passing a value to `missing` is only usable if the source's time coordinate as an\n5235         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n5236         if the target coordinate, generated from this frequency, has dates equivalent to the\n5237         source. It is usually **not** appropriate to use this mode with:\n5238 \n5239         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n5240         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n5241             or 'mH' where 24 % m != 0).\n5242 \n5243         If one of the source or target calendars is `\"360_day\"`, `align_on` must\n5244         be specified and two options are offered.\n5245 \n5246         - \"year\"\n5247             The dates are translated according to their relative position in the year,\n5248             ignoring their original month and day information, meaning that the\n5249             missing/surplus days are added/removed at regular intervals.\n5250 \n5251             From a `360_day` to a standard calendar, the output will be missing the\n5252             following dates (day of year in parentheses):\n5253 \n5254             To a leap year:\n5255                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n5256                 September 31st (275) and November 30th (335).\n5257             To a non-leap year:\n5258                 February 6th (36), April 19th (109), July 2nd (183),\n5259                 September 12th (255), November 25th (329).\n5260 \n5261             From a standard calendar to a `\"360_day\"`, the following dates in the\n5262             source array will be dropped:\n5263 \n5264             From a leap year:\n5265                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n5266                 September 31st (275), December 1st (336)\n5267             From a non-leap year:\n5268                 February 6th (37), April 20th (110), July 2nd (183),\n5269                 September 13th (256), November 25th (329)\n5270 \n5271             This option is best used on daily and subdaily data.\n5272 \n5273         - \"date\"\n5274             The month/day information is conserved and invalid dates are dropped\n5275             from the output. This means that when converting from a `\"360_day\"` to a\n5276             standard calendar, all 31st (Jan, March, May, July, August, October and\n5277             December) will be missing as there is no equivalent dates in the\n5278             `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n5279             will be dropped as there are no equivalent dates in a standard calendar.\n5280 \n5281             This option is best used with data on a frequency coarser than daily.\n5282         \"\"\"\n5283         return convert_calendar(\n5284             self,\n5285             calendar,\n5286             dim=dim,\n5287             align_on=align_on,\n5288             missing=missing,\n5289             use_cftime=use_cftime,\n5290         )\n5291 \n5292     def interp_calendar(\n5293         self,\n5294         target: pd.DatetimeIndex | CFTimeIndex | DataArray,\n5295         dim: str = \"time\",\n5296     ) -> DataArray:\n5297         \"\"\"Interpolates the DataArray to another calendar based on decimal year measure.\n5298 \n5299         Each timestamp in `source` and `target` are first converted to their decimal\n5300         year equivalent then `source` is interpolated on the target coordinate.\n5301         The decimal year of a timestamp is its year plus its sub-year component\n5302         converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n5303         2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n5304 \n5305         This method should only be used when the time (HH:MM:SS) information of\n5306         time coordinate is not important.\n5307 \n5308         Parameters\n5309         ----------\n5310         target: DataArray or DatetimeIndex or CFTimeIndex\n5311             The target time coordinate of a valid dtype\n5312             (np.datetime64 or cftime objects)\n5313         dim : str\n5314             The time coordinate name.\n5315 \n5316         Return\n5317         ------\n5318         DataArray\n5319             The source interpolated on the decimal years of target,\n5320         \"\"\"\n5321         return interp_calendar(self, target, dim=dim)\n5322 \n5323     def groupby(\n5324         self,\n5325         group: Hashable | DataArray | IndexVariable,\n5326         squeeze: bool = True,\n5327         restore_coord_dims: bool = False,\n5328     ) -> DataArrayGroupBy:\n5329         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n5330 \n5331         Parameters\n5332         ----------\n5333         group : Hashable, DataArray or IndexVariable\n5334             Array whose unique values should be used to group this array. If a\n5335             Hashable, must be the name of a coordinate contained in this dataarray.\n5336         squeeze : bool, default: True\n5337             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n5338             controls whether the subarrays have a dimension of length 1 along\n5339             that dimension or if the dimension is squeezed out.\n5340         restore_coord_dims : bool, default: False\n5341             If True, also restore the dimension order of multi-dimensional\n5342             coordinates.\n5343 \n5344         Returns\n5345         -------\n5346         grouped : DataArrayGroupBy\n5347             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n5348             iterated over in the form of `(unique_value, grouped_array)` pairs.\n5349 \n5350         Examples\n5351         --------\n5352         Calculate daily anomalies for daily data:\n5353 \n5354         >>> da = xr.DataArray(\n5355         ...     np.linspace(0, 1826, num=1827),\n5356         ...     coords=[pd.date_range(\"1/1/2000\", \"31/12/2004\", freq=\"D\")],\n5357         ...     dims=\"time\",\n5358         ... )\n5359         >>> da\n5360         <xarray.DataArray (time: 1827)>\n5361         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,\n5362                1.826e+03])\n5363         Coordinates:\n5364           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n5365         >>> da.groupby(\"time.dayofyear\") - da.groupby(\"time.dayofyear\").mean(\"time\")\n5366         <xarray.DataArray (time: 1827)>\n5367         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n5368         Coordinates:\n5369           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n5370             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366\n5371 \n5372         See Also\n5373         --------\n5374         DataArray.groupby_bins\n5375         Dataset.groupby\n5376         core.groupby.DataArrayGroupBy\n5377         pandas.DataFrame.groupby\n5378         \"\"\"\n5379         from .groupby import DataArrayGroupBy\n5380 \n5381         # While we don't generally check the type of every arg, passing\n5382         # multiple dimensions as multiple arguments is common enough, and the\n5383         # consequences hidden enough (strings evaluate as true) to warrant\n5384         # checking here.\n5385         # A future version could make squeeze kwarg only, but would face\n5386         # backward-compat issues.\n5387         if not isinstance(squeeze, bool):\n5388             raise TypeError(\n5389                 f\"`squeeze` must be True or False, but {squeeze} was supplied\"\n5390             )\n5391 \n5392         return DataArrayGroupBy(\n5393             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims\n5394         )\n5395 \n5396     def groupby_bins(\n5397         self,\n5398         group: Hashable | DataArray | IndexVariable,\n5399         bins: ArrayLike,\n5400         right: bool = True,\n5401         labels: ArrayLike | Literal[False] | None = None,\n5402         precision: int = 3,\n5403         include_lowest: bool = False,\n5404         squeeze: bool = True,\n5405         restore_coord_dims: bool = False,\n5406     ) -> DataArrayGroupBy:\n5407         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n5408 \n5409         Rather than using all unique values of `group`, the values are discretized\n5410         first by applying `pandas.cut` [1]_ to `group`.\n5411 \n5412         Parameters\n5413         ----------\n5414         group : Hashable, DataArray or IndexVariable\n5415             Array whose binned values should be used to group this array. If a\n5416             Hashable, must be the name of a coordinate contained in this dataarray.\n5417         bins : int or array-like\n5418             If bins is an int, it defines the number of equal-width bins in the\n5419             range of x. However, in this case, the range of x is extended by .1%\n5420             on each side to include the min or max values of x. If bins is a\n5421             sequence it defines the bin edges allowing for non-uniform bin\n5422             width. No extension of the range of x is done in this case.\n5423         right : bool, default: True\n5424             Indicates whether the bins include the rightmost edge or not. If\n5425             right == True (the default), then the bins [1,2,3,4] indicate\n5426             (1,2], (2,3], (3,4].\n5427         labels : array-like, False or None, default: None\n5428             Used as labels for the resulting bins. Must be of the same length as\n5429             the resulting bins. If False, string bin labels are assigned by\n5430             `pandas.cut`.\n5431         precision : int, default: 3\n5432             The precision at which to store and display the bins labels.\n5433         include_lowest : bool, default: False\n5434             Whether the first interval should be left-inclusive or not.\n5435         squeeze : bool, default: True\n5436             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n5437             controls whether the subarrays have a dimension of length 1 along\n5438             that dimension or if the dimension is squeezed out.\n5439         restore_coord_dims : bool, default: False\n5440             If True, also restore the dimension order of multi-dimensional\n5441             coordinates.\n5442 \n5443         Returns\n5444         -------\n5445         grouped : DataArrayGroupBy\n5446             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n5447             iterated over in the form of `(unique_value, grouped_array)` pairs.\n5448             The name of the group has the added suffix `_bins` in order to\n5449             distinguish it from the original variable.\n5450 \n5451         See Also\n5452         --------\n5453         DataArray.groupby\n5454         Dataset.groupby_bins\n5455         core.groupby.DataArrayGroupBy\n5456         pandas.DataFrame.groupby\n5457 \n5458         References\n5459         ----------\n5460         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n5461         \"\"\"\n5462         from .groupby import DataArrayGroupBy\n5463 \n5464         return DataArrayGroupBy(\n5465             self,\n5466             group,\n5467             squeeze=squeeze,\n5468             bins=bins,\n5469             restore_coord_dims=restore_coord_dims,\n5470             cut_kwargs={\n5471                 \"right\": right,\n5472                 \"labels\": labels,\n5473                 \"precision\": precision,\n5474                 \"include_lowest\": include_lowest,\n5475             },\n5476         )\n5477 \n5478     def weighted(self, weights: DataArray) -> DataArrayWeighted:\n5479         \"\"\"\n5480         Weighted DataArray operations.\n5481 \n5482         Parameters\n5483         ----------\n5484         weights : DataArray\n5485             An array of weights associated with the values in this Dataset.\n5486             Each value in the data contributes to the reduction operation\n5487             according to its associated weight.\n5488 \n5489         Notes\n5490         -----\n5491         ``weights`` must be a DataArray and cannot contain missing values.\n5492         Missing values can be replaced by ``weights.fillna(0)``.\n5493 \n5494         Returns\n5495         -------\n5496         core.weighted.DataArrayWeighted\n5497 \n5498         See Also\n5499         --------\n5500         Dataset.weighted\n5501         \"\"\"\n5502         from .weighted import DataArrayWeighted\n5503 \n5504         return DataArrayWeighted(self, weights)\n5505 \n5506     def rolling(\n5507         self,\n5508         dim: Mapping[Any, int] | None = None,\n5509         min_periods: int | None = None,\n5510         center: bool | Mapping[Any, bool] = False,\n5511         **window_kwargs: int,\n5512     ) -> DataArrayRolling:\n5513         \"\"\"\n5514         Rolling window object for DataArrays.\n5515 \n5516         Parameters\n5517         ----------\n5518         dim : dict, optional\n5519             Mapping from the dimension name to create the rolling iterator\n5520             along (e.g. `time`) to its moving window size.\n5521         min_periods : int or None, default: None\n5522             Minimum number of observations in window required to have a value\n5523             (otherwise result is NA). The default, None, is equivalent to\n5524             setting min_periods equal to the size of the window.\n5525         center : bool or Mapping to int, default: False\n5526             Set the labels at the center of the window.\n5527         **window_kwargs : optional\n5528             The keyword arguments form of ``dim``.\n5529             One of dim or window_kwargs must be provided.\n5530 \n5531         Returns\n5532         -------\n5533         core.rolling.DataArrayRolling\n5534 \n5535         Examples\n5536         --------\n5537         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n5538 \n5539         >>> da = xr.DataArray(\n5540         ...     np.linspace(0, 11, num=12),\n5541         ...     coords=[\n5542         ...         pd.date_range(\n5543         ...             \"1999-12-15\",\n5544         ...             periods=12,\n5545         ...             freq=pd.DateOffset(months=1),\n5546         ...         )\n5547         ...     ],\n5548         ...     dims=\"time\",\n5549         ... )\n5550         >>> da\n5551         <xarray.DataArray (time: 12)>\n5552         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n5553         Coordinates:\n5554           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n5555         >>> da.rolling(time=3, center=True).mean()\n5556         <xarray.DataArray (time: 12)>\n5557         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n5558         Coordinates:\n5559           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n5560 \n5561         Remove the NaNs using ``dropna()``:\n5562 \n5563         >>> da.rolling(time=3, center=True).mean().dropna(\"time\")\n5564         <xarray.DataArray (time: 10)>\n5565         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n5566         Coordinates:\n5567           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15\n5568 \n5569         See Also\n5570         --------\n5571         core.rolling.DataArrayRolling\n5572         Dataset.rolling\n5573         \"\"\"\n5574         from .rolling import DataArrayRolling\n5575 \n5576         dim = either_dict_or_kwargs(dim, window_kwargs, \"rolling\")\n5577         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)\n5578 \n5579     def coarsen(\n5580         self,\n5581         dim: Mapping[Any, int] | None = None,\n5582         boundary: CoarsenBoundaryOptions = \"exact\",\n5583         side: SideOptions | Mapping[Any, SideOptions] = \"left\",\n5584         coord_func: str | Callable | Mapping[Any, str | Callable] = \"mean\",\n5585         **window_kwargs: int,\n5586     ) -> DataArrayCoarsen:\n5587         \"\"\"\n5588         Coarsen object for DataArrays.\n5589 \n5590         Parameters\n5591         ----------\n5592         dim : mapping of hashable to int, optional\n5593             Mapping from the dimension name to the window size.\n5594         boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n5595             If 'exact', a ValueError will be raised if dimension size is not a\n5596             multiple of the window size. If 'trim', the excess entries are\n5597             dropped. If 'pad', NA will be padded.\n5598         side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n5599         coord_func : str or mapping of hashable to str, default: \"mean\"\n5600             function (name) that is applied to the coordinates,\n5601             or a mapping from coordinate name to function (name).\n5602 \n5603         Returns\n5604         -------\n5605         core.rolling.DataArrayCoarsen\n5606 \n5607         Examples\n5608         --------\n5609         Coarsen the long time series by averaging over every four days.\n5610 \n5611         >>> da = xr.DataArray(\n5612         ...     np.linspace(0, 364, num=364),\n5613         ...     dims=\"time\",\n5614         ...     coords={\"time\": pd.date_range(\"1999-12-15\", periods=364)},\n5615         ... )\n5616         >>> da  # +doctest: ELLIPSIS\n5617         <xarray.DataArray (time: 364)>\n5618         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,\n5619                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,\n5620                  8.02203857,   9.02479339,  10.02754821,  11.03030303,\n5621         ...\n5622                356.98071625, 357.98347107, 358.9862259 , 359.98898072,\n5623                360.99173554, 361.99449036, 362.99724518, 364.        ])\n5624         Coordinates:\n5625           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n5626         >>> da.coarsen(time=3, boundary=\"trim\").mean()  # +doctest: ELLIPSIS\n5627         <xarray.DataArray (time: 121)>\n5628         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,\n5629                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,\n5630                 25.06887052,  28.07713499,  31.08539945,  34.09366391,\n5631         ...\n5632                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,\n5633                361.99449036])\n5634         Coordinates:\n5635           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n5636         >>>\n5637 \n5638         See Also\n5639         --------\n5640         core.rolling.DataArrayCoarsen\n5641         Dataset.coarsen\n5642         \"\"\"\n5643         from .rolling import DataArrayCoarsen\n5644 \n5645         dim = either_dict_or_kwargs(dim, window_kwargs, \"coarsen\")\n5646         return DataArrayCoarsen(\n5647             self,\n5648             dim,\n5649             boundary=boundary,\n5650             side=side,\n5651             coord_func=coord_func,\n5652         )\n5653 \n5654     def resample(\n5655         self,\n5656         indexer: Mapping[Any, str] | None = None,\n5657         skipna: bool | None = None,\n5658         closed: SideOptions | None = None,\n5659         label: SideOptions | None = None,\n5660         base: int = 0,\n5661         keep_attrs: bool | None = None,\n5662         loffset: datetime.timedelta | str | None = None,\n5663         restore_coord_dims: bool | None = None,\n5664         **indexer_kwargs: str,\n5665     ) -> DataArrayResample:\n5666         \"\"\"Returns a Resample object for performing resampling operations.\n5667 \n5668         Handles both downsampling and upsampling. The resampled\n5669         dimension must be a datetime-like coordinate. If any intervals\n5670         contain no values from the original object, they will be given\n5671         the value ``NaN``.\n5672 \n5673         Parameters\n5674         ----------\n5675         indexer : Mapping of Hashable to str, optional\n5676             Mapping from the dimension name to resample frequency [1]_. The\n5677             dimension must be datetime-like.\n5678         skipna : bool, optional\n5679             Whether to skip missing values when aggregating in downsampling.\n5680         closed : {\"left\", \"right\"}, optional\n5681             Side of each interval to treat as closed.\n5682         label : {\"left\", \"right\"}, optional\n5683             Side of each interval to use for labeling.\n5684         base : int, default = 0\n5685             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n5686             aggregated intervals. For example, for \"24H\" frequency, base could\n5687             range from 0 through 23.\n5688         loffset : timedelta or str, optional\n5689             Offset used to adjust the resampled time labels. Some pandas date\n5690             offset strings are supported.\n5691         restore_coord_dims : bool, optional\n5692             If True, also restore the dimension order of multi-dimensional\n5693             coordinates.\n5694         **indexer_kwargs : str\n5695             The keyword arguments form of ``indexer``.\n5696             One of indexer or indexer_kwargs must be provided.\n5697 \n5698         Returns\n5699         -------\n5700         resampled : core.resample.DataArrayResample\n5701             This object resampled.\n5702 \n5703         Examples\n5704         --------\n5705         Downsample monthly time-series data to seasonal data:\n5706 \n5707         >>> da = xr.DataArray(\n5708         ...     np.linspace(0, 11, num=12),\n5709         ...     coords=[\n5710         ...         pd.date_range(\n5711         ...             \"1999-12-15\",\n5712         ...             periods=12,\n5713         ...             freq=pd.DateOffset(months=1),\n5714         ...         )\n5715         ...     ],\n5716         ...     dims=\"time\",\n5717         ... )\n5718         >>> da\n5719         <xarray.DataArray (time: 12)>\n5720         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n5721         Coordinates:\n5722           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n5723         >>> da.resample(time=\"QS-DEC\").mean()\n5724         <xarray.DataArray (time: 4)>\n5725         array([ 1.,  4.,  7., 10.])\n5726         Coordinates:\n5727           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n5728 \n5729         Upsample monthly time-series data to daily data:\n5730 \n5731         >>> da.resample(time=\"1D\").interpolate(\"linear\")  # +doctest: ELLIPSIS\n5732         <xarray.DataArray (time: 337)>\n5733         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,\n5734                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,\n5735                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,\n5736         ...\n5737                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,\n5738                10.96774194, 11.        ])\n5739         Coordinates:\n5740           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n5741 \n5742         Limit scope of upsampling method\n5743 \n5744         >>> da.resample(time=\"1D\").nearest(tolerance=\"1D\")\n5745         <xarray.DataArray (time: 337)>\n5746         array([ 0.,  0., nan, ..., nan, 11., 11.])\n5747         Coordinates:\n5748           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n5749 \n5750         See Also\n5751         --------\n5752         Dataset.resample\n5753         pandas.Series.resample\n5754         pandas.DataFrame.resample\n5755 \n5756         References\n5757         ----------\n5758         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n5759         \"\"\"\n5760         from .resample import DataArrayResample\n5761 \n5762         return self._resample(\n5763             resample_cls=DataArrayResample,\n5764             indexer=indexer,\n5765             skipna=skipna,\n5766             closed=closed,\n5767             label=label,\n5768             base=base,\n5769             keep_attrs=keep_attrs,\n5770             loffset=loffset,\n5771             restore_coord_dims=restore_coord_dims,\n5772             **indexer_kwargs,\n5773         )\n5774 \n5775     # this needs to be at the end, or mypy will confuse with `str`\n5776     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n5777     str = utils.UncachedAccessor(StringAccessor[\"DataArray\"])\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]