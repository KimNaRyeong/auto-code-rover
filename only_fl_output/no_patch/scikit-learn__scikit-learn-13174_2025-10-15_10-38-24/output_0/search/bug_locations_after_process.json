[
    {
        "rel_file_path": "sklearn/ensemble/weight_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/weight_boosting.py",
        "start": 414,
        "end": 430,
        "class_name": "AdaBoostClassifier",
        "method_name": "_validate_estimator",
        "code": "414     def _validate_estimator(self):\n415         \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n416         super()._validate_estimator(\n417             default=DecisionTreeClassifier(max_depth=1))\n418 \n419         #  SAMME-R requires predict_proba-enabled base estimators\n420         if self.algorithm == 'SAMME.R':\n421             if not hasattr(self.base_estimator_, 'predict_proba'):\n422                 raise TypeError(\n423                     \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n424                     \"that the weak learner supports the calculation of class \"\n425                     \"probabilities with a predict_proba method.\\n\"\n426                     \"Please change the base estimator or set \"\n427                     \"algorithm='SAMME' instead.\")\n428         if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\n429             raise ValueError(\"%s doesn't support sample_weight.\"\n430                              % self.base_estimator_.__class__.__name__)\n",
        "intended_behavior": "This method should be updated to either relax the data type validation of X or dynamically adjust validation based on the capabilities of the base estimator. Instead of enforcing X to be an array or sparse matrix of numerics, the validation step should verify if the base estimator can handle the data format of X. If the base estimator can process more versatile data types (e.g., lists of text blobs), _validate_estimator should not restrict this capability."
    },
    {
        "rel_file_path": "sklearn/ensemble/weight_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/weight_boosting.py",
        "start": 295,
        "end": 848,
        "class_name": "AdaBoostClassifier",
        "method_name": null,
        "code": "295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):\n296     \"\"\"An AdaBoost classifier.\n297 \n298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n299     classifier on the original dataset and then fits additional copies of the\n300     classifier on the same dataset but where the weights of incorrectly\n301     classified instances are adjusted such that subsequent classifiers focus\n302     more on difficult cases.\n303 \n304     This class implements the algorithm known as AdaBoost-SAMME [2].\n305 \n306     Read more in the :ref:`User Guide <adaboost>`.\n307 \n308     Parameters\n309     ----------\n310     base_estimator : object, optional (default=None)\n311         The base estimator from which the boosted ensemble is built.\n312         Support for sample weighting is required, as well as proper\n313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``\n315 \n316     n_estimators : integer, optional (default=50)\n317         The maximum number of estimators at which boosting is terminated.\n318         In case of perfect fit, the learning procedure is stopped early.\n319 \n320     learning_rate : float, optional (default=1.)\n321         Learning rate shrinks the contribution of each classifier by\n322         ``learning_rate``. There is a trade-off between ``learning_rate`` and\n323         ``n_estimators``.\n324 \n325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n327         ``base_estimator`` must support calculation of class probabilities.\n328         If 'SAMME' then use the SAMME discrete boosting algorithm.\n329         The SAMME.R algorithm typically converges faster than SAMME,\n330         achieving a lower test error with fewer boosting iterations.\n331 \n332     random_state : int, RandomState instance or None, optional (default=None)\n333         If int, random_state is the seed used by the random number generator;\n334         If RandomState instance, random_state is the random number generator;\n335         If None, the random number generator is the RandomState instance used\n336         by `np.random`.\n337 \n338     Attributes\n339     ----------\n340     estimators_ : list of classifiers\n341         The collection of fitted sub-estimators.\n342 \n343     classes_ : array of shape = [n_classes]\n344         The classes labels.\n345 \n346     n_classes_ : int\n347         The number of classes.\n348 \n349     estimator_weights_ : array of floats\n350         Weights for each estimator in the boosted ensemble.\n351 \n352     estimator_errors_ : array of floats\n353         Classification error for each estimator in the boosted\n354         ensemble.\n355 \n356     feature_importances_ : array of shape = [n_features]\n357         The feature importances if supported by the ``base_estimator``.\n358 \n359     See also\n360     --------\n361     AdaBoostRegressor, GradientBoostingClassifier,\n362     sklearn.tree.DecisionTreeClassifier\n363 \n364     References\n365     ----------\n366     .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n367            on-Line Learning and an Application to Boosting\", 1995.\n368 \n369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n370 \n371     \"\"\"\n372     def __init__(self,\n373                  base_estimator=None,\n374                  n_estimators=50,\n375                  learning_rate=1.,\n376                  algorithm='SAMME.R',\n377                  random_state=None):\n378 \n379         super().__init__(\n380             base_estimator=base_estimator,\n381             n_estimators=n_estimators,\n382             learning_rate=learning_rate,\n383             random_state=random_state)\n384 \n385         self.algorithm = algorithm\n386 \n387     def fit(self, X, y, sample_weight=None):\n388         \"\"\"Build a boosted classifier from the training set (X, y).\n389 \n390         Parameters\n391         ----------\n392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n393             The training input samples. Sparse matrix can be CSC, CSR, COO,\n394             DOK, or LIL. DOK and LIL are converted to CSR.\n395 \n396         y : array-like of shape = [n_samples]\n397             The target values (class labels).\n398 \n399         sample_weight : array-like of shape = [n_samples], optional\n400             Sample weights. If None, the sample weights are initialized to\n401             ``1 / n_samples``.\n402 \n403         Returns\n404         -------\n405         self : object\n406         \"\"\"\n407         # Check that algorithm is supported\n408         if self.algorithm not in ('SAMME', 'SAMME.R'):\n409             raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n410 \n411         # Fit\n412         return super().fit(X, y, sample_weight)\n413 \n414     def _validate_estimator(self):\n415         \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n416         super()._validate_estimator(\n417             default=DecisionTreeClassifier(max_depth=1))\n418 \n419         #  SAMME-R requires predict_proba-enabled base estimators\n420         if self.algorithm == 'SAMME.R':\n421             if not hasattr(self.base_estimator_, 'predict_proba'):\n422                 raise TypeError(\n423                     \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n424                     \"that the weak learner supports the calculation of class \"\n425                     \"probabilities with a predict_proba method.\\n\"\n426                     \"Please change the base estimator or set \"\n427                     \"algorithm='SAMME' instead.\")\n428         if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\n429             raise ValueError(\"%s doesn't support sample_weight.\"\n430                              % self.base_estimator_.__class__.__name__)\n431 \n432     def _boost(self, iboost, X, y, sample_weight, random_state):\n433         \"\"\"Implement a single boost.\n434 \n435         Perform a single boost according to the real multi-class SAMME.R\n436         algorithm or to the discrete SAMME algorithm and return the updated\n437         sample weights.\n438 \n439         Parameters\n440         ----------\n441         iboost : int\n442             The index of the current boost iteration.\n443 \n444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n445             The training input samples. Sparse matrix can be CSC, CSR, COO,\n446             DOK, or LIL. DOK and LIL are converted to CSR.\n447 \n448         y : array-like of shape = [n_samples]\n449             The target values (class labels).\n450 \n451         sample_weight : array-like of shape = [n_samples]\n452             The current sample weights.\n453 \n454         random_state : RandomState\n455             The current random number generator\n456 \n457         Returns\n458         -------\n459         sample_weight : array-like of shape = [n_samples] or None\n460             The reweighted sample weights.\n461             If None then boosting has terminated early.\n462 \n463         estimator_weight : float\n464             The weight for the current boost.\n465             If None then boosting has terminated early.\n466 \n467         estimator_error : float\n468             The classification error for the current boost.\n469             If None then boosting has terminated early.\n470         \"\"\"\n471         if self.algorithm == 'SAMME.R':\n472             return self._boost_real(iboost, X, y, sample_weight, random_state)\n473 \n474         else:  # elif self.algorithm == \"SAMME\":\n475             return self._boost_discrete(iboost, X, y, sample_weight,\n476                                         random_state)\n477 \n478     def _boost_real(self, iboost, X, y, sample_weight, random_state):\n479         \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n480         estimator = self._make_estimator(random_state=random_state)\n481 \n482         estimator.fit(X, y, sample_weight=sample_weight)\n483 \n484         y_predict_proba = estimator.predict_proba(X)\n485 \n486         if iboost == 0:\n487             self.classes_ = getattr(estimator, 'classes_', None)\n488             self.n_classes_ = len(self.classes_)\n489 \n490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\n491                                        axis=0)\n492 \n493         # Instances incorrectly classified\n494         incorrect = y_predict != y\n495 \n496         # Error fraction\n497         estimator_error = np.mean(\n498             np.average(incorrect, weights=sample_weight, axis=0))\n499 \n500         # Stop if classification is perfect\n501         if estimator_error <= 0:\n502             return sample_weight, 1., 0.\n503 \n504         # Construct y coding as described in Zhu et al [2]:\n505         #\n506         #    y_k = 1 if c == k else -1 / (K - 1)\n507         #\n508         # where K == n_classes_ and c, k in [0, K) are indices along the second\n509         # axis of the y coding with c being the index corresponding to the true\n510         # class label.\n511         n_classes = self.n_classes_\n512         classes = self.classes_\n513         y_codes = np.array([-1. / (n_classes - 1), 1.])\n514         y_coding = y_codes.take(classes == y[:, np.newaxis])\n515 \n516         # Displace zero probabilities so the log is defined.\n517         # Also fix negative elements which may occur with\n518         # negative sample weights.\n519         proba = y_predict_proba  # alias for readability\n520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n521 \n522         # Boost weight using multi-class AdaBoost SAMME.R alg\n523         estimator_weight = (-1. * self.learning_rate\n524                             * ((n_classes - 1.) / n_classes)\n525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))\n526 \n527         # Only boost the weights if it will fit again\n528         if not iboost == self.n_estimators - 1:\n529             # Only boost positive weights\n530             sample_weight *= np.exp(estimator_weight *\n531                                     ((sample_weight > 0) |\n532                                      (estimator_weight < 0)))\n533 \n534         return sample_weight, 1., estimator_error\n535 \n536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n537         \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n538         estimator = self._make_estimator(random_state=random_state)\n539 \n540         estimator.fit(X, y, sample_weight=sample_weight)\n541 \n542         y_predict = estimator.predict(X)\n543 \n544         if iboost == 0:\n545             self.classes_ = getattr(estimator, 'classes_', None)\n546             self.n_classes_ = len(self.classes_)\n547 \n548         # Instances incorrectly classified\n549         incorrect = y_predict != y\n550 \n551         # Error fraction\n552         estimator_error = np.mean(\n553             np.average(incorrect, weights=sample_weight, axis=0))\n554 \n555         # Stop if classification is perfect\n556         if estimator_error <= 0:\n557             return sample_weight, 1., 0.\n558 \n559         n_classes = self.n_classes_\n560 \n561         # Stop if the error is at least as bad as random guessing\n562         if estimator_error >= 1. - (1. / n_classes):\n563             self.estimators_.pop(-1)\n564             if len(self.estimators_) == 0:\n565                 raise ValueError('BaseClassifier in AdaBoostClassifier '\n566                                  'ensemble is worse than random, ensemble '\n567                                  'can not be fit.')\n568             return None, None, None\n569 \n570         # Boost weight using multi-class AdaBoost SAMME alg\n571         estimator_weight = self.learning_rate * (\n572             np.log((1. - estimator_error) / estimator_error) +\n573             np.log(n_classes - 1.))\n574 \n575         # Only boost the weights if I will fit again\n576         if not iboost == self.n_estimators - 1:\n577             # Only boost positive weights\n578             sample_weight *= np.exp(estimator_weight * incorrect *\n579                                     ((sample_weight > 0) |\n580                                      (estimator_weight < 0)))\n581 \n582         return sample_weight, estimator_weight, estimator_error\n583 \n584     def predict(self, X):\n585         \"\"\"Predict classes for X.\n586 \n587         The predicted class of an input sample is computed as the weighted mean\n588         prediction of the classifiers in the ensemble.\n589 \n590         Parameters\n591         ----------\n592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n593             The training input samples. Sparse matrix can be CSC, CSR, COO,\n594             DOK, or LIL. DOK and LIL are converted to CSR.\n595 \n596         Returns\n597         -------\n598         y : array of shape = [n_samples]\n599             The predicted classes.\n600         \"\"\"\n601         pred = self.decision_function(X)\n602 \n603         if self.n_classes_ == 2:\n604             return self.classes_.take(pred > 0, axis=0)\n605 \n606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n607 \n608     def staged_predict(self, X):\n609         \"\"\"Return staged predictions for X.\n610 \n611         The predicted class of an input sample is computed as the weighted mean\n612         prediction of the classifiers in the ensemble.\n613 \n614         This generator method yields the ensemble prediction after each\n615         iteration of boosting and therefore allows monitoring, such as to\n616         determine the prediction on a test set after each boost.\n617 \n618         Parameters\n619         ----------\n620         X : array-like of shape = [n_samples, n_features]\n621             The input samples.\n622 \n623         Returns\n624         -------\n625         y : generator of array, shape = [n_samples]\n626             The predicted classes.\n627         \"\"\"\n628         n_classes = self.n_classes_\n629         classes = self.classes_\n630 \n631         if n_classes == 2:\n632             for pred in self.staged_decision_function(X):\n633                 yield np.array(classes.take(pred > 0, axis=0))\n634 \n635         else:\n636             for pred in self.staged_decision_function(X):\n637                 yield np.array(classes.take(\n638                     np.argmax(pred, axis=1), axis=0))\n639 \n640     def decision_function(self, X):\n641         \"\"\"Compute the decision function of ``X``.\n642 \n643         Parameters\n644         ----------\n645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n646             The training input samples. Sparse matrix can be CSC, CSR, COO,\n647             DOK, or LIL. DOK and LIL are converted to CSR.\n648 \n649         Returns\n650         -------\n651         score : array, shape = [n_samples, k]\n652             The decision function of the input samples. The order of\n653             outputs is the same of that of the `classes_` attribute.\n654             Binary classification is a special cases with ``k == 1``,\n655             otherwise ``k==n_classes``. For binary classification,\n656             values closer to -1 or 1 mean more like the first or second\n657             class in ``classes_``, respectively.\n658         \"\"\"\n659         check_is_fitted(self, \"n_classes_\")\n660         X = self._validate_X_predict(X)\n661 \n662         n_classes = self.n_classes_\n663         classes = self.classes_[:, np.newaxis]\n664 \n665         if self.algorithm == 'SAMME.R':\n666             # The weights are all 1. for SAMME.R\n667             pred = sum(_samme_proba(estimator, n_classes, X)\n668                        for estimator in self.estimators_)\n669         else:   # self.algorithm == \"SAMME\"\n670             pred = sum((estimator.predict(X) == classes).T * w\n671                        for estimator, w in zip(self.estimators_,\n672                                                self.estimator_weights_))\n673 \n674         pred /= self.estimator_weights_.sum()\n675         if n_classes == 2:\n676             pred[:, 0] *= -1\n677             return pred.sum(axis=1)\n678         return pred\n679 \n680     def staged_decision_function(self, X):\n681         \"\"\"Compute decision function of ``X`` for each boosting iteration.\n682 \n683         This method allows monitoring (i.e. determine error on testing set)\n684         after each boosting iteration.\n685 \n686         Parameters\n687         ----------\n688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n689             The training input samples. Sparse matrix can be CSC, CSR, COO,\n690             DOK, or LIL. DOK and LIL are converted to CSR.\n691 \n692         Returns\n693         -------\n694         score : generator of array, shape = [n_samples, k]\n695             The decision function of the input samples. The order of\n696             outputs is the same of that of the `classes_` attribute.\n697             Binary classification is a special cases with ``k == 1``,\n698             otherwise ``k==n_classes``. For binary classification,\n699             values closer to -1 or 1 mean more like the first or second\n700             class in ``classes_``, respectively.\n701         \"\"\"\n702         check_is_fitted(self, \"n_classes_\")\n703         X = self._validate_X_predict(X)\n704 \n705         n_classes = self.n_classes_\n706         classes = self.classes_[:, np.newaxis]\n707         pred = None\n708         norm = 0.\n709 \n710         for weight, estimator in zip(self.estimator_weights_,\n711                                      self.estimators_):\n712             norm += weight\n713 \n714             if self.algorithm == 'SAMME.R':\n715                 # The weights are all 1. for SAMME.R\n716                 current_pred = _samme_proba(estimator, n_classes, X)\n717             else:  # elif self.algorithm == \"SAMME\":\n718                 current_pred = estimator.predict(X)\n719                 current_pred = (current_pred == classes).T * weight\n720 \n721             if pred is None:\n722                 pred = current_pred\n723             else:\n724                 pred += current_pred\n725 \n726             if n_classes == 2:\n727                 tmp_pred = np.copy(pred)\n728                 tmp_pred[:, 0] *= -1\n729                 yield (tmp_pred / norm).sum(axis=1)\n730             else:\n731                 yield pred / norm\n732 \n733     def predict_proba(self, X):\n734         \"\"\"Predict class probabilities for X.\n735 \n736         The predicted class probabilities of an input sample is computed as\n737         the weighted mean predicted class probabilities of the classifiers\n738         in the ensemble.\n739 \n740         Parameters\n741         ----------\n742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n743             The training input samples. Sparse matrix can be CSC, CSR, COO,\n744             DOK, or LIL. DOK and LIL are converted to CSR.\n745 \n746         Returns\n747         -------\n748         p : array of shape = [n_samples, n_classes]\n749             The class probabilities of the input samples. The order of\n750             outputs is the same of that of the `classes_` attribute.\n751         \"\"\"\n752         check_is_fitted(self, \"n_classes_\")\n753 \n754         n_classes = self.n_classes_\n755         X = self._validate_X_predict(X)\n756 \n757         if n_classes == 1:\n758             return np.ones((X.shape[0], 1))\n759 \n760         if self.algorithm == 'SAMME.R':\n761             # The weights are all 1. for SAMME.R\n762             proba = sum(_samme_proba(estimator, n_classes, X)\n763                         for estimator in self.estimators_)\n764         else:   # self.algorithm == \"SAMME\"\n765             proba = sum(estimator.predict_proba(X) * w\n766                         for estimator, w in zip(self.estimators_,\n767                                                 self.estimator_weights_))\n768 \n769         proba /= self.estimator_weights_.sum()\n770         proba = np.exp((1. / (n_classes - 1)) * proba)\n771         normalizer = proba.sum(axis=1)[:, np.newaxis]\n772         normalizer[normalizer == 0.0] = 1.0\n773         proba /= normalizer\n774 \n775         return proba\n776 \n777     def staged_predict_proba(self, X):\n778         \"\"\"Predict class probabilities for X.\n779 \n780         The predicted class probabilities of an input sample is computed as\n781         the weighted mean predicted class probabilities of the classifiers\n782         in the ensemble.\n783 \n784         This generator method yields the ensemble predicted class probabilities\n785         after each iteration of boosting and therefore allows monitoring, such\n786         as to determine the predicted class probabilities on a test set after\n787         each boost.\n788 \n789         Parameters\n790         ----------\n791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n792             The training input samples. Sparse matrix can be CSC, CSR, COO,\n793             DOK, or LIL. DOK and LIL are converted to CSR.\n794 \n795         Returns\n796         -------\n797         p : generator of array, shape = [n_samples]\n798             The class probabilities of the input samples. The order of\n799             outputs is the same of that of the `classes_` attribute.\n800         \"\"\"\n801         X = self._validate_X_predict(X)\n802 \n803         n_classes = self.n_classes_\n804         proba = None\n805         norm = 0.\n806 \n807         for weight, estimator in zip(self.estimator_weights_,\n808                                      self.estimators_):\n809             norm += weight\n810 \n811             if self.algorithm == 'SAMME.R':\n812                 # The weights are all 1. for SAMME.R\n813                 current_proba = _samme_proba(estimator, n_classes, X)\n814             else:  # elif self.algorithm == \"SAMME\":\n815                 current_proba = estimator.predict_proba(X) * weight\n816 \n817             if proba is None:\n818                 proba = current_proba\n819             else:\n820                 proba += current_proba\n821 \n822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))\n823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]\n824             normalizer[normalizer == 0.0] = 1.0\n825             real_proba /= normalizer\n826 \n827             yield real_proba\n828 \n829     def predict_log_proba(self, X):\n830         \"\"\"Predict class log-probabilities for X.\n831 \n832         The predicted class log-probabilities of an input sample is computed as\n833         the weighted mean predicted class log-probabilities of the classifiers\n834         in the ensemble.\n835 \n836         Parameters\n837         ----------\n838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n839             The training input samples. Sparse matrix can be CSC, CSR, COO,\n840             DOK, or LIL. DOK and LIL are converted to CSR.\n841 \n842         Returns\n843         -------\n844         p : array of shape = [n_samples, n_classes]\n845             The class probabilities of the input samples. The order of\n846             outputs is the same of that of the `classes_` attribute.\n847         \"\"\"\n848         return np.log(self.predict_proba(X))\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/weight_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/weight_boosting.py",
        "start": 387,
        "end": 412,
        "class_name": "AdaBoostClassifier",
        "method_name": "fit",
        "code": "387     def fit(self, X, y, sample_weight=None):\n388         \"\"\"Build a boosted classifier from the training set (X, y).\n389 \n390         Parameters\n391         ----------\n392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n393             The training input samples. Sparse matrix can be CSC, CSR, COO,\n394             DOK, or LIL. DOK and LIL are converted to CSR.\n395 \n396         y : array-like of shape = [n_samples]\n397             The target values (class labels).\n398 \n399         sample_weight : array-like of shape = [n_samples], optional\n400             Sample weights. If None, the sample weights are initialized to\n401             ``1 / n_samples``.\n402 \n403         Returns\n404         -------\n405         self : object\n406         \"\"\"\n407         # Check that algorithm is supported\n408         if self.algorithm not in ('SAMME', 'SAMME.R'):\n409             raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n410 \n411         # Fit\n412         return super().fit(X, y, sample_weight)\n",
        "intended_behavior": "Modify the fit method to accommodate and check for the flexible handling of different data types by the base estimator. Before validation and feeding X to the base estimator, the fit method should verify that X's format is compatible with the base estimator's requirements. This may involve incorporating a check that adjusts or skips stringent data type validations if the base estimator is known to support more flexible data inputs. The overall functionality of fitting the model should remain intact, with the only change being the more adaptable handling of input data types."
    },
    {
        "rel_file_path": "sklearn/ensemble/weight_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/weight_boosting.py",
        "start": 295,
        "end": 848,
        "class_name": "AdaBoostClassifier",
        "method_name": null,
        "code": "295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):\n296     \"\"\"An AdaBoost classifier.\n297 \n298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n299     classifier on the original dataset and then fits additional copies of the\n300     classifier on the same dataset but where the weights of incorrectly\n301     classified instances are adjusted such that subsequent classifiers focus\n302     more on difficult cases.\n303 \n304     This class implements the algorithm known as AdaBoost-SAMME [2].\n305 \n306     Read more in the :ref:`User Guide <adaboost>`.\n307 \n308     Parameters\n309     ----------\n310     base_estimator : object, optional (default=None)\n311         The base estimator from which the boosted ensemble is built.\n312         Support for sample weighting is required, as well as proper\n313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``\n315 \n316     n_estimators : integer, optional (default=50)\n317         The maximum number of estimators at which boosting is terminated.\n318         In case of perfect fit, the learning procedure is stopped early.\n319 \n320     learning_rate : float, optional (default=1.)\n321         Learning rate shrinks the contribution of each classifier by\n322         ``learning_rate``. There is a trade-off between ``learning_rate`` and\n323         ``n_estimators``.\n324 \n325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')\n326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n327         ``base_estimator`` must support calculation of class probabilities.\n328         If 'SAMME' then use the SAMME discrete boosting algorithm.\n329         The SAMME.R algorithm typically converges faster than SAMME,\n330         achieving a lower test error with fewer boosting iterations.\n331 \n332     random_state : int, RandomState instance or None, optional (default=None)\n333         If int, random_state is the seed used by the random number generator;\n334         If RandomState instance, random_state is the random number generator;\n335         If None, the random number generator is the RandomState instance used\n336         by `np.random`.\n337 \n338     Attributes\n339     ----------\n340     estimators_ : list of classifiers\n341         The collection of fitted sub-estimators.\n342 \n343     classes_ : array of shape = [n_classes]\n344         The classes labels.\n345 \n346     n_classes_ : int\n347         The number of classes.\n348 \n349     estimator_weights_ : array of floats\n350         Weights for each estimator in the boosted ensemble.\n351 \n352     estimator_errors_ : array of floats\n353         Classification error for each estimator in the boosted\n354         ensemble.\n355 \n356     feature_importances_ : array of shape = [n_features]\n357         The feature importances if supported by the ``base_estimator``.\n358 \n359     See also\n360     --------\n361     AdaBoostRegressor, GradientBoostingClassifier,\n362     sklearn.tree.DecisionTreeClassifier\n363 \n364     References\n365     ----------\n366     .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n367            on-Line Learning and an Application to Boosting\", 1995.\n368 \n369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n370 \n371     \"\"\"\n372     def __init__(self,\n373                  base_estimator=None,\n374                  n_estimators=50,\n375                  learning_rate=1.,\n376                  algorithm='SAMME.R',\n377                  random_state=None):\n378 \n379         super().__init__(\n380             base_estimator=base_estimator,\n381             n_estimators=n_estimators,\n382             learning_rate=learning_rate,\n383             random_state=random_state)\n384 \n385         self.algorithm = algorithm\n386 \n387     def fit(self, X, y, sample_weight=None):\n388         \"\"\"Build a boosted classifier from the training set (X, y).\n389 \n390         Parameters\n391         ----------\n392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n393             The training input samples. Sparse matrix can be CSC, CSR, COO,\n394             DOK, or LIL. DOK and LIL are converted to CSR.\n395 \n396         y : array-like of shape = [n_samples]\n397             The target values (class labels).\n398 \n399         sample_weight : array-like of shape = [n_samples], optional\n400             Sample weights. If None, the sample weights are initialized to\n401             ``1 / n_samples``.\n402 \n403         Returns\n404         -------\n405         self : object\n406         \"\"\"\n407         # Check that algorithm is supported\n408         if self.algorithm not in ('SAMME', 'SAMME.R'):\n409             raise ValueError(\"algorithm %s is not supported\" % self.algorithm)\n410 \n411         # Fit\n412         return super().fit(X, y, sample_weight)\n413 \n414     def _validate_estimator(self):\n415         \"\"\"Check the estimator and set the base_estimator_ attribute.\"\"\"\n416         super()._validate_estimator(\n417             default=DecisionTreeClassifier(max_depth=1))\n418 \n419         #  SAMME-R requires predict_proba-enabled base estimators\n420         if self.algorithm == 'SAMME.R':\n421             if not hasattr(self.base_estimator_, 'predict_proba'):\n422                 raise TypeError(\n423                     \"AdaBoostClassifier with algorithm='SAMME.R' requires \"\n424                     \"that the weak learner supports the calculation of class \"\n425                     \"probabilities with a predict_proba method.\\n\"\n426                     \"Please change the base estimator or set \"\n427                     \"algorithm='SAMME' instead.\")\n428         if not has_fit_parameter(self.base_estimator_, \"sample_weight\"):\n429             raise ValueError(\"%s doesn't support sample_weight.\"\n430                              % self.base_estimator_.__class__.__name__)\n431 \n432     def _boost(self, iboost, X, y, sample_weight, random_state):\n433         \"\"\"Implement a single boost.\n434 \n435         Perform a single boost according to the real multi-class SAMME.R\n436         algorithm or to the discrete SAMME algorithm and return the updated\n437         sample weights.\n438 \n439         Parameters\n440         ----------\n441         iboost : int\n442             The index of the current boost iteration.\n443 \n444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n445             The training input samples. Sparse matrix can be CSC, CSR, COO,\n446             DOK, or LIL. DOK and LIL are converted to CSR.\n447 \n448         y : array-like of shape = [n_samples]\n449             The target values (class labels).\n450 \n451         sample_weight : array-like of shape = [n_samples]\n452             The current sample weights.\n453 \n454         random_state : RandomState\n455             The current random number generator\n456 \n457         Returns\n458         -------\n459         sample_weight : array-like of shape = [n_samples] or None\n460             The reweighted sample weights.\n461             If None then boosting has terminated early.\n462 \n463         estimator_weight : float\n464             The weight for the current boost.\n465             If None then boosting has terminated early.\n466 \n467         estimator_error : float\n468             The classification error for the current boost.\n469             If None then boosting has terminated early.\n470         \"\"\"\n471         if self.algorithm == 'SAMME.R':\n472             return self._boost_real(iboost, X, y, sample_weight, random_state)\n473 \n474         else:  # elif self.algorithm == \"SAMME\":\n475             return self._boost_discrete(iboost, X, y, sample_weight,\n476                                         random_state)\n477 \n478     def _boost_real(self, iboost, X, y, sample_weight, random_state):\n479         \"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\n480         estimator = self._make_estimator(random_state=random_state)\n481 \n482         estimator.fit(X, y, sample_weight=sample_weight)\n483 \n484         y_predict_proba = estimator.predict_proba(X)\n485 \n486         if iboost == 0:\n487             self.classes_ = getattr(estimator, 'classes_', None)\n488             self.n_classes_ = len(self.classes_)\n489 \n490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),\n491                                        axis=0)\n492 \n493         # Instances incorrectly classified\n494         incorrect = y_predict != y\n495 \n496         # Error fraction\n497         estimator_error = np.mean(\n498             np.average(incorrect, weights=sample_weight, axis=0))\n499 \n500         # Stop if classification is perfect\n501         if estimator_error <= 0:\n502             return sample_weight, 1., 0.\n503 \n504         # Construct y coding as described in Zhu et al [2]:\n505         #\n506         #    y_k = 1 if c == k else -1 / (K - 1)\n507         #\n508         # where K == n_classes_ and c, k in [0, K) are indices along the second\n509         # axis of the y coding with c being the index corresponding to the true\n510         # class label.\n511         n_classes = self.n_classes_\n512         classes = self.classes_\n513         y_codes = np.array([-1. / (n_classes - 1), 1.])\n514         y_coding = y_codes.take(classes == y[:, np.newaxis])\n515 \n516         # Displace zero probabilities so the log is defined.\n517         # Also fix negative elements which may occur with\n518         # negative sample weights.\n519         proba = y_predict_proba  # alias for readability\n520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)\n521 \n522         # Boost weight using multi-class AdaBoost SAMME.R alg\n523         estimator_weight = (-1. * self.learning_rate\n524                             * ((n_classes - 1.) / n_classes)\n525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))\n526 \n527         # Only boost the weights if it will fit again\n528         if not iboost == self.n_estimators - 1:\n529             # Only boost positive weights\n530             sample_weight *= np.exp(estimator_weight *\n531                                     ((sample_weight > 0) |\n532                                      (estimator_weight < 0)))\n533 \n534         return sample_weight, 1., estimator_error\n535 \n536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):\n537         \"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\n538         estimator = self._make_estimator(random_state=random_state)\n539 \n540         estimator.fit(X, y, sample_weight=sample_weight)\n541 \n542         y_predict = estimator.predict(X)\n543 \n544         if iboost == 0:\n545             self.classes_ = getattr(estimator, 'classes_', None)\n546             self.n_classes_ = len(self.classes_)\n547 \n548         # Instances incorrectly classified\n549         incorrect = y_predict != y\n550 \n551         # Error fraction\n552         estimator_error = np.mean(\n553             np.average(incorrect, weights=sample_weight, axis=0))\n554 \n555         # Stop if classification is perfect\n556         if estimator_error <= 0:\n557             return sample_weight, 1., 0.\n558 \n559         n_classes = self.n_classes_\n560 \n561         # Stop if the error is at least as bad as random guessing\n562         if estimator_error >= 1. - (1. / n_classes):\n563             self.estimators_.pop(-1)\n564             if len(self.estimators_) == 0:\n565                 raise ValueError('BaseClassifier in AdaBoostClassifier '\n566                                  'ensemble is worse than random, ensemble '\n567                                  'can not be fit.')\n568             return None, None, None\n569 \n570         # Boost weight using multi-class AdaBoost SAMME alg\n571         estimator_weight = self.learning_rate * (\n572             np.log((1. - estimator_error) / estimator_error) +\n573             np.log(n_classes - 1.))\n574 \n575         # Only boost the weights if I will fit again\n576         if not iboost == self.n_estimators - 1:\n577             # Only boost positive weights\n578             sample_weight *= np.exp(estimator_weight * incorrect *\n579                                     ((sample_weight > 0) |\n580                                      (estimator_weight < 0)))\n581 \n582         return sample_weight, estimator_weight, estimator_error\n583 \n584     def predict(self, X):\n585         \"\"\"Predict classes for X.\n586 \n587         The predicted class of an input sample is computed as the weighted mean\n588         prediction of the classifiers in the ensemble.\n589 \n590         Parameters\n591         ----------\n592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n593             The training input samples. Sparse matrix can be CSC, CSR, COO,\n594             DOK, or LIL. DOK and LIL are converted to CSR.\n595 \n596         Returns\n597         -------\n598         y : array of shape = [n_samples]\n599             The predicted classes.\n600         \"\"\"\n601         pred = self.decision_function(X)\n602 \n603         if self.n_classes_ == 2:\n604             return self.classes_.take(pred > 0, axis=0)\n605 \n606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)\n607 \n608     def staged_predict(self, X):\n609         \"\"\"Return staged predictions for X.\n610 \n611         The predicted class of an input sample is computed as the weighted mean\n612         prediction of the classifiers in the ensemble.\n613 \n614         This generator method yields the ensemble prediction after each\n615         iteration of boosting and therefore allows monitoring, such as to\n616         determine the prediction on a test set after each boost.\n617 \n618         Parameters\n619         ----------\n620         X : array-like of shape = [n_samples, n_features]\n621             The input samples.\n622 \n623         Returns\n624         -------\n625         y : generator of array, shape = [n_samples]\n626             The predicted classes.\n627         \"\"\"\n628         n_classes = self.n_classes_\n629         classes = self.classes_\n630 \n631         if n_classes == 2:\n632             for pred in self.staged_decision_function(X):\n633                 yield np.array(classes.take(pred > 0, axis=0))\n634 \n635         else:\n636             for pred in self.staged_decision_function(X):\n637                 yield np.array(classes.take(\n638                     np.argmax(pred, axis=1), axis=0))\n639 \n640     def decision_function(self, X):\n641         \"\"\"Compute the decision function of ``X``.\n642 \n643         Parameters\n644         ----------\n645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n646             The training input samples. Sparse matrix can be CSC, CSR, COO,\n647             DOK, or LIL. DOK and LIL are converted to CSR.\n648 \n649         Returns\n650         -------\n651         score : array, shape = [n_samples, k]\n652             The decision function of the input samples. The order of\n653             outputs is the same of that of the `classes_` attribute.\n654             Binary classification is a special cases with ``k == 1``,\n655             otherwise ``k==n_classes``. For binary classification,\n656             values closer to -1 or 1 mean more like the first or second\n657             class in ``classes_``, respectively.\n658         \"\"\"\n659         check_is_fitted(self, \"n_classes_\")\n660         X = self._validate_X_predict(X)\n661 \n662         n_classes = self.n_classes_\n663         classes = self.classes_[:, np.newaxis]\n664 \n665         if self.algorithm == 'SAMME.R':\n666             # The weights are all 1. for SAMME.R\n667             pred = sum(_samme_proba(estimator, n_classes, X)\n668                        for estimator in self.estimators_)\n669         else:   # self.algorithm == \"SAMME\"\n670             pred = sum((estimator.predict(X) == classes).T * w\n671                        for estimator, w in zip(self.estimators_,\n672                                                self.estimator_weights_))\n673 \n674         pred /= self.estimator_weights_.sum()\n675         if n_classes == 2:\n676             pred[:, 0] *= -1\n677             return pred.sum(axis=1)\n678         return pred\n679 \n680     def staged_decision_function(self, X):\n681         \"\"\"Compute decision function of ``X`` for each boosting iteration.\n682 \n683         This method allows monitoring (i.e. determine error on testing set)\n684         after each boosting iteration.\n685 \n686         Parameters\n687         ----------\n688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n689             The training input samples. Sparse matrix can be CSC, CSR, COO,\n690             DOK, or LIL. DOK and LIL are converted to CSR.\n691 \n692         Returns\n693         -------\n694         score : generator of array, shape = [n_samples, k]\n695             The decision function of the input samples. The order of\n696             outputs is the same of that of the `classes_` attribute.\n697             Binary classification is a special cases with ``k == 1``,\n698             otherwise ``k==n_classes``. For binary classification,\n699             values closer to -1 or 1 mean more like the first or second\n700             class in ``classes_``, respectively.\n701         \"\"\"\n702         check_is_fitted(self, \"n_classes_\")\n703         X = self._validate_X_predict(X)\n704 \n705         n_classes = self.n_classes_\n706         classes = self.classes_[:, np.newaxis]\n707         pred = None\n708         norm = 0.\n709 \n710         for weight, estimator in zip(self.estimator_weights_,\n711                                      self.estimators_):\n712             norm += weight\n713 \n714             if self.algorithm == 'SAMME.R':\n715                 # The weights are all 1. for SAMME.R\n716                 current_pred = _samme_proba(estimator, n_classes, X)\n717             else:  # elif self.algorithm == \"SAMME\":\n718                 current_pred = estimator.predict(X)\n719                 current_pred = (current_pred == classes).T * weight\n720 \n721             if pred is None:\n722                 pred = current_pred\n723             else:\n724                 pred += current_pred\n725 \n726             if n_classes == 2:\n727                 tmp_pred = np.copy(pred)\n728                 tmp_pred[:, 0] *= -1\n729                 yield (tmp_pred / norm).sum(axis=1)\n730             else:\n731                 yield pred / norm\n732 \n733     def predict_proba(self, X):\n734         \"\"\"Predict class probabilities for X.\n735 \n736         The predicted class probabilities of an input sample is computed as\n737         the weighted mean predicted class probabilities of the classifiers\n738         in the ensemble.\n739 \n740         Parameters\n741         ----------\n742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n743             The training input samples. Sparse matrix can be CSC, CSR, COO,\n744             DOK, or LIL. DOK and LIL are converted to CSR.\n745 \n746         Returns\n747         -------\n748         p : array of shape = [n_samples, n_classes]\n749             The class probabilities of the input samples. The order of\n750             outputs is the same of that of the `classes_` attribute.\n751         \"\"\"\n752         check_is_fitted(self, \"n_classes_\")\n753 \n754         n_classes = self.n_classes_\n755         X = self._validate_X_predict(X)\n756 \n757         if n_classes == 1:\n758             return np.ones((X.shape[0], 1))\n759 \n760         if self.algorithm == 'SAMME.R':\n761             # The weights are all 1. for SAMME.R\n762             proba = sum(_samme_proba(estimator, n_classes, X)\n763                         for estimator in self.estimators_)\n764         else:   # self.algorithm == \"SAMME\"\n765             proba = sum(estimator.predict_proba(X) * w\n766                         for estimator, w in zip(self.estimators_,\n767                                                 self.estimator_weights_))\n768 \n769         proba /= self.estimator_weights_.sum()\n770         proba = np.exp((1. / (n_classes - 1)) * proba)\n771         normalizer = proba.sum(axis=1)[:, np.newaxis]\n772         normalizer[normalizer == 0.0] = 1.0\n773         proba /= normalizer\n774 \n775         return proba\n776 \n777     def staged_predict_proba(self, X):\n778         \"\"\"Predict class probabilities for X.\n779 \n780         The predicted class probabilities of an input sample is computed as\n781         the weighted mean predicted class probabilities of the classifiers\n782         in the ensemble.\n783 \n784         This generator method yields the ensemble predicted class probabilities\n785         after each iteration of boosting and therefore allows monitoring, such\n786         as to determine the predicted class probabilities on a test set after\n787         each boost.\n788 \n789         Parameters\n790         ----------\n791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n792             The training input samples. Sparse matrix can be CSC, CSR, COO,\n793             DOK, or LIL. DOK and LIL are converted to CSR.\n794 \n795         Returns\n796         -------\n797         p : generator of array, shape = [n_samples]\n798             The class probabilities of the input samples. The order of\n799             outputs is the same of that of the `classes_` attribute.\n800         \"\"\"\n801         X = self._validate_X_predict(X)\n802 \n803         n_classes = self.n_classes_\n804         proba = None\n805         norm = 0.\n806 \n807         for weight, estimator in zip(self.estimator_weights_,\n808                                      self.estimators_):\n809             norm += weight\n810 \n811             if self.algorithm == 'SAMME.R':\n812                 # The weights are all 1. for SAMME.R\n813                 current_proba = _samme_proba(estimator, n_classes, X)\n814             else:  # elif self.algorithm == \"SAMME\":\n815                 current_proba = estimator.predict_proba(X) * weight\n816 \n817             if proba is None:\n818                 proba = current_proba\n819             else:\n820                 proba += current_proba\n821 \n822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))\n823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]\n824             normalizer[normalizer == 0.0] = 1.0\n825             real_proba /= normalizer\n826 \n827             yield real_proba\n828 \n829     def predict_log_proba(self, X):\n830         \"\"\"Predict class log-probabilities for X.\n831 \n832         The predicted class log-probabilities of an input sample is computed as\n833         the weighted mean predicted class log-probabilities of the classifiers\n834         in the ensemble.\n835 \n836         Parameters\n837         ----------\n838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]\n839             The training input samples. Sparse matrix can be CSC, CSR, COO,\n840             DOK, or LIL. DOK and LIL are converted to CSR.\n841 \n842         Returns\n843         -------\n844         p : array of shape = [n_samples, n_classes]\n845             The class probabilities of the input samples. The order of\n846             outputs is the same of that of the `classes_` attribute.\n847         \"\"\"\n848         return np.log(self.predict_proba(X))\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]