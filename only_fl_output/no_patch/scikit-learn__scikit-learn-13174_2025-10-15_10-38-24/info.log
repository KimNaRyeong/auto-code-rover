2025-10-15 10:38:25.004 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 10:38:28.484 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=198, output_tokens=12, cost=0.002340
2025-10-15 10:38:28.485 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 10:38:28.489 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 10:38:42.566 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=506, output_tokens=457, cost=0.018770
2025-10-15 10:38:42.570 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 10:38:46.172 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=811, output_tokens=81, cost=0.010540
2025-10-15 10:38:46.173 | DEBUG    | Extracted a valid json.
2025-10-15 10:38:46.174 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"AdaBoost\")",
        "search_method_in_class(\"fit\", \"AdaBoost\")",
        "search_method_in_class(\"predict\", \"AdaBoost\")",
        "search_code(\"base estimator\")",
        "search_class(\"RandomForest\")",
        "search_class(\"GradientBoosting\")"
    ],
    "bug_locations": []
}
2025-10-15 10:38:46.247 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 10:38:46.248 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 10:39:00.767 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1417, output_tokens=705, cost=0.035320
2025-10-15 10:39:00.774 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 10:39:11.012 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2338, output_tokens=489, cost=0.038050
2025-10-15 10:39:11.023 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 10:39:19.856 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=843, output_tokens=318, cost=0.017970
2025-10-15 10:39:19.858 | DEBUG    | Extracted a valid json.
2025-10-15 10:39:19.858 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code_in_file(\"AdaBoostClassifier\", \"sklearn/ensemble/weight_boosting.py\")",
        "search_code_in_file(\"fit\", \"sklearn/ensemble/weight_boosting.py\")",
        "search_code_in_file(\"predict\", \"sklearn/ensemble/weight_boosting.py\")",
        "search_code_in_file(\"BaseEstimator\", \"sklearn/base.py\")"
    ],
    "bug_locations": [
        {
            "file": "sklearn/ensemble/weight_boosting.py",
            "class": "AdaBoostClassifier",
            "method": "fit, predict, and possibly other methods involved in data validation",
            "intended_behavior": "These methods should include a validation step that's flexible enough to handle various types of data inputs as long as the base estimator can process them. This implies validation of X prior to passing it to the base estimator should check if the base estimator supports the data type or structure of X rather than enforcing a strict numeric array or sparse matrix requirement."
        },
        {
            "file": "sklearn/ensemble/base.py",
            "class": "",
            "method": "",
            "intended_behavior": "Introduce or modify a utility function/mixin method that standardizes the validation process across ensemble models. This function/method should dynamically determine the capabilities of the base estimator regarding the types of data it can process and validate X accordingly. This approach allows ensemble methods to inherit a flexible validation mechanism that respects the base estimator's requirements."
        }
    ]
}
2025-10-15 10:39:19.943 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 10:39:19.943 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 10:39:43.620 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4013, output_tokens=806, cost=0.064310
2025-10-15 10:39:43.628 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 10:39:55.694 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5035, output_tokens=410, cost=0.062650
2025-10-15 10:39:55.701 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 10:40:04.660 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=764, output_tokens=296, cost=0.016520
2025-10-15 10:40:04.661 | DEBUG    | Extracted a valid json.
2025-10-15 10:40:04.661 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/ensemble/weight_boosting.py", 
            "class": "AdaBoostClassifier", 
            "method": "_validate_estimator", 
            "intended_behavior": "This method should be updated to either relax the data type validation of X or dynamically adjust validation based on the capabilities of the base estimator. Instead of enforcing X to be an array or sparse matrix of numerics, the validation step should verify if the base estimator can handle the data format of X. If the base estimator can process more versatile data types (e.g., lists of text blobs), _validate_estimator should not restrict this capability."
        }, 
        {
            "file": "sklearn/ensemble/weight_boosting.py", 
            "class": "AdaBoostClassifier", 
            "method": "fit", 
            "intended_behavior": "Modify the fit method to accommodate and check for the flexible handling of different data types by the base estimator. Before validation and feeding X to the base estimator, the fit method should verify that X's format is compatible with the base estimator's requirements. This may involve incorporating a check that adjusts or skips stringent data type validations if the base estimator is known to support more flexible data inputs. The overall functionality of fitting the model should remain intact, with the only change being the more adaptable handling of input data types."
        }
    ]
}
2025-10-15 10:40:04.665 | DEBUG    | Found override of _validate_estimator in BaseEnsemble
2025-10-15 10:40:04.666 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x74155fd22c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x74146d85f0e0>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x74146fb623e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x74146d85f0e0>, <app.raw_tasks.RawSweTask object at 0x74146d85f110>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x74146fb62480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x74146d59eb40>
    └ <function run_task_in_subprocess at 0x74146fb62660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x74146d59eb40>
    │        │      └ <function run_raw_task at 0x74146fb62700>
    │        └ <function ProcessPoolExecutor.submit at 0x74155f94a020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x74155f949da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x74155f949ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x74155f949f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x74155fb14ea0>
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x74155fb860c0>)>
    │    │        └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-338' parent=45407 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    └ <function Popen._launch at 0x74146db7f7e0>
    └ <multiprocessing.popen_fork.Popen object at 0x74146ee6e660>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x74155fb158a0>
           └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x74155fb14e00>
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x74146fb82390>, <multiprocessing.queues.SimpleQueue object at 0x74146e0864...
    │    │        └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    └ <function _process_worker at 0x74155f9491c0>
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x74155fdad640>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x74147096ec30>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x74155fdad640>
        │         └ <function run_raw_task at 0x74146fb62700>
        └ <concurrent.futures.process._CallItem object at 0x74155fdad640>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output/scikit-learn__scikit-learn-13174_2025-10-15_10-38-24'
             │            │    └ <function RawSweTask.to_task at 0x74146fb616c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x74147096ec30>
             └ <function do_inference at 0x74146fb628e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x74146fdf0400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7414702104a0>
       │                 │         │            └ SweTask(task_id='scikit-learn__scikit-learn-13174', problem_statement='Minimize validation of X in ensembles with a base esti...
       │                 │         └ <app.manage.ProjectApiManager object at 0x74146fb4ba40>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output/scikit-learn__scikit-learn-13174_2025-10-15_10-38-24/output_0')
       └ <function _run_one_task at 0x74146fdf18a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x74146fe9dbc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x74146e5aca70>
                                  └ <app.manage.ProjectApiManager object at 0x74146fb4ba40>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'sklearn/ensemble/weight_boosting.py', 'class': 'AdaBoostClassifier', 'method': '_validate_estimator', 'intended_beh...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x74146fe9d8a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x741470afb9e0>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x74146e5aca70>
    │                 └ <method 'extend' of 'list' objects>
    └ []

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ '_validate_estimator'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    │                      │   └ 'AdaBoostClassifier'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    └ <function SearchBackend._get_inherited_methods at 0x74146fe9d800>
    └ <app.search.search_backend.SearchBackend object at 0x741470afb9e0>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseEnsemble', 'method_name': '_validate_estimator'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x74146fe9cf40>
                                  └ <app.search.search_backend.SearchBackend object at 0x741470afb9e0>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x741470afb9e0>, {'class_name': 'BaseEnsemble', 'method_name': '_validate...
           └ <function SearchBackend.search_method_in_class at 0x74146fe9cea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-15 10:40:04.673 | DEBUG    | Found override of fit in BaseWeightBoosting
2025-10-15 10:40:04.673 | ERROR    | SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
Traceback (most recent call last):

  File "/home/kimnal0/auto-code-rover/app/main.py", line 656, in <module>
    main()
    └ <function main at 0x74155fd22c00>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 120, in main
    run_task_groups(groups, num_processes, organize_output=True)
    │               │       └ 1
    │               └ {'setup_astropy__astropy__4.3': [<app.raw_tasks.RawSweTask object at 0x74146d85f0e0>, <app.raw_tasks.RawSweTask object at 0x7...
    └ <function run_task_groups at 0x74146fb623e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 396, in run_task_groups
    run_tasks_serial(all_tasks)
    │                └ [<app.raw_tasks.RawSweTask object at 0x74146d85f0e0>, <app.raw_tasks.RawSweTask object at 0x74146d85f110>, <app.raw_tasks.Raw...
    └ <function run_tasks_serial at 0x74146fb62480>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 414, in run_tasks_serial
    run_task_in_subprocess(task)
    │                      └ <app.raw_tasks.RawSweTask object at 0x74146d59eb40>
    └ <function run_task_in_subprocess at 0x74146fb62660>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 462, in run_task_in_subprocess
    executor.submit(run_raw_task, task)
    │        │      │             └ <app.raw_tasks.RawSweTask object at 0x74146d59eb40>
    │        │      └ <function run_raw_task at 0x74146fb62700>
    │        └ <function ProcessPoolExecutor.submit at 0x74155f94a020>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>

  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 831, in submit
    self._start_executor_manager_thread()
    │    └ <function ProcessPoolExecutor._start_executor_manager_thread at 0x74155f949da0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 770, in _start_executor_manager_thread
    self._launch_processes()
    │    └ <function ProcessPoolExecutor._launch_processes at 0x74155f949ee0>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 797, in _launch_processes
    self._spawn_process()
    │    └ <function ProcessPoolExecutor._spawn_process at 0x74155f949f80>
    └ <concurrent.futures.process.ProcessPoolExecutor object at 0x74146fd79100>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 807, in _spawn_process
    p.start()
    │ └ <function BaseProcess.start at 0x74155fb14ea0>
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
    │    │        │    │      └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    │        │    └ <staticmethod(<function ForkProcess._Popen at 0x74155fb860c0>)>
    │    │        └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    └ None
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/context.py", line 282, in _Popen
    return Popen(process_obj)
           │     └ <ForkProcess name='ForkProcess-338' parent=45407 started>
           └ <class 'multiprocessing.popen_fork.Popen'>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
    │    │       └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    └ <function Popen._launch at 0x74146db7f7e0>
    └ <multiprocessing.popen_fork.Popen object at 0x74146ee6e660>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/popen_fork.py", line 71, in _launch
    code = process_obj._bootstrap(parent_sentinel=child_r)
           │           │                          └ 13
           │           └ <function BaseProcess._bootstrap at 0x74155fb158a0>
           └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
    │    └ <function BaseProcess.run at 0x74155fb14e00>
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
    │    │        │    │        │    └ {}
    │    │        │    │        └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    │        │    └ (<concurrent.futures.process._SafeQueue object at 0x74146fb82390>, <multiprocessing.queues.SimpleQueue object at 0x74146e0864...
    │    │        └ <ForkProcess name='ForkProcess-338' parent=45407 started>
    │    └ <function _process_worker at 0x74155f9491c0>
    └ <ForkProcess name='ForkProcess-338' parent=45407 started>
  File "/home/kimnal0/miniconda3/envs/auto-code-rover/lib/python3.12/concurrent/futures/process.py", line 263, in _process_worker
    r = call_item.fn(*call_item.args, **call_item.kwargs)
        │         │   │         │       │         └ {}
        │         │   │         │       └ <concurrent.futures.process._CallItem object at 0x74155fdad640>
        │         │   │         └ (<app.raw_tasks.RawSweTask object at 0x74147096ec30>,)
        │         │   └ <concurrent.futures.process._CallItem object at 0x74155fdad640>
        │         └ <function run_raw_task at 0x74146fb62700>
        └ <concurrent.futures.process._CallItem object at 0x74155fdad640>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 494, in run_raw_task
    run_ok = do_inference(task.to_task(), task_output_dir)
             │            │    │          └ '/home/kimnal0/auto-code-rover/only_fl_output/scikit-learn__scikit-learn-13174_2025-10-15_10-38-24'
             │            │    └ <function RawSweTask.to_task at 0x74146fb616c0>
             │            └ <app.raw_tasks.RawSweTask object at 0x74147096ec30>
             └ <function do_inference at 0x74146fb628e0>

  File "/home/kimnal0/auto-code-rover/app/main.py", line 590, in do_inference
    run_ok = inference.run_one_task(
             │         └ <function run_one_task at 0x74146fdf0400>
             └ <module 'app.inference' from '/home/kimnal0/auto-code-rover/app/inference.py'>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 128, in run_one_task
    if _run_one_task(str(out_dir), api_manager, task.get_issue_statement()):
       │                 │         │            │    └ <function SweTask.get_issue_statement at 0x7414702104a0>
       │                 │         │            └ SweTask(task_id='scikit-learn__scikit-learn-13174', problem_statement='Minimize validation of X in ensembles with a base esti...
       │                 │         └ <app.manage.ProjectApiManager object at 0x74146fb4ba40>
       │                 └ Path('/home/kimnal0/auto-code-rover/only_fl_output/scikit-learn__scikit-learn-13174_2025-10-15_10-38-24/output_0')
       └ <function _run_one_task at 0x74146fdf18a0>

  File "/home/kimnal0/auto-code-rover/app/inference.py", line 303, in _run_one_task
    bug_locs, search_msg_thread = api_manager.search_manager.search_iterative(
                                  │           │              └ <function SearchManager.search_iterative at 0x74146fe9dbc0>
                                  │           └ <app.search.search_manage.SearchManager object at 0x74146e5aca70>
                                  └ <app.manage.ProjectApiManager object at 0x74146fb4ba40>

  File "/home/kimnal0/auto-code-rover/app/search/search_manage.py", line 125, in search_iterative
    new_bug_locations.extend(self.backend.get_bug_loc_snippets_new(loc))
    │                 │      │    │       │                        └ {'file': 'sklearn/ensemble/weight_boosting.py', 'class': 'AdaBoostClassifier', 'method': 'fit', 'intended_behavior': "Modify ...
    │                 │      │    │       └ <function SearchBackend.get_bug_loc_snippets_new at 0x74146fe9d8a0>
    │                 │      │    └ <app.search.search_backend.SearchBackend object at 0x741470afb9e0>
    │                 │      └ <app.search.search_manage.SearchManager object at 0x74146e5aca70>
    │                 └ <method 'extend' of 'list' objects>
    └ [<file>sklearn/ensemble/weight_boosting.py</file>
      <class>AdaBoostClassifier</class>
      <method>_validate_estimator</method>
      <cod...

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 825, in get_bug_loc_snippets_new
    self._get_inherited_methods(res.class_name, res.func_name)
    │    │                      │   │           │   └ 'fit'
    │    │                      │   │           └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    │                      │   └ 'AdaBoostClassifier'
    │    │                      └ SearchResult(file_path='/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit...
    │    └ <function SearchBackend._get_inherited_methods at 0x74146fe9d800>
    └ <app.search.search_backend.SearchBackend object at 0x741470afb9e0>

  File "/home/kimnal0/auto-code-rover/app/search/search_backend.py", line 748, in _get_inherited_methods
    output, search_res, call_ok = self.search_method_in_class(super_call)
                                  │    │                      └ {'class_name': 'BaseWeightBoosting', 'method_name': 'fit'}
                                  │    └ <function SearchBackend.search_method_in_class at 0x74146fe9cf40>
                                  └ <app.search.search_backend.SearchBackend object at 0x741470afb9e0>

> File "/home/kimnal0/auto-code-rover/app/utils.py", line 343, in wrapper
    return func(*args, **kwargs)
           │     │       └ {}
           │     └ (<app.search.search_backend.SearchBackend object at 0x741470afb9e0>, {'class_name': 'BaseWeightBoosting', 'method_name': 'fit'})
           └ <function SearchBackend.search_method_in_class at 0x74146fe9cea0>

TypeError: SearchBackend.search_method_in_class() missing 1 required positional argument: 'class_name'
2025-10-15 10:40:04.678 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>_validate_estimator</method>
<code>
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)

</code><intended_behavior>This method should be updated to either relax the data type validation of X or dynamically adjust validation based on the capabilities of the base estimator. Instead of enforcing X to be an array or sparse matrix of numerics, the validation step should verify if the base estimator can handle the data format of X. If the base estimator can process more versatile data types (e.g., lists of text blobs), _validate_estimator should not restrict this capability.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>fit</method>
<code>
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)

</code><intended_behavior>Modify the fit method to accommodate and check for the flexible handling of different data types by the base estimator. Before validation and feeding X to the base estimator, the fit method should verify that X's format is compatible with the base estimator's requirements. This may involve incorporating a check that adjusts or skips stringent data type validations if the base estimator is known to support more flexible data inputs. The overall functionality of fitting the model should remain intact, with the only change being the more adaptable handling of input data types.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 10:40:04.678 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>_validate_estimator</method>
<code>
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)

</code><intended_behavior>This method should be updated to either relax the data type validation of X or dynamically adjust validation based on the capabilities of the base estimator. Instead of enforcing X to be an array or sparse matrix of numerics, the validation step should verify if the base estimator can handle the data format of X. If the base estimator can process more versatile data types (e.g., lists of text blobs), _validate_estimator should not restrict this capability.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>fit</method>
<code>
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)

</code><intended_behavior>Modify the fit method to accommodate and check for the flexible handling of different data types by the base estimator. Before validation and feeding X to the base estimator, the fit method should verify that X's format is compatible with the base estimator's requirements. This may involve incorporating a check that adjusts or skips stringent data type validations if the base estimator is known to support more flexible data inputs. The overall functionality of fitting the model should remain intact, with the only change being the more adaptable handling of input data types.</intended_behavior>, <file>sklearn/ensemble/weight_boosting.py</file>
<class>AdaBoostClassifier</class>
<method>None</method>
<code>
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 10:40:04.678 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 10:40:04.679 | INFO     | Search-only: skipping patch selection
2025-10-15 10:40:04.779 | INFO     | Task scikit-learn__scikit-learn-13174 completed successfully.
2025-10-15 10:40:04.780 | INFO     | No patch generated. You can try running ACR again.
