2025-10-15 11:12:27.007 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 11:12:29.203 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=779, output_tokens=12, cost=0.008150
2025-10-15 11:12:43.380 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=959, output_tokens=563, cost=0.026480
2025-10-15 11:12:44.620 | INFO     | registered non reproducing test 0
2025-10-15 11:12:56.735 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2379, output_tokens=559, cost=0.040560
2025-10-15 11:12:57.783 | INFO     | registered non reproducing test 1
2025-10-15 11:13:12.904 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2375, output_tokens=651, cost=0.043280
2025-10-15 11:13:13.964 | INFO     | registered non reproducing test 2
2025-10-15 11:13:13.964 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 11:13:13.970 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 11:13:18.376 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1078, output_tokens=166, cost=0.015760
2025-10-15 11:13:18.380 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 11:13:21.670 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=520, output_tokens=79, cost=0.007570
2025-10-15 11:13:21.672 | DEBUG    | Extracted a valid json.
2025-10-15 11:13:21.672 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"MLPRegressor\")",
        "search_class(\"MLPClassifier\")",
        "search_method_in_class(\"fit\", \"MLPRegressor\")",
        "search_method_in_class(\"fit\", \"MLPClassifier\")",
        "search_code(\"solver='lbfgs'\")"
    ],
    "bug_locations": []
}
2025-10-15 11:13:21.733 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 11:13:21.733 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 11:13:46.375 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2225, output_tokens=965, cost=0.051200
2025-10-15 11:13:46.392 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 11:13:58.704 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3406, output_tokens=469, cost=0.048130
2025-10-15 11:13:58.714 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 11:14:05.358 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=823, output_tokens=248, cost=0.015670
2025-10-15 11:14:05.360 | DEBUG    | Extracted a valid json.
2025-10-15 11:14:05.360 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method_in_class(\"_fit\", \"BaseMultilayerPerceptron\")",
        "search_code_in_file(\"max_iter\", \"sklearn/neural_network/multilayer_perceptron.py\")"
    ],
    "bug_locations": [
        {
            "file": "sklearn/neural_network/multilayer_perceptron.py",
            "class": "MLPRegressor",
            "method": "",
            "intended_behavior": "The code should correctly interpret the `max_iter` parameter specifically for the L-BFGS solver by ensuring it influences the `maxiter` setting within the solver algorithm, rather than (or in addition to) just `maxfun`."
        },
        {
            "file": "sklearn/neural_network/multilayer_perceptron.py",
            "class": "MLPClassifier",
            "method": "",
            "intended_behavior": "The code should correctly interpret the `max_iter` parameter specifically for the L-BFGS solver by ensuring it influences the `maxiter` setting within the solver algorithm, rather than (or in addition to) just `maxfun`."
        }
    ]
}
2025-10-15 11:14:05.397 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 11:14:05.398 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 11:14:22.677 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4979, output_tokens=707, cost=0.071000
2025-10-15 11:14:22.690 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 11:14:29.918 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5902, output_tokens=265, cost=0.066970
2025-10-15 11:14:29.924 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 11:14:39.785 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=619, output_tokens=267, cost=0.014200
2025-10-15 11:14:39.786 | DEBUG    | Extracted a valid json.
2025-10-15 11:14:39.786 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/neural_network/multilayer_perceptron.py",
            "class": "BaseMultilayerPerceptron",
            "method": "_fit_lbfgs",
            "intended_behavior": "In the `_fit_lbfgs` method, the `max_iter` parameter should be explicitly passed to the L-BFGS solver as `maxiter`, ensuring the solver's iteration limit directly matches the user-specified `max_iter` setting in the `MLPRegressor` or `MLPClassifier`. This involves adjusting the method to correctly configure the L-BFGS optimizer call within scikit-learn, explicitly setting the `maxiter` option based on the model's `max_iter` attribute. This adjustment allows for greater flexibility and control by the user over the training process, ensuring that the maximum number of iterations reflects the parameter's intended usage. The change must maintain the existing functionality for other settings and solvers, focusing solely on correctly applying the `max_iter` parameter when the `lbfgs` solver is selected. This modification resolves the limitation where the number of iterations cannot exceed a default limit, allowing for extended training sessions as required for more complex models or datasets."
        }
    ]
}
2025-10-15 11:14:39.793 | DEBUG    | Bug location extracted successfully: [<file>sklearn/neural_network/multilayer_perceptron.py</file>
<class>BaseMultilayerPerceptron</class>
<method>_fit_lbfgs</method>
<code>
429     def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
430                    intercept_grads, layer_units):
431         # Store meta information for the parameters
432         self._coef_indptr = []
433         self._intercept_indptr = []
434         start = 0
435 
436         # Save sizes and indices of coefficients for faster unpacking
437         for i in range(self.n_layers_ - 1):
438             n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]
439 
440             end = start + (n_fan_in * n_fan_out)
441             self._coef_indptr.append((start, end, (n_fan_in, n_fan_out)))
442             start = end
443 
444         # Save sizes and indices of intercepts for faster unpacking
445         for i in range(self.n_layers_ - 1):
446             end = start + layer_units[i + 1]
447             self._intercept_indptr.append((start, end))
448             start = end
449 
450         # Run LBFGS
451         packed_coef_inter = _pack(self.coefs_,
452                                   self.intercepts_)
453 
454         if self.verbose is True or self.verbose >= 1:
455             iprint = 1
456         else:
457             iprint = -1
458 
459         optimal_parameters, self.loss_, d = fmin_l_bfgs_b(
460             x0=packed_coef_inter,
461             func=self._loss_grad_lbfgs,
462             maxfun=self.max_iter,
463             iprint=iprint,
464             pgtol=self.tol,
465             args=(X, y, activations, deltas, coef_grads, intercept_grads))
466 
467         self._unpack(optimal_parameters)

</code><intended_behavior>In the `_fit_lbfgs` method, the `max_iter` parameter should be explicitly passed to the L-BFGS solver as `maxiter`, ensuring the solver's iteration limit directly matches the user-specified `max_iter` setting in the `MLPRegressor` or `MLPClassifier`. This involves adjusting the method to correctly configure the L-BFGS optimizer call within scikit-learn, explicitly setting the `maxiter` option based on the model's `max_iter` attribute. This adjustment allows for greater flexibility and control by the user over the training process, ensuring that the maximum number of iterations reflects the parameter's intended usage. The change must maintain the existing functionality for other settings and solvers, focusing solely on correctly applying the `max_iter` parameter when the `lbfgs` solver is selected. This modification resolves the limitation where the number of iterations cannot exceed a default limit, allowing for extended training sessions as required for more complex models or datasets.</intended_behavior>, <file>sklearn/neural_network/multilayer_perceptron.py</file>
<class>BaseMultilayerPerceptron</class>
<method>None</method>
<code>
39 class BaseMultilayerPerceptron(BaseEstimator, metaclass=ABCMeta):
40     """Base class for MLP classification and regression.
41 
42     Warning: This class should not be used directly.
43     Use derived classes instead.
44 
45     .. versionadded:: 0.18
46     """
47 
48     @abstractmethod
49     def __init__(self, hidden_layer_sizes, activation, solver,
50                  alpha, batch_size, learning_rate, learning_rate_init, power_t,
51                  max_iter, loss, shuffle, random_state, tol, verbose,
52                  warm_start, momentum, nesterovs_momentum, early_stopping,
53                  validation_fraction, beta_1, beta_2, epsilon,
54                  n_iter_no_change):
55         self.activation = activation
56         self.solver = solver
57         self.alpha = alpha
58         self.batch_size = batch_size
59         self.learning_rate = learning_rate
60         self.learning_rate_init = learning_rate_init
61         self.power_t = power_t
62         self.max_iter = max_iter
63         self.loss = loss
64         self.hidden_layer_sizes = hidden_layer_sizes
65         self.shuffle = shuffle
66         self.random_state = random_state
67         self.tol = tol
68         self.verbose = verbose
69         self.warm_start = warm_start
70         self.momentum = momentum
71         self.nesterovs_momentum = nesterovs_momentum
72         self.early_stopping = early_stopping
73         self.validation_fraction = validation_fraction
74         self.beta_1 = beta_1
75         self.beta_2 = beta_2
76         self.epsilon = epsilon
77         self.n_iter_no_change = n_iter_no_change
78 
79     def _unpack(self, packed_parameters):
80         """Extract the coefficients and intercepts from packed_parameters."""
81         for i in range(self.n_layers_ - 1):
82             start, end, shape = self._coef_indptr[i]
83             self.coefs_[i] = np.reshape(packed_parameters[start:end], shape)
84 
85             start, end = self._intercept_indptr[i]
86             self.intercepts_[i] = packed_parameters[start:end]
87 
88     def _forward_pass(self, activations):
89         """Perform a forward pass on the network by computing the values
90         of the neurons in the hidden layers and the output layer.
91 
92         Parameters
93         ----------
94         activations : list, length = n_layers - 1
95             The ith element of the list holds the values of the ith layer.
96         """
97         hidden_activation = ACTIVATIONS[self.activation]
98         # Iterate over the hidden layers
99         for i in range(self.n_layers_ - 1):
100             activations[i + 1] = safe_sparse_dot(activations[i],
101                                                  self.coefs_[i])
102             activations[i + 1] += self.intercepts_[i]
103 
104             # For the hidden layers
105             if (i + 1) != (self.n_layers_ - 1):
106                 activations[i + 1] = hidden_activation(activations[i + 1])
107 
108         # For the last layer
109         output_activation = ACTIVATIONS[self.out_activation_]
110         activations[i + 1] = output_activation(activations[i + 1])
111 
112         return activations
113 
114     def _compute_loss_grad(self, layer, n_samples, activations, deltas,
115                            coef_grads, intercept_grads):
116         """Compute the gradient of loss with respect to coefs and intercept for
117         specified layer.
118 
119         This function does backpropagation for the specified one layer.
120         """
121         coef_grads[layer] = safe_sparse_dot(activations[layer].T,
122                                             deltas[layer])
123         coef_grads[layer] += (self.alpha * self.coefs_[layer])
124         coef_grads[layer] /= n_samples
125 
126         intercept_grads[layer] = np.mean(deltas[layer], 0)
127 
128         return coef_grads, intercept_grads
129 
130     def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
131                          coef_grads, intercept_grads):
132         """Compute the MLP loss function and its corresponding derivatives
133         with respect to the different parameters given in the initialization.
134 
135         Returned gradients are packed in a single vector so it can be used
136         in lbfgs
137 
138         Parameters
139         ----------
140         packed_coef_inter : array-like
141             A vector comprising the flattened coefficients and intercepts.
142 
143         X : {array-like, sparse matrix}, shape (n_samples, n_features)
144             The input data.
145 
146         y : array-like, shape (n_samples,)
147             The target values.
148 
149         activations : list, length = n_layers - 1
150             The ith element of the list holds the values of the ith layer.
151 
152         deltas : list, length = n_layers - 1
153             The ith element of the list holds the difference between the
154             activations of the i + 1 layer and the backpropagated error.
155             More specifically, deltas are gradients of loss with respect to z
156             in each layer, where z = wx + b is the value of a particular layer
157             before passing through the activation function
158 
159         coef_grads : list, length = n_layers - 1
160             The ith element contains the amount of change used to update the
161             coefficient parameters of the ith layer in an iteration.
162 
163         intercept_grads : list, length = n_layers - 1
164             The ith element contains the amount of change used to update the
165             intercept parameters of the ith layer in an iteration.
166 
167         Returns
168         -------
169         loss : float
170         grad : array-like, shape (number of nodes of all layers,)
171         """
172         self._unpack(packed_coef_inter)
173         loss, coef_grads, intercept_grads = self._backprop(
174             X, y, activations, deltas, coef_grads, intercept_grads)
175         self.n_iter_ += 1
176         grad = _pack(coef_grads, intercept_grads)
177         return loss, grad
178 
179     def _backprop(self, X, y, activations, deltas, coef_grads,
180                   intercept_grads):
181         """Compute the MLP loss function and its corresponding derivatives
182         with respect to each parameter: weights and bias vectors.
183 
184         Parameters
185         ----------
186         X : {array-like, sparse matrix}, shape (n_samples, n_features)
187             The input data.
188 
189         y : array-like, shape (n_samples,)
190             The target values.
191 
192         activations : list, length = n_layers - 1
193              The ith element of the list holds the values of the ith layer.
194 
195         deltas : list, length = n_layers - 1
196             The ith element of the list holds the difference between the
197             activations of the i + 1 layer and the backpropagated error.
198             More specifically, deltas are gradients of loss with respect to z
199             in each layer, where z = wx + b is the value of a particular layer
200             before passing through the activation function
201 
202         coef_grads : list, length = n_layers - 1
203             The ith element contains the amount of change used to update the
204             coefficient parameters of the ith layer in an iteration.
205 
206         intercept_grads : list, length = n_layers - 1
207             The ith element contains the amount of change used to update the
208             intercept parameters of the ith layer in an iteration.
209 
210         Returns
211         -------
212         loss : float
213         coef_grads : list, length = n_layers - 1
214         intercept_grads : list, length = n_layers - 1
215         """
216         n_samples = X.shape[0]
217 
218         # Forward propagate
219         activations = self._forward_pass(activations)
220 
221         # Get loss
222         loss_func_name = self.loss
223         if loss_func_name == 'log_loss' and self.out_activation_ == 'logistic':
224             loss_func_name = 'binary_log_loss'
225         loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])
226         # Add L2 regularization term to loss
227         values = np.sum(
228             np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))
229         loss += (0.5 * self.alpha) * values / n_samples
230 
231         # Backward propagate
232         last = self.n_layers_ - 2
233 
234         # The calculation of delta[last] here works with following
235         # combinations of output activation and loss function:
236         # sigmoid and binary cross entropy, softmax and categorical cross
237         # entropy, and identity with squared loss
238         deltas[last] = activations[-1] - y
239 
240         # Compute gradient for the last layer
241         coef_grads, intercept_grads = self._compute_loss_grad(
242             last, n_samples, activations, deltas, coef_grads, intercept_grads)
243 
244         # Iterate over the hidden layers
245         for i in range(self.n_layers_ - 2, 0, -1):
246             deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)
247             inplace_derivative = DERIVATIVES[self.activation]
248             inplace_derivative(activations[i], deltas[i - 1])
249 
250             coef_grads, intercept_grads = self._compute_loss_grad(
251                 i - 1, n_samples, activations, deltas, coef_grads,
252                 intercept_grads)
253 
254         return loss, coef_grads, intercept_grads
255 
256     def _initialize(self, y, layer_units):
257         # set all attributes, allocate weights etc for first call
258         # Initialize parameters
259         self.n_iter_ = 0
260         self.t_ = 0
261         self.n_outputs_ = y.shape[1]
262 
263         # Compute the number of layers
264         self.n_layers_ = len(layer_units)
265 
266         # Output for regression
267         if not is_classifier(self):
268             self.out_activation_ = 'identity'
269         # Output for multi class
270         elif self._label_binarizer.y_type_ == 'multiclass':
271             self.out_activation_ = 'softmax'
272         # Output for binary class and multi-label
273         else:
274             self.out_activation_ = 'logistic'
275 
276         # Initialize coefficient and intercept layers
277         self.coefs_ = []
278         self.intercepts_ = []
279 
280         for i in range(self.n_layers_ - 1):
281             coef_init, intercept_init = self._init_coef(layer_units[i],
282                                                         layer_units[i + 1])
283             self.coefs_.append(coef_init)
284             self.intercepts_.append(intercept_init)
285 
286         if self.solver in _STOCHASTIC_SOLVERS:
287             self.loss_curve_ = []
288             self._no_improvement_count = 0
289             if self.early_stopping:
290                 self.validation_scores_ = []
291                 self.best_validation_score_ = -np.inf
292             else:
293                 self.best_loss_ = np.inf
294 
295     def _init_coef(self, fan_in, fan_out):
296         # Use the initialization method recommended by
297         # Glorot et al.
298         factor = 6.
299         if self.activation == 'logistic':
300             factor = 2.
301         init_bound = np.sqrt(factor / (fan_in + fan_out))
302 
303         # Generate weights and bias:
304         coef_init = self._random_state.uniform(-init_bound, init_bound,
305                                                (fan_in, fan_out))
306         intercept_init = self._random_state.uniform(-init_bound, init_bound,
307                                                     fan_out)
308         return coef_init, intercept_init
309 
310     def _fit(self, X, y, incremental=False):
311         # Make sure self.hidden_layer_sizes is a list
312         hidden_layer_sizes = self.hidden_layer_sizes
313         if not hasattr(hidden_layer_sizes, "__iter__"):
314             hidden_layer_sizes = [hidden_layer_sizes]
315         hidden_layer_sizes = list(hidden_layer_sizes)
316 
317         # Validate input parameters.
318         self._validate_hyperparameters()
319         if np.any(np.array(hidden_layer_sizes) <= 0):
320             raise ValueError("hidden_layer_sizes must be > 0, got %s." %
321                              hidden_layer_sizes)
322 
323         X, y = self._validate_input(X, y, incremental)
324         n_samples, n_features = X.shape
325 
326         # Ensure y is 2D
327         if y.ndim == 1:
328             y = y.reshape((-1, 1))
329 
330         self.n_outputs_ = y.shape[1]
331 
332         layer_units = ([n_features] + hidden_layer_sizes +
333                        [self.n_outputs_])
334 
335         # check random state
336         self._random_state = check_random_state(self.random_state)
337 
338         if not hasattr(self, 'coefs_') or (not self.warm_start and not
339                                            incremental):
340             # First time training the model
341             self._initialize(y, layer_units)
342 
343         # lbfgs does not support mini-batches
344         if self.solver == 'lbfgs':
345             batch_size = n_samples
346         elif self.batch_size == 'auto':
347             batch_size = min(200, n_samples)
348         else:
349             if self.batch_size < 1 or self.batch_size > n_samples:
350                 warnings.warn("Got `batch_size` less than 1 or larger than "
351                               "sample size. It is going to be clipped")
352             batch_size = np.clip(self.batch_size, 1, n_samples)
353 
354         # Initialize lists
355         activations = [X]
356         activations.extend(np.empty((batch_size, n_fan_out))
357                            for n_fan_out in layer_units[1:])
358         deltas = [np.empty_like(a_layer) for a_layer in activations]
359 
360         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,
361                       n_fan_out_ in zip(layer_units[:-1],
362                                         layer_units[1:])]
363 
364         intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in
365                            layer_units[1:]]
366 
367         # Run the Stochastic optimization solver
368         if self.solver in _STOCHASTIC_SOLVERS:
369             self._fit_stochastic(X, y, activations, deltas, coef_grads,
370                                  intercept_grads, layer_units, incremental)
371 
372         # Run the LBFGS solver
373         elif self.solver == 'lbfgs':
374             self._fit_lbfgs(X, y, activations, deltas, coef_grads,
375                             intercept_grads, layer_units)
376         return self
377 
378     def _validate_hyperparameters(self):
379         if not isinstance(self.shuffle, bool):
380             raise ValueError("shuffle must be either True or False, got %s." %
381                              self.shuffle)
382         if self.max_iter <= 0:
383             raise ValueError("max_iter must be > 0, got %s." % self.max_iter)
384         if self.alpha < 0.0:
385             raise ValueError("alpha must be >= 0, got %s." % self.alpha)
386         if (self.learning_rate in ["constant", "invscaling", "adaptive"] and
387                 self.learning_rate_init <= 0.0):
388             raise ValueError("learning_rate_init must be > 0, got %s." %
389                              self.learning_rate)
390         if self.momentum > 1 or self.momentum < 0:
391             raise ValueError("momentum must be >= 0 and <= 1, got %s" %
392                              self.momentum)
393         if not isinstance(self.nesterovs_momentum, bool):
394             raise ValueError("nesterovs_momentum must be either True or False,"
395                              " got %s." % self.nesterovs_momentum)
396         if not isinstance(self.early_stopping, bool):
397             raise ValueError("early_stopping must be either True or False,"
398                              " got %s." % self.early_stopping)
399         if self.validation_fraction < 0 or self.validation_fraction >= 1:
400             raise ValueError("validation_fraction must be >= 0 and < 1, "
401                              "got %s" % self.validation_fraction)
402         if self.beta_1 < 0 or self.beta_1 >= 1:
403             raise ValueError("beta_1 must be >= 0 and < 1, got %s" %
404                              self.beta_1)
405         if self.beta_2 < 0 or self.beta_2 >= 1:
406             raise ValueError("beta_2 must be >= 0 and < 1, got %s" %
407                              self.beta_2)
408         if self.epsilon <= 0.0:
409             raise ValueError("epsilon must be > 0, got %s." % self.epsilon)
410         if self.n_iter_no_change <= 0:
411             raise ValueError("n_iter_no_change must be > 0, got %s."
412                              % self.n_iter_no_change)
413 
414         # raise ValueError if not registered
415         supported_activations = ('identity', 'logistic', 'tanh', 'relu')
416         if self.activation not in supported_activations:
417             raise ValueError("The activation '%s' is not supported. Supported "
418                              "activations are %s." % (self.activation,
419                                                       supported_activations))
420         if self.learning_rate not in ["constant", "invscaling", "adaptive"]:
421             raise ValueError("learning rate %s is not supported. " %
422                              self.learning_rate)
423         supported_solvers = _STOCHASTIC_SOLVERS + ["lbfgs"]
424         if self.solver not in supported_solvers:
425             raise ValueError("The solver %s is not supported. "
426                              " Expected one of: %s" %
427                              (self.solver, ", ".join(supported_solvers)))
428 
429     def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
430                    intercept_grads, layer_units):
431         # Store meta information for the parameters
432         self._coef_indptr = []
433         self._intercept_indptr = []
434         start = 0
435 
436         # Save sizes and indices of coefficients for faster unpacking
437         for i in range(self.n_layers_ - 1):
438             n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]
439 
440             end = start + (n_fan_in * n_fan_out)
441             self._coef_indptr.append((start, end, (n_fan_in, n_fan_out)))
442             start = end
443 
444         # Save sizes and indices of intercepts for faster unpacking
445         for i in range(self.n_layers_ - 1):
446             end = start + layer_units[i + 1]
447             self._intercept_indptr.append((start, end))
448             start = end
449 
450         # Run LBFGS
451         packed_coef_inter = _pack(self.coefs_,
452                                   self.intercepts_)
453 
454         if self.verbose is True or self.verbose >= 1:
455             iprint = 1
456         else:
457             iprint = -1
458 
459         optimal_parameters, self.loss_, d = fmin_l_bfgs_b(
460             x0=packed_coef_inter,
461             func=self._loss_grad_lbfgs,
462             maxfun=self.max_iter,
463             iprint=iprint,
464             pgtol=self.tol,
465             args=(X, y, activations, deltas, coef_grads, intercept_grads))
466 
467         self._unpack(optimal_parameters)
468 
469     def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
470                         intercept_grads, layer_units, incremental):
471 
472         if not incremental or not hasattr(self, '_optimizer'):
473             params = self.coefs_ + self.intercepts_
474 
475             if self.solver == 'sgd':
476                 self._optimizer = SGDOptimizer(
477                     params, self.learning_rate_init, self.learning_rate,
478                     self.momentum, self.nesterovs_momentum, self.power_t)
479             elif self.solver == 'adam':
480                 self._optimizer = AdamOptimizer(
481                     params, self.learning_rate_init, self.beta_1, self.beta_2,
482                     self.epsilon)
483 
484         # early_stopping in partial_fit doesn't make sense
485         early_stopping = self.early_stopping and not incremental
486         if early_stopping:
487             # don't stratify in multilabel classification
488             should_stratify = is_classifier(self) and self.n_outputs_ == 1
489             stratify = y if should_stratify else None
490             X, X_val, y, y_val = train_test_split(
491                 X, y, random_state=self._random_state,
492                 test_size=self.validation_fraction,
493                 stratify=stratify)
494             if is_classifier(self):
495                 y_val = self._label_binarizer.inverse_transform(y_val)
496         else:
497             X_val = None
498             y_val = None
499 
500         n_samples = X.shape[0]
501 
502         if self.batch_size == 'auto':
503             batch_size = min(200, n_samples)
504         else:
505             batch_size = np.clip(self.batch_size, 1, n_samples)
506 
507         try:
508             for it in range(self.max_iter):
509                 if self.shuffle:
510                     X, y = shuffle(X, y, random_state=self._random_state)
511                 accumulated_loss = 0.0
512                 for batch_slice in gen_batches(n_samples, batch_size):
513                     activations[0] = X[batch_slice]
514                     batch_loss, coef_grads, intercept_grads = self._backprop(
515                         X[batch_slice], y[batch_slice], activations, deltas,
516                         coef_grads, intercept_grads)
517                     accumulated_loss += batch_loss * (batch_slice.stop -
518                                                       batch_slice.start)
519 
520                     # update weights
521                     grads = coef_grads + intercept_grads
522                     self._optimizer.update_params(grads)
523 
524                 self.n_iter_ += 1
525                 self.loss_ = accumulated_loss / X.shape[0]
526 
527                 self.t_ += n_samples
528                 self.loss_curve_.append(self.loss_)
529                 if self.verbose:
530                     print("Iteration %d, loss = %.8f" % (self.n_iter_,
531                                                          self.loss_))
532 
533                 # update no_improvement_count based on training loss or
534                 # validation score according to early_stopping
535                 self._update_no_improvement_count(early_stopping, X_val, y_val)
536 
537                 # for learning rate that needs to be updated at iteration end
538                 self._optimizer.iteration_ends(self.t_)
539 
540                 if self._no_improvement_count > self.n_iter_no_change:
541                     # not better than last `n_iter_no_change` iterations by tol
542                     # stop or decrease learning rate
543                     if early_stopping:
544                         msg = ("Validation score did not improve more than "
545                                "tol=%f for %d consecutive epochs." % (
546                                    self.tol, self.n_iter_no_change))
547                     else:
548                         msg = ("Training loss did not improve more than tol=%f"
549                                " for %d consecutive epochs." % (
550                                    self.tol, self.n_iter_no_change))
551 
552                     is_stopping = self._optimizer.trigger_stopping(
553                         msg, self.verbose)
554                     if is_stopping:
555                         break
556                     else:
557                         self._no_improvement_count = 0
558 
559                 if incremental:
560                     break
561 
562                 if self.n_iter_ == self.max_iter:
563                     warnings.warn(
564                         "Stochastic Optimizer: Maximum iterations (%d) "
565                         "reached and the optimization hasn't converged yet."
566                         % self.max_iter, ConvergenceWarning)
567         except KeyboardInterrupt:
568             warnings.warn("Training interrupted by user.")
569 
570         if early_stopping:
571             # restore best weights
572             self.coefs_ = self._best_coefs
573             self.intercepts_ = self._best_intercepts
574 
575     def _update_no_improvement_count(self, early_stopping, X_val, y_val):
576         if early_stopping:
577             # compute validation score, use that for stopping
578             self.validation_scores_.append(self.score(X_val, y_val))
579 
580             if self.verbose:
581                 print("Validation score: %f" % self.validation_scores_[-1])
582             # update best parameters
583             # use validation_scores_, not loss_curve_
584             # let's hope no-one overloads .score with mse
585             last_valid_score = self.validation_scores_[-1]
586 
587             if last_valid_score < (self.best_validation_score_ +
588                                    self.tol):
589                 self._no_improvement_count += 1
590             else:
591                 self._no_improvement_count = 0
592 
593             if last_valid_score > self.best_validation_score_:
594                 self.best_validation_score_ = last_valid_score
595                 self._best_coefs = [c.copy() for c in self.coefs_]
596                 self._best_intercepts = [i.copy()
597                                          for i in self.intercepts_]
598         else:
599             if self.loss_curve_[-1] > self.best_loss_ - self.tol:
600                 self._no_improvement_count += 1
601             else:
602                 self._no_improvement_count = 0
603             if self.loss_curve_[-1] < self.best_loss_:
604                 self.best_loss_ = self.loss_curve_[-1]
605 
606     def fit(self, X, y):
607         """Fit the model to data matrix X and target(s) y.
608 
609         Parameters
610         ----------
611         X : array-like or sparse matrix, shape (n_samples, n_features)
612             The input data.
613 
614         y : array-like, shape (n_samples,) or (n_samples, n_outputs)
615             The target values (class labels in classification, real numbers in
616             regression).
617 
618         Returns
619         -------
620         self : returns a trained MLP model.
621         """
622         return self._fit(X, y, incremental=False)
623 
624     @property
625     def partial_fit(self):
626         """Update the model with a single iteration over the given data.
627 
628         Parameters
629         ----------
630         X : {array-like, sparse matrix}, shape (n_samples, n_features)
631             The input data.
632 
633         y : array-like, shape (n_samples,)
634             The target values.
635 
636         Returns
637         -------
638         self : returns a trained MLP model.
639         """
640         if self.solver not in _STOCHASTIC_SOLVERS:
641             raise AttributeError("partial_fit is only available for stochastic"
642                                  " optimizers. %s is not stochastic."
643                                  % self.solver)
644         return self._partial_fit
645 
646     def _partial_fit(self, X, y):
647         return self._fit(X, y, incremental=True)
648 
649     def _predict(self, X):
650         """Predict using the trained model
651 
652         Parameters
653         ----------
654         X : {array-like, sparse matrix}, shape (n_samples, n_features)
655             The input data.
656 
657         Returns
658         -------
659         y_pred : array-like, shape (n_samples,) or (n_samples, n_outputs)
660             The decision function of the samples for each class in the model.
661         """
662         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])
663 
664         # Make sure self.hidden_layer_sizes is a list
665         hidden_layer_sizes = self.hidden_layer_sizes
666         if not hasattr(hidden_layer_sizes, "__iter__"):
667             hidden_layer_sizes = [hidden_layer_sizes]
668         hidden_layer_sizes = list(hidden_layer_sizes)
669 
670         layer_units = [X.shape[1]] + hidden_layer_sizes + \
671             [self.n_outputs_]
672 
673         # Initialize layers
674         activations = [X]
675 
676         for i in range(self.n_layers_ - 1):
677             activations.append(np.empty((X.shape[0],
678                                          layer_units[i + 1])))
679         # forward propagate
680         self._forward_pass(activations)
681         y_pred = activations[-1]
682 
683         return y_pred

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 11:14:39.793 | INFO     | Search completed. Bug locations: [<file>sklearn/neural_network/multilayer_perceptron.py</file>
<class>BaseMultilayerPerceptron</class>
<method>_fit_lbfgs</method>
<code>
429     def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
430                    intercept_grads, layer_units):
431         # Store meta information for the parameters
432         self._coef_indptr = []
433         self._intercept_indptr = []
434         start = 0
435 
436         # Save sizes and indices of coefficients for faster unpacking
437         for i in range(self.n_layers_ - 1):
438             n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]
439 
440             end = start + (n_fan_in * n_fan_out)
441             self._coef_indptr.append((start, end, (n_fan_in, n_fan_out)))
442             start = end
443 
444         # Save sizes and indices of intercepts for faster unpacking
445         for i in range(self.n_layers_ - 1):
446             end = start + layer_units[i + 1]
447             self._intercept_indptr.append((start, end))
448             start = end
449 
450         # Run LBFGS
451         packed_coef_inter = _pack(self.coefs_,
452                                   self.intercepts_)
453 
454         if self.verbose is True or self.verbose >= 1:
455             iprint = 1
456         else:
457             iprint = -1
458 
459         optimal_parameters, self.loss_, d = fmin_l_bfgs_b(
460             x0=packed_coef_inter,
461             func=self._loss_grad_lbfgs,
462             maxfun=self.max_iter,
463             iprint=iprint,
464             pgtol=self.tol,
465             args=(X, y, activations, deltas, coef_grads, intercept_grads))
466 
467         self._unpack(optimal_parameters)

</code><intended_behavior>In the `_fit_lbfgs` method, the `max_iter` parameter should be explicitly passed to the L-BFGS solver as `maxiter`, ensuring the solver's iteration limit directly matches the user-specified `max_iter` setting in the `MLPRegressor` or `MLPClassifier`. This involves adjusting the method to correctly configure the L-BFGS optimizer call within scikit-learn, explicitly setting the `maxiter` option based on the model's `max_iter` attribute. This adjustment allows for greater flexibility and control by the user over the training process, ensuring that the maximum number of iterations reflects the parameter's intended usage. The change must maintain the existing functionality for other settings and solvers, focusing solely on correctly applying the `max_iter` parameter when the `lbfgs` solver is selected. This modification resolves the limitation where the number of iterations cannot exceed a default limit, allowing for extended training sessions as required for more complex models or datasets.</intended_behavior>, <file>sklearn/neural_network/multilayer_perceptron.py</file>
<class>BaseMultilayerPerceptron</class>
<method>None</method>
<code>
39 class BaseMultilayerPerceptron(BaseEstimator, metaclass=ABCMeta):
40     """Base class for MLP classification and regression.
41 
42     Warning: This class should not be used directly.
43     Use derived classes instead.
44 
45     .. versionadded:: 0.18
46     """
47 
48     @abstractmethod
49     def __init__(self, hidden_layer_sizes, activation, solver,
50                  alpha, batch_size, learning_rate, learning_rate_init, power_t,
51                  max_iter, loss, shuffle, random_state, tol, verbose,
52                  warm_start, momentum, nesterovs_momentum, early_stopping,
53                  validation_fraction, beta_1, beta_2, epsilon,
54                  n_iter_no_change):
55         self.activation = activation
56         self.solver = solver
57         self.alpha = alpha
58         self.batch_size = batch_size
59         self.learning_rate = learning_rate
60         self.learning_rate_init = learning_rate_init
61         self.power_t = power_t
62         self.max_iter = max_iter
63         self.loss = loss
64         self.hidden_layer_sizes = hidden_layer_sizes
65         self.shuffle = shuffle
66         self.random_state = random_state
67         self.tol = tol
68         self.verbose = verbose
69         self.warm_start = warm_start
70         self.momentum = momentum
71         self.nesterovs_momentum = nesterovs_momentum
72         self.early_stopping = early_stopping
73         self.validation_fraction = validation_fraction
74         self.beta_1 = beta_1
75         self.beta_2 = beta_2
76         self.epsilon = epsilon
77         self.n_iter_no_change = n_iter_no_change
78 
79     def _unpack(self, packed_parameters):
80         """Extract the coefficients and intercepts from packed_parameters."""
81         for i in range(self.n_layers_ - 1):
82             start, end, shape = self._coef_indptr[i]
83             self.coefs_[i] = np.reshape(packed_parameters[start:end], shape)
84 
85             start, end = self._intercept_indptr[i]
86             self.intercepts_[i] = packed_parameters[start:end]
87 
88     def _forward_pass(self, activations):
89         """Perform a forward pass on the network by computing the values
90         of the neurons in the hidden layers and the output layer.
91 
92         Parameters
93         ----------
94         activations : list, length = n_layers - 1
95             The ith element of the list holds the values of the ith layer.
96         """
97         hidden_activation = ACTIVATIONS[self.activation]
98         # Iterate over the hidden layers
99         for i in range(self.n_layers_ - 1):
100             activations[i + 1] = safe_sparse_dot(activations[i],
101                                                  self.coefs_[i])
102             activations[i + 1] += self.intercepts_[i]
103 
104             # For the hidden layers
105             if (i + 1) != (self.n_layers_ - 1):
106                 activations[i + 1] = hidden_activation(activations[i + 1])
107 
108         # For the last layer
109         output_activation = ACTIVATIONS[self.out_activation_]
110         activations[i + 1] = output_activation(activations[i + 1])
111 
112         return activations
113 
114     def _compute_loss_grad(self, layer, n_samples, activations, deltas,
115                            coef_grads, intercept_grads):
116         """Compute the gradient of loss with respect to coefs and intercept for
117         specified layer.
118 
119         This function does backpropagation for the specified one layer.
120         """
121         coef_grads[layer] = safe_sparse_dot(activations[layer].T,
122                                             deltas[layer])
123         coef_grads[layer] += (self.alpha * self.coefs_[layer])
124         coef_grads[layer] /= n_samples
125 
126         intercept_grads[layer] = np.mean(deltas[layer], 0)
127 
128         return coef_grads, intercept_grads
129 
130     def _loss_grad_lbfgs(self, packed_coef_inter, X, y, activations, deltas,
131                          coef_grads, intercept_grads):
132         """Compute the MLP loss function and its corresponding derivatives
133         with respect to the different parameters given in the initialization.
134 
135         Returned gradients are packed in a single vector so it can be used
136         in lbfgs
137 
138         Parameters
139         ----------
140         packed_coef_inter : array-like
141             A vector comprising the flattened coefficients and intercepts.
142 
143         X : {array-like, sparse matrix}, shape (n_samples, n_features)
144             The input data.
145 
146         y : array-like, shape (n_samples,)
147             The target values.
148 
149         activations : list, length = n_layers - 1
150             The ith element of the list holds the values of the ith layer.
151 
152         deltas : list, length = n_layers - 1
153             The ith element of the list holds the difference between the
154             activations of the i + 1 layer and the backpropagated error.
155             More specifically, deltas are gradients of loss with respect to z
156             in each layer, where z = wx + b is the value of a particular layer
157             before passing through the activation function
158 
159         coef_grads : list, length = n_layers - 1
160             The ith element contains the amount of change used to update the
161             coefficient parameters of the ith layer in an iteration.
162 
163         intercept_grads : list, length = n_layers - 1
164             The ith element contains the amount of change used to update the
165             intercept parameters of the ith layer in an iteration.
166 
167         Returns
168         -------
169         loss : float
170         grad : array-like, shape (number of nodes of all layers,)
171         """
172         self._unpack(packed_coef_inter)
173         loss, coef_grads, intercept_grads = self._backprop(
174             X, y, activations, deltas, coef_grads, intercept_grads)
175         self.n_iter_ += 1
176         grad = _pack(coef_grads, intercept_grads)
177         return loss, grad
178 
179     def _backprop(self, X, y, activations, deltas, coef_grads,
180                   intercept_grads):
181         """Compute the MLP loss function and its corresponding derivatives
182         with respect to each parameter: weights and bias vectors.
183 
184         Parameters
185         ----------
186         X : {array-like, sparse matrix}, shape (n_samples, n_features)
187             The input data.
188 
189         y : array-like, shape (n_samples,)
190             The target values.
191 
192         activations : list, length = n_layers - 1
193              The ith element of the list holds the values of the ith layer.
194 
195         deltas : list, length = n_layers - 1
196             The ith element of the list holds the difference between the
197             activations of the i + 1 layer and the backpropagated error.
198             More specifically, deltas are gradients of loss with respect to z
199             in each layer, where z = wx + b is the value of a particular layer
200             before passing through the activation function
201 
202         coef_grads : list, length = n_layers - 1
203             The ith element contains the amount of change used to update the
204             coefficient parameters of the ith layer in an iteration.
205 
206         intercept_grads : list, length = n_layers - 1
207             The ith element contains the amount of change used to update the
208             intercept parameters of the ith layer in an iteration.
209 
210         Returns
211         -------
212         loss : float
213         coef_grads : list, length = n_layers - 1
214         intercept_grads : list, length = n_layers - 1
215         """
216         n_samples = X.shape[0]
217 
218         # Forward propagate
219         activations = self._forward_pass(activations)
220 
221         # Get loss
222         loss_func_name = self.loss
223         if loss_func_name == 'log_loss' and self.out_activation_ == 'logistic':
224             loss_func_name = 'binary_log_loss'
225         loss = LOSS_FUNCTIONS[loss_func_name](y, activations[-1])
226         # Add L2 regularization term to loss
227         values = np.sum(
228             np.array([np.dot(s.ravel(), s.ravel()) for s in self.coefs_]))
229         loss += (0.5 * self.alpha) * values / n_samples
230 
231         # Backward propagate
232         last = self.n_layers_ - 2
233 
234         # The calculation of delta[last] here works with following
235         # combinations of output activation and loss function:
236         # sigmoid and binary cross entropy, softmax and categorical cross
237         # entropy, and identity with squared loss
238         deltas[last] = activations[-1] - y
239 
240         # Compute gradient for the last layer
241         coef_grads, intercept_grads = self._compute_loss_grad(
242             last, n_samples, activations, deltas, coef_grads, intercept_grads)
243 
244         # Iterate over the hidden layers
245         for i in range(self.n_layers_ - 2, 0, -1):
246             deltas[i - 1] = safe_sparse_dot(deltas[i], self.coefs_[i].T)
247             inplace_derivative = DERIVATIVES[self.activation]
248             inplace_derivative(activations[i], deltas[i - 1])
249 
250             coef_grads, intercept_grads = self._compute_loss_grad(
251                 i - 1, n_samples, activations, deltas, coef_grads,
252                 intercept_grads)
253 
254         return loss, coef_grads, intercept_grads
255 
256     def _initialize(self, y, layer_units):
257         # set all attributes, allocate weights etc for first call
258         # Initialize parameters
259         self.n_iter_ = 0
260         self.t_ = 0
261         self.n_outputs_ = y.shape[1]
262 
263         # Compute the number of layers
264         self.n_layers_ = len(layer_units)
265 
266         # Output for regression
267         if not is_classifier(self):
268             self.out_activation_ = 'identity'
269         # Output for multi class
270         elif self._label_binarizer.y_type_ == 'multiclass':
271             self.out_activation_ = 'softmax'
272         # Output for binary class and multi-label
273         else:
274             self.out_activation_ = 'logistic'
275 
276         # Initialize coefficient and intercept layers
277         self.coefs_ = []
278         self.intercepts_ = []
279 
280         for i in range(self.n_layers_ - 1):
281             coef_init, intercept_init = self._init_coef(layer_units[i],
282                                                         layer_units[i + 1])
283             self.coefs_.append(coef_init)
284             self.intercepts_.append(intercept_init)
285 
286         if self.solver in _STOCHASTIC_SOLVERS:
287             self.loss_curve_ = []
288             self._no_improvement_count = 0
289             if self.early_stopping:
290                 self.validation_scores_ = []
291                 self.best_validation_score_ = -np.inf
292             else:
293                 self.best_loss_ = np.inf
294 
295     def _init_coef(self, fan_in, fan_out):
296         # Use the initialization method recommended by
297         # Glorot et al.
298         factor = 6.
299         if self.activation == 'logistic':
300             factor = 2.
301         init_bound = np.sqrt(factor / (fan_in + fan_out))
302 
303         # Generate weights and bias:
304         coef_init = self._random_state.uniform(-init_bound, init_bound,
305                                                (fan_in, fan_out))
306         intercept_init = self._random_state.uniform(-init_bound, init_bound,
307                                                     fan_out)
308         return coef_init, intercept_init
309 
310     def _fit(self, X, y, incremental=False):
311         # Make sure self.hidden_layer_sizes is a list
312         hidden_layer_sizes = self.hidden_layer_sizes
313         if not hasattr(hidden_layer_sizes, "__iter__"):
314             hidden_layer_sizes = [hidden_layer_sizes]
315         hidden_layer_sizes = list(hidden_layer_sizes)
316 
317         # Validate input parameters.
318         self._validate_hyperparameters()
319         if np.any(np.array(hidden_layer_sizes) <= 0):
320             raise ValueError("hidden_layer_sizes must be > 0, got %s." %
321                              hidden_layer_sizes)
322 
323         X, y = self._validate_input(X, y, incremental)
324         n_samples, n_features = X.shape
325 
326         # Ensure y is 2D
327         if y.ndim == 1:
328             y = y.reshape((-1, 1))
329 
330         self.n_outputs_ = y.shape[1]
331 
332         layer_units = ([n_features] + hidden_layer_sizes +
333                        [self.n_outputs_])
334 
335         # check random state
336         self._random_state = check_random_state(self.random_state)
337 
338         if not hasattr(self, 'coefs_') or (not self.warm_start and not
339                                            incremental):
340             # First time training the model
341             self._initialize(y, layer_units)
342 
343         # lbfgs does not support mini-batches
344         if self.solver == 'lbfgs':
345             batch_size = n_samples
346         elif self.batch_size == 'auto':
347             batch_size = min(200, n_samples)
348         else:
349             if self.batch_size < 1 or self.batch_size > n_samples:
350                 warnings.warn("Got `batch_size` less than 1 or larger than "
351                               "sample size. It is going to be clipped")
352             batch_size = np.clip(self.batch_size, 1, n_samples)
353 
354         # Initialize lists
355         activations = [X]
356         activations.extend(np.empty((batch_size, n_fan_out))
357                            for n_fan_out in layer_units[1:])
358         deltas = [np.empty_like(a_layer) for a_layer in activations]
359 
360         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,
361                       n_fan_out_ in zip(layer_units[:-1],
362                                         layer_units[1:])]
363 
364         intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in
365                            layer_units[1:]]
366 
367         # Run the Stochastic optimization solver
368         if self.solver in _STOCHASTIC_SOLVERS:
369             self._fit_stochastic(X, y, activations, deltas, coef_grads,
370                                  intercept_grads, layer_units, incremental)
371 
372         # Run the LBFGS solver
373         elif self.solver == 'lbfgs':
374             self._fit_lbfgs(X, y, activations, deltas, coef_grads,
375                             intercept_grads, layer_units)
376         return self
377 
378     def _validate_hyperparameters(self):
379         if not isinstance(self.shuffle, bool):
380             raise ValueError("shuffle must be either True or False, got %s." %
381                              self.shuffle)
382         if self.max_iter <= 0:
383             raise ValueError("max_iter must be > 0, got %s." % self.max_iter)
384         if self.alpha < 0.0:
385             raise ValueError("alpha must be >= 0, got %s." % self.alpha)
386         if (self.learning_rate in ["constant", "invscaling", "adaptive"] and
387                 self.learning_rate_init <= 0.0):
388             raise ValueError("learning_rate_init must be > 0, got %s." %
389                              self.learning_rate)
390         if self.momentum > 1 or self.momentum < 0:
391             raise ValueError("momentum must be >= 0 and <= 1, got %s" %
392                              self.momentum)
393         if not isinstance(self.nesterovs_momentum, bool):
394             raise ValueError("nesterovs_momentum must be either True or False,"
395                              " got %s." % self.nesterovs_momentum)
396         if not isinstance(self.early_stopping, bool):
397             raise ValueError("early_stopping must be either True or False,"
398                              " got %s." % self.early_stopping)
399         if self.validation_fraction < 0 or self.validation_fraction >= 1:
400             raise ValueError("validation_fraction must be >= 0 and < 1, "
401                              "got %s" % self.validation_fraction)
402         if self.beta_1 < 0 or self.beta_1 >= 1:
403             raise ValueError("beta_1 must be >= 0 and < 1, got %s" %
404                              self.beta_1)
405         if self.beta_2 < 0 or self.beta_2 >= 1:
406             raise ValueError("beta_2 must be >= 0 and < 1, got %s" %
407                              self.beta_2)
408         if self.epsilon <= 0.0:
409             raise ValueError("epsilon must be > 0, got %s." % self.epsilon)
410         if self.n_iter_no_change <= 0:
411             raise ValueError("n_iter_no_change must be > 0, got %s."
412                              % self.n_iter_no_change)
413 
414         # raise ValueError if not registered
415         supported_activations = ('identity', 'logistic', 'tanh', 'relu')
416         if self.activation not in supported_activations:
417             raise ValueError("The activation '%s' is not supported. Supported "
418                              "activations are %s." % (self.activation,
419                                                       supported_activations))
420         if self.learning_rate not in ["constant", "invscaling", "adaptive"]:
421             raise ValueError("learning rate %s is not supported. " %
422                              self.learning_rate)
423         supported_solvers = _STOCHASTIC_SOLVERS + ["lbfgs"]
424         if self.solver not in supported_solvers:
425             raise ValueError("The solver %s is not supported. "
426                              " Expected one of: %s" %
427                              (self.solver, ", ".join(supported_solvers)))
428 
429     def _fit_lbfgs(self, X, y, activations, deltas, coef_grads,
430                    intercept_grads, layer_units):
431         # Store meta information for the parameters
432         self._coef_indptr = []
433         self._intercept_indptr = []
434         start = 0
435 
436         # Save sizes and indices of coefficients for faster unpacking
437         for i in range(self.n_layers_ - 1):
438             n_fan_in, n_fan_out = layer_units[i], layer_units[i + 1]
439 
440             end = start + (n_fan_in * n_fan_out)
441             self._coef_indptr.append((start, end, (n_fan_in, n_fan_out)))
442             start = end
443 
444         # Save sizes and indices of intercepts for faster unpacking
445         for i in range(self.n_layers_ - 1):
446             end = start + layer_units[i + 1]
447             self._intercept_indptr.append((start, end))
448             start = end
449 
450         # Run LBFGS
451         packed_coef_inter = _pack(self.coefs_,
452                                   self.intercepts_)
453 
454         if self.verbose is True or self.verbose >= 1:
455             iprint = 1
456         else:
457             iprint = -1
458 
459         optimal_parameters, self.loss_, d = fmin_l_bfgs_b(
460             x0=packed_coef_inter,
461             func=self._loss_grad_lbfgs,
462             maxfun=self.max_iter,
463             iprint=iprint,
464             pgtol=self.tol,
465             args=(X, y, activations, deltas, coef_grads, intercept_grads))
466 
467         self._unpack(optimal_parameters)
468 
469     def _fit_stochastic(self, X, y, activations, deltas, coef_grads,
470                         intercept_grads, layer_units, incremental):
471 
472         if not incremental or not hasattr(self, '_optimizer'):
473             params = self.coefs_ + self.intercepts_
474 
475             if self.solver == 'sgd':
476                 self._optimizer = SGDOptimizer(
477                     params, self.learning_rate_init, self.learning_rate,
478                     self.momentum, self.nesterovs_momentum, self.power_t)
479             elif self.solver == 'adam':
480                 self._optimizer = AdamOptimizer(
481                     params, self.learning_rate_init, self.beta_1, self.beta_2,
482                     self.epsilon)
483 
484         # early_stopping in partial_fit doesn't make sense
485         early_stopping = self.early_stopping and not incremental
486         if early_stopping:
487             # don't stratify in multilabel classification
488             should_stratify = is_classifier(self) and self.n_outputs_ == 1
489             stratify = y if should_stratify else None
490             X, X_val, y, y_val = train_test_split(
491                 X, y, random_state=self._random_state,
492                 test_size=self.validation_fraction,
493                 stratify=stratify)
494             if is_classifier(self):
495                 y_val = self._label_binarizer.inverse_transform(y_val)
496         else:
497             X_val = None
498             y_val = None
499 
500         n_samples = X.shape[0]
501 
502         if self.batch_size == 'auto':
503             batch_size = min(200, n_samples)
504         else:
505             batch_size = np.clip(self.batch_size, 1, n_samples)
506 
507         try:
508             for it in range(self.max_iter):
509                 if self.shuffle:
510                     X, y = shuffle(X, y, random_state=self._random_state)
511                 accumulated_loss = 0.0
512                 for batch_slice in gen_batches(n_samples, batch_size):
513                     activations[0] = X[batch_slice]
514                     batch_loss, coef_grads, intercept_grads = self._backprop(
515                         X[batch_slice], y[batch_slice], activations, deltas,
516                         coef_grads, intercept_grads)
517                     accumulated_loss += batch_loss * (batch_slice.stop -
518                                                       batch_slice.start)
519 
520                     # update weights
521                     grads = coef_grads + intercept_grads
522                     self._optimizer.update_params(grads)
523 
524                 self.n_iter_ += 1
525                 self.loss_ = accumulated_loss / X.shape[0]
526 
527                 self.t_ += n_samples
528                 self.loss_curve_.append(self.loss_)
529                 if self.verbose:
530                     print("Iteration %d, loss = %.8f" % (self.n_iter_,
531                                                          self.loss_))
532 
533                 # update no_improvement_count based on training loss or
534                 # validation score according to early_stopping
535                 self._update_no_improvement_count(early_stopping, X_val, y_val)
536 
537                 # for learning rate that needs to be updated at iteration end
538                 self._optimizer.iteration_ends(self.t_)
539 
540                 if self._no_improvement_count > self.n_iter_no_change:
541                     # not better than last `n_iter_no_change` iterations by tol
542                     # stop or decrease learning rate
543                     if early_stopping:
544                         msg = ("Validation score did not improve more than "
545                                "tol=%f for %d consecutive epochs." % (
546                                    self.tol, self.n_iter_no_change))
547                     else:
548                         msg = ("Training loss did not improve more than tol=%f"
549                                " for %d consecutive epochs." % (
550                                    self.tol, self.n_iter_no_change))
551 
552                     is_stopping = self._optimizer.trigger_stopping(
553                         msg, self.verbose)
554                     if is_stopping:
555                         break
556                     else:
557                         self._no_improvement_count = 0
558 
559                 if incremental:
560                     break
561 
562                 if self.n_iter_ == self.max_iter:
563                     warnings.warn(
564                         "Stochastic Optimizer: Maximum iterations (%d) "
565                         "reached and the optimization hasn't converged yet."
566                         % self.max_iter, ConvergenceWarning)
567         except KeyboardInterrupt:
568             warnings.warn("Training interrupted by user.")
569 
570         if early_stopping:
571             # restore best weights
572             self.coefs_ = self._best_coefs
573             self.intercepts_ = self._best_intercepts
574 
575     def _update_no_improvement_count(self, early_stopping, X_val, y_val):
576         if early_stopping:
577             # compute validation score, use that for stopping
578             self.validation_scores_.append(self.score(X_val, y_val))
579 
580             if self.verbose:
581                 print("Validation score: %f" % self.validation_scores_[-1])
582             # update best parameters
583             # use validation_scores_, not loss_curve_
584             # let's hope no-one overloads .score with mse
585             last_valid_score = self.validation_scores_[-1]
586 
587             if last_valid_score < (self.best_validation_score_ +
588                                    self.tol):
589                 self._no_improvement_count += 1
590             else:
591                 self._no_improvement_count = 0
592 
593             if last_valid_score > self.best_validation_score_:
594                 self.best_validation_score_ = last_valid_score
595                 self._best_coefs = [c.copy() for c in self.coefs_]
596                 self._best_intercepts = [i.copy()
597                                          for i in self.intercepts_]
598         else:
599             if self.loss_curve_[-1] > self.best_loss_ - self.tol:
600                 self._no_improvement_count += 1
601             else:
602                 self._no_improvement_count = 0
603             if self.loss_curve_[-1] < self.best_loss_:
604                 self.best_loss_ = self.loss_curve_[-1]
605 
606     def fit(self, X, y):
607         """Fit the model to data matrix X and target(s) y.
608 
609         Parameters
610         ----------
611         X : array-like or sparse matrix, shape (n_samples, n_features)
612             The input data.
613 
614         y : array-like, shape (n_samples,) or (n_samples, n_outputs)
615             The target values (class labels in classification, real numbers in
616             regression).
617 
618         Returns
619         -------
620         self : returns a trained MLP model.
621         """
622         return self._fit(X, y, incremental=False)
623 
624     @property
625     def partial_fit(self):
626         """Update the model with a single iteration over the given data.
627 
628         Parameters
629         ----------
630         X : {array-like, sparse matrix}, shape (n_samples, n_features)
631             The input data.
632 
633         y : array-like, shape (n_samples,)
634             The target values.
635 
636         Returns
637         -------
638         self : returns a trained MLP model.
639         """
640         if self.solver not in _STOCHASTIC_SOLVERS:
641             raise AttributeError("partial_fit is only available for stochastic"
642                                  " optimizers. %s is not stochastic."
643                                  % self.solver)
644         return self._partial_fit
645 
646     def _partial_fit(self, X, y):
647         return self._fit(X, y, incremental=True)
648 
649     def _predict(self, X):
650         """Predict using the trained model
651 
652         Parameters
653         ----------
654         X : {array-like, sparse matrix}, shape (n_samples, n_features)
655             The input data.
656 
657         Returns
658         -------
659         y_pred : array-like, shape (n_samples,) or (n_samples, n_outputs)
660             The decision function of the samples for each class in the model.
661         """
662         X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])
663 
664         # Make sure self.hidden_layer_sizes is a list
665         hidden_layer_sizes = self.hidden_layer_sizes
666         if not hasattr(hidden_layer_sizes, "__iter__"):
667             hidden_layer_sizes = [hidden_layer_sizes]
668         hidden_layer_sizes = list(hidden_layer_sizes)
669 
670         layer_units = [X.shape[1]] + hidden_layer_sizes + \
671             [self.n_outputs_]
672 
673         # Initialize layers
674         activations = [X]
675 
676         for i in range(self.n_layers_ - 1):
677             activations.append(np.empty((X.shape[0],
678                                          layer_units[i + 1])))
679         # forward propagate
680         self._forward_pass(activations)
681         y_pred = activations[-1]
682 
683         return y_pred

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 11:14:39.794 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 11:14:39.794 | INFO     | Search-only: skipping patch selection
2025-10-15 11:14:39.862 | INFO     | Task scikit-learn__scikit-learn-9274 completed successfully.
2025-10-15 11:14:39.863 | INFO     | No patch generated. You can try running ACR again.
