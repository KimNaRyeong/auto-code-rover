[
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 1254,
        "end": 1332,
        "class_name": "DataArray",
        "method_name": "chunk",
        "code": "1254     def chunk(\n1255         self: T_DataArray,\n1256         chunks: (\n1257             int\n1258             | Literal[\"auto\"]\n1259             | tuple[int, ...]\n1260             | tuple[tuple[int, ...], ...]\n1261             | Mapping[Any, None | int | tuple[int, ...]]\n1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n1263         name_prefix: str = \"xarray-\",\n1264         token: str | None = None,\n1265         lock: bool = False,\n1266         inline_array: bool = False,\n1267         **chunks_kwargs: Any,\n1268     ) -> T_DataArray:\n1269         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n1270 \n1271         If this variable is a non-dask array, it will be converted to dask\n1272         array. If it's a dask array, it will be rechunked to the given chunk\n1273         sizes.\n1274 \n1275         If neither chunks is not provided for one or more dimensions, chunk\n1276         sizes along that dimension will not be updated; non-dask arrays will be\n1277         converted into dask arrays with a single block.\n1278 \n1279         Parameters\n1280         ----------\n1281         chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n1282             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n1283             ``{\"x\": 5, \"y\": 5}``.\n1284         name_prefix : str, optional\n1285             Prefix for the name of the new dask array.\n1286         token : str, optional\n1287             Token uniquely identifying this array.\n1288         lock : optional\n1289             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1290             already as dask array.\n1291         inline_array: optional\n1292             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1293             already as dask array.\n1294         **chunks_kwargs : {dim: chunks, ...}, optional\n1295             The keyword arguments form of ``chunks``.\n1296             One of chunks or chunks_kwargs must be provided.\n1297 \n1298         Returns\n1299         -------\n1300         chunked : xarray.DataArray\n1301 \n1302         See Also\n1303         --------\n1304         DataArray.chunks\n1305         DataArray.chunksizes\n1306         xarray.unify_chunks\n1307         dask.array.from_array\n1308         \"\"\"\n1309         if chunks is None:\n1310             warnings.warn(\n1311                 \"None value for 'chunks' is deprecated. \"\n1312                 \"It will raise an error in the future. Use instead '{}'\",\n1313                 category=FutureWarning,\n1314             )\n1315             chunks = {}\n1316 \n1317         if isinstance(chunks, (float, str, int)):\n1318             # ignoring type; unclear why it won't accept a Literal into the value.\n1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore\n1320         elif isinstance(chunks, (tuple, list)):\n1321             chunks = dict(zip(self.dims, chunks))\n1322         else:\n1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n1324 \n1325         ds = self._to_temp_dataset().chunk(\n1326             chunks,\n1327             name_prefix=name_prefix,\n1328             token=token,\n1329             lock=lock,\n1330             inline_array=inline_array,\n1331         )\n1332         return self._from_temp_dataset(ds)\n",
        "intended_behavior": "The `chunk` method should be updated or extended to allow specifying the parallel computation backend (e.g., `dask` or `cubed`) via an additional argument or configuration setting. This modification should allow the method to determine the appropriate chunking mechanism based on the selected backend while preserving current functionality for `dask`."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 229,
        "end": 6971,
        "class_name": "DataArray",
        "method_name": null,
        "code": "229 class DataArray(\n230     AbstractArray,\n231     DataWithCoords,\n232     DataArrayArithmetic,\n233     DataArrayAggregations,\n234 ):\n235     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n236 \n237     DataArray provides a wrapper around numpy ndarrays that uses\n238     labeled dimensions and coordinates to support metadata aware\n239     operations. The API is similar to that for the pandas Series or\n240     DataFrame, but DataArray objects can have any number of dimensions,\n241     and their contents have fixed data types.\n242 \n243     Additional features over raw numpy arrays:\n244 \n245     - Apply operations over dimensions by name: ``x.sum('time')``.\n246     - Select or assign values by integer location (like numpy):\n247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or\n248       ``x.sel(time='2014-01-01')``.\n249     - Mathematical operations (e.g., ``x - y``) vectorize across\n250       multiple dimensions (known in numpy as \"broadcasting\") based on\n251       dimension names, regardless of their original order.\n252     - Keep track of arbitrary metadata in the form of a Python\n253       dictionary: ``x.attrs``\n254     - Convert to a pandas Series: ``x.to_series()``.\n255 \n256     Getting items from or doing mathematical operations with a\n257     DataArray always returns another DataArray.\n258 \n259     Parameters\n260     ----------\n261     data : array_like\n262         Values for this array. Must be an ``numpy.ndarray``, ndarray\n263         like, or castable to an ``ndarray``. If a self-described xarray\n264         or pandas object, attempts are made to use this array's\n265         metadata to fill in other unspecified arguments. A view of the\n266         array's data is used instead of a copy if possible.\n267     coords : sequence or dict of array_like, optional\n268         Coordinates (tick labels) to use for indexing along each\n269         dimension. The following notations are accepted:\n270 \n271         - mapping {dimension name: array-like}\n272         - sequence of tuples that are valid arguments for\n273           ``xarray.Variable()``\n274           - (dims, data)\n275           - (dims, data, attrs)\n276           - (dims, data, attrs, encoding)\n277 \n278         Additionally, it is possible to define a coord whose name\n279         does not match the dimension name, or a coord based on multiple\n280         dimensions, with one of the following notations:\n281 \n282         - mapping {coord name: DataArray}\n283         - mapping {coord name: Variable}\n284         - mapping {coord name: (dimension name, array-like)}\n285         - mapping {coord name: (tuple of dimension names, array-like)}\n286 \n287     dims : Hashable or sequence of Hashable, optional\n288         Name(s) of the data dimension(s). Must be either a Hashable\n289         (only for 1D data) or a sequence of Hashables with length equal\n290         to the number of dimensions. If this argument is omitted,\n291         dimension names are taken from ``coords`` (if possible) and\n292         otherwise default to ``['dim_0', ... 'dim_n']``.\n293     name : str or None, optional\n294         Name of this array.\n295     attrs : dict_like or None, optional\n296         Attributes to assign to the new instance. By default, an empty\n297         attribute dictionary is initialized.\n298 \n299     Examples\n300     --------\n301     Create data:\n302 \n303     >>> np.random.seed(0)\n304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n307     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n308     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n309 \n310     Initialize a dataarray with multiple dimensions:\n311 \n312     >>> da = xr.DataArray(\n313     ...     data=temperature,\n314     ...     dims=[\"x\", \"y\", \"time\"],\n315     ...     coords=dict(\n316     ...         lon=([\"x\", \"y\"], lon),\n317     ...         lat=([\"x\", \"y\"], lat),\n318     ...         time=time,\n319     ...         reference_time=reference_time,\n320     ...     ),\n321     ...     attrs=dict(\n322     ...         description=\"Ambient temperature.\",\n323     ...         units=\"degC\",\n324     ...     ),\n325     ... )\n326     >>> da\n327     <xarray.DataArray (x: 2, y: 2, time: 3)>\n328     array([[[29.11241877, 18.20125767, 22.82990387],\n329             [32.92714559, 29.94046392,  7.18177696]],\n330     <BLANKLINE>\n331            [[22.60070734, 13.78914233, 14.17424919],\n332             [18.28478802, 16.15234857, 26.63418806]]])\n333     Coordinates:\n334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n335         lat             (x, y) float64 42.25 42.21 42.63 42.59\n336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n337         reference_time  datetime64[ns] 2014-09-05\n338     Dimensions without coordinates: x, y\n339     Attributes:\n340         description:  Ambient temperature.\n341         units:        degC\n342 \n343     Find out where the coldest temperature was:\n344 \n345     >>> da.isel(da.argmin(...))\n346     <xarray.DataArray ()>\n347     array(7.18177696)\n348     Coordinates:\n349         lon             float64 -99.32\n350         lat             float64 42.21\n351         time            datetime64[ns] 2014-09-08\n352         reference_time  datetime64[ns] 2014-09-05\n353     Attributes:\n354         description:  Ambient temperature.\n355         units:        degC\n356     \"\"\"\n357 \n358     _cache: dict[str, Any]\n359     _coords: dict[Any, Variable]\n360     _close: Callable[[], None] | None\n361     _indexes: dict[Hashable, Index]\n362     _name: Hashable | None\n363     _variable: Variable\n364 \n365     __slots__ = (\n366         \"_cache\",\n367         \"_coords\",\n368         \"_close\",\n369         \"_indexes\",\n370         \"_name\",\n371         \"_variable\",\n372         \"__weakref__\",\n373     )\n374 \n375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor[\"DataArray\"])\n376 \n377     def __init__(\n378         self,\n379         data: Any = dtypes.NA,\n380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]\n381         | Mapping[Any, Any]\n382         | None = None,\n383         dims: Hashable | Sequence[Hashable] | None = None,\n384         name: Hashable | None = None,\n385         attrs: Mapping | None = None,\n386         # internal parameters\n387         indexes: dict[Hashable, Index] | None = None,\n388         fastpath: bool = False,\n389     ) -> None:\n390         if fastpath:\n391             variable = data\n392             assert dims is None\n393             assert attrs is None\n394             assert indexes is not None\n395         else:\n396             # TODO: (benbovy - explicit indexes) remove\n397             # once it becomes part of the public interface\n398             if indexes is not None:\n399                 raise ValueError(\"Providing explicit indexes is not supported yet\")\n400 \n401             # try to fill in arguments from data if they weren't supplied\n402             if coords is None:\n403                 if isinstance(data, DataArray):\n404                     coords = data.coords\n405                 elif isinstance(data, pd.Series):\n406                     coords = [data.index]\n407                 elif isinstance(data, pd.DataFrame):\n408                     coords = [data.index, data.columns]\n409                 elif isinstance(data, (pd.Index, IndexVariable)):\n410                     coords = [data]\n411 \n412             if dims is None:\n413                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n414             if name is None:\n415                 name = getattr(data, \"name\", None)\n416             if attrs is None and not isinstance(data, PANDAS_TYPES):\n417                 attrs = getattr(data, \"attrs\", None)\n418 \n419             data = _check_data_shape(data, coords, dims)\n420             data = as_compatible_data(data)\n421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n422             variable = Variable(dims, data, attrs, fastpath=True)\n423             indexes, coords = _create_indexes_from_coords(coords)\n424 \n425         # These fully describe a DataArray\n426         self._variable = variable\n427         assert isinstance(coords, dict)\n428         self._coords = coords\n429         self._name = name\n430 \n431         # TODO(shoyer): document this argument, once it becomes part of the\n432         # public interface.\n433         self._indexes = indexes\n434 \n435         self._close = None\n436 \n437     @classmethod\n438     def _construct_direct(\n439         cls: type[T_DataArray],\n440         variable: Variable,\n441         coords: dict[Any, Variable],\n442         name: Hashable,\n443         indexes: dict[Hashable, Index],\n444     ) -> T_DataArray:\n445         \"\"\"Shortcut around __init__ for internal use when we want to skip\n446         costly validation\n447         \"\"\"\n448         obj = object.__new__(cls)\n449         obj._variable = variable\n450         obj._coords = coords\n451         obj._name = name\n452         obj._indexes = indexes\n453         obj._close = None\n454         return obj\n455 \n456     def _replace(\n457         self: T_DataArray,\n458         variable: Variable | None = None,\n459         coords=None,\n460         name: Hashable | None | Default = _default,\n461         indexes=None,\n462     ) -> T_DataArray:\n463         if variable is None:\n464             variable = self.variable\n465         if coords is None:\n466             coords = self._coords\n467         if indexes is None:\n468             indexes = self._indexes\n469         if name is _default:\n470             name = self.name\n471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)\n472 \n473     def _replace_maybe_drop_dims(\n474         self: T_DataArray,\n475         variable: Variable,\n476         name: Hashable | None | Default = _default,\n477     ) -> T_DataArray:\n478         if variable.dims == self.dims and variable.shape == self.shape:\n479             coords = self._coords.copy()\n480             indexes = self._indexes\n481         elif variable.dims == self.dims:\n482             # Shape has changed (e.g. from reduce(..., keepdims=True)\n483             new_sizes = dict(zip(self.dims, variable.shape))\n484             coords = {\n485                 k: v\n486                 for k, v in self._coords.items()\n487                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n488             }\n489             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n490         else:\n491             allowed_dims = set(variable.dims)\n492             coords = {\n493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n494             }\n495             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n496         return self._replace(variable, coords, name, indexes=indexes)\n497 \n498     def _overwrite_indexes(\n499         self: T_DataArray,\n500         indexes: Mapping[Any, Index],\n501         coords: Mapping[Any, Variable] | None = None,\n502         drop_coords: list[Hashable] | None = None,\n503         rename_dims: Mapping[Any, Any] | None = None,\n504     ) -> T_DataArray:\n505         \"\"\"Maybe replace indexes and their corresponding coordinates.\"\"\"\n506         if not indexes:\n507             return self\n508 \n509         if coords is None:\n510             coords = {}\n511         if drop_coords is None:\n512             drop_coords = []\n513 \n514         new_variable = self.variable.copy()\n515         new_coords = self._coords.copy()\n516         new_indexes = dict(self._indexes)\n517 \n518         for name in indexes:\n519             new_coords[name] = coords[name]\n520             new_indexes[name] = indexes[name]\n521 \n522         for name in drop_coords:\n523             new_coords.pop(name)\n524             new_indexes.pop(name)\n525 \n526         if rename_dims:\n527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)\n528 \n529         return self._replace(\n530             variable=new_variable, coords=new_coords, indexes=new_indexes\n531         )\n532 \n533     def _to_temp_dataset(self) -> Dataset:\n534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n535 \n536     def _from_temp_dataset(\n537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default\n538     ) -> T_DataArray:\n539         variable = dataset._variables.pop(_THIS_ARRAY)\n540         coords = dataset._variables\n541         indexes = dataset._indexes\n542         return self._replace(variable, coords, name, indexes=indexes)\n543 \n544     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n545         \"\"\"splits dataarray along dimension 'dim'\"\"\"\n546 \n547         def subset(dim, label):\n548             array = self.loc[{dim: label}]\n549             array.attrs = {}\n550             return as_variable(array)\n551 \n552         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n553         variables.update({k: v for k, v in self._coords.items() if k != dim})\n554         coord_names = set(self._coords) - {dim}\n555         indexes = filter_indexes_from_coords(self._indexes, coord_names)\n556         dataset = Dataset._construct_direct(\n557             variables, coord_names, indexes=indexes, attrs=self.attrs\n558         )\n559         return dataset\n560 \n561     def _to_dataset_whole(\n562         self, name: Hashable = None, shallow_copy: bool = True\n563     ) -> Dataset:\n564         if name is None:\n565             name = self.name\n566         if name is None:\n567             raise ValueError(\n568                 \"unable to convert unnamed DataArray to a \"\n569                 \"Dataset without providing an explicit name\"\n570             )\n571         if name in self.coords:\n572             raise ValueError(\n573                 \"cannot create a Dataset from a DataArray with \"\n574                 \"the same name as one of its coordinates\"\n575             )\n576         # use private APIs for speed: this is called by _to_temp_dataset(),\n577         # which is used in the guts of a lot of operations (e.g., reindex)\n578         variables = self._coords.copy()\n579         variables[name] = self.variable\n580         if shallow_copy:\n581             for k in variables:\n582                 variables[k] = variables[k].copy(deep=False)\n583         indexes = self._indexes\n584 \n585         coord_names = set(self._coords)\n586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)\n587 \n588     def to_dataset(\n589         self,\n590         dim: Hashable = None,\n591         *,\n592         name: Hashable = None,\n593         promote_attrs: bool = False,\n594     ) -> Dataset:\n595         \"\"\"Convert a DataArray to a Dataset.\n596 \n597         Parameters\n598         ----------\n599         dim : Hashable, optional\n600             Name of the dimension on this array along which to split this array\n601             into separate variables. If not provided, this array is converted\n602             into a Dataset of one variable.\n603         name : Hashable, optional\n604             Name to substitute for this array's name. Only valid if ``dim`` is\n605             not provided.\n606         promote_attrs : bool, default: False\n607             Set to True to shallow copy attrs of DataArray to returned Dataset.\n608 \n609         Returns\n610         -------\n611         dataset : Dataset\n612         \"\"\"\n613         if dim is not None and dim not in self.dims:\n614             raise TypeError(\n615                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n616             )\n617 \n618         if dim is not None:\n619             if name is not None:\n620                 raise TypeError(\"cannot supply both dim and name arguments\")\n621             result = self._to_dataset_split(dim)\n622         else:\n623             result = self._to_dataset_whole(name)\n624 \n625         if promote_attrs:\n626             result.attrs = dict(self.attrs)\n627 \n628         return result\n629 \n630     @property\n631     def name(self) -> Hashable | None:\n632         \"\"\"The name of this array.\"\"\"\n633         return self._name\n634 \n635     @name.setter\n636     def name(self, value: Hashable | None) -> None:\n637         self._name = value\n638 \n639     @property\n640     def variable(self) -> Variable:\n641         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n642         return self._variable\n643 \n644     @property\n645     def dtype(self) -> np.dtype:\n646         \"\"\"\n647         Data-type of the array\u2019s elements.\n648 \n649         See Also\n650         --------\n651         ndarray.dtype\n652         numpy.dtype\n653         \"\"\"\n654         return self.variable.dtype\n655 \n656     @property\n657     def shape(self) -> tuple[int, ...]:\n658         \"\"\"\n659         Tuple of array dimensions.\n660 \n661         See Also\n662         --------\n663         numpy.ndarray.shape\n664         \"\"\"\n665         return self.variable.shape\n666 \n667     @property\n668     def size(self) -> int:\n669         \"\"\"\n670         Number of elements in the array.\n671 \n672         Equal to ``np.prod(a.shape)``, i.e., the product of the array\u2019s dimensions.\n673 \n674         See Also\n675         --------\n676         numpy.ndarray.size\n677         \"\"\"\n678         return self.variable.size\n679 \n680     @property\n681     def nbytes(self) -> int:\n682         \"\"\"\n683         Total bytes consumed by the elements of this DataArray's data.\n684 \n685         If the underlying data array does not include ``nbytes``, estimates\n686         the bytes consumed based on the ``size`` and ``dtype``.\n687         \"\"\"\n688         return self.variable.nbytes\n689 \n690     @property\n691     def ndim(self) -> int:\n692         \"\"\"\n693         Number of array dimensions.\n694 \n695         See Also\n696         --------\n697         numpy.ndarray.ndim\n698         \"\"\"\n699         return self.variable.ndim\n700 \n701     def __len__(self) -> int:\n702         return len(self.variable)\n703 \n704     @property\n705     def data(self) -> Any:\n706         \"\"\"\n707         The DataArray's data as an array. The underlying array type\n708         (e.g. dask, sparse, pint) is preserved.\n709 \n710         See Also\n711         --------\n712         DataArray.to_numpy\n713         DataArray.as_numpy\n714         DataArray.values\n715         \"\"\"\n716         return self.variable.data\n717 \n718     @data.setter\n719     def data(self, value: Any) -> None:\n720         self.variable.data = value\n721 \n722     @property\n723     def values(self) -> np.ndarray:\n724         \"\"\"\n725         The array's data as a numpy.ndarray.\n726 \n727         If the array's data is not a numpy.ndarray this will attempt to convert\n728         it naively using np.array(), which will raise an error if the array\n729         type does not support coercion like this (e.g. cupy).\n730         \"\"\"\n731         return self.variable.values\n732 \n733     @values.setter\n734     def values(self, value: Any) -> None:\n735         self.variable.values = value\n736 \n737     def to_numpy(self) -> np.ndarray:\n738         \"\"\"\n739         Coerces wrapped data to numpy and returns a numpy.ndarray.\n740 \n741         See Also\n742         --------\n743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.\n744         Dataset.as_numpy\n745         DataArray.values\n746         DataArray.data\n747         \"\"\"\n748         return self.variable.to_numpy()\n749 \n750     def as_numpy(self: T_DataArray) -> T_DataArray:\n751         \"\"\"\n752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.\n753 \n754         See Also\n755         --------\n756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.\n757         Dataset.as_numpy : Converts all variables in a Dataset.\n758         DataArray.values\n759         DataArray.data\n760         \"\"\"\n761         coords = {k: v.as_numpy() for k, v in self._coords.items()}\n762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)\n763 \n764     @property\n765     def _in_memory(self) -> bool:\n766         return self.variable._in_memory\n767 \n768     def _to_index(self) -> pd.Index:\n769         return self.variable._to_index()\n770 \n771     def to_index(self) -> pd.Index:\n772         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n773         arrays.\n774         \"\"\"\n775         return self.variable.to_index()\n776 \n777     @property\n778     def dims(self) -> tuple[Hashable, ...]:\n779         \"\"\"Tuple of dimension names associated with this array.\n780 \n781         Note that the type of this property is inconsistent with\n782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n783         consistently named properties.\n784 \n785         See Also\n786         --------\n787         DataArray.sizes\n788         Dataset.dims\n789         \"\"\"\n790         return self.variable.dims\n791 \n792     @dims.setter\n793     def dims(self, value: Any) -> NoReturn:\n794         raise AttributeError(\n795             \"you cannot assign dims on a DataArray. Use \"\n796             \".rename() or .swap_dims() instead.\"\n797         )\n798 \n799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n800         if utils.is_dict_like(key):\n801             return key\n802         key = indexing.expanded_indexer(key, self.ndim)\n803         return dict(zip(self.dims, key))\n804 \n805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:\n806         from xarray.core.dataset import _get_virtual_variable\n807 \n808         try:\n809             var = self._coords[key]\n810         except KeyError:\n811             dim_sizes = dict(zip(self.dims, self.shape))\n812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)\n813 \n814         return self._replace_maybe_drop_dims(var, name=key)\n815 \n816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:\n817         if isinstance(key, str):\n818             return self._getitem_coord(key)\n819         else:\n820             # xarray-style array indexing\n821             return self.isel(indexers=self._item_key_to_dict(key))\n822 \n823     def __setitem__(self, key: Any, value: Any) -> None:\n824         if isinstance(key, str):\n825             self.coords[key] = value\n826         else:\n827             # Coordinates in key, value and self[key] should be consistent.\n828             # TODO Coordinate consistency in key is checked here, but it\n829             # causes unnecessary indexing. It should be optimized.\n830             obj = self[key]\n831             if isinstance(value, DataArray):\n832                 assert_coordinate_consistent(value, obj.coords.variables)\n833             # DataArray key -> Variable key\n834             key = {\n835                 k: v.variable if isinstance(v, DataArray) else v\n836                 for k, v in self._item_key_to_dict(key).items()\n837             }\n838             self.variable[key] = value\n839 \n840     def __delitem__(self, key: Any) -> None:\n841         del self.coords[key]\n842 \n843     @property\n844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n845         \"\"\"Places to look-up items for attribute-style access\"\"\"\n846         yield from self._item_sources\n847         yield self.attrs\n848 \n849     @property\n850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n851         \"\"\"Places to look-up items for key-completion\"\"\"\n852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)\n853 \n854         # virtual coordinates\n855         # uses empty dict -- everything here can already be found in self.coords.\n856         yield HybridMappingProxy(keys=self.dims, mapping={})\n857 \n858     def __contains__(self, key: Any) -> bool:\n859         return key in self.data\n860 \n861     @property\n862     def loc(self) -> _LocIndexer:\n863         \"\"\"Attribute for location based indexing like pandas.\"\"\"\n864         return _LocIndexer(self)\n865 \n866     @property\n867     def attrs(self) -> dict[Any, Any]:\n868         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n869         return self.variable.attrs\n870 \n871     @attrs.setter\n872     def attrs(self, value: Mapping[Any, Any]) -> None:\n873         self.variable.attrs = dict(value)\n874 \n875     @property\n876     def encoding(self) -> dict[Any, Any]:\n877         \"\"\"Dictionary of format-specific settings for how this array should be\n878         serialized.\"\"\"\n879         return self.variable.encoding\n880 \n881     @encoding.setter\n882     def encoding(self, value: Mapping[Any, Any]) -> None:\n883         self.variable.encoding = dict(value)\n884 \n885     def reset_encoding(self: T_DataArray) -> T_DataArray:\n886         \"\"\"Return a new DataArray without encoding on the array or any attached\n887         coords.\"\"\"\n888         ds = self._to_temp_dataset().reset_encoding()\n889         return self._from_temp_dataset(ds)\n890 \n891     @property\n892     def indexes(self) -> Indexes:\n893         \"\"\"Mapping of pandas.Index objects used for label based indexing.\n894 \n895         Raises an error if this Dataset has indexes that cannot be coerced\n896         to pandas.Index objects.\n897 \n898         See Also\n899         --------\n900         DataArray.xindexes\n901 \n902         \"\"\"\n903         return self.xindexes.to_pandas_indexes()\n904 \n905     @property\n906     def xindexes(self) -> Indexes:\n907         \"\"\"Mapping of xarray Index objects used for label based indexing.\"\"\"\n908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})\n909 \n910     @property\n911     def coords(self) -> DataArrayCoordinates:\n912         \"\"\"Dictionary-like container of coordinate arrays.\"\"\"\n913         return DataArrayCoordinates(self)\n914 \n915     @overload\n916     def reset_coords(\n917         self: T_DataArray,\n918         names: Dims = None,\n919         drop: Literal[False] = False,\n920     ) -> Dataset:\n921         ...\n922 \n923     @overload\n924     def reset_coords(\n925         self: T_DataArray,\n926         names: Dims = None,\n927         *,\n928         drop: Literal[True],\n929     ) -> T_DataArray:\n930         ...\n931 \n932     def reset_coords(\n933         self: T_DataArray,\n934         names: Dims = None,\n935         drop: bool = False,\n936     ) -> T_DataArray | Dataset:\n937         \"\"\"Given names of coordinates, reset them to become variables.\n938 \n939         Parameters\n940         ----------\n941         names : str, Iterable of Hashable or None, optional\n942             Name(s) of non-index coordinates in this dataset to reset into\n943             variables. By default, all non-index coordinates are reset.\n944         drop : bool, default: False\n945             If True, remove coordinates instead of converting them into\n946             variables.\n947 \n948         Returns\n949         -------\n950         Dataset, or DataArray if ``drop == True``\n951 \n952         Examples\n953         --------\n954         >>> temperature = np.arange(25).reshape(5, 5)\n955         >>> pressure = np.arange(50, 75).reshape(5, 5)\n956         >>> da = xr.DataArray(\n957         ...     data=temperature,\n958         ...     dims=[\"x\", \"y\"],\n959         ...     coords=dict(\n960         ...         lon=(\"x\", np.arange(10, 15)),\n961         ...         lat=(\"y\", np.arange(20, 25)),\n962         ...         Pressure=([\"x\", \"y\"], pressure),\n963         ...     ),\n964         ...     name=\"Temperature\",\n965         ... )\n966         >>> da\n967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>\n968         array([[ 0,  1,  2,  3,  4],\n969                [ 5,  6,  7,  8,  9],\n970                [10, 11, 12, 13, 14],\n971                [15, 16, 17, 18, 19],\n972                [20, 21, 22, 23, 24]])\n973         Coordinates:\n974             lon       (x) int64 10 11 12 13 14\n975             lat       (y) int64 20 21 22 23 24\n976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74\n977         Dimensions without coordinates: x, y\n978 \n979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:\n980 \n981         >>> da.reset_coords(names=\"Pressure\")\n982         <xarray.Dataset>\n983         Dimensions:      (x: 5, y: 5)\n984         Coordinates:\n985             lon          (x) int64 10 11 12 13 14\n986             lat          (y) int64 20 21 22 23 24\n987         Dimensions without coordinates: x, y\n988         Data variables:\n989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74\n990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24\n991 \n992         Return DataArray without targeted coordinate:\n993 \n994         >>> da.reset_coords(names=\"Pressure\", drop=True)\n995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>\n996         array([[ 0,  1,  2,  3,  4],\n997                [ 5,  6,  7,  8,  9],\n998                [10, 11, 12, 13, 14],\n999                [15, 16, 17, 18, 19],\n1000                [20, 21, 22, 23, 24]])\n1001         Coordinates:\n1002             lon      (x) int64 10 11 12 13 14\n1003             lat      (y) int64 20 21 22 23 24\n1004         Dimensions without coordinates: x, y\n1005         \"\"\"\n1006         if names is None:\n1007             names = set(self.coords) - set(self._indexes)\n1008         dataset = self.coords.to_dataset().reset_coords(names, drop)\n1009         if drop:\n1010             return self._replace(coords=dataset._variables)\n1011         if self.name is None:\n1012             raise ValueError(\n1013                 \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n1014             )\n1015         dataset[self.name] = self.variable\n1016         return dataset\n1017 \n1018     def __dask_tokenize__(self):\n1019         from dask.base import normalize_token\n1020 \n1021         return normalize_token((type(self), self._variable, self._coords, self._name))\n1022 \n1023     def __dask_graph__(self):\n1024         return self._to_temp_dataset().__dask_graph__()\n1025 \n1026     def __dask_keys__(self):\n1027         return self._to_temp_dataset().__dask_keys__()\n1028 \n1029     def __dask_layers__(self):\n1030         return self._to_temp_dataset().__dask_layers__()\n1031 \n1032     @property\n1033     def __dask_optimize__(self):\n1034         return self._to_temp_dataset().__dask_optimize__\n1035 \n1036     @property\n1037     def __dask_scheduler__(self):\n1038         return self._to_temp_dataset().__dask_scheduler__\n1039 \n1040     def __dask_postcompute__(self):\n1041         func, args = self._to_temp_dataset().__dask_postcompute__()\n1042         return self._dask_finalize, (self.name, func) + args\n1043 \n1044     def __dask_postpersist__(self):\n1045         func, args = self._to_temp_dataset().__dask_postpersist__()\n1046         return self._dask_finalize, (self.name, func) + args\n1047 \n1048     @staticmethod\n1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:\n1050         ds = func(results, *args, **kwargs)\n1051         variable = ds._variables.pop(_THIS_ARRAY)\n1052         coords = ds._variables\n1053         indexes = ds._indexes\n1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n1055 \n1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:\n1057         \"\"\"Manually trigger loading of this array's data from disk or a\n1058         remote source into memory and return this array.\n1059 \n1060         Normally, it should not be necessary to call this method in user code,\n1061         because all xarray functions should either work on deferred data or\n1062         load data automatically. However, this method can be necessary when\n1063         working with many file objects on disk.\n1064 \n1065         Parameters\n1066         ----------\n1067         **kwargs : dict\n1068             Additional keyword arguments passed on to ``dask.compute``.\n1069 \n1070         See Also\n1071         --------\n1072         dask.compute\n1073         \"\"\"\n1074         ds = self._to_temp_dataset().load(**kwargs)\n1075         new = self._from_temp_dataset(ds)\n1076         self._variable = new._variable\n1077         self._coords = new._coords\n1078         return self\n1079 \n1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:\n1081         \"\"\"Manually trigger loading of this array's data from disk or a\n1082         remote source into memory and return a new array. The original is\n1083         left unaltered.\n1084 \n1085         Normally, it should not be necessary to call this method in user code,\n1086         because all xarray functions should either work on deferred data or\n1087         load data automatically. However, this method can be necessary when\n1088         working with many file objects on disk.\n1089 \n1090         Parameters\n1091         ----------\n1092         **kwargs : dict\n1093             Additional keyword arguments passed on to ``dask.compute``.\n1094 \n1095         See Also\n1096         --------\n1097         dask.compute\n1098         \"\"\"\n1099         new = self.copy(deep=False)\n1100         return new.load(**kwargs)\n1101 \n1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:\n1103         \"\"\"Trigger computation in constituent dask arrays\n1104 \n1105         This keeps them as dask arrays but encourages them to keep data in\n1106         memory.  This is particularly useful when on a distributed machine.\n1107         When on a single machine consider using ``.compute()`` instead.\n1108 \n1109         Parameters\n1110         ----------\n1111         **kwargs : dict\n1112             Additional keyword arguments passed on to ``dask.persist``.\n1113 \n1114         See Also\n1115         --------\n1116         dask.persist\n1117         \"\"\"\n1118         ds = self._to_temp_dataset().persist(**kwargs)\n1119         return self._from_temp_dataset(ds)\n1120 \n1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:\n1122         \"\"\"Returns a copy of this array.\n1123 \n1124         If `deep=True`, a deep copy is made of the data array.\n1125         Otherwise, a shallow copy is made, and the returned data array's\n1126         values are a new view of this data array's values.\n1127 \n1128         Use `data` to create a new object with the same structure as\n1129         original but entirely new data.\n1130 \n1131         Parameters\n1132         ----------\n1133         deep : bool, optional\n1134             Whether the data array and its coordinates are loaded into memory\n1135             and copied onto the new object. Default is True.\n1136         data : array_like, optional\n1137             Data to use in the new object. Must have same shape as original.\n1138             When `data` is used, `deep` is ignored for all data variables,\n1139             and only used for coords.\n1140 \n1141         Returns\n1142         -------\n1143         copy : DataArray\n1144             New object with dimensions, attributes, coordinates, name,\n1145             encoding, and optionally data copied from original.\n1146 \n1147         Examples\n1148         --------\n1149         Shallow versus deep copy\n1150 \n1151         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n1152         >>> array.copy()\n1153         <xarray.DataArray (x: 3)>\n1154         array([1, 2, 3])\n1155         Coordinates:\n1156           * x        (x) <U1 'a' 'b' 'c'\n1157         >>> array_0 = array.copy(deep=False)\n1158         >>> array_0[0] = 7\n1159         >>> array_0\n1160         <xarray.DataArray (x: 3)>\n1161         array([7, 2, 3])\n1162         Coordinates:\n1163           * x        (x) <U1 'a' 'b' 'c'\n1164         >>> array\n1165         <xarray.DataArray (x: 3)>\n1166         array([7, 2, 3])\n1167         Coordinates:\n1168           * x        (x) <U1 'a' 'b' 'c'\n1169 \n1170         Changing the data using the ``data`` argument maintains the\n1171         structure of the original object, but with the new data. Original\n1172         object is unaffected.\n1173 \n1174         >>> array.copy(data=[0.1, 0.2, 0.3])\n1175         <xarray.DataArray (x: 3)>\n1176         array([0.1, 0.2, 0.3])\n1177         Coordinates:\n1178           * x        (x) <U1 'a' 'b' 'c'\n1179         >>> array\n1180         <xarray.DataArray (x: 3)>\n1181         array([7, 2, 3])\n1182         Coordinates:\n1183           * x        (x) <U1 'a' 'b' 'c'\n1184 \n1185         See Also\n1186         --------\n1187         pandas.DataFrame.copy\n1188         \"\"\"\n1189         return self._copy(deep=deep, data=data)\n1190 \n1191     def _copy(\n1192         self: T_DataArray,\n1193         deep: bool = True,\n1194         data: Any = None,\n1195         memo: dict[int, Any] | None = None,\n1196     ) -> T_DataArray:\n1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)\n1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n1199 \n1200         coords = {}\n1201         for k, v in self._coords.items():\n1202             if k in index_vars:\n1203                 coords[k] = index_vars[k]\n1204             else:\n1205                 coords[k] = v._copy(deep=deep, memo=memo)\n1206 \n1207         return self._replace(variable, coords, indexes=indexes)\n1208 \n1209     def __copy__(self: T_DataArray) -> T_DataArray:\n1210         return self._copy(deep=False)\n1211 \n1212     def __deepcopy__(\n1213         self: T_DataArray, memo: dict[int, Any] | None = None\n1214     ) -> T_DataArray:\n1215         return self._copy(deep=True, memo=memo)\n1216 \n1217     # mutable objects should not be Hashable\n1218     # https://github.com/python/mypy/issues/4266\n1219     __hash__ = None  # type: ignore[assignment]\n1220 \n1221     @property\n1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:\n1223         \"\"\"\n1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if\n1225         the underlying data is not a dask array.\n1226 \n1227         See Also\n1228         --------\n1229         DataArray.chunk\n1230         DataArray.chunksizes\n1231         xarray.unify_chunks\n1232         \"\"\"\n1233         return self.variable.chunks\n1234 \n1235     @property\n1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:\n1237         \"\"\"\n1238         Mapping from dimension names to block lengths for this dataarray's data, or None if\n1239         the underlying data is not a dask array.\n1240         Cannot be modified directly, but can be modified by calling .chunk().\n1241 \n1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes\n1243         instead of a tuple of chunk shapes.\n1244 \n1245         See Also\n1246         --------\n1247         DataArray.chunk\n1248         DataArray.chunks\n1249         xarray.unify_chunks\n1250         \"\"\"\n1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]\n1252         return get_chunksizes(all_variables)\n1253 \n1254     def chunk(\n1255         self: T_DataArray,\n1256         chunks: (\n1257             int\n1258             | Literal[\"auto\"]\n1259             | tuple[int, ...]\n1260             | tuple[tuple[int, ...], ...]\n1261             | Mapping[Any, None | int | tuple[int, ...]]\n1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n1263         name_prefix: str = \"xarray-\",\n1264         token: str | None = None,\n1265         lock: bool = False,\n1266         inline_array: bool = False,\n1267         **chunks_kwargs: Any,\n1268     ) -> T_DataArray:\n1269         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n1270 \n1271         If this variable is a non-dask array, it will be converted to dask\n1272         array. If it's a dask array, it will be rechunked to the given chunk\n1273         sizes.\n1274 \n1275         If neither chunks is not provided for one or more dimensions, chunk\n1276         sizes along that dimension will not be updated; non-dask arrays will be\n1277         converted into dask arrays with a single block.\n1278 \n1279         Parameters\n1280         ----------\n1281         chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n1282             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n1283             ``{\"x\": 5, \"y\": 5}``.\n1284         name_prefix : str, optional\n1285             Prefix for the name of the new dask array.\n1286         token : str, optional\n1287             Token uniquely identifying this array.\n1288         lock : optional\n1289             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1290             already as dask array.\n1291         inline_array: optional\n1292             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1293             already as dask array.\n1294         **chunks_kwargs : {dim: chunks, ...}, optional\n1295             The keyword arguments form of ``chunks``.\n1296             One of chunks or chunks_kwargs must be provided.\n1297 \n1298         Returns\n1299         -------\n1300         chunked : xarray.DataArray\n1301 \n1302         See Also\n1303         --------\n1304         DataArray.chunks\n1305         DataArray.chunksizes\n1306         xarray.unify_chunks\n1307         dask.array.from_array\n1308         \"\"\"\n1309         if chunks is None:\n1310             warnings.warn(\n1311                 \"None value for 'chunks' is deprecated. \"\n1312                 \"It will raise an error in the future. Use instead '{}'\",\n1313                 category=FutureWarning,\n1314             )\n1315             chunks = {}\n1316 \n1317         if isinstance(chunks, (float, str, int)):\n1318             # ignoring type; unclear why it won't accept a Literal into the value.\n1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore\n1320         elif isinstance(chunks, (tuple, list)):\n1321             chunks = dict(zip(self.dims, chunks))\n1322         else:\n1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n1324 \n1325         ds = self._to_temp_dataset().chunk(\n1326             chunks,\n1327             name_prefix=name_prefix,\n1328             token=token,\n1329             lock=lock,\n1330             inline_array=inline_array,\n1331         )\n1332         return self._from_temp_dataset(ds)\n1333 \n1334     def isel(\n1335         self: T_DataArray,\n1336         indexers: Mapping[Any, Any] | None = None,\n1337         drop: bool = False,\n1338         missing_dims: ErrorOptionsWithWarn = \"raise\",\n1339         **indexers_kwargs: Any,\n1340     ) -> T_DataArray:\n1341         \"\"\"Return a new DataArray whose data is given by selecting indexes\n1342         along the specified dimension(s).\n1343 \n1344         Parameters\n1345         ----------\n1346         indexers : dict, optional\n1347             A dict with keys matching dimensions and values given\n1348             by integers, slice objects or arrays.\n1349             indexer can be a integer, slice, array-like or DataArray.\n1350             If DataArrays are passed as indexers, xarray-style indexing will be\n1351             carried out. See :ref:`indexing` for the details.\n1352             One of indexers or indexers_kwargs must be provided.\n1353         drop : bool, default: False\n1354             If ``drop=True``, drop coordinates variables indexed by integers\n1355             instead of making them scalar.\n1356         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n1357             What to do if dimensions that should be selected from are not present in the\n1358             DataArray:\n1359             - \"raise\": raise an exception\n1360             - \"warn\": raise a warning, and ignore the missing dimensions\n1361             - \"ignore\": ignore the missing dimensions\n1362         **indexers_kwargs : {dim: indexer, ...}, optional\n1363             The keyword arguments form of ``indexers``.\n1364 \n1365         Returns\n1366         -------\n1367         indexed : xarray.DataArray\n1368 \n1369         See Also\n1370         --------\n1371         Dataset.isel\n1372         DataArray.sel\n1373 \n1374         Examples\n1375         --------\n1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=(\"x\", \"y\"))\n1377         >>> da\n1378         <xarray.DataArray (x: 5, y: 5)>\n1379         array([[ 0,  1,  2,  3,  4],\n1380                [ 5,  6,  7,  8,  9],\n1381                [10, 11, 12, 13, 14],\n1382                [15, 16, 17, 18, 19],\n1383                [20, 21, 22, 23, 24]])\n1384         Dimensions without coordinates: x, y\n1385 \n1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1388         >>> da = da.isel(x=tgt_x, y=tgt_y)\n1389         >>> da\n1390         <xarray.DataArray (points: 5)>\n1391         array([ 0,  6, 12, 18, 24])\n1392         Dimensions without coordinates: points\n1393         \"\"\"\n1394 \n1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1396 \n1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1398             ds = self._to_temp_dataset()._isel_fancy(\n1399                 indexers, drop=drop, missing_dims=missing_dims\n1400             )\n1401             return self._from_temp_dataset(ds)\n1402 \n1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1404         # lists, or zero or one-dimensional np.ndarray's\n1405 \n1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)\n1408 \n1409         coords = {}\n1410         for coord_name, coord_value in self._coords.items():\n1411             if coord_name in index_variables:\n1412                 coord_value = index_variables[coord_name]\n1413             else:\n1414                 coord_indexers = {\n1415                     k: v for k, v in indexers.items() if k in coord_value.dims\n1416                 }\n1417                 if coord_indexers:\n1418                     coord_value = coord_value.isel(coord_indexers)\n1419                     if drop and coord_value.ndim == 0:\n1420                         continue\n1421             coords[coord_name] = coord_value\n1422 \n1423         return self._replace(variable=variable, coords=coords, indexes=indexes)\n1424 \n1425     def sel(\n1426         self: T_DataArray,\n1427         indexers: Mapping[Any, Any] | None = None,\n1428         method: str | None = None,\n1429         tolerance=None,\n1430         drop: bool = False,\n1431         **indexers_kwargs: Any,\n1432     ) -> T_DataArray:\n1433         \"\"\"Return a new DataArray whose data is given by selecting index\n1434         labels along the specified dimension(s).\n1435 \n1436         In contrast to `DataArray.isel`, indexers for this method should use\n1437         labels instead of integers.\n1438 \n1439         Under the hood, this method is powered by using pandas's powerful Index\n1440         objects. This makes label based indexing essentially just as fast as\n1441         using integer indexing.\n1442 \n1443         It also means this method uses pandas's (well documented) logic for\n1444         indexing. This means you can use string shortcuts for datetime indexes\n1445         (e.g., '2000-01' to select all values in January 2000). It also means\n1446         that slices are treated as inclusive of both the start and stop values,\n1447         unlike normal Python indexing.\n1448 \n1449         .. warning::\n1450 \n1451           Do not try to assign values when using any of the indexing methods\n1452           ``isel`` or ``sel``::\n1453 \n1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1455             # DO NOT do this\n1456             da.isel(x=[0, 1, 2])[1] = -1\n1457 \n1458           Assigning values with the chained indexing using ``.sel`` or\n1459           ``.isel`` fails silently.\n1460 \n1461         Parameters\n1462         ----------\n1463         indexers : dict, optional\n1464             A dict with keys matching dimensions and values given\n1465             by scalars, slices or arrays of tick labels. For dimensions with\n1466             multi-index, the indexer may also be a dict-like object with keys\n1467             matching index level names.\n1468             If DataArrays are passed as indexers, xarray-style indexing will be\n1469             carried out. See :ref:`indexing` for the details.\n1470             One of indexers or indexers_kwargs must be provided.\n1471         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1472             Method to use for inexact matches:\n1473 \n1474             - None (default): only exact matches\n1475             - pad / ffill: propagate last valid index value forward\n1476             - backfill / bfill: propagate next valid index value backward\n1477             - nearest: use nearest valid index value\n1478 \n1479         tolerance : optional\n1480             Maximum distance between original and new labels for inexact\n1481             matches. The values of the index at the matching locations must\n1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1483         drop : bool, optional\n1484             If ``drop=True``, drop coordinates variables in `indexers` instead\n1485             of making them scalar.\n1486         **indexers_kwargs : {dim: indexer, ...}, optional\n1487             The keyword arguments form of ``indexers``.\n1488             One of indexers or indexers_kwargs must be provided.\n1489 \n1490         Returns\n1491         -------\n1492         obj : DataArray\n1493             A new DataArray with the same contents as this DataArray, except the\n1494             data and each dimension is indexed by the appropriate indexers.\n1495             If indexer DataArrays have coordinates that do not conflict with\n1496             this object, then these coordinates will be attached.\n1497             In general, each array's data will be a view of the array's data\n1498             in this DataArray, unless vectorized indexing was triggered by using\n1499             an array indexer, in which case the data will be a copy.\n1500 \n1501         See Also\n1502         --------\n1503         Dataset.sel\n1504         DataArray.isel\n1505 \n1506         Examples\n1507         --------\n1508         >>> da = xr.DataArray(\n1509         ...     np.arange(25).reshape(5, 5),\n1510         ...     coords={\"x\": np.arange(5), \"y\": np.arange(5)},\n1511         ...     dims=(\"x\", \"y\"),\n1512         ... )\n1513         >>> da\n1514         <xarray.DataArray (x: 5, y: 5)>\n1515         array([[ 0,  1,  2,  3,  4],\n1516                [ 5,  6,  7,  8,  9],\n1517                [10, 11, 12, 13, 14],\n1518                [15, 16, 17, 18, 19],\n1519                [20, 21, 22, 23, 24]])\n1520         Coordinates:\n1521           * x        (x) int64 0 1 2 3 4\n1522           * y        (y) int64 0 1 2 3 4\n1523 \n1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method=\"nearest\")\n1527         >>> da\n1528         <xarray.DataArray (points: 5)>\n1529         array([ 0,  6, 12, 18, 24])\n1530         Coordinates:\n1531             x        (points) int64 0 1 2 3 4\n1532             y        (points) int64 0 1 2 3 4\n1533         Dimensions without coordinates: points\n1534         \"\"\"\n1535         ds = self._to_temp_dataset().sel(\n1536             indexers=indexers,\n1537             drop=drop,\n1538             method=method,\n1539             tolerance=tolerance,\n1540             **indexers_kwargs,\n1541         )\n1542         return self._from_temp_dataset(ds)\n1543 \n1544     def head(\n1545         self: T_DataArray,\n1546         indexers: Mapping[Any, int] | int | None = None,\n1547         **indexers_kwargs: Any,\n1548     ) -> T_DataArray:\n1549         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1550         values along the specified dimension(s). Default `n` = 5\n1551 \n1552         See Also\n1553         --------\n1554         Dataset.head\n1555         DataArray.tail\n1556         DataArray.thin\n1557 \n1558         Examples\n1559         --------\n1560         >>> da = xr.DataArray(\n1561         ...     np.arange(25).reshape(5, 5),\n1562         ...     dims=(\"x\", \"y\"),\n1563         ... )\n1564         >>> da\n1565         <xarray.DataArray (x: 5, y: 5)>\n1566         array([[ 0,  1,  2,  3,  4],\n1567                [ 5,  6,  7,  8,  9],\n1568                [10, 11, 12, 13, 14],\n1569                [15, 16, 17, 18, 19],\n1570                [20, 21, 22, 23, 24]])\n1571         Dimensions without coordinates: x, y\n1572 \n1573         >>> da.head(x=1)\n1574         <xarray.DataArray (x: 1, y: 5)>\n1575         array([[0, 1, 2, 3, 4]])\n1576         Dimensions without coordinates: x, y\n1577 \n1578         >>> da.head({\"x\": 2, \"y\": 2})\n1579         <xarray.DataArray (x: 2, y: 2)>\n1580         array([[0, 1],\n1581                [5, 6]])\n1582         Dimensions without coordinates: x, y\n1583         \"\"\"\n1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1585         return self._from_temp_dataset(ds)\n1586 \n1587     def tail(\n1588         self: T_DataArray,\n1589         indexers: Mapping[Any, int] | int | None = None,\n1590         **indexers_kwargs: Any,\n1591     ) -> T_DataArray:\n1592         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1593         values along the specified dimension(s). Default `n` = 5\n1594 \n1595         See Also\n1596         --------\n1597         Dataset.tail\n1598         DataArray.head\n1599         DataArray.thin\n1600 \n1601         Examples\n1602         --------\n1603         >>> da = xr.DataArray(\n1604         ...     np.arange(25).reshape(5, 5),\n1605         ...     dims=(\"x\", \"y\"),\n1606         ... )\n1607         >>> da\n1608         <xarray.DataArray (x: 5, y: 5)>\n1609         array([[ 0,  1,  2,  3,  4],\n1610                [ 5,  6,  7,  8,  9],\n1611                [10, 11, 12, 13, 14],\n1612                [15, 16, 17, 18, 19],\n1613                [20, 21, 22, 23, 24]])\n1614         Dimensions without coordinates: x, y\n1615 \n1616         >>> da.tail(y=1)\n1617         <xarray.DataArray (x: 5, y: 1)>\n1618         array([[ 4],\n1619                [ 9],\n1620                [14],\n1621                [19],\n1622                [24]])\n1623         Dimensions without coordinates: x, y\n1624 \n1625         >>> da.tail({\"x\": 2, \"y\": 2})\n1626         <xarray.DataArray (x: 2, y: 2)>\n1627         array([[18, 19],\n1628                [23, 24]])\n1629         Dimensions without coordinates: x, y\n1630         \"\"\"\n1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1632         return self._from_temp_dataset(ds)\n1633 \n1634     def thin(\n1635         self: T_DataArray,\n1636         indexers: Mapping[Any, int] | int | None = None,\n1637         **indexers_kwargs: Any,\n1638     ) -> T_DataArray:\n1639         \"\"\"Return a new DataArray whose data is given by each `n` value\n1640         along the specified dimension(s).\n1641 \n1642         Examples\n1643         --------\n1644         >>> x_arr = np.arange(0, 26)\n1645         >>> x_arr\n1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n1647                17, 18, 19, 20, 21, 22, 23, 24, 25])\n1648         >>> x = xr.DataArray(\n1649         ...     np.reshape(x_arr, (2, 13)),\n1650         ...     dims=(\"x\", \"y\"),\n1651         ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n1652         ... )\n1653         >>> x\n1654         <xarray.DataArray (x: 2, y: 13)>\n1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])\n1657         Coordinates:\n1658           * x        (x) int64 0 1\n1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n1660 \n1661         >>>\n1662         >>> x.thin(3)\n1663         <xarray.DataArray (x: 1, y: 5)>\n1664         array([[ 0,  3,  6,  9, 12]])\n1665         Coordinates:\n1666           * x        (x) int64 0\n1667           * y        (y) int64 0 3 6 9 12\n1668         >>> x.thin({\"x\": 2, \"y\": 5})\n1669         <xarray.DataArray (x: 1, y: 3)>\n1670         array([[ 0,  5, 10]])\n1671         Coordinates:\n1672           * x        (x) int64 0\n1673           * y        (y) int64 0 5 10\n1674 \n1675         See Also\n1676         --------\n1677         Dataset.thin\n1678         DataArray.head\n1679         DataArray.tail\n1680         \"\"\"\n1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1682         return self._from_temp_dataset(ds)\n1683 \n1684     def broadcast_like(\n1685         self: T_DataArray,\n1686         other: DataArray | Dataset,\n1687         exclude: Iterable[Hashable] | None = None,\n1688     ) -> T_DataArray:\n1689         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1690 \n1691         This is equivalent to xr.broadcast(other, self)[1]\n1692 \n1693         xarray objects are broadcast against each other in arithmetic\n1694         operations, so this method is not be necessary for most uses.\n1695 \n1696         If no change is needed, the input data is returned to the output\n1697         without being copied.\n1698 \n1699         If new coords are added by the broadcast, their values are\n1700         NaN filled.\n1701 \n1702         Parameters\n1703         ----------\n1704         other : Dataset or DataArray\n1705             Object against which to broadcast this array.\n1706         exclude : iterable of Hashable, optional\n1707             Dimensions that must not be broadcasted\n1708 \n1709         Returns\n1710         -------\n1711         new_da : DataArray\n1712             The caller broadcasted against ``other``.\n1713 \n1714         Examples\n1715         --------\n1716         >>> arr1 = xr.DataArray(\n1717         ...     np.random.randn(2, 3),\n1718         ...     dims=(\"x\", \"y\"),\n1719         ...     coords={\"x\": [\"a\", \"b\"], \"y\": [\"a\", \"b\", \"c\"]},\n1720         ... )\n1721         >>> arr2 = xr.DataArray(\n1722         ...     np.random.randn(3, 2),\n1723         ...     dims=(\"x\", \"y\"),\n1724         ...     coords={\"x\": [\"a\", \"b\", \"c\"], \"y\": [\"a\", \"b\"]},\n1725         ... )\n1726         >>> arr1\n1727         <xarray.DataArray (x: 2, y: 3)>\n1728         array([[ 1.76405235,  0.40015721,  0.97873798],\n1729                [ 2.2408932 ,  1.86755799, -0.97727788]])\n1730         Coordinates:\n1731           * x        (x) <U1 'a' 'b'\n1732           * y        (y) <U1 'a' 'b' 'c'\n1733         >>> arr2\n1734         <xarray.DataArray (x: 3, y: 2)>\n1735         array([[ 0.95008842, -0.15135721],\n1736                [-0.10321885,  0.4105985 ],\n1737                [ 0.14404357,  1.45427351]])\n1738         Coordinates:\n1739           * x        (x) <U1 'a' 'b' 'c'\n1740           * y        (y) <U1 'a' 'b'\n1741         >>> arr1.broadcast_like(arr2)\n1742         <xarray.DataArray (x: 3, y: 3)>\n1743         array([[ 1.76405235,  0.40015721,  0.97873798],\n1744                [ 2.2408932 ,  1.86755799, -0.97727788],\n1745                [        nan,         nan,         nan]])\n1746         Coordinates:\n1747           * x        (x) <U1 'a' 'b' 'c'\n1748           * y        (y) <U1 'a' 'b' 'c'\n1749         \"\"\"\n1750         if exclude is None:\n1751             exclude = set()\n1752         else:\n1753             exclude = set(exclude)\n1754         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1755 \n1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1757 \n1758         return _broadcast_helper(\n1759             cast(\"T_DataArray\", args[1]), exclude, dims_map, common_coords\n1760         )\n1761 \n1762     def _reindex_callback(\n1763         self: T_DataArray,\n1764         aligner: alignment.Aligner,\n1765         dim_pos_indexers: dict[Hashable, Any],\n1766         variables: dict[Hashable, Variable],\n1767         indexes: dict[Hashable, Index],\n1768         fill_value: Any,\n1769         exclude_dims: frozenset[Hashable],\n1770         exclude_vars: frozenset[Hashable],\n1771     ) -> T_DataArray:\n1772         \"\"\"Callback called from ``Aligner`` to create a new reindexed DataArray.\"\"\"\n1773 \n1774         if isinstance(fill_value, dict):\n1775             fill_value = fill_value.copy()\n1776             sentinel = object()\n1777             value = fill_value.pop(self.name, sentinel)\n1778             if value is not sentinel:\n1779                 fill_value[_THIS_ARRAY] = value\n1780 \n1781         ds = self._to_temp_dataset()\n1782         reindexed = ds._reindex_callback(\n1783             aligner,\n1784             dim_pos_indexers,\n1785             variables,\n1786             indexes,\n1787             fill_value,\n1788             exclude_dims,\n1789             exclude_vars,\n1790         )\n1791         return self._from_temp_dataset(reindexed)\n1792 \n1793     def reindex_like(\n1794         self: T_DataArray,\n1795         other: DataArray | Dataset,\n1796         method: ReindexMethodOptions = None,\n1797         tolerance: int | float | Iterable[int | float] | None = None,\n1798         copy: bool = True,\n1799         fill_value=dtypes.NA,\n1800     ) -> T_DataArray:\n1801         \"\"\"Conform this object onto the indexes of another object, filling in\n1802         missing values with ``fill_value``. The default fill value is NaN.\n1803 \n1804         Parameters\n1805         ----------\n1806         other : Dataset or DataArray\n1807             Object with an 'indexes' attribute giving a mapping from dimension\n1808             names to pandas.Index objects, which provides coordinates upon\n1809             which to index the variables in this dataset. The indexes on this\n1810             other object need not be the same as the indexes on this\n1811             dataset. Any mis-matched index values will be filled in with\n1812             NaN, and any mis-matched dimension names will simply be ignored.\n1813         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1814             Method to use for filling index values from other not found on this\n1815             data array:\n1816 \n1817             - None (default): don't fill gaps\n1818             - pad / ffill: propagate last valid index value forward\n1819             - backfill / bfill: propagate next valid index value backward\n1820             - nearest: use nearest valid index value\n1821 \n1822         tolerance : optional\n1823             Maximum distance between original and new labels for inexact\n1824             matches. The values of the index at the matching locations must\n1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1826             Tolerance may be a scalar value, which applies the same tolerance\n1827             to all values, or list-like, which applies variable tolerance per\n1828             element. List-like must be the same size as the index and its dtype\n1829             must exactly match the index\u2019s type.\n1830         copy : bool, default: True\n1831             If ``copy=True``, data in the return value is always copied. If\n1832             ``copy=False`` and reindexing is unnecessary, or can be performed\n1833             with only slice operations, then the output may share memory with\n1834             the input. In either case, a new xarray object is always returned.\n1835         fill_value : scalar or dict-like, optional\n1836             Value to use for newly missing values. If a dict-like, maps\n1837             variable names (including coordinates) to fill values. Use this\n1838             data array's name to refer to the data array's values.\n1839 \n1840         Returns\n1841         -------\n1842         reindexed : DataArray\n1843             Another dataset array, with this array's data but coordinates from\n1844             the other object.\n1845 \n1846         Examples\n1847         --------\n1848         >>> data = np.arange(12).reshape(4, 3)\n1849         >>> da1 = xr.DataArray(\n1850         ...     data=data,\n1851         ...     dims=[\"x\", \"y\"],\n1852         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n1853         ... )\n1854         >>> da1\n1855         <xarray.DataArray (x: 4, y: 3)>\n1856         array([[ 0,  1,  2],\n1857                [ 3,  4,  5],\n1858                [ 6,  7,  8],\n1859                [ 9, 10, 11]])\n1860         Coordinates:\n1861           * x        (x) int64 10 20 30 40\n1862           * y        (y) int64 70 80 90\n1863         >>> da2 = xr.DataArray(\n1864         ...     data=data,\n1865         ...     dims=[\"x\", \"y\"],\n1866         ...     coords={\"x\": [40, 30, 20, 10], \"y\": [90, 80, 70]},\n1867         ... )\n1868         >>> da2\n1869         <xarray.DataArray (x: 4, y: 3)>\n1870         array([[ 0,  1,  2],\n1871                [ 3,  4,  5],\n1872                [ 6,  7,  8],\n1873                [ 9, 10, 11]])\n1874         Coordinates:\n1875           * x        (x) int64 40 30 20 10\n1876           * y        (y) int64 90 80 70\n1877 \n1878         Reindexing with both DataArrays having the same coordinates set, but in different order:\n1879 \n1880         >>> da1.reindex_like(da2)\n1881         <xarray.DataArray (x: 4, y: 3)>\n1882         array([[11, 10,  9],\n1883                [ 8,  7,  6],\n1884                [ 5,  4,  3],\n1885                [ 2,  1,  0]])\n1886         Coordinates:\n1887           * x        (x) int64 40 30 20 10\n1888           * y        (y) int64 90 80 70\n1889 \n1890         Reindexing with the other array having coordinates which the source array doesn't have:\n1891 \n1892         >>> data = np.arange(12).reshape(4, 3)\n1893         >>> da1 = xr.DataArray(\n1894         ...     data=data,\n1895         ...     dims=[\"x\", \"y\"],\n1896         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n1897         ... )\n1898         >>> da2 = xr.DataArray(\n1899         ...     data=data,\n1900         ...     dims=[\"x\", \"y\"],\n1901         ...     coords={\"x\": [20, 10, 29, 39], \"y\": [70, 80, 90]},\n1902         ... )\n1903         >>> da1.reindex_like(da2)\n1904         <xarray.DataArray (x: 4, y: 3)>\n1905         array([[ 3.,  4.,  5.],\n1906                [ 0.,  1.,  2.],\n1907                [nan, nan, nan],\n1908                [nan, nan, nan]])\n1909         Coordinates:\n1910           * x        (x) int64 20 10 29 39\n1911           * y        (y) int64 70 80 90\n1912 \n1913         Filling missing values with the previous valid index with respect to the coordinates' value:\n1914 \n1915         >>> da1.reindex_like(da2, method=\"ffill\")\n1916         <xarray.DataArray (x: 4, y: 3)>\n1917         array([[3, 4, 5],\n1918                [0, 1, 2],\n1919                [3, 4, 5],\n1920                [6, 7, 8]])\n1921         Coordinates:\n1922           * x        (x) int64 20 10 29 39\n1923           * y        (y) int64 70 80 90\n1924 \n1925         Filling missing values while tolerating specified error for inexact matches:\n1926 \n1927         >>> da1.reindex_like(da2, method=\"ffill\", tolerance=5)\n1928         <xarray.DataArray (x: 4, y: 3)>\n1929         array([[ 3.,  4.,  5.],\n1930                [ 0.,  1.,  2.],\n1931                [nan, nan, nan],\n1932                [nan, nan, nan]])\n1933         Coordinates:\n1934           * x        (x) int64 20 10 29 39\n1935           * y        (y) int64 70 80 90\n1936 \n1937         Filling missing values with manually specified values:\n1938 \n1939         >>> da1.reindex_like(da2, fill_value=19)\n1940         <xarray.DataArray (x: 4, y: 3)>\n1941         array([[ 3,  4,  5],\n1942                [ 0,  1,  2],\n1943                [19, 19, 19],\n1944                [19, 19, 19]])\n1945         Coordinates:\n1946           * x        (x) int64 20 10 29 39\n1947           * y        (y) int64 70 80 90\n1948 \n1949         See Also\n1950         --------\n1951         DataArray.reindex\n1952         align\n1953         \"\"\"\n1954         return alignment.reindex_like(\n1955             self,\n1956             other=other,\n1957             method=method,\n1958             tolerance=tolerance,\n1959             copy=copy,\n1960             fill_value=fill_value,\n1961         )\n1962 \n1963     def reindex(\n1964         self: T_DataArray,\n1965         indexers: Mapping[Any, Any] | None = None,\n1966         method: ReindexMethodOptions = None,\n1967         tolerance: float | Iterable[float] | None = None,\n1968         copy: bool = True,\n1969         fill_value=dtypes.NA,\n1970         **indexers_kwargs: Any,\n1971     ) -> T_DataArray:\n1972         \"\"\"Conform this object onto the indexes of another object, filling in\n1973         missing values with ``fill_value``. The default fill value is NaN.\n1974 \n1975         Parameters\n1976         ----------\n1977         indexers : dict, optional\n1978             Dictionary with keys given by dimension names and values given by\n1979             arrays of coordinates tick labels. Any mis-matched coordinate\n1980             values will be filled in with NaN, and any mis-matched dimension\n1981             names will simply be ignored.\n1982             One of indexers or indexers_kwargs must be provided.\n1983         copy : bool, optional\n1984             If ``copy=True``, data in the return value is always copied. If\n1985             ``copy=False`` and reindexing is unnecessary, or can be performed\n1986             with only slice operations, then the output may share memory with\n1987             the input. In either case, a new xarray object is always returned.\n1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1989             Method to use for filling index values in ``indexers`` not found on\n1990             this data array:\n1991 \n1992             - None (default): don't fill gaps\n1993             - pad / ffill: propagate last valid index value forward\n1994             - backfill / bfill: propagate next valid index value backward\n1995             - nearest: use nearest valid index value\n1996 \n1997         tolerance : float | Iterable[float] | None, default: None\n1998             Maximum distance between original and new labels for inexact\n1999             matches. The values of the index at the matching locations must\n2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2001             Tolerance may be a scalar value, which applies the same tolerance\n2002             to all values, or list-like, which applies variable tolerance per\n2003             element. List-like must be the same size as the index and its dtype\n2004             must exactly match the index\u2019s type.\n2005         fill_value : scalar or dict-like, optional\n2006             Value to use for newly missing values. If a dict-like, maps\n2007             variable names (including coordinates) to fill values. Use this\n2008             data array's name to refer to the data array's values.\n2009         **indexers_kwargs : {dim: indexer, ...}, optional\n2010             The keyword arguments form of ``indexers``.\n2011             One of indexers or indexers_kwargs must be provided.\n2012 \n2013         Returns\n2014         -------\n2015         reindexed : DataArray\n2016             Another dataset array, with this array's data but replaced\n2017             coordinates.\n2018 \n2019         Examples\n2020         --------\n2021         Reverse latitude:\n2022 \n2023         >>> da = xr.DataArray(\n2024         ...     np.arange(4),\n2025         ...     coords=[np.array([90, 89, 88, 87])],\n2026         ...     dims=\"lat\",\n2027         ... )\n2028         >>> da\n2029         <xarray.DataArray (lat: 4)>\n2030         array([0, 1, 2, 3])\n2031         Coordinates:\n2032           * lat      (lat) int64 90 89 88 87\n2033         >>> da.reindex(lat=da.lat[::-1])\n2034         <xarray.DataArray (lat: 4)>\n2035         array([3, 2, 1, 0])\n2036         Coordinates:\n2037           * lat      (lat) int64 87 88 89 90\n2038 \n2039         See Also\n2040         --------\n2041         DataArray.reindex_like\n2042         align\n2043         \"\"\"\n2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n2045         return alignment.reindex(\n2046             self,\n2047             indexers=indexers,\n2048             method=method,\n2049             tolerance=tolerance,\n2050             copy=copy,\n2051             fill_value=fill_value,\n2052         )\n2053 \n2054     def interp(\n2055         self: T_DataArray,\n2056         coords: Mapping[Any, Any] | None = None,\n2057         method: InterpOptions = \"linear\",\n2058         assume_sorted: bool = False,\n2059         kwargs: Mapping[str, Any] | None = None,\n2060         **coords_kwargs: Any,\n2061     ) -> T_DataArray:\n2062         \"\"\"Interpolate a DataArray onto new coordinates\n2063 \n2064         Performs univariate or multivariate interpolation of a DataArray onto\n2065         new coordinates using scipy's interpolation routines. If interpolating\n2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n2067         called. When interpolating along multiple existing dimensions, an\n2068         attempt is made to decompose the interpolation into multiple\n2069         1-dimensional interpolations. If this is possible,\n2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n2071         :py:func:`scipy.interpolate.interpn` is called.\n2072 \n2073         Parameters\n2074         ----------\n2075         coords : dict, optional\n2076             Mapping from dimension names to the new coordinates.\n2077             New coordinate can be a scalar, array-like or DataArray.\n2078             If DataArrays are passed as new coordinates, their dimensions are\n2079             used for the broadcasting. Missing values are skipped.\n2080         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n2081             The method used to interpolate. The method should be supported by\n2082             the scipy interpolator:\n2083 \n2084             - ``interp1d``: {\"linear\", \"nearest\", \"zero\", \"slinear\",\n2085               \"quadratic\", \"cubic\", \"polynomial\"}\n2086             - ``interpn``: {\"linear\", \"nearest\"}\n2087 \n2088             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n2089             also be provided.\n2090         assume_sorted : bool, default: False\n2091             If False, values of x can be in any order and they are sorted\n2092             first. If True, x has to be an array of monotonically increasing\n2093             values.\n2094         kwargs : dict-like or None, default: None\n2095             Additional keyword arguments passed to scipy's interpolator. Valid\n2096             options and their behavior depend whether ``interp1d`` or\n2097             ``interpn`` is used.\n2098         **coords_kwargs : {dim: coordinate, ...}, optional\n2099             The keyword arguments form of ``coords``.\n2100             One of coords or coords_kwargs must be provided.\n2101 \n2102         Returns\n2103         -------\n2104         interpolated : DataArray\n2105             New dataarray on the new coordinates.\n2106 \n2107         Notes\n2108         -----\n2109         scipy is required.\n2110 \n2111         See Also\n2112         --------\n2113         scipy.interpolate.interp1d\n2114         scipy.interpolate.interpn\n2115 \n2116         Examples\n2117         --------\n2118         >>> da = xr.DataArray(\n2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n2120         ...     dims=(\"x\", \"y\"),\n2121         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n2122         ... )\n2123         >>> da\n2124         <xarray.DataArray (x: 3, y: 4)>\n2125         array([[ 1.,  4.,  2.,  9.],\n2126                [ 2.,  7.,  6., nan],\n2127                [ 6., nan,  5.,  8.]])\n2128         Coordinates:\n2129           * x        (x) int64 0 1 2\n2130           * y        (y) int64 10 12 14 16\n2131 \n2132         1D linear interpolation (the default):\n2133 \n2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])\n2135         <xarray.DataArray (x: 4, y: 4)>\n2136         array([[1.  , 4.  , 2.  ,  nan],\n2137                [1.75, 6.25, 5.  ,  nan],\n2138                [3.  ,  nan, 5.75,  nan],\n2139                [5.  ,  nan, 5.25,  nan]])\n2140         Coordinates:\n2141           * y        (y) int64 10 12 14 16\n2142           * x        (x) float64 0.0 0.75 1.25 1.75\n2143 \n2144         1D nearest interpolation:\n2145 \n2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n2147         <xarray.DataArray (x: 4, y: 4)>\n2148         array([[ 1.,  4.,  2.,  9.],\n2149                [ 2.,  7.,  6., nan],\n2150                [ 2.,  7.,  6., nan],\n2151                [ 6., nan,  5.,  8.]])\n2152         Coordinates:\n2153           * y        (y) int64 10 12 14 16\n2154           * x        (x) float64 0.0 0.75 1.25 1.75\n2155 \n2156         1D linear extrapolation:\n2157 \n2158         >>> da.interp(\n2159         ...     x=[1, 1.5, 2.5, 3.5],\n2160         ...     method=\"linear\",\n2161         ...     kwargs={\"fill_value\": \"extrapolate\"},\n2162         ... )\n2163         <xarray.DataArray (x: 4, y: 4)>\n2164         array([[ 2. ,  7. ,  6. ,  nan],\n2165                [ 4. ,  nan,  5.5,  nan],\n2166                [ 8. ,  nan,  4.5,  nan],\n2167                [12. ,  nan,  3.5,  nan]])\n2168         Coordinates:\n2169           * y        (y) int64 10 12 14 16\n2170           * x        (x) float64 1.0 1.5 2.5 3.5\n2171 \n2172         2D linear interpolation:\n2173 \n2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n2175         <xarray.DataArray (x: 4, y: 3)>\n2176         array([[2.5  , 3.   ,   nan],\n2177                [4.   , 5.625,   nan],\n2178                [  nan,   nan,   nan],\n2179                [  nan,   nan,   nan]])\n2180         Coordinates:\n2181           * x        (x) float64 0.0 0.75 1.25 1.75\n2182           * y        (y) int64 11 13 15\n2183         \"\"\"\n2184         if self.dtype.kind not in \"uifc\":\n2185             raise TypeError(\n2186                 \"interp only works for a numeric type array. \"\n2187                 \"Given {}.\".format(self.dtype)\n2188             )\n2189         ds = self._to_temp_dataset().interp(\n2190             coords,\n2191             method=method,\n2192             kwargs=kwargs,\n2193             assume_sorted=assume_sorted,\n2194             **coords_kwargs,\n2195         )\n2196         return self._from_temp_dataset(ds)\n2197 \n2198     def interp_like(\n2199         self: T_DataArray,\n2200         other: DataArray | Dataset,\n2201         method: InterpOptions = \"linear\",\n2202         assume_sorted: bool = False,\n2203         kwargs: Mapping[str, Any] | None = None,\n2204     ) -> T_DataArray:\n2205         \"\"\"Interpolate this object onto the coordinates of another object,\n2206         filling out of range values with NaN.\n2207 \n2208         If interpolating along a single existing dimension,\n2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n2210         along multiple existing dimensions, an attempt is made to decompose the\n2211         interpolation into multiple 1-dimensional interpolations. If this is\n2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n2213         :py:func:`scipy.interpolate.interpn` is called.\n2214 \n2215         Parameters\n2216         ----------\n2217         other : Dataset or DataArray\n2218             Object with an 'indexes' attribute giving a mapping from dimension\n2219             names to an 1d array-like, which provides coordinates upon\n2220             which to index the variables in this dataset. Missing values are skipped.\n2221         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n2222             The method used to interpolate. The method should be supported by\n2223             the scipy interpolator:\n2224 \n2225             - {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\",\n2226               \"polynomial\"} when ``interp1d`` is called.\n2227             - {\"linear\", \"nearest\"} when ``interpn`` is called.\n2228 \n2229             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n2230             also be provided.\n2231         assume_sorted : bool, default: False\n2232             If False, values of coordinates that are interpolated over can be\n2233             in any order and they are sorted first. If True, interpolated\n2234             coordinates are assumed to be an array of monotonically increasing\n2235             values.\n2236         kwargs : dict, optional\n2237             Additional keyword passed to scipy's interpolator.\n2238 \n2239         Returns\n2240         -------\n2241         interpolated : DataArray\n2242             Another dataarray by interpolating this dataarray's data along the\n2243             coordinates of the other object.\n2244 \n2245         Examples\n2246         --------\n2247         >>> data = np.arange(12).reshape(4, 3)\n2248         >>> da1 = xr.DataArray(\n2249         ...     data=data,\n2250         ...     dims=[\"x\", \"y\"],\n2251         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n2252         ... )\n2253         >>> da1\n2254         <xarray.DataArray (x: 4, y: 3)>\n2255         array([[ 0,  1,  2],\n2256                [ 3,  4,  5],\n2257                [ 6,  7,  8],\n2258                [ 9, 10, 11]])\n2259         Coordinates:\n2260           * x        (x) int64 10 20 30 40\n2261           * y        (y) int64 70 80 90\n2262         >>> da2 = xr.DataArray(\n2263         ...     data=data,\n2264         ...     dims=[\"x\", \"y\"],\n2265         ...     coords={\"x\": [10, 20, 29, 39], \"y\": [70, 80, 90]},\n2266         ... )\n2267         >>> da2\n2268         <xarray.DataArray (x: 4, y: 3)>\n2269         array([[ 0,  1,  2],\n2270                [ 3,  4,  5],\n2271                [ 6,  7,  8],\n2272                [ 9, 10, 11]])\n2273         Coordinates:\n2274           * x        (x) int64 10 20 29 39\n2275           * y        (y) int64 70 80 90\n2276 \n2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:\n2278 \n2279         >>> da2.interp_like(da1)\n2280         <xarray.DataArray (x: 4, y: 3)>\n2281         array([[0. , 1. , 2. ],\n2282                [3. , 4. , 5. ],\n2283                [6.3, 7.3, 8.3],\n2284                [nan, nan, nan]])\n2285         Coordinates:\n2286           * x        (x) int64 10 20 30 40\n2287           * y        (y) int64 70 80 90\n2288 \n2289         Could also extrapolate missing values:\n2290 \n2291         >>> da2.interp_like(da1, kwargs={\"fill_value\": \"extrapolate\"})\n2292         <xarray.DataArray (x: 4, y: 3)>\n2293         array([[ 0. ,  1. ,  2. ],\n2294                [ 3. ,  4. ,  5. ],\n2295                [ 6.3,  7.3,  8.3],\n2296                [ 9.3, 10.3, 11.3]])\n2297         Coordinates:\n2298           * x        (x) int64 10 20 30 40\n2299           * y        (y) int64 70 80 90\n2300 \n2301         Notes\n2302         -----\n2303         scipy is required.\n2304         If the dataarray has object-type coordinates, reindex is used for these\n2305         coordinates instead of the interpolation.\n2306 \n2307         See Also\n2308         --------\n2309         DataArray.interp\n2310         DataArray.reindex_like\n2311         \"\"\"\n2312         if self.dtype.kind not in \"uifc\":\n2313             raise TypeError(\n2314                 \"interp only works for a numeric type array. \"\n2315                 \"Given {}.\".format(self.dtype)\n2316             )\n2317         ds = self._to_temp_dataset().interp_like(\n2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n2319         )\n2320         return self._from_temp_dataset(ds)\n2321 \n2322     # change type of self and return to T_DataArray once\n2323     # https://github.com/python/mypy/issues/12846 is resolved\n2324     def rename(\n2325         self,\n2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,\n2327         **names: Hashable,\n2328     ) -> DataArray:\n2329         \"\"\"Returns a new DataArray with renamed coordinates, dimensions or a new name.\n2330 \n2331         Parameters\n2332         ----------\n2333         new_name_or_name_dict : str or dict-like, optional\n2334             If the argument is dict-like, it used as a mapping from old\n2335             names to new names for coordinates or dimensions. Otherwise,\n2336             use the argument as the new name for this array.\n2337         **names : Hashable, optional\n2338             The keyword arguments form of a mapping from old names to\n2339             new names for coordinates or dimensions.\n2340             One of new_name_or_name_dict or names must be provided.\n2341 \n2342         Returns\n2343         -------\n2344         renamed : DataArray\n2345             Renamed array or array with renamed coordinates.\n2346 \n2347         See Also\n2348         --------\n2349         Dataset.rename\n2350         DataArray.swap_dims\n2351         \"\"\"\n2352         if new_name_or_name_dict is None and not names:\n2353             # change name to None?\n2354             return self._replace(name=None)\n2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:\n2356             # change dims/coords\n2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n2358             dataset = self._to_temp_dataset()._rename(name_dict)\n2359             return self._from_temp_dataset(dataset)\n2360         if utils.hashable(new_name_or_name_dict) and names:\n2361             # change name + dims/coords\n2362             dataset = self._to_temp_dataset()._rename(names)\n2363             dataarray = self._from_temp_dataset(dataset)\n2364             return dataarray._replace(name=new_name_or_name_dict)\n2365         # only change name\n2366         return self._replace(name=new_name_or_name_dict)\n2367 \n2368     def swap_dims(\n2369         self: T_DataArray,\n2370         dims_dict: Mapping[Any, Hashable] | None = None,\n2371         **dims_kwargs,\n2372     ) -> T_DataArray:\n2373         \"\"\"Returns a new DataArray with swapped dimensions.\n2374 \n2375         Parameters\n2376         ----------\n2377         dims_dict : dict-like\n2378             Dictionary whose keys are current dimension names and whose values\n2379             are new names.\n2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n2381             The keyword arguments form of ``dims_dict``.\n2382             One of dims_dict or dims_kwargs must be provided.\n2383 \n2384         Returns\n2385         -------\n2386         swapped : DataArray\n2387             DataArray with swapped dimensions.\n2388 \n2389         Examples\n2390         --------\n2391         >>> arr = xr.DataArray(\n2392         ...     data=[0, 1],\n2393         ...     dims=\"x\",\n2394         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n2395         ... )\n2396         >>> arr\n2397         <xarray.DataArray (x: 2)>\n2398         array([0, 1])\n2399         Coordinates:\n2400           * x        (x) <U1 'a' 'b'\n2401             y        (x) int64 0 1\n2402 \n2403         >>> arr.swap_dims({\"x\": \"y\"})\n2404         <xarray.DataArray (y: 2)>\n2405         array([0, 1])\n2406         Coordinates:\n2407             x        (y) <U1 'a' 'b'\n2408           * y        (y) int64 0 1\n2409 \n2410         >>> arr.swap_dims({\"x\": \"z\"})\n2411         <xarray.DataArray (z: 2)>\n2412         array([0, 1])\n2413         Coordinates:\n2414             x        (z) <U1 'a' 'b'\n2415             y        (z) int64 0 1\n2416         Dimensions without coordinates: z\n2417 \n2418         See Also\n2419         --------\n2420         DataArray.rename\n2421         Dataset.swap_dims\n2422         \"\"\"\n2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n2424         ds = self._to_temp_dataset().swap_dims(dims_dict)\n2425         return self._from_temp_dataset(ds)\n2426 \n2427     # change type of self and return to T_DataArray once\n2428     # https://github.com/python/mypy/issues/12846 is resolved\n2429     def expand_dims(\n2430         self,\n2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,\n2432         axis: None | int | Sequence[int] = None,\n2433         **dim_kwargs: Any,\n2434     ) -> DataArray:\n2435         \"\"\"Return a new object with an additional axis (or axes) inserted at\n2436         the corresponding position in the array shape. The new object is a\n2437         view into the underlying array, not a copy.\n2438 \n2439         If dim is already a scalar coordinate, it will be promoted to a 1D\n2440         coordinate consisting of a single value.\n2441 \n2442         Parameters\n2443         ----------\n2444         dim : Hashable, sequence of Hashable, dict, or None, optional\n2445             Dimensions to include on the new variable.\n2446             If provided as str or sequence of str, then dimensions are inserted\n2447             with length 1. If provided as a dict, then the keys are the new\n2448             dimensions and the values are either integers (giving the length of\n2449             the new dimensions) or sequence/ndarray (giving the coordinates of\n2450             the new dimensions).\n2451         axis : int, sequence of int, or None, default: None\n2452             Axis position(s) where new axis is to be inserted (position(s) on\n2453             the result array). If a sequence of integers is passed,\n2454             multiple axes are inserted. In this case, dim arguments should be\n2455             same length list. If axis=None is passed, all the axes will be\n2456             inserted to the start of the result array.\n2457         **dim_kwargs : int or sequence or ndarray\n2458             The keywords are arbitrary dimensions being inserted and the values\n2459             are either the lengths of the new dims (if int is given), or their\n2460             coordinates. Note, this is an alternative to passing a dict to the\n2461             dim kwarg and will only be used if dim is None.\n2462 \n2463         Returns\n2464         -------\n2465         expanded : DataArray\n2466             This object, but with additional dimension(s).\n2467 \n2468         See Also\n2469         --------\n2470         Dataset.expand_dims\n2471 \n2472         Examples\n2473         --------\n2474         >>> da = xr.DataArray(np.arange(5), dims=(\"x\"))\n2475         >>> da\n2476         <xarray.DataArray (x: 5)>\n2477         array([0, 1, 2, 3, 4])\n2478         Dimensions without coordinates: x\n2479 \n2480         Add new dimension of length 2:\n2481 \n2482         >>> da.expand_dims(dim={\"y\": 2})\n2483         <xarray.DataArray (y: 2, x: 5)>\n2484         array([[0, 1, 2, 3, 4],\n2485                [0, 1, 2, 3, 4]])\n2486         Dimensions without coordinates: y, x\n2487 \n2488         >>> da.expand_dims(dim={\"y\": 2}, axis=1)\n2489         <xarray.DataArray (x: 5, y: 2)>\n2490         array([[0, 0],\n2491                [1, 1],\n2492                [2, 2],\n2493                [3, 3],\n2494                [4, 4]])\n2495         Dimensions without coordinates: x, y\n2496 \n2497         Add a new dimension with coordinates from array:\n2498 \n2499         >>> da.expand_dims(dim={\"y\": np.arange(5)}, axis=0)\n2500         <xarray.DataArray (y: 5, x: 5)>\n2501         array([[0, 1, 2, 3, 4],\n2502                [0, 1, 2, 3, 4],\n2503                [0, 1, 2, 3, 4],\n2504                [0, 1, 2, 3, 4],\n2505                [0, 1, 2, 3, 4]])\n2506         Coordinates:\n2507           * y        (y) int64 0 1 2 3 4\n2508         Dimensions without coordinates: x\n2509         \"\"\"\n2510         if isinstance(dim, int):\n2511             raise TypeError(\"dim should be Hashable or sequence/mapping of Hashables\")\n2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n2513             if len(dim) != len(set(dim)):\n2514                 raise ValueError(\"dims should not contain duplicate values.\")\n2515             dim = dict.fromkeys(dim, 1)\n2516         elif dim is not None and not isinstance(dim, Mapping):\n2517             dim = {cast(Hashable, dim): 1}\n2518 \n2519         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n2520         ds = self._to_temp_dataset().expand_dims(dim, axis)\n2521         return self._from_temp_dataset(ds)\n2522 \n2523     # change type of self and return to T_DataArray once\n2524     # https://github.com/python/mypy/issues/12846 is resolved\n2525     def set_index(\n2526         self,\n2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,\n2528         append: bool = False,\n2529         **indexes_kwargs: Hashable | Sequence[Hashable],\n2530     ) -> DataArray:\n2531         \"\"\"Set DataArray (multi-)indexes using one or more existing\n2532         coordinates.\n2533 \n2534         This legacy method is limited to pandas (multi-)indexes and\n2535         1-dimensional \"dimension\" coordinates. See\n2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom\n2537         Xarray-compatible index from one or more arbitrary coordinates.\n2538 \n2539         Parameters\n2540         ----------\n2541         indexes : {dim: index, ...}\n2542             Mapping from names matching dimensions and values given\n2543             by (lists of) the names of existing coordinates or variables to set\n2544             as new (multi-)index.\n2545         append : bool, default: False\n2546             If True, append the supplied index(es) to the existing index(es).\n2547             Otherwise replace the existing index(es).\n2548         **indexes_kwargs : optional\n2549             The keyword arguments form of ``indexes``.\n2550             One of indexes or indexes_kwargs must be provided.\n2551 \n2552         Returns\n2553         -------\n2554         obj : DataArray\n2555             Another DataArray, with this data but replaced coordinates.\n2556 \n2557         Examples\n2558         --------\n2559         >>> arr = xr.DataArray(\n2560         ...     data=np.ones((2, 3)),\n2561         ...     dims=[\"x\", \"y\"],\n2562         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n2563         ... )\n2564         >>> arr\n2565         <xarray.DataArray (x: 2, y: 3)>\n2566         array([[1., 1., 1.],\n2567                [1., 1., 1.]])\n2568         Coordinates:\n2569           * x        (x) int64 0 1\n2570           * y        (y) int64 0 1 2\n2571             a        (x) int64 3 4\n2572         >>> arr.set_index(x=\"a\")\n2573         <xarray.DataArray (x: 2, y: 3)>\n2574         array([[1., 1., 1.],\n2575                [1., 1., 1.]])\n2576         Coordinates:\n2577           * x        (x) int64 3 4\n2578           * y        (y) int64 0 1 2\n2579 \n2580         See Also\n2581         --------\n2582         DataArray.reset_index\n2583         DataArray.set_xindex\n2584         \"\"\"\n2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)\n2586         return self._from_temp_dataset(ds)\n2587 \n2588     # change type of self and return to T_DataArray once\n2589     # https://github.com/python/mypy/issues/12846 is resolved\n2590     def reset_index(\n2591         self,\n2592         dims_or_levels: Hashable | Sequence[Hashable],\n2593         drop: bool = False,\n2594     ) -> DataArray:\n2595         \"\"\"Reset the specified index(es) or multi-index level(s).\n2596 \n2597         This legacy method is specific to pandas (multi-)indexes and\n2598         1-dimensional \"dimension\" coordinates. See the more generic\n2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`\n2600         method to respectively drop and set pandas or custom indexes for\n2601         arbitrary coordinates.\n2602 \n2603         Parameters\n2604         ----------\n2605         dims_or_levels : Hashable or sequence of Hashable\n2606             Name(s) of the dimension(s) and/or multi-index level(s) that will\n2607             be reset.\n2608         drop : bool, default: False\n2609             If True, remove the specified indexes and/or multi-index levels\n2610             instead of extracting them as new coordinates (default: False).\n2611 \n2612         Returns\n2613         -------\n2614         obj : DataArray\n2615             Another dataarray, with this dataarray's data but replaced\n2616             coordinates.\n2617 \n2618         See Also\n2619         --------\n2620         DataArray.set_index\n2621         DataArray.set_xindex\n2622         DataArray.drop_indexes\n2623         \"\"\"\n2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)\n2625         return self._from_temp_dataset(ds)\n2626 \n2627     def set_xindex(\n2628         self: T_DataArray,\n2629         coord_names: str | Sequence[Hashable],\n2630         index_cls: type[Index] | None = None,\n2631         **options,\n2632     ) -> T_DataArray:\n2633         \"\"\"Set a new, Xarray-compatible index from one or more existing\n2634         coordinate(s).\n2635 \n2636         Parameters\n2637         ----------\n2638         coord_names : str or list\n2639             Name(s) of the coordinate(s) used to build the index.\n2640             If several names are given, their order matters.\n2641         index_cls : subclass of :class:`~xarray.indexes.Index`\n2642             The type of index to create. By default, try setting\n2643             a pandas (multi-)index from the supplied coordinates.\n2644         **options\n2645             Options passed to the index constructor.\n2646 \n2647         Returns\n2648         -------\n2649         obj : DataArray\n2650             Another dataarray, with this dataarray's data and with a new index.\n2651 \n2652         \"\"\"\n2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)\n2654         return self._from_temp_dataset(ds)\n2655 \n2656     def reorder_levels(\n2657         self: T_DataArray,\n2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,\n2659         **dim_order_kwargs: Sequence[int | Hashable],\n2660     ) -> T_DataArray:\n2661         \"\"\"Rearrange index levels using input order.\n2662 \n2663         Parameters\n2664         ----------\n2665         dim_order dict-like of Hashable to int or Hashable: optional\n2666             Mapping from names matching dimensions and values given\n2667             by lists representing new level orders. Every given dimension\n2668             must have a multi-index.\n2669         **dim_order_kwargs : optional\n2670             The keyword arguments form of ``dim_order``.\n2671             One of dim_order or dim_order_kwargs must be provided.\n2672 \n2673         Returns\n2674         -------\n2675         obj : DataArray\n2676             Another dataarray, with this dataarray's data but replaced\n2677             coordinates.\n2678         \"\"\"\n2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)\n2680         return self._from_temp_dataset(ds)\n2681 \n2682     def stack(\n2683         self: T_DataArray,\n2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,\n2685         create_index: bool | None = True,\n2686         index_cls: type[Index] = PandasMultiIndex,\n2687         **dimensions_kwargs: Sequence[Hashable],\n2688     ) -> T_DataArray:\n2689         \"\"\"\n2690         Stack any number of existing dimensions into a single new dimension.\n2691 \n2692         New dimensions will be added at the end, and the corresponding\n2693         coordinate variables will be combined into a MultiIndex.\n2694 \n2695         Parameters\n2696         ----------\n2697         dimensions : mapping of Hashable to sequence of Hashable\n2698             Mapping of the form `new_name=(dim1, dim2, ...)`.\n2699             Names of new dimensions, and the existing dimensions that they\n2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n2702             all dimensions.\n2703         create_index : bool or None, default: True\n2704             If True, create a multi-index for each of the stacked dimensions.\n2705             If False, don't create any index.\n2706             If None, create a multi-index only if exactly one single (1-d) coordinate\n2707             index is found for every dimension to stack.\n2708         index_cls: class, optional\n2709             Can be used to pass a custom multi-index type. Must be an Xarray index that\n2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.\n2711         **dimensions_kwargs\n2712             The keyword arguments form of ``dimensions``.\n2713             One of dimensions or dimensions_kwargs must be provided.\n2714 \n2715         Returns\n2716         -------\n2717         stacked : DataArray\n2718             DataArray with stacked data.\n2719 \n2720         Examples\n2721         --------\n2722         >>> arr = xr.DataArray(\n2723         ...     np.arange(6).reshape(2, 3),\n2724         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2725         ... )\n2726         >>> arr\n2727         <xarray.DataArray (x: 2, y: 3)>\n2728         array([[0, 1, 2],\n2729                [3, 4, 5]])\n2730         Coordinates:\n2731           * x        (x) <U1 'a' 'b'\n2732           * y        (y) int64 0 1 2\n2733         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2734         >>> stacked.indexes[\"z\"]\n2735         MultiIndex([('a', 0),\n2736                     ('a', 1),\n2737                     ('a', 2),\n2738                     ('b', 0),\n2739                     ('b', 1),\n2740                     ('b', 2)],\n2741                    name='z')\n2742 \n2743         See Also\n2744         --------\n2745         DataArray.unstack\n2746         \"\"\"\n2747         ds = self._to_temp_dataset().stack(\n2748             dimensions,\n2749             create_index=create_index,\n2750             index_cls=index_cls,\n2751             **dimensions_kwargs,\n2752         )\n2753         return self._from_temp_dataset(ds)\n2754 \n2755     # change type of self and return to T_DataArray once\n2756     # https://github.com/python/mypy/issues/12846 is resolved\n2757     def unstack(\n2758         self,\n2759         dim: Dims = None,\n2760         fill_value: Any = dtypes.NA,\n2761         sparse: bool = False,\n2762     ) -> DataArray:\n2763         \"\"\"\n2764         Unstack existing dimensions corresponding to MultiIndexes into\n2765         multiple new dimensions.\n2766 \n2767         New dimensions will be added at the end.\n2768 \n2769         Parameters\n2770         ----------\n2771         dim : str, Iterable of Hashable or None, optional\n2772             Dimension(s) over which to unstack. By default unstacks all\n2773             MultiIndexes.\n2774         fill_value : scalar or dict-like, default: nan\n2775             Value to be filled. If a dict-like, maps variable names to\n2776             fill values. Use the data array's name to refer to its\n2777             name. If not provided or if the dict-like does not contain\n2778             all variables, the dtype's NA value will be used.\n2779         sparse : bool, default: False\n2780             Use sparse-array if True\n2781 \n2782         Returns\n2783         -------\n2784         unstacked : DataArray\n2785             Array with unstacked data.\n2786 \n2787         Examples\n2788         --------\n2789         >>> arr = xr.DataArray(\n2790         ...     np.arange(6).reshape(2, 3),\n2791         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2792         ... )\n2793         >>> arr\n2794         <xarray.DataArray (x: 2, y: 3)>\n2795         array([[0, 1, 2],\n2796                [3, 4, 5]])\n2797         Coordinates:\n2798           * x        (x) <U1 'a' 'b'\n2799           * y        (y) int64 0 1 2\n2800         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2801         >>> stacked.indexes[\"z\"]\n2802         MultiIndex([('a', 0),\n2803                     ('a', 1),\n2804                     ('a', 2),\n2805                     ('b', 0),\n2806                     ('b', 1),\n2807                     ('b', 2)],\n2808                    name='z')\n2809         >>> roundtripped = stacked.unstack()\n2810         >>> arr.identical(roundtripped)\n2811         True\n2812 \n2813         See Also\n2814         --------\n2815         DataArray.stack\n2816         \"\"\"\n2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n2818         return self._from_temp_dataset(ds)\n2819 \n2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:\n2821         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n2822         stacked coordinate.\n2823 \n2824         This is the inverse operation of Dataset.to_stacked_array.\n2825 \n2826         Parameters\n2827         ----------\n2828         dim : Hashable\n2829             Name of existing dimension to unstack\n2830         level : int or Hashable, default: 0\n2831             The MultiIndex level to expand to a dataset along. Can either be\n2832             the integer index of the level or its name.\n2833 \n2834         Returns\n2835         -------\n2836         unstacked: Dataset\n2837 \n2838         Examples\n2839         --------\n2840         >>> arr = xr.DataArray(\n2841         ...     np.arange(6).reshape(2, 3),\n2842         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2843         ... )\n2844         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n2845         >>> data\n2846         <xarray.Dataset>\n2847         Dimensions:  (x: 2, y: 3)\n2848         Coordinates:\n2849           * x        (x) <U1 'a' 'b'\n2850           * y        (y) int64 0 1 2\n2851         Data variables:\n2852             a        (x, y) int64 0 1 2 3 4 5\n2853             b        (x) int64 0 3\n2854         >>> stacked = data.to_stacked_array(\"z\", [\"x\"])\n2855         >>> stacked.indexes[\"z\"]\n2856         MultiIndex([('a', 0.0),\n2857                     ('a', 1.0),\n2858                     ('a', 2.0),\n2859                     ('b', nan)],\n2860                    name='z')\n2861         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n2862         >>> data.identical(roundtripped)\n2863         True\n2864 \n2865         See Also\n2866         --------\n2867         Dataset.to_stacked_array\n2868         \"\"\"\n2869         idx = self._indexes[dim].to_pandas_index()\n2870         if not isinstance(idx, pd.MultiIndex):\n2871             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n2872 \n2873         level_number = idx._get_level_number(level)\n2874         variables = idx.levels[level_number]\n2875         variable_dim = idx.names[level_number]\n2876 \n2877         # pull variables out of datarray\n2878         data_dict = {}\n2879         for k in variables:\n2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)\n2881 \n2882         # unstacked dataset\n2883         return Dataset(data_dict)\n2884 \n2885     def transpose(\n2886         self: T_DataArray,\n2887         *dims: Hashable,\n2888         transpose_coords: bool = True,\n2889         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2890     ) -> T_DataArray:\n2891         \"\"\"Return a new DataArray object with transposed dimensions.\n2892 \n2893         Parameters\n2894         ----------\n2895         *dims : Hashable, optional\n2896             By default, reverse the dimensions. Otherwise, reorder the\n2897             dimensions to this order.\n2898         transpose_coords : bool, default: True\n2899             If True, also transpose the coordinates of this DataArray.\n2900         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2901             What to do if dimensions that should be selected from are not present in the\n2902             DataArray:\n2903             - \"raise\": raise an exception\n2904             - \"warn\": raise a warning, and ignore the missing dimensions\n2905             - \"ignore\": ignore the missing dimensions\n2906 \n2907         Returns\n2908         -------\n2909         transposed : DataArray\n2910             The returned DataArray's array is transposed.\n2911 \n2912         Notes\n2913         -----\n2914         This operation returns a view of this array's data. It is\n2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n2916         -- the data will be fully loaded.\n2917 \n2918         See Also\n2919         --------\n2920         numpy.transpose\n2921         Dataset.transpose\n2922         \"\"\"\n2923         if dims:\n2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))\n2925         variable = self.variable.transpose(*dims)\n2926         if transpose_coords:\n2927             coords: dict[Hashable, Variable] = {}\n2928             for name, coord in self.coords.items():\n2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n2930                 coords[name] = coord.variable.transpose(*coord_dims)\n2931             return self._replace(variable, coords)\n2932         else:\n2933             return self._replace(variable)\n2934 \n2935     @property\n2936     def T(self: T_DataArray) -> T_DataArray:\n2937         return self.transpose()\n2938 \n2939     # change type of self and return to T_DataArray once\n2940     # https://github.com/python/mypy/issues/12846 is resolved\n2941     def drop_vars(\n2942         self,\n2943         names: Hashable | Iterable[Hashable],\n2944         *,\n2945         errors: ErrorOptions = \"raise\",\n2946     ) -> DataArray:\n2947         \"\"\"Returns an array with dropped variables.\n2948 \n2949         Parameters\n2950         ----------\n2951         names : Hashable or iterable of Hashable\n2952             Name(s) of variables to drop.\n2953         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n2954             If 'raise', raises a ValueError error if any of the variable\n2955             passed are not in the dataset. If 'ignore', any given names that are in the\n2956             DataArray are dropped and no error is raised.\n2957 \n2958         Returns\n2959         -------\n2960         dropped : Dataset\n2961             New Dataset copied from `self` with variables removed.\n2962 \n2963         Examples\n2964         -------\n2965         >>> data = np.arange(12).reshape(4, 3)\n2966         >>> da = xr.DataArray(\n2967         ...     data=data,\n2968         ...     dims=[\"x\", \"y\"],\n2969         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n2970         ... )\n2971         >>> da\n2972         <xarray.DataArray (x: 4, y: 3)>\n2973         array([[ 0,  1,  2],\n2974                [ 3,  4,  5],\n2975                [ 6,  7,  8],\n2976                [ 9, 10, 11]])\n2977         Coordinates:\n2978           * x        (x) int64 10 20 30 40\n2979           * y        (y) int64 70 80 90\n2980 \n2981         Removing a single variable:\n2982 \n2983         >>> da.drop_vars(\"x\")\n2984         <xarray.DataArray (x: 4, y: 3)>\n2985         array([[ 0,  1,  2],\n2986                [ 3,  4,  5],\n2987                [ 6,  7,  8],\n2988                [ 9, 10, 11]])\n2989         Coordinates:\n2990           * y        (y) int64 70 80 90\n2991         Dimensions without coordinates: x\n2992 \n2993         Removing a list of variables:\n2994 \n2995         >>> da.drop_vars([\"x\", \"y\"])\n2996         <xarray.DataArray (x: 4, y: 3)>\n2997         array([[ 0,  1,  2],\n2998                [ 3,  4,  5],\n2999                [ 6,  7,  8],\n3000                [ 9, 10, 11]])\n3001         Dimensions without coordinates: x, y\n3002         \"\"\"\n3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n3004         return self._from_temp_dataset(ds)\n3005 \n3006     def drop_indexes(\n3007         self: T_DataArray,\n3008         coord_names: Hashable | Iterable[Hashable],\n3009         *,\n3010         errors: ErrorOptions = \"raise\",\n3011     ) -> T_DataArray:\n3012         \"\"\"Drop the indexes assigned to the given coordinates.\n3013 \n3014         Parameters\n3015         ----------\n3016         coord_names : hashable or iterable of hashable\n3017             Name(s) of the coordinate(s) for which to drop the index.\n3018         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n3019             If 'raise', raises a ValueError error if any of the coordinates\n3020             passed have no index or are not in the dataset.\n3021             If 'ignore', no error is raised.\n3022 \n3023         Returns\n3024         -------\n3025         dropped : DataArray\n3026             A new dataarray with dropped indexes.\n3027         \"\"\"\n3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)\n3029         return self._from_temp_dataset(ds)\n3030 \n3031     def drop(\n3032         self: T_DataArray,\n3033         labels: Mapping[Any, Any] | None = None,\n3034         dim: Hashable | None = None,\n3035         *,\n3036         errors: ErrorOptions = \"raise\",\n3037         **labels_kwargs,\n3038     ) -> T_DataArray:\n3039         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n3040 \n3041         Using either `drop_vars` or `drop_sel` is encouraged\n3042 \n3043         See Also\n3044         --------\n3045         DataArray.drop_vars\n3046         DataArray.drop_sel\n3047         \"\"\"\n3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)\n3049         return self._from_temp_dataset(ds)\n3050 \n3051     def drop_sel(\n3052         self: T_DataArray,\n3053         labels: Mapping[Any, Any] | None = None,\n3054         *,\n3055         errors: ErrorOptions = \"raise\",\n3056         **labels_kwargs,\n3057     ) -> T_DataArray:\n3058         \"\"\"Drop index labels from this DataArray.\n3059 \n3060         Parameters\n3061         ----------\n3062         labels : mapping of Hashable to Any\n3063             Index labels to drop\n3064         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n3065             If 'raise', raises a ValueError error if\n3066             any of the index labels passed are not\n3067             in the dataset. If 'ignore', any given labels that are in the\n3068             dataset are dropped and no error is raised.\n3069         **labels_kwargs : {dim: label, ...}, optional\n3070             The keyword arguments form of ``dim`` and ``labels``\n3071 \n3072         Returns\n3073         -------\n3074         dropped : DataArray\n3075 \n3076         Examples\n3077         --------\n3078         >>> da = xr.DataArray(\n3079         ...     np.arange(25).reshape(5, 5),\n3080         ...     coords={\"x\": np.arange(0, 9, 2), \"y\": np.arange(0, 13, 3)},\n3081         ...     dims=(\"x\", \"y\"),\n3082         ... )\n3083         >>> da\n3084         <xarray.DataArray (x: 5, y: 5)>\n3085         array([[ 0,  1,  2,  3,  4],\n3086                [ 5,  6,  7,  8,  9],\n3087                [10, 11, 12, 13, 14],\n3088                [15, 16, 17, 18, 19],\n3089                [20, 21, 22, 23, 24]])\n3090         Coordinates:\n3091           * x        (x) int64 0 2 4 6 8\n3092           * y        (y) int64 0 3 6 9 12\n3093 \n3094         >>> da.drop_sel(x=[0, 2], y=9)\n3095         <xarray.DataArray (x: 3, y: 4)>\n3096         array([[10, 11, 12, 14],\n3097                [15, 16, 17, 19],\n3098                [20, 21, 22, 24]])\n3099         Coordinates:\n3100           * x        (x) int64 4 6 8\n3101           * y        (y) int64 0 3 6 12\n3102 \n3103         >>> da.drop_sel({\"x\": 6, \"y\": [0, 3]})\n3104         <xarray.DataArray (x: 4, y: 3)>\n3105         array([[ 2,  3,  4],\n3106                [ 7,  8,  9],\n3107                [12, 13, 14],\n3108                [22, 23, 24]])\n3109         Coordinates:\n3110           * x        (x) int64 0 2 4 8\n3111           * y        (y) int64 6 9 12\n3112         \"\"\"\n3113         if labels_kwargs or isinstance(labels, dict):\n3114             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n3115 \n3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n3117         return self._from_temp_dataset(ds)\n3118 \n3119     def drop_isel(\n3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs\n3121     ) -> T_DataArray:\n3122         \"\"\"Drop index positions from this DataArray.\n3123 \n3124         Parameters\n3125         ----------\n3126         indexers : mapping of Hashable to Any or None, default: None\n3127             Index locations to drop\n3128         **indexers_kwargs : {dim: position, ...}, optional\n3129             The keyword arguments form of ``dim`` and ``positions``\n3130 \n3131         Returns\n3132         -------\n3133         dropped : DataArray\n3134 \n3135         Raises\n3136         ------\n3137         IndexError\n3138 \n3139         Examples\n3140         --------\n3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=(\"X\", \"Y\"))\n3142         >>> da\n3143         <xarray.DataArray (X: 5, Y: 5)>\n3144         array([[ 0,  1,  2,  3,  4],\n3145                [ 5,  6,  7,  8,  9],\n3146                [10, 11, 12, 13, 14],\n3147                [15, 16, 17, 18, 19],\n3148                [20, 21, 22, 23, 24]])\n3149         Dimensions without coordinates: X, Y\n3150 \n3151         >>> da.drop_isel(X=[0, 4], Y=2)\n3152         <xarray.DataArray (X: 3, Y: 4)>\n3153         array([[ 5,  6,  8,  9],\n3154                [10, 11, 13, 14],\n3155                [15, 16, 18, 19]])\n3156         Dimensions without coordinates: X, Y\n3157 \n3158         >>> da.drop_isel({\"X\": 3, \"Y\": 3})\n3159         <xarray.DataArray (X: 4, Y: 4)>\n3160         array([[ 0,  1,  2,  4],\n3161                [ 5,  6,  7,  9],\n3162                [10, 11, 12, 14],\n3163                [20, 21, 22, 24]])\n3164         Dimensions without coordinates: X, Y\n3165         \"\"\"\n3166         dataset = self._to_temp_dataset()\n3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)\n3168         return self._from_temp_dataset(dataset)\n3169 \n3170     def dropna(\n3171         self: T_DataArray,\n3172         dim: Hashable,\n3173         how: Literal[\"any\", \"all\"] = \"any\",\n3174         thresh: int | None = None,\n3175     ) -> T_DataArray:\n3176         \"\"\"Returns a new array with dropped labels for missing values along\n3177         the provided dimension.\n3178 \n3179         Parameters\n3180         ----------\n3181         dim : Hashable\n3182             Dimension along which to drop missing values. Dropping along\n3183             multiple dimensions simultaneously is not yet supported.\n3184         how : {\"any\", \"all\"}, default: \"any\"\n3185             - any : if any NA values are present, drop that label\n3186             - all : if all values are NA, drop that label\n3187 \n3188         thresh : int or None, default: None\n3189             If supplied, require this many non-NA values.\n3190 \n3191         Returns\n3192         -------\n3193         dropped : DataArray\n3194 \n3195         Examples\n3196         --------\n3197         >>> temperature = [\n3198         ...     [0, 4, 2, 9],\n3199         ...     [np.nan, np.nan, np.nan, np.nan],\n3200         ...     [np.nan, 4, 2, 0],\n3201         ...     [3, 1, 0, 0],\n3202         ... ]\n3203         >>> da = xr.DataArray(\n3204         ...     data=temperature,\n3205         ...     dims=[\"Y\", \"X\"],\n3206         ...     coords=dict(\n3207         ...         lat=(\"Y\", np.array([-20.0, -20.25, -20.50, -20.75])),\n3208         ...         lon=(\"X\", np.array([10.0, 10.25, 10.5, 10.75])),\n3209         ...     ),\n3210         ... )\n3211         >>> da\n3212         <xarray.DataArray (Y: 4, X: 4)>\n3213         array([[ 0.,  4.,  2.,  9.],\n3214                [nan, nan, nan, nan],\n3215                [nan,  4.,  2.,  0.],\n3216                [ 3.,  1.,  0.,  0.]])\n3217         Coordinates:\n3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75\n3219             lon      (X) float64 10.0 10.25 10.5 10.75\n3220         Dimensions without coordinates: Y, X\n3221 \n3222         >>> da.dropna(dim=\"Y\", how=\"any\")\n3223         <xarray.DataArray (Y: 2, X: 4)>\n3224         array([[0., 4., 2., 9.],\n3225                [3., 1., 0., 0.]])\n3226         Coordinates:\n3227             lat      (Y) float64 -20.0 -20.75\n3228             lon      (X) float64 10.0 10.25 10.5 10.75\n3229         Dimensions without coordinates: Y, X\n3230 \n3231         Drop values only if all values along the dimension are NaN:\n3232 \n3233         >>> da.dropna(dim=\"Y\", how=\"all\")\n3234         <xarray.DataArray (Y: 3, X: 4)>\n3235         array([[ 0.,  4.,  2.,  9.],\n3236                [nan,  4.,  2.,  0.],\n3237                [ 3.,  1.,  0.,  0.]])\n3238         Coordinates:\n3239             lat      (Y) float64 -20.0 -20.5 -20.75\n3240             lon      (X) float64 10.0 10.25 10.5 10.75\n3241         Dimensions without coordinates: Y, X\n3242         \"\"\"\n3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n3244         return self._from_temp_dataset(ds)\n3245 \n3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:\n3247         \"\"\"Fill missing values in this object.\n3248 \n3249         This operation follows the normal broadcasting and alignment rules that\n3250         xarray uses for binary arithmetic, except the result is aligned to this\n3251         object (``join='left'``) instead of aligned to the intersection of\n3252         index coordinates (``join='inner'``).\n3253 \n3254         Parameters\n3255         ----------\n3256         value : scalar, ndarray or DataArray\n3257             Used to fill all matching missing values in this array. If the\n3258             argument is a DataArray, it is first aligned with (reindexed to)\n3259             this array.\n3260 \n3261         Returns\n3262         -------\n3263         filled : DataArray\n3264 \n3265         Examples\n3266         --------\n3267         >>> da = xr.DataArray(\n3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),\n3269         ...     dims=\"Z\",\n3270         ...     coords=dict(\n3271         ...         Z=(\"Z\", np.arange(6)),\n3272         ...         height=(\"Z\", np.array([0, 10, 20, 30, 40, 50])),\n3273         ...     ),\n3274         ... )\n3275         >>> da\n3276         <xarray.DataArray (Z: 6)>\n3277         array([ 1.,  4., nan,  0.,  3., nan])\n3278         Coordinates:\n3279           * Z        (Z) int64 0 1 2 3 4 5\n3280             height   (Z) int64 0 10 20 30 40 50\n3281 \n3282         Fill all NaN values with 0:\n3283 \n3284         >>> da.fillna(0)\n3285         <xarray.DataArray (Z: 6)>\n3286         array([1., 4., 0., 0., 3., 0.])\n3287         Coordinates:\n3288           * Z        (Z) int64 0 1 2 3 4 5\n3289             height   (Z) int64 0 10 20 30 40 50\n3290 \n3291         Fill NaN values with corresponding values in array:\n3292 \n3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))\n3294         <xarray.DataArray (Z: 6)>\n3295         array([1., 4., 4., 0., 3., 9.])\n3296         Coordinates:\n3297           * Z        (Z) int64 0 1 2 3 4 5\n3298             height   (Z) int64 0 10 20 30 40 50\n3299         \"\"\"\n3300         if utils.is_dict_like(value):\n3301             raise TypeError(\n3302                 \"cannot provide fill value as a dictionary with \"\n3303                 \"fillna on a DataArray\"\n3304             )\n3305         out = ops.fillna(self, value)\n3306         return out\n3307 \n3308     def interpolate_na(\n3309         self: T_DataArray,\n3310         dim: Hashable | None = None,\n3311         method: InterpOptions = \"linear\",\n3312         limit: int | None = None,\n3313         use_coordinate: bool | str = True,\n3314         max_gap: (\n3315             None\n3316             | int\n3317             | float\n3318             | str\n3319             | pd.Timedelta\n3320             | np.timedelta64\n3321             | datetime.timedelta\n3322         ) = None,\n3323         keep_attrs: bool | None = None,\n3324         **kwargs: Any,\n3325     ) -> T_DataArray:\n3326         \"\"\"Fill in NaNs by interpolating according to different methods.\n3327 \n3328         Parameters\n3329         ----------\n3330         dim : Hashable or None, optional\n3331             Specifies the dimension along which to interpolate.\n3332         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3333             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3334             String indicating which method to use for interpolation:\n3335 \n3336             - 'linear': linear interpolation. Additional keyword\n3337               arguments are passed to :py:func:`numpy.interp`\n3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3339               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3340               ``method='polynomial'``, the ``order`` keyword argument must also be\n3341               provided.\n3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3343               respective :py:class:`scipy.interpolate` classes.\n3344 \n3345         use_coordinate : bool or str, default: True\n3346             Specifies which index to use as the x values in the interpolation\n3347             formulated as `y = f(x)`. If False, values are treated as if\n3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is\n3349             used. If ``use_coordinate`` is a string, it specifies the name of a\n3350             coordinate variable to use as the index.\n3351         limit : int or None, default: None\n3352             Maximum number of consecutive NaNs to fill. Must be greater than 0\n3353             or None for no limit. This filling is done regardless of the size of\n3354             the gap in the data. To only interpolate over gaps less than a given length,\n3355             see ``max_gap``.\n3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n3358             Use None for no limit. When interpolating along a datetime64 dimension\n3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n3360 \n3361             - a string that is valid input for pandas.to_timedelta\n3362             - a :py:class:`numpy.timedelta64` object\n3363             - a :py:class:`pandas.Timedelta` object\n3364             - a :py:class:`datetime.timedelta` object\n3365 \n3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n3367             dimensions has not been implemented yet. Gap length is defined as the difference\n3368             between coordinate values at the first data point after a gap and the last value\n3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n3370             between coordinate values at the first (last) valid data point and the first (last) NaN.\n3371             For example, consider::\n3372 \n3373                 <xarray.DataArray (x: 9)>\n3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n3375                 Coordinates:\n3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n3377 \n3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n3379         keep_attrs : bool or None, default: None\n3380             If True, the dataarray's attributes (`attrs`) will be copied from\n3381             the original object to the new one.  If False, the new\n3382             object will be returned without attributes.\n3383         **kwargs : dict, optional\n3384             parameters passed verbatim to the underlying interpolation function\n3385 \n3386         Returns\n3387         -------\n3388         interpolated: DataArray\n3389             Filled in DataArray.\n3390 \n3391         See Also\n3392         --------\n3393         numpy.interp\n3394         scipy.interpolate\n3395 \n3396         Examples\n3397         --------\n3398         >>> da = xr.DataArray(\n3399         ...     [np.nan, 2, 3, np.nan, 0], dims=\"x\", coords={\"x\": [0, 1, 2, 3, 4]}\n3400         ... )\n3401         >>> da\n3402         <xarray.DataArray (x: 5)>\n3403         array([nan,  2.,  3., nan,  0.])\n3404         Coordinates:\n3405           * x        (x) int64 0 1 2 3 4\n3406 \n3407         >>> da.interpolate_na(dim=\"x\", method=\"linear\")\n3408         <xarray.DataArray (x: 5)>\n3409         array([nan, 2. , 3. , 1.5, 0. ])\n3410         Coordinates:\n3411           * x        (x) int64 0 1 2 3 4\n3412 \n3413         >>> da.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n3414         <xarray.DataArray (x: 5)>\n3415         array([1. , 2. , 3. , 1.5, 0. ])\n3416         Coordinates:\n3417           * x        (x) int64 0 1 2 3 4\n3418         \"\"\"\n3419         from xarray.core.missing import interp_na\n3420 \n3421         return interp_na(\n3422             self,\n3423             dim=dim,\n3424             method=method,\n3425             limit=limit,\n3426             use_coordinate=use_coordinate,\n3427             max_gap=max_gap,\n3428             keep_attrs=keep_attrs,\n3429             **kwargs,\n3430         )\n3431 \n3432     def ffill(\n3433         self: T_DataArray, dim: Hashable, limit: int | None = None\n3434     ) -> T_DataArray:\n3435         \"\"\"Fill NaN values by propagating values forward\n3436 \n3437         *Requires bottleneck.*\n3438 \n3439         Parameters\n3440         ----------\n3441         dim : Hashable\n3442             Specifies the dimension along which to propagate values when\n3443             filling.\n3444         limit : int or None, default: None\n3445             The maximum number of consecutive NaN values to forward fill. In\n3446             other words, if there is a gap with more than this number of\n3447             consecutive NaNs, it will only be partially filled. Must be greater\n3448             than 0 or None for no limit. Must be None or greater than or equal\n3449             to axis length if filling along chunked axes (dimensions).\n3450 \n3451         Returns\n3452         -------\n3453         filled : DataArray\n3454 \n3455         Examples\n3456         --------\n3457         >>> temperature = np.array(\n3458         ...     [\n3459         ...         [np.nan, 1, 3],\n3460         ...         [0, np.nan, 5],\n3461         ...         [5, np.nan, np.nan],\n3462         ...         [3, np.nan, np.nan],\n3463         ...         [0, 2, 0],\n3464         ...     ]\n3465         ... )\n3466         >>> da = xr.DataArray(\n3467         ...     data=temperature,\n3468         ...     dims=[\"Y\", \"X\"],\n3469         ...     coords=dict(\n3470         ...         lat=(\"Y\", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),\n3471         ...         lon=(\"X\", np.array([10.0, 10.25, 10.5])),\n3472         ...     ),\n3473         ... )\n3474         >>> da\n3475         <xarray.DataArray (Y: 5, X: 3)>\n3476         array([[nan,  1.,  3.],\n3477                [ 0., nan,  5.],\n3478                [ 5., nan, nan],\n3479                [ 3., nan, nan],\n3480                [ 0.,  2.,  0.]])\n3481         Coordinates:\n3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3483             lon      (X) float64 10.0 10.25 10.5\n3484         Dimensions without coordinates: Y, X\n3485 \n3486         Fill all NaN values:\n3487 \n3488         >>> da.ffill(dim=\"Y\", limit=None)\n3489         <xarray.DataArray (Y: 5, X: 3)>\n3490         array([[nan,  1.,  3.],\n3491                [ 0.,  1.,  5.],\n3492                [ 5.,  1.,  5.],\n3493                [ 3.,  1.,  5.],\n3494                [ 0.,  2.,  0.]])\n3495         Coordinates:\n3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3497             lon      (X) float64 10.0 10.25 10.5\n3498         Dimensions without coordinates: Y, X\n3499 \n3500         Fill only the first of consecutive NaN values:\n3501 \n3502         >>> da.ffill(dim=\"Y\", limit=1)\n3503         <xarray.DataArray (Y: 5, X: 3)>\n3504         array([[nan,  1.,  3.],\n3505                [ 0.,  1.,  5.],\n3506                [ 5., nan,  5.],\n3507                [ 3., nan, nan],\n3508                [ 0.,  2.,  0.]])\n3509         Coordinates:\n3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3511             lon      (X) float64 10.0 10.25 10.5\n3512         Dimensions without coordinates: Y, X\n3513         \"\"\"\n3514         from xarray.core.missing import ffill\n3515 \n3516         return ffill(self, dim, limit=limit)\n3517 \n3518     def bfill(\n3519         self: T_DataArray, dim: Hashable, limit: int | None = None\n3520     ) -> T_DataArray:\n3521         \"\"\"Fill NaN values by propagating values backward\n3522 \n3523         *Requires bottleneck.*\n3524 \n3525         Parameters\n3526         ----------\n3527         dim : str\n3528             Specifies the dimension along which to propagate values when\n3529             filling.\n3530         limit : int or None, default: None\n3531             The maximum number of consecutive NaN values to backward fill. In\n3532             other words, if there is a gap with more than this number of\n3533             consecutive NaNs, it will only be partially filled. Must be greater\n3534             than 0 or None for no limit. Must be None or greater than or equal\n3535             to axis length if filling along chunked axes (dimensions).\n3536 \n3537         Returns\n3538         -------\n3539         filled : DataArray\n3540 \n3541         Examples\n3542         --------\n3543         >>> temperature = np.array(\n3544         ...     [\n3545         ...         [0, 1, 3],\n3546         ...         [0, np.nan, 5],\n3547         ...         [5, np.nan, np.nan],\n3548         ...         [3, np.nan, np.nan],\n3549         ...         [np.nan, 2, 0],\n3550         ...     ]\n3551         ... )\n3552         >>> da = xr.DataArray(\n3553         ...     data=temperature,\n3554         ...     dims=[\"Y\", \"X\"],\n3555         ...     coords=dict(\n3556         ...         lat=(\"Y\", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),\n3557         ...         lon=(\"X\", np.array([10.0, 10.25, 10.5])),\n3558         ...     ),\n3559         ... )\n3560         >>> da\n3561         <xarray.DataArray (Y: 5, X: 3)>\n3562         array([[ 0.,  1.,  3.],\n3563                [ 0., nan,  5.],\n3564                [ 5., nan, nan],\n3565                [ 3., nan, nan],\n3566                [nan,  2.,  0.]])\n3567         Coordinates:\n3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3569             lon      (X) float64 10.0 10.25 10.5\n3570         Dimensions without coordinates: Y, X\n3571 \n3572         Fill all NaN values:\n3573 \n3574         >>> da.bfill(dim=\"Y\", limit=None)\n3575         <xarray.DataArray (Y: 5, X: 3)>\n3576         array([[ 0.,  1.,  3.],\n3577                [ 0.,  2.,  5.],\n3578                [ 5.,  2.,  0.],\n3579                [ 3.,  2.,  0.],\n3580                [nan,  2.,  0.]])\n3581         Coordinates:\n3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3583             lon      (X) float64 10.0 10.25 10.5\n3584         Dimensions without coordinates: Y, X\n3585 \n3586         Fill only the first of consecutive NaN values:\n3587 \n3588         >>> da.bfill(dim=\"Y\", limit=1)\n3589         <xarray.DataArray (Y: 5, X: 3)>\n3590         array([[ 0.,  1.,  3.],\n3591                [ 0., nan,  5.],\n3592                [ 5., nan, nan],\n3593                [ 3.,  2.,  0.],\n3594                [nan,  2.,  0.]])\n3595         Coordinates:\n3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3597             lon      (X) float64 10.0 10.25 10.5\n3598         Dimensions without coordinates: Y, X\n3599         \"\"\"\n3600         from xarray.core.missing import bfill\n3601 \n3602         return bfill(self, dim, limit=limit)\n3603 \n3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n3605         \"\"\"Combine two DataArray objects, with union of coordinates.\n3606 \n3607         This operation follows the normal broadcasting and alignment rules of\n3608         ``join='outer'``.  Default to non-null values of array calling the\n3609         method.  Use np.nan to fill in vacant cells after alignment.\n3610 \n3611         Parameters\n3612         ----------\n3613         other : DataArray\n3614             Used to fill all matching missing values in this array.\n3615 \n3616         Returns\n3617         -------\n3618         DataArray\n3619         \"\"\"\n3620         return ops.fillna(self, other, join=\"outer\")\n3621 \n3622     def reduce(\n3623         self: T_DataArray,\n3624         func: Callable[..., Any],\n3625         dim: Dims = None,\n3626         *,\n3627         axis: int | Sequence[int] | None = None,\n3628         keep_attrs: bool | None = None,\n3629         keepdims: bool = False,\n3630         **kwargs: Any,\n3631     ) -> T_DataArray:\n3632         \"\"\"Reduce this array by applying `func` along some dimension(s).\n3633 \n3634         Parameters\n3635         ----------\n3636         func : callable\n3637             Function which can be called in the form\n3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n3639             np.ndarray over an integer valued axis.\n3640         dim : \"...\", str, Iterable of Hashable or None, optional\n3641             Dimension(s) over which to apply `func`. By default `func` is\n3642             applied over all dimensions.\n3643         axis : int or sequence of int, optional\n3644             Axis(es) over which to repeatedly apply `func`. Only one of the\n3645             'dim' and 'axis' arguments can be supplied. If neither are\n3646             supplied, then the reduction is calculated over the flattened array\n3647             (by calling `f(x)` without an axis argument).\n3648         keep_attrs : bool or None, optional\n3649             If True, the variable's attributes (`attrs`) will be copied from\n3650             the original object to the new one.  If False (default), the new\n3651             object will be returned without attributes.\n3652         keepdims : bool, default: False\n3653             If True, the dimensions which are reduced are left in the result\n3654             as dimensions of size one. Coordinates that use these dimensions\n3655             are removed.\n3656         **kwargs : dict\n3657             Additional keyword arguments passed on to `func`.\n3658 \n3659         Returns\n3660         -------\n3661         reduced : DataArray\n3662             DataArray with this object's array replaced with an array with\n3663             summarized data and the indicated dimension(s) removed.\n3664         \"\"\"\n3665 \n3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n3667         return self._replace_maybe_drop_dims(var)\n3668 \n3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:\n3670         \"\"\"Convert this array into a pandas object with the same shape.\n3671 \n3672         The type of the returned object depends on the number of DataArray\n3673         dimensions:\n3674 \n3675         * 0D -> `xarray.DataArray`\n3676         * 1D -> `pandas.Series`\n3677         * 2D -> `pandas.DataFrame`\n3678 \n3679         Only works for arrays with 2 or fewer dimensions.\n3680 \n3681         The DataArray constructor performs the inverse transformation.\n3682 \n3683         Returns\n3684         -------\n3685         result : DataArray | Series | DataFrame\n3686             DataArray, pandas Series or pandas DataFrame.\n3687         \"\"\"\n3688         # TODO: consolidate the info about pandas constructors and the\n3689         # attributes that correspond to their indexes into a separate module?\n3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n3691         try:\n3692             constructor = constructors[self.ndim]\n3693         except KeyError:\n3694             raise ValueError(\n3695                 f\"Cannot convert arrays with {self.ndim} dimensions into \"\n3696                 \"pandas objects. Requires 2 or fewer dimensions.\"\n3697             )\n3698         indexes = [self.get_index(dim) for dim in self.dims]\n3699         return constructor(self.values, *indexes)\n3700 \n3701     def to_dataframe(\n3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None\n3703     ) -> pd.DataFrame:\n3704         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n3705 \n3706         The DataFrame is indexed by the Cartesian product of index coordinates\n3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are\n3708         included as columns in the DataFrame.\n3709 \n3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which\n3711         doesn't rely on a MultiIndex to build the DataFrame.\n3712 \n3713         Parameters\n3714         ----------\n3715         name: Hashable or None, optional\n3716             Name to give to this array (required if unnamed).\n3717         dim_order: Sequence of Hashable or None, optional\n3718             Hierarchical dimension order for the resulting dataframe.\n3719             Array content is transposed to this order and then written out as flat\n3720             vectors in contiguous order, so the last dimension in this list\n3721             will be contiguous in the resulting DataFrame. This has a major\n3722             influence on which operations are efficient on the resulting\n3723             dataframe.\n3724 \n3725             If provided, must include all dimensions of this DataArray. By default,\n3726             dimensions are sorted according to the DataArray dimensions order.\n3727 \n3728         Returns\n3729         -------\n3730         result: DataFrame\n3731             DataArray as a pandas DataFrame.\n3732 \n3733         See also\n3734         --------\n3735         DataArray.to_pandas\n3736         DataArray.to_series\n3737         \"\"\"\n3738         if name is None:\n3739             name = self.name\n3740         if name is None:\n3741             raise ValueError(\n3742                 \"cannot convert an unnamed DataArray to a \"\n3743                 \"DataFrame: use the ``name`` parameter\"\n3744             )\n3745         if self.ndim == 0:\n3746             raise ValueError(\"cannot convert a scalar to a DataFrame\")\n3747 \n3748         # By using a unique name, we can convert a DataArray into a DataFrame\n3749         # even if it shares a name with one of its coordinates.\n3750         # I would normally use unique_name = object() but that results in a\n3751         # dataframe with columns in the wrong order, for reasons I have not\n3752         # been able to debug (possibly a pandas bug?).\n3753         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n3754         ds = self._to_dataset_whole(name=unique_name)\n3755 \n3756         if dim_order is None:\n3757             ordered_dims = dict(zip(self.dims, self.shape))\n3758         else:\n3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)\n3760 \n3761         df = ds._to_dataframe(ordered_dims)\n3762         df.columns = [name if c == unique_name else c for c in df.columns]\n3763         return df\n3764 \n3765     def to_series(self) -> pd.Series:\n3766         \"\"\"Convert this array into a pandas.Series.\n3767 \n3768         The Series is indexed by the Cartesian product of index coordinates\n3769         (in the form of a :py:class:`pandas.MultiIndex`).\n3770 \n3771         Returns\n3772         -------\n3773         result : Series\n3774             DataArray as a pandas Series.\n3775 \n3776         See also\n3777         --------\n3778         DataArray.to_pandas\n3779         DataArray.to_dataframe\n3780         \"\"\"\n3781         index = self.coords.to_index()\n3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n3783 \n3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n3785         \"\"\"Convert this array into a numpy.ma.MaskedArray\n3786 \n3787         Parameters\n3788         ----------\n3789         copy : bool, default: True\n3790             If True make a copy of the array in the result. If False,\n3791             a MaskedArray view of DataArray.values is returned.\n3792 \n3793         Returns\n3794         -------\n3795         result : MaskedArray\n3796             Masked where invalid values (nan or inf) occur.\n3797         \"\"\"\n3798         values = self.to_numpy()  # only compute lazy arrays once\n3799         isnull = pd.isnull(values)\n3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n3801 \n3802     # path=None writes to bytes\n3803     @overload\n3804     def to_netcdf(\n3805         self,\n3806         path: None = None,\n3807         mode: Literal[\"w\", \"a\"] = \"w\",\n3808         format: T_NetcdfTypes | None = None,\n3809         group: str | None = None,\n3810         engine: T_NetcdfEngine | None = None,\n3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3812         unlimited_dims: Iterable[Hashable] | None = None,\n3813         compute: bool = True,\n3814         invalid_netcdf: bool = False,\n3815     ) -> bytes:\n3816         ...\n3817 \n3818     # default return None\n3819     @overload\n3820     def to_netcdf(\n3821         self,\n3822         path: str | PathLike,\n3823         mode: Literal[\"w\", \"a\"] = \"w\",\n3824         format: T_NetcdfTypes | None = None,\n3825         group: str | None = None,\n3826         engine: T_NetcdfEngine | None = None,\n3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3828         unlimited_dims: Iterable[Hashable] | None = None,\n3829         compute: Literal[True] = True,\n3830         invalid_netcdf: bool = False,\n3831     ) -> None:\n3832         ...\n3833 \n3834     # compute=False returns dask.Delayed\n3835     @overload\n3836     def to_netcdf(\n3837         self,\n3838         path: str | PathLike,\n3839         mode: Literal[\"w\", \"a\"] = \"w\",\n3840         format: T_NetcdfTypes | None = None,\n3841         group: str | None = None,\n3842         engine: T_NetcdfEngine | None = None,\n3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3844         unlimited_dims: Iterable[Hashable] | None = None,\n3845         *,\n3846         compute: Literal[False],\n3847         invalid_netcdf: bool = False,\n3848     ) -> Delayed:\n3849         ...\n3850 \n3851     def to_netcdf(\n3852         self,\n3853         path: str | PathLike | None = None,\n3854         mode: Literal[\"w\", \"a\"] = \"w\",\n3855         format: T_NetcdfTypes | None = None,\n3856         group: str | None = None,\n3857         engine: T_NetcdfEngine | None = None,\n3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3859         unlimited_dims: Iterable[Hashable] | None = None,\n3860         compute: bool = True,\n3861         invalid_netcdf: bool = False,\n3862     ) -> bytes | Delayed | None:\n3863         \"\"\"Write DataArray contents to a netCDF file.\n3864 \n3865         Parameters\n3866         ----------\n3867         path : str, path-like or None, optional\n3868             Path to which to save this dataset. File-like objects are only\n3869             supported by the scipy engine. If no path is provided, this\n3870             function returns the resulting netCDF file as bytes; in this case,\n3871             we need to use scipy, which does not support netCDF version 4 (the\n3872             default format becomes NETCDF3_64BIT).\n3873         mode : {\"w\", \"a\"}, default: \"w\"\n3874             Write ('w') or append ('a') mode. If mode='w', any existing file at\n3875             this location will be overwritten. If mode='a', existing variables\n3876             will be overwritten.\n3877         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n3878                   \"NETCDF3_CLASSIC\"}, optional\n3879             File format for the resulting netCDF file:\n3880 \n3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n3882               features.\n3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n3884               netCDF 3 compatible API features.\n3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n3886               which fully supports 2+ GB files, but is only compatible with\n3887               clients linked against netCDF version 3.6.0 or later.\n3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n3889               handle 2+ GB files very well.\n3890 \n3891             All formats are supported by the netCDF4-python library.\n3892             scipy.io.netcdf only supports the last two formats.\n3893 \n3894             The default format is NETCDF4 if you are saving a file to disk and\n3895             have the netCDF4-python library available. Otherwise, xarray falls\n3896             back to using scipy to write netCDF files and defaults to the\n3897             NETCDF3_64BIT format (scipy does not support netCDF4).\n3898         group : str, optional\n3899             Path to the netCDF4 group in the given file to open (only works for\n3900             format='NETCDF4'). The group(s) will be created if necessary.\n3901         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n3902             Engine to use when writing netCDF files. If not provided, the\n3903             default engine is chosen based on available dependencies, with a\n3904             preference for 'netcdf4' if writing to a file on disk.\n3905         encoding : dict, optional\n3906             Nested dictionary with variable names as keys and dictionaries of\n3907             variable specific encodings as values, e.g.,\n3908             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n3909             \"zlib\": True}, ...}``\n3910 \n3911             The `h5netcdf` engine supports both the NetCDF4-style compression\n3912             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n3913             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n3914             This allows using any compression plugin installed in the HDF5\n3915             library, e.g. LZF.\n3916 \n3917         unlimited_dims : iterable of Hashable, optional\n3918             Dimension(s) that should be serialized as unlimited dimensions.\n3919             By default, no dimensions are treated as unlimited dimensions.\n3920             Note that unlimited_dims may also be set via\n3921             ``dataset.encoding[\"unlimited_dims\"]``.\n3922         compute: bool, default: True\n3923             If true compute immediately, otherwise return a\n3924             ``dask.delayed.Delayed`` object that can be computed later.\n3925         invalid_netcdf: bool, default: False\n3926             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n3927             hdf5 files which are invalid netcdf as described in\n3928             https://github.com/h5netcdf/h5netcdf.\n3929 \n3930         Returns\n3931         -------\n3932         store: bytes or Delayed or None\n3933             * ``bytes`` if path is None\n3934             * ``dask.delayed.Delayed`` if compute is False\n3935             * None otherwise\n3936 \n3937         Notes\n3938         -----\n3939         Only xarray.Dataset objects can be written to netCDF files, so\n3940         the xarray.DataArray is converted to a xarray.Dataset object\n3941         containing a single variable. If the DataArray has no name, or if the\n3942         name is the same as a coordinate name, then it is given the name\n3943         ``\"__xarray_dataarray_variable__\"``.\n3944 \n3945         See Also\n3946         --------\n3947         Dataset.to_netcdf\n3948         \"\"\"\n3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf\n3950 \n3951         if self.name is None:\n3952             # If no name is set then use a generic xarray name\n3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3954         elif self.name in self.coords or self.name in self.dims:\n3955             # The name is the same as one of the coords names, which netCDF\n3956             # doesn't support, so rename it but keep track of the old name\n3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3958             dataset.attrs[DATAARRAY_NAME] = self.name\n3959         else:\n3960             # No problems with the name - so we're fine!\n3961             dataset = self.to_dataset()\n3962 \n3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n3964             dataset,\n3965             path,\n3966             mode=mode,\n3967             format=format,\n3968             group=group,\n3969             engine=engine,\n3970             encoding=encoding,\n3971             unlimited_dims=unlimited_dims,\n3972             compute=compute,\n3973             multifile=False,\n3974             invalid_netcdf=invalid_netcdf,\n3975         )\n3976 \n3977     # compute=True (default) returns ZarrStore\n3978     @overload\n3979     def to_zarr(\n3980         self,\n3981         store: MutableMapping | str | PathLike[str] | None = None,\n3982         chunk_store: MutableMapping | str | PathLike | None = None,\n3983         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n3984         synchronizer=None,\n3985         group: str | None = None,\n3986         encoding: Mapping | None = None,\n3987         compute: Literal[True] = True,\n3988         consolidated: bool | None = None,\n3989         append_dim: Hashable | None = None,\n3990         region: Mapping[str, slice] | None = None,\n3991         safe_chunks: bool = True,\n3992         storage_options: dict[str, str] | None = None,\n3993         zarr_version: int | None = None,\n3994     ) -> ZarrStore:\n3995         ...\n3996 \n3997     # compute=False returns dask.Delayed\n3998     @overload\n3999     def to_zarr(\n4000         self,\n4001         store: MutableMapping | str | PathLike[str] | None = None,\n4002         chunk_store: MutableMapping | str | PathLike | None = None,\n4003         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n4004         synchronizer=None,\n4005         group: str | None = None,\n4006         encoding: Mapping | None = None,\n4007         *,\n4008         compute: Literal[False],\n4009         consolidated: bool | None = None,\n4010         append_dim: Hashable | None = None,\n4011         region: Mapping[str, slice] | None = None,\n4012         safe_chunks: bool = True,\n4013         storage_options: dict[str, str] | None = None,\n4014         zarr_version: int | None = None,\n4015     ) -> Delayed:\n4016         ...\n4017 \n4018     def to_zarr(\n4019         self,\n4020         store: MutableMapping | str | PathLike[str] | None = None,\n4021         chunk_store: MutableMapping | str | PathLike | None = None,\n4022         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n4023         synchronizer=None,\n4024         group: str | None = None,\n4025         encoding: Mapping | None = None,\n4026         compute: bool = True,\n4027         consolidated: bool | None = None,\n4028         append_dim: Hashable | None = None,\n4029         region: Mapping[str, slice] | None = None,\n4030         safe_chunks: bool = True,\n4031         storage_options: dict[str, str] | None = None,\n4032         zarr_version: int | None = None,\n4033     ) -> ZarrStore | Delayed:\n4034         \"\"\"Write DataArray contents to a Zarr store\n4035 \n4036         Zarr chunks are determined in the following way:\n4037 \n4038         - From the ``chunks`` attribute in each variable's ``encoding``\n4039           (can be set via `DataArray.chunk`).\n4040         - If the variable is a Dask array, from the dask chunks\n4041         - If neither Dask chunks nor encoding chunks are present, chunks will\n4042           be determined automatically by Zarr\n4043         - If both Dask chunks and encoding chunks are present, encoding chunks\n4044           will be used, provided that there is a many-to-one relationship between\n4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and\n4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.\n4047           This restriction ensures that no synchronization / locks are required\n4048           when writing. To disable this restriction, use ``safe_chunks=False``.\n4049 \n4050         Parameters\n4051         ----------\n4052         store : MutableMapping, str or path-like, optional\n4053             Store or path to directory in local or remote file system.\n4054         chunk_store : MutableMapping, str or path-like, optional\n4055             Store or path to directory in local or remote file system only for Zarr\n4056             array chunks. Requires zarr-python v2.4.0 or later.\n4057         mode : {\"w\", \"w-\", \"a\", \"r+\", None}, optional\n4058             Persistence mode: \"w\" means create (overwrite if exists);\n4059             \"w-\" means create (fail if exists);\n4060             \"a\" means override existing variables (create if does not exist);\n4061             \"r+\" means modify existing array *values* only (raise an error if\n4062             any metadata or shapes would change).\n4063             The default mode is \"a\" if ``append_dim`` is set. Otherwise, it is\n4064             \"r+\" if ``region`` is set and ``w-`` otherwise.\n4065         synchronizer : object, optional\n4066             Zarr array synchronizer.\n4067         group : str, optional\n4068             Group path. (a.k.a. `path` in zarr terminology.)\n4069         encoding : dict, optional\n4070             Nested dictionary with variable names as keys and dictionaries of\n4071             variable specific encodings as values, e.g.,\n4072             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n4073         compute : bool, default: True\n4074             If True write array data immediately, otherwise return a\n4075             ``dask.delayed.Delayed`` object that can be computed to write\n4076             array data later. Metadata is always updated eagerly.\n4077         consolidated : bool, optional\n4078             If True, apply zarr's `consolidate_metadata` function to the store\n4079             after writing metadata and read existing stores with consolidated\n4080             metadata; if False, do not. The default (`consolidated=None`) means\n4081             write consolidated metadata and attempt to read consolidated\n4082             metadata for existing stores (falling back to non-consolidated).\n4083 \n4084             When the experimental ``zarr_version=3``, ``consolidated`` must be\n4085             either be ``None`` or ``False``.\n4086         append_dim : hashable, optional\n4087             If set, the dimension along which the data will be appended. All\n4088             other dimensions on overridden variables must remain the same size.\n4089         region : dict, optional\n4090             Optional mapping from dimension names to integer slices along\n4091             dataarray dimensions to indicate the region of existing zarr array(s)\n4092             in which to write this datarray's data. For example,\n4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate\n4094             that values should be written to the region ``0:1000`` along ``x``\n4095             and ``10000:11000`` along ``y``.\n4096 \n4097             Two restrictions apply to the use of ``region``:\n4098 \n4099             - If ``region`` is set, _all_ variables in a dataarray must have at\n4100               least one dimension in common with the region. Other variables\n4101               should be written in a separate call to ``to_zarr()``.\n4102             - Dimensions cannot be included in both ``region`` and\n4103               ``append_dim`` at the same time. To create empty arrays to fill\n4104               in with ``region``, use a separate call to ``to_zarr()`` with\n4105               ``compute=False``. See \"Appending to existing Zarr stores\" in\n4106               the reference documentation for full details.\n4107         safe_chunks : bool, default: True\n4108             If True, only allow writes to when there is a many-to-one relationship\n4109             between Zarr chunks (specified in encoding) and Dask chunks.\n4110             Set False to override this restriction; however, data may become corrupted\n4111             if Zarr arrays are written in parallel. This option may be useful in combination\n4112             with ``compute=False`` to initialize a Zarr store from an existing\n4113             DataArray with arbitrary chunk structure.\n4114         storage_options : dict, optional\n4115             Any additional parameters for the storage backend (ignored for local\n4116             paths).\n4117         zarr_version : int or None, optional\n4118             The desired zarr spec version to target (currently 2 or 3). The\n4119             default of None will attempt to determine the zarr version from\n4120             ``store`` when possible, otherwise defaulting to 2.\n4121 \n4122         Returns\n4123         -------\n4124             * ``dask.delayed.Delayed`` if compute is False\n4125             * ZarrStore otherwise\n4126 \n4127         References\n4128         ----------\n4129         https://zarr.readthedocs.io/\n4130 \n4131         Notes\n4132         -----\n4133         Zarr chunking behavior:\n4134             If chunks are found in the encoding argument or attribute\n4135             corresponding to any DataArray, those chunks are used.\n4136             If a DataArray is a dask array, it is written with those chunks.\n4137             If not other chunks are found, Zarr uses its own heuristics to\n4138             choose automatic chunk sizes.\n4139 \n4140         encoding:\n4141             The encoding attribute (if exists) of the DataArray(s) will be\n4142             used. Override any existing encodings by providing the ``encoding`` kwarg.\n4143 \n4144         See Also\n4145         --------\n4146         Dataset.to_zarr\n4147         :ref:`io.zarr`\n4148             The I/O user guide, with more details and examples.\n4149         \"\"\"\n4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr\n4151 \n4152         if self.name is None:\n4153             # If no name is set then use a generic xarray name\n4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n4155         elif self.name in self.coords or self.name in self.dims:\n4156             # The name is the same as one of the coords names, which the netCDF data model\n4157             # does not support, so rename it but keep track of the old name\n4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n4159             dataset.attrs[DATAARRAY_NAME] = self.name\n4160         else:\n4161             # No problems with the name - so we're fine!\n4162             dataset = self.to_dataset()\n4163 \n4164         return to_zarr(  # type: ignore[call-overload,misc]\n4165             dataset,\n4166             store=store,\n4167             chunk_store=chunk_store,\n4168             mode=mode,\n4169             synchronizer=synchronizer,\n4170             group=group,\n4171             encoding=encoding,\n4172             compute=compute,\n4173             consolidated=consolidated,\n4174             append_dim=append_dim,\n4175             region=region,\n4176             safe_chunks=safe_chunks,\n4177             storage_options=storage_options,\n4178             zarr_version=zarr_version,\n4179         )\n4180 \n4181     def to_dict(\n4182         self, data: bool | Literal[\"list\", \"array\"] = \"list\", encoding: bool = False\n4183     ) -> dict[str, Any]:\n4184         \"\"\"\n4185         Convert this xarray.DataArray into a dictionary following xarray\n4186         naming conventions.\n4187 \n4188         Converts all variables and attributes to native Python objects.\n4189         Useful for converting to json. To avoid datetime incompatibility\n4190         use decode_times=False kwarg in xarray.open_dataset.\n4191 \n4192         Parameters\n4193         ----------\n4194         data : bool or {\"list\", \"array\"}, default: \"list\"\n4195             Whether to include the actual data in the dictionary. When set to\n4196             False, returns just the schema. If set to \"array\", returns data as\n4197             underlying array type. If set to \"list\" (or True for backwards\n4198             compatibility), returns data in lists of Python data types. Note\n4199             that for obtaining the \"list\" output efficiently, use\n4200             `da.compute().to_dict(data=\"list\")`.\n4201 \n4202         encoding : bool, default: False\n4203             Whether to include the Dataset's encoding in the dictionary.\n4204 \n4205         Returns\n4206         -------\n4207         dict: dict\n4208 \n4209         See Also\n4210         --------\n4211         DataArray.from_dict\n4212         Dataset.to_dict\n4213         \"\"\"\n4214         d = self.variable.to_dict(data=data)\n4215         d.update({\"coords\": {}, \"name\": self.name})\n4216         for k, coord in self.coords.items():\n4217             d[\"coords\"][k] = coord.variable.to_dict(data=data)\n4218         if encoding:\n4219             d[\"encoding\"] = dict(self.encoding)\n4220         return d\n4221 \n4222     @classmethod\n4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:\n4224         \"\"\"Convert a dictionary into an xarray.DataArray\n4225 \n4226         Parameters\n4227         ----------\n4228         d : dict\n4229             Mapping with a minimum structure of {\"dims\": [...], \"data\": [...]}\n4230 \n4231         Returns\n4232         -------\n4233         obj : xarray.DataArray\n4234 \n4235         See Also\n4236         --------\n4237         DataArray.to_dict\n4238         Dataset.from_dict\n4239 \n4240         Examples\n4241         --------\n4242         >>> d = {\"dims\": \"t\", \"data\": [1, 2, 3]}\n4243         >>> da = xr.DataArray.from_dict(d)\n4244         >>> da\n4245         <xarray.DataArray (t: 3)>\n4246         array([1, 2, 3])\n4247         Dimensions without coordinates: t\n4248 \n4249         >>> d = {\n4250         ...     \"coords\": {\n4251         ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n4252         ...     },\n4253         ...     \"attrs\": {\"title\": \"air temperature\"},\n4254         ...     \"dims\": \"t\",\n4255         ...     \"data\": [10, 20, 30],\n4256         ...     \"name\": \"a\",\n4257         ... }\n4258         >>> da = xr.DataArray.from_dict(d)\n4259         >>> da\n4260         <xarray.DataArray 'a' (t: 3)>\n4261         array([10, 20, 30])\n4262         Coordinates:\n4263           * t        (t) int64 0 1 2\n4264         Attributes:\n4265             title:    air temperature\n4266         \"\"\"\n4267         coords = None\n4268         if \"coords\" in d:\n4269             try:\n4270                 coords = {\n4271                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n4272                     for k, v in d[\"coords\"].items()\n4273                 }\n4274             except KeyError as e:\n4275                 raise ValueError(\n4276                     \"cannot convert dict when coords are missing the key \"\n4277                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n4278                 )\n4279         try:\n4280             data = d[\"data\"]\n4281         except KeyError:\n4282             raise ValueError(\"cannot convert dict without the key 'data''\")\n4283         else:\n4284             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n4285 \n4286         obj.encoding.update(d.get(\"encoding\", {}))\n4287 \n4288         return obj\n4289 \n4290     @classmethod\n4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:\n4292         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n4293 \n4294         If the series's index is a MultiIndex, it will be expanded into a\n4295         tensor product of one-dimensional coordinates (filling in missing\n4296         values with NaN). Thus this operation should be the inverse of the\n4297         `to_series` method.\n4298 \n4299         Parameters\n4300         ----------\n4301         series : Series\n4302             Pandas Series object to convert.\n4303         sparse : bool, default: False\n4304             If sparse=True, creates a sparse array instead of a dense NumPy array.\n4305             Requires the pydata/sparse package.\n4306 \n4307         See Also\n4308         --------\n4309         DataArray.to_series\n4310         Dataset.from_dataframe\n4311         \"\"\"\n4312         temp_name = \"__temporary_name\"\n4313         df = pd.DataFrame({temp_name: series})\n4314         ds = Dataset.from_dataframe(df, sparse=sparse)\n4315         result = cast(DataArray, ds[temp_name])\n4316         result.name = series.name\n4317         return result\n4318 \n4319     def to_cdms2(self) -> cdms2_Variable:\n4320         \"\"\"Convert this array into a cdms2.Variable\"\"\"\n4321         from xarray.convert import to_cdms2\n4322 \n4323         return to_cdms2(self)\n4324 \n4325     @classmethod\n4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:\n4327         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\"\"\"\n4328         from xarray.convert import from_cdms2\n4329 \n4330         return from_cdms2(variable)\n4331 \n4332     def to_iris(self) -> iris_Cube:\n4333         \"\"\"Convert this array into a iris.cube.Cube\"\"\"\n4334         from xarray.convert import to_iris\n4335 \n4336         return to_iris(self)\n4337 \n4338     @classmethod\n4339     def from_iris(cls, cube: iris_Cube) -> DataArray:\n4340         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\"\"\"\n4341         from xarray.convert import from_iris\n4342 \n4343         return from_iris(cube)\n4344 \n4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:\n4346         \"\"\"Helper function for equals, broadcast_equals, and identical\"\"\"\n4347 \n4348         def compat(x, y):\n4349             return getattr(x.variable, compat_str)(y.variable)\n4350 \n4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n4352             self, other\n4353         )\n4354 \n4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:\n4356         \"\"\"Two DataArrays are broadcast equal if they are equal after\n4357         broadcasting them against each other such that they have the same\n4358         dimensions.\n4359 \n4360         Parameters\n4361         ----------\n4362         other : DataArray\n4363             DataArray to compare to.\n4364 \n4365         Returns\n4366         ----------\n4367         equal : bool\n4368             True if the two DataArrays are broadcast equal.\n4369 \n4370         See Also\n4371         --------\n4372         DataArray.equals\n4373         DataArray.identical\n4374 \n4375         Examples\n4376         --------\n4377         >>> a = xr.DataArray([1, 2], dims=\"X\")\n4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=[\"X\", \"Y\"])\n4379         >>> a\n4380         <xarray.DataArray (X: 2)>\n4381         array([1, 2])\n4382         Dimensions without coordinates: X\n4383         >>> b\n4384         <xarray.DataArray (X: 2, Y: 2)>\n4385         array([[1, 1],\n4386                [2, 2]])\n4387         Dimensions without coordinates: X, Y\n4388 \n4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.\n4390 \n4391         >>> a.equals(b)\n4392         False\n4393         >>> a2, b2 = xr.broadcast(a, b)\n4394         >>> a2.equals(b2)\n4395         True\n4396         >>> a.broadcast_equals(b)\n4397         True\n4398         \"\"\"\n4399         try:\n4400             return self._all_compat(other, \"broadcast_equals\")\n4401         except (TypeError, AttributeError):\n4402             return False\n4403 \n4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:\n4405         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n4406         values; otherwise False.\n4407 \n4408         DataArrays can still be equal (like pandas objects) if they have NaN\n4409         values in the same locations.\n4410 \n4411         This method is necessary because `v1 == v2` for ``DataArray``\n4412         does element-wise comparisons (like numpy.ndarrays).\n4413 \n4414         Parameters\n4415         ----------\n4416         other : DataArray\n4417             DataArray to compare to.\n4418 \n4419         Returns\n4420         ----------\n4421         equal : bool\n4422             True if the two DataArrays are equal.\n4423 \n4424         See Also\n4425         --------\n4426         DataArray.broadcast_equals\n4427         DataArray.identical\n4428 \n4429         Examples\n4430         --------\n4431         >>> a = xr.DataArray([1, 2, 3], dims=\"X\")\n4432         >>> b = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"m\"))\n4433         >>> c = xr.DataArray([1, 2, 3], dims=\"Y\")\n4434         >>> d = xr.DataArray([3, 2, 1], dims=\"X\")\n4435         >>> a\n4436         <xarray.DataArray (X: 3)>\n4437         array([1, 2, 3])\n4438         Dimensions without coordinates: X\n4439         >>> b\n4440         <xarray.DataArray (X: 3)>\n4441         array([1, 2, 3])\n4442         Dimensions without coordinates: X\n4443         Attributes:\n4444             units:    m\n4445         >>> c\n4446         <xarray.DataArray (Y: 3)>\n4447         array([1, 2, 3])\n4448         Dimensions without coordinates: Y\n4449         >>> d\n4450         <xarray.DataArray (X: 3)>\n4451         array([3, 2, 1])\n4452         Dimensions without coordinates: X\n4453 \n4454         >>> a.equals(b)\n4455         True\n4456         >>> a.equals(c)\n4457         False\n4458         >>> a.equals(d)\n4459         False\n4460         \"\"\"\n4461         try:\n4462             return self._all_compat(other, \"equals\")\n4463         except (TypeError, AttributeError):\n4464             return False\n4465 \n4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:\n4467         \"\"\"Like equals, but also checks the array name and attributes, and\n4468         attributes on all coordinates.\n4469 \n4470         Parameters\n4471         ----------\n4472         other : DataArray\n4473             DataArray to compare to.\n4474 \n4475         Returns\n4476         ----------\n4477         equal : bool\n4478             True if the two DataArrays are identical.\n4479 \n4480         See Also\n4481         --------\n4482         DataArray.broadcast_equals\n4483         DataArray.equals\n4484 \n4485         Examples\n4486         --------\n4487         >>> a = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"m\"), name=\"Width\")\n4488         >>> b = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"m\"), name=\"Width\")\n4489         >>> c = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"ft\"), name=\"Width\")\n4490         >>> a\n4491         <xarray.DataArray 'Width' (X: 3)>\n4492         array([1, 2, 3])\n4493         Dimensions without coordinates: X\n4494         Attributes:\n4495             units:    m\n4496         >>> b\n4497         <xarray.DataArray 'Width' (X: 3)>\n4498         array([1, 2, 3])\n4499         Dimensions without coordinates: X\n4500         Attributes:\n4501             units:    m\n4502         >>> c\n4503         <xarray.DataArray 'Width' (X: 3)>\n4504         array([1, 2, 3])\n4505         Dimensions without coordinates: X\n4506         Attributes:\n4507             units:    ft\n4508 \n4509         >>> a.equals(b)\n4510         True\n4511         >>> a.identical(b)\n4512         True\n4513 \n4514         >>> a.equals(c)\n4515         True\n4516         >>> a.identical(c)\n4517         False\n4518         \"\"\"\n4519         try:\n4520             return self.name == other.name and self._all_compat(other, \"identical\")\n4521         except (TypeError, AttributeError):\n4522             return False\n4523 \n4524     def _result_name(self, other: Any = None) -> Hashable | None:\n4525         # use the same naming heuristics as pandas:\n4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n4527         other_name = getattr(other, \"name\", _default)\n4528         if other_name is _default or other_name == self.name:\n4529             return self.name\n4530         else:\n4531             return None\n4532 \n4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:\n4534         new_var = self.variable.__array_wrap__(obj, context)\n4535         return self._replace(new_var)\n4536 \n4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:\n4538         return self.dot(obj)\n4539 \n4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n4541         # currently somewhat duplicative, as only other DataArrays are\n4542         # compatible with matmul\n4543         return computation.dot(other, self)\n4544 \n4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:\n4546         keep_attrs = kwargs.pop(\"keep_attrs\", None)\n4547         if keep_attrs is None:\n4548             keep_attrs = _get_keep_attrs(default=True)\n4549         with warnings.catch_warnings():\n4550             warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n4551             warnings.filterwarnings(\n4552                 \"ignore\", r\"Mean of empty slice\", category=RuntimeWarning\n4553             )\n4554             with np.errstate(all=\"ignore\"):\n4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n4556             if keep_attrs:\n4557                 da.attrs = self.attrs\n4558             return da\n4559 \n4560     def _binary_op(\n4561         self: T_DataArray,\n4562         other: Any,\n4563         f: Callable,\n4564         reflexive: bool = False,\n4565     ) -> T_DataArray:\n4566         from xarray.core.groupby import GroupBy\n4567 \n4568         if isinstance(other, (Dataset, GroupBy)):\n4569             return NotImplemented\n4570         if isinstance(other, DataArray):\n4571             align_type = OPTIONS[\"arithmetic_join\"]\n4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore\n4573         other_variable = getattr(other, \"variable\", other)\n4574         other_coords = getattr(other, \"coords\", None)\n4575 \n4576         variable = (\n4577             f(self.variable, other_variable)\n4578             if not reflexive\n4579             else f(other_variable, self.variable)\n4580         )\n4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)\n4582         name = self._result_name(other)\n4583 \n4584         return self._replace(variable, coords, name, indexes=indexes)\n4585 \n4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:\n4587         from xarray.core.groupby import GroupBy\n4588 \n4589         if isinstance(other, GroupBy):\n4590             raise TypeError(\n4591                 \"in-place operations between a DataArray and \"\n4592                 \"a grouped object are not permitted\"\n4593             )\n4594         # n.b. we can't align other to self (with other.reindex_like(self))\n4595         # because `other` may be converted into floats, which would cause\n4596         # in-place arithmetic to fail unpredictably. Instead, we simply\n4597         # don't support automatic alignment with in-place arithmetic.\n4598         other_coords = getattr(other, \"coords\", None)\n4599         other_variable = getattr(other, \"variable\", other)\n4600         try:\n4601             with self.coords._merge_inplace(other_coords):\n4602                 f(self.variable, other_variable)\n4603         except MergeError as exc:\n4604             raise MergeError(\n4605                 \"Automatic alignment is not supported for in-place operations.\\n\"\n4606                 \"Consider aligning the indices manually or using a not-in-place operation.\\n\"\n4607                 \"See https://github.com/pydata/xarray/issues/3910 for more explanations.\"\n4608             ) from exc\n4609         return self\n4610 \n4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:\n4612         self.attrs = other.attrs\n4613 \n4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)\n4615 \n4616     def _title_for_slice(self, truncate: int = 50) -> str:\n4617         \"\"\"\n4618         If the dataarray has 1 dimensional coordinates or comes from a slice\n4619         we can show that info in the title\n4620 \n4621         Parameters\n4622         ----------\n4623         truncate : int, default: 50\n4624             maximum number of characters for title\n4625 \n4626         Returns\n4627         -------\n4628         title : string\n4629             Can be used for plot titles\n4630 \n4631         \"\"\"\n4632         one_dims = []\n4633         for dim, coord in self.coords.items():\n4634             if coord.size == 1:\n4635                 one_dims.append(\n4636                     \"{dim} = {v}{unit}\".format(\n4637                         dim=dim,\n4638                         v=format_item(coord.values),\n4639                         unit=_get_units_from_attrs(coord),\n4640                     )\n4641                 )\n4642 \n4643         title = \", \".join(one_dims)\n4644         if len(title) > truncate:\n4645             title = title[: (truncate - 3)] + \"...\"\n4646 \n4647         return title\n4648 \n4649     def diff(\n4650         self: T_DataArray,\n4651         dim: Hashable,\n4652         n: int = 1,\n4653         label: Literal[\"upper\", \"lower\"] = \"upper\",\n4654     ) -> T_DataArray:\n4655         \"\"\"Calculate the n-th order discrete difference along given axis.\n4656 \n4657         Parameters\n4658         ----------\n4659         dim : Hashable\n4660             Dimension over which to calculate the finite difference.\n4661         n : int, default: 1\n4662             The number of times values are differenced.\n4663         label : {\"upper\", \"lower\"}, default: \"upper\"\n4664             The new coordinate in dimension ``dim`` will have the\n4665             values of either the minuend's or subtrahend's coordinate\n4666             for values 'upper' and 'lower', respectively.\n4667 \n4668         Returns\n4669         -------\n4670         difference : DataArray\n4671             The n-th order finite difference of this object.\n4672 \n4673         Notes\n4674         -----\n4675         `n` matches numpy's behavior and is different from pandas' first argument named\n4676         `periods`.\n4677 \n4678         Examples\n4679         --------\n4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n4681         >>> arr.diff(\"x\")\n4682         <xarray.DataArray (x: 3)>\n4683         array([0, 1, 0])\n4684         Coordinates:\n4685           * x        (x) int64 2 3 4\n4686         >>> arr.diff(\"x\", 2)\n4687         <xarray.DataArray (x: 2)>\n4688         array([ 1, -1])\n4689         Coordinates:\n4690           * x        (x) int64 3 4\n4691 \n4692         See Also\n4693         --------\n4694         DataArray.differentiate\n4695         \"\"\"\n4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n4697         return self._from_temp_dataset(ds)\n4698 \n4699     def shift(\n4700         self: T_DataArray,\n4701         shifts: Mapping[Any, int] | None = None,\n4702         fill_value: Any = dtypes.NA,\n4703         **shifts_kwargs: int,\n4704     ) -> T_DataArray:\n4705         \"\"\"Shift this DataArray by an offset along one or more dimensions.\n4706 \n4707         Only the data is moved; coordinates stay in place. This is consistent\n4708         with the behavior of ``shift`` in pandas.\n4709 \n4710         Values shifted from beyond array bounds will appear at one end of\n4711         each dimension, which are filled according to `fill_value`. For periodic\n4712         offsets instead see `roll`.\n4713 \n4714         Parameters\n4715         ----------\n4716         shifts : mapping of Hashable to int or None, optional\n4717             Integer offset to shift along each of the given dimensions.\n4718             Positive offsets shift to the right; negative offsets shift to the\n4719             left.\n4720         fill_value : scalar, optional\n4721             Value to use for newly missing values\n4722         **shifts_kwargs\n4723             The keyword arguments form of ``shifts``.\n4724             One of shifts or shifts_kwargs must be provided.\n4725 \n4726         Returns\n4727         -------\n4728         shifted : DataArray\n4729             DataArray with the same coordinates and attributes but shifted\n4730             data.\n4731 \n4732         See Also\n4733         --------\n4734         roll\n4735 \n4736         Examples\n4737         --------\n4738         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n4739         >>> arr.shift(x=1)\n4740         <xarray.DataArray (x: 3)>\n4741         array([nan,  5.,  6.])\n4742         Dimensions without coordinates: x\n4743         \"\"\"\n4744         variable = self.variable.shift(\n4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n4746         )\n4747         return self._replace(variable=variable)\n4748 \n4749     def roll(\n4750         self: T_DataArray,\n4751         shifts: Mapping[Hashable, int] | None = None,\n4752         roll_coords: bool = False,\n4753         **shifts_kwargs: int,\n4754     ) -> T_DataArray:\n4755         \"\"\"Roll this array by an offset along one or more dimensions.\n4756 \n4757         Unlike shift, roll treats the given dimensions as periodic, so will not\n4758         create any missing values to be filled.\n4759 \n4760         Unlike shift, roll may rotate all variables, including coordinates\n4761         if specified. The direction of rotation is consistent with\n4762         :py:func:`numpy.roll`.\n4763 \n4764         Parameters\n4765         ----------\n4766         shifts : mapping of Hashable to int, optional\n4767             Integer offset to rotate each of the given dimensions.\n4768             Positive offsets roll to the right; negative offsets roll to the\n4769             left.\n4770         roll_coords : bool, default: False\n4771             Indicates whether to roll the coordinates by the offset too.\n4772         **shifts_kwargs : {dim: offset, ...}, optional\n4773             The keyword arguments form of ``shifts``.\n4774             One of shifts or shifts_kwargs must be provided.\n4775 \n4776         Returns\n4777         -------\n4778         rolled : DataArray\n4779             DataArray with the same attributes but rolled data and coordinates.\n4780 \n4781         See Also\n4782         --------\n4783         shift\n4784 \n4785         Examples\n4786         --------\n4787         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n4788         >>> arr.roll(x=1)\n4789         <xarray.DataArray (x: 3)>\n4790         array([7, 5, 6])\n4791         Dimensions without coordinates: x\n4792         \"\"\"\n4793         ds = self._to_temp_dataset().roll(\n4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n4795         )\n4796         return self._from_temp_dataset(ds)\n4797 \n4798     @property\n4799     def real(self: T_DataArray) -> T_DataArray:\n4800         \"\"\"\n4801         The real part of the array.\n4802 \n4803         See Also\n4804         --------\n4805         numpy.ndarray.real\n4806         \"\"\"\n4807         return self._replace(self.variable.real)\n4808 \n4809     @property\n4810     def imag(self: T_DataArray) -> T_DataArray:\n4811         \"\"\"\n4812         The imaginary part of the array.\n4813 \n4814         See Also\n4815         --------\n4816         numpy.ndarray.imag\n4817         \"\"\"\n4818         return self._replace(self.variable.imag)\n4819 \n4820     def dot(\n4821         self: T_DataArray,\n4822         other: T_DataArray,\n4823         dims: Dims = None,\n4824     ) -> T_DataArray:\n4825         \"\"\"Perform dot product of two DataArrays along their shared dims.\n4826 \n4827         Equivalent to taking taking tensordot over all shared dims.\n4828 \n4829         Parameters\n4830         ----------\n4831         other : DataArray\n4832             The other array with which the dot product is performed.\n4833         dims : ..., str, Iterable of Hashable or None, optional\n4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.\n4835             If not specified, then all the common dimensions are summed over.\n4836 \n4837         Returns\n4838         -------\n4839         result : DataArray\n4840             Array resulting from the dot product over all shared dimensions.\n4841 \n4842         See Also\n4843         --------\n4844         dot\n4845         numpy.tensordot\n4846 \n4847         Examples\n4848         --------\n4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n4850         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n4851         >>> dm_vals = np.arange(4)\n4852         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n4853 \n4854         >>> dm.dims\n4855         ('z',)\n4856 \n4857         >>> da.dims\n4858         ('x', 'y', 'z')\n4859 \n4860         >>> dot_result = da.dot(dm)\n4861         >>> dot_result.dims\n4862         ('x', 'y')\n4863 \n4864         \"\"\"\n4865         if isinstance(other, Dataset):\n4866             raise NotImplementedError(\n4867                 \"dot products are not yet supported with Dataset objects.\"\n4868             )\n4869         if not isinstance(other, DataArray):\n4870             raise TypeError(\"dot only operates on DataArrays.\")\n4871 \n4872         return computation.dot(self, other, dims=dims)\n4873 \n4874     # change type of self and return to T_DataArray once\n4875     # https://github.com/python/mypy/issues/12846 is resolved\n4876     def sortby(\n4877         self,\n4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],\n4879         ascending: bool = True,\n4880     ) -> DataArray:\n4881         \"\"\"Sort object by labels or values (along an axis).\n4882 \n4883         Sorts the dataarray, either along specified dimensions,\n4884         or according to values of 1-D dataarrays that share dimension\n4885         with calling object.\n4886 \n4887         If the input variables are dataarrays, then the dataarrays are aligned\n4888         (via left-join) to the calling object prior to sorting by cell values.\n4889         NaNs are sorted to the end, following Numpy convention.\n4890 \n4891         If multiple sorts along the same dimension is\n4892         given, numpy's lexsort is performed along that dimension:\n4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n4894         and the FIRST key in the sequence is used as the primary sort key,\n4895         followed by the 2nd key, etc.\n4896 \n4897         Parameters\n4898         ----------\n4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray\n4900             1D DataArray objects or name(s) of 1D variable(s) in\n4901             coords whose values are used to sort this array.\n4902         ascending : bool, default: True\n4903             Whether to sort by ascending or descending order.\n4904 \n4905         Returns\n4906         -------\n4907         sorted : DataArray\n4908             A new dataarray where all the specified dims are sorted by dim\n4909             labels.\n4910 \n4911         See Also\n4912         --------\n4913         Dataset.sortby\n4914         numpy.sort\n4915         pandas.sort_values\n4916         pandas.sort_index\n4917 \n4918         Examples\n4919         --------\n4920         >>> da = xr.DataArray(\n4921         ...     np.random.rand(5),\n4922         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n4923         ...     dims=\"time\",\n4924         ... )\n4925         >>> da\n4926         <xarray.DataArray (time: 5)>\n4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])\n4928         Coordinates:\n4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05\n4930 \n4931         >>> da.sortby(da)\n4932         <xarray.DataArray (time: 5)>\n4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])\n4934         Coordinates:\n4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02\n4936         \"\"\"\n4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n4938         return self._from_temp_dataset(ds)\n4939 \n4940     def quantile(\n4941         self: T_DataArray,\n4942         q: ArrayLike,\n4943         dim: Dims = None,\n4944         method: QuantileMethods = \"linear\",\n4945         keep_attrs: bool | None = None,\n4946         skipna: bool | None = None,\n4947         interpolation: QuantileMethods | None = None,\n4948     ) -> T_DataArray:\n4949         \"\"\"Compute the qth quantile of the data along the specified dimension.\n4950 \n4951         Returns the qth quantiles(s) of the array elements.\n4952 \n4953         Parameters\n4954         ----------\n4955         q : float or array-like of float\n4956             Quantile to compute, which must be between 0 and 1 inclusive.\n4957         dim : str or Iterable of Hashable, optional\n4958             Dimension(s) over which to apply quantile.\n4959         method : str, default: \"linear\"\n4960             This optional parameter specifies the interpolation method to use when the\n4961             desired quantile lies between two data points. The options sorted by their R\n4962             type as summarized in the H&F paper [1]_ are:\n4963 \n4964                 1. \"inverted_cdf\" (*)\n4965                 2. \"averaged_inverted_cdf\" (*)\n4966                 3. \"closest_observation\" (*)\n4967                 4. \"interpolated_inverted_cdf\" (*)\n4968                 5. \"hazen\" (*)\n4969                 6. \"weibull\" (*)\n4970                 7. \"linear\"  (default)\n4971                 8. \"median_unbiased\" (*)\n4972                 9. \"normal_unbiased\" (*)\n4973 \n4974             The first three methods are discontiuous. The following discontinuous\n4975             variations of the default \"linear\" (7.) option are also available:\n4976 \n4977                 * \"lower\"\n4978                 * \"higher\"\n4979                 * \"midpoint\"\n4980                 * \"nearest\"\n4981 \n4982             See :py:func:`numpy.quantile` or [1]_ for details. The \"method\" argument\n4983             was previously called \"interpolation\", renamed in accordance with numpy\n4984             version 1.22.0.\n4985 \n4986             (*) These methods require numpy version 1.22 or newer.\n4987 \n4988         keep_attrs : bool or None, optional\n4989             If True, the dataset's attributes (`attrs`) will be copied from\n4990             the original object to the new one.  If False (default), the new\n4991             object will be returned without attributes.\n4992         skipna : bool or None, optional\n4993             If True, skip missing values (as marked by NaN). By default, only\n4994             skips missing values for float dtypes; other dtypes either do not\n4995             have a sentinel missing value (int) or skipna=True has not been\n4996             implemented (object, datetime64 or timedelta64).\n4997 \n4998         Returns\n4999         -------\n5000         quantiles : DataArray\n5001             If `q` is a single quantile, then the result\n5002             is a scalar. If multiple percentiles are given, first axis of\n5003             the result corresponds to the quantile and a quantile dimension\n5004             is added to the return array. The other dimensions are the\n5005             dimensions that remain after the reduction of the array.\n5006 \n5007         See Also\n5008         --------\n5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n5010 \n5011         Examples\n5012         --------\n5013         >>> da = xr.DataArray(\n5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n5015         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n5016         ...     dims=(\"x\", \"y\"),\n5017         ... )\n5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n5019         <xarray.DataArray ()>\n5020         array(0.7)\n5021         Coordinates:\n5022             quantile  float64 0.0\n5023         >>> da.quantile(0, dim=\"x\")\n5024         <xarray.DataArray (y: 4)>\n5025         array([0.7, 4.2, 2.6, 1.5])\n5026         Coordinates:\n5027           * y         (y) float64 1.0 1.5 2.0 2.5\n5028             quantile  float64 0.0\n5029         >>> da.quantile([0, 0.5, 1])\n5030         <xarray.DataArray (quantile: 3)>\n5031         array([0.7, 3.4, 9.4])\n5032         Coordinates:\n5033           * quantile  (quantile) float64 0.0 0.5 1.0\n5034         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n5035         <xarray.DataArray (quantile: 3, y: 4)>\n5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n5037                [3.6 , 5.75, 6.  , 1.7 ],\n5038                [6.5 , 7.3 , 9.4 , 1.9 ]])\n5039         Coordinates:\n5040           * y         (y) float64 1.0 1.5 2.0 2.5\n5041           * quantile  (quantile) float64 0.0 0.5 1.0\n5042 \n5043         References\n5044         ----------\n5045         .. [1] R. J. Hyndman and Y. Fan,\n5046            \"Sample quantiles in statistical packages,\"\n5047            The American Statistician, 50(4), pp. 361-365, 1996\n5048         \"\"\"\n5049 \n5050         ds = self._to_temp_dataset().quantile(\n5051             q,\n5052             dim=dim,\n5053             keep_attrs=keep_attrs,\n5054             method=method,\n5055             skipna=skipna,\n5056             interpolation=interpolation,\n5057         )\n5058         return self._from_temp_dataset(ds)\n5059 \n5060     def rank(\n5061         self: T_DataArray,\n5062         dim: Hashable,\n5063         pct: bool = False,\n5064         keep_attrs: bool | None = None,\n5065     ) -> T_DataArray:\n5066         \"\"\"Ranks the data.\n5067 \n5068         Equal values are assigned a rank that is the average of the ranks that\n5069         would have been otherwise assigned to all of the values within that\n5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n5071 \n5072         NaNs in the input array are returned as NaNs.\n5073 \n5074         The `bottleneck` library is required.\n5075 \n5076         Parameters\n5077         ----------\n5078         dim : Hashable\n5079             Dimension over which to compute rank.\n5080         pct : bool, default: False\n5081             If True, compute percentage ranks, otherwise compute integer ranks.\n5082         keep_attrs : bool or None, optional\n5083             If True, the dataset's attributes (`attrs`) will be copied from\n5084             the original object to the new one.  If False (default), the new\n5085             object will be returned without attributes.\n5086 \n5087         Returns\n5088         -------\n5089         ranked : DataArray\n5090             DataArray with the same coordinates and dtype 'float64'.\n5091 \n5092         Examples\n5093         --------\n5094         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n5095         >>> arr.rank(\"x\")\n5096         <xarray.DataArray (x: 3)>\n5097         array([1., 2., 3.])\n5098         Dimensions without coordinates: x\n5099         \"\"\"\n5100 \n5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n5102         return self._from_temp_dataset(ds)\n5103 \n5104     def differentiate(\n5105         self: T_DataArray,\n5106         coord: Hashable,\n5107         edge_order: Literal[1, 2] = 1,\n5108         datetime_unit: DatetimeUnitOptions = None,\n5109     ) -> T_DataArray:\n5110         \"\"\" Differentiate the array with the second order accurate central\n5111         differences.\n5112 \n5113         .. note::\n5114             This feature is limited to simple cartesian geometry, i.e. coord\n5115             must be one dimensional.\n5116 \n5117         Parameters\n5118         ----------\n5119         coord : Hashable\n5120             The coordinate to be used to compute the gradient.\n5121         edge_order : {1, 2}, default: 1\n5122             N-th order accurate differences at the boundaries.\n5123         datetime_unit : {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n5124                          \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, optional\n5125             Unit to compute gradient. Only valid for datetime coordinate.\n5126 \n5127         Returns\n5128         -------\n5129         differentiated: DataArray\n5130 \n5131         See also\n5132         --------\n5133         numpy.gradient: corresponding numpy function\n5134 \n5135         Examples\n5136         --------\n5137 \n5138         >>> da = xr.DataArray(\n5139         ...     np.arange(12).reshape(4, 3),\n5140         ...     dims=[\"x\", \"y\"],\n5141         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n5142         ... )\n5143         >>> da\n5144         <xarray.DataArray (x: 4, y: 3)>\n5145         array([[ 0,  1,  2],\n5146                [ 3,  4,  5],\n5147                [ 6,  7,  8],\n5148                [ 9, 10, 11]])\n5149         Coordinates:\n5150           * x        (x) float64 0.0 0.1 1.1 1.2\n5151         Dimensions without coordinates: y\n5152         >>>\n5153         >>> da.differentiate(\"x\")\n5154         <xarray.DataArray (x: 4, y: 3)>\n5155         array([[30.        , 30.        , 30.        ],\n5156                [27.54545455, 27.54545455, 27.54545455],\n5157                [27.54545455, 27.54545455, 27.54545455],\n5158                [30.        , 30.        , 30.        ]])\n5159         Coordinates:\n5160           * x        (x) float64 0.0 0.1 1.1 1.2\n5161         Dimensions without coordinates: y\n5162         \"\"\"\n5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n5164         return self._from_temp_dataset(ds)\n5165 \n5166     # change type of self and return to T_DataArray once\n5167     # https://github.com/python/mypy/issues/12846 is resolved\n5168     def integrate(\n5169         self,\n5170         coord: Hashable | Sequence[Hashable] = None,\n5171         datetime_unit: DatetimeUnitOptions = None,\n5172     ) -> DataArray:\n5173         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n5174 \n5175         .. note::\n5176             This feature is limited to simple cartesian geometry, i.e. coord\n5177             must be one dimensional.\n5178 \n5179         Parameters\n5180         ----------\n5181         coord : Hashable, or sequence of Hashable\n5182             Coordinate(s) used for the integration.\n5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n5184                         'ps', 'fs', 'as', None}, optional\n5185             Specify the unit if a datetime coordinate is used.\n5186 \n5187         Returns\n5188         -------\n5189         integrated : DataArray\n5190 \n5191         See also\n5192         --------\n5193         Dataset.integrate\n5194         numpy.trapz : corresponding numpy function\n5195 \n5196         Examples\n5197         --------\n5198 \n5199         >>> da = xr.DataArray(\n5200         ...     np.arange(12).reshape(4, 3),\n5201         ...     dims=[\"x\", \"y\"],\n5202         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n5203         ... )\n5204         >>> da\n5205         <xarray.DataArray (x: 4, y: 3)>\n5206         array([[ 0,  1,  2],\n5207                [ 3,  4,  5],\n5208                [ 6,  7,  8],\n5209                [ 9, 10, 11]])\n5210         Coordinates:\n5211           * x        (x) float64 0.0 0.1 1.1 1.2\n5212         Dimensions without coordinates: y\n5213         >>>\n5214         >>> da.integrate(\"x\")\n5215         <xarray.DataArray (y: 3)>\n5216         array([5.4, 6.6, 7.8])\n5217         Dimensions without coordinates: y\n5218         \"\"\"\n5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)\n5220         return self._from_temp_dataset(ds)\n5221 \n5222     # change type of self and return to T_DataArray once\n5223     # https://github.com/python/mypy/issues/12846 is resolved\n5224     def cumulative_integrate(\n5225         self,\n5226         coord: Hashable | Sequence[Hashable] = None,\n5227         datetime_unit: DatetimeUnitOptions = None,\n5228     ) -> DataArray:\n5229         \"\"\"Integrate cumulatively along the given coordinate using the trapezoidal rule.\n5230 \n5231         .. note::\n5232             This feature is limited to simple cartesian geometry, i.e. coord\n5233             must be one dimensional.\n5234 \n5235             The first entry of the cumulative integral is always 0, in order to keep the\n5236             length of the dimension unchanged between input and output.\n5237 \n5238         Parameters\n5239         ----------\n5240         coord : Hashable, or sequence of Hashable\n5241             Coordinate(s) used for the integration.\n5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n5243                         'ps', 'fs', 'as', None}, optional\n5244             Specify the unit if a datetime coordinate is used.\n5245 \n5246         Returns\n5247         -------\n5248         integrated : DataArray\n5249 \n5250         See also\n5251         --------\n5252         Dataset.cumulative_integrate\n5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function\n5254 \n5255         Examples\n5256         --------\n5257 \n5258         >>> da = xr.DataArray(\n5259         ...     np.arange(12).reshape(4, 3),\n5260         ...     dims=[\"x\", \"y\"],\n5261         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n5262         ... )\n5263         >>> da\n5264         <xarray.DataArray (x: 4, y: 3)>\n5265         array([[ 0,  1,  2],\n5266                [ 3,  4,  5],\n5267                [ 6,  7,  8],\n5268                [ 9, 10, 11]])\n5269         Coordinates:\n5270           * x        (x) float64 0.0 0.1 1.1 1.2\n5271         Dimensions without coordinates: y\n5272         >>>\n5273         >>> da.cumulative_integrate(\"x\")\n5274         <xarray.DataArray (x: 4, y: 3)>\n5275         array([[0.  , 0.  , 0.  ],\n5276                [0.15, 0.25, 0.35],\n5277                [4.65, 5.75, 6.85],\n5278                [5.4 , 6.6 , 7.8 ]])\n5279         Coordinates:\n5280           * x        (x) float64 0.0 0.1 1.1 1.2\n5281         Dimensions without coordinates: y\n5282         \"\"\"\n5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)\n5284         return self._from_temp_dataset(ds)\n5285 \n5286     def unify_chunks(self) -> DataArray:\n5287         \"\"\"Unify chunk size along all chunked dimensions of this DataArray.\n5288 \n5289         Returns\n5290         -------\n5291         DataArray with consistent chunk sizes for all dask-array variables\n5292 \n5293         See Also\n5294         --------\n5295         dask.array.core.unify_chunks\n5296         \"\"\"\n5297 \n5298         return unify_chunks(self)[0]\n5299 \n5300     def map_blocks(\n5301         self,\n5302         func: Callable[..., T_Xarray],\n5303         args: Sequence[Any] = (),\n5304         kwargs: Mapping[str, Any] | None = None,\n5305         template: DataArray | Dataset | None = None,\n5306     ) -> T_Xarray:\n5307         \"\"\"\n5308         Apply a function to each block of this DataArray.\n5309 \n5310         .. warning::\n5311             This method is experimental and its signature may change.\n5312 \n5313         Parameters\n5314         ----------\n5315         func : callable\n5316             User-provided function that accepts a DataArray as its first\n5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),\n5318             corresponding to one chunk along each chunked dimension. ``func`` will be\n5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n5320 \n5321             This function must return either a single DataArray or a single Dataset.\n5322 \n5323             This function cannot add a new chunked dimension.\n5324         args : sequence\n5325             Passed to func after unpacking and subsetting any xarray objects by blocks.\n5326             xarray objects in args must be aligned with this object, otherwise an error is raised.\n5327         kwargs : mapping\n5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n5329             subset to blocks. Passing dask collections in kwargs is not allowed.\n5330         template : DataArray or Dataset, optional\n5331             xarray object representing the final result after compute is called. If not provided,\n5332             the function will be first run on mocked-up data, that looks like this object but\n5333             has sizes 0, to determine properties of the returned object such as dtype,\n5334             variable names, attributes, new dimensions and new indexes (if any).\n5335             ``template`` must be provided if the function changes the size of existing dimensions.\n5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n5337             ``attrs`` set by ``func`` will be ignored.\n5338 \n5339         Returns\n5340         -------\n5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n5342         function.\n5343 \n5344         Notes\n5345         -----\n5346         This function is designed for when ``func`` needs to manipulate a whole xarray object\n5347         subset to each block. Each block is loaded into memory. In the more common case where\n5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n5349 \n5350         If none of the variables in this object is backed by dask arrays, calling this function is\n5351         equivalent to calling ``func(obj, *args, **kwargs)``.\n5352 \n5353         See Also\n5354         --------\n5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n5356         xarray.DataArray.map_blocks\n5357 \n5358         Examples\n5359         --------\n5360         Calculate an anomaly from climatology using ``.groupby()``. Using\n5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n5362         its indices, and its methods like ``.groupby()``.\n5363 \n5364         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n5365         ...     gb = da.groupby(groupby_type)\n5366         ...     clim = gb.mean(dim=\"time\")\n5367         ...     return gb - clim\n5368         ...\n5369         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n5370         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n5371         >>> np.random.seed(123)\n5372         >>> array = xr.DataArray(\n5373         ...     np.random.rand(len(time)),\n5374         ...     dims=[\"time\"],\n5375         ...     coords={\"time\": time, \"month\": month},\n5376         ... ).chunk()\n5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()\n5378         <xarray.DataArray (time: 24)>\n5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n5384         Coordinates:\n5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n5387 \n5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n5389         to the function being applied in ``xr.map_blocks()``:\n5390 \n5391         >>> array.map_blocks(\n5392         ...     calculate_anomaly, kwargs={\"groupby_type\": \"time.year\"}, template=array\n5393         ... )  # doctest: +ELLIPSIS\n5394         <xarray.DataArray (time: 24)>\n5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>\n5396         Coordinates:\n5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n5399         \"\"\"\n5400         from xarray.core.parallel import map_blocks\n5401 \n5402         return map_blocks(func, self, args, kwargs, template)\n5403 \n5404     def polyfit(\n5405         self,\n5406         dim: Hashable,\n5407         deg: int,\n5408         skipna: bool | None = None,\n5409         rcond: float | None = None,\n5410         w: Hashable | Any | None = None,\n5411         full: bool = False,\n5412         cov: bool | Literal[\"unscaled\"] = False,\n5413     ) -> Dataset:\n5414         \"\"\"\n5415         Least squares polynomial fit.\n5416 \n5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n5418         invalid values when `skipna = True`.\n5419 \n5420         Parameters\n5421         ----------\n5422         dim : Hashable\n5423             Coordinate along which to fit the polynomials.\n5424         deg : int\n5425             Degree of the fitting polynomial.\n5426         skipna : bool or None, optional\n5427             If True, removes all invalid values before fitting each 1D slices of the array.\n5428             Default is True if data is stored in a dask.array or if there is any\n5429             invalid values, False otherwise.\n5430         rcond : float or None, optional\n5431             Relative condition number to the fit.\n5432         w : Hashable, array-like or None, optional\n5433             Weights to apply to the y-coordinate of the sample points.\n5434             Can be an array-like object or the name of a coordinate in the dataset.\n5435         full : bool, default: False\n5436             Whether to return the residuals, matrix rank and singular values in addition\n5437             to the coefficients.\n5438         cov : bool or \"unscaled\", default: False\n5439             Whether to return to the covariance matrix in addition to the coefficients.\n5440             The matrix is not scaled if `cov='unscaled'`.\n5441 \n5442         Returns\n5443         -------\n5444         polyfit_results : Dataset\n5445             A single dataset which contains:\n5446 \n5447             polyfit_coefficients\n5448                 The coefficients of the best fit.\n5449             polyfit_residuals\n5450                 The residuals of the least-square computation (only included if `full=True`).\n5451                 When the matrix rank is deficient, np.nan is returned.\n5452             [dim]_matrix_rank\n5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n5454             [dim]_singular_value\n5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n5456             polyfit_covariance\n5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n5458 \n5459         See Also\n5460         --------\n5461         numpy.polyfit\n5462         numpy.polyval\n5463         xarray.polyval\n5464         \"\"\"\n5465         return self._to_temp_dataset().polyfit(\n5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n5467         )\n5468 \n5469     def pad(\n5470         self: T_DataArray,\n5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,\n5472         mode: PadModeOptions = \"constant\",\n5473         stat_length: int\n5474         | tuple[int, int]\n5475         | Mapping[Any, tuple[int, int]]\n5476         | None = None,\n5477         constant_values: float\n5478         | tuple[float, float]\n5479         | Mapping[Any, tuple[float, float]]\n5480         | None = None,\n5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,\n5482         reflect_type: PadReflectOptions = None,\n5483         keep_attrs: bool | None = None,\n5484         **pad_width_kwargs: Any,\n5485     ) -> T_DataArray:\n5486         \"\"\"Pad this array along one or more dimensions.\n5487 \n5488         .. warning::\n5489             This function is experimental and its behaviour is likely to change\n5490             especially regarding padding of dimension coordinates (or IndexVariables).\n5491 \n5492         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n5493         coordinates will be padded with the same mode, otherwise coordinates\n5494         are padded using the \"constant\" mode with fill_value dtypes.NA.\n5495 \n5496         Parameters\n5497         ----------\n5498         pad_width : mapping of Hashable to tuple of int\n5499             Mapping with the form of {dim: (pad_before, pad_after)}\n5500             describing the number of values padded along each dimension.\n5501             {dim: pad} is a shortcut for pad_before = pad_after = pad\n5502         mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \\\n5503             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n5504             How to pad the DataArray (taken from numpy docs):\n5505 \n5506             - \"constant\": Pads with a constant value.\n5507             - \"edge\": Pads with the edge values of array.\n5508             - \"linear_ramp\": Pads with the linear ramp between end_value and the\n5509               array edge value.\n5510             - \"maximum\": Pads with the maximum value of all or part of the\n5511               vector along each axis.\n5512             - \"mean\": Pads with the mean value of all or part of the\n5513               vector along each axis.\n5514             - \"median\": Pads with the median value of all or part of the\n5515               vector along each axis.\n5516             - \"minimum\": Pads with the minimum value of all or part of the\n5517               vector along each axis.\n5518             - \"reflect\": Pads with the reflection of the vector mirrored on\n5519               the first and last values of the vector along each axis.\n5520             - \"symmetric\": Pads with the reflection of the vector mirrored\n5521               along the edge of the array.\n5522             - \"wrap\": Pads with the wrap of the vector along the axis.\n5523               The first values are used to pad the end and the\n5524               end values are used to pad the beginning.\n5525 \n5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None\n5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n5528             values at edge of each axis used to calculate the statistic value.\n5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n5530             statistic lengths along each dimension.\n5531             ((before, after),) yields same before and after statistic lengths\n5532             for each dimension.\n5533             (stat_length,) or int is a shortcut for before = after = statistic\n5534             length for all axes.\n5535             Default is ``None``, to use the entire axis.\n5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n5537             Used in 'constant'.  The values to set the padded values for each\n5538             axis.\n5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n5540             pad constants along each dimension.\n5541             ``((before, after),)`` yields same before and after constants for each\n5542             dimension.\n5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n5544             all dimensions.\n5545             Default is 0.\n5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n5547             Used in 'linear_ramp'.  The values used for the ending value of the\n5548             linear_ramp and that will form the edge of the padded array.\n5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n5550             end values along each dimension.\n5551             ``((before, after),)`` yields same before and after end values for each\n5552             axis.\n5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n5554             all axes.\n5555             Default is 0.\n5556         reflect_type : {\"even\", \"odd\", None}, optional\n5557             Used in \"reflect\", and \"symmetric\". The \"even\" style is the\n5558             default with an unaltered reflection around the edge value. For\n5559             the \"odd\" style, the extended part of the array is created by\n5560             subtracting the reflected values from two times the edge value.\n5561         keep_attrs : bool or None, optional\n5562             If True, the attributes (``attrs``) will be copied from the\n5563             original object to the new one. If False, the new object\n5564             will be returned without attributes.\n5565         **pad_width_kwargs\n5566             The keyword arguments form of ``pad_width``.\n5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n5568 \n5569         Returns\n5570         -------\n5571         padded : DataArray\n5572             DataArray with the padded coordinates and data.\n5573 \n5574         See Also\n5575         --------\n5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n5577 \n5578         Notes\n5579         -----\n5580         For ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n5581         promoted to ``float`` and padded with ``np.nan``.\n5582 \n5583         Padding coordinates will drop their corresponding index (if any) and will reset default\n5584         indexes for dimension coordinates.\n5585 \n5586         Examples\n5587         --------\n5588         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0, 1, 2])])\n5589         >>> arr.pad(x=(1, 2), constant_values=0)\n5590         <xarray.DataArray (x: 6)>\n5591         array([0, 5, 6, 7, 0, 0])\n5592         Coordinates:\n5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n5594 \n5595         >>> da = xr.DataArray(\n5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n5597         ...     dims=[\"x\", \"y\"],\n5598         ...     coords={\"x\": [0, 1], \"y\": [10, 20, 30, 40], \"z\": (\"x\", [100, 200])},\n5599         ... )\n5600         >>> da.pad(x=1)\n5601         <xarray.DataArray (x: 4, y: 4)>\n5602         array([[nan, nan, nan, nan],\n5603                [ 0.,  1.,  2.,  3.],\n5604                [10., 11., 12., 13.],\n5605                [nan, nan, nan, nan]])\n5606         Coordinates:\n5607           * x        (x) float64 nan 0.0 1.0 nan\n5608           * y        (y) int64 10 20 30 40\n5609             z        (x) float64 nan 100.0 200.0 nan\n5610 \n5611         Careful, ``constant_values`` are coerced to the data type of the array which may\n5612         lead to a loss of precision:\n5613 \n5614         >>> da.pad(x=1, constant_values=1.23456789)\n5615         <xarray.DataArray (x: 4, y: 4)>\n5616         array([[ 1,  1,  1,  1],\n5617                [ 0,  1,  2,  3],\n5618                [10, 11, 12, 13],\n5619                [ 1,  1,  1,  1]])\n5620         Coordinates:\n5621           * x        (x) float64 nan 0.0 1.0 nan\n5622           * y        (y) int64 10 20 30 40\n5623             z        (x) float64 nan 100.0 200.0 nan\n5624         \"\"\"\n5625         ds = self._to_temp_dataset().pad(\n5626             pad_width=pad_width,\n5627             mode=mode,\n5628             stat_length=stat_length,\n5629             constant_values=constant_values,\n5630             end_values=end_values,\n5631             reflect_type=reflect_type,\n5632             keep_attrs=keep_attrs,\n5633             **pad_width_kwargs,\n5634         )\n5635         return self._from_temp_dataset(ds)\n5636 \n5637     def idxmin(\n5638         self,\n5639         dim: Hashable | None = None,\n5640         skipna: bool | None = None,\n5641         fill_value: Any = dtypes.NA,\n5642         keep_attrs: bool | None = None,\n5643     ) -> DataArray:\n5644         \"\"\"Return the coordinate label of the minimum value along a dimension.\n5645 \n5646         Returns a new `DataArray` named after the dimension with the values of\n5647         the coordinate labels along that dimension corresponding to minimum\n5648         values along that dimension.\n5649 \n5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n5652 \n5653         Parameters\n5654         ----------\n5655         dim : str, optional\n5656             Dimension over which to apply `idxmin`.  This is optional for 1D\n5657             arrays, but required for arrays with 2 or more dimensions.\n5658         skipna : bool or None, default: None\n5659             If True, skip missing values (as marked by NaN). By default, only\n5660             skips missing values for ``float``, ``complex``, and ``object``\n5661             dtypes; other dtypes either do not have a sentinel missing value\n5662             (``int``) or ``skipna=True`` has not been implemented\n5663             (``datetime64`` or ``timedelta64``).\n5664         fill_value : Any, default: NaN\n5665             Value to be filled in case all of the values along a dimension are\n5666             null.  By default this is NaN.  The fill value and result are\n5667             automatically converted to a compatible dtype if possible.\n5668             Ignored if ``skipna`` is False.\n5669         keep_attrs : bool or None, optional\n5670             If True, the attributes (``attrs``) will be copied from the\n5671             original object to the new one. If False, the new object\n5672             will be returned without attributes.\n5673 \n5674         Returns\n5675         -------\n5676         reduced : DataArray\n5677             New `DataArray` object with `idxmin` applied to its data and the\n5678             indicated dimension removed.\n5679 \n5680         See Also\n5681         --------\n5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n5683 \n5684         Examples\n5685         --------\n5686         >>> array = xr.DataArray(\n5687         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n5688         ... )\n5689         >>> array.min()\n5690         <xarray.DataArray ()>\n5691         array(-2)\n5692         >>> array.argmin(...)\n5693         {'x': <xarray.DataArray ()>\n5694         array(4)}\n5695         >>> array.idxmin()\n5696         <xarray.DataArray 'x' ()>\n5697         array('e', dtype='<U1')\n5698 \n5699         >>> array = xr.DataArray(\n5700         ...     [\n5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n5704         ...     ],\n5705         ...     dims=[\"y\", \"x\"],\n5706         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n5707         ... )\n5708         >>> array.min(dim=\"x\")\n5709         <xarray.DataArray (y: 3)>\n5710         array([-2., -4.,  1.])\n5711         Coordinates:\n5712           * y        (y) int64 -1 0 1\n5713         >>> array.argmin(dim=\"x\")\n5714         <xarray.DataArray (y: 3)>\n5715         array([4, 0, 2])\n5716         Coordinates:\n5717           * y        (y) int64 -1 0 1\n5718         >>> array.idxmin(dim=\"x\")\n5719         <xarray.DataArray 'x' (y: 3)>\n5720         array([16.,  0.,  4.])\n5721         Coordinates:\n5722           * y        (y) int64 -1 0 1\n5723         \"\"\"\n5724         return computation._calc_idxminmax(\n5725             array=self,\n5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n5727             dim=dim,\n5728             skipna=skipna,\n5729             fill_value=fill_value,\n5730             keep_attrs=keep_attrs,\n5731         )\n5732 \n5733     def idxmax(\n5734         self,\n5735         dim: Hashable = None,\n5736         skipna: bool | None = None,\n5737         fill_value: Any = dtypes.NA,\n5738         keep_attrs: bool | None = None,\n5739     ) -> DataArray:\n5740         \"\"\"Return the coordinate label of the maximum value along a dimension.\n5741 \n5742         Returns a new `DataArray` named after the dimension with the values of\n5743         the coordinate labels along that dimension corresponding to maximum\n5744         values along that dimension.\n5745 \n5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n5748 \n5749         Parameters\n5750         ----------\n5751         dim : Hashable, optional\n5752             Dimension over which to apply `idxmax`.  This is optional for 1D\n5753             arrays, but required for arrays with 2 or more dimensions.\n5754         skipna : bool or None, default: None\n5755             If True, skip missing values (as marked by NaN). By default, only\n5756             skips missing values for ``float``, ``complex``, and ``object``\n5757             dtypes; other dtypes either do not have a sentinel missing value\n5758             (``int``) or ``skipna=True`` has not been implemented\n5759             (``datetime64`` or ``timedelta64``).\n5760         fill_value : Any, default: NaN\n5761             Value to be filled in case all of the values along a dimension are\n5762             null.  By default this is NaN.  The fill value and result are\n5763             automatically converted to a compatible dtype if possible.\n5764             Ignored if ``skipna`` is False.\n5765         keep_attrs : bool or None, optional\n5766             If True, the attributes (``attrs``) will be copied from the\n5767             original object to the new one. If False, the new object\n5768             will be returned without attributes.\n5769 \n5770         Returns\n5771         -------\n5772         reduced : DataArray\n5773             New `DataArray` object with `idxmax` applied to its data and the\n5774             indicated dimension removed.\n5775 \n5776         See Also\n5777         --------\n5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n5779 \n5780         Examples\n5781         --------\n5782         >>> array = xr.DataArray(\n5783         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n5784         ... )\n5785         >>> array.max()\n5786         <xarray.DataArray ()>\n5787         array(2)\n5788         >>> array.argmax(...)\n5789         {'x': <xarray.DataArray ()>\n5790         array(1)}\n5791         >>> array.idxmax()\n5792         <xarray.DataArray 'x' ()>\n5793         array('b', dtype='<U1')\n5794 \n5795         >>> array = xr.DataArray(\n5796         ...     [\n5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n5800         ...     ],\n5801         ...     dims=[\"y\", \"x\"],\n5802         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n5803         ... )\n5804         >>> array.max(dim=\"x\")\n5805         <xarray.DataArray (y: 3)>\n5806         array([2., 2., 1.])\n5807         Coordinates:\n5808           * y        (y) int64 -1 0 1\n5809         >>> array.argmax(dim=\"x\")\n5810         <xarray.DataArray (y: 3)>\n5811         array([0, 2, 2])\n5812         Coordinates:\n5813           * y        (y) int64 -1 0 1\n5814         >>> array.idxmax(dim=\"x\")\n5815         <xarray.DataArray 'x' (y: 3)>\n5816         array([0., 4., 4.])\n5817         Coordinates:\n5818           * y        (y) int64 -1 0 1\n5819         \"\"\"\n5820         return computation._calc_idxminmax(\n5821             array=self,\n5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n5823             dim=dim,\n5824             skipna=skipna,\n5825             fill_value=fill_value,\n5826             keep_attrs=keep_attrs,\n5827         )\n5828 \n5829     # change type of self and return to T_DataArray once\n5830     # https://github.com/python/mypy/issues/12846 is resolved\n5831     def argmin(\n5832         self,\n5833         dim: Dims = None,\n5834         axis: int | None = None,\n5835         keep_attrs: bool | None = None,\n5836         skipna: bool | None = None,\n5837     ) -> DataArray | dict[Hashable, DataArray]:\n5838         \"\"\"Index or indices of the minimum of the DataArray over one or more dimensions.\n5839 \n5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n5841         which can be passed directly to isel(). If a single str is passed to 'dim' then\n5842         returns a DataArray with dtype int.\n5843 \n5844         If there are multiple minima, the indices of the first one found will be\n5845         returned.\n5846 \n5847         Parameters\n5848         ----------\n5849         dim : \"...\", str, Iterable of Hashable or None, optional\n5850             The dimensions over which to find the minimum. By default, finds minimum over\n5851             all dimensions - for now returning an int for backward compatibility, but\n5852             this is deprecated, in future will return a dict with indices for all\n5853             dimensions; to return a dict with all dimensions now, pass '...'.\n5854         axis : int or None, optional\n5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n5856             can be supplied.\n5857         keep_attrs : bool or None, optional\n5858             If True, the attributes (`attrs`) will be copied from the original\n5859             object to the new one. If False, the new object will be\n5860             returned without attributes.\n5861         skipna : bool or None, optional\n5862             If True, skip missing values (as marked by NaN). By default, only\n5863             skips missing values for float dtypes; other dtypes either do not\n5864             have a sentinel missing value (int) or skipna=True has not been\n5865             implemented (object, datetime64 or timedelta64).\n5866 \n5867         Returns\n5868         -------\n5869         result : DataArray or dict of DataArray\n5870 \n5871         See Also\n5872         --------\n5873         Variable.argmin, DataArray.idxmin\n5874 \n5875         Examples\n5876         --------\n5877         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n5878         >>> array.min()\n5879         <xarray.DataArray ()>\n5880         array(-1)\n5881         >>> array.argmin(...)\n5882         {'x': <xarray.DataArray ()>\n5883         array(2)}\n5884         >>> array.isel(array.argmin(...))\n5885         <xarray.DataArray ()>\n5886         array(-1)\n5887 \n5888         >>> array = xr.DataArray(\n5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],\n5890         ...     dims=(\"x\", \"y\", \"z\"),\n5891         ... )\n5892         >>> array.min(dim=\"x\")\n5893         <xarray.DataArray (y: 3, z: 3)>\n5894         array([[ 1,  2,  1],\n5895                [ 2, -5,  1],\n5896                [ 2,  1,  1]])\n5897         Dimensions without coordinates: y, z\n5898         >>> array.argmin(dim=\"x\")\n5899         <xarray.DataArray (y: 3, z: 3)>\n5900         array([[1, 0, 0],\n5901                [1, 1, 1],\n5902                [0, 0, 1]])\n5903         Dimensions without coordinates: y, z\n5904         >>> array.argmin(dim=[\"x\"])\n5905         {'x': <xarray.DataArray (y: 3, z: 3)>\n5906         array([[1, 0, 0],\n5907                [1, 1, 1],\n5908                [0, 0, 1]])\n5909         Dimensions without coordinates: y, z}\n5910         >>> array.min(dim=(\"x\", \"z\"))\n5911         <xarray.DataArray (y: 3)>\n5912         array([ 1, -5,  1])\n5913         Dimensions without coordinates: y\n5914         >>> array.argmin(dim=[\"x\", \"z\"])\n5915         {'x': <xarray.DataArray (y: 3)>\n5916         array([0, 1, 0])\n5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n5918         array([2, 1, 1])\n5919         Dimensions without coordinates: y}\n5920         >>> array.isel(array.argmin(dim=[\"x\", \"z\"]))\n5921         <xarray.DataArray (y: 3)>\n5922         array([ 1, -5,  1])\n5923         Dimensions without coordinates: y\n5924         \"\"\"\n5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)\n5926         if isinstance(result, dict):\n5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n5928         else:\n5929             return self._replace_maybe_drop_dims(result)\n5930 \n5931     # change type of self and return to T_DataArray once\n5932     # https://github.com/python/mypy/issues/12846 is resolved\n5933     def argmax(\n5934         self,\n5935         dim: Dims = None,\n5936         axis: int | None = None,\n5937         keep_attrs: bool | None = None,\n5938         skipna: bool | None = None,\n5939     ) -> DataArray | dict[Hashable, DataArray]:\n5940         \"\"\"Index or indices of the maximum of the DataArray over one or more dimensions.\n5941 \n5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n5943         which can be passed directly to isel(). If a single str is passed to 'dim' then\n5944         returns a DataArray with dtype int.\n5945 \n5946         If there are multiple maxima, the indices of the first one found will be\n5947         returned.\n5948 \n5949         Parameters\n5950         ----------\n5951         dim : \"...\", str, Iterable of Hashable or None, optional\n5952             The dimensions over which to find the maximum. By default, finds maximum over\n5953             all dimensions - for now returning an int for backward compatibility, but\n5954             this is deprecated, in future will return a dict with indices for all\n5955             dimensions; to return a dict with all dimensions now, pass '...'.\n5956         axis : int or None, optional\n5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments\n5958             can be supplied.\n5959         keep_attrs : bool or None, optional\n5960             If True, the attributes (`attrs`) will be copied from the original\n5961             object to the new one. If False, the new object will be\n5962             returned without attributes.\n5963         skipna : bool or None, optional\n5964             If True, skip missing values (as marked by NaN). By default, only\n5965             skips missing values for float dtypes; other dtypes either do not\n5966             have a sentinel missing value (int) or skipna=True has not been\n5967             implemented (object, datetime64 or timedelta64).\n5968 \n5969         Returns\n5970         -------\n5971         result : DataArray or dict of DataArray\n5972 \n5973         See Also\n5974         --------\n5975         Variable.argmax, DataArray.idxmax\n5976 \n5977         Examples\n5978         --------\n5979         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n5980         >>> array.max()\n5981         <xarray.DataArray ()>\n5982         array(3)\n5983         >>> array.argmax(...)\n5984         {'x': <xarray.DataArray ()>\n5985         array(3)}\n5986         >>> array.isel(array.argmax(...))\n5987         <xarray.DataArray ()>\n5988         array(3)\n5989 \n5990         >>> array = xr.DataArray(\n5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],\n5992         ...     dims=(\"x\", \"y\", \"z\"),\n5993         ... )\n5994         >>> array.max(dim=\"x\")\n5995         <xarray.DataArray (y: 3, z: 3)>\n5996         array([[3, 3, 2],\n5997                [3, 5, 2],\n5998                [2, 3, 3]])\n5999         Dimensions without coordinates: y, z\n6000         >>> array.argmax(dim=\"x\")\n6001         <xarray.DataArray (y: 3, z: 3)>\n6002         array([[0, 1, 1],\n6003                [0, 1, 0],\n6004                [0, 1, 0]])\n6005         Dimensions without coordinates: y, z\n6006         >>> array.argmax(dim=[\"x\"])\n6007         {'x': <xarray.DataArray (y: 3, z: 3)>\n6008         array([[0, 1, 1],\n6009                [0, 1, 0],\n6010                [0, 1, 0]])\n6011         Dimensions without coordinates: y, z}\n6012         >>> array.max(dim=(\"x\", \"z\"))\n6013         <xarray.DataArray (y: 3)>\n6014         array([3, 5, 3])\n6015         Dimensions without coordinates: y\n6016         >>> array.argmax(dim=[\"x\", \"z\"])\n6017         {'x': <xarray.DataArray (y: 3)>\n6018         array([0, 1, 0])\n6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n6020         array([0, 1, 2])\n6021         Dimensions without coordinates: y}\n6022         >>> array.isel(array.argmax(dim=[\"x\", \"z\"]))\n6023         <xarray.DataArray (y: 3)>\n6024         array([3, 5, 3])\n6025         Dimensions without coordinates: y\n6026         \"\"\"\n6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)\n6028         if isinstance(result, dict):\n6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n6030         else:\n6031             return self._replace_maybe_drop_dims(result)\n6032 \n6033     def query(\n6034         self,\n6035         queries: Mapping[Any, Any] | None = None,\n6036         parser: QueryParserOptions = \"pandas\",\n6037         engine: QueryEngineOptions = None,\n6038         missing_dims: ErrorOptionsWithWarn = \"raise\",\n6039         **queries_kwargs: Any,\n6040     ) -> DataArray:\n6041         \"\"\"Return a new data array indexed along the specified\n6042         dimension(s), where the indexers are given as strings containing\n6043         Python expressions to be evaluated against the values in the array.\n6044 \n6045         Parameters\n6046         ----------\n6047         queries : dict-like or None, optional\n6048             A dict-like with keys matching dimensions and values given by strings\n6049             containing Python expressions to be evaluated against the data variables\n6050             in the dataset. The expressions will be evaluated using the pandas\n6051             eval() function, and can contain any valid Python expressions but cannot\n6052             contain any Python statements.\n6053         parser : {\"pandas\", \"python\"}, default: \"pandas\"\n6054             The parser to use to construct the syntax tree from the expression.\n6055             The default of 'pandas' parses code slightly different than standard\n6056             Python. Alternatively, you can parse an expression using the 'python'\n6057             parser to retain strict Python semantics.\n6058         engine : {\"python\", \"numexpr\", None}, default: None\n6059             The engine used to evaluate the expression. Supported engines are:\n6060 \n6061             - None: tries to use numexpr, falls back to python\n6062             - \"numexpr\": evaluates expressions using numexpr\n6063             - \"python\": performs operations as if you had eval\u2019d in top level python\n6064 \n6065         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n6066             What to do if dimensions that should be selected from are not present in the\n6067             DataArray:\n6068 \n6069             - \"raise\": raise an exception\n6070             - \"warn\": raise a warning, and ignore the missing dimensions\n6071             - \"ignore\": ignore the missing dimensions\n6072 \n6073         **queries_kwargs : {dim: query, ...}, optional\n6074             The keyword arguments form of ``queries``.\n6075             One of queries or queries_kwargs must be provided.\n6076 \n6077         Returns\n6078         -------\n6079         obj : DataArray\n6080             A new DataArray with the same contents as this dataset, indexed by\n6081             the results of the appropriate queries.\n6082 \n6083         See Also\n6084         --------\n6085         DataArray.isel\n6086         Dataset.query\n6087         pandas.eval\n6088 \n6089         Examples\n6090         --------\n6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims=\"x\", name=\"a\")\n6092         >>> da\n6093         <xarray.DataArray 'a' (x: 5)>\n6094         array([0, 1, 2, 3, 4])\n6095         Dimensions without coordinates: x\n6096         >>> da.query(x=\"a > 2\")\n6097         <xarray.DataArray 'a' (x: 2)>\n6098         array([3, 4])\n6099         Dimensions without coordinates: x\n6100         \"\"\"\n6101 \n6102         ds = self._to_dataset_whole(shallow_copy=True)\n6103         ds = ds.query(\n6104             queries=queries,\n6105             parser=parser,\n6106             engine=engine,\n6107             missing_dims=missing_dims,\n6108             **queries_kwargs,\n6109         )\n6110         return ds[self.name]\n6111 \n6112     def curvefit(\n6113         self,\n6114         coords: str | DataArray | Iterable[str | DataArray],\n6115         func: Callable[..., Any],\n6116         reduce_dims: Dims = None,\n6117         skipna: bool = True,\n6118         p0: dict[str, Any] | None = None,\n6119         bounds: dict[str, Any] | None = None,\n6120         param_names: Sequence[str] | None = None,\n6121         kwargs: dict[str, Any] | None = None,\n6122     ) -> Dataset:\n6123         \"\"\"\n6124         Curve fitting optimization for arbitrary functions.\n6125 \n6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n6127 \n6128         Parameters\n6129         ----------\n6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable\n6131             Independent coordinate(s) over which to perform the curve fitting. Must share\n6132             at least one dimension with the calling object. When fitting multi-dimensional\n6133             functions, supply `coords` as a sequence in the same order as arguments in\n6134             `func`. To fit along existing dimensions of the calling object, `coords` can\n6135             also be specified as a str or sequence of strs.\n6136         func : callable\n6137             User specified function in the form `f(x, *params)` which returns a numpy\n6138             array of length `len(x)`. `params` are the fittable parameters which are optimized\n6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n6140             coordinates, e.g. `f((x0, x1), *params)`.\n6141         reduce_dims : str, Iterable of Hashable or None, optional\n6142             Additional dimension(s) over which to aggregate while fitting. For example,\n6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n6144             aggregate all lat and lon points and fit the specified function along the\n6145             time dimension.\n6146         skipna : bool, default: True\n6147             Whether to skip missing values when fitting. Default is True.\n6148         p0 : dict-like or None, optional\n6149             Optional dictionary of parameter names to initial guesses passed to the\n6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n6151             be assigned initial values following the default scipy behavior.\n6152         bounds : dict-like or None, optional\n6153             Optional dictionary of parameter names to bounding values passed to the\n6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n6155             will be unbounded following the default scipy behavior.\n6156         param_names : sequence of Hashable or None, optional\n6157             Sequence of names for the fittable parameters of `func`. If not supplied,\n6158             this will be automatically determined by arguments of `func`. `param_names`\n6159             should be manually supplied when fitting a function that takes a variable\n6160             number of parameters.\n6161         **kwargs : optional\n6162             Additional keyword arguments to passed to scipy curve_fit.\n6163 \n6164         Returns\n6165         -------\n6166         curvefit_results : Dataset\n6167             A single dataset which contains:\n6168 \n6169             [var]_curvefit_coefficients\n6170                 The coefficients of the best fit.\n6171             [var]_curvefit_covariance\n6172                 The covariance matrix of the coefficient estimates.\n6173 \n6174         See Also\n6175         --------\n6176         DataArray.polyfit\n6177         scipy.optimize.curve_fit\n6178         \"\"\"\n6179         return self._to_temp_dataset().curvefit(\n6180             coords,\n6181             func,\n6182             reduce_dims=reduce_dims,\n6183             skipna=skipna,\n6184             p0=p0,\n6185             bounds=bounds,\n6186             param_names=param_names,\n6187             kwargs=kwargs,\n6188         )\n6189 \n6190     def drop_duplicates(\n6191         self: T_DataArray,\n6192         dim: Hashable | Iterable[Hashable],\n6193         keep: Literal[\"first\", \"last\", False] = \"first\",\n6194     ) -> T_DataArray:\n6195         \"\"\"Returns a new DataArray with duplicate dimension values removed.\n6196 \n6197         Parameters\n6198         ----------\n6199         dim : dimension label or labels\n6200             Pass `...` to drop duplicates along all dimensions.\n6201         keep : {\"first\", \"last\", False}, default: \"first\"\n6202             Determines which duplicates (if any) to keep.\n6203 \n6204             - ``\"first\"`` : Drop duplicates except for the first occurrence.\n6205             - ``\"last\"`` : Drop duplicates except for the last occurrence.\n6206             - False : Drop all duplicates.\n6207 \n6208         Returns\n6209         -------\n6210         DataArray\n6211 \n6212         See Also\n6213         --------\n6214         Dataset.drop_duplicates\n6215 \n6216         Examples\n6217         --------\n6218         >>> da = xr.DataArray(\n6219         ...     np.arange(25).reshape(5, 5),\n6220         ...     dims=(\"x\", \"y\"),\n6221         ...     coords={\"x\": np.array([0, 0, 1, 2, 3]), \"y\": np.array([0, 1, 2, 3, 3])},\n6222         ... )\n6223         >>> da\n6224         <xarray.DataArray (x: 5, y: 5)>\n6225         array([[ 0,  1,  2,  3,  4],\n6226                [ 5,  6,  7,  8,  9],\n6227                [10, 11, 12, 13, 14],\n6228                [15, 16, 17, 18, 19],\n6229                [20, 21, 22, 23, 24]])\n6230         Coordinates:\n6231           * x        (x) int64 0 0 1 2 3\n6232           * y        (y) int64 0 1 2 3 3\n6233 \n6234         >>> da.drop_duplicates(dim=\"x\")\n6235         <xarray.DataArray (x: 4, y: 5)>\n6236         array([[ 0,  1,  2,  3,  4],\n6237                [10, 11, 12, 13, 14],\n6238                [15, 16, 17, 18, 19],\n6239                [20, 21, 22, 23, 24]])\n6240         Coordinates:\n6241           * x        (x) int64 0 1 2 3\n6242           * y        (y) int64 0 1 2 3 3\n6243 \n6244         >>> da.drop_duplicates(dim=\"x\", keep=\"last\")\n6245         <xarray.DataArray (x: 4, y: 5)>\n6246         array([[ 5,  6,  7,  8,  9],\n6247                [10, 11, 12, 13, 14],\n6248                [15, 16, 17, 18, 19],\n6249                [20, 21, 22, 23, 24]])\n6250         Coordinates:\n6251           * x        (x) int64 0 1 2 3\n6252           * y        (y) int64 0 1 2 3 3\n6253 \n6254         Drop all duplicate dimension values:\n6255 \n6256         >>> da.drop_duplicates(dim=...)\n6257         <xarray.DataArray (x: 4, y: 4)>\n6258         array([[ 0,  1,  2,  3],\n6259                [10, 11, 12, 13],\n6260                [15, 16, 17, 18],\n6261                [20, 21, 22, 23]])\n6262         Coordinates:\n6263           * x        (x) int64 0 1 2 3\n6264           * y        (y) int64 0 1 2 3\n6265         \"\"\"\n6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)\n6267         return self._from_temp_dataset(deduplicated)\n6268 \n6269     def convert_calendar(\n6270         self,\n6271         calendar: str,\n6272         dim: str = \"time\",\n6273         align_on: str | None = None,\n6274         missing: Any | None = None,\n6275         use_cftime: bool | None = None,\n6276     ) -> DataArray:\n6277         \"\"\"Convert the DataArray to another calendar.\n6278 \n6279         Only converts the individual timestamps, does not modify any data except\n6280         in dropping invalid/surplus dates or inserting missing dates.\n6281 \n6282         If the source and target calendars are either no_leap, all_leap or a\n6283         standard type, only the type of the time array is modified.\n6284         When converting to a leap year from a non-leap year, the 29th of February\n6285         is removed from the array. In the other direction the 29th of February\n6286         will be missing in the output, unless `missing` is specified,\n6287         in which case that value is inserted.\n6288 \n6289         For conversions involving `360_day` calendars, see Notes.\n6290 \n6291         This method is safe to use with sub-daily data as it doesn't touch the\n6292         time part of the timestamps.\n6293 \n6294         Parameters\n6295         ---------\n6296         calendar : str\n6297             The target calendar name.\n6298         dim : str\n6299             Name of the time coordinate.\n6300         align_on : {None, 'date', 'year'}\n6301             Must be specified when either source or target is a `360_day` calendar,\n6302            ignored otherwise. See Notes.\n6303         missing : Optional[any]\n6304             By default, i.e. if the value is None, this method will simply attempt\n6305             to convert the dates in the source calendar to the same dates in the\n6306             target calendar, and drop any of those that are not possible to\n6307             represent.  If a value is provided, a new time coordinate will be\n6308             created in the target calendar with the same frequency as the original\n6309             time coordinate; for any dates that are not present in the source, the\n6310             data will be filled with this value.  Note that using this mode requires\n6311             that the source data have an inferable frequency; for more information\n6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n6313             target calendar combinations, this could result in many missing values, see notes.\n6314         use_cftime : boolean, optional\n6315             Whether to use cftime objects in the output, only used if `calendar`\n6316             is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n6317             If True, the new time axis uses cftime objects.\n6318             If None (default), it uses :py:class:`numpy.datetime64` values if the\n6319             date range permits it, and :py:class:`cftime.datetime` objects if not.\n6320             If False, it uses :py:class:`numpy.datetime64`  or fails.\n6321 \n6322         Returns\n6323         -------\n6324         DataArray\n6325             Copy of the dataarray with the time coordinate converted to the\n6326             target calendar. If 'missing' was None (default), invalid dates in\n6327             the new calendar are dropped, but missing dates are not inserted.\n6328             If `missing` was given, the new data is reindexed to have a time axis\n6329             with the same frequency as the source, but in the new calendar; any\n6330             missing datapoints are filled with `missing`.\n6331 \n6332         Notes\n6333         -----\n6334         Passing a value to `missing` is only usable if the source's time coordinate as an\n6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n6336         if the target coordinate, generated from this frequency, has dates equivalent to the\n6337         source. It is usually **not** appropriate to use this mode with:\n6338 \n6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n6341             or 'mH' where 24 % m != 0).\n6342 \n6343         If one of the source or target calendars is `\"360_day\"`, `align_on` must\n6344         be specified and two options are offered.\n6345 \n6346         - \"year\"\n6347             The dates are translated according to their relative position in the year,\n6348             ignoring their original month and day information, meaning that the\n6349             missing/surplus days are added/removed at regular intervals.\n6350 \n6351             From a `360_day` to a standard calendar, the output will be missing the\n6352             following dates (day of year in parentheses):\n6353 \n6354             To a leap year:\n6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n6356                 September 31st (275) and November 30th (335).\n6357             To a non-leap year:\n6358                 February 6th (36), April 19th (109), July 2nd (183),\n6359                 September 12th (255), November 25th (329).\n6360 \n6361             From a standard calendar to a `\"360_day\"`, the following dates in the\n6362             source array will be dropped:\n6363 \n6364             From a leap year:\n6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n6366                 September 31st (275), December 1st (336)\n6367             From a non-leap year:\n6368                 February 6th (37), April 20th (110), July 2nd (183),\n6369                 September 13th (256), November 25th (329)\n6370 \n6371             This option is best used on daily and subdaily data.\n6372 \n6373         - \"date\"\n6374             The month/day information is conserved and invalid dates are dropped\n6375             from the output. This means that when converting from a `\"360_day\"` to a\n6376             standard calendar, all 31st (Jan, March, May, July, August, October and\n6377             December) will be missing as there is no equivalent dates in the\n6378             `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n6379             will be dropped as there are no equivalent dates in a standard calendar.\n6380 \n6381             This option is best used with data on a frequency coarser than daily.\n6382         \"\"\"\n6383         return convert_calendar(\n6384             self,\n6385             calendar,\n6386             dim=dim,\n6387             align_on=align_on,\n6388             missing=missing,\n6389             use_cftime=use_cftime,\n6390         )\n6391 \n6392     def interp_calendar(\n6393         self,\n6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,\n6395         dim: str = \"time\",\n6396     ) -> DataArray:\n6397         \"\"\"Interpolates the DataArray to another calendar based on decimal year measure.\n6398 \n6399         Each timestamp in `source` and `target` are first converted to their decimal\n6400         year equivalent then `source` is interpolated on the target coordinate.\n6401         The decimal year of a timestamp is its year plus its sub-year component\n6402         converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n6403         2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n6404 \n6405         This method should only be used when the time (HH:MM:SS) information of\n6406         time coordinate is not important.\n6407 \n6408         Parameters\n6409         ----------\n6410         target: DataArray or DatetimeIndex or CFTimeIndex\n6411             The target time coordinate of a valid dtype\n6412             (np.datetime64 or cftime objects)\n6413         dim : str\n6414             The time coordinate name.\n6415 \n6416         Return\n6417         ------\n6418         DataArray\n6419             The source interpolated on the decimal years of target,\n6420         \"\"\"\n6421         return interp_calendar(self, target, dim=dim)\n6422 \n6423     def groupby(\n6424         self,\n6425         group: Hashable | DataArray | IndexVariable,\n6426         squeeze: bool = True,\n6427         restore_coord_dims: bool = False,\n6428     ) -> DataArrayGroupBy:\n6429         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n6430 \n6431         Parameters\n6432         ----------\n6433         group : Hashable, DataArray or IndexVariable\n6434             Array whose unique values should be used to group this array. If a\n6435             Hashable, must be the name of a coordinate contained in this dataarray.\n6436         squeeze : bool, default: True\n6437             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n6438             controls whether the subarrays have a dimension of length 1 along\n6439             that dimension or if the dimension is squeezed out.\n6440         restore_coord_dims : bool, default: False\n6441             If True, also restore the dimension order of multi-dimensional\n6442             coordinates.\n6443 \n6444         Returns\n6445         -------\n6446         grouped : DataArrayGroupBy\n6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n6448             iterated over in the form of `(unique_value, grouped_array)` pairs.\n6449 \n6450         Examples\n6451         --------\n6452         Calculate daily anomalies for daily data:\n6453 \n6454         >>> da = xr.DataArray(\n6455         ...     np.linspace(0, 1826, num=1827),\n6456         ...     coords=[pd.date_range(\"2000-01-01\", \"2004-12-31\", freq=\"D\")],\n6457         ...     dims=\"time\",\n6458         ... )\n6459         >>> da\n6460         <xarray.DataArray (time: 1827)>\n6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,\n6462                1.826e+03])\n6463         Coordinates:\n6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n6465         >>> da.groupby(\"time.dayofyear\") - da.groupby(\"time.dayofyear\").mean(\"time\")\n6466         <xarray.DataArray (time: 1827)>\n6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n6468         Coordinates:\n6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366\n6471 \n6472         See Also\n6473         --------\n6474         :ref:`groupby`\n6475             Users guide explanation of how to group and bin data.\n6476         DataArray.groupby_bins\n6477         Dataset.groupby\n6478         core.groupby.DataArrayGroupBy\n6479         pandas.DataFrame.groupby\n6480         \"\"\"\n6481         from xarray.core.groupby import (\n6482             DataArrayGroupBy,\n6483             ResolvedUniqueGrouper,\n6484             UniqueGrouper,\n6485             _validate_groupby_squeeze,\n6486         )\n6487 \n6488         _validate_groupby_squeeze(squeeze)\n6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)\n6490         return DataArrayGroupBy(\n6491             self,\n6492             (rgrouper,),\n6493             squeeze=squeeze,\n6494             restore_coord_dims=restore_coord_dims,\n6495         )\n6496 \n6497     def groupby_bins(\n6498         self,\n6499         group: Hashable | DataArray | IndexVariable,\n6500         bins: ArrayLike,\n6501         right: bool = True,\n6502         labels: ArrayLike | Literal[False] | None = None,\n6503         precision: int = 3,\n6504         include_lowest: bool = False,\n6505         squeeze: bool = True,\n6506         restore_coord_dims: bool = False,\n6507     ) -> DataArrayGroupBy:\n6508         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n6509 \n6510         Rather than using all unique values of `group`, the values are discretized\n6511         first by applying `pandas.cut` [1]_ to `group`.\n6512 \n6513         Parameters\n6514         ----------\n6515         group : Hashable, DataArray or IndexVariable\n6516             Array whose binned values should be used to group this array. If a\n6517             Hashable, must be the name of a coordinate contained in this dataarray.\n6518         bins : int or array-like\n6519             If bins is an int, it defines the number of equal-width bins in the\n6520             range of x. However, in this case, the range of x is extended by .1%\n6521             on each side to include the min or max values of x. If bins is a\n6522             sequence it defines the bin edges allowing for non-uniform bin\n6523             width. No extension of the range of x is done in this case.\n6524         right : bool, default: True\n6525             Indicates whether the bins include the rightmost edge or not. If\n6526             right == True (the default), then the bins [1,2,3,4] indicate\n6527             (1,2], (2,3], (3,4].\n6528         labels : array-like, False or None, default: None\n6529             Used as labels for the resulting bins. Must be of the same length as\n6530             the resulting bins. If False, string bin labels are assigned by\n6531             `pandas.cut`.\n6532         precision : int, default: 3\n6533             The precision at which to store and display the bins labels.\n6534         include_lowest : bool, default: False\n6535             Whether the first interval should be left-inclusive or not.\n6536         squeeze : bool, default: True\n6537             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n6538             controls whether the subarrays have a dimension of length 1 along\n6539             that dimension or if the dimension is squeezed out.\n6540         restore_coord_dims : bool, default: False\n6541             If True, also restore the dimension order of multi-dimensional\n6542             coordinates.\n6543 \n6544         Returns\n6545         -------\n6546         grouped : DataArrayGroupBy\n6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n6548             iterated over in the form of `(unique_value, grouped_array)` pairs.\n6549             The name of the group has the added suffix `_bins` in order to\n6550             distinguish it from the original variable.\n6551 \n6552         See Also\n6553         --------\n6554         :ref:`groupby`\n6555             Users guide explanation of how to group and bin data.\n6556         DataArray.groupby\n6557         Dataset.groupby_bins\n6558         core.groupby.DataArrayGroupBy\n6559         pandas.DataFrame.groupby\n6560 \n6561         References\n6562         ----------\n6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n6564         \"\"\"\n6565         from xarray.core.groupby import (\n6566             BinGrouper,\n6567             DataArrayGroupBy,\n6568             ResolvedBinGrouper,\n6569             _validate_groupby_squeeze,\n6570         )\n6571 \n6572         _validate_groupby_squeeze(squeeze)\n6573         grouper = BinGrouper(\n6574             bins=bins,\n6575             cut_kwargs={\n6576                 \"right\": right,\n6577                 \"labels\": labels,\n6578                 \"precision\": precision,\n6579                 \"include_lowest\": include_lowest,\n6580             },\n6581         )\n6582         rgrouper = ResolvedBinGrouper(grouper, group, self)\n6583 \n6584         return DataArrayGroupBy(\n6585             self,\n6586             (rgrouper,),\n6587             squeeze=squeeze,\n6588             restore_coord_dims=restore_coord_dims,\n6589         )\n6590 \n6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:\n6592         \"\"\"\n6593         Weighted DataArray operations.\n6594 \n6595         Parameters\n6596         ----------\n6597         weights : DataArray\n6598             An array of weights associated with the values in this Dataset.\n6599             Each value in the data contributes to the reduction operation\n6600             according to its associated weight.\n6601 \n6602         Notes\n6603         -----\n6604         ``weights`` must be a DataArray and cannot contain missing values.\n6605         Missing values can be replaced by ``weights.fillna(0)``.\n6606 \n6607         Returns\n6608         -------\n6609         core.weighted.DataArrayWeighted\n6610 \n6611         See Also\n6612         --------\n6613         Dataset.weighted\n6614         \"\"\"\n6615         from xarray.core.weighted import DataArrayWeighted\n6616 \n6617         return DataArrayWeighted(self, weights)\n6618 \n6619     def rolling(\n6620         self,\n6621         dim: Mapping[Any, int] | None = None,\n6622         min_periods: int | None = None,\n6623         center: bool | Mapping[Any, bool] = False,\n6624         **window_kwargs: int,\n6625     ) -> DataArrayRolling:\n6626         \"\"\"\n6627         Rolling window object for DataArrays.\n6628 \n6629         Parameters\n6630         ----------\n6631         dim : dict, optional\n6632             Mapping from the dimension name to create the rolling iterator\n6633             along (e.g. `time`) to its moving window size.\n6634         min_periods : int or None, default: None\n6635             Minimum number of observations in window required to have a value\n6636             (otherwise result is NA). The default, None, is equivalent to\n6637             setting min_periods equal to the size of the window.\n6638         center : bool or Mapping to int, default: False\n6639             Set the labels at the center of the window.\n6640         **window_kwargs : optional\n6641             The keyword arguments form of ``dim``.\n6642             One of dim or window_kwargs must be provided.\n6643 \n6644         Returns\n6645         -------\n6646         core.rolling.DataArrayRolling\n6647 \n6648         Examples\n6649         --------\n6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n6651 \n6652         >>> da = xr.DataArray(\n6653         ...     np.linspace(0, 11, num=12),\n6654         ...     coords=[\n6655         ...         pd.date_range(\n6656         ...             \"1999-12-15\",\n6657         ...             periods=12,\n6658         ...             freq=pd.DateOffset(months=1),\n6659         ...         )\n6660         ...     ],\n6661         ...     dims=\"time\",\n6662         ... )\n6663         >>> da\n6664         <xarray.DataArray (time: 12)>\n6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n6666         Coordinates:\n6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n6668         >>> da.rolling(time=3, center=True).mean()\n6669         <xarray.DataArray (time: 12)>\n6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n6671         Coordinates:\n6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n6673 \n6674         Remove the NaNs using ``dropna()``:\n6675 \n6676         >>> da.rolling(time=3, center=True).mean().dropna(\"time\")\n6677         <xarray.DataArray (time: 10)>\n6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n6679         Coordinates:\n6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15\n6681 \n6682         See Also\n6683         --------\n6684         core.rolling.DataArrayRolling\n6685         Dataset.rolling\n6686         \"\"\"\n6687         from xarray.core.rolling import DataArrayRolling\n6688 \n6689         dim = either_dict_or_kwargs(dim, window_kwargs, \"rolling\")\n6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)\n6691 \n6692     def coarsen(\n6693         self,\n6694         dim: Mapping[Any, int] | None = None,\n6695         boundary: CoarsenBoundaryOptions = \"exact\",\n6696         side: SideOptions | Mapping[Any, SideOptions] = \"left\",\n6697         coord_func: str | Callable | Mapping[Any, str | Callable] = \"mean\",\n6698         **window_kwargs: int,\n6699     ) -> DataArrayCoarsen:\n6700         \"\"\"\n6701         Coarsen object for DataArrays.\n6702 \n6703         Parameters\n6704         ----------\n6705         dim : mapping of hashable to int, optional\n6706             Mapping from the dimension name to the window size.\n6707         boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n6708             If 'exact', a ValueError will be raised if dimension size is not a\n6709             multiple of the window size. If 'trim', the excess entries are\n6710             dropped. If 'pad', NA will be padded.\n6711         side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n6712         coord_func : str or mapping of hashable to str, default: \"mean\"\n6713             function (name) that is applied to the coordinates,\n6714             or a mapping from coordinate name to function (name).\n6715 \n6716         Returns\n6717         -------\n6718         core.rolling.DataArrayCoarsen\n6719 \n6720         Examples\n6721         --------\n6722         Coarsen the long time series by averaging over every three days.\n6723 \n6724         >>> da = xr.DataArray(\n6725         ...     np.linspace(0, 364, num=364),\n6726         ...     dims=\"time\",\n6727         ...     coords={\"time\": pd.date_range(\"1999-12-15\", periods=364)},\n6728         ... )\n6729         >>> da  # +doctest: ELLIPSIS\n6730         <xarray.DataArray (time: 364)>\n6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,\n6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,\n6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,\n6734         ...\n6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,\n6736                360.99173554, 361.99449036, 362.99724518, 364.        ])\n6737         Coordinates:\n6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n6739         >>> da.coarsen(time=3, boundary=\"trim\").mean()  # +doctest: ELLIPSIS\n6740         <xarray.DataArray (time: 121)>\n6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,\n6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,\n6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,\n6744         ...\n6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,\n6746                361.99449036])\n6747         Coordinates:\n6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n6749         >>>\n6750 \n6751         See Also\n6752         --------\n6753         core.rolling.DataArrayCoarsen\n6754         Dataset.coarsen\n6755         \"\"\"\n6756         from xarray.core.rolling import DataArrayCoarsen\n6757 \n6758         dim = either_dict_or_kwargs(dim, window_kwargs, \"coarsen\")\n6759         return DataArrayCoarsen(\n6760             self,\n6761             dim,\n6762             boundary=boundary,\n6763             side=side,\n6764             coord_func=coord_func,\n6765         )\n6766 \n6767     def resample(\n6768         self,\n6769         indexer: Mapping[Any, str] | None = None,\n6770         skipna: bool | None = None,\n6771         closed: SideOptions | None = None,\n6772         label: SideOptions | None = None,\n6773         base: int | None = None,\n6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,\n6775         origin: str | DatetimeLike = \"start_day\",\n6776         keep_attrs: bool | None = None,\n6777         loffset: datetime.timedelta | str | None = None,\n6778         restore_coord_dims: bool | None = None,\n6779         **indexer_kwargs: str,\n6780     ) -> DataArrayResample:\n6781         \"\"\"Returns a Resample object for performing resampling operations.\n6782 \n6783         Handles both downsampling and upsampling. The resampled\n6784         dimension must be a datetime-like coordinate. If any intervals\n6785         contain no values from the original object, they will be given\n6786         the value ``NaN``.\n6787 \n6788         Parameters\n6789         ----------\n6790         indexer : Mapping of Hashable to str, optional\n6791             Mapping from the dimension name to resample frequency [1]_. The\n6792             dimension must be datetime-like.\n6793         skipna : bool, optional\n6794             Whether to skip missing values when aggregating in downsampling.\n6795         closed : {\"left\", \"right\"}, optional\n6796             Side of each interval to treat as closed.\n6797         label : {\"left\", \"right\"}, optional\n6798             Side of each interval to use for labeling.\n6799         base : int, optional\n6800             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n6801             aggregated intervals. For example, for \"24H\" frequency, base could\n6802             range from 0 through 23.\n6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'\n6804             The datetime on which to adjust the grouping. The timezone of origin\n6805             must match the timezone of the index.\n6806 \n6807             If a datetime is not used, these values are also supported:\n6808             - 'epoch': `origin` is 1970-01-01\n6809             - 'start': `origin` is the first value of the timeseries\n6810             - 'start_day': `origin` is the first day at midnight of the timeseries\n6811             - 'end': `origin` is the last value of the timeseries\n6812             - 'end_day': `origin` is the ceiling midnight of the last day\n6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None\n6814             An offset timedelta added to the origin.\n6815         loffset : timedelta or str, optional\n6816             Offset used to adjust the resampled time labels. Some pandas date\n6817             offset strings are supported.\n6818         restore_coord_dims : bool, optional\n6819             If True, also restore the dimension order of multi-dimensional\n6820             coordinates.\n6821         **indexer_kwargs : str\n6822             The keyword arguments form of ``indexer``.\n6823             One of indexer or indexer_kwargs must be provided.\n6824 \n6825         Returns\n6826         -------\n6827         resampled : core.resample.DataArrayResample\n6828             This object resampled.\n6829 \n6830         Examples\n6831         --------\n6832         Downsample monthly time-series data to seasonal data:\n6833 \n6834         >>> da = xr.DataArray(\n6835         ...     np.linspace(0, 11, num=12),\n6836         ...     coords=[\n6837         ...         pd.date_range(\n6838         ...             \"1999-12-15\",\n6839         ...             periods=12,\n6840         ...             freq=pd.DateOffset(months=1),\n6841         ...         )\n6842         ...     ],\n6843         ...     dims=\"time\",\n6844         ... )\n6845         >>> da\n6846         <xarray.DataArray (time: 12)>\n6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n6848         Coordinates:\n6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n6850         >>> da.resample(time=\"QS-DEC\").mean()\n6851         <xarray.DataArray (time: 4)>\n6852         array([ 1.,  4.,  7., 10.])\n6853         Coordinates:\n6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n6855 \n6856         Upsample monthly time-series data to daily data:\n6857 \n6858         >>> da.resample(time=\"1D\").interpolate(\"linear\")  # +doctest: ELLIPSIS\n6859         <xarray.DataArray (time: 337)>\n6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,\n6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,\n6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,\n6863         ...\n6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,\n6865                10.96774194, 11.        ])\n6866         Coordinates:\n6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n6868 \n6869         Limit scope of upsampling method\n6870 \n6871         >>> da.resample(time=\"1D\").nearest(tolerance=\"1D\")\n6872         <xarray.DataArray (time: 337)>\n6873         array([ 0.,  0., nan, ..., nan, 11., 11.])\n6874         Coordinates:\n6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n6876 \n6877         See Also\n6878         --------\n6879         Dataset.resample\n6880         pandas.Series.resample\n6881         pandas.DataFrame.resample\n6882 \n6883         References\n6884         ----------\n6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n6886         \"\"\"\n6887         from xarray.core.resample import DataArrayResample\n6888 \n6889         return self._resample(\n6890             resample_cls=DataArrayResample,\n6891             indexer=indexer,\n6892             skipna=skipna,\n6893             closed=closed,\n6894             label=label,\n6895             base=base,\n6896             offset=offset,\n6897             origin=origin,\n6898             keep_attrs=keep_attrs,\n6899             loffset=loffset,\n6900             restore_coord_dims=restore_coord_dims,\n6901             **indexer_kwargs,\n6902         )\n6903 \n6904     def to_dask_dataframe(\n6905         self,\n6906         dim_order: Sequence[Hashable] | None = None,\n6907         set_index: bool = False,\n6908     ) -> DaskDataFrame:\n6909         \"\"\"Convert this array into a dask.dataframe.DataFrame.\n6910 \n6911         Parameters\n6912         ----------\n6913         dim_order : Sequence of Hashable or None , optional\n6914             Hierarchical dimension order for the resulting dataframe.\n6915             Array content is transposed to this order and then written out as flat\n6916             vectors in contiguous order, so the last dimension in this list\n6917             will be contiguous in the resulting DataFrame. This has a major influence\n6918             on which operations are efficient on the resulting dask dataframe.\n6919         set_index : bool, default: False\n6920             If set_index=True, the dask DataFrame is indexed by this dataset's\n6921             coordinate. Since dask DataFrames do not support multi-indexes,\n6922             set_index only works if the dataset only contains one dimension.\n6923 \n6924         Returns\n6925         -------\n6926         dask.dataframe.DataFrame\n6927 \n6928         Examples\n6929         --------\n6930         >>> da = xr.DataArray(\n6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),\n6932         ...     dims=(\"time\", \"lat\", \"lon\"),\n6933         ...     coords={\n6934         ...         \"time\": np.arange(4),\n6935         ...         \"lat\": [-30, -20],\n6936         ...         \"lon\": [120, 130],\n6937         ...     },\n6938         ...     name=\"eg_dataarray\",\n6939         ...     attrs={\"units\": \"Celsius\", \"description\": \"Random temperature data\"},\n6940         ... )\n6941         >>> da.to_dask_dataframe([\"lat\", \"lon\", \"time\"]).compute()\n6942             lat  lon  time  eg_dataarray\n6943         0   -30  120     0             0\n6944         1   -30  120     1             4\n6945         2   -30  120     2             8\n6946         3   -30  120     3            12\n6947         4   -30  130     0             1\n6948         5   -30  130     1             5\n6949         6   -30  130     2             9\n6950         7   -30  130     3            13\n6951         8   -20  120     0             2\n6952         9   -20  120     1             6\n6953         10  -20  120     2            10\n6954         11  -20  120     3            14\n6955         12  -20  130     0             3\n6956         13  -20  130     1             7\n6957         14  -20  130     2            11\n6958         15  -20  130     3            15\n6959         \"\"\"\n6960         if self.name is None:\n6961             raise ValueError(\n6962                 \"Cannot convert an unnamed DataArray to a \"\n6963                 \"dask dataframe : use the ``.rename`` method to assign a name.\"\n6964             )\n6965         name = self.name\n6966         ds = self._to_dataset_whole(name, shallow_copy=False)\n6967         return ds.to_dask_dataframe(dim_order, set_index)\n6968 \n6969     # this needs to be at the end, or mypy will confuse with `str`\n6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n6971     str = utils.UncachedAccessor(StringAccessor[\"DataArray\"])\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataset.py",
        "start": 2199,
        "end": 2273,
        "class_name": "Dataset",
        "method_name": "chunk",
        "code": "2199     def chunk(\n2200         self: T_Dataset,\n2201         chunks: (\n2202             int | Literal[\"auto\"] | Mapping[Any, None | int | str | tuple[int, ...]]\n2203         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n2204         name_prefix: str = \"xarray-\",\n2205         token: str | None = None,\n2206         lock: bool = False,\n2207         inline_array: bool = False,\n2208         **chunks_kwargs: None | int | str | tuple[int, ...],\n2209     ) -> T_Dataset:\n2210         \"\"\"Coerce all arrays in this dataset into dask arrays with the given\n2211         chunks.\n2212 \n2213         Non-dask arrays in this dataset will be converted to dask arrays. Dask\n2214         arrays will be rechunked to the given chunk sizes.\n2215 \n2216         If neither chunks is not provided for one or more dimensions, chunk\n2217         sizes along that dimension will not be updated; non-dask arrays will be\n2218         converted into dask arrays with a single block.\n2219 \n2220         Parameters\n2221         ----------\n2222         chunks : int, tuple of int, \"auto\" or mapping of hashable to int, optional\n2223             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, or\n2224             ``{\"x\": 5, \"y\": 5}``.\n2225         name_prefix : str, default: \"xarray-\"\n2226             Prefix for the name of any new dask arrays.\n2227         token : str, optional\n2228             Token uniquely identifying this dataset.\n2229         lock : bool, default: False\n2230             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2231             already as dask array.\n2232         inline_array: bool, default: False\n2233             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2234             already as dask array.\n2235         **chunks_kwargs : {dim: chunks, ...}, optional\n2236             The keyword arguments form of ``chunks``.\n2237             One of chunks or chunks_kwargs must be provided\n2238 \n2239         Returns\n2240         -------\n2241         chunked : xarray.Dataset\n2242 \n2243         See Also\n2244         --------\n2245         Dataset.chunks\n2246         Dataset.chunksizes\n2247         xarray.unify_chunks\n2248         dask.array.from_array\n2249         \"\"\"\n2250         if chunks is None and chunks_kwargs is None:\n2251             warnings.warn(\n2252                 \"None value for 'chunks' is deprecated. \"\n2253                 \"It will raise an error in the future. Use instead '{}'\",\n2254                 category=FutureWarning,\n2255             )\n2256             chunks = {}\n2257 \n2258         if isinstance(chunks, (Number, str, int)):\n2259             chunks = dict.fromkeys(self.dims, chunks)\n2260         else:\n2261             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n2262 \n2263         bad_dims = chunks.keys() - self.dims.keys()\n2264         if bad_dims:\n2265             raise ValueError(\n2266                 f\"some chunks keys are not dimensions on this object: {bad_dims}\"\n2267             )\n2268 \n2269         variables = {\n2270             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)\n2271             for k, v in self.variables.items()\n2272         }\n2273         return self._replace(variables)\n",
        "intended_behavior": "Similar to the `DataArray` class, the `chunk` method in the `Dataset` class should be refactored to support an abstracted way of chunking that is agnostic to the parallel computation backend used. This involves adding logic to handle different backends and determining the chunking strategy accordingly."
    },
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataset.py",
        "start": 439,
        "end": 9273,
        "class_name": "Dataset",
        "method_name": null,
        "code": "439 class Dataset(\n440     DataWithCoords,\n441     DatasetAggregations,\n442     DatasetArithmetic,\n443     Mapping[Hashable, \"DataArray\"],\n444 ):\n445     \"\"\"A multi-dimensional, in memory, array database.\n446 \n447     A dataset resembles an in-memory representation of a NetCDF file,\n448     and consists of variables, coordinates and attributes which\n449     together form a self describing dataset.\n450 \n451     Dataset implements the mapping interface with keys given by variable\n452     names and values given by DataArray objects for each variable name.\n453 \n454     One dimensional variables with name equal to their dimension are\n455     index coordinates used for label based indexing.\n456 \n457     To load data from a file or file-like object, use the `open_dataset`\n458     function.\n459 \n460     Parameters\n461     ----------\n462     data_vars : dict-like, optional\n463         A mapping from variable names to :py:class:`~xarray.DataArray`\n464         objects, :py:class:`~xarray.Variable` objects or to tuples of\n465         the form ``(dims, data[, attrs])`` which can be used as\n466         arguments to create a new ``Variable``. Each dimension must\n467         have the same length in all variables in which it appears.\n468 \n469         The following notations are accepted:\n470 \n471         - mapping {var name: DataArray}\n472         - mapping {var name: Variable}\n473         - mapping {var name: (dimension name, array-like)}\n474         - mapping {var name: (tuple of dimension names, array-like)}\n475         - mapping {dimension name: array-like}\n476           (it will be automatically moved to coords, see below)\n477 \n478         Each dimension must have the same length in all variables in\n479         which it appears.\n480     coords : dict-like, optional\n481         Another mapping in similar form as the `data_vars` argument,\n482         except the each item is saved on the dataset as a \"coordinate\".\n483         These variables have an associated meaning: they describe\n484         constant/fixed/independent quantities, unlike the\n485         varying/measured/dependent quantities that belong in\n486         `variables`. Coordinates values may be given by 1-dimensional\n487         arrays or scalars, in which case `dims` do not need to be\n488         supplied: 1D arrays will be assumed to give index values along\n489         the dimension with the same name.\n490 \n491         The following notations are accepted:\n492 \n493         - mapping {coord name: DataArray}\n494         - mapping {coord name: Variable}\n495         - mapping {coord name: (dimension name, array-like)}\n496         - mapping {coord name: (tuple of dimension names, array-like)}\n497         - mapping {dimension name: array-like}\n498           (the dimension name is implicitly set to be the same as the\n499           coord name)\n500 \n501         The last notation implies that the coord name is the same as\n502         the dimension name.\n503 \n504     attrs : dict-like, optional\n505         Global attributes to save on this dataset.\n506 \n507     Examples\n508     --------\n509     Create data:\n510 \n511     >>> np.random.seed(0)\n512     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n513     >>> precipitation = 10 * np.random.rand(2, 2, 3)\n514     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n515     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n516     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n517     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n518 \n519     Initialize a dataset with multiple dimensions:\n520 \n521     >>> ds = xr.Dataset(\n522     ...     data_vars=dict(\n523     ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n524     ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n525     ...     ),\n526     ...     coords=dict(\n527     ...         lon=([\"x\", \"y\"], lon),\n528     ...         lat=([\"x\", \"y\"], lat),\n529     ...         time=time,\n530     ...         reference_time=reference_time,\n531     ...     ),\n532     ...     attrs=dict(description=\"Weather related data.\"),\n533     ... )\n534     >>> ds\n535     <xarray.Dataset>\n536     Dimensions:         (x: 2, y: 2, time: 3)\n537     Coordinates:\n538         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n539         lat             (x, y) float64 42.25 42.21 42.63 42.59\n540       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n541         reference_time  datetime64[ns] 2014-09-05\n542     Dimensions without coordinates: x, y\n543     Data variables:\n544         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n545         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n546     Attributes:\n547         description:  Weather related data.\n548 \n549     Find out where the coldest temperature was and what values the\n550     other variables had:\n551 \n552     >>> ds.isel(ds.temperature.argmin(...))\n553     <xarray.Dataset>\n554     Dimensions:         ()\n555     Coordinates:\n556         lon             float64 -99.32\n557         lat             float64 42.21\n558         time            datetime64[ns] 2014-09-08\n559         reference_time  datetime64[ns] 2014-09-05\n560     Data variables:\n561         temperature     float64 7.182\n562         precipitation   float64 8.326\n563     Attributes:\n564         description:  Weather related data.\n565     \"\"\"\n566 \n567     _attrs: dict[Hashable, Any] | None\n568     _cache: dict[str, Any]\n569     _coord_names: set[Hashable]\n570     _dims: dict[Hashable, int]\n571     _encoding: dict[Hashable, Any] | None\n572     _close: Callable[[], None] | None\n573     _indexes: dict[Hashable, Index]\n574     _variables: dict[Hashable, Variable]\n575 \n576     __slots__ = (\n577         \"_attrs\",\n578         \"_cache\",\n579         \"_coord_names\",\n580         \"_dims\",\n581         \"_encoding\",\n582         \"_close\",\n583         \"_indexes\",\n584         \"_variables\",\n585         \"__weakref__\",\n586     )\n587 \n588     def __init__(\n589         self,\n590         # could make a VariableArgs to use more generally, and refine these\n591         # categories\n592         data_vars: Mapping[Any, Any] | None = None,\n593         coords: Mapping[Any, Any] | None = None,\n594         attrs: Mapping[Any, Any] | None = None,\n595     ) -> None:\n596         # TODO(shoyer): expose indexes as a public argument in __init__\n597 \n598         if data_vars is None:\n599             data_vars = {}\n600         if coords is None:\n601             coords = {}\n602 \n603         both_data_and_coords = set(data_vars) & set(coords)\n604         if both_data_and_coords:\n605             raise ValueError(\n606                 f\"variables {both_data_and_coords!r} are found in both data_vars and coords\"\n607             )\n608 \n609         if isinstance(coords, Dataset):\n610             coords = coords.variables\n611 \n612         variables, coord_names, dims, indexes, _ = merge_data_and_coords(\n613             data_vars, coords, compat=\"broadcast_equals\"\n614         )\n615 \n616         self._attrs = dict(attrs) if attrs is not None else None\n617         self._close = None\n618         self._encoding = None\n619         self._variables = variables\n620         self._coord_names = coord_names\n621         self._dims = dims\n622         self._indexes = indexes\n623 \n624     @classmethod\n625     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:\n626         \"\"\"Create a new dataset from the contents of a backends.*DataStore\n627         object\n628         \"\"\"\n629         variables, attributes = store.load()\n630         if decoder:\n631             variables, attributes = decoder(variables, attributes)\n632         obj = cls(variables, attrs=attributes)\n633         obj.set_close(store.close)\n634         return obj\n635 \n636     @property\n637     def variables(self) -> Frozen[Hashable, Variable]:\n638         \"\"\"Low level interface to Dataset contents as dict of Variable objects.\n639 \n640         This ordered dictionary is frozen to prevent mutation that could\n641         violate Dataset invariants. It contains all variable objects\n642         constituting the Dataset, including both data variables and\n643         coordinates.\n644         \"\"\"\n645         return Frozen(self._variables)\n646 \n647     @property\n648     def attrs(self) -> dict[Any, Any]:\n649         \"\"\"Dictionary of global attributes on this dataset\"\"\"\n650         if self._attrs is None:\n651             self._attrs = {}\n652         return self._attrs\n653 \n654     @attrs.setter\n655     def attrs(self, value: Mapping[Any, Any]) -> None:\n656         self._attrs = dict(value)\n657 \n658     @property\n659     def encoding(self) -> dict[Any, Any]:\n660         \"\"\"Dictionary of global encoding attributes on this dataset\"\"\"\n661         if self._encoding is None:\n662             self._encoding = {}\n663         return self._encoding\n664 \n665     @encoding.setter\n666     def encoding(self, value: Mapping[Any, Any]) -> None:\n667         self._encoding = dict(value)\n668 \n669     def reset_encoding(self: T_Dataset) -> T_Dataset:\n670         \"\"\"Return a new Dataset without encoding on the dataset or any of its\n671         variables/coords.\"\"\"\n672         variables = {k: v.reset_encoding() for k, v in self.variables.items()}\n673         return self._replace(variables=variables, encoding={})\n674 \n675     @property\n676     def dims(self) -> Frozen[Hashable, int]:\n677         \"\"\"Mapping from dimension names to lengths.\n678 \n679         Cannot be modified directly, but is updated when adding new variables.\n680 \n681         Note that type of this object differs from `DataArray.dims`.\n682         See `Dataset.sizes` and `DataArray.sizes` for consistently named\n683         properties.\n684 \n685         See Also\n686         --------\n687         Dataset.sizes\n688         DataArray.dims\n689         \"\"\"\n690         return Frozen(self._dims)\n691 \n692     @property\n693     def sizes(self) -> Frozen[Hashable, int]:\n694         \"\"\"Mapping from dimension names to lengths.\n695 \n696         Cannot be modified directly, but is updated when adding new variables.\n697 \n698         This is an alias for `Dataset.dims` provided for the benefit of\n699         consistency with `DataArray.sizes`.\n700 \n701         See Also\n702         --------\n703         DataArray.sizes\n704         \"\"\"\n705         return self.dims\n706 \n707     @property\n708     def dtypes(self) -> Frozen[Hashable, np.dtype]:\n709         \"\"\"Mapping from data variable names to dtypes.\n710 \n711         Cannot be modified directly, but is updated when adding new variables.\n712 \n713         See Also\n714         --------\n715         DataArray.dtype\n716         \"\"\"\n717         return Frozen(\n718             {\n719                 n: v.dtype\n720                 for n, v in self._variables.items()\n721                 if n not in self._coord_names\n722             }\n723         )\n724 \n725     def load(self: T_Dataset, **kwargs) -> T_Dataset:\n726         \"\"\"Manually trigger loading and/or computation of this dataset's data\n727         from disk or a remote source into memory and return this dataset.\n728         Unlike compute, the original dataset is modified and returned.\n729 \n730         Normally, it should not be necessary to call this method in user code,\n731         because all xarray functions should either work on deferred data or\n732         load data automatically. However, this method can be necessary when\n733         working with many file objects on disk.\n734 \n735         Parameters\n736         ----------\n737         **kwargs : dict\n738             Additional keyword arguments passed on to ``dask.compute``.\n739 \n740         See Also\n741         --------\n742         dask.compute\n743         \"\"\"\n744         # access .data to coerce everything to numpy or dask arrays\n745         lazy_data = {\n746             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n747         }\n748         if lazy_data:\n749             import dask.array as da\n750 \n751             # evaluate all the dask arrays simultaneously\n752             evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n753 \n754             for k, data in zip(lazy_data, evaluated_data):\n755                 self.variables[k].data = data\n756 \n757         # load everything else sequentially\n758         for k, v in self.variables.items():\n759             if k not in lazy_data:\n760                 v.load()\n761 \n762         return self\n763 \n764     def __dask_tokenize__(self):\n765         from dask.base import normalize_token\n766 \n767         return normalize_token(\n768             (type(self), self._variables, self._coord_names, self._attrs)\n769         )\n770 \n771     def __dask_graph__(self):\n772         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}\n773         graphs = {k: v for k, v in graphs.items() if v is not None}\n774         if not graphs:\n775             return None\n776         else:\n777             try:\n778                 from dask.highlevelgraph import HighLevelGraph\n779 \n780                 return HighLevelGraph.merge(*graphs.values())\n781             except ImportError:\n782                 from dask import sharedict\n783 \n784                 return sharedict.merge(*graphs.values())\n785 \n786     def __dask_keys__(self):\n787         import dask\n788 \n789         return [\n790             v.__dask_keys__()\n791             for v in self.variables.values()\n792             if dask.is_dask_collection(v)\n793         ]\n794 \n795     def __dask_layers__(self):\n796         import dask\n797 \n798         return sum(\n799             (\n800                 v.__dask_layers__()\n801                 for v in self.variables.values()\n802                 if dask.is_dask_collection(v)\n803             ),\n804             (),\n805         )\n806 \n807     @property\n808     def __dask_optimize__(self):\n809         import dask.array as da\n810 \n811         return da.Array.__dask_optimize__\n812 \n813     @property\n814     def __dask_scheduler__(self):\n815         import dask.array as da\n816 \n817         return da.Array.__dask_scheduler__\n818 \n819     def __dask_postcompute__(self):\n820         return self._dask_postcompute, ()\n821 \n822     def __dask_postpersist__(self):\n823         return self._dask_postpersist, ()\n824 \n825     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:\n826         import dask\n827 \n828         variables = {}\n829         results_iter = iter(results)\n830 \n831         for k, v in self._variables.items():\n832             if dask.is_dask_collection(v):\n833                 rebuild, args = v.__dask_postcompute__()\n834                 v = rebuild(next(results_iter), *args)\n835             variables[k] = v\n836 \n837         return type(self)._construct_direct(\n838             variables,\n839             self._coord_names,\n840             self._dims,\n841             self._attrs,\n842             self._indexes,\n843             self._encoding,\n844             self._close,\n845         )\n846 \n847     def _dask_postpersist(\n848         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] | None = None\n849     ) -> T_Dataset:\n850         from dask import is_dask_collection\n851         from dask.highlevelgraph import HighLevelGraph\n852         from dask.optimization import cull\n853 \n854         variables = {}\n855 \n856         for k, v in self._variables.items():\n857             if not is_dask_collection(v):\n858                 variables[k] = v\n859                 continue\n860 \n861             if isinstance(dsk, HighLevelGraph):\n862                 # dask >= 2021.3\n863                 # __dask_postpersist__() was called by dask.highlevelgraph.\n864                 # Don't use dsk.cull(), as we need to prevent partial layers:\n865                 # https://github.com/dask/dask/issues/7137\n866                 layers = v.__dask_layers__()\n867                 if rename:\n868                     layers = [rename.get(k, k) for k in layers]\n869                 dsk2 = dsk.cull_layers(layers)\n870             elif rename:  # pragma: nocover\n871                 # At the moment of writing, this is only for forward compatibility.\n872                 # replace_name_in_key requires dask >= 2021.3.\n873                 from dask.base import flatten, replace_name_in_key\n874 \n875                 keys = [\n876                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())\n877                 ]\n878                 dsk2, _ = cull(dsk, keys)\n879             else:\n880                 # __dask_postpersist__() was called by dask.optimize or dask.persist\n881                 dsk2, _ = cull(dsk, v.__dask_keys__())\n882 \n883             rebuild, args = v.__dask_postpersist__()\n884             # rename was added in dask 2021.3\n885             kwargs = {\"rename\": rename} if rename else {}\n886             variables[k] = rebuild(dsk2, *args, **kwargs)\n887 \n888         return type(self)._construct_direct(\n889             variables,\n890             self._coord_names,\n891             self._dims,\n892             self._attrs,\n893             self._indexes,\n894             self._encoding,\n895             self._close,\n896         )\n897 \n898     def compute(self: T_Dataset, **kwargs) -> T_Dataset:\n899         \"\"\"Manually trigger loading and/or computation of this dataset's data\n900         from disk or a remote source into memory and return a new dataset.\n901         Unlike load, the original dataset is left unaltered.\n902 \n903         Normally, it should not be necessary to call this method in user code,\n904         because all xarray functions should either work on deferred data or\n905         load data automatically. However, this method can be necessary when\n906         working with many file objects on disk.\n907 \n908         Parameters\n909         ----------\n910         **kwargs : dict\n911             Additional keyword arguments passed on to ``dask.compute``.\n912 \n913         See Also\n914         --------\n915         dask.compute\n916         \"\"\"\n917         new = self.copy(deep=False)\n918         return new.load(**kwargs)\n919 \n920     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:\n921         \"\"\"Persist all Dask arrays in memory\"\"\"\n922         # access .data to coerce everything to numpy or dask arrays\n923         lazy_data = {\n924             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n925         }\n926         if lazy_data:\n927             import dask\n928 \n929             # evaluate all the dask arrays simultaneously\n930             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)\n931 \n932             for k, data in zip(lazy_data, evaluated_data):\n933                 self.variables[k].data = data\n934 \n935         return self\n936 \n937     def persist(self: T_Dataset, **kwargs) -> T_Dataset:\n938         \"\"\"Trigger computation, keeping data as dask arrays\n939 \n940         This operation can be used to trigger computation on underlying dask\n941         arrays, similar to ``.compute()`` or ``.load()``.  However this\n942         operation keeps the data as dask arrays. This is particularly useful\n943         when using the dask.distributed scheduler and you want to load a large\n944         amount of data into distributed memory.\n945 \n946         Parameters\n947         ----------\n948         **kwargs : dict\n949             Additional keyword arguments passed on to ``dask.persist``.\n950 \n951         See Also\n952         --------\n953         dask.persist\n954         \"\"\"\n955         new = self.copy(deep=False)\n956         return new._persist_inplace(**kwargs)\n957 \n958     @classmethod\n959     def _construct_direct(\n960         cls: type[T_Dataset],\n961         variables: dict[Any, Variable],\n962         coord_names: set[Hashable],\n963         dims: dict[Any, int] | None = None,\n964         attrs: dict | None = None,\n965         indexes: dict[Any, Index] | None = None,\n966         encoding: dict | None = None,\n967         close: Callable[[], None] | None = None,\n968     ) -> T_Dataset:\n969         \"\"\"Shortcut around __init__ for internal use when we want to skip\n970         costly validation\n971         \"\"\"\n972         if dims is None:\n973             dims = calculate_dimensions(variables)\n974         if indexes is None:\n975             indexes = {}\n976         obj = object.__new__(cls)\n977         obj._variables = variables\n978         obj._coord_names = coord_names\n979         obj._dims = dims\n980         obj._indexes = indexes\n981         obj._attrs = attrs\n982         obj._close = close\n983         obj._encoding = encoding\n984         return obj\n985 \n986     def _replace(\n987         self: T_Dataset,\n988         variables: dict[Hashable, Variable] | None = None,\n989         coord_names: set[Hashable] | None = None,\n990         dims: dict[Any, int] | None = None,\n991         attrs: dict[Hashable, Any] | None | Default = _default,\n992         indexes: dict[Hashable, Index] | None = None,\n993         encoding: dict | None | Default = _default,\n994         inplace: bool = False,\n995     ) -> T_Dataset:\n996         \"\"\"Fastpath constructor for internal use.\n997 \n998         Returns an object with optionally with replaced attributes.\n999 \n1000         Explicitly passed arguments are *not* copied when placed on the new\n1001         dataset. It is up to the caller to ensure that they have the right type\n1002         and are not used elsewhere.\n1003         \"\"\"\n1004         if inplace:\n1005             if variables is not None:\n1006                 self._variables = variables\n1007             if coord_names is not None:\n1008                 self._coord_names = coord_names\n1009             if dims is not None:\n1010                 self._dims = dims\n1011             if attrs is not _default:\n1012                 self._attrs = attrs\n1013             if indexes is not None:\n1014                 self._indexes = indexes\n1015             if encoding is not _default:\n1016                 self._encoding = encoding\n1017             obj = self\n1018         else:\n1019             if variables is None:\n1020                 variables = self._variables.copy()\n1021             if coord_names is None:\n1022                 coord_names = self._coord_names.copy()\n1023             if dims is None:\n1024                 dims = self._dims.copy()\n1025             if attrs is _default:\n1026                 attrs = copy.copy(self._attrs)\n1027             if indexes is None:\n1028                 indexes = self._indexes.copy()\n1029             if encoding is _default:\n1030                 encoding = copy.copy(self._encoding)\n1031             obj = self._construct_direct(\n1032                 variables, coord_names, dims, attrs, indexes, encoding\n1033             )\n1034         return obj\n1035 \n1036     def _replace_with_new_dims(\n1037         self: T_Dataset,\n1038         variables: dict[Hashable, Variable],\n1039         coord_names: set | None = None,\n1040         attrs: dict[Hashable, Any] | None | Default = _default,\n1041         indexes: dict[Hashable, Index] | None = None,\n1042         inplace: bool = False,\n1043     ) -> T_Dataset:\n1044         \"\"\"Replace variables with recalculated dimensions.\"\"\"\n1045         dims = calculate_dimensions(variables)\n1046         return self._replace(\n1047             variables, coord_names, dims, attrs, indexes, inplace=inplace\n1048         )\n1049 \n1050     def _replace_vars_and_dims(\n1051         self: T_Dataset,\n1052         variables: dict[Hashable, Variable],\n1053         coord_names: set | None = None,\n1054         dims: dict[Hashable, int] | None = None,\n1055         attrs: dict[Hashable, Any] | None | Default = _default,\n1056         inplace: bool = False,\n1057     ) -> T_Dataset:\n1058         \"\"\"Deprecated version of _replace_with_new_dims().\n1059 \n1060         Unlike _replace_with_new_dims(), this method always recalculates\n1061         indexes from variables.\n1062         \"\"\"\n1063         if dims is None:\n1064             dims = calculate_dimensions(variables)\n1065         return self._replace(\n1066             variables, coord_names, dims, attrs, indexes=None, inplace=inplace\n1067         )\n1068 \n1069     def _overwrite_indexes(\n1070         self: T_Dataset,\n1071         indexes: Mapping[Hashable, Index],\n1072         variables: Mapping[Hashable, Variable] | None = None,\n1073         drop_variables: list[Hashable] | None = None,\n1074         drop_indexes: list[Hashable] | None = None,\n1075         rename_dims: Mapping[Hashable, Hashable] | None = None,\n1076     ) -> T_Dataset:\n1077         \"\"\"Maybe replace indexes.\n1078 \n1079         This function may do a lot more depending on index query\n1080         results.\n1081 \n1082         \"\"\"\n1083         if not indexes:\n1084             return self\n1085 \n1086         if variables is None:\n1087             variables = {}\n1088         if drop_variables is None:\n1089             drop_variables = []\n1090         if drop_indexes is None:\n1091             drop_indexes = []\n1092 \n1093         new_variables = self._variables.copy()\n1094         new_coord_names = self._coord_names.copy()\n1095         new_indexes = dict(self._indexes)\n1096 \n1097         index_variables = {}\n1098         no_index_variables = {}\n1099         for name, var in variables.items():\n1100             old_var = self._variables.get(name)\n1101             if old_var is not None:\n1102                 var.attrs.update(old_var.attrs)\n1103                 var.encoding.update(old_var.encoding)\n1104             if name in indexes:\n1105                 index_variables[name] = var\n1106             else:\n1107                 no_index_variables[name] = var\n1108 \n1109         for name in indexes:\n1110             new_indexes[name] = indexes[name]\n1111 \n1112         for name, var in index_variables.items():\n1113             new_coord_names.add(name)\n1114             new_variables[name] = var\n1115 \n1116         # append no-index variables at the end\n1117         for k in no_index_variables:\n1118             new_variables.pop(k)\n1119         new_variables.update(no_index_variables)\n1120 \n1121         for name in drop_indexes:\n1122             new_indexes.pop(name)\n1123 \n1124         for name in drop_variables:\n1125             new_variables.pop(name)\n1126             new_indexes.pop(name, None)\n1127             new_coord_names.remove(name)\n1128 \n1129         replaced = self._replace(\n1130             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes\n1131         )\n1132 \n1133         if rename_dims:\n1134             # skip rename indexes: they should already have the right name(s)\n1135             dims = replaced._rename_dims(rename_dims)\n1136             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)\n1137             return replaced._replace(\n1138                 variables=new_variables, coord_names=new_coord_names, dims=dims\n1139             )\n1140         else:\n1141             return replaced\n1142 \n1143     def copy(\n1144         self: T_Dataset, deep: bool = False, data: Mapping[Any, ArrayLike] | None = None\n1145     ) -> T_Dataset:\n1146         \"\"\"Returns a copy of this dataset.\n1147 \n1148         If `deep=True`, a deep copy is made of each of the component variables.\n1149         Otherwise, a shallow copy of each of the component variable is made, so\n1150         that the underlying memory region of the new dataset is the same as in\n1151         the original dataset.\n1152 \n1153         Use `data` to create a new object with the same structure as\n1154         original but entirely new data.\n1155 \n1156         Parameters\n1157         ----------\n1158         deep : bool, default: False\n1159             Whether each component variable is loaded into memory and copied onto\n1160             the new object. Default is False.\n1161         data : dict-like or None, optional\n1162             Data to use in the new object. Each item in `data` must have same\n1163             shape as corresponding data variable in original. When `data` is\n1164             used, `deep` is ignored for the data variables and only used for\n1165             coords.\n1166 \n1167         Returns\n1168         -------\n1169         object : Dataset\n1170             New object with dimensions, attributes, coordinates, name, encoding,\n1171             and optionally data copied from original.\n1172 \n1173         Examples\n1174         --------\n1175         Shallow copy versus deep copy\n1176 \n1177         >>> da = xr.DataArray(np.random.randn(2, 3))\n1178         >>> ds = xr.Dataset(\n1179         ...     {\"foo\": da, \"bar\": (\"x\", [-1, 2])},\n1180         ...     coords={\"x\": [\"one\", \"two\"]},\n1181         ... )\n1182         >>> ds.copy()\n1183         <xarray.Dataset>\n1184         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1185         Coordinates:\n1186           * x        (x) <U3 'one' 'two'\n1187         Dimensions without coordinates: dim_0, dim_1\n1188         Data variables:\n1189             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n1190             bar      (x) int64 -1 2\n1191 \n1192         >>> ds_0 = ds.copy(deep=False)\n1193         >>> ds_0[\"foo\"][0, 0] = 7\n1194         >>> ds_0\n1195         <xarray.Dataset>\n1196         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1197         Coordinates:\n1198           * x        (x) <U3 'one' 'two'\n1199         Dimensions without coordinates: dim_0, dim_1\n1200         Data variables:\n1201             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1202             bar      (x) int64 -1 2\n1203 \n1204         >>> ds\n1205         <xarray.Dataset>\n1206         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1207         Coordinates:\n1208           * x        (x) <U3 'one' 'two'\n1209         Dimensions without coordinates: dim_0, dim_1\n1210         Data variables:\n1211             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1212             bar      (x) int64 -1 2\n1213 \n1214         Changing the data using the ``data`` argument maintains the\n1215         structure of the original object, but with the new data. Original\n1216         object is unaffected.\n1217 \n1218         >>> ds.copy(data={\"foo\": np.arange(6).reshape(2, 3), \"bar\": [\"a\", \"b\"]})\n1219         <xarray.Dataset>\n1220         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1221         Coordinates:\n1222           * x        (x) <U3 'one' 'two'\n1223         Dimensions without coordinates: dim_0, dim_1\n1224         Data variables:\n1225             foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n1226             bar      (x) <U1 'a' 'b'\n1227 \n1228         >>> ds\n1229         <xarray.Dataset>\n1230         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1231         Coordinates:\n1232           * x        (x) <U3 'one' 'two'\n1233         Dimensions without coordinates: dim_0, dim_1\n1234         Data variables:\n1235             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1236             bar      (x) int64 -1 2\n1237 \n1238         See Also\n1239         --------\n1240         pandas.DataFrame.copy\n1241         \"\"\"\n1242         return self._copy(deep=deep, data=data)\n1243 \n1244     def _copy(\n1245         self: T_Dataset,\n1246         deep: bool = False,\n1247         data: Mapping[Any, ArrayLike] | None = None,\n1248         memo: dict[int, Any] | None = None,\n1249     ) -> T_Dataset:\n1250         if data is None:\n1251             data = {}\n1252         elif not utils.is_dict_like(data):\n1253             raise ValueError(\"Data must be dict-like\")\n1254 \n1255         if data:\n1256             var_keys = set(self.data_vars.keys())\n1257             data_keys = set(data.keys())\n1258             keys_not_in_vars = data_keys - var_keys\n1259             if keys_not_in_vars:\n1260                 raise ValueError(\n1261                     \"Data must only contain variables in original \"\n1262                     \"dataset. Extra variables: {}\".format(keys_not_in_vars)\n1263                 )\n1264             keys_missing_from_data = var_keys - data_keys\n1265             if keys_missing_from_data:\n1266                 raise ValueError(\n1267                     \"Data must contain all variables in original \"\n1268                     \"dataset. Data is missing {}\".format(keys_missing_from_data)\n1269                 )\n1270 \n1271         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n1272 \n1273         variables = {}\n1274         for k, v in self._variables.items():\n1275             if k in index_vars:\n1276                 variables[k] = index_vars[k]\n1277             else:\n1278                 variables[k] = v._copy(deep=deep, data=data.get(k), memo=memo)\n1279 \n1280         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)\n1281         encoding = (\n1282             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)\n1283         )\n1284 \n1285         return self._replace(variables, indexes=indexes, attrs=attrs, encoding=encoding)\n1286 \n1287     def __copy__(self: T_Dataset) -> T_Dataset:\n1288         return self._copy(deep=False)\n1289 \n1290     def __deepcopy__(self: T_Dataset, memo: dict[int, Any] | None = None) -> T_Dataset:\n1291         return self._copy(deep=True, memo=memo)\n1292 \n1293     def as_numpy(self: T_Dataset) -> T_Dataset:\n1294         \"\"\"\n1295         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.\n1296 \n1297         See also\n1298         --------\n1299         DataArray.as_numpy\n1300         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.\n1301         \"\"\"\n1302         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}\n1303         return self._replace(variables=numpy_variables)\n1304 \n1305     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:\n1306         \"\"\"Create a new Dataset with the listed variables from this dataset and\n1307         the all relevant coordinates. Skips all validation.\n1308         \"\"\"\n1309         variables: dict[Hashable, Variable] = {}\n1310         coord_names = set()\n1311         indexes: dict[Hashable, Index] = {}\n1312 \n1313         for name in names:\n1314             try:\n1315                 variables[name] = self._variables[name]\n1316             except KeyError:\n1317                 ref_name, var_name, var = _get_virtual_variable(\n1318                     self._variables, name, self.dims\n1319                 )\n1320                 variables[var_name] = var\n1321                 if ref_name in self._coord_names or ref_name in self.dims:\n1322                     coord_names.add(var_name)\n1323                 if (var_name,) == var.dims:\n1324                     index, index_vars = create_default_index_implicit(var, names)\n1325                     indexes.update({k: index for k in index_vars})\n1326                     variables.update(index_vars)\n1327                     coord_names.update(index_vars)\n1328 \n1329         needed_dims: OrderedSet[Hashable] = OrderedSet()\n1330         for v in variables.values():\n1331             needed_dims.update(v.dims)\n1332 \n1333         dims = {k: self.dims[k] for k in needed_dims}\n1334 \n1335         # preserves ordering of coordinates\n1336         for k in self._variables:\n1337             if k not in self._coord_names:\n1338                 continue\n1339 \n1340             if set(self.variables[k].dims) <= needed_dims:\n1341                 variables[k] = self._variables[k]\n1342                 coord_names.add(k)\n1343 \n1344         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))\n1345 \n1346         return self._replace(variables, coord_names, dims, indexes=indexes)\n1347 \n1348     def _construct_dataarray(self, name: Hashable) -> DataArray:\n1349         \"\"\"Construct a DataArray by indexing this dataset\"\"\"\n1350         from xarray.core.dataarray import DataArray\n1351 \n1352         try:\n1353             variable = self._variables[name]\n1354         except KeyError:\n1355             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)\n1356 \n1357         needed_dims = set(variable.dims)\n1358 \n1359         coords: dict[Hashable, Variable] = {}\n1360         # preserve ordering\n1361         for k in self._variables:\n1362             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:\n1363                 coords[k] = self.variables[k]\n1364 \n1365         indexes = filter_indexes_from_coords(self._indexes, set(coords))\n1366 \n1367         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n1368 \n1369     @property\n1370     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1371         \"\"\"Places to look-up items for attribute-style access\"\"\"\n1372         yield from self._item_sources\n1373         yield self.attrs\n1374 \n1375     @property\n1376     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1377         \"\"\"Places to look-up items for key-completion\"\"\"\n1378         yield self.data_vars\n1379         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)\n1380 \n1381         # virtual coordinates\n1382         yield HybridMappingProxy(keys=self.dims, mapping=self)\n1383 \n1384     def __contains__(self, key: object) -> bool:\n1385         \"\"\"The 'in' operator will return true or false depending on whether\n1386         'key' is an array in the dataset or not.\n1387         \"\"\"\n1388         return key in self._variables\n1389 \n1390     def __len__(self) -> int:\n1391         return len(self.data_vars)\n1392 \n1393     def __bool__(self) -> bool:\n1394         return bool(self.data_vars)\n1395 \n1396     def __iter__(self) -> Iterator[Hashable]:\n1397         return iter(self.data_vars)\n1398 \n1399     def __array__(self, dtype=None):\n1400         raise TypeError(\n1401             \"cannot directly convert an xarray.Dataset into a \"\n1402             \"numpy array. Instead, create an xarray.DataArray \"\n1403             \"first, either with indexing on the Dataset or by \"\n1404             \"invoking the `to_array()` method.\"\n1405         )\n1406 \n1407     @property\n1408     def nbytes(self) -> int:\n1409         \"\"\"\n1410         Total bytes consumed by the data arrays of all variables in this dataset.\n1411 \n1412         If the backend array for any variable does not include ``nbytes``, estimates\n1413         the total bytes for that array based on the ``size`` and ``dtype``.\n1414         \"\"\"\n1415         return sum(v.nbytes for v in self.variables.values())\n1416 \n1417     @property\n1418     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:\n1419         \"\"\"Attribute for location based indexing. Only supports __getitem__,\n1420         and only when the key is a dict of the form {dim: labels}.\n1421         \"\"\"\n1422         return _LocIndexer(self)\n1423 \n1424     @overload\n1425     def __getitem__(self, key: Hashable) -> DataArray:\n1426         ...\n1427 \n1428     # Mapping is Iterable\n1429     @overload\n1430     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:\n1431         ...\n1432 \n1433     def __getitem__(\n1434         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]\n1435     ) -> T_Dataset | DataArray:\n1436         \"\"\"Access variables or coordinates of this dataset as a\n1437         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.\n1438 \n1439         Indexing with a list of names will return a new ``Dataset`` object.\n1440         \"\"\"\n1441         if utils.is_dict_like(key):\n1442             return self.isel(**key)\n1443         if utils.hashable(key):\n1444             return self._construct_dataarray(key)\n1445         if utils.iterable_of_hashable(key):\n1446             return self._copy_listed(key)\n1447         raise ValueError(f\"Unsupported key-type {type(key)}\")\n1448 \n1449     def __setitem__(\n1450         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any\n1451     ) -> None:\n1452         \"\"\"Add an array to this dataset.\n1453         Multiple arrays can be added at the same time, in which case each of\n1454         the following operations is applied to the respective value.\n1455 \n1456         If key is dict-like, update all variables in the dataset\n1457         one by one with the given value at the given location.\n1458         If the given value is also a dataset, select corresponding variables\n1459         in the given value and in the dataset to be changed.\n1460 \n1461         If value is a `\n1462         from .dataarray import DataArray`, call its `select_vars()` method, rename it\n1463         to `key` and merge the contents of the resulting dataset into this\n1464         dataset.\n1465 \n1466         If value is a `Variable` object (or tuple of form\n1467         ``(dims, data[, attrs])``), add it to this dataset as a new\n1468         variable.\n1469         \"\"\"\n1470         from xarray.core.dataarray import DataArray\n1471 \n1472         if utils.is_dict_like(key):\n1473             # check for consistency and convert value to dataset\n1474             value = self._setitem_check(key, value)\n1475             # loop over dataset variables and set new values\n1476             processed = []\n1477             for name, var in self.items():\n1478                 try:\n1479                     var[key] = value[name]\n1480                     processed.append(name)\n1481                 except Exception as e:\n1482                     if processed:\n1483                         raise RuntimeError(\n1484                             \"An error occurred while setting values of the\"\n1485                             f\" variable '{name}'. The following variables have\"\n1486                             f\" been successfully updated:\\n{processed}\"\n1487                         ) from e\n1488                     else:\n1489                         raise e\n1490 \n1491         elif utils.hashable(key):\n1492             if isinstance(value, Dataset):\n1493                 raise TypeError(\n1494                     \"Cannot assign a Dataset to a single key - only a DataArray or Variable \"\n1495                     \"object can be stored under a single key.\"\n1496                 )\n1497             self.update({key: value})\n1498 \n1499         elif utils.iterable_of_hashable(key):\n1500             keylist = list(key)\n1501             if len(keylist) == 0:\n1502                 raise ValueError(\"Empty list of variables to be set\")\n1503             if len(keylist) == 1:\n1504                 self.update({keylist[0]: value})\n1505             else:\n1506                 if len(keylist) != len(value):\n1507                     raise ValueError(\n1508                         f\"Different lengths of variables to be set \"\n1509                         f\"({len(keylist)}) and data used as input for \"\n1510                         f\"setting ({len(value)})\"\n1511                     )\n1512                 if isinstance(value, Dataset):\n1513                     self.update(dict(zip(keylist, value.data_vars.values())))\n1514                 elif isinstance(value, DataArray):\n1515                     raise ValueError(\"Cannot assign single DataArray to multiple keys\")\n1516                 else:\n1517                     self.update(dict(zip(keylist, value)))\n1518 \n1519         else:\n1520             raise ValueError(f\"Unsupported key-type {type(key)}\")\n1521 \n1522     def _setitem_check(self, key, value):\n1523         \"\"\"Consistency check for __setitem__\n1524 \n1525         When assigning values to a subset of a Dataset, do consistency check beforehand\n1526         to avoid leaving the dataset in a partially updated state when an error occurs.\n1527         \"\"\"\n1528         from xarray.core.alignment import align\n1529         from xarray.core.dataarray import DataArray\n1530 \n1531         if isinstance(value, Dataset):\n1532             missing_vars = [\n1533                 name for name in value.data_vars if name not in self.data_vars\n1534             ]\n1535             if missing_vars:\n1536                 raise ValueError(\n1537                     f\"Variables {missing_vars} in new values\"\n1538                     f\" not available in original dataset:\\n{self}\"\n1539                 )\n1540         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):\n1541             raise TypeError(\n1542                 \"Dataset assignment only accepts DataArrays, Datasets, and scalars.\"\n1543             )\n1544 \n1545         new_value = Dataset()\n1546         for name, var in self.items():\n1547             # test indexing\n1548             try:\n1549                 var_k = var[key]\n1550             except Exception as e:\n1551                 raise ValueError(\n1552                     f\"Variable '{name}': indexer {key} not available\"\n1553                 ) from e\n1554 \n1555             if isinstance(value, Dataset):\n1556                 val = value[name]\n1557             else:\n1558                 val = value\n1559 \n1560             if isinstance(val, DataArray):\n1561                 # check consistency of dimensions\n1562                 for dim in val.dims:\n1563                     if dim not in var_k.dims:\n1564                         raise KeyError(\n1565                             f\"Variable '{name}': dimension '{dim}' appears in new values \"\n1566                             f\"but not in the indexed original data\"\n1567                         )\n1568                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)\n1569                 if dims != val.dims:\n1570                     raise ValueError(\n1571                         f\"Variable '{name}': dimension order differs between\"\n1572                         f\" original and new data:\\n{dims}\\nvs.\\n{val.dims}\"\n1573                     )\n1574             else:\n1575                 val = np.array(val)\n1576 \n1577             # type conversion\n1578             new_value[name] = val.astype(var_k.dtype, copy=False)\n1579 \n1580         # check consistency of dimension sizes and dimension coordinates\n1581         if isinstance(value, DataArray) or isinstance(value, Dataset):\n1582             align(self[key], value, join=\"exact\", copy=False)\n1583 \n1584         return new_value\n1585 \n1586     def __delitem__(self, key: Hashable) -> None:\n1587         \"\"\"Remove a variable from this dataset.\"\"\"\n1588         assert_no_index_corrupted(self.xindexes, {key})\n1589 \n1590         if key in self._indexes:\n1591             del self._indexes[key]\n1592         del self._variables[key]\n1593         self._coord_names.discard(key)\n1594         self._dims = calculate_dimensions(self._variables)\n1595 \n1596     # mutable objects should not be hashable\n1597     # https://github.com/python/mypy/issues/4266\n1598     __hash__ = None  # type: ignore[assignment]\n1599 \n1600     def _all_compat(self, other: Dataset, compat_str: str) -> bool:\n1601         \"\"\"Helper function for equals and identical\"\"\"\n1602 \n1603         # some stores (e.g., scipy) do not seem to preserve order, so don't\n1604         # require matching order for equality\n1605         def compat(x: Variable, y: Variable) -> bool:\n1606             return getattr(x, compat_str)(y)\n1607 \n1608         return self._coord_names == other._coord_names and utils.dict_equiv(\n1609             self._variables, other._variables, compat=compat\n1610         )\n1611 \n1612     def broadcast_equals(self, other: Dataset) -> bool:\n1613         \"\"\"Two Datasets are broadcast equal if they are equal after\n1614         broadcasting all variables against each other.\n1615 \n1616         For example, variables that are scalar in one dataset but non-scalar in\n1617         the other dataset can still be broadcast equal if the the non-scalar\n1618         variable is a constant.\n1619 \n1620         See Also\n1621         --------\n1622         Dataset.equals\n1623         Dataset.identical\n1624         \"\"\"\n1625         try:\n1626             return self._all_compat(other, \"broadcast_equals\")\n1627         except (TypeError, AttributeError):\n1628             return False\n1629 \n1630     def equals(self, other: Dataset) -> bool:\n1631         \"\"\"Two Datasets are equal if they have matching variables and\n1632         coordinates, all of which are equal.\n1633 \n1634         Datasets can still be equal (like pandas objects) if they have NaN\n1635         values in the same locations.\n1636 \n1637         This method is necessary because `v1 == v2` for ``Dataset``\n1638         does element-wise comparisons (like numpy.ndarrays).\n1639 \n1640         See Also\n1641         --------\n1642         Dataset.broadcast_equals\n1643         Dataset.identical\n1644         \"\"\"\n1645         try:\n1646             return self._all_compat(other, \"equals\")\n1647         except (TypeError, AttributeError):\n1648             return False\n1649 \n1650     def identical(self, other: Dataset) -> bool:\n1651         \"\"\"Like equals, but also checks all dataset attributes and the\n1652         attributes on all variables and coordinates.\n1653 \n1654         See Also\n1655         --------\n1656         Dataset.broadcast_equals\n1657         Dataset.equals\n1658         \"\"\"\n1659         try:\n1660             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(\n1661                 other, \"identical\"\n1662             )\n1663         except (TypeError, AttributeError):\n1664             return False\n1665 \n1666     @property\n1667     def indexes(self) -> Indexes[pd.Index]:\n1668         \"\"\"Mapping of pandas.Index objects used for label based indexing.\n1669 \n1670         Raises an error if this Dataset has indexes that cannot be coerced\n1671         to pandas.Index objects.\n1672 \n1673         See Also\n1674         --------\n1675         Dataset.xindexes\n1676 \n1677         \"\"\"\n1678         return self.xindexes.to_pandas_indexes()\n1679 \n1680     @property\n1681     def xindexes(self) -> Indexes[Index]:\n1682         \"\"\"Mapping of xarray Index objects used for label based indexing.\"\"\"\n1683         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})\n1684 \n1685     @property\n1686     def coords(self) -> DatasetCoordinates:\n1687         \"\"\"Dictionary of xarray.DataArray objects corresponding to coordinate\n1688         variables\n1689         \"\"\"\n1690         return DatasetCoordinates(self)\n1691 \n1692     @property\n1693     def data_vars(self) -> DataVariables:\n1694         \"\"\"Dictionary of DataArray objects corresponding to data variables\"\"\"\n1695         return DataVariables(self)\n1696 \n1697     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:\n1698         \"\"\"Given names of one or more variables, set them as coordinates\n1699 \n1700         Parameters\n1701         ----------\n1702         names : hashable or iterable of hashable\n1703             Name(s) of variables in this dataset to convert into coordinates.\n1704 \n1705         Returns\n1706         -------\n1707         Dataset\n1708 \n1709         See Also\n1710         --------\n1711         Dataset.swap_dims\n1712         Dataset.assign_coords\n1713         \"\"\"\n1714         # TODO: allow inserting new coordinates with this method, like\n1715         # DataFrame.set_index?\n1716         # nb. check in self._variables, not self.data_vars to insure that the\n1717         # operation is idempotent\n1718         if isinstance(names, str) or not isinstance(names, Iterable):\n1719             names = [names]\n1720         else:\n1721             names = list(names)\n1722         self._assert_all_in_dataset(names)\n1723         obj = self.copy()\n1724         obj._coord_names.update(names)\n1725         return obj\n1726 \n1727     def reset_coords(\n1728         self: T_Dataset,\n1729         names: Dims = None,\n1730         drop: bool = False,\n1731     ) -> T_Dataset:\n1732         \"\"\"Given names of coordinates, reset them to become variables\n1733 \n1734         Parameters\n1735         ----------\n1736         names : str, Iterable of Hashable or None, optional\n1737             Name(s) of non-index coordinates in this dataset to reset into\n1738             variables. By default, all non-index coordinates are reset.\n1739         drop : bool, default: False\n1740             If True, remove coordinates instead of converting them into\n1741             variables.\n1742 \n1743         Returns\n1744         -------\n1745         Dataset\n1746         \"\"\"\n1747         if names is None:\n1748             names = self._coord_names - set(self._indexes)\n1749         else:\n1750             if isinstance(names, str) or not isinstance(names, Iterable):\n1751                 names = [names]\n1752             else:\n1753                 names = list(names)\n1754             self._assert_all_in_dataset(names)\n1755             bad_coords = set(names) & set(self._indexes)\n1756             if bad_coords:\n1757                 raise ValueError(\n1758                     f\"cannot remove index coordinates with reset_coords: {bad_coords}\"\n1759                 )\n1760         obj = self.copy()\n1761         obj._coord_names.difference_update(names)\n1762         if drop:\n1763             for name in names:\n1764                 del obj._variables[name]\n1765         return obj\n1766 \n1767     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:\n1768         \"\"\"Store dataset contents to a backends.*DataStore object.\"\"\"\n1769         from xarray.backends.api import dump_to_store\n1770 \n1771         # TODO: rename and/or cleanup this method to make it more consistent\n1772         # with to_netcdf()\n1773         dump_to_store(self, store, **kwargs)\n1774 \n1775     # path=None writes to bytes\n1776     @overload\n1777     def to_netcdf(\n1778         self,\n1779         path: None = None,\n1780         mode: Literal[\"w\", \"a\"] = \"w\",\n1781         format: T_NetcdfTypes | None = None,\n1782         group: str | None = None,\n1783         engine: T_NetcdfEngine | None = None,\n1784         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1785         unlimited_dims: Iterable[Hashable] | None = None,\n1786         compute: bool = True,\n1787         invalid_netcdf: bool = False,\n1788     ) -> bytes:\n1789         ...\n1790 \n1791     # default return None\n1792     @overload\n1793     def to_netcdf(\n1794         self,\n1795         path: str | PathLike,\n1796         mode: Literal[\"w\", \"a\"] = \"w\",\n1797         format: T_NetcdfTypes | None = None,\n1798         group: str | None = None,\n1799         engine: T_NetcdfEngine | None = None,\n1800         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1801         unlimited_dims: Iterable[Hashable] | None = None,\n1802         compute: Literal[True] = True,\n1803         invalid_netcdf: bool = False,\n1804     ) -> None:\n1805         ...\n1806 \n1807     # compute=False returns dask.Delayed\n1808     @overload\n1809     def to_netcdf(\n1810         self,\n1811         path: str | PathLike,\n1812         mode: Literal[\"w\", \"a\"] = \"w\",\n1813         format: T_NetcdfTypes | None = None,\n1814         group: str | None = None,\n1815         engine: T_NetcdfEngine | None = None,\n1816         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1817         unlimited_dims: Iterable[Hashable] | None = None,\n1818         *,\n1819         compute: Literal[False],\n1820         invalid_netcdf: bool = False,\n1821     ) -> Delayed:\n1822         ...\n1823 \n1824     def to_netcdf(\n1825         self,\n1826         path: str | PathLike | None = None,\n1827         mode: Literal[\"w\", \"a\"] = \"w\",\n1828         format: T_NetcdfTypes | None = None,\n1829         group: str | None = None,\n1830         engine: T_NetcdfEngine | None = None,\n1831         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1832         unlimited_dims: Iterable[Hashable] | None = None,\n1833         compute: bool = True,\n1834         invalid_netcdf: bool = False,\n1835     ) -> bytes | Delayed | None:\n1836         \"\"\"Write dataset contents to a netCDF file.\n1837 \n1838         Parameters\n1839         ----------\n1840         path : str, path-like or file-like, optional\n1841             Path to which to save this dataset. File-like objects are only\n1842             supported by the scipy engine. If no path is provided, this\n1843             function returns the resulting netCDF file as bytes; in this case,\n1844             we need to use scipy, which does not support netCDF version 4 (the\n1845             default format becomes NETCDF3_64BIT).\n1846         mode : {\"w\", \"a\"}, default: \"w\"\n1847             Write ('w') or append ('a') mode. If mode='w', any existing file at\n1848             this location will be overwritten. If mode='a', existing variables\n1849             will be overwritten.\n1850         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n1851                   \"NETCDF3_CLASSIC\"}, optional\n1852             File format for the resulting netCDF file:\n1853 \n1854             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n1855               features.\n1856             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n1857               netCDF 3 compatible API features.\n1858             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n1859               which fully supports 2+ GB files, but is only compatible with\n1860               clients linked against netCDF version 3.6.0 or later.\n1861             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n1862               handle 2+ GB files very well.\n1863 \n1864             All formats are supported by the netCDF4-python library.\n1865             scipy.io.netcdf only supports the last two formats.\n1866 \n1867             The default format is NETCDF4 if you are saving a file to disk and\n1868             have the netCDF4-python library available. Otherwise, xarray falls\n1869             back to using scipy to write netCDF files and defaults to the\n1870             NETCDF3_64BIT format (scipy does not support netCDF4).\n1871         group : str, optional\n1872             Path to the netCDF4 group in the given file to open (only works for\n1873             format='NETCDF4'). The group(s) will be created if necessary.\n1874         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n1875             Engine to use when writing netCDF files. If not provided, the\n1876             default engine is chosen based on available dependencies, with a\n1877             preference for 'netcdf4' if writing to a file on disk.\n1878         encoding : dict, optional\n1879             Nested dictionary with variable names as keys and dictionaries of\n1880             variable specific encodings as values, e.g.,\n1881             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n1882             \"zlib\": True}, ...}``\n1883 \n1884             The `h5netcdf` engine supports both the NetCDF4-style compression\n1885             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n1886             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n1887             This allows using any compression plugin installed in the HDF5\n1888             library, e.g. LZF.\n1889 \n1890         unlimited_dims : iterable of hashable, optional\n1891             Dimension(s) that should be serialized as unlimited dimensions.\n1892             By default, no dimensions are treated as unlimited dimensions.\n1893             Note that unlimited_dims may also be set via\n1894             ``dataset.encoding[\"unlimited_dims\"]``.\n1895         compute: bool, default: True\n1896             If true compute immediately, otherwise return a\n1897             ``dask.delayed.Delayed`` object that can be computed later.\n1898         invalid_netcdf: bool, default: False\n1899             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n1900             hdf5 files which are invalid netcdf as described in\n1901             https://github.com/h5netcdf/h5netcdf.\n1902 \n1903         Returns\n1904         -------\n1905             * ``bytes`` if path is None\n1906             * ``dask.delayed.Delayed`` if compute is False\n1907             * None otherwise\n1908 \n1909         See Also\n1910         --------\n1911         DataArray.to_netcdf\n1912         \"\"\"\n1913         if encoding is None:\n1914             encoding = {}\n1915         from xarray.backends.api import to_netcdf\n1916 \n1917         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n1918             self,\n1919             path,\n1920             mode=mode,\n1921             format=format,\n1922             group=group,\n1923             engine=engine,\n1924             encoding=encoding,\n1925             unlimited_dims=unlimited_dims,\n1926             compute=compute,\n1927             multifile=False,\n1928             invalid_netcdf=invalid_netcdf,\n1929         )\n1930 \n1931     # compute=True (default) returns ZarrStore\n1932     @overload\n1933     def to_zarr(\n1934         self,\n1935         store: MutableMapping | str | PathLike[str] | None = None,\n1936         chunk_store: MutableMapping | str | PathLike | None = None,\n1937         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1938         synchronizer=None,\n1939         group: str | None = None,\n1940         encoding: Mapping | None = None,\n1941         compute: Literal[True] = True,\n1942         consolidated: bool | None = None,\n1943         append_dim: Hashable | None = None,\n1944         region: Mapping[str, slice] | None = None,\n1945         safe_chunks: bool = True,\n1946         storage_options: dict[str, str] | None = None,\n1947         zarr_version: int | None = None,\n1948     ) -> ZarrStore:\n1949         ...\n1950 \n1951     # compute=False returns dask.Delayed\n1952     @overload\n1953     def to_zarr(\n1954         self,\n1955         store: MutableMapping | str | PathLike[str] | None = None,\n1956         chunk_store: MutableMapping | str | PathLike | None = None,\n1957         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1958         synchronizer=None,\n1959         group: str | None = None,\n1960         encoding: Mapping | None = None,\n1961         *,\n1962         compute: Literal[False],\n1963         consolidated: bool | None = None,\n1964         append_dim: Hashable | None = None,\n1965         region: Mapping[str, slice] | None = None,\n1966         safe_chunks: bool = True,\n1967         storage_options: dict[str, str] | None = None,\n1968         zarr_version: int | None = None,\n1969     ) -> Delayed:\n1970         ...\n1971 \n1972     def to_zarr(\n1973         self,\n1974         store: MutableMapping | str | PathLike[str] | None = None,\n1975         chunk_store: MutableMapping | str | PathLike | None = None,\n1976         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1977         synchronizer=None,\n1978         group: str | None = None,\n1979         encoding: Mapping | None = None,\n1980         compute: bool = True,\n1981         consolidated: bool | None = None,\n1982         append_dim: Hashable | None = None,\n1983         region: Mapping[str, slice] | None = None,\n1984         safe_chunks: bool = True,\n1985         storage_options: dict[str, str] | None = None,\n1986         zarr_version: int | None = None,\n1987     ) -> ZarrStore | Delayed:\n1988         \"\"\"Write dataset contents to a zarr group.\n1989 \n1990         Zarr chunks are determined in the following way:\n1991 \n1992         - From the ``chunks`` attribute in each variable's ``encoding``\n1993           (can be set via `Dataset.chunk`).\n1994         - If the variable is a Dask array, from the dask chunks\n1995         - If neither Dask chunks nor encoding chunks are present, chunks will\n1996           be determined automatically by Zarr\n1997         - If both Dask chunks and encoding chunks are present, encoding chunks\n1998           will be used, provided that there is a many-to-one relationship between\n1999           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and\n2000           evenly divide encoding chunks); otherwise raise a ``ValueError``.\n2001           This restriction ensures that no synchronization / locks are required\n2002           when writing. To disable this restriction, use ``safe_chunks=False``.\n2003 \n2004         Parameters\n2005         ----------\n2006         store : MutableMapping, str or path-like, optional\n2007             Store or path to directory in local or remote file system.\n2008         chunk_store : MutableMapping, str or path-like, optional\n2009             Store or path to directory in local or remote file system only for Zarr\n2010             array chunks. Requires zarr-python v2.4.0 or later.\n2011         mode : {\"w\", \"w-\", \"a\", \"r+\", None}, optional\n2012             Persistence mode: \"w\" means create (overwrite if exists);\n2013             \"w-\" means create (fail if exists);\n2014             \"a\" means override existing variables (create if does not exist);\n2015             \"r+\" means modify existing array *values* only (raise an error if\n2016             any metadata or shapes would change).\n2017             The default mode is \"a\" if ``append_dim`` is set. Otherwise, it is\n2018             \"r+\" if ``region`` is set and ``w-`` otherwise.\n2019         synchronizer : object, optional\n2020             Zarr array synchronizer.\n2021         group : str, optional\n2022             Group path. (a.k.a. `path` in zarr terminology.)\n2023         encoding : dict, optional\n2024             Nested dictionary with variable names as keys and dictionaries of\n2025             variable specific encodings as values, e.g.,\n2026             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n2027         compute : bool, default: True\n2028             If True write array data immediately, otherwise return a\n2029             ``dask.delayed.Delayed`` object that can be computed to write\n2030             array data later. Metadata is always updated eagerly.\n2031         consolidated : bool, optional\n2032             If True, apply zarr's `consolidate_metadata` function to the store\n2033             after writing metadata and read existing stores with consolidated\n2034             metadata; if False, do not. The default (`consolidated=None`) means\n2035             write consolidated metadata and attempt to read consolidated\n2036             metadata for existing stores (falling back to non-consolidated).\n2037 \n2038             When the experimental ``zarr_version=3``, ``consolidated`` must be\n2039             either be ``None`` or ``False``.\n2040         append_dim : hashable, optional\n2041             If set, the dimension along which the data will be appended. All\n2042             other dimensions on overridden variables must remain the same size.\n2043         region : dict, optional\n2044             Optional mapping from dimension names to integer slices along\n2045             dataset dimensions to indicate the region of existing zarr array(s)\n2046             in which to write this dataset's data. For example,\n2047             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate\n2048             that values should be written to the region ``0:1000`` along ``x``\n2049             and ``10000:11000`` along ``y``.\n2050 \n2051             Two restrictions apply to the use of ``region``:\n2052 \n2053             - If ``region`` is set, _all_ variables in a dataset must have at\n2054               least one dimension in common with the region. Other variables\n2055               should be written in a separate call to ``to_zarr()``.\n2056             - Dimensions cannot be included in both ``region`` and\n2057               ``append_dim`` at the same time. To create empty arrays to fill\n2058               in with ``region``, use a separate call to ``to_zarr()`` with\n2059               ``compute=False``. See \"Appending to existing Zarr stores\" in\n2060               the reference documentation for full details.\n2061         safe_chunks : bool, default: True\n2062             If True, only allow writes to when there is a many-to-one relationship\n2063             between Zarr chunks (specified in encoding) and Dask chunks.\n2064             Set False to override this restriction; however, data may become corrupted\n2065             if Zarr arrays are written in parallel. This option may be useful in combination\n2066             with ``compute=False`` to initialize a Zarr from an existing\n2067             Dataset with arbitrary chunk structure.\n2068         storage_options : dict, optional\n2069             Any additional parameters for the storage backend (ignored for local\n2070             paths).\n2071         zarr_version : int or None, optional\n2072             The desired zarr spec version to target (currently 2 or 3). The\n2073             default of None will attempt to determine the zarr version from\n2074             ``store`` when possible, otherwise defaulting to 2.\n2075 \n2076         Returns\n2077         -------\n2078             * ``dask.delayed.Delayed`` if compute is False\n2079             * ZarrStore otherwise\n2080 \n2081         References\n2082         ----------\n2083         https://zarr.readthedocs.io/\n2084 \n2085         Notes\n2086         -----\n2087         Zarr chunking behavior:\n2088             If chunks are found in the encoding argument or attribute\n2089             corresponding to any DataArray, those chunks are used.\n2090             If a DataArray is a dask array, it is written with those chunks.\n2091             If not other chunks are found, Zarr uses its own heuristics to\n2092             choose automatic chunk sizes.\n2093 \n2094         encoding:\n2095             The encoding attribute (if exists) of the DataArray(s) will be\n2096             used. Override any existing encodings by providing the ``encoding`` kwarg.\n2097 \n2098         See Also\n2099         --------\n2100         :ref:`io.zarr`\n2101             The I/O user guide, with more details and examples.\n2102         \"\"\"\n2103         from xarray.backends.api import to_zarr\n2104 \n2105         return to_zarr(  # type: ignore[call-overload,misc]\n2106             self,\n2107             store=store,\n2108             chunk_store=chunk_store,\n2109             storage_options=storage_options,\n2110             mode=mode,\n2111             synchronizer=synchronizer,\n2112             group=group,\n2113             encoding=encoding,\n2114             compute=compute,\n2115             consolidated=consolidated,\n2116             append_dim=append_dim,\n2117             region=region,\n2118             safe_chunks=safe_chunks,\n2119             zarr_version=zarr_version,\n2120         )\n2121 \n2122     def __repr__(self) -> str:\n2123         return formatting.dataset_repr(self)\n2124 \n2125     def _repr_html_(self) -> str:\n2126         if OPTIONS[\"display_style\"] == \"text\":\n2127             return f\"<pre>{escape(repr(self))}</pre>\"\n2128         return formatting_html.dataset_repr(self)\n2129 \n2130     def info(self, buf: IO | None = None) -> None:\n2131         \"\"\"\n2132         Concise summary of a Dataset variables and attributes.\n2133 \n2134         Parameters\n2135         ----------\n2136         buf : file-like, default: sys.stdout\n2137             writable buffer\n2138 \n2139         See Also\n2140         --------\n2141         pandas.DataFrame.assign\n2142         ncdump : netCDF's ncdump\n2143         \"\"\"\n2144         if buf is None:  # pragma: no cover\n2145             buf = sys.stdout\n2146 \n2147         lines = []\n2148         lines.append(\"xarray.Dataset {\")\n2149         lines.append(\"dimensions:\")\n2150         for name, size in self.dims.items():\n2151             lines.append(f\"\\t{name} = {size} ;\")\n2152         lines.append(\"\\nvariables:\")\n2153         for name, da in self.variables.items():\n2154             dims = \", \".join(map(str, da.dims))\n2155             lines.append(f\"\\t{da.dtype} {name}({dims}) ;\")\n2156             for k, v in da.attrs.items():\n2157                 lines.append(f\"\\t\\t{name}:{k} = {v} ;\")\n2158         lines.append(\"\\n// global attributes:\")\n2159         for k, v in self.attrs.items():\n2160             lines.append(f\"\\t:{k} = {v} ;\")\n2161         lines.append(\"}\")\n2162 \n2163         buf.write(\"\\n\".join(lines))\n2164 \n2165     @property\n2166     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:\n2167         \"\"\"\n2168         Mapping from dimension names to block lengths for this dataset's data, or None if\n2169         the underlying data is not a dask array.\n2170         Cannot be modified directly, but can be modified by calling .chunk().\n2171 \n2172         Same as Dataset.chunksizes, but maintained for backwards compatibility.\n2173 \n2174         See Also\n2175         --------\n2176         Dataset.chunk\n2177         Dataset.chunksizes\n2178         xarray.unify_chunks\n2179         \"\"\"\n2180         return get_chunksizes(self.variables.values())\n2181 \n2182     @property\n2183     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:\n2184         \"\"\"\n2185         Mapping from dimension names to block lengths for this dataset's data, or None if\n2186         the underlying data is not a dask array.\n2187         Cannot be modified directly, but can be modified by calling .chunk().\n2188 \n2189         Same as Dataset.chunks.\n2190 \n2191         See Also\n2192         --------\n2193         Dataset.chunk\n2194         Dataset.chunks\n2195         xarray.unify_chunks\n2196         \"\"\"\n2197         return get_chunksizes(self.variables.values())\n2198 \n2199     def chunk(\n2200         self: T_Dataset,\n2201         chunks: (\n2202             int | Literal[\"auto\"] | Mapping[Any, None | int | str | tuple[int, ...]]\n2203         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n2204         name_prefix: str = \"xarray-\",\n2205         token: str | None = None,\n2206         lock: bool = False,\n2207         inline_array: bool = False,\n2208         **chunks_kwargs: None | int | str | tuple[int, ...],\n2209     ) -> T_Dataset:\n2210         \"\"\"Coerce all arrays in this dataset into dask arrays with the given\n2211         chunks.\n2212 \n2213         Non-dask arrays in this dataset will be converted to dask arrays. Dask\n2214         arrays will be rechunked to the given chunk sizes.\n2215 \n2216         If neither chunks is not provided for one or more dimensions, chunk\n2217         sizes along that dimension will not be updated; non-dask arrays will be\n2218         converted into dask arrays with a single block.\n2219 \n2220         Parameters\n2221         ----------\n2222         chunks : int, tuple of int, \"auto\" or mapping of hashable to int, optional\n2223             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, or\n2224             ``{\"x\": 5, \"y\": 5}``.\n2225         name_prefix : str, default: \"xarray-\"\n2226             Prefix for the name of any new dask arrays.\n2227         token : str, optional\n2228             Token uniquely identifying this dataset.\n2229         lock : bool, default: False\n2230             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2231             already as dask array.\n2232         inline_array: bool, default: False\n2233             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2234             already as dask array.\n2235         **chunks_kwargs : {dim: chunks, ...}, optional\n2236             The keyword arguments form of ``chunks``.\n2237             One of chunks or chunks_kwargs must be provided\n2238 \n2239         Returns\n2240         -------\n2241         chunked : xarray.Dataset\n2242 \n2243         See Also\n2244         --------\n2245         Dataset.chunks\n2246         Dataset.chunksizes\n2247         xarray.unify_chunks\n2248         dask.array.from_array\n2249         \"\"\"\n2250         if chunks is None and chunks_kwargs is None:\n2251             warnings.warn(\n2252                 \"None value for 'chunks' is deprecated. \"\n2253                 \"It will raise an error in the future. Use instead '{}'\",\n2254                 category=FutureWarning,\n2255             )\n2256             chunks = {}\n2257 \n2258         if isinstance(chunks, (Number, str, int)):\n2259             chunks = dict.fromkeys(self.dims, chunks)\n2260         else:\n2261             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n2262 \n2263         bad_dims = chunks.keys() - self.dims.keys()\n2264         if bad_dims:\n2265             raise ValueError(\n2266                 f\"some chunks keys are not dimensions on this object: {bad_dims}\"\n2267             )\n2268 \n2269         variables = {\n2270             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)\n2271             for k, v in self.variables.items()\n2272         }\n2273         return self._replace(variables)\n2274 \n2275     def _validate_indexers(\n2276         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = \"raise\"\n2277     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:\n2278         \"\"\"Here we make sure\n2279         + indexer has a valid keys\n2280         + indexer is in a valid data type\n2281         + string indexers are cast to the appropriate date type if the\n2282           associated index is a DatetimeIndex or CFTimeIndex\n2283         \"\"\"\n2284         from xarray.coding.cftimeindex import CFTimeIndex\n2285         from xarray.core.dataarray import DataArray\n2286 \n2287         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n2288 \n2289         # all indexers should be int, slice, np.ndarrays, or Variable\n2290         for k, v in indexers.items():\n2291             if isinstance(v, (int, slice, Variable)):\n2292                 yield k, v\n2293             elif isinstance(v, DataArray):\n2294                 yield k, v.variable\n2295             elif isinstance(v, tuple):\n2296                 yield k, as_variable(v)\n2297             elif isinstance(v, Dataset):\n2298                 raise TypeError(\"cannot use a Dataset as an indexer\")\n2299             elif isinstance(v, Sequence) and len(v) == 0:\n2300                 yield k, np.empty((0,), dtype=\"int64\")\n2301             else:\n2302                 if not is_duck_array(v):\n2303                     v = np.asarray(v)\n2304 \n2305                 if v.dtype.kind in \"US\":\n2306                     index = self._indexes[k].to_pandas_index()\n2307                     if isinstance(index, pd.DatetimeIndex):\n2308                         v = v.astype(\"datetime64[ns]\")\n2309                     elif isinstance(index, CFTimeIndex):\n2310                         v = _parse_array_of_cftime_strings(v, index.date_type)\n2311 \n2312                 if v.ndim > 1:\n2313                     raise IndexError(\n2314                         \"Unlabeled multi-dimensional array cannot be \"\n2315                         \"used for indexing: {}\".format(k)\n2316                     )\n2317                 yield k, v\n2318 \n2319     def _validate_interp_indexers(\n2320         self, indexers: Mapping[Any, Any]\n2321     ) -> Iterator[tuple[Hashable, Variable]]:\n2322         \"\"\"Variant of _validate_indexers to be used for interpolation\"\"\"\n2323         for k, v in self._validate_indexers(indexers):\n2324             if isinstance(v, Variable):\n2325                 if v.ndim == 1:\n2326                     yield k, v.to_index_variable()\n2327                 else:\n2328                     yield k, v\n2329             elif isinstance(v, int):\n2330                 yield k, Variable((), v, attrs=self.coords[k].attrs)\n2331             elif isinstance(v, np.ndarray):\n2332                 if v.ndim == 0:\n2333                     yield k, Variable((), v, attrs=self.coords[k].attrs)\n2334                 elif v.ndim == 1:\n2335                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)\n2336                 else:\n2337                     raise AssertionError()  # Already tested by _validate_indexers\n2338             else:\n2339                 raise TypeError(type(v))\n2340 \n2341     def _get_indexers_coords_and_indexes(self, indexers):\n2342         \"\"\"Extract coordinates and indexes from indexers.\n2343 \n2344         Only coordinate with a name different from any of self.variables will\n2345         be attached.\n2346         \"\"\"\n2347         from xarray.core.dataarray import DataArray\n2348 \n2349         coords_list = []\n2350         for k, v in indexers.items():\n2351             if isinstance(v, DataArray):\n2352                 if v.dtype.kind == \"b\":\n2353                     if v.ndim != 1:  # we only support 1-d boolean array\n2354                         raise ValueError(\n2355                             \"{:d}d-boolean array is used for indexing along \"\n2356                             \"dimension {!r}, but only 1d boolean arrays are \"\n2357                             \"supported.\".format(v.ndim, k)\n2358                         )\n2359                     # Make sure in case of boolean DataArray, its\n2360                     # coordinate also should be indexed.\n2361                     v_coords = v[v.values.nonzero()[0]].coords\n2362                 else:\n2363                     v_coords = v.coords\n2364                 coords_list.append(v_coords)\n2365 \n2366         # we don't need to call align() explicitly or check indexes for\n2367         # alignment, because merge_variables already checks for exact alignment\n2368         # between dimension coordinates\n2369         coords, indexes = merge_coordinates_without_align(coords_list)\n2370         assert_coordinate_consistent(self, coords)\n2371 \n2372         # silently drop the conflicted variables.\n2373         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}\n2374         attached_indexes = {\n2375             k: v for k, v in indexes.items() if k not in self._variables\n2376         }\n2377         return attached_coords, attached_indexes\n2378 \n2379     def isel(\n2380         self: T_Dataset,\n2381         indexers: Mapping[Any, Any] | None = None,\n2382         drop: bool = False,\n2383         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2384         **indexers_kwargs: Any,\n2385     ) -> T_Dataset:\n2386         \"\"\"Returns a new dataset with each array indexed along the specified\n2387         dimension(s).\n2388 \n2389         This method selects values from each array using its `__getitem__`\n2390         method, except this method does not require knowing the order of\n2391         each array's dimensions.\n2392 \n2393         Parameters\n2394         ----------\n2395         indexers : dict, optional\n2396             A dict with keys matching dimensions and values given\n2397             by integers, slice objects or arrays.\n2398             indexer can be a integer, slice, array-like or DataArray.\n2399             If DataArrays are passed as indexers, xarray-style indexing will be\n2400             carried out. See :ref:`indexing` for the details.\n2401             One of indexers or indexers_kwargs must be provided.\n2402         drop : bool, default: False\n2403             If ``drop=True``, drop coordinates variables indexed by integers\n2404             instead of making them scalar.\n2405         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2406             What to do if dimensions that should be selected from are not present in the\n2407             Dataset:\n2408             - \"raise\": raise an exception\n2409             - \"warn\": raise a warning, and ignore the missing dimensions\n2410             - \"ignore\": ignore the missing dimensions\n2411 \n2412         **indexers_kwargs : {dim: indexer, ...}, optional\n2413             The keyword arguments form of ``indexers``.\n2414             One of indexers or indexers_kwargs must be provided.\n2415 \n2416         Returns\n2417         -------\n2418         obj : Dataset\n2419             A new Dataset with the same contents as this dataset, except each\n2420             array and dimension is indexed by the appropriate indexers.\n2421             If indexer DataArrays have coordinates that do not conflict with\n2422             this object, then these coordinates will be attached.\n2423             In general, each array's data will be a view of the array's data\n2424             in this dataset, unless vectorized indexing was triggered by using\n2425             an array indexer, in which case the data will be a copy.\n2426 \n2427         See Also\n2428         --------\n2429         Dataset.sel\n2430         DataArray.isel\n2431         \"\"\"\n2432         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n2433         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n2434             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n2435 \n2436         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n2437         # lists, or zero or one-dimensional np.ndarray's\n2438         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n2439 \n2440         variables = {}\n2441         dims: dict[Hashable, int] = {}\n2442         coord_names = self._coord_names.copy()\n2443 \n2444         indexes, index_variables = isel_indexes(self.xindexes, indexers)\n2445 \n2446         for name, var in self._variables.items():\n2447             # preserve variable order\n2448             if name in index_variables:\n2449                 var = index_variables[name]\n2450             else:\n2451                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}\n2452                 if var_indexers:\n2453                     var = var.isel(var_indexers)\n2454                     if drop and var.ndim == 0 and name in coord_names:\n2455                         coord_names.remove(name)\n2456                         continue\n2457             variables[name] = var\n2458             dims.update(zip(var.dims, var.shape))\n2459 \n2460         return self._construct_direct(\n2461             variables=variables,\n2462             coord_names=coord_names,\n2463             dims=dims,\n2464             attrs=self._attrs,\n2465             indexes=indexes,\n2466             encoding=self._encoding,\n2467             close=self._close,\n2468         )\n2469 \n2470     def _isel_fancy(\n2471         self: T_Dataset,\n2472         indexers: Mapping[Any, Any],\n2473         *,\n2474         drop: bool,\n2475         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2476     ) -> T_Dataset:\n2477         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))\n2478 \n2479         variables: dict[Hashable, Variable] = {}\n2480         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)\n2481 \n2482         for name, var in self.variables.items():\n2483             if name in index_variables:\n2484                 new_var = index_variables[name]\n2485             else:\n2486                 var_indexers = {\n2487                     k: v for k, v in valid_indexers.items() if k in var.dims\n2488                 }\n2489                 if var_indexers:\n2490                     new_var = var.isel(indexers=var_indexers)\n2491                     # drop scalar coordinates\n2492                     # https://github.com/pydata/xarray/issues/6554\n2493                     if name in self.coords and drop and new_var.ndim == 0:\n2494                         continue\n2495                 else:\n2496                     new_var = var.copy(deep=False)\n2497                 if name not in indexes:\n2498                     new_var = new_var.to_base_variable()\n2499             variables[name] = new_var\n2500 \n2501         coord_names = self._coord_names & variables.keys()\n2502         selected = self._replace_with_new_dims(variables, coord_names, indexes)\n2503 \n2504         # Extract coordinates from indexers\n2505         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n2506         variables.update(coord_vars)\n2507         indexes.update(new_indexes)\n2508         coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n2509         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2510 \n2511     def sel(\n2512         self: T_Dataset,\n2513         indexers: Mapping[Any, Any] | None = None,\n2514         method: str | None = None,\n2515         tolerance: int | float | Iterable[int | float] | None = None,\n2516         drop: bool = False,\n2517         **indexers_kwargs: Any,\n2518     ) -> T_Dataset:\n2519         \"\"\"Returns a new dataset with each array indexed by tick labels\n2520         along the specified dimension(s).\n2521 \n2522         In contrast to `Dataset.isel`, indexers for this method should use\n2523         labels instead of integers.\n2524 \n2525         Under the hood, this method is powered by using pandas's powerful Index\n2526         objects. This makes label based indexing essentially just as fast as\n2527         using integer indexing.\n2528 \n2529         It also means this method uses pandas's (well documented) logic for\n2530         indexing. This means you can use string shortcuts for datetime indexes\n2531         (e.g., '2000-01' to select all values in January 2000). It also means\n2532         that slices are treated as inclusive of both the start and stop values,\n2533         unlike normal Python indexing.\n2534 \n2535         Parameters\n2536         ----------\n2537         indexers : dict, optional\n2538             A dict with keys matching dimensions and values given\n2539             by scalars, slices or arrays of tick labels. For dimensions with\n2540             multi-index, the indexer may also be a dict-like object with keys\n2541             matching index level names.\n2542             If DataArrays are passed as indexers, xarray-style indexing will be\n2543             carried out. See :ref:`indexing` for the details.\n2544             One of indexers or indexers_kwargs must be provided.\n2545         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n2546             Method to use for inexact matches:\n2547 \n2548             * None (default): only exact matches\n2549             * pad / ffill: propagate last valid index value forward\n2550             * backfill / bfill: propagate next valid index value backward\n2551             * nearest: use nearest valid index value\n2552         tolerance : optional\n2553             Maximum distance between original and new labels for inexact\n2554             matches. The values of the index at the matching locations must\n2555             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2556         drop : bool, optional\n2557             If ``drop=True``, drop coordinates variables in `indexers` instead\n2558             of making them scalar.\n2559         **indexers_kwargs : {dim: indexer, ...}, optional\n2560             The keyword arguments form of ``indexers``.\n2561             One of indexers or indexers_kwargs must be provided.\n2562 \n2563         Returns\n2564         -------\n2565         obj : Dataset\n2566             A new Dataset with the same contents as this dataset, except each\n2567             variable and dimension is indexed by the appropriate indexers.\n2568             If indexer DataArrays have coordinates that do not conflict with\n2569             this object, then these coordinates will be attached.\n2570             In general, each array's data will be a view of the array's data\n2571             in this dataset, unless vectorized indexing was triggered by using\n2572             an array indexer, in which case the data will be a copy.\n2573 \n2574         See Also\n2575         --------\n2576         Dataset.isel\n2577         DataArray.sel\n2578         \"\"\"\n2579         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"sel\")\n2580         query_results = map_index_queries(\n2581             self, indexers=indexers, method=method, tolerance=tolerance\n2582         )\n2583 \n2584         if drop:\n2585             no_scalar_variables = {}\n2586             for k, v in query_results.variables.items():\n2587                 if v.dims:\n2588                     no_scalar_variables[k] = v\n2589                 else:\n2590                     if k in self._coord_names:\n2591                         query_results.drop_coords.append(k)\n2592             query_results.variables = no_scalar_variables\n2593 \n2594         result = self.isel(indexers=query_results.dim_indexers, drop=drop)\n2595         return result._overwrite_indexes(*query_results.as_tuple()[1:])\n2596 \n2597     def head(\n2598         self: T_Dataset,\n2599         indexers: Mapping[Any, int] | int | None = None,\n2600         **indexers_kwargs: Any,\n2601     ) -> T_Dataset:\n2602         \"\"\"Returns a new dataset with the first `n` values of each array\n2603         for the specified dimension(s).\n2604 \n2605         Parameters\n2606         ----------\n2607         indexers : dict or int, default: 5\n2608             A dict with keys matching dimensions and integer values `n`\n2609             or a single integer `n` applied over all dimensions.\n2610             One of indexers or indexers_kwargs must be provided.\n2611         **indexers_kwargs : {dim: n, ...}, optional\n2612             The keyword arguments form of ``indexers``.\n2613             One of indexers or indexers_kwargs must be provided.\n2614 \n2615         See Also\n2616         --------\n2617         Dataset.tail\n2618         Dataset.thin\n2619         DataArray.head\n2620         \"\"\"\n2621         if not indexers_kwargs:\n2622             if indexers is None:\n2623                 indexers = 5\n2624             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2625                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2626         if isinstance(indexers, int):\n2627             indexers = {dim: indexers for dim in self.dims}\n2628         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"head\")\n2629         for k, v in indexers.items():\n2630             if not isinstance(v, int):\n2631                 raise TypeError(\n2632                     \"expected integer type indexer for \"\n2633                     f\"dimension {k!r}, found {type(v)!r}\"\n2634                 )\n2635             elif v < 0:\n2636                 raise ValueError(\n2637                     \"expected positive integer as indexer \"\n2638                     f\"for dimension {k!r}, found {v}\"\n2639                 )\n2640         indexers_slices = {k: slice(val) for k, val in indexers.items()}\n2641         return self.isel(indexers_slices)\n2642 \n2643     def tail(\n2644         self: T_Dataset,\n2645         indexers: Mapping[Any, int] | int | None = None,\n2646         **indexers_kwargs: Any,\n2647     ) -> T_Dataset:\n2648         \"\"\"Returns a new dataset with the last `n` values of each array\n2649         for the specified dimension(s).\n2650 \n2651         Parameters\n2652         ----------\n2653         indexers : dict or int, default: 5\n2654             A dict with keys matching dimensions and integer values `n`\n2655             or a single integer `n` applied over all dimensions.\n2656             One of indexers or indexers_kwargs must be provided.\n2657         **indexers_kwargs : {dim: n, ...}, optional\n2658             The keyword arguments form of ``indexers``.\n2659             One of indexers or indexers_kwargs must be provided.\n2660 \n2661         See Also\n2662         --------\n2663         Dataset.head\n2664         Dataset.thin\n2665         DataArray.tail\n2666         \"\"\"\n2667         if not indexers_kwargs:\n2668             if indexers is None:\n2669                 indexers = 5\n2670             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2671                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2672         if isinstance(indexers, int):\n2673             indexers = {dim: indexers for dim in self.dims}\n2674         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"tail\")\n2675         for k, v in indexers.items():\n2676             if not isinstance(v, int):\n2677                 raise TypeError(\n2678                     \"expected integer type indexer for \"\n2679                     f\"dimension {k!r}, found {type(v)!r}\"\n2680                 )\n2681             elif v < 0:\n2682                 raise ValueError(\n2683                     \"expected positive integer as indexer \"\n2684                     f\"for dimension {k!r}, found {v}\"\n2685                 )\n2686         indexers_slices = {\n2687             k: slice(-val, None) if val != 0 else slice(val)\n2688             for k, val in indexers.items()\n2689         }\n2690         return self.isel(indexers_slices)\n2691 \n2692     def thin(\n2693         self: T_Dataset,\n2694         indexers: Mapping[Any, int] | int | None = None,\n2695         **indexers_kwargs: Any,\n2696     ) -> T_Dataset:\n2697         \"\"\"Returns a new dataset with each array indexed along every `n`-th\n2698         value for the specified dimension(s)\n2699 \n2700         Parameters\n2701         ----------\n2702         indexers : dict or int\n2703             A dict with keys matching dimensions and integer values `n`\n2704             or a single integer `n` applied over all dimensions.\n2705             One of indexers or indexers_kwargs must be provided.\n2706         **indexers_kwargs : {dim: n, ...}, optional\n2707             The keyword arguments form of ``indexers``.\n2708             One of indexers or indexers_kwargs must be provided.\n2709 \n2710         Examples\n2711         --------\n2712         >>> x_arr = np.arange(0, 26)\n2713         >>> x_arr\n2714         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n2715                17, 18, 19, 20, 21, 22, 23, 24, 25])\n2716         >>> x = xr.DataArray(\n2717         ...     np.reshape(x_arr, (2, 13)),\n2718         ...     dims=(\"x\", \"y\"),\n2719         ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n2720         ... )\n2721         >>> x_ds = xr.Dataset({\"foo\": x})\n2722         >>> x_ds\n2723         <xarray.Dataset>\n2724         Dimensions:  (x: 2, y: 13)\n2725         Coordinates:\n2726           * x        (x) int64 0 1\n2727           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n2728         Data variables:\n2729             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25\n2730 \n2731         >>> x_ds.thin(3)\n2732         <xarray.Dataset>\n2733         Dimensions:  (x: 1, y: 5)\n2734         Coordinates:\n2735           * x        (x) int64 0\n2736           * y        (y) int64 0 3 6 9 12\n2737         Data variables:\n2738             foo      (x, y) int64 0 3 6 9 12\n2739         >>> x.thin({\"x\": 2, \"y\": 5})\n2740         <xarray.DataArray (x: 1, y: 3)>\n2741         array([[ 0,  5, 10]])\n2742         Coordinates:\n2743           * x        (x) int64 0\n2744           * y        (y) int64 0 5 10\n2745 \n2746         See Also\n2747         --------\n2748         Dataset.head\n2749         Dataset.tail\n2750         DataArray.thin\n2751         \"\"\"\n2752         if (\n2753             not indexers_kwargs\n2754             and not isinstance(indexers, int)\n2755             and not is_dict_like(indexers)\n2756         ):\n2757             raise TypeError(\"indexers must be either dict-like or a single integer\")\n2758         if isinstance(indexers, int):\n2759             indexers = {dim: indexers for dim in self.dims}\n2760         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"thin\")\n2761         for k, v in indexers.items():\n2762             if not isinstance(v, int):\n2763                 raise TypeError(\n2764                     \"expected integer type indexer for \"\n2765                     f\"dimension {k!r}, found {type(v)!r}\"\n2766                 )\n2767             elif v < 0:\n2768                 raise ValueError(\n2769                     \"expected positive integer as indexer \"\n2770                     f\"for dimension {k!r}, found {v}\"\n2771                 )\n2772             elif v == 0:\n2773                 raise ValueError(\"step cannot be zero\")\n2774         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}\n2775         return self.isel(indexers_slices)\n2776 \n2777     def broadcast_like(\n2778         self: T_Dataset,\n2779         other: Dataset | DataArray,\n2780         exclude: Iterable[Hashable] | None = None,\n2781     ) -> T_Dataset:\n2782         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n2783         This is equivalent to xr.broadcast(other, self)[1]\n2784 \n2785         Parameters\n2786         ----------\n2787         other : Dataset or DataArray\n2788             Object against which to broadcast this array.\n2789         exclude : iterable of hashable, optional\n2790             Dimensions that must not be broadcasted\n2791 \n2792         \"\"\"\n2793         if exclude is None:\n2794             exclude = set()\n2795         else:\n2796             exclude = set(exclude)\n2797         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n2798 \n2799         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n2800 \n2801         return _broadcast_helper(\n2802             cast(\"T_Dataset\", args[1]), exclude, dims_map, common_coords\n2803         )\n2804 \n2805     def _reindex_callback(\n2806         self,\n2807         aligner: alignment.Aligner,\n2808         dim_pos_indexers: dict[Hashable, Any],\n2809         variables: dict[Hashable, Variable],\n2810         indexes: dict[Hashable, Index],\n2811         fill_value: Any,\n2812         exclude_dims: frozenset[Hashable],\n2813         exclude_vars: frozenset[Hashable],\n2814     ) -> Dataset:\n2815         \"\"\"Callback called from ``Aligner`` to create a new reindexed Dataset.\"\"\"\n2816 \n2817         new_variables = variables.copy()\n2818         new_indexes = indexes.copy()\n2819 \n2820         # re-assign variable metadata\n2821         for name, new_var in new_variables.items():\n2822             var = self._variables.get(name)\n2823             if var is not None:\n2824                 new_var.attrs = var.attrs\n2825                 new_var.encoding = var.encoding\n2826 \n2827         # pass through indexes from excluded dimensions\n2828         # no extra check needed for multi-coordinate indexes, potential conflicts\n2829         # should already have been detected when aligning the indexes\n2830         for name, idx in self._indexes.items():\n2831             var = self._variables[name]\n2832             if set(var.dims) <= exclude_dims:\n2833                 new_indexes[name] = idx\n2834                 new_variables[name] = var\n2835 \n2836         if not dim_pos_indexers:\n2837             # fast path for no reindexing necessary\n2838             if set(new_indexes) - set(self._indexes):\n2839                 # this only adds new indexes and their coordinate variables\n2840                 reindexed = self._overwrite_indexes(new_indexes, new_variables)\n2841             else:\n2842                 reindexed = self.copy(deep=aligner.copy)\n2843         else:\n2844             to_reindex = {\n2845                 k: v\n2846                 for k, v in self.variables.items()\n2847                 if k not in variables and k not in exclude_vars\n2848             }\n2849             reindexed_vars = alignment.reindex_variables(\n2850                 to_reindex,\n2851                 dim_pos_indexers,\n2852                 copy=aligner.copy,\n2853                 fill_value=fill_value,\n2854                 sparse=aligner.sparse,\n2855             )\n2856             new_variables.update(reindexed_vars)\n2857             new_coord_names = self._coord_names | set(new_indexes)\n2858             reindexed = self._replace_with_new_dims(\n2859                 new_variables, new_coord_names, indexes=new_indexes\n2860             )\n2861 \n2862         return reindexed\n2863 \n2864     def reindex_like(\n2865         self: T_Dataset,\n2866         other: Dataset | DataArray,\n2867         method: ReindexMethodOptions = None,\n2868         tolerance: int | float | Iterable[int | float] | None = None,\n2869         copy: bool = True,\n2870         fill_value: Any = xrdtypes.NA,\n2871     ) -> T_Dataset:\n2872         \"\"\"Conform this object onto the indexes of another object, filling in\n2873         missing values with ``fill_value``. The default fill value is NaN.\n2874 \n2875         Parameters\n2876         ----------\n2877         other : Dataset or DataArray\n2878             Object with an 'indexes' attribute giving a mapping from dimension\n2879             names to pandas.Index objects, which provides coordinates upon\n2880             which to index the variables in this dataset. The indexes on this\n2881             other object need not be the same as the indexes on this\n2882             dataset. Any mis-matched index values will be filled in with\n2883             NaN, and any mis-matched dimension names will simply be ignored.\n2884         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n2885             Method to use for filling index values from other not found in this\n2886             dataset:\n2887 \n2888             - None (default): don't fill gaps\n2889             - \"pad\" / \"ffill\": propagate last valid index value forward\n2890             - \"backfill\" / \"bfill\": propagate next valid index value backward\n2891             - \"nearest\": use nearest valid index value\n2892 \n2893         tolerance : optional\n2894             Maximum distance between original and new labels for inexact\n2895             matches. The values of the index at the matching locations must\n2896             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2897             Tolerance may be a scalar value, which applies the same tolerance\n2898             to all values, or list-like, which applies variable tolerance per\n2899             element. List-like must be the same size as the index and its dtype\n2900             must exactly match the index\u2019s type.\n2901         copy : bool, default: True\n2902             If ``copy=True``, data in the return value is always copied. If\n2903             ``copy=False`` and reindexing is unnecessary, or can be performed\n2904             with only slice operations, then the output may share memory with\n2905             the input. In either case, a new xarray object is always returned.\n2906         fill_value : scalar or dict-like, optional\n2907             Value to use for newly missing values. If a dict-like maps\n2908             variable names to fill values.\n2909 \n2910         Returns\n2911         -------\n2912         reindexed : Dataset\n2913             Another dataset, with this dataset's data but coordinates from the\n2914             other object.\n2915 \n2916         See Also\n2917         --------\n2918         Dataset.reindex\n2919         align\n2920         \"\"\"\n2921         return alignment.reindex_like(\n2922             self,\n2923             other=other,\n2924             method=method,\n2925             tolerance=tolerance,\n2926             copy=copy,\n2927             fill_value=fill_value,\n2928         )\n2929 \n2930     def reindex(\n2931         self: T_Dataset,\n2932         indexers: Mapping[Any, Any] | None = None,\n2933         method: ReindexMethodOptions = None,\n2934         tolerance: int | float | Iterable[int | float] | None = None,\n2935         copy: bool = True,\n2936         fill_value: Any = xrdtypes.NA,\n2937         **indexers_kwargs: Any,\n2938     ) -> T_Dataset:\n2939         \"\"\"Conform this object onto a new set of indexes, filling in\n2940         missing values with ``fill_value``. The default fill value is NaN.\n2941 \n2942         Parameters\n2943         ----------\n2944         indexers : dict, optional\n2945             Dictionary with keys given by dimension names and values given by\n2946             arrays of coordinates tick labels. Any mis-matched coordinate\n2947             values will be filled in with NaN, and any mis-matched dimension\n2948             names will simply be ignored.\n2949             One of indexers or indexers_kwargs must be provided.\n2950         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n2951             Method to use for filling index values in ``indexers`` not found in\n2952             this dataset:\n2953 \n2954             - None (default): don't fill gaps\n2955             - \"pad\" / \"ffill\": propagate last valid index value forward\n2956             - \"backfill\" / \"bfill\": propagate next valid index value backward\n2957             - \"nearest\": use nearest valid index value\n2958 \n2959         tolerance : optional\n2960             Maximum distance between original and new labels for inexact\n2961             matches. The values of the index at the matching locations must\n2962             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2963             Tolerance may be a scalar value, which applies the same tolerance\n2964             to all values, or list-like, which applies variable tolerance per\n2965             element. List-like must be the same size as the index and its dtype\n2966             must exactly match the index\u2019s type.\n2967         copy : bool, default: True\n2968             If ``copy=True``, data in the return value is always copied. If\n2969             ``copy=False`` and reindexing is unnecessary, or can be performed\n2970             with only slice operations, then the output may share memory with\n2971             the input. In either case, a new xarray object is always returned.\n2972         fill_value : scalar or dict-like, optional\n2973             Value to use for newly missing values. If a dict-like,\n2974             maps variable names (including coordinates) to fill values.\n2975         sparse : bool, default: False\n2976             use sparse-array.\n2977         **indexers_kwargs : {dim: indexer, ...}, optional\n2978             Keyword arguments in the same form as ``indexers``.\n2979             One of indexers or indexers_kwargs must be provided.\n2980 \n2981         Returns\n2982         -------\n2983         reindexed : Dataset\n2984             Another dataset, with this dataset's data but replaced coordinates.\n2985 \n2986         See Also\n2987         --------\n2988         Dataset.reindex_like\n2989         align\n2990         pandas.Index.get_indexer\n2991 \n2992         Examples\n2993         --------\n2994         Create a dataset with some fictional data.\n2995 \n2996         >>> x = xr.Dataset(\n2997         ...     {\n2998         ...         \"temperature\": (\"station\", 20 * np.random.rand(4)),\n2999         ...         \"pressure\": (\"station\", 500 * np.random.rand(4)),\n3000         ...     },\n3001         ...     coords={\"station\": [\"boston\", \"nyc\", \"seattle\", \"denver\"]},\n3002         ... )\n3003         >>> x\n3004         <xarray.Dataset>\n3005         Dimensions:      (station: 4)\n3006         Coordinates:\n3007           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'\n3008         Data variables:\n3009             temperature  (station) float64 10.98 14.3 12.06 10.9\n3010             pressure     (station) float64 211.8 322.9 218.8 445.9\n3011         >>> x.indexes\n3012         Indexes:\n3013             station  Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')\n3014 \n3015         Create a new index and reindex the dataset. By default values in the new index that\n3016         do not have corresponding records in the dataset are assigned `NaN`.\n3017 \n3018         >>> new_index = [\"boston\", \"austin\", \"seattle\", \"lincoln\"]\n3019         >>> x.reindex({\"station\": new_index})\n3020         <xarray.Dataset>\n3021         Dimensions:      (station: 4)\n3022         Coordinates:\n3023           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n3024         Data variables:\n3025             temperature  (station) float64 10.98 nan 12.06 nan\n3026             pressure     (station) float64 211.8 nan 218.8 nan\n3027 \n3028         We can fill in the missing values by passing a value to the keyword `fill_value`.\n3029 \n3030         >>> x.reindex({\"station\": new_index}, fill_value=0)\n3031         <xarray.Dataset>\n3032         Dimensions:      (station: 4)\n3033         Coordinates:\n3034           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n3035         Data variables:\n3036             temperature  (station) float64 10.98 0.0 12.06 0.0\n3037             pressure     (station) float64 211.8 0.0 218.8 0.0\n3038 \n3039         We can also use different fill values for each variable.\n3040 \n3041         >>> x.reindex(\n3042         ...     {\"station\": new_index}, fill_value={\"temperature\": 0, \"pressure\": 100}\n3043         ... )\n3044         <xarray.Dataset>\n3045         Dimensions:      (station: 4)\n3046         Coordinates:\n3047           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n3048         Data variables:\n3049             temperature  (station) float64 10.98 0.0 12.06 0.0\n3050             pressure     (station) float64 211.8 100.0 218.8 100.0\n3051 \n3052         Because the index is not monotonically increasing or decreasing, we cannot use arguments\n3053         to the keyword method to fill the `NaN` values.\n3054 \n3055         >>> x.reindex({\"station\": new_index}, method=\"nearest\")\n3056         Traceback (most recent call last):\n3057         ...\n3058             raise ValueError('index must be monotonic increasing or decreasing')\n3059         ValueError: index must be monotonic increasing or decreasing\n3060 \n3061         To further illustrate the filling functionality in reindex, we will create a\n3062         dataset with a monotonically increasing index (for example, a sequence of dates).\n3063 \n3064         >>> x2 = xr.Dataset(\n3065         ...     {\n3066         ...         \"temperature\": (\n3067         ...             \"time\",\n3068         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],\n3069         ...         ),\n3070         ...         \"pressure\": (\"time\", 500 * np.random.rand(6)),\n3071         ...     },\n3072         ...     coords={\"time\": pd.date_range(\"01/01/2019\", periods=6, freq=\"D\")},\n3073         ... )\n3074         >>> x2\n3075         <xarray.Dataset>\n3076         Dimensions:      (time: 6)\n3077         Coordinates:\n3078           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n3079         Data variables:\n3080             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n3081             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8\n3082 \n3083         Suppose we decide to expand the dataset to cover a wider date range.\n3084 \n3085         >>> time_index2 = pd.date_range(\"12/29/2018\", periods=10, freq=\"D\")\n3086         >>> x2.reindex({\"time\": time_index2})\n3087         <xarray.Dataset>\n3088         Dimensions:      (time: 10)\n3089         Coordinates:\n3090           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n3091         Data variables:\n3092             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n3093             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan\n3094 \n3095         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n3096         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n3097 \n3098         For example, to back-propagate the last valid value to fill the `NaN` values,\n3099         pass `bfill` as an argument to the `method` keyword.\n3100 \n3101         >>> x3 = x2.reindex({\"time\": time_index2}, method=\"bfill\")\n3102         >>> x3\n3103         <xarray.Dataset>\n3104         Dimensions:      (time: 10)\n3105         Coordinates:\n3106           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n3107         Data variables:\n3108             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n3109             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan\n3110 \n3111         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n3112         will not be filled by any of the value propagation schemes.\n3113 \n3114         >>> x2.where(x2.temperature.isnull(), drop=True)\n3115         <xarray.Dataset>\n3116         Dimensions:      (time: 1)\n3117         Coordinates:\n3118           * time         (time) datetime64[ns] 2019-01-03\n3119         Data variables:\n3120             temperature  (time) float64 nan\n3121             pressure     (time) float64 395.9\n3122         >>> x3.where(x3.temperature.isnull(), drop=True)\n3123         <xarray.Dataset>\n3124         Dimensions:      (time: 2)\n3125         Coordinates:\n3126           * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n3127         Data variables:\n3128             temperature  (time) float64 nan nan\n3129             pressure     (time) float64 395.9 nan\n3130 \n3131         This is because filling while reindexing does not look at dataset values, but only compares\n3132         the original and desired indexes. If you do want to fill in the `NaN` values present in the\n3133         original dataset, use the :py:meth:`~Dataset.fillna()` method.\n3134 \n3135         \"\"\"\n3136         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n3137         return alignment.reindex(\n3138             self,\n3139             indexers=indexers,\n3140             method=method,\n3141             tolerance=tolerance,\n3142             copy=copy,\n3143             fill_value=fill_value,\n3144         )\n3145 \n3146     def _reindex(\n3147         self: T_Dataset,\n3148         indexers: Mapping[Any, Any] | None = None,\n3149         method: str | None = None,\n3150         tolerance: int | float | Iterable[int | float] | None = None,\n3151         copy: bool = True,\n3152         fill_value: Any = xrdtypes.NA,\n3153         sparse: bool = False,\n3154         **indexers_kwargs: Any,\n3155     ) -> T_Dataset:\n3156         \"\"\"\n3157         Same as reindex but supports sparse option.\n3158         \"\"\"\n3159         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n3160         return alignment.reindex(\n3161             self,\n3162             indexers=indexers,\n3163             method=method,\n3164             tolerance=tolerance,\n3165             copy=copy,\n3166             fill_value=fill_value,\n3167             sparse=sparse,\n3168         )\n3169 \n3170     def interp(\n3171         self: T_Dataset,\n3172         coords: Mapping[Any, Any] | None = None,\n3173         method: InterpOptions = \"linear\",\n3174         assume_sorted: bool = False,\n3175         kwargs: Mapping[str, Any] | None = None,\n3176         method_non_numeric: str = \"nearest\",\n3177         **coords_kwargs: Any,\n3178     ) -> T_Dataset:\n3179         \"\"\"Interpolate a Dataset onto new coordinates\n3180 \n3181         Performs univariate or multivariate interpolation of a Dataset onto\n3182         new coordinates using scipy's interpolation routines. If interpolating\n3183         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n3184         called.  When interpolating along multiple existing dimensions, an\n3185         attempt is made to decompose the interpolation into multiple\n3186         1-dimensional interpolations. If this is possible,\n3187         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n3188         :py:func:`scipy.interpolate.interpn` is called.\n3189 \n3190         Parameters\n3191         ----------\n3192         coords : dict, optional\n3193             Mapping from dimension names to the new coordinates.\n3194             New coordinate can be a scalar, array-like or DataArray.\n3195             If DataArrays are passed as new coordinates, their dimensions are\n3196             used for the broadcasting. Missing values are skipped.\n3197         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3198             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3199             String indicating which method to use for interpolation:\n3200 \n3201             - 'linear': linear interpolation. Additional keyword\n3202               arguments are passed to :py:func:`numpy.interp`\n3203             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3204               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3205               ``method='polynomial'``, the ``order`` keyword argument must also be\n3206               provided.\n3207             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3208               respective :py:class:`scipy.interpolate` classes.\n3209 \n3210         assume_sorted : bool, default: False\n3211             If False, values of coordinates that are interpolated over can be\n3212             in any order and they are sorted first. If True, interpolated\n3213             coordinates are assumed to be an array of monotonically increasing\n3214             values.\n3215         kwargs : dict, optional\n3216             Additional keyword arguments passed to scipy's interpolator. Valid\n3217             options and their behavior depend whether ``interp1d`` or\n3218             ``interpn`` is used.\n3219         method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n3220             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n3221             ``\"nearest\"`` is used by default.\n3222         **coords_kwargs : {dim: coordinate, ...}, optional\n3223             The keyword arguments form of ``coords``.\n3224             One of coords or coords_kwargs must be provided.\n3225 \n3226         Returns\n3227         -------\n3228         interpolated : Dataset\n3229             New dataset on the new coordinates.\n3230 \n3231         Notes\n3232         -----\n3233         scipy is required.\n3234 \n3235         See Also\n3236         --------\n3237         scipy.interpolate.interp1d\n3238         scipy.interpolate.interpn\n3239 \n3240         Examples\n3241         --------\n3242         >>> ds = xr.Dataset(\n3243         ...     data_vars={\n3244         ...         \"a\": (\"x\", [5, 7, 4]),\n3245         ...         \"b\": (\n3246         ...             (\"x\", \"y\"),\n3247         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n3248         ...         ),\n3249         ...     },\n3250         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n3251         ... )\n3252         >>> ds\n3253         <xarray.Dataset>\n3254         Dimensions:  (x: 3, y: 4)\n3255         Coordinates:\n3256           * x        (x) int64 0 1 2\n3257           * y        (y) int64 10 12 14 16\n3258         Data variables:\n3259             a        (x) int64 5 7 4\n3260             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0\n3261 \n3262         1D interpolation with the default method (linear):\n3263 \n3264         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])\n3265         <xarray.Dataset>\n3266         Dimensions:  (x: 4, y: 4)\n3267         Coordinates:\n3268           * y        (y) int64 10 12 14 16\n3269           * x        (x) float64 0.0 0.75 1.25 1.75\n3270         Data variables:\n3271             a        (x) float64 5.0 6.5 6.25 4.75\n3272             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan\n3273 \n3274         1D interpolation with a different method:\n3275 \n3276         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n3277         <xarray.Dataset>\n3278         Dimensions:  (x: 4, y: 4)\n3279         Coordinates:\n3280           * y        (y) int64 10 12 14 16\n3281           * x        (x) float64 0.0 0.75 1.25 1.75\n3282         Data variables:\n3283             a        (x) float64 5.0 7.0 7.0 4.0\n3284             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0\n3285 \n3286         1D extrapolation:\n3287 \n3288         >>> ds.interp(\n3289         ...     x=[1, 1.5, 2.5, 3.5],\n3290         ...     method=\"linear\",\n3291         ...     kwargs={\"fill_value\": \"extrapolate\"},\n3292         ... )\n3293         <xarray.Dataset>\n3294         Dimensions:  (x: 4, y: 4)\n3295         Coordinates:\n3296           * y        (y) int64 10 12 14 16\n3297           * x        (x) float64 1.0 1.5 2.5 3.5\n3298         Data variables:\n3299             a        (x) float64 7.0 5.5 2.5 -0.5\n3300             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan\n3301 \n3302         2D interpolation:\n3303 \n3304         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n3305         <xarray.Dataset>\n3306         Dimensions:  (x: 4, y: 3)\n3307         Coordinates:\n3308           * x        (x) float64 0.0 0.75 1.25 1.75\n3309           * y        (y) int64 11 13 15\n3310         Data variables:\n3311             a        (x) float64 5.0 6.5 6.25 4.75\n3312             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan\n3313         \"\"\"\n3314         from xarray.core import missing\n3315 \n3316         if kwargs is None:\n3317             kwargs = {}\n3318 \n3319         coords = either_dict_or_kwargs(coords, coords_kwargs, \"interp\")\n3320         indexers = dict(self._validate_interp_indexers(coords))\n3321 \n3322         if coords:\n3323             # This avoids broadcasting over coordinates that are both in\n3324             # the original array AND in the indexing array. It essentially\n3325             # forces interpolation along the shared coordinates.\n3326             sdims = (\n3327                 set(self.dims)\n3328                 .intersection(*[set(nx.dims) for nx in indexers.values()])\n3329                 .difference(coords.keys())\n3330             )\n3331             indexers.update({d: self.variables[d] for d in sdims})\n3332 \n3333         obj = self if assume_sorted else self.sortby([k for k in coords])\n3334 \n3335         def maybe_variable(obj, k):\n3336             # workaround to get variable for dimension without coordinate.\n3337             try:\n3338                 return obj._variables[k]\n3339             except KeyError:\n3340                 return as_variable((k, range(obj.dims[k])))\n3341 \n3342         def _validate_interp_indexer(x, new_x):\n3343             # In the case of datetimes, the restrictions placed on indexers\n3344             # used with interp are stronger than those which are placed on\n3345             # isel, so we need an additional check after _validate_indexers.\n3346             if _contains_datetime_like_objects(\n3347                 x\n3348             ) and not _contains_datetime_like_objects(new_x):\n3349                 raise TypeError(\n3350                     \"When interpolating over a datetime-like \"\n3351                     \"coordinate, the coordinates to \"\n3352                     \"interpolate to must be either datetime \"\n3353                     \"strings or datetimes. \"\n3354                     \"Instead got\\n{}\".format(new_x)\n3355                 )\n3356             return x, new_x\n3357 \n3358         validated_indexers = {\n3359             k: _validate_interp_indexer(maybe_variable(obj, k), v)\n3360             for k, v in indexers.items()\n3361         }\n3362 \n3363         # optimization: subset to coordinate range of the target index\n3364         if method in [\"linear\", \"nearest\"]:\n3365             for k, v in validated_indexers.items():\n3366                 obj, newidx = missing._localize(obj, {k: v})\n3367                 validated_indexers[k] = newidx[k]\n3368 \n3369         # optimization: create dask coordinate arrays once per Dataset\n3370         # rather than once per Variable when dask.array.unify_chunks is called later\n3371         # GH4739\n3372         if obj.__dask_graph__():\n3373             dask_indexers = {\n3374                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())\n3375                 for k, (index, dest) in validated_indexers.items()\n3376             }\n3377 \n3378         variables: dict[Hashable, Variable] = {}\n3379         reindex: bool = False\n3380         for name, var in obj._variables.items():\n3381             if name in indexers:\n3382                 continue\n3383 \n3384             if is_duck_dask_array(var.data):\n3385                 use_indexers = dask_indexers\n3386             else:\n3387                 use_indexers = validated_indexers\n3388 \n3389             dtype_kind = var.dtype.kind\n3390             if dtype_kind in \"uifc\":\n3391                 # For normal number types do the interpolation:\n3392                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}\n3393                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)\n3394             elif dtype_kind in \"ObU\" and (use_indexers.keys() & var.dims):\n3395                 # For types that we do not understand do stepwise\n3396                 # interpolation to avoid modifying the elements.\n3397                 # reindex the variable instead because it supports\n3398                 # booleans and objects and retains the dtype but inside\n3399                 # this loop there might be some duplicate code that slows it\n3400                 # down, therefore collect these signals and run it later:\n3401                 reindex = True\n3402             elif all(d not in indexers for d in var.dims):\n3403                 # For anything else we can only keep variables if they\n3404                 # are not dependent on any coords that are being\n3405                 # interpolated along:\n3406                 variables[name] = var\n3407 \n3408         if reindex:\n3409             reindex_indexers = {\n3410                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)\n3411             }\n3412             reindexed = alignment.reindex(\n3413                 obj,\n3414                 indexers=reindex_indexers,\n3415                 method=method_non_numeric,\n3416                 exclude_vars=variables.keys(),\n3417             )\n3418             indexes = dict(reindexed._indexes)\n3419             variables.update(reindexed.variables)\n3420         else:\n3421             # Get the indexes that are not being interpolated along\n3422             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}\n3423 \n3424         # Get the coords that also exist in the variables:\n3425         coord_names = obj._coord_names & variables.keys()\n3426         selected = self._replace_with_new_dims(\n3427             variables.copy(), coord_names, indexes=indexes\n3428         )\n3429 \n3430         # Attach indexer as coordinate\n3431         for k, v in indexers.items():\n3432             assert isinstance(v, Variable)\n3433             if v.dims == (k,):\n3434                 index = PandasIndex(v, k, coord_dtype=v.dtype)\n3435                 index_vars = index.create_variables({k: v})\n3436                 indexes[k] = index\n3437                 variables.update(index_vars)\n3438             else:\n3439                 variables[k] = v\n3440 \n3441         # Extract coordinates from indexers\n3442         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)\n3443         variables.update(coord_vars)\n3444         indexes.update(new_indexes)\n3445 \n3446         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()\n3447         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n3448 \n3449     def interp_like(\n3450         self,\n3451         other: Dataset | DataArray,\n3452         method: InterpOptions = \"linear\",\n3453         assume_sorted: bool = False,\n3454         kwargs: Mapping[str, Any] | None = None,\n3455         method_non_numeric: str = \"nearest\",\n3456     ) -> Dataset:\n3457         \"\"\"Interpolate this object onto the coordinates of another object,\n3458         filling the out of range values with NaN.\n3459 \n3460         If interpolating along a single existing dimension,\n3461         :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n3462         along multiple existing dimensions, an attempt is made to decompose the\n3463         interpolation into multiple 1-dimensional interpolations. If this is\n3464         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n3465         :py:func:`scipy.interpolate.interpn` is called.\n3466 \n3467         Parameters\n3468         ----------\n3469         other : Dataset or DataArray\n3470             Object with an 'indexes' attribute giving a mapping from dimension\n3471             names to an 1d array-like, which provides coordinates upon\n3472             which to index the variables in this dataset. Missing values are skipped.\n3473         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3474             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3475             String indicating which method to use for interpolation:\n3476 \n3477             - 'linear': linear interpolation. Additional keyword\n3478               arguments are passed to :py:func:`numpy.interp`\n3479             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3480               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3481               ``method='polynomial'``, the ``order`` keyword argument must also be\n3482               provided.\n3483             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3484               respective :py:class:`scipy.interpolate` classes.\n3485 \n3486         assume_sorted : bool, default: False\n3487             If False, values of coordinates that are interpolated over can be\n3488             in any order and they are sorted first. If True, interpolated\n3489             coordinates are assumed to be an array of monotonically increasing\n3490             values.\n3491         kwargs : dict, optional\n3492             Additional keyword passed to scipy's interpolator.\n3493         method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n3494             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n3495             ``\"nearest\"`` is used by default.\n3496 \n3497         Returns\n3498         -------\n3499         interpolated : Dataset\n3500             Another dataset by interpolating this dataset's data along the\n3501             coordinates of the other object.\n3502 \n3503         Notes\n3504         -----\n3505         scipy is required.\n3506         If the dataset has object-type coordinates, reindex is used for these\n3507         coordinates instead of the interpolation.\n3508 \n3509         See Also\n3510         --------\n3511         Dataset.interp\n3512         Dataset.reindex_like\n3513         \"\"\"\n3514         if kwargs is None:\n3515             kwargs = {}\n3516 \n3517         # pick only dimension coordinates with a single index\n3518         coords = {}\n3519         other_indexes = other.xindexes\n3520         for dim in self.dims:\n3521             other_dim_coords = other_indexes.get_all_coords(dim, errors=\"ignore\")\n3522             if len(other_dim_coords) == 1:\n3523                 coords[dim] = other_dim_coords[dim]\n3524 \n3525         numeric_coords: dict[Hashable, pd.Index] = {}\n3526         object_coords: dict[Hashable, pd.Index] = {}\n3527         for k, v in coords.items():\n3528             if v.dtype.kind in \"uifcMm\":\n3529                 numeric_coords[k] = v\n3530             else:\n3531                 object_coords[k] = v\n3532 \n3533         ds = self\n3534         if object_coords:\n3535             # We do not support interpolation along object coordinate.\n3536             # reindex instead.\n3537             ds = self.reindex(object_coords)\n3538         return ds.interp(\n3539             coords=numeric_coords,\n3540             method=method,\n3541             assume_sorted=assume_sorted,\n3542             kwargs=kwargs,\n3543             method_non_numeric=method_non_numeric,\n3544         )\n3545 \n3546     # Helper methods for rename()\n3547     def _rename_vars(\n3548         self, name_dict, dims_dict\n3549     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:\n3550         variables = {}\n3551         coord_names = set()\n3552         for k, v in self.variables.items():\n3553             var = v.copy(deep=False)\n3554             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n3555             name = name_dict.get(k, k)\n3556             if name in variables:\n3557                 raise ValueError(f\"the new name {name!r} conflicts\")\n3558             variables[name] = var\n3559             if k in self._coord_names:\n3560                 coord_names.add(name)\n3561         return variables, coord_names\n3562 \n3563     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:\n3564         return {name_dict.get(k, k): v for k, v in self.dims.items()}\n3565 \n3566     def _rename_indexes(\n3567         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]\n3568     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n3569         if not self._indexes:\n3570             return {}, {}\n3571 \n3572         indexes = {}\n3573         variables = {}\n3574 \n3575         for index, coord_names in self.xindexes.group_by_index():\n3576             new_index = index.rename(name_dict, dims_dict)\n3577             new_coord_names = [name_dict.get(k, k) for k in coord_names]\n3578             indexes.update({k: new_index for k in new_coord_names})\n3579             new_index_vars = new_index.create_variables(\n3580                 {\n3581                     new: self._variables[old]\n3582                     for old, new in zip(coord_names, new_coord_names)\n3583                 }\n3584             )\n3585             variables.update(new_index_vars)\n3586 \n3587         return indexes, variables\n3588 \n3589     def _rename_all(\n3590         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]\n3591     ) -> tuple[\n3592         dict[Hashable, Variable],\n3593         set[Hashable],\n3594         dict[Hashable, int],\n3595         dict[Hashable, Index],\n3596     ]:\n3597         variables, coord_names = self._rename_vars(name_dict, dims_dict)\n3598         dims = self._rename_dims(dims_dict)\n3599 \n3600         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)\n3601         variables = {k: index_vars.get(k, v) for k, v in variables.items()}\n3602 \n3603         return variables, coord_names, dims, indexes\n3604 \n3605     def _rename(\n3606         self: T_Dataset,\n3607         name_dict: Mapping[Any, Hashable] | None = None,\n3608         **names: Hashable,\n3609     ) -> T_Dataset:\n3610         \"\"\"Also used internally by DataArray so that the warning (if any)\n3611         is raised at the right stack level.\n3612         \"\"\"\n3613         name_dict = either_dict_or_kwargs(name_dict, names, \"rename\")\n3614         for k in name_dict.keys():\n3615             if k not in self and k not in self.dims:\n3616                 raise ValueError(\n3617                     f\"cannot rename {k!r} because it is not a \"\n3618                     \"variable or dimension in this dataset\"\n3619                 )\n3620 \n3621             create_dim_coord = False\n3622             new_k = name_dict[k]\n3623 \n3624             if k in self.dims and new_k in self._coord_names:\n3625                 coord_dims = self._variables[name_dict[k]].dims\n3626                 if coord_dims == (k,):\n3627                     create_dim_coord = True\n3628             elif k in self._coord_names and new_k in self.dims:\n3629                 coord_dims = self._variables[k].dims\n3630                 if coord_dims == (new_k,):\n3631                     create_dim_coord = True\n3632 \n3633             if create_dim_coord:\n3634                 warnings.warn(\n3635                     f\"rename {k!r} to {name_dict[k]!r} does not create an index \"\n3636                     \"anymore. Try using swap_dims instead or use set_index \"\n3637                     \"after rename to create an indexed coordinate.\",\n3638                     UserWarning,\n3639                     stacklevel=3,\n3640                 )\n3641 \n3642         variables, coord_names, dims, indexes = self._rename_all(\n3643             name_dict=name_dict, dims_dict=name_dict\n3644         )\n3645         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3646 \n3647     def rename(\n3648         self: T_Dataset,\n3649         name_dict: Mapping[Any, Hashable] | None = None,\n3650         **names: Hashable,\n3651     ) -> T_Dataset:\n3652         \"\"\"Returns a new object with renamed variables, coordinates and dimensions.\n3653 \n3654         Parameters\n3655         ----------\n3656         name_dict : dict-like, optional\n3657             Dictionary whose keys are current variable, coordinate or dimension names and\n3658             whose values are the desired names.\n3659         **names : optional\n3660             Keyword form of ``name_dict``.\n3661             One of name_dict or names must be provided.\n3662 \n3663         Returns\n3664         -------\n3665         renamed : Dataset\n3666             Dataset with renamed variables, coordinates and dimensions.\n3667 \n3668         See Also\n3669         --------\n3670         Dataset.swap_dims\n3671         Dataset.rename_vars\n3672         Dataset.rename_dims\n3673         DataArray.rename\n3674         \"\"\"\n3675         return self._rename(name_dict=name_dict, **names)\n3676 \n3677     def rename_dims(\n3678         self: T_Dataset,\n3679         dims_dict: Mapping[Any, Hashable] | None = None,\n3680         **dims: Hashable,\n3681     ) -> T_Dataset:\n3682         \"\"\"Returns a new object with renamed dimensions only.\n3683 \n3684         Parameters\n3685         ----------\n3686         dims_dict : dict-like, optional\n3687             Dictionary whose keys are current dimension names and\n3688             whose values are the desired names. The desired names must\n3689             not be the name of an existing dimension or Variable in the Dataset.\n3690         **dims : optional\n3691             Keyword form of ``dims_dict``.\n3692             One of dims_dict or dims must be provided.\n3693 \n3694         Returns\n3695         -------\n3696         renamed : Dataset\n3697             Dataset with renamed dimensions.\n3698 \n3699         See Also\n3700         --------\n3701         Dataset.swap_dims\n3702         Dataset.rename\n3703         Dataset.rename_vars\n3704         DataArray.rename\n3705         \"\"\"\n3706         dims_dict = either_dict_or_kwargs(dims_dict, dims, \"rename_dims\")\n3707         for k, v in dims_dict.items():\n3708             if k not in self.dims:\n3709                 raise ValueError(\n3710                     f\"cannot rename {k!r} because it is not a \"\n3711                     \"dimension in this dataset\"\n3712                 )\n3713             if v in self.dims or v in self:\n3714                 raise ValueError(\n3715                     f\"Cannot rename {k} to {v} because {v} already exists. \"\n3716                     \"Try using swap_dims instead.\"\n3717                 )\n3718 \n3719         variables, coord_names, sizes, indexes = self._rename_all(\n3720             name_dict={}, dims_dict=dims_dict\n3721         )\n3722         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)\n3723 \n3724     def rename_vars(\n3725         self: T_Dataset,\n3726         name_dict: Mapping[Any, Hashable] | None = None,\n3727         **names: Hashable,\n3728     ) -> T_Dataset:\n3729         \"\"\"Returns a new object with renamed variables including coordinates\n3730 \n3731         Parameters\n3732         ----------\n3733         name_dict : dict-like, optional\n3734             Dictionary whose keys are current variable or coordinate names and\n3735             whose values are the desired names.\n3736         **names : optional\n3737             Keyword form of ``name_dict``.\n3738             One of name_dict or names must be provided.\n3739 \n3740         Returns\n3741         -------\n3742         renamed : Dataset\n3743             Dataset with renamed variables including coordinates\n3744 \n3745         See Also\n3746         --------\n3747         Dataset.swap_dims\n3748         Dataset.rename\n3749         Dataset.rename_dims\n3750         DataArray.rename\n3751         \"\"\"\n3752         name_dict = either_dict_or_kwargs(name_dict, names, \"rename_vars\")\n3753         for k in name_dict:\n3754             if k not in self:\n3755                 raise ValueError(\n3756                     f\"cannot rename {k!r} because it is not a \"\n3757                     \"variable or coordinate in this dataset\"\n3758                 )\n3759         variables, coord_names, dims, indexes = self._rename_all(\n3760             name_dict=name_dict, dims_dict={}\n3761         )\n3762         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3763 \n3764     def swap_dims(\n3765         self: T_Dataset, dims_dict: Mapping[Any, Hashable] | None = None, **dims_kwargs\n3766     ) -> T_Dataset:\n3767         \"\"\"Returns a new object with swapped dimensions.\n3768 \n3769         Parameters\n3770         ----------\n3771         dims_dict : dict-like\n3772             Dictionary whose keys are current dimension names and whose values\n3773             are new names.\n3774         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n3775             The keyword arguments form of ``dims_dict``.\n3776             One of dims_dict or dims_kwargs must be provided.\n3777 \n3778         Returns\n3779         -------\n3780         swapped : Dataset\n3781             Dataset with swapped dimensions.\n3782 \n3783         Examples\n3784         --------\n3785         >>> ds = xr.Dataset(\n3786         ...     data_vars={\"a\": (\"x\", [5, 7]), \"b\": (\"x\", [0.1, 2.4])},\n3787         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n3788         ... )\n3789         >>> ds\n3790         <xarray.Dataset>\n3791         Dimensions:  (x: 2)\n3792         Coordinates:\n3793           * x        (x) <U1 'a' 'b'\n3794             y        (x) int64 0 1\n3795         Data variables:\n3796             a        (x) int64 5 7\n3797             b        (x) float64 0.1 2.4\n3798 \n3799         >>> ds.swap_dims({\"x\": \"y\"})\n3800         <xarray.Dataset>\n3801         Dimensions:  (y: 2)\n3802         Coordinates:\n3803             x        (y) <U1 'a' 'b'\n3804           * y        (y) int64 0 1\n3805         Data variables:\n3806             a        (y) int64 5 7\n3807             b        (y) float64 0.1 2.4\n3808 \n3809         >>> ds.swap_dims({\"x\": \"z\"})\n3810         <xarray.Dataset>\n3811         Dimensions:  (z: 2)\n3812         Coordinates:\n3813             x        (z) <U1 'a' 'b'\n3814             y        (z) int64 0 1\n3815         Dimensions without coordinates: z\n3816         Data variables:\n3817             a        (z) int64 5 7\n3818             b        (z) float64 0.1 2.4\n3819 \n3820         See Also\n3821         --------\n3822         Dataset.rename\n3823         DataArray.swap_dims\n3824         \"\"\"\n3825         # TODO: deprecate this method in favor of a (less confusing)\n3826         # rename_dims() method that only renames dimensions.\n3827 \n3828         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n3829         for k, v in dims_dict.items():\n3830             if k not in self.dims:\n3831                 raise ValueError(\n3832                     f\"cannot swap from dimension {k!r} because it is \"\n3833                     \"not an existing dimension\"\n3834                 )\n3835             if v in self.variables and self.variables[v].dims != (k,):\n3836                 raise ValueError(\n3837                     f\"replacement dimension {v!r} is not a 1D \"\n3838                     f\"variable along the old dimension {k!r}\"\n3839                 )\n3840 \n3841         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}\n3842 \n3843         coord_names = self._coord_names.copy()\n3844         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})\n3845 \n3846         variables: dict[Hashable, Variable] = {}\n3847         indexes: dict[Hashable, Index] = {}\n3848         for k, v in self.variables.items():\n3849             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n3850             var: Variable\n3851             if k in result_dims:\n3852                 var = v.to_index_variable()\n3853                 var.dims = dims\n3854                 if k in self._indexes:\n3855                     indexes[k] = self._indexes[k]\n3856                     variables[k] = var\n3857                 else:\n3858                     index, index_vars = create_default_index_implicit(var)\n3859                     indexes.update({name: index for name in index_vars})\n3860                     variables.update(index_vars)\n3861                     coord_names.update(index_vars)\n3862             else:\n3863                 var = v.to_base_variable()\n3864                 var.dims = dims\n3865                 variables[k] = var\n3866 \n3867         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n3868 \n3869     # change type of self and return to T_Dataset once\n3870     # https://github.com/python/mypy/issues/12846 is resolved\n3871     def expand_dims(\n3872         self,\n3873         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,\n3874         axis: None | int | Sequence[int] = None,\n3875         **dim_kwargs: Any,\n3876     ) -> Dataset:\n3877         \"\"\"Return a new object with an additional axis (or axes) inserted at\n3878         the corresponding position in the array shape.  The new object is a\n3879         view into the underlying array, not a copy.\n3880 \n3881         If dim is already a scalar coordinate, it will be promoted to a 1D\n3882         coordinate consisting of a single value.\n3883 \n3884         Parameters\n3885         ----------\n3886         dim : hashable, sequence of hashable, mapping, or None\n3887             Dimensions to include on the new variable. If provided as hashable\n3888             or sequence of hashable, then dimensions are inserted with length\n3889             1. If provided as a mapping, then the keys are the new dimensions\n3890             and the values are either integers (giving the length of the new\n3891             dimensions) or array-like (giving the coordinates of the new\n3892             dimensions).\n3893         axis : int, sequence of int, or None, default: None\n3894             Axis position(s) where new axis is to be inserted (position(s) on\n3895             the result array). If a sequence of integers is passed,\n3896             multiple axes are inserted. In this case, dim arguments should be\n3897             same length list. If axis=None is passed, all the axes will be\n3898             inserted to the start of the result array.\n3899         **dim_kwargs : int or sequence or ndarray\n3900             The keywords are arbitrary dimensions being inserted and the values\n3901             are either the lengths of the new dims (if int is given), or their\n3902             coordinates. Note, this is an alternative to passing a dict to the\n3903             dim kwarg and will only be used if dim is None.\n3904 \n3905         Returns\n3906         -------\n3907         expanded : Dataset\n3908             This object, but with additional dimension(s).\n3909 \n3910         See Also\n3911         --------\n3912         DataArray.expand_dims\n3913         \"\"\"\n3914         if dim is None:\n3915             pass\n3916         elif isinstance(dim, Mapping):\n3917             # We're later going to modify dim in place; don't tamper with\n3918             # the input\n3919             dim = dict(dim)\n3920         elif isinstance(dim, int):\n3921             raise TypeError(\n3922                 \"dim should be hashable or sequence of hashables or mapping\"\n3923             )\n3924         elif isinstance(dim, str) or not isinstance(dim, Sequence):\n3925             dim = {dim: 1}\n3926         elif isinstance(dim, Sequence):\n3927             if len(dim) != len(set(dim)):\n3928                 raise ValueError(\"dims should not contain duplicate values.\")\n3929             dim = {d: 1 for d in dim}\n3930 \n3931         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n3932         assert isinstance(dim, MutableMapping)\n3933 \n3934         if axis is None:\n3935             axis = list(range(len(dim)))\n3936         elif not isinstance(axis, Sequence):\n3937             axis = [axis]\n3938 \n3939         if len(dim) != len(axis):\n3940             raise ValueError(\"lengths of dim and axis should be identical.\")\n3941         for d in dim:\n3942             if d in self.dims:\n3943                 raise ValueError(f\"Dimension {d} already exists.\")\n3944             if d in self._variables and not utils.is_scalar(self._variables[d]):\n3945                 raise ValueError(\n3946                     \"{dim} already exists as coordinate or\"\n3947                     \" variable name.\".format(dim=d)\n3948                 )\n3949 \n3950         variables: dict[Hashable, Variable] = {}\n3951         indexes: dict[Hashable, Index] = dict(self._indexes)\n3952         coord_names = self._coord_names.copy()\n3953         # If dim is a dict, then ensure that the values are either integers\n3954         # or iterables.\n3955         for k, v in dim.items():\n3956             if hasattr(v, \"__iter__\"):\n3957                 # If the value for the new dimension is an iterable, then\n3958                 # save the coordinates to the variables dict, and set the\n3959                 # value within the dim dict to the length of the iterable\n3960                 # for later use.\n3961                 index = PandasIndex(v, k)\n3962                 indexes[k] = index\n3963                 variables.update(index.create_variables())\n3964                 coord_names.add(k)\n3965                 dim[k] = variables[k].size\n3966             elif isinstance(v, int):\n3967                 pass  # Do nothing if the dimensions value is just an int\n3968             else:\n3969                 raise TypeError(\n3970                     \"The value of new dimension {k} must be \"\n3971                     \"an iterable or an int\".format(k=k)\n3972                 )\n3973 \n3974         for k, v in self._variables.items():\n3975             if k not in dim:\n3976                 if k in coord_names:  # Do not change coordinates\n3977                     variables[k] = v\n3978                 else:\n3979                     result_ndim = len(v.dims) + len(axis)\n3980                     for a in axis:\n3981                         if a < -result_ndim or result_ndim - 1 < a:\n3982                             raise IndexError(\n3983                                 f\"Axis {a} of variable {k} is out of bounds of the \"\n3984                                 f\"expanded dimension size {result_ndim}\"\n3985                             )\n3986 \n3987                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]\n3988                     if len(axis_pos) != len(set(axis_pos)):\n3989                         raise ValueError(\"axis should not contain duplicate values\")\n3990                     # We need to sort them to make sure `axis` equals to the\n3991                     # axis positions of the result array.\n3992                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n3993 \n3994                     all_dims = list(zip(v.dims, v.shape))\n3995                     for d, c in zip_axis_dim:\n3996                         all_dims.insert(d, c)\n3997                     variables[k] = v.set_dims(dict(all_dims))\n3998             else:\n3999                 if k not in variables:\n4000                     # If dims includes a label of a non-dimension coordinate,\n4001                     # it will be promoted to a 1D coordinate with a single value.\n4002                     index, index_vars = create_default_index_implicit(v.set_dims(k))\n4003                     indexes[k] = index\n4004                     variables.update(index_vars)\n4005 \n4006         return self._replace_with_new_dims(\n4007             variables, coord_names=coord_names, indexes=indexes\n4008         )\n4009 \n4010     # change type of self and return to T_Dataset once\n4011     # https://github.com/python/mypy/issues/12846 is resolved\n4012     def set_index(\n4013         self,\n4014         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,\n4015         append: bool = False,\n4016         **indexes_kwargs: Hashable | Sequence[Hashable],\n4017     ) -> Dataset:\n4018         \"\"\"Set Dataset (multi-)indexes using one or more existing coordinates\n4019         or variables.\n4020 \n4021         This legacy method is limited to pandas (multi-)indexes and\n4022         1-dimensional \"dimension\" coordinates. See\n4023         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom\n4024         Xarray-compatible index from one or more arbitrary coordinates.\n4025 \n4026         Parameters\n4027         ----------\n4028         indexes : {dim: index, ...}\n4029             Mapping from names matching dimensions and values given\n4030             by (lists of) the names of existing coordinates or variables to set\n4031             as new (multi-)index.\n4032         append : bool, default: False\n4033             If True, append the supplied index(es) to the existing index(es).\n4034             Otherwise replace the existing index(es) (default).\n4035         **indexes_kwargs : optional\n4036             The keyword arguments form of ``indexes``.\n4037             One of indexes or indexes_kwargs must be provided.\n4038 \n4039         Returns\n4040         -------\n4041         obj : Dataset\n4042             Another dataset, with this dataset's data but replaced coordinates.\n4043 \n4044         Examples\n4045         --------\n4046         >>> arr = xr.DataArray(\n4047         ...     data=np.ones((2, 3)),\n4048         ...     dims=[\"x\", \"y\"],\n4049         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n4050         ... )\n4051         >>> ds = xr.Dataset({\"v\": arr})\n4052         >>> ds\n4053         <xarray.Dataset>\n4054         Dimensions:  (x: 2, y: 3)\n4055         Coordinates:\n4056           * x        (x) int64 0 1\n4057           * y        (y) int64 0 1 2\n4058             a        (x) int64 3 4\n4059         Data variables:\n4060             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n4061         >>> ds.set_index(x=\"a\")\n4062         <xarray.Dataset>\n4063         Dimensions:  (x: 2, y: 3)\n4064         Coordinates:\n4065           * x        (x) int64 3 4\n4066           * y        (y) int64 0 1 2\n4067         Data variables:\n4068             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n4069 \n4070         See Also\n4071         --------\n4072         Dataset.reset_index\n4073         Dataset.set_xindex\n4074         Dataset.swap_dims\n4075         \"\"\"\n4076         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, \"set_index\")\n4077 \n4078         new_indexes: dict[Hashable, Index] = {}\n4079         new_variables: dict[Hashable, Variable] = {}\n4080         drop_indexes: set[Hashable] = set()\n4081         drop_variables: set[Hashable] = set()\n4082         replace_dims: dict[Hashable, Hashable] = {}\n4083         all_var_names: set[Hashable] = set()\n4084 \n4085         for dim, _var_names in dim_coords.items():\n4086             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):\n4087                 var_names = [_var_names]\n4088             else:\n4089                 var_names = list(_var_names)\n4090 \n4091             invalid_vars = set(var_names) - set(self._variables)\n4092             if invalid_vars:\n4093                 raise ValueError(\n4094                     \", \".join([str(v) for v in invalid_vars])\n4095                     + \" variable(s) do not exist\"\n4096                 )\n4097 \n4098             all_var_names.update(var_names)\n4099             drop_variables.update(var_names)\n4100 \n4101             # drop any pre-existing index involved and its corresponding coordinates\n4102             index_coord_names = self.xindexes.get_all_coords(dim, errors=\"ignore\")\n4103             all_index_coord_names = set(index_coord_names)\n4104             for k in var_names:\n4105                 all_index_coord_names.update(\n4106                     self.xindexes.get_all_coords(k, errors=\"ignore\")\n4107                 )\n4108 \n4109             drop_indexes.update(all_index_coord_names)\n4110             drop_variables.update(all_index_coord_names)\n4111 \n4112             if len(var_names) == 1 and (not append or dim not in self._indexes):\n4113                 var_name = var_names[0]\n4114                 var = self._variables[var_name]\n4115                 if var.dims != (dim,):\n4116                     raise ValueError(\n4117                         f\"dimension mismatch: try setting an index for dimension {dim!r} with \"\n4118                         f\"variable {var_name!r} that has dimensions {var.dims}\"\n4119                     )\n4120                 idx = PandasIndex.from_variables({dim: var}, options={})\n4121                 idx_vars = idx.create_variables({var_name: var})\n4122 \n4123                 # trick to preserve coordinate order in this case\n4124                 if dim in self._coord_names:\n4125                     drop_variables.remove(dim)\n4126             else:\n4127                 if append:\n4128                     current_variables = {\n4129                         k: self._variables[k] for k in index_coord_names\n4130                     }\n4131                 else:\n4132                     current_variables = {}\n4133                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(\n4134                     dim,\n4135                     current_variables,\n4136                     {k: self._variables[k] for k in var_names},\n4137                 )\n4138                 for n in idx.index.names:\n4139                     replace_dims[n] = dim\n4140 \n4141             new_indexes.update({k: idx for k in idx_vars})\n4142             new_variables.update(idx_vars)\n4143 \n4144         # re-add deindexed coordinates (convert to base variables)\n4145         for k in drop_variables:\n4146             if (\n4147                 k not in new_variables\n4148                 and k not in all_var_names\n4149                 and k in self._coord_names\n4150             ):\n4151                 new_variables[k] = self._variables[k].to_base_variable()\n4152 \n4153         indexes_: dict[Any, Index] = {\n4154             k: v for k, v in self._indexes.items() if k not in drop_indexes\n4155         }\n4156         indexes_.update(new_indexes)\n4157 \n4158         variables = {\n4159             k: v for k, v in self._variables.items() if k not in drop_variables\n4160         }\n4161         variables.update(new_variables)\n4162 \n4163         # update dimensions if necessary, GH: 3512\n4164         for k, v in variables.items():\n4165             if any(d in replace_dims for d in v.dims):\n4166                 new_dims = [replace_dims.get(d, d) for d in v.dims]\n4167                 variables[k] = v._replace(dims=new_dims)\n4168 \n4169         coord_names = self._coord_names - drop_variables | set(new_variables)\n4170 \n4171         return self._replace_with_new_dims(\n4172             variables, coord_names=coord_names, indexes=indexes_\n4173         )\n4174 \n4175     def reset_index(\n4176         self: T_Dataset,\n4177         dims_or_levels: Hashable | Sequence[Hashable],\n4178         drop: bool = False,\n4179     ) -> T_Dataset:\n4180         \"\"\"Reset the specified index(es) or multi-index level(s).\n4181 \n4182         This legacy method is specific to pandas (multi-)indexes and\n4183         1-dimensional \"dimension\" coordinates. See the more generic\n4184         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`\n4185         method to respectively drop and set pandas or custom indexes for\n4186         arbitrary coordinates.\n4187 \n4188         Parameters\n4189         ----------\n4190         dims_or_levels : Hashable or Sequence of Hashable\n4191             Name(s) of the dimension(s) and/or multi-index level(s) that will\n4192             be reset.\n4193         drop : bool, default: False\n4194             If True, remove the specified indexes and/or multi-index levels\n4195             instead of extracting them as new coordinates (default: False).\n4196 \n4197         Returns\n4198         -------\n4199         obj : Dataset\n4200             Another dataset, with this dataset's data but replaced coordinates.\n4201 \n4202         See Also\n4203         --------\n4204         Dataset.set_index\n4205         Dataset.set_xindex\n4206         Dataset.drop_indexes\n4207         \"\"\"\n4208         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):\n4209             dims_or_levels = [dims_or_levels]\n4210 \n4211         invalid_coords = set(dims_or_levels) - set(self._indexes)\n4212         if invalid_coords:\n4213             raise ValueError(\n4214                 f\"{tuple(invalid_coords)} are not coordinates with an index\"\n4215             )\n4216 \n4217         drop_indexes: set[Hashable] = set()\n4218         drop_variables: set[Hashable] = set()\n4219         seen: set[Index] = set()\n4220         new_indexes: dict[Hashable, Index] = {}\n4221         new_variables: dict[Hashable, Variable] = {}\n4222 \n4223         def drop_or_convert(var_names):\n4224             if drop:\n4225                 drop_variables.update(var_names)\n4226             else:\n4227                 base_vars = {\n4228                     k: self._variables[k].to_base_variable() for k in var_names\n4229                 }\n4230                 new_variables.update(base_vars)\n4231 \n4232         for name in dims_or_levels:\n4233             index = self._indexes[name]\n4234 \n4235             if index in seen:\n4236                 continue\n4237             seen.add(index)\n4238 \n4239             idx_var_names = set(self.xindexes.get_all_coords(name))\n4240             drop_indexes.update(idx_var_names)\n4241 \n4242             if isinstance(index, PandasMultiIndex):\n4243                 # special case for pd.MultiIndex\n4244                 level_names = index.index.names\n4245                 keep_level_vars = {\n4246                     k: self._variables[k]\n4247                     for k in level_names\n4248                     if k not in dims_or_levels\n4249                 }\n4250 \n4251                 if index.dim not in dims_or_levels and keep_level_vars:\n4252                     # do not drop the multi-index completely\n4253                     # instead replace it by a new (multi-)index with dropped level(s)\n4254                     idx = index.keep_levels(keep_level_vars)\n4255                     idx_vars = idx.create_variables(keep_level_vars)\n4256                     new_indexes.update({k: idx for k in idx_vars})\n4257                     new_variables.update(idx_vars)\n4258                     if not isinstance(idx, PandasMultiIndex):\n4259                         # multi-index reduced to single index\n4260                         # backward compatibility: unique level coordinate renamed to dimension\n4261                         drop_variables.update(keep_level_vars)\n4262                     drop_or_convert(\n4263                         [k for k in level_names if k not in keep_level_vars]\n4264                     )\n4265                 else:\n4266                     # always drop the multi-index dimension variable\n4267                     drop_variables.add(index.dim)\n4268                     drop_or_convert(level_names)\n4269             else:\n4270                 drop_or_convert(idx_var_names)\n4271 \n4272         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}\n4273         indexes.update(new_indexes)\n4274 \n4275         variables = {\n4276             k: v for k, v in self._variables.items() if k not in drop_variables\n4277         }\n4278         variables.update(new_variables)\n4279 \n4280         coord_names = self._coord_names - drop_variables\n4281 \n4282         return self._replace_with_new_dims(\n4283             variables, coord_names=coord_names, indexes=indexes\n4284         )\n4285 \n4286     def set_xindex(\n4287         self: T_Dataset,\n4288         coord_names: str | Sequence[Hashable],\n4289         index_cls: type[Index] | None = None,\n4290         **options,\n4291     ) -> T_Dataset:\n4292         \"\"\"Set a new, Xarray-compatible index from one or more existing\n4293         coordinate(s).\n4294 \n4295         Parameters\n4296         ----------\n4297         coord_names : str or list\n4298             Name(s) of the coordinate(s) used to build the index.\n4299             If several names are given, their order matters.\n4300         index_cls : subclass of :class:`~xarray.indexes.Index`, optional\n4301             The type of index to create. By default, try setting\n4302             a ``PandasIndex`` if ``len(coord_names) == 1``,\n4303             otherwise a ``PandasMultiIndex``.\n4304         **options\n4305             Options passed to the index constructor.\n4306 \n4307         Returns\n4308         -------\n4309         obj : Dataset\n4310             Another dataset, with this dataset's data and with a new index.\n4311 \n4312         \"\"\"\n4313         # the Sequence check is required for mypy\n4314         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):\n4315             coord_names = [coord_names]\n4316 \n4317         if index_cls is None:\n4318             if len(coord_names) == 1:\n4319                 index_cls = PandasIndex\n4320             else:\n4321                 index_cls = PandasMultiIndex\n4322         else:\n4323             if not issubclass(index_cls, Index):\n4324                 raise TypeError(f\"{index_cls} is not a subclass of xarray.Index\")\n4325 \n4326         invalid_coords = set(coord_names) - self._coord_names\n4327 \n4328         if invalid_coords:\n4329             msg = [\"invalid coordinate(s)\"]\n4330             no_vars = invalid_coords - set(self._variables)\n4331             data_vars = invalid_coords - no_vars\n4332             if no_vars:\n4333                 msg.append(f\"those variables don't exist: {no_vars}\")\n4334             if data_vars:\n4335                 msg.append(\n4336                     f\"those variables are data variables: {data_vars}, use `set_coords` first\"\n4337                 )\n4338             raise ValueError(\"\\n\".join(msg))\n4339 \n4340         # we could be more clever here (e.g., drop-in index replacement if index\n4341         # coordinates do not conflict), but let's not allow this for now\n4342         indexed_coords = set(coord_names) & set(self._indexes)\n4343 \n4344         if indexed_coords:\n4345             raise ValueError(\n4346                 f\"those coordinates already have an index: {indexed_coords}\"\n4347             )\n4348 \n4349         coord_vars = {name: self._variables[name] for name in coord_names}\n4350 \n4351         index = index_cls.from_variables(coord_vars, options=options)\n4352 \n4353         new_coord_vars = index.create_variables(coord_vars)\n4354 \n4355         # special case for setting a pandas multi-index from level coordinates\n4356         # TODO: remove it once we depreciate pandas multi-index dimension (tuple\n4357         # elements) coordinate\n4358         if isinstance(index, PandasMultiIndex):\n4359             coord_names = [index.dim] + list(coord_names)\n4360 \n4361         variables: dict[Hashable, Variable]\n4362         indexes: dict[Hashable, Index]\n4363 \n4364         if len(coord_names) == 1:\n4365             variables = self._variables.copy()\n4366             indexes = self._indexes.copy()\n4367 \n4368             name = list(coord_names).pop()\n4369             if name in new_coord_vars:\n4370                 variables[name] = new_coord_vars[name]\n4371             indexes[name] = index\n4372         else:\n4373             # reorder variables and indexes so that coordinates having the same\n4374             # index are next to each other\n4375             variables = {}\n4376             for name, var in self._variables.items():\n4377                 if name not in coord_names:\n4378                     variables[name] = var\n4379 \n4380             indexes = {}\n4381             for name, idx in self._indexes.items():\n4382                 if name not in coord_names:\n4383                     indexes[name] = idx\n4384 \n4385             for name in coord_names:\n4386                 try:\n4387                     variables[name] = new_coord_vars[name]\n4388                 except KeyError:\n4389                     variables[name] = self._variables[name]\n4390                 indexes[name] = index\n4391 \n4392         return self._replace(\n4393             variables=variables,\n4394             coord_names=self._coord_names | set(coord_names),\n4395             indexes=indexes,\n4396         )\n4397 \n4398     def reorder_levels(\n4399         self: T_Dataset,\n4400         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,\n4401         **dim_order_kwargs: Sequence[int | Hashable],\n4402     ) -> T_Dataset:\n4403         \"\"\"Rearrange index levels using input order.\n4404 \n4405         Parameters\n4406         ----------\n4407         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional\n4408             Mapping from names matching dimensions and values given\n4409             by lists representing new level orders. Every given dimension\n4410             must have a multi-index.\n4411         **dim_order_kwargs : Sequence of int or Hashable, optional\n4412             The keyword arguments form of ``dim_order``.\n4413             One of dim_order or dim_order_kwargs must be provided.\n4414 \n4415         Returns\n4416         -------\n4417         obj : Dataset\n4418             Another dataset, with this dataset's data but replaced\n4419             coordinates.\n4420         \"\"\"\n4421         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n4422         variables = self._variables.copy()\n4423         indexes = dict(self._indexes)\n4424         new_indexes: dict[Hashable, Index] = {}\n4425         new_variables: dict[Hashable, IndexVariable] = {}\n4426 \n4427         for dim, order in dim_order.items():\n4428             index = self._indexes[dim]\n4429 \n4430             if not isinstance(index, PandasMultiIndex):\n4431                 raise ValueError(f\"coordinate {dim} has no MultiIndex\")\n4432 \n4433             level_vars = {k: self._variables[k] for k in order}\n4434             idx = index.reorder_levels(level_vars)\n4435             idx_vars = idx.create_variables(level_vars)\n4436             new_indexes.update({k: idx for k in idx_vars})\n4437             new_variables.update(idx_vars)\n4438 \n4439         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}\n4440         indexes.update(new_indexes)\n4441 \n4442         variables = {k: v for k, v in self._variables.items() if k not in new_variables}\n4443         variables.update(new_variables)\n4444 \n4445         return self._replace(variables, indexes=indexes)\n4446 \n4447     def _get_stack_index(\n4448         self,\n4449         dim,\n4450         multi=False,\n4451         create_index=False,\n4452     ) -> tuple[Index | None, dict[Hashable, Variable]]:\n4453         \"\"\"Used by stack and unstack to get one pandas (multi-)index among\n4454         the indexed coordinates along dimension `dim`.\n4455 \n4456         If exactly one index is found, return it with its corresponding\n4457         coordinate variables(s), otherwise return None and an empty dict.\n4458 \n4459         If `create_index=True`, create a new index if none is found or raise\n4460         an error if multiple indexes are found.\n4461 \n4462         \"\"\"\n4463         stack_index: Index | None = None\n4464         stack_coords: dict[Hashable, Variable] = {}\n4465 \n4466         for name, index in self._indexes.items():\n4467             var = self._variables[name]\n4468             if (\n4469                 var.ndim == 1\n4470                 and var.dims[0] == dim\n4471                 and (\n4472                     # stack: must be a single coordinate index\n4473                     not multi\n4474                     and not self.xindexes.is_multi(name)\n4475                     # unstack: must be an index that implements .unstack\n4476                     or multi\n4477                     and type(index).unstack is not Index.unstack\n4478                 )\n4479             ):\n4480                 if stack_index is not None and index is not stack_index:\n4481                     # more than one index found, stop\n4482                     if create_index:\n4483                         raise ValueError(\n4484                             f\"cannot stack dimension {dim!r} with `create_index=True` \"\n4485                             \"and with more than one index found along that dimension\"\n4486                         )\n4487                     return None, {}\n4488                 stack_index = index\n4489                 stack_coords[name] = var\n4490 \n4491         if create_index and stack_index is None:\n4492             if dim in self._variables:\n4493                 var = self._variables[dim]\n4494             else:\n4495                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)\n4496             # dummy index (only `stack_coords` will be used to construct the multi-index)\n4497             stack_index = PandasIndex([0], dim)\n4498             stack_coords = {dim: var}\n4499 \n4500         return stack_index, stack_coords\n4501 \n4502     def _stack_once(\n4503         self: T_Dataset,\n4504         dims: Sequence[Hashable | ellipsis],\n4505         new_dim: Hashable,\n4506         index_cls: type[Index],\n4507         create_index: bool | None = True,\n4508     ) -> T_Dataset:\n4509         if dims == ...:\n4510             raise ValueError(\"Please use [...] for dims, rather than just ...\")\n4511         if ... in dims:\n4512             dims = list(infix_dims(dims, self.dims))\n4513 \n4514         new_variables: dict[Hashable, Variable] = {}\n4515         stacked_var_names: list[Hashable] = []\n4516         drop_indexes: list[Hashable] = []\n4517 \n4518         for name, var in self.variables.items():\n4519             if any(d in var.dims for d in dims):\n4520                 add_dims = [d for d in dims if d not in var.dims]\n4521                 vdims = list(var.dims) + add_dims\n4522                 shape = [self.dims[d] for d in vdims]\n4523                 exp_var = var.set_dims(vdims, shape)\n4524                 stacked_var = exp_var.stack(**{new_dim: dims})\n4525                 new_variables[name] = stacked_var\n4526                 stacked_var_names.append(name)\n4527             else:\n4528                 new_variables[name] = var.copy(deep=False)\n4529 \n4530         # drop indexes of stacked coordinates (if any)\n4531         for name in stacked_var_names:\n4532             drop_indexes += list(self.xindexes.get_all_coords(name, errors=\"ignore\"))\n4533 \n4534         new_indexes = {}\n4535         new_coord_names = set(self._coord_names)\n4536         if create_index or create_index is None:\n4537             product_vars: dict[Any, Variable] = {}\n4538             for dim in dims:\n4539                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)\n4540                 if idx is not None:\n4541                     product_vars.update(idx_vars)\n4542 \n4543             if len(product_vars) == len(dims):\n4544                 idx = index_cls.stack(product_vars, new_dim)\n4545                 new_indexes[new_dim] = idx\n4546                 new_indexes.update({k: idx for k in product_vars})\n4547                 idx_vars = idx.create_variables(product_vars)\n4548                 # keep consistent multi-index coordinate order\n4549                 for k in idx_vars:\n4550                     new_variables.pop(k, None)\n4551                 new_variables.update(idx_vars)\n4552                 new_coord_names.update(idx_vars)\n4553 \n4554         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}\n4555         indexes.update(new_indexes)\n4556 \n4557         return self._replace_with_new_dims(\n4558             new_variables, coord_names=new_coord_names, indexes=indexes\n4559         )\n4560 \n4561     def stack(\n4562         self: T_Dataset,\n4563         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,\n4564         create_index: bool | None = True,\n4565         index_cls: type[Index] = PandasMultiIndex,\n4566         **dimensions_kwargs: Sequence[Hashable | ellipsis],\n4567     ) -> T_Dataset:\n4568         \"\"\"\n4569         Stack any number of existing dimensions into a single new dimension.\n4570 \n4571         New dimensions will be added at the end, and by default the corresponding\n4572         coordinate variables will be combined into a MultiIndex.\n4573 \n4574         Parameters\n4575         ----------\n4576         dimensions : mapping of hashable to sequence of hashable\n4577             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new\n4578             dimensions, and the existing dimensions that they replace. An\n4579             ellipsis (`...`) will be replaced by all unlisted dimensions.\n4580             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n4581             all dimensions.\n4582         create_index : bool or None, default: True\n4583 \n4584             - True: create a multi-index for each of the stacked dimensions.\n4585             - False: don't create any index.\n4586             - None. create a multi-index only if exactly one single (1-d) coordinate\n4587               index is found for every dimension to stack.\n4588 \n4589         index_cls: Index-class, default: PandasMultiIndex\n4590             Can be used to pass a custom multi-index type (must be an Xarray index that\n4591             implements `.stack()`). By default, a pandas multi-index wrapper is used.\n4592         **dimensions_kwargs\n4593             The keyword arguments form of ``dimensions``.\n4594             One of dimensions or dimensions_kwargs must be provided.\n4595 \n4596         Returns\n4597         -------\n4598         stacked : Dataset\n4599             Dataset with stacked data.\n4600 \n4601         See Also\n4602         --------\n4603         Dataset.unstack\n4604         \"\"\"\n4605         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n4606         result = self\n4607         for new_dim, dims in dimensions.items():\n4608             result = result._stack_once(dims, new_dim, index_cls, create_index)\n4609         return result\n4610 \n4611     def to_stacked_array(\n4612         self,\n4613         new_dim: Hashable,\n4614         sample_dims: Collection[Hashable],\n4615         variable_dim: Hashable = \"variable\",\n4616         name: Hashable | None = None,\n4617     ) -> DataArray:\n4618         \"\"\"Combine variables of differing dimensionality into a DataArray\n4619         without broadcasting.\n4620 \n4621         This method is similar to Dataset.to_array but does not broadcast the\n4622         variables.\n4623 \n4624         Parameters\n4625         ----------\n4626         new_dim : hashable\n4627             Name of the new stacked coordinate\n4628         sample_dims : Collection of hashables\n4629             List of dimensions that **will not** be stacked. Each array in the\n4630             dataset must share these dimensions. For machine learning\n4631             applications, these define the dimensions over which samples are\n4632             drawn.\n4633         variable_dim : hashable, default: \"variable\"\n4634             Name of the level in the stacked coordinate which corresponds to\n4635             the variables.\n4636         name : hashable, optional\n4637             Name of the new data array.\n4638 \n4639         Returns\n4640         -------\n4641         stacked : DataArray\n4642             DataArray with the specified dimensions and data variables\n4643             stacked together. The stacked coordinate is named ``new_dim``\n4644             and represented by a MultiIndex object with a level containing the\n4645             data variable names. The name of this level is controlled using\n4646             the ``variable_dim`` argument.\n4647 \n4648         See Also\n4649         --------\n4650         Dataset.to_array\n4651         Dataset.stack\n4652         DataArray.to_unstacked_dataset\n4653 \n4654         Examples\n4655         --------\n4656         >>> data = xr.Dataset(\n4657         ...     data_vars={\n4658         ...         \"a\": ((\"x\", \"y\"), [[0, 1, 2], [3, 4, 5]]),\n4659         ...         \"b\": (\"x\", [6, 7]),\n4660         ...     },\n4661         ...     coords={\"y\": [\"u\", \"v\", \"w\"]},\n4662         ... )\n4663 \n4664         >>> data\n4665         <xarray.Dataset>\n4666         Dimensions:  (x: 2, y: 3)\n4667         Coordinates:\n4668           * y        (y) <U1 'u' 'v' 'w'\n4669         Dimensions without coordinates: x\n4670         Data variables:\n4671             a        (x, y) int64 0 1 2 3 4 5\n4672             b        (x) int64 6 7\n4673 \n4674         >>> data.to_stacked_array(\"z\", sample_dims=[\"x\"])\n4675         <xarray.DataArray 'a' (x: 2, z: 4)>\n4676         array([[0, 1, 2, 6],\n4677                [3, 4, 5, 7]])\n4678         Coordinates:\n4679           * z         (z) object MultiIndex\n4680           * variable  (z) object 'a' 'a' 'a' 'b'\n4681           * y         (z) object 'u' 'v' 'w' nan\n4682         Dimensions without coordinates: x\n4683 \n4684         \"\"\"\n4685         from xarray.core.concat import concat\n4686 \n4687         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n4688 \n4689         for variable in self:\n4690             dims = self[variable].dims\n4691             dims_include_sample_dims = set(sample_dims) <= set(dims)\n4692             if not dims_include_sample_dims:\n4693                 raise ValueError(\n4694                     \"All variables in the dataset must contain the \"\n4695                     \"dimensions {}.\".format(dims)\n4696                 )\n4697 \n4698         def ensure_stackable(val):\n4699             assign_coords = {variable_dim: val.name}\n4700             for dim in stacking_dims:\n4701                 if dim not in val.dims:\n4702                     assign_coords[dim] = None\n4703 \n4704             expand_dims = set(stacking_dims).difference(set(val.dims))\n4705             expand_dims.add(variable_dim)\n4706             # must be list for .expand_dims\n4707             expand_dims = list(expand_dims)\n4708 \n4709             return (\n4710                 val.assign_coords(**assign_coords)\n4711                 .expand_dims(expand_dims)\n4712                 .stack({new_dim: (variable_dim,) + stacking_dims})\n4713             )\n4714 \n4715         # concatenate the arrays\n4716         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n4717         data_array = concat(stackable_vars, dim=new_dim)\n4718 \n4719         if name is not None:\n4720             data_array.name = name\n4721 \n4722         return data_array\n4723 \n4724     def _unstack_once(\n4725         self: T_Dataset,\n4726         dim: Hashable,\n4727         index_and_vars: tuple[Index, dict[Hashable, Variable]],\n4728         fill_value,\n4729         sparse: bool = False,\n4730     ) -> T_Dataset:\n4731         index, index_vars = index_and_vars\n4732         variables: dict[Hashable, Variable] = {}\n4733         indexes = {k: v for k, v in self._indexes.items() if k != dim}\n4734 \n4735         new_indexes, clean_index = index.unstack()\n4736         indexes.update(new_indexes)\n4737 \n4738         for name, idx in new_indexes.items():\n4739             variables.update(idx.create_variables(index_vars))\n4740 \n4741         for name, var in self.variables.items():\n4742             if name not in index_vars:\n4743                 if dim in var.dims:\n4744                     if isinstance(fill_value, Mapping):\n4745                         fill_value_ = fill_value[name]\n4746                     else:\n4747                         fill_value_ = fill_value\n4748 \n4749                     variables[name] = var._unstack_once(\n4750                         index=clean_index,\n4751                         dim=dim,\n4752                         fill_value=fill_value_,\n4753                         sparse=sparse,\n4754                     )\n4755                 else:\n4756                     variables[name] = var\n4757 \n4758         coord_names = set(self._coord_names) - {dim} | set(new_indexes)\n4759 \n4760         return self._replace_with_new_dims(\n4761             variables, coord_names=coord_names, indexes=indexes\n4762         )\n4763 \n4764     def _unstack_full_reindex(\n4765         self: T_Dataset,\n4766         dim: Hashable,\n4767         index_and_vars: tuple[Index, dict[Hashable, Variable]],\n4768         fill_value,\n4769         sparse: bool,\n4770     ) -> T_Dataset:\n4771         index, index_vars = index_and_vars\n4772         variables: dict[Hashable, Variable] = {}\n4773         indexes = {k: v for k, v in self._indexes.items() if k != dim}\n4774 \n4775         new_indexes, clean_index = index.unstack()\n4776         indexes.update(new_indexes)\n4777 \n4778         new_index_variables = {}\n4779         for name, idx in new_indexes.items():\n4780             new_index_variables.update(idx.create_variables(index_vars))\n4781 \n4782         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}\n4783         variables.update(new_index_variables)\n4784 \n4785         # take a shortcut in case the MultiIndex was not modified.\n4786         full_idx = pd.MultiIndex.from_product(\n4787             clean_index.levels, names=clean_index.names\n4788         )\n4789         if clean_index.equals(full_idx):\n4790             obj = self\n4791         else:\n4792             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex\n4793             xr_full_idx = PandasMultiIndex(full_idx, dim)\n4794             indexers = Indexes(\n4795                 {k: xr_full_idx for k in index_vars},\n4796                 xr_full_idx.create_variables(index_vars),\n4797             )\n4798             obj = self._reindex(\n4799                 indexers, copy=False, fill_value=fill_value, sparse=sparse\n4800             )\n4801 \n4802         for name, var in obj.variables.items():\n4803             if name not in index_vars:\n4804                 if dim in var.dims:\n4805                     variables[name] = var.unstack({dim: new_dim_sizes})\n4806                 else:\n4807                     variables[name] = var\n4808 \n4809         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)\n4810 \n4811         return self._replace_with_new_dims(\n4812             variables, coord_names=coord_names, indexes=indexes\n4813         )\n4814 \n4815     def unstack(\n4816         self: T_Dataset,\n4817         dim: Dims = None,\n4818         fill_value: Any = xrdtypes.NA,\n4819         sparse: bool = False,\n4820     ) -> T_Dataset:\n4821         \"\"\"\n4822         Unstack existing dimensions corresponding to MultiIndexes into\n4823         multiple new dimensions.\n4824 \n4825         New dimensions will be added at the end.\n4826 \n4827         Parameters\n4828         ----------\n4829         dim : str, Iterable of Hashable or None, optional\n4830             Dimension(s) over which to unstack. By default unstacks all\n4831             MultiIndexes.\n4832         fill_value : scalar or dict-like, default: nan\n4833             value to be filled. If a dict-like, maps variable names to\n4834             fill values. If not provided or if the dict-like does not\n4835             contain all variables, the dtype's NA value will be used.\n4836         sparse : bool, default: False\n4837             use sparse-array if True\n4838 \n4839         Returns\n4840         -------\n4841         unstacked : Dataset\n4842             Dataset with unstacked data.\n4843 \n4844         See Also\n4845         --------\n4846         Dataset.stack\n4847         \"\"\"\n4848 \n4849         if dim is None:\n4850             dims = list(self.dims)\n4851         else:\n4852             if isinstance(dim, str) or not isinstance(dim, Iterable):\n4853                 dims = [dim]\n4854             else:\n4855                 dims = list(dim)\n4856 \n4857             missing_dims = [d for d in dims if d not in self.dims]\n4858             if missing_dims:\n4859                 raise ValueError(\n4860                     f\"Dataset does not contain the dimensions: {missing_dims}\"\n4861                 )\n4862 \n4863         # each specified dimension must have exactly one multi-index\n4864         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}\n4865         for d in dims:\n4866             idx, idx_vars = self._get_stack_index(d, multi=True)\n4867             if idx is not None:\n4868                 stacked_indexes[d] = idx, idx_vars\n4869 \n4870         if dim is None:\n4871             dims = list(stacked_indexes)\n4872         else:\n4873             non_multi_dims = set(dims) - set(stacked_indexes)\n4874             if non_multi_dims:\n4875                 raise ValueError(\n4876                     \"cannot unstack dimensions that do not \"\n4877                     f\"have exactly one multi-index: {tuple(non_multi_dims)}\"\n4878                 )\n4879 \n4880         result = self.copy(deep=False)\n4881 \n4882         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,\n4883         # so we can't just access self.variables[v].data for every variable.\n4884         # We only check the non-index variables.\n4885         # https://github.com/pydata/xarray/issues/5902\n4886         nonindexes = [\n4887             self.variables[k] for k in set(self.variables) - set(self._indexes)\n4888         ]\n4889         # Notes for each of these cases:\n4890         # 1. Dask arrays don't support assignment by index, which the fast unstack\n4891         #    function requires.\n4892         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125\n4893         # 2. Sparse doesn't currently support (though we could special-case it)\n4894         #    https://github.com/pydata/sparse/issues/422\n4895         # 3. pint requires checking if it's a NumPy array until\n4896         #    https://github.com/pydata/xarray/pull/4751 is resolved,\n4897         #    Once that is resolved, explicitly exclude pint arrays.\n4898         #    pint doesn't implement `np.full_like` in a way that's\n4899         #    currently compatible.\n4900         sparse_array_type = array_type(\"sparse\")\n4901         needs_full_reindex = any(\n4902             is_duck_dask_array(v.data)\n4903             or isinstance(v.data, sparse_array_type)\n4904             or not isinstance(v.data, np.ndarray)\n4905             for v in nonindexes\n4906         )\n4907 \n4908         for d in dims:\n4909             if needs_full_reindex:\n4910                 result = result._unstack_full_reindex(\n4911                     d, stacked_indexes[d], fill_value, sparse\n4912                 )\n4913             else:\n4914                 result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)\n4915         return result\n4916 \n4917     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:\n4918         \"\"\"Update this dataset's variables with those from another dataset.\n4919 \n4920         Just like :py:meth:`dict.update` this is a in-place operation.\n4921         For a non-inplace version, see :py:meth:`Dataset.merge`.\n4922 \n4923         Parameters\n4924         ----------\n4925         other : Dataset or mapping\n4926             Variables with which to update this dataset. One of:\n4927 \n4928             - Dataset\n4929             - mapping {var name: DataArray}\n4930             - mapping {var name: Variable}\n4931             - mapping {var name: (dimension name, array-like)}\n4932             - mapping {var name: (tuple of dimension names, array-like)}\n4933 \n4934         Returns\n4935         -------\n4936         updated : Dataset\n4937             Updated dataset. Note that since the update is in-place this is the input\n4938             dataset.\n4939 \n4940             It is deprecated since version 0.17 and scheduled to be removed in 0.21.\n4941 \n4942         Raises\n4943         ------\n4944         ValueError\n4945             If any dimensions would have inconsistent sizes in the updated\n4946             dataset.\n4947 \n4948         See Also\n4949         --------\n4950         Dataset.assign\n4951         Dataset.merge\n4952         \"\"\"\n4953         merge_result = dataset_update_method(self, other)\n4954         return self._replace(inplace=True, **merge_result._asdict())\n4955 \n4956     def merge(\n4957         self: T_Dataset,\n4958         other: CoercibleMapping | DataArray,\n4959         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),\n4960         compat: CompatOptions = \"no_conflicts\",\n4961         join: JoinOptions = \"outer\",\n4962         fill_value: Any = xrdtypes.NA,\n4963         combine_attrs: CombineAttrsOptions = \"override\",\n4964     ) -> T_Dataset:\n4965         \"\"\"Merge the arrays of two datasets into a single dataset.\n4966 \n4967         This method generally does not allow for overriding data, with the\n4968         exception of attributes, which are ignored on the second dataset.\n4969         Variables with the same name are checked for conflicts via the equals\n4970         or identical methods.\n4971 \n4972         Parameters\n4973         ----------\n4974         other : Dataset or mapping\n4975             Dataset or variables to merge with this dataset.\n4976         overwrite_vars : hashable or iterable of hashable, optional\n4977             If provided, update variables of these name(s) without checking for\n4978             conflicts in this dataset.\n4979         compat : {\"identical\", \"equals\", \"broadcast_equals\", \\\n4980                   \"no_conflicts\", \"override\", \"minimal\"}, default: \"no_conflicts\"\n4981             String indicating how to compare variables of the same name for\n4982             potential conflicts:\n4983 \n4984             - 'identical': all values, dimensions and attributes must be the\n4985               same.\n4986             - 'equals': all values and dimensions must be the same.\n4987             - 'broadcast_equals': all values must be equal when variables are\n4988               broadcast against each other to ensure common dimensions.\n4989             - 'no_conflicts': only values which are not null in both datasets\n4990               must be equal. The returned dataset then contains the combination\n4991               of all non-null values.\n4992             - 'override': skip comparing and pick variable from first dataset\n4993             - 'minimal': drop conflicting coordinates\n4994 \n4995         join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\", \"override\"}, \\\n4996                default: \"outer\"\n4997             Method for joining ``self`` and ``other`` along shared dimensions:\n4998 \n4999             - 'outer': use the union of the indexes\n5000             - 'inner': use the intersection of the indexes\n5001             - 'left': use indexes from ``self``\n5002             - 'right': use indexes from ``other``\n5003             - 'exact': error instead of aligning non-equal indexes\n5004             - 'override': use indexes from ``self`` that are the same size\n5005               as those of ``other`` in that dimension\n5006 \n5007         fill_value : scalar or dict-like, optional\n5008             Value to use for newly missing values. If a dict-like, maps\n5009             variable names (including coordinates) to fill values.\n5010         combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"drop_conflicts\", \\\n5011                          \"override\"} or callable, default: \"override\"\n5012             A callable or a string indicating how to combine attrs of the objects being\n5013             merged:\n5014 \n5015             - \"drop\": empty attrs on returned Dataset.\n5016             - \"identical\": all attrs must be the same on every object.\n5017             - \"no_conflicts\": attrs from all objects are combined, any that have\n5018               the same name must also have the same value.\n5019             - \"drop_conflicts\": attrs from all objects are combined, any that have\n5020               the same name but different values are dropped.\n5021             - \"override\": skip comparing and copy attrs from the first dataset to\n5022               the result.\n5023 \n5024             If a callable, it must expect a sequence of ``attrs`` dicts and a context object\n5025             as its only parameters.\n5026 \n5027         Returns\n5028         -------\n5029         merged : Dataset\n5030             Merged dataset.\n5031 \n5032         Raises\n5033         ------\n5034         MergeError\n5035             If any variables conflict (see ``compat``).\n5036 \n5037         See Also\n5038         --------\n5039         Dataset.update\n5040         \"\"\"\n5041         from xarray.core.dataarray import DataArray\n5042 \n5043         other = other.to_dataset() if isinstance(other, DataArray) else other\n5044         merge_result = dataset_merge_method(\n5045             self,\n5046             other,\n5047             overwrite_vars=overwrite_vars,\n5048             compat=compat,\n5049             join=join,\n5050             fill_value=fill_value,\n5051             combine_attrs=combine_attrs,\n5052         )\n5053         return self._replace(**merge_result._asdict())\n5054 \n5055     def _assert_all_in_dataset(\n5056         self, names: Iterable[Hashable], virtual_okay: bool = False\n5057     ) -> None:\n5058         bad_names = set(names) - set(self._variables)\n5059         if virtual_okay:\n5060             bad_names -= self.virtual_variables\n5061         if bad_names:\n5062             ordered_bad_names = [name for name in names if name in bad_names]\n5063             raise ValueError(\n5064                 f\"These variables cannot be found in this dataset: {ordered_bad_names}\"\n5065             )\n5066 \n5067     def drop_vars(\n5068         self: T_Dataset,\n5069         names: Hashable | Iterable[Hashable],\n5070         *,\n5071         errors: ErrorOptions = \"raise\",\n5072     ) -> T_Dataset:\n5073         \"\"\"Drop variables from this dataset.\n5074 \n5075         Parameters\n5076         ----------\n5077         names : hashable or iterable of hashable\n5078             Name(s) of variables to drop.\n5079         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5080             If 'raise', raises a ValueError error if any of the variable\n5081             passed are not in the dataset. If 'ignore', any given names that are in the\n5082             dataset are dropped and no error is raised.\n5083 \n5084         Returns\n5085         -------\n5086         dropped : Dataset\n5087 \n5088         \"\"\"\n5089         # the Iterable check is required for mypy\n5090         if is_scalar(names) or not isinstance(names, Iterable):\n5091             names = {names}\n5092         else:\n5093             names = set(names)\n5094         if errors == \"raise\":\n5095             self._assert_all_in_dataset(names)\n5096 \n5097         # GH6505\n5098         other_names = set()\n5099         for var in names:\n5100             maybe_midx = self._indexes.get(var, None)\n5101             if isinstance(maybe_midx, PandasMultiIndex):\n5102                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])\n5103                 idx_other_names = idx_coord_names - set(names)\n5104                 other_names.update(idx_other_names)\n5105         if other_names:\n5106             names |= set(other_names)\n5107             warnings.warn(\n5108                 f\"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. \"\n5109                 f\"Please also drop the following variables: {other_names!r} to avoid an error in the future.\",\n5110                 DeprecationWarning,\n5111                 stacklevel=2,\n5112             )\n5113 \n5114         assert_no_index_corrupted(self.xindexes, names)\n5115 \n5116         variables = {k: v for k, v in self._variables.items() if k not in names}\n5117         coord_names = {k for k in self._coord_names if k in variables}\n5118         indexes = {k: v for k, v in self._indexes.items() if k not in names}\n5119         return self._replace_with_new_dims(\n5120             variables, coord_names=coord_names, indexes=indexes\n5121         )\n5122 \n5123     def drop_indexes(\n5124         self: T_Dataset,\n5125         coord_names: Hashable | Iterable[Hashable],\n5126         *,\n5127         errors: ErrorOptions = \"raise\",\n5128     ) -> T_Dataset:\n5129         \"\"\"Drop the indexes assigned to the given coordinates.\n5130 \n5131         Parameters\n5132         ----------\n5133         coord_names : hashable or iterable of hashable\n5134             Name(s) of the coordinate(s) for which to drop the index.\n5135         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5136             If 'raise', raises a ValueError error if any of the coordinates\n5137             passed have no index or are not in the dataset.\n5138             If 'ignore', no error is raised.\n5139 \n5140         Returns\n5141         -------\n5142         dropped : Dataset\n5143             A new dataset with dropped indexes.\n5144 \n5145         \"\"\"\n5146         # the Iterable check is required for mypy\n5147         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):\n5148             coord_names = {coord_names}\n5149         else:\n5150             coord_names = set(coord_names)\n5151 \n5152         if errors == \"raise\":\n5153             invalid_coords = coord_names - self._coord_names\n5154             if invalid_coords:\n5155                 raise ValueError(f\"those coordinates don't exist: {invalid_coords}\")\n5156 \n5157             unindexed_coords = set(coord_names) - set(self._indexes)\n5158             if unindexed_coords:\n5159                 raise ValueError(\n5160                     f\"those coordinates do not have an index: {unindexed_coords}\"\n5161                 )\n5162 \n5163         assert_no_index_corrupted(self.xindexes, coord_names, action=\"remove index(es)\")\n5164 \n5165         variables = {}\n5166         for name, var in self._variables.items():\n5167             if name in coord_names:\n5168                 variables[name] = var.to_base_variable()\n5169             else:\n5170                 variables[name] = var\n5171 \n5172         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}\n5173 \n5174         return self._replace(variables=variables, indexes=indexes)\n5175 \n5176     def drop(\n5177         self: T_Dataset,\n5178         labels=None,\n5179         dim=None,\n5180         *,\n5181         errors: ErrorOptions = \"raise\",\n5182         **labels_kwargs,\n5183     ) -> T_Dataset:\n5184         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n5185 \n5186         Using either `drop_vars` or `drop_sel` is encouraged\n5187 \n5188         See Also\n5189         --------\n5190         Dataset.drop_vars\n5191         Dataset.drop_sel\n5192         \"\"\"\n5193         if errors not in [\"raise\", \"ignore\"]:\n5194             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n5195 \n5196         if is_dict_like(labels) and not isinstance(labels, dict):\n5197             warnings.warn(\n5198                 \"dropping coordinates using `drop` is be deprecated; use drop_vars.\",\n5199                 FutureWarning,\n5200                 stacklevel=2,\n5201             )\n5202             return self.drop_vars(labels, errors=errors)\n5203 \n5204         if labels_kwargs or isinstance(labels, dict):\n5205             if dim is not None:\n5206                 raise ValueError(\"cannot specify dim and dict-like arguments.\")\n5207             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n5208 \n5209         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):\n5210             warnings.warn(\n5211                 \"dropping variables using `drop` will be deprecated; using drop_vars is encouraged.\",\n5212                 PendingDeprecationWarning,\n5213                 stacklevel=2,\n5214             )\n5215             return self.drop_vars(labels, errors=errors)\n5216         if dim is not None:\n5217             warnings.warn(\n5218                 \"dropping labels using list-like labels is deprecated; using \"\n5219                 \"dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).\",\n5220                 DeprecationWarning,\n5221                 stacklevel=2,\n5222             )\n5223             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)\n5224 \n5225         warnings.warn(\n5226             \"dropping labels using `drop` will be deprecated; using drop_sel is encouraged.\",\n5227             PendingDeprecationWarning,\n5228             stacklevel=2,\n5229         )\n5230         return self.drop_sel(labels, errors=errors)\n5231 \n5232     def drop_sel(\n5233         self: T_Dataset, labels=None, *, errors: ErrorOptions = \"raise\", **labels_kwargs\n5234     ) -> T_Dataset:\n5235         \"\"\"Drop index labels from this dataset.\n5236 \n5237         Parameters\n5238         ----------\n5239         labels : mapping of hashable to Any\n5240             Index labels to drop\n5241         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5242             If 'raise', raises a ValueError error if\n5243             any of the index labels passed are not\n5244             in the dataset. If 'ignore', any given labels that are in the\n5245             dataset are dropped and no error is raised.\n5246         **labels_kwargs : {dim: label, ...}, optional\n5247             The keyword arguments form of ``dim`` and ``labels``\n5248 \n5249         Returns\n5250         -------\n5251         dropped : Dataset\n5252 \n5253         Examples\n5254         --------\n5255         >>> data = np.arange(6).reshape(2, 3)\n5256         >>> labels = [\"a\", \"b\", \"c\"]\n5257         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n5258         >>> ds\n5259         <xarray.Dataset>\n5260         Dimensions:  (x: 2, y: 3)\n5261         Coordinates:\n5262           * y        (y) <U1 'a' 'b' 'c'\n5263         Dimensions without coordinates: x\n5264         Data variables:\n5265             A        (x, y) int64 0 1 2 3 4 5\n5266         >>> ds.drop_sel(y=[\"a\", \"c\"])\n5267         <xarray.Dataset>\n5268         Dimensions:  (x: 2, y: 1)\n5269         Coordinates:\n5270           * y        (y) <U1 'b'\n5271         Dimensions without coordinates: x\n5272         Data variables:\n5273             A        (x, y) int64 1 4\n5274         >>> ds.drop_sel(y=\"b\")\n5275         <xarray.Dataset>\n5276         Dimensions:  (x: 2, y: 2)\n5277         Coordinates:\n5278           * y        (y) <U1 'a' 'c'\n5279         Dimensions without coordinates: x\n5280         Data variables:\n5281             A        (x, y) int64 0 2 3 5\n5282         \"\"\"\n5283         if errors not in [\"raise\", \"ignore\"]:\n5284             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n5285 \n5286         labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop_sel\")\n5287 \n5288         ds = self\n5289         for dim, labels_for_dim in labels.items():\n5290             # Don't cast to set, as it would harm performance when labels\n5291             # is a large numpy array\n5292             if utils.is_scalar(labels_for_dim):\n5293                 labels_for_dim = [labels_for_dim]\n5294             labels_for_dim = np.asarray(labels_for_dim)\n5295             try:\n5296                 index = self.get_index(dim)\n5297             except KeyError:\n5298                 raise ValueError(f\"dimension {dim!r} does not have coordinate labels\")\n5299             new_index = index.drop(labels_for_dim, errors=errors)\n5300             ds = ds.loc[{dim: new_index}]\n5301         return ds\n5302 \n5303     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:\n5304         \"\"\"Drop index positions from this Dataset.\n5305 \n5306         Parameters\n5307         ----------\n5308         indexers : mapping of hashable to Any\n5309             Index locations to drop\n5310         **indexers_kwargs : {dim: position, ...}, optional\n5311             The keyword arguments form of ``dim`` and ``positions``\n5312 \n5313         Returns\n5314         -------\n5315         dropped : Dataset\n5316 \n5317         Raises\n5318         ------\n5319         IndexError\n5320 \n5321         Examples\n5322         --------\n5323         >>> data = np.arange(6).reshape(2, 3)\n5324         >>> labels = [\"a\", \"b\", \"c\"]\n5325         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n5326         >>> ds\n5327         <xarray.Dataset>\n5328         Dimensions:  (x: 2, y: 3)\n5329         Coordinates:\n5330           * y        (y) <U1 'a' 'b' 'c'\n5331         Dimensions without coordinates: x\n5332         Data variables:\n5333             A        (x, y) int64 0 1 2 3 4 5\n5334         >>> ds.drop_isel(y=[0, 2])\n5335         <xarray.Dataset>\n5336         Dimensions:  (x: 2, y: 1)\n5337         Coordinates:\n5338           * y        (y) <U1 'b'\n5339         Dimensions without coordinates: x\n5340         Data variables:\n5341             A        (x, y) int64 1 4\n5342         >>> ds.drop_isel(y=1)\n5343         <xarray.Dataset>\n5344         Dimensions:  (x: 2, y: 2)\n5345         Coordinates:\n5346           * y        (y) <U1 'a' 'c'\n5347         Dimensions without coordinates: x\n5348         Data variables:\n5349             A        (x, y) int64 0 2 3 5\n5350         \"\"\"\n5351 \n5352         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"drop_isel\")\n5353 \n5354         ds = self\n5355         dimension_index = {}\n5356         for dim, pos_for_dim in indexers.items():\n5357             # Don't cast to set, as it would harm performance when labels\n5358             # is a large numpy array\n5359             if utils.is_scalar(pos_for_dim):\n5360                 pos_for_dim = [pos_for_dim]\n5361             pos_for_dim = np.asarray(pos_for_dim)\n5362             index = self.get_index(dim)\n5363             new_index = index.delete(pos_for_dim)\n5364             dimension_index[dim] = new_index\n5365         ds = ds.loc[dimension_index]\n5366         return ds\n5367 \n5368     def drop_dims(\n5369         self: T_Dataset,\n5370         drop_dims: str | Iterable[Hashable],\n5371         *,\n5372         errors: ErrorOptions = \"raise\",\n5373     ) -> T_Dataset:\n5374         \"\"\"Drop dimensions and associated variables from this dataset.\n5375 \n5376         Parameters\n5377         ----------\n5378         drop_dims : str or Iterable of Hashable\n5379             Dimension or dimensions to drop.\n5380         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5381             If 'raise', raises a ValueError error if any of the\n5382             dimensions passed are not in the dataset. If 'ignore', any given\n5383             dimensions that are in the dataset are dropped and no error is raised.\n5384 \n5385         Returns\n5386         -------\n5387         obj : Dataset\n5388             The dataset without the given dimensions (or any variables\n5389             containing those dimensions).\n5390         \"\"\"\n5391         if errors not in [\"raise\", \"ignore\"]:\n5392             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n5393 \n5394         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):\n5395             drop_dims = {drop_dims}\n5396         else:\n5397             drop_dims = set(drop_dims)\n5398 \n5399         if errors == \"raise\":\n5400             missing_dims = drop_dims - set(self.dims)\n5401             if missing_dims:\n5402                 raise ValueError(\n5403                     f\"Dataset does not contain the dimensions: {missing_dims}\"\n5404                 )\n5405 \n5406         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}\n5407         return self.drop_vars(drop_vars)\n5408 \n5409     def transpose(\n5410         self: T_Dataset,\n5411         *dims: Hashable,\n5412         missing_dims: ErrorOptionsWithWarn = \"raise\",\n5413     ) -> T_Dataset:\n5414         \"\"\"Return a new Dataset object with all array dimensions transposed.\n5415 \n5416         Although the order of dimensions on each array will change, the dataset\n5417         dimensions themselves will remain in fixed (sorted) order.\n5418 \n5419         Parameters\n5420         ----------\n5421         *dims : hashable, optional\n5422             By default, reverse the dimensions on each array. Otherwise,\n5423             reorder the dimensions to this order.\n5424         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n5425             What to do if dimensions that should be selected from are not present in the\n5426             Dataset:\n5427             - \"raise\": raise an exception\n5428             - \"warn\": raise a warning, and ignore the missing dimensions\n5429             - \"ignore\": ignore the missing dimensions\n5430 \n5431         Returns\n5432         -------\n5433         transposed : Dataset\n5434             Each array in the dataset (including) coordinates will be\n5435             transposed to the given order.\n5436 \n5437         Notes\n5438         -----\n5439         This operation returns a view of each array's data. It is\n5440         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n5441         -- the data will be fully loaded into memory.\n5442 \n5443         See Also\n5444         --------\n5445         numpy.transpose\n5446         DataArray.transpose\n5447         \"\"\"\n5448         # Raise error if list is passed as dims\n5449         if (len(dims) > 0) and (isinstance(dims[0], list)):\n5450             list_fix = [f\"{repr(x)}\" if isinstance(x, str) else f\"{x}\" for x in dims[0]]\n5451             raise TypeError(\n5452                 f'transpose requires dims to be passed as multiple arguments. Expected `{\", \".join(list_fix)}`. Received `{dims[0]}` instead'\n5453             )\n5454 \n5455         # Use infix_dims to check once for missing dimensions\n5456         if len(dims) != 0:\n5457             _ = list(infix_dims(dims, self.dims, missing_dims))\n5458 \n5459         ds = self.copy()\n5460         for name, var in self._variables.items():\n5461             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))\n5462             ds._variables[name] = var.transpose(*var_dims)\n5463         return ds\n5464 \n5465     def dropna(\n5466         self: T_Dataset,\n5467         dim: Hashable,\n5468         how: Literal[\"any\", \"all\"] = \"any\",\n5469         thresh: int | None = None,\n5470         subset: Iterable[Hashable] | None = None,\n5471     ) -> T_Dataset:\n5472         \"\"\"Returns a new dataset with dropped labels for missing values along\n5473         the provided dimension.\n5474 \n5475         Parameters\n5476         ----------\n5477         dim : hashable\n5478             Dimension along which to drop missing values. Dropping along\n5479             multiple dimensions simultaneously is not yet supported.\n5480         how : {\"any\", \"all\"}, default: \"any\"\n5481             - any : if any NA values are present, drop that label\n5482             - all : if all values are NA, drop that label\n5483 \n5484         thresh : int or None, optional\n5485             If supplied, require this many non-NA values (summed over all the subset variables).\n5486         subset : iterable of hashable or None, optional\n5487             Which variables to check for missing values. By default, all\n5488             variables in the dataset are checked.\n5489 \n5490         Returns\n5491         -------\n5492         Dataset\n5493         \"\"\"\n5494         # TODO: consider supporting multiple dimensions? Or not, given that\n5495         # there are some ugly edge cases, e.g., pandas's dropna differs\n5496         # depending on the order of the supplied axes.\n5497 \n5498         if dim not in self.dims:\n5499             raise ValueError(f\"{dim} must be a single dataset dimension\")\n5500 \n5501         if subset is None:\n5502             subset = iter(self.data_vars)\n5503 \n5504         count = np.zeros(self.dims[dim], dtype=np.int64)\n5505         size = np.int_(0)  # for type checking\n5506 \n5507         for k in subset:\n5508             array = self._variables[k]\n5509             if dim in array.dims:\n5510                 dims = [d for d in array.dims if d != dim]\n5511                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]\n5512                 size += math.prod([self.dims[d] for d in dims])\n5513 \n5514         if thresh is not None:\n5515             mask = count >= thresh\n5516         elif how == \"any\":\n5517             mask = count == size\n5518         elif how == \"all\":\n5519             mask = count > 0\n5520         elif how is not None:\n5521             raise ValueError(f\"invalid how option: {how}\")\n5522         else:\n5523             raise TypeError(\"must specify how or thresh\")\n5524 \n5525         return self.isel({dim: mask})\n5526 \n5527     def fillna(self: T_Dataset, value: Any) -> T_Dataset:\n5528         \"\"\"Fill missing values in this object.\n5529 \n5530         This operation follows the normal broadcasting and alignment rules that\n5531         xarray uses for binary arithmetic, except the result is aligned to this\n5532         object (``join='left'``) instead of aligned to the intersection of\n5533         index coordinates (``join='inner'``).\n5534 \n5535         Parameters\n5536         ----------\n5537         value : scalar, ndarray, DataArray, dict or Dataset\n5538             Used to fill all matching missing values in this dataset's data\n5539             variables. Scalars, ndarrays or DataArrays arguments are used to\n5540             fill all data with aligned coordinates (for DataArrays).\n5541             Dictionaries or datasets match data variables and then align\n5542             coordinates if necessary.\n5543 \n5544         Returns\n5545         -------\n5546         Dataset\n5547 \n5548         Examples\n5549         --------\n5550         >>> ds = xr.Dataset(\n5551         ...     {\n5552         ...         \"A\": (\"x\", [np.nan, 2, np.nan, 0]),\n5553         ...         \"B\": (\"x\", [3, 4, np.nan, 1]),\n5554         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5]),\n5555         ...         \"D\": (\"x\", [np.nan, 3, np.nan, 4]),\n5556         ...     },\n5557         ...     coords={\"x\": [0, 1, 2, 3]},\n5558         ... )\n5559         >>> ds\n5560         <xarray.Dataset>\n5561         Dimensions:  (x: 4)\n5562         Coordinates:\n5563           * x        (x) int64 0 1 2 3\n5564         Data variables:\n5565             A        (x) float64 nan 2.0 nan 0.0\n5566             B        (x) float64 3.0 4.0 nan 1.0\n5567             C        (x) float64 nan nan nan 5.0\n5568             D        (x) float64 nan 3.0 nan 4.0\n5569 \n5570         Replace all `NaN` values with 0s.\n5571 \n5572         >>> ds.fillna(0)\n5573         <xarray.Dataset>\n5574         Dimensions:  (x: 4)\n5575         Coordinates:\n5576           * x        (x) int64 0 1 2 3\n5577         Data variables:\n5578             A        (x) float64 0.0 2.0 0.0 0.0\n5579             B        (x) float64 3.0 4.0 0.0 1.0\n5580             C        (x) float64 0.0 0.0 0.0 5.0\n5581             D        (x) float64 0.0 3.0 0.0 4.0\n5582 \n5583         Replace all `NaN` elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.\n5584 \n5585         >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n5586         >>> ds.fillna(value=values)\n5587         <xarray.Dataset>\n5588         Dimensions:  (x: 4)\n5589         Coordinates:\n5590           * x        (x) int64 0 1 2 3\n5591         Data variables:\n5592             A        (x) float64 0.0 2.0 0.0 0.0\n5593             B        (x) float64 3.0 4.0 1.0 1.0\n5594             C        (x) float64 2.0 2.0 2.0 5.0\n5595             D        (x) float64 3.0 3.0 3.0 4.0\n5596         \"\"\"\n5597         if utils.is_dict_like(value):\n5598             value_keys = getattr(value, \"data_vars\", value).keys()\n5599             if not set(value_keys) <= set(self.data_vars.keys()):\n5600                 raise ValueError(\n5601                     \"all variables in the argument to `fillna` \"\n5602                     \"must be contained in the original dataset\"\n5603                 )\n5604         out = ops.fillna(self, value)\n5605         return out\n5606 \n5607     def interpolate_na(\n5608         self: T_Dataset,\n5609         dim: Hashable | None = None,\n5610         method: InterpOptions = \"linear\",\n5611         limit: int | None = None,\n5612         use_coordinate: bool | Hashable = True,\n5613         max_gap: (\n5614             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta\n5615         ) = None,\n5616         **kwargs: Any,\n5617     ) -> T_Dataset:\n5618         \"\"\"Fill in NaNs by interpolating according to different methods.\n5619 \n5620         Parameters\n5621         ----------\n5622         dim : Hashable or None, optional\n5623             Specifies the dimension along which to interpolate.\n5624         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n5625             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n5626             String indicating which method to use for interpolation:\n5627 \n5628             - 'linear': linear interpolation. Additional keyword\n5629               arguments are passed to :py:func:`numpy.interp`\n5630             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n5631               are passed to :py:func:`scipy.interpolate.interp1d`. If\n5632               ``method='polynomial'``, the ``order`` keyword argument must also be\n5633               provided.\n5634             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n5635               respective :py:class:`scipy.interpolate` classes.\n5636 \n5637         use_coordinate : bool or Hashable, default: True\n5638             Specifies which index to use as the x values in the interpolation\n5639             formulated as `y = f(x)`. If False, values are treated as if\n5640             equally-spaced along ``dim``. If True, the IndexVariable `dim` is\n5641             used. If ``use_coordinate`` is a string, it specifies the name of a\n5642             coordinate variable to use as the index.\n5643         limit : int, default: None\n5644             Maximum number of consecutive NaNs to fill. Must be greater than 0\n5645             or None for no limit. This filling is done regardless of the size of\n5646             the gap in the data. To only interpolate over gaps less than a given length,\n5647             see ``max_gap``.\n5648         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n5649             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n5650             Use None for no limit. When interpolating along a datetime64 dimension\n5651             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n5652 \n5653             - a string that is valid input for pandas.to_timedelta\n5654             - a :py:class:`numpy.timedelta64` object\n5655             - a :py:class:`pandas.Timedelta` object\n5656             - a :py:class:`datetime.timedelta` object\n5657 \n5658             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n5659             dimensions has not been implemented yet. Gap length is defined as the difference\n5660             between coordinate values at the first data point after a gap and the last value\n5661             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n5662             between coordinate values at the first (last) valid data point and the first (last) NaN.\n5663             For example, consider::\n5664 \n5665                 <xarray.DataArray (x: 9)>\n5666                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n5667                 Coordinates:\n5668                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n5669 \n5670             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n5671         **kwargs : dict, optional\n5672             parameters passed verbatim to the underlying interpolation function\n5673 \n5674         Returns\n5675         -------\n5676         interpolated: Dataset\n5677             Filled in Dataset.\n5678 \n5679         See Also\n5680         --------\n5681         numpy.interp\n5682         scipy.interpolate\n5683 \n5684         Examples\n5685         --------\n5686         >>> ds = xr.Dataset(\n5687         ...     {\n5688         ...         \"A\": (\"x\", [np.nan, 2, 3, np.nan, 0]),\n5689         ...         \"B\": (\"x\", [3, 4, np.nan, 1, 7]),\n5690         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5, 0]),\n5691         ...         \"D\": (\"x\", [np.nan, 3, np.nan, -1, 4]),\n5692         ...     },\n5693         ...     coords={\"x\": [0, 1, 2, 3, 4]},\n5694         ... )\n5695         >>> ds\n5696         <xarray.Dataset>\n5697         Dimensions:  (x: 5)\n5698         Coordinates:\n5699           * x        (x) int64 0 1 2 3 4\n5700         Data variables:\n5701             A        (x) float64 nan 2.0 3.0 nan 0.0\n5702             B        (x) float64 3.0 4.0 nan 1.0 7.0\n5703             C        (x) float64 nan nan nan 5.0 0.0\n5704             D        (x) float64 nan 3.0 nan -1.0 4.0\n5705 \n5706         >>> ds.interpolate_na(dim=\"x\", method=\"linear\")\n5707         <xarray.Dataset>\n5708         Dimensions:  (x: 5)\n5709         Coordinates:\n5710           * x        (x) int64 0 1 2 3 4\n5711         Data variables:\n5712             A        (x) float64 nan 2.0 3.0 1.5 0.0\n5713             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n5714             C        (x) float64 nan nan nan 5.0 0.0\n5715             D        (x) float64 nan 3.0 1.0 -1.0 4.0\n5716 \n5717         >>> ds.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n5718         <xarray.Dataset>\n5719         Dimensions:  (x: 5)\n5720         Coordinates:\n5721           * x        (x) int64 0 1 2 3 4\n5722         Data variables:\n5723             A        (x) float64 1.0 2.0 3.0 1.5 0.0\n5724             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n5725             C        (x) float64 20.0 15.0 10.0 5.0 0.0\n5726             D        (x) float64 5.0 3.0 1.0 -1.0 4.0\n5727         \"\"\"\n5728         from xarray.core.missing import _apply_over_vars_with_dim, interp_na\n5729 \n5730         new = _apply_over_vars_with_dim(\n5731             interp_na,\n5732             self,\n5733             dim=dim,\n5734             method=method,\n5735             limit=limit,\n5736             use_coordinate=use_coordinate,\n5737             max_gap=max_gap,\n5738             **kwargs,\n5739         )\n5740         return new\n5741 \n5742     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:\n5743         \"\"\"Fill NaN values by propagating values forward\n5744 \n5745         *Requires bottleneck.*\n5746 \n5747         Parameters\n5748         ----------\n5749         dim : Hashable\n5750             Specifies the dimension along which to propagate values when\n5751             filling.\n5752         limit : int or None, optional\n5753             The maximum number of consecutive NaN values to forward fill. In\n5754             other words, if there is a gap with more than this number of\n5755             consecutive NaNs, it will only be partially filled. Must be greater\n5756             than 0 or None for no limit. Must be None or greater than or equal\n5757             to axis length if filling along chunked axes (dimensions).\n5758 \n5759         Returns\n5760         -------\n5761         Dataset\n5762         \"\"\"\n5763         from xarray.core.missing import _apply_over_vars_with_dim, ffill\n5764 \n5765         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)\n5766         return new\n5767 \n5768     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:\n5769         \"\"\"Fill NaN values by propagating values backward\n5770 \n5771         *Requires bottleneck.*\n5772 \n5773         Parameters\n5774         ----------\n5775         dim : Hashable\n5776             Specifies the dimension along which to propagate values when\n5777             filling.\n5778         limit : int or None, optional\n5779             The maximum number of consecutive NaN values to backward fill. In\n5780             other words, if there is a gap with more than this number of\n5781             consecutive NaNs, it will only be partially filled. Must be greater\n5782             than 0 or None for no limit. Must be None or greater than or equal\n5783             to axis length if filling along chunked axes (dimensions).\n5784 \n5785         Returns\n5786         -------\n5787         Dataset\n5788         \"\"\"\n5789         from xarray.core.missing import _apply_over_vars_with_dim, bfill\n5790 \n5791         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n5792         return new\n5793 \n5794     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:\n5795         \"\"\"Combine two Datasets, default to data_vars of self.\n5796 \n5797         The new coordinates follow the normal broadcasting and alignment rules\n5798         of ``join='outer'``.  Vacant cells in the expanded coordinates are\n5799         filled with np.nan.\n5800 \n5801         Parameters\n5802         ----------\n5803         other : Dataset\n5804             Used to fill all matching missing values in this array.\n5805 \n5806         Returns\n5807         -------\n5808         Dataset\n5809         \"\"\"\n5810         out = ops.fillna(self, other, join=\"outer\", dataset_join=\"outer\")\n5811         return out\n5812 \n5813     def reduce(\n5814         self: T_Dataset,\n5815         func: Callable,\n5816         dim: Dims = None,\n5817         *,\n5818         keep_attrs: bool | None = None,\n5819         keepdims: bool = False,\n5820         numeric_only: bool = False,\n5821         **kwargs: Any,\n5822     ) -> T_Dataset:\n5823         \"\"\"Reduce this dataset by applying `func` along some dimension(s).\n5824 \n5825         Parameters\n5826         ----------\n5827         func : callable\n5828             Function which can be called in the form\n5829             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n5830             np.ndarray over an integer valued axis.\n5831         dim : str, Iterable of Hashable or None, optional\n5832             Dimension(s) over which to apply `func`. By default `func` is\n5833             applied over all dimensions.\n5834         keep_attrs : bool or None, optional\n5835             If True, the dataset's attributes (`attrs`) will be copied from\n5836             the original object to the new one.  If False (default), the new\n5837             object will be returned without attributes.\n5838         keepdims : bool, default: False\n5839             If True, the dimensions which are reduced are left in the result\n5840             as dimensions of size one. Coordinates that use these dimensions\n5841             are removed.\n5842         numeric_only : bool, default: False\n5843             If True, only apply ``func`` to variables with a numeric dtype.\n5844         **kwargs : Any\n5845             Additional keyword arguments passed on to ``func``.\n5846 \n5847         Returns\n5848         -------\n5849         reduced : Dataset\n5850             Dataset with this object's DataArrays replaced with new DataArrays\n5851             of summarized data and the indicated dimension(s) removed.\n5852         \"\"\"\n5853         if kwargs.get(\"axis\", None) is not None:\n5854             raise ValueError(\n5855                 \"passing 'axis' to Dataset reduce methods is ambiguous.\"\n5856                 \" Please use 'dim' instead.\"\n5857             )\n5858 \n5859         if dim is None or dim is ...:\n5860             dims = set(self.dims)\n5861         elif isinstance(dim, str) or not isinstance(dim, Iterable):\n5862             dims = {dim}\n5863         else:\n5864             dims = set(dim)\n5865 \n5866         missing_dimensions = [d for d in dims if d not in self.dims]\n5867         if missing_dimensions:\n5868             raise ValueError(\n5869                 f\"Dataset does not contain the dimensions: {missing_dimensions}\"\n5870             )\n5871 \n5872         if keep_attrs is None:\n5873             keep_attrs = _get_keep_attrs(default=False)\n5874 \n5875         variables: dict[Hashable, Variable] = {}\n5876         for name, var in self._variables.items():\n5877             reduce_dims = [d for d in var.dims if d in dims]\n5878             if name in self.coords:\n5879                 if not reduce_dims:\n5880                     variables[name] = var\n5881             else:\n5882                 if (\n5883                     # Some reduction functions (e.g. std, var) need to run on variables\n5884                     # that don't have the reduce dims: PR5393\n5885                     not reduce_dims\n5886                     or not numeric_only\n5887                     or np.issubdtype(var.dtype, np.number)\n5888                     or (var.dtype == np.bool_)\n5889                 ):\n5890                     # prefer to aggregate over axis=None rather than\n5891                     # axis=(0, 1) if they will be equivalent, because\n5892                     # the former is often more efficient\n5893                     # keep single-element dims as list, to support Hashables\n5894                     reduce_maybe_single = (\n5895                         None\n5896                         if len(reduce_dims) == var.ndim and var.ndim != 1\n5897                         else reduce_dims\n5898                     )\n5899                     variables[name] = var.reduce(\n5900                         func,\n5901                         dim=reduce_maybe_single,\n5902                         keep_attrs=keep_attrs,\n5903                         keepdims=keepdims,\n5904                         **kwargs,\n5905                     )\n5906 \n5907         coord_names = {k for k in self.coords if k in variables}\n5908         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n5909         attrs = self.attrs if keep_attrs else None\n5910         return self._replace_with_new_dims(\n5911             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n5912         )\n5913 \n5914     def map(\n5915         self: T_Dataset,\n5916         func: Callable,\n5917         keep_attrs: bool | None = None,\n5918         args: Iterable[Any] = (),\n5919         **kwargs: Any,\n5920     ) -> T_Dataset:\n5921         \"\"\"Apply a function to each data variable in this dataset\n5922 \n5923         Parameters\n5924         ----------\n5925         func : callable\n5926             Function which can be called in the form `func(x, *args, **kwargs)`\n5927             to transform each DataArray `x` in this dataset into another\n5928             DataArray.\n5929         keep_attrs : bool or None, optional\n5930             If True, both the dataset's and variables' attributes (`attrs`) will be\n5931             copied from the original objects to the new ones. If False, the new dataset\n5932             and variables will be returned without copying the attributes.\n5933         args : iterable, optional\n5934             Positional arguments passed on to `func`.\n5935         **kwargs : Any\n5936             Keyword arguments passed on to `func`.\n5937 \n5938         Returns\n5939         -------\n5940         applied : Dataset\n5941             Resulting dataset from applying ``func`` to each data variable.\n5942 \n5943         Examples\n5944         --------\n5945         >>> da = xr.DataArray(np.random.randn(2, 3))\n5946         >>> ds = xr.Dataset({\"foo\": da, \"bar\": (\"x\", [-1, 2])})\n5947         >>> ds\n5948         <xarray.Dataset>\n5949         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n5950         Dimensions without coordinates: dim_0, dim_1, x\n5951         Data variables:\n5952             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n5953             bar      (x) int64 -1 2\n5954         >>> ds.map(np.fabs)\n5955         <xarray.Dataset>\n5956         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n5957         Dimensions without coordinates: dim_0, dim_1, x\n5958         Data variables:\n5959             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773\n5960             bar      (x) float64 1.0 2.0\n5961         \"\"\"\n5962         if keep_attrs is None:\n5963             keep_attrs = _get_keep_attrs(default=False)\n5964         variables = {\n5965             k: maybe_wrap_array(v, func(v, *args, **kwargs))\n5966             for k, v in self.data_vars.items()\n5967         }\n5968         if keep_attrs:\n5969             for k, v in variables.items():\n5970                 v._copy_attrs_from(self.data_vars[k])\n5971         attrs = self.attrs if keep_attrs else None\n5972         return type(self)(variables, attrs=attrs)\n5973 \n5974     def apply(\n5975         self: T_Dataset,\n5976         func: Callable,\n5977         keep_attrs: bool | None = None,\n5978         args: Iterable[Any] = (),\n5979         **kwargs: Any,\n5980     ) -> T_Dataset:\n5981         \"\"\"\n5982         Backward compatible implementation of ``map``\n5983 \n5984         See Also\n5985         --------\n5986         Dataset.map\n5987         \"\"\"\n5988         warnings.warn(\n5989             \"Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged\",\n5990             PendingDeprecationWarning,\n5991             stacklevel=2,\n5992         )\n5993         return self.map(func, keep_attrs, args, **kwargs)\n5994 \n5995     def assign(\n5996         self: T_Dataset,\n5997         variables: Mapping[Any, Any] | None = None,\n5998         **variables_kwargs: Any,\n5999     ) -> T_Dataset:\n6000         \"\"\"Assign new data variables to a Dataset, returning a new object\n6001         with all the original variables in addition to the new ones.\n6002 \n6003         Parameters\n6004         ----------\n6005         variables : mapping of hashable to Any\n6006             Mapping from variables names to the new values. If the new values\n6007             are callable, they are computed on the Dataset and assigned to new\n6008             data variables. If the values are not callable, (e.g. a DataArray,\n6009             scalar, or array), they are simply assigned.\n6010         **variables_kwargs\n6011             The keyword arguments form of ``variables``.\n6012             One of variables or variables_kwargs must be provided.\n6013 \n6014         Returns\n6015         -------\n6016         ds : Dataset\n6017             A new Dataset with the new variables in addition to all the\n6018             existing variables.\n6019 \n6020         Notes\n6021         -----\n6022         Since ``kwargs`` is a dictionary, the order of your arguments may not\n6023         be preserved, and so the order of the new variables is not well\n6024         defined. Assigning multiple variables within the same ``assign`` is\n6025         possible, but you cannot reference other variables created within the\n6026         same ``assign`` call.\n6027 \n6028         See Also\n6029         --------\n6030         pandas.DataFrame.assign\n6031 \n6032         Examples\n6033         --------\n6034         >>> x = xr.Dataset(\n6035         ...     {\n6036         ...         \"temperature_c\": (\n6037         ...             (\"lat\", \"lon\"),\n6038         ...             20 * np.random.rand(4).reshape(2, 2),\n6039         ...         ),\n6040         ...         \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n6041         ...     },\n6042         ...     coords={\"lat\": [10, 20], \"lon\": [150, 160]},\n6043         ... )\n6044         >>> x\n6045         <xarray.Dataset>\n6046         Dimensions:        (lat: 2, lon: 2)\n6047         Coordinates:\n6048           * lat            (lat) int64 10 20\n6049           * lon            (lon) int64 150 160\n6050         Data variables:\n6051             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n6052             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n6053 \n6054         Where the value is a callable, evaluated on dataset:\n6055 \n6056         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)\n6057         <xarray.Dataset>\n6058         Dimensions:        (lat: 2, lon: 2)\n6059         Coordinates:\n6060           * lat            (lat) int64 10 20\n6061           * lon            (lon) int64 150 160\n6062         Data variables:\n6063             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n6064             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n6065             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n6066 \n6067         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:\n6068 \n6069         >>> x.assign(temperature_f=x[\"temperature_c\"] * 9 / 5 + 32)\n6070         <xarray.Dataset>\n6071         Dimensions:        (lat: 2, lon: 2)\n6072         Coordinates:\n6073           * lat            (lat) int64 10 20\n6074           * lon            (lon) int64 150 160\n6075         Data variables:\n6076             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n6077             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n6078             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n6079 \n6080         \"\"\"\n6081         variables = either_dict_or_kwargs(variables, variables_kwargs, \"assign\")\n6082         data = self.copy()\n6083         # do all calculations first...\n6084         results: CoercibleMapping = data._calc_assign_results(variables)\n6085         data.coords._maybe_drop_multiindex_coords(set(results.keys()))\n6086         # ... and then assign\n6087         data.update(results)\n6088         return data\n6089 \n6090     def to_array(\n6091         self, dim: Hashable = \"variable\", name: Hashable | None = None\n6092     ) -> DataArray:\n6093         \"\"\"Convert this dataset into an xarray.DataArray\n6094 \n6095         The data variables of this dataset will be broadcast against each other\n6096         and stacked along the first axis of the new array. All coordinates of\n6097         this dataset will remain coordinates.\n6098 \n6099         Parameters\n6100         ----------\n6101         dim : Hashable, default: \"variable\"\n6102             Name of the new dimension.\n6103         name : Hashable or None, optional\n6104             Name of the new data array.\n6105 \n6106         Returns\n6107         -------\n6108         array : xarray.DataArray\n6109         \"\"\"\n6110         from xarray.core.dataarray import DataArray\n6111 \n6112         data_vars = [self.variables[k] for k in self.data_vars]\n6113         broadcast_vars = broadcast_variables(*data_vars)\n6114         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)\n6115 \n6116         dims = (dim,) + broadcast_vars[0].dims\n6117         variable = Variable(dims, data, self.attrs, fastpath=True)\n6118 \n6119         coords = {k: v.variable for k, v in self.coords.items()}\n6120         indexes = filter_indexes_from_coords(self._indexes, set(coords))\n6121         new_dim_index = PandasIndex(list(self.data_vars), dim)\n6122         indexes[dim] = new_dim_index\n6123         coords.update(new_dim_index.create_variables())\n6124 \n6125         return DataArray._construct_direct(variable, coords, name, indexes)\n6126 \n6127     def _normalize_dim_order(\n6128         self, dim_order: Sequence[Hashable] | None = None\n6129     ) -> dict[Hashable, int]:\n6130         \"\"\"\n6131         Check the validity of the provided dimensions if any and return the mapping\n6132         between dimension name and their size.\n6133 \n6134         Parameters\n6135         ----------\n6136         dim_order: Sequence of Hashable or None, optional\n6137             Dimension order to validate (default to the alphabetical order if None).\n6138 \n6139         Returns\n6140         -------\n6141         result : dict[Hashable, int]\n6142             Validated dimensions mapping.\n6143 \n6144         \"\"\"\n6145         if dim_order is None:\n6146             dim_order = list(self.dims)\n6147         elif set(dim_order) != set(self.dims):\n6148             raise ValueError(\n6149                 \"dim_order {} does not match the set of dimensions of this \"\n6150                 \"Dataset: {}\".format(dim_order, list(self.dims))\n6151             )\n6152 \n6153         ordered_dims = {k: self.dims[k] for k in dim_order}\n6154 \n6155         return ordered_dims\n6156 \n6157     def to_pandas(self) -> pd.Series | pd.DataFrame:\n6158         \"\"\"Convert this dataset into a pandas object without changing the number of dimensions.\n6159 \n6160         The type of the returned object depends on the number of Dataset\n6161         dimensions:\n6162 \n6163         * 0D -> `pandas.Series`\n6164         * 1D -> `pandas.DataFrame`\n6165 \n6166         Only works for Datasets with 1 or fewer dimensions.\n6167         \"\"\"\n6168         if len(self.dims) == 0:\n6169             return pd.Series({k: v.item() for k, v in self.items()})\n6170         if len(self.dims) == 1:\n6171             return self.to_dataframe()\n6172         raise ValueError(\n6173             \"cannot convert Datasets with %s dimensions into \"\n6174             \"pandas objects without changing the number of dimensions. \"\n6175             \"Please use Dataset.to_dataframe() instead.\" % len(self.dims)\n6176         )\n6177 \n6178     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):\n6179         columns = [k for k in self.variables if k not in self.dims]\n6180         data = [\n6181             self._variables[k].set_dims(ordered_dims).values.reshape(-1)\n6182             for k in columns\n6183         ]\n6184         index = self.coords.to_index([*ordered_dims])\n6185         return pd.DataFrame(dict(zip(columns, data)), index=index)\n6186 \n6187     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:\n6188         \"\"\"Convert this dataset into a pandas.DataFrame.\n6189 \n6190         Non-index variables in this dataset form the columns of the\n6191         DataFrame. The DataFrame is indexed by the Cartesian product of\n6192         this dataset's indices.\n6193 \n6194         Parameters\n6195         ----------\n6196         dim_order: Sequence of Hashable or None, optional\n6197             Hierarchical dimension order for the resulting dataframe. All\n6198             arrays are transposed to this order and then written out as flat\n6199             vectors in contiguous order, so the last dimension in this list\n6200             will be contiguous in the resulting DataFrame. This has a major\n6201             influence on which operations are efficient on the resulting\n6202             dataframe.\n6203 \n6204             If provided, must include all dimensions of this dataset. By\n6205             default, dimensions are sorted alphabetically.\n6206 \n6207         Returns\n6208         -------\n6209         result : DataFrame\n6210             Dataset as a pandas DataFrame.\n6211 \n6212         \"\"\"\n6213 \n6214         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n6215 \n6216         return self._to_dataframe(ordered_dims=ordered_dims)\n6217 \n6218     def _set_sparse_data_from_dataframe(\n6219         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple\n6220     ) -> None:\n6221         from sparse import COO\n6222 \n6223         if isinstance(idx, pd.MultiIndex):\n6224             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)\n6225             is_sorted = idx.is_monotonic_increasing\n6226             shape = tuple(lev.size for lev in idx.levels)\n6227         else:\n6228             coords = np.arange(idx.size).reshape(1, -1)\n6229             is_sorted = True\n6230             shape = (idx.size,)\n6231 \n6232         for name, values in arrays:\n6233             # In virtually all real use cases, the sparse array will now have\n6234             # missing values and needs a fill_value. For consistency, don't\n6235             # special case the rare exceptions (e.g., dtype=int without a\n6236             # MultiIndex).\n6237             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)\n6238             values = np.asarray(values, dtype=dtype)\n6239 \n6240             data = COO(\n6241                 coords,\n6242                 values,\n6243                 shape,\n6244                 has_duplicates=False,\n6245                 sorted=is_sorted,\n6246                 fill_value=fill_value,\n6247             )\n6248             self[name] = (dims, data)\n6249 \n6250     def _set_numpy_data_from_dataframe(\n6251         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple\n6252     ) -> None:\n6253         if not isinstance(idx, pd.MultiIndex):\n6254             for name, values in arrays:\n6255                 self[name] = (dims, values)\n6256             return\n6257 \n6258         # NB: similar, more general logic, now exists in\n6259         # variable.unstack_once; we could consider combining them at some\n6260         # point.\n6261 \n6262         shape = tuple(lev.size for lev in idx.levels)\n6263         indexer = tuple(idx.codes)\n6264 \n6265         # We already verified that the MultiIndex has all unique values, so\n6266         # there are missing values if and only if the size of output arrays is\n6267         # larger that the index.\n6268         missing_values = math.prod(shape) > idx.shape[0]\n6269 \n6270         for name, values in arrays:\n6271             # NumPy indexing is much faster than using DataFrame.reindex() to\n6272             # fill in missing values:\n6273             # https://stackoverflow.com/a/35049899/809705\n6274             if missing_values:\n6275                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)\n6276                 data = np.full(shape, fill_value, dtype)\n6277             else:\n6278                 # If there are no missing values, keep the existing dtype\n6279                 # instead of promoting to support NA, e.g., keep integer\n6280                 # columns as integers.\n6281                 # TODO: consider removing this special case, which doesn't\n6282                 # exist for sparse=True.\n6283                 data = np.zeros(shape, values.dtype)\n6284             data[indexer] = values\n6285             self[name] = (dims, data)\n6286 \n6287     @classmethod\n6288     def from_dataframe(\n6289         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False\n6290     ) -> T_Dataset:\n6291         \"\"\"Convert a pandas.DataFrame into an xarray.Dataset\n6292 \n6293         Each column will be converted into an independent variable in the\n6294         Dataset. If the dataframe's index is a MultiIndex, it will be expanded\n6295         into a tensor product of one-dimensional indices (filling in missing\n6296         values with NaN). This method will produce a Dataset very similar to\n6297         that on which the 'to_dataframe' method was called, except with\n6298         possibly redundant dimensions (since all dataset variables will have\n6299         the same dimensionality)\n6300 \n6301         Parameters\n6302         ----------\n6303         dataframe : DataFrame\n6304             DataFrame from which to copy data and indices.\n6305         sparse : bool, default: False\n6306             If true, create a sparse arrays instead of dense numpy arrays. This\n6307             can potentially save a large amount of memory if the DataFrame has\n6308             a MultiIndex. Requires the sparse package (sparse.pydata.org).\n6309 \n6310         Returns\n6311         -------\n6312         New Dataset.\n6313 \n6314         See Also\n6315         --------\n6316         xarray.DataArray.from_series\n6317         pandas.DataFrame.to_xarray\n6318         \"\"\"\n6319         # TODO: Add an option to remove dimensions along which the variables\n6320         # are constant, to enable consistent serialization to/from a dataframe,\n6321         # even if some variables have different dimensionality.\n6322 \n6323         if not dataframe.columns.is_unique:\n6324             raise ValueError(\"cannot convert DataFrame with non-unique columns\")\n6325 \n6326         idx = remove_unused_levels_categories(dataframe.index)\n6327 \n6328         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:\n6329             raise ValueError(\n6330                 \"cannot convert a DataFrame with a non-unique MultiIndex into xarray\"\n6331             )\n6332 \n6333         # Cast to a NumPy array first, in case the Series is a pandas Extension\n6334         # array (which doesn't have a valid NumPy dtype)\n6335         # TODO: allow users to control how this casting happens, e.g., by\n6336         # forwarding arguments to pandas.Series.to_numpy?\n6337         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]\n6338 \n6339         indexes: dict[Hashable, Index] = {}\n6340         index_vars: dict[Hashable, Variable] = {}\n6341 \n6342         if isinstance(idx, pd.MultiIndex):\n6343             dims = tuple(\n6344                 name if name is not None else \"level_%i\" % n\n6345                 for n, name in enumerate(idx.names)\n6346             )\n6347             for dim, lev in zip(dims, idx.levels):\n6348                 xr_idx = PandasIndex(lev, dim)\n6349                 indexes[dim] = xr_idx\n6350                 index_vars.update(xr_idx.create_variables())\n6351         else:\n6352             index_name = idx.name if idx.name is not None else \"index\"\n6353             dims = (index_name,)\n6354             xr_idx = PandasIndex(idx, index_name)\n6355             indexes[index_name] = xr_idx\n6356             index_vars.update(xr_idx.create_variables())\n6357 \n6358         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)\n6359 \n6360         if sparse:\n6361             obj._set_sparse_data_from_dataframe(idx, arrays, dims)\n6362         else:\n6363             obj._set_numpy_data_from_dataframe(idx, arrays, dims)\n6364         return obj\n6365 \n6366     def to_dask_dataframe(\n6367         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False\n6368     ) -> DaskDataFrame:\n6369         \"\"\"\n6370         Convert this dataset into a dask.dataframe.DataFrame.\n6371 \n6372         The dimensions, coordinates and data variables in this dataset form\n6373         the columns of the DataFrame.\n6374 \n6375         Parameters\n6376         ----------\n6377         dim_order : list, optional\n6378             Hierarchical dimension order for the resulting dataframe. All\n6379             arrays are transposed to this order and then written out as flat\n6380             vectors in contiguous order, so the last dimension in this list\n6381             will be contiguous in the resulting DataFrame. This has a major\n6382             influence on which operations are efficient on the resulting dask\n6383             dataframe.\n6384 \n6385             If provided, must include all dimensions of this dataset. By\n6386             default, dimensions are sorted alphabetically.\n6387         set_index : bool, default: False\n6388             If set_index=True, the dask DataFrame is indexed by this dataset's\n6389             coordinate. Since dask DataFrames do not support multi-indexes,\n6390             set_index only works if the dataset only contains one dimension.\n6391 \n6392         Returns\n6393         -------\n6394         dask.dataframe.DataFrame\n6395         \"\"\"\n6396 \n6397         import dask.array as da\n6398         import dask.dataframe as dd\n6399 \n6400         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n6401 \n6402         columns = list(ordered_dims)\n6403         columns.extend(k for k in self.coords if k not in self.dims)\n6404         columns.extend(self.data_vars)\n6405 \n6406         series_list = []\n6407         for name in columns:\n6408             try:\n6409                 var = self.variables[name]\n6410             except KeyError:\n6411                 # dimension without a matching coordinate\n6412                 size = self.dims[name]\n6413                 data = da.arange(size, chunks=size, dtype=np.int64)\n6414                 var = Variable((name,), data)\n6415 \n6416             # IndexVariable objects have a dummy .chunk() method\n6417             if isinstance(var, IndexVariable):\n6418                 var = var.to_base_variable()\n6419 \n6420             # Make sure var is a dask array, otherwise the array can become too large\n6421             # when it is broadcasted to several dimensions:\n6422             if not is_duck_dask_array(var._data):\n6423                 var = var.chunk()\n6424 \n6425             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data\n6426             series = dd.from_array(dask_array.reshape(-1), columns=[name])\n6427             series_list.append(series)\n6428 \n6429         df = dd.concat(series_list, axis=1)\n6430 \n6431         if set_index:\n6432             dim_order = [*ordered_dims]\n6433 \n6434             if len(dim_order) == 1:\n6435                 (dim,) = dim_order\n6436                 df = df.set_index(dim)\n6437             else:\n6438                 # triggers an error about multi-indexes, even if only one\n6439                 # dimension is passed\n6440                 df = df.set_index(dim_order)\n6441 \n6442         return df\n6443 \n6444     def to_dict(\n6445         self, data: bool | Literal[\"list\", \"array\"] = \"list\", encoding: bool = False\n6446     ) -> dict[str, Any]:\n6447         \"\"\"\n6448         Convert this dataset to a dictionary following xarray naming\n6449         conventions.\n6450 \n6451         Converts all variables and attributes to native Python objects\n6452         Useful for converting to json. To avoid datetime incompatibility\n6453         use decode_times=False kwarg in xarrray.open_dataset.\n6454 \n6455         Parameters\n6456         ----------\n6457         data : bool or {\"list\", \"array\"}, default: \"list\"\n6458             Whether to include the actual data in the dictionary. When set to\n6459             False, returns just the schema. If set to \"array\", returns data as\n6460             underlying array type. If set to \"list\" (or True for backwards\n6461             compatibility), returns data in lists of Python data types. Note\n6462             that for obtaining the \"list\" output efficiently, use\n6463             `ds.compute().to_dict(data=\"list\")`.\n6464 \n6465         encoding : bool, default: False\n6466             Whether to include the Dataset's encoding in the dictionary.\n6467 \n6468         Returns\n6469         -------\n6470         d : dict\n6471             Dict with keys: \"coords\", \"attrs\", \"dims\", \"data_vars\" and optionally\n6472             \"encoding\".\n6473 \n6474         See Also\n6475         --------\n6476         Dataset.from_dict\n6477         DataArray.to_dict\n6478         \"\"\"\n6479         d: dict = {\n6480             \"coords\": {},\n6481             \"attrs\": decode_numpy_dict_values(self.attrs),\n6482             \"dims\": dict(self.dims),\n6483             \"data_vars\": {},\n6484         }\n6485         for k in self.coords:\n6486             d[\"coords\"].update(\n6487                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}\n6488             )\n6489         for k in self.data_vars:\n6490             d[\"data_vars\"].update(\n6491                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}\n6492             )\n6493         if encoding:\n6494             d[\"encoding\"] = dict(self.encoding)\n6495         return d\n6496 \n6497     @classmethod\n6498     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:\n6499         \"\"\"Convert a dictionary into an xarray.Dataset.\n6500 \n6501         Parameters\n6502         ----------\n6503         d : dict-like\n6504             Mapping with a minimum structure of\n6505                 ``{\"var_0\": {\"dims\": [..], \"data\": [..]}, \\\n6506                             ...}``\n6507 \n6508         Returns\n6509         -------\n6510         obj : Dataset\n6511 \n6512         See also\n6513         --------\n6514         Dataset.to_dict\n6515         DataArray.from_dict\n6516 \n6517         Examples\n6518         --------\n6519         >>> d = {\n6520         ...     \"t\": {\"dims\": (\"t\"), \"data\": [0, 1, 2]},\n6521         ...     \"a\": {\"dims\": (\"t\"), \"data\": [\"a\", \"b\", \"c\"]},\n6522         ...     \"b\": {\"dims\": (\"t\"), \"data\": [10, 20, 30]},\n6523         ... }\n6524         >>> ds = xr.Dataset.from_dict(d)\n6525         >>> ds\n6526         <xarray.Dataset>\n6527         Dimensions:  (t: 3)\n6528         Coordinates:\n6529           * t        (t) int64 0 1 2\n6530         Data variables:\n6531             a        (t) <U1 'a' 'b' 'c'\n6532             b        (t) int64 10 20 30\n6533 \n6534         >>> d = {\n6535         ...     \"coords\": {\n6536         ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n6537         ...     },\n6538         ...     \"attrs\": {\"title\": \"air temperature\"},\n6539         ...     \"dims\": \"t\",\n6540         ...     \"data_vars\": {\n6541         ...         \"a\": {\"dims\": \"t\", \"data\": [10, 20, 30]},\n6542         ...         \"b\": {\"dims\": \"t\", \"data\": [\"a\", \"b\", \"c\"]},\n6543         ...     },\n6544         ... }\n6545         >>> ds = xr.Dataset.from_dict(d)\n6546         >>> ds\n6547         <xarray.Dataset>\n6548         Dimensions:  (t: 3)\n6549         Coordinates:\n6550           * t        (t) int64 0 1 2\n6551         Data variables:\n6552             a        (t) int64 10 20 30\n6553             b        (t) <U1 'a' 'b' 'c'\n6554         Attributes:\n6555             title:    air temperature\n6556 \n6557         \"\"\"\n6558 \n6559         variables: Iterable[tuple[Hashable, Any]]\n6560         if not {\"coords\", \"data_vars\"}.issubset(set(d)):\n6561             variables = d.items()\n6562         else:\n6563             import itertools\n6564 \n6565             variables = itertools.chain(\n6566                 d.get(\"coords\", {}).items(), d.get(\"data_vars\", {}).items()\n6567             )\n6568         try:\n6569             variable_dict = {\n6570                 k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"), v.get(\"encoding\"))\n6571                 for k, v in variables\n6572             }\n6573         except KeyError as e:\n6574             raise ValueError(\n6575                 \"cannot convert dict without the key \"\n6576                 \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n6577             )\n6578         obj = cls(variable_dict)\n6579 \n6580         # what if coords aren't dims?\n6581         coords = set(d.get(\"coords\", {})) - set(d.get(\"dims\", {}))\n6582         obj = obj.set_coords(coords)\n6583 \n6584         obj.attrs.update(d.get(\"attrs\", {}))\n6585         obj.encoding.update(d.get(\"encoding\", {}))\n6586 \n6587         return obj\n6588 \n6589     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:\n6590         variables = {}\n6591         keep_attrs = kwargs.pop(\"keep_attrs\", None)\n6592         if keep_attrs is None:\n6593             keep_attrs = _get_keep_attrs(default=True)\n6594         for k, v in self._variables.items():\n6595             if k in self._coord_names:\n6596                 variables[k] = v\n6597             else:\n6598                 variables[k] = f(v, *args, **kwargs)\n6599                 if keep_attrs:\n6600                     variables[k].attrs = v._attrs\n6601         attrs = self._attrs if keep_attrs else None\n6602         return self._replace_with_new_dims(variables, attrs=attrs)\n6603 \n6604     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:\n6605         from xarray.core.dataarray import DataArray\n6606         from xarray.core.groupby import GroupBy\n6607 \n6608         if isinstance(other, GroupBy):\n6609             return NotImplemented\n6610         align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n6611         if isinstance(other, (DataArray, Dataset)):\n6612             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]\n6613         g = f if not reflexive else lambda x, y: f(y, x)\n6614         ds = self._calculate_binary_op(g, other, join=align_type)\n6615         keep_attrs = _get_keep_attrs(default=False)\n6616         if keep_attrs:\n6617             ds.attrs = self.attrs\n6618         return ds\n6619 \n6620     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:\n6621         from xarray.core.dataarray import DataArray\n6622         from xarray.core.groupby import GroupBy\n6623 \n6624         if isinstance(other, GroupBy):\n6625             raise TypeError(\n6626                 \"in-place operations between a Dataset and \"\n6627                 \"a grouped object are not permitted\"\n6628             )\n6629         # we don't actually modify arrays in-place with in-place Dataset\n6630         # arithmetic -- this lets us automatically align things\n6631         if isinstance(other, (DataArray, Dataset)):\n6632             other = other.reindex_like(self, copy=False)\n6633         g = ops.inplace_to_noninplace_op(f)\n6634         ds = self._calculate_binary_op(g, other, inplace=True)\n6635         self._replace_with_new_dims(\n6636             ds._variables,\n6637             ds._coord_names,\n6638             attrs=ds._attrs,\n6639             indexes=ds._indexes,\n6640             inplace=True,\n6641         )\n6642         return self\n6643 \n6644     def _calculate_binary_op(\n6645         self, f, other, join=\"inner\", inplace: bool = False\n6646     ) -> Dataset:\n6647         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n6648             if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n6649                 raise ValueError(\n6650                     \"datasets must have the same data variables \"\n6651                     f\"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}\"\n6652                 )\n6653 \n6654             dest_vars = {}\n6655 \n6656             for k in lhs_data_vars:\n6657                 if k in rhs_data_vars:\n6658                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n6659                 elif join in [\"left\", \"outer\"]:\n6660                     dest_vars[k] = f(lhs_vars[k], np.nan)\n6661             for k in rhs_data_vars:\n6662                 if k not in dest_vars and join in [\"right\", \"outer\"]:\n6663                     dest_vars[k] = f(rhs_vars[k], np.nan)\n6664             return dest_vars\n6665 \n6666         if utils.is_dict_like(other) and not isinstance(other, Dataset):\n6667             # can't use our shortcut of doing the binary operation with\n6668             # Variable objects, so apply over our data vars instead.\n6669             new_data_vars = apply_over_both(\n6670                 self.data_vars, other, self.data_vars, other\n6671             )\n6672             return type(self)(new_data_vars)\n6673 \n6674         other_coords: Coordinates | None = getattr(other, \"coords\", None)\n6675         ds = self.coords.merge(other_coords)\n6676 \n6677         if isinstance(other, Dataset):\n6678             new_vars = apply_over_both(\n6679                 self.data_vars, other.data_vars, self.variables, other.variables\n6680             )\n6681         else:\n6682             other_variable = getattr(other, \"variable\", other)\n6683             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n6684         ds._variables.update(new_vars)\n6685         ds._dims = calculate_dimensions(ds._variables)\n6686         return ds\n6687 \n6688     def _copy_attrs_from(self, other):\n6689         self.attrs = other.attrs\n6690         for v in other.variables:\n6691             if v in self.variables:\n6692                 self.variables[v].attrs = other.variables[v].attrs\n6693 \n6694     def diff(\n6695         self: T_Dataset,\n6696         dim: Hashable,\n6697         n: int = 1,\n6698         label: Literal[\"upper\", \"lower\"] = \"upper\",\n6699     ) -> T_Dataset:\n6700         \"\"\"Calculate the n-th order discrete difference along given axis.\n6701 \n6702         Parameters\n6703         ----------\n6704         dim : Hashable\n6705             Dimension over which to calculate the finite difference.\n6706         n : int, default: 1\n6707             The number of times values are differenced.\n6708         label : {\"upper\", \"lower\"}, default: \"upper\"\n6709             The new coordinate in dimension ``dim`` will have the\n6710             values of either the minuend's or subtrahend's coordinate\n6711             for values 'upper' and 'lower', respectively.\n6712 \n6713         Returns\n6714         -------\n6715         difference : Dataset\n6716             The n-th order finite difference of this object.\n6717 \n6718         Notes\n6719         -----\n6720         `n` matches numpy's behavior and is different from pandas' first argument named\n6721         `periods`.\n6722 \n6723         Examples\n6724         --------\n6725         >>> ds = xr.Dataset({\"foo\": (\"x\", [5, 5, 6, 6])})\n6726         >>> ds.diff(\"x\")\n6727         <xarray.Dataset>\n6728         Dimensions:  (x: 3)\n6729         Dimensions without coordinates: x\n6730         Data variables:\n6731             foo      (x) int64 0 1 0\n6732         >>> ds.diff(\"x\", 2)\n6733         <xarray.Dataset>\n6734         Dimensions:  (x: 2)\n6735         Dimensions without coordinates: x\n6736         Data variables:\n6737             foo      (x) int64 1 -1\n6738 \n6739         See Also\n6740         --------\n6741         Dataset.differentiate\n6742         \"\"\"\n6743         if n == 0:\n6744             return self\n6745         if n < 0:\n6746             raise ValueError(f\"order `n` must be non-negative but got {n}\")\n6747 \n6748         # prepare slices\n6749         slice_start = {dim: slice(None, -1)}\n6750         slice_end = {dim: slice(1, None)}\n6751 \n6752         # prepare new coordinate\n6753         if label == \"upper\":\n6754             slice_new = slice_end\n6755         elif label == \"lower\":\n6756             slice_new = slice_start\n6757         else:\n6758             raise ValueError(\"The 'label' argument has to be either 'upper' or 'lower'\")\n6759 \n6760         indexes, index_vars = isel_indexes(self.xindexes, slice_new)\n6761         variables = {}\n6762 \n6763         for name, var in self.variables.items():\n6764             if name in index_vars:\n6765                 variables[name] = index_vars[name]\n6766             elif dim in var.dims:\n6767                 if name in self.data_vars:\n6768                     variables[name] = var.isel(slice_end) - var.isel(slice_start)\n6769                 else:\n6770                     variables[name] = var.isel(slice_new)\n6771             else:\n6772                 variables[name] = var\n6773 \n6774         difference = self._replace_with_new_dims(variables, indexes=indexes)\n6775 \n6776         if n > 1:\n6777             return difference.diff(dim, n - 1)\n6778         else:\n6779             return difference\n6780 \n6781     def shift(\n6782         self: T_Dataset,\n6783         shifts: Mapping[Any, int] | None = None,\n6784         fill_value: Any = xrdtypes.NA,\n6785         **shifts_kwargs: int,\n6786     ) -> T_Dataset:\n6787         \"\"\"Shift this dataset by an offset along one or more dimensions.\n6788 \n6789         Only data variables are moved; coordinates stay in place. This is\n6790         consistent with the behavior of ``shift`` in pandas.\n6791 \n6792         Values shifted from beyond array bounds will appear at one end of\n6793         each dimension, which are filled according to `fill_value`. For periodic\n6794         offsets instead see `roll`.\n6795 \n6796         Parameters\n6797         ----------\n6798         shifts : mapping of hashable to int\n6799             Integer offset to shift along each of the given dimensions.\n6800             Positive offsets shift to the right; negative offsets shift to the\n6801             left.\n6802         fill_value : scalar or dict-like, optional\n6803             Value to use for newly missing values. If a dict-like, maps\n6804             variable names (including coordinates) to fill values.\n6805         **shifts_kwargs\n6806             The keyword arguments form of ``shifts``.\n6807             One of shifts or shifts_kwargs must be provided.\n6808 \n6809         Returns\n6810         -------\n6811         shifted : Dataset\n6812             Dataset with the same coordinates and attributes but shifted data\n6813             variables.\n6814 \n6815         See Also\n6816         --------\n6817         roll\n6818 \n6819         Examples\n6820         --------\n6821         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n6822         >>> ds.shift(x=2)\n6823         <xarray.Dataset>\n6824         Dimensions:  (x: 5)\n6825         Dimensions without coordinates: x\n6826         Data variables:\n6827             foo      (x) object nan nan 'a' 'b' 'c'\n6828         \"\"\"\n6829         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n6830         invalid = [k for k in shifts if k not in self.dims]\n6831         if invalid:\n6832             raise ValueError(f\"dimensions {invalid!r} do not exist\")\n6833 \n6834         variables = {}\n6835         for name, var in self.variables.items():\n6836             if name in self.data_vars:\n6837                 fill_value_ = (\n6838                     fill_value.get(name, xrdtypes.NA)\n6839                     if isinstance(fill_value, dict)\n6840                     else fill_value\n6841                 )\n6842 \n6843                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}\n6844                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)\n6845             else:\n6846                 variables[name] = var\n6847 \n6848         return self._replace(variables)\n6849 \n6850     def roll(\n6851         self: T_Dataset,\n6852         shifts: Mapping[Any, int] | None = None,\n6853         roll_coords: bool = False,\n6854         **shifts_kwargs: int,\n6855     ) -> T_Dataset:\n6856         \"\"\"Roll this dataset by an offset along one or more dimensions.\n6857 \n6858         Unlike shift, roll treats the given dimensions as periodic, so will not\n6859         create any missing values to be filled.\n6860 \n6861         Also unlike shift, roll may rotate all variables, including coordinates\n6862         if specified. The direction of rotation is consistent with\n6863         :py:func:`numpy.roll`.\n6864 \n6865         Parameters\n6866         ----------\n6867         shifts : mapping of hashable to int, optional\n6868             A dict with keys matching dimensions and values given\n6869             by integers to rotate each of the given dimensions. Positive\n6870             offsets roll to the right; negative offsets roll to the left.\n6871         roll_coords : bool, default: False\n6872             Indicates whether to roll the coordinates by the offset too.\n6873         **shifts_kwargs : {dim: offset, ...}, optional\n6874             The keyword arguments form of ``shifts``.\n6875             One of shifts or shifts_kwargs must be provided.\n6876 \n6877         Returns\n6878         -------\n6879         rolled : Dataset\n6880             Dataset with the same attributes but rolled data and coordinates.\n6881 \n6882         See Also\n6883         --------\n6884         shift\n6885 \n6886         Examples\n6887         --------\n6888         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))}, coords={\"x\": np.arange(5)})\n6889         >>> ds.roll(x=2)\n6890         <xarray.Dataset>\n6891         Dimensions:  (x: 5)\n6892         Coordinates:\n6893           * x        (x) int64 0 1 2 3 4\n6894         Data variables:\n6895             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n6896 \n6897         >>> ds.roll(x=2, roll_coords=True)\n6898         <xarray.Dataset>\n6899         Dimensions:  (x: 5)\n6900         Coordinates:\n6901           * x        (x) int64 3 4 0 1 2\n6902         Data variables:\n6903             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n6904 \n6905         \"\"\"\n6906         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n6907         invalid = [k for k in shifts if k not in self.dims]\n6908         if invalid:\n6909             raise ValueError(f\"dimensions {invalid!r} do not exist\")\n6910 \n6911         unrolled_vars: tuple[Hashable, ...]\n6912 \n6913         if roll_coords:\n6914             indexes, index_vars = roll_indexes(self.xindexes, shifts)\n6915             unrolled_vars = ()\n6916         else:\n6917             indexes = dict(self._indexes)\n6918             index_vars = dict(self.xindexes.variables)\n6919             unrolled_vars = tuple(self.coords)\n6920 \n6921         variables = {}\n6922         for k, var in self.variables.items():\n6923             if k in index_vars:\n6924                 variables[k] = index_vars[k]\n6925             elif k not in unrolled_vars:\n6926                 variables[k] = var.roll(\n6927                     shifts={k: s for k, s in shifts.items() if k in var.dims}\n6928                 )\n6929             else:\n6930                 variables[k] = var\n6931 \n6932         return self._replace(variables, indexes=indexes)\n6933 \n6934     def sortby(\n6935         self: T_Dataset,\n6936         variables: Hashable | DataArray | list[Hashable | DataArray],\n6937         ascending: bool = True,\n6938     ) -> T_Dataset:\n6939         \"\"\"\n6940         Sort object by labels or values (along an axis).\n6941 \n6942         Sorts the dataset, either along specified dimensions,\n6943         or according to values of 1-D dataarrays that share dimension\n6944         with calling object.\n6945 \n6946         If the input variables are dataarrays, then the dataarrays are aligned\n6947         (via left-join) to the calling object prior to sorting by cell values.\n6948         NaNs are sorted to the end, following Numpy convention.\n6949 \n6950         If multiple sorts along the same dimension is\n6951         given, numpy's lexsort is performed along that dimension:\n6952         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n6953         and the FIRST key in the sequence is used as the primary sort key,\n6954         followed by the 2nd key, etc.\n6955 \n6956         Parameters\n6957         ----------\n6958         variables : Hashable, DataArray, or list of hashable or DataArray\n6959             1D DataArray objects or name(s) of 1D variable(s) in\n6960             coords/data_vars whose values are used to sort the dataset.\n6961         ascending : bool, default: True\n6962             Whether to sort by ascending or descending order.\n6963 \n6964         Returns\n6965         -------\n6966         sorted : Dataset\n6967             A new dataset where all the specified dims are sorted by dim\n6968             labels.\n6969 \n6970         See Also\n6971         --------\n6972         DataArray.sortby\n6973         numpy.sort\n6974         pandas.sort_values\n6975         pandas.sort_index\n6976 \n6977         Examples\n6978         --------\n6979         >>> ds = xr.Dataset(\n6980         ...     {\n6981         ...         \"A\": ((\"x\", \"y\"), [[1, 2], [3, 4]]),\n6982         ...         \"B\": ((\"x\", \"y\"), [[5, 6], [7, 8]]),\n6983         ...     },\n6984         ...     coords={\"x\": [\"b\", \"a\"], \"y\": [1, 0]},\n6985         ... )\n6986         >>> ds = ds.sortby(\"x\")\n6987         >>> ds\n6988         <xarray.Dataset>\n6989         Dimensions:  (x: 2, y: 2)\n6990         Coordinates:\n6991           * x        (x) <U1 'a' 'b'\n6992           * y        (y) int64 1 0\n6993         Data variables:\n6994             A        (x, y) int64 3 4 1 2\n6995             B        (x, y) int64 7 8 5 6\n6996         \"\"\"\n6997         from xarray.core.dataarray import DataArray\n6998 \n6999         if not isinstance(variables, list):\n7000             variables = [variables]\n7001         else:\n7002             variables = variables\n7003         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]\n7004         aligned_vars = align(self, *arrays, join=\"left\")  # type: ignore[type-var]\n7005         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]\n7006         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]\n7007         vars_by_dim = defaultdict(list)\n7008         for data_array in aligned_other_vars:\n7009             if data_array.ndim != 1:\n7010                 raise ValueError(\"Input DataArray is not 1-D.\")\n7011             (key,) = data_array.dims\n7012             vars_by_dim[key].append(data_array)\n7013 \n7014         indices = {}\n7015         for key, arrays in vars_by_dim.items():\n7016             order = np.lexsort(tuple(reversed(arrays)))\n7017             indices[key] = order if ascending else order[::-1]\n7018         return aligned_self.isel(indices)\n7019 \n7020     def quantile(\n7021         self: T_Dataset,\n7022         q: ArrayLike,\n7023         dim: Dims = None,\n7024         method: QuantileMethods = \"linear\",\n7025         numeric_only: bool = False,\n7026         keep_attrs: bool | None = None,\n7027         skipna: bool | None = None,\n7028         interpolation: QuantileMethods | None = None,\n7029     ) -> T_Dataset:\n7030         \"\"\"Compute the qth quantile of the data along the specified dimension.\n7031 \n7032         Returns the qth quantiles(s) of the array elements for each variable\n7033         in the Dataset.\n7034 \n7035         Parameters\n7036         ----------\n7037         q : float or array-like of float\n7038             Quantile to compute, which must be between 0 and 1 inclusive.\n7039         dim : str or Iterable of Hashable, optional\n7040             Dimension(s) over which to apply quantile.\n7041         method : str, default: \"linear\"\n7042             This optional parameter specifies the interpolation method to use when the\n7043             desired quantile lies between two data points. The options sorted by their R\n7044             type as summarized in the H&F paper [1]_ are:\n7045 \n7046                 1. \"inverted_cdf\" (*)\n7047                 2. \"averaged_inverted_cdf\" (*)\n7048                 3. \"closest_observation\" (*)\n7049                 4. \"interpolated_inverted_cdf\" (*)\n7050                 5. \"hazen\" (*)\n7051                 6. \"weibull\" (*)\n7052                 7. \"linear\"  (default)\n7053                 8. \"median_unbiased\" (*)\n7054                 9. \"normal_unbiased\" (*)\n7055 \n7056             The first three methods are discontiuous.  The following discontinuous\n7057             variations of the default \"linear\" (7.) option are also available:\n7058 \n7059                 * \"lower\"\n7060                 * \"higher\"\n7061                 * \"midpoint\"\n7062                 * \"nearest\"\n7063 \n7064             See :py:func:`numpy.quantile` or [1]_ for details. The \"method\" argument\n7065             was previously called \"interpolation\", renamed in accordance with numpy\n7066             version 1.22.0.\n7067 \n7068             (*) These methods require numpy version 1.22 or newer.\n7069 \n7070         keep_attrs : bool, optional\n7071             If True, the dataset's attributes (`attrs`) will be copied from\n7072             the original object to the new one.  If False (default), the new\n7073             object will be returned without attributes.\n7074         numeric_only : bool, optional\n7075             If True, only apply ``func`` to variables with a numeric dtype.\n7076         skipna : bool, optional\n7077             If True, skip missing values (as marked by NaN). By default, only\n7078             skips missing values for float dtypes; other dtypes either do not\n7079             have a sentinel missing value (int) or skipna=True has not been\n7080             implemented (object, datetime64 or timedelta64).\n7081 \n7082         Returns\n7083         -------\n7084         quantiles : Dataset\n7085             If `q` is a single quantile, then the result is a scalar for each\n7086             variable in data_vars. If multiple percentiles are given, first\n7087             axis of the result corresponds to the quantile and a quantile\n7088             dimension is added to the return Dataset. The other dimensions are\n7089             the dimensions that remain after the reduction of the array.\n7090 \n7091         See Also\n7092         --------\n7093         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile\n7094 \n7095         Examples\n7096         --------\n7097         >>> ds = xr.Dataset(\n7098         ...     {\"a\": ((\"x\", \"y\"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},\n7099         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n7100         ... )\n7101         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)\n7102         <xarray.Dataset>\n7103         Dimensions:   ()\n7104         Coordinates:\n7105             quantile  float64 0.0\n7106         Data variables:\n7107             a         float64 0.7\n7108         >>> ds.quantile(0, dim=\"x\")\n7109         <xarray.Dataset>\n7110         Dimensions:   (y: 4)\n7111         Coordinates:\n7112           * y         (y) float64 1.0 1.5 2.0 2.5\n7113             quantile  float64 0.0\n7114         Data variables:\n7115             a         (y) float64 0.7 4.2 2.6 1.5\n7116         >>> ds.quantile([0, 0.5, 1])\n7117         <xarray.Dataset>\n7118         Dimensions:   (quantile: 3)\n7119         Coordinates:\n7120           * quantile  (quantile) float64 0.0 0.5 1.0\n7121         Data variables:\n7122             a         (quantile) float64 0.7 3.4 9.4\n7123         >>> ds.quantile([0, 0.5, 1], dim=\"x\")\n7124         <xarray.Dataset>\n7125         Dimensions:   (quantile: 3, y: 4)\n7126         Coordinates:\n7127           * y         (y) float64 1.0 1.5 2.0 2.5\n7128           * quantile  (quantile) float64 0.0 0.5 1.0\n7129         Data variables:\n7130             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9\n7131 \n7132         References\n7133         ----------\n7134         .. [1] R. J. Hyndman and Y. Fan,\n7135            \"Sample quantiles in statistical packages,\"\n7136            The American Statistician, 50(4), pp. 361-365, 1996\n7137         \"\"\"\n7138 \n7139         # interpolation renamed to method in version 0.21.0\n7140         # check here and in variable to avoid repeated warnings\n7141         if interpolation is not None:\n7142             warnings.warn(\n7143                 \"The `interpolation` argument to quantile was renamed to `method`.\",\n7144                 FutureWarning,\n7145             )\n7146 \n7147             if method != \"linear\":\n7148                 raise TypeError(\"Cannot pass interpolation and method keywords!\")\n7149 \n7150             method = interpolation\n7151 \n7152         dims: set[Hashable]\n7153         if isinstance(dim, str):\n7154             dims = {dim}\n7155         elif dim is None or dim is ...:\n7156             dims = set(self.dims)\n7157         else:\n7158             dims = set(dim)\n7159 \n7160         _assert_empty(\n7161             tuple(d for d in dims if d not in self.dims),\n7162             \"Dataset does not contain the dimensions: %s\",\n7163         )\n7164 \n7165         q = np.asarray(q, dtype=np.float64)\n7166 \n7167         variables = {}\n7168         for name, var in self.variables.items():\n7169             reduce_dims = [d for d in var.dims if d in dims]\n7170             if reduce_dims or not var.dims:\n7171                 if name not in self.coords:\n7172                     if (\n7173                         not numeric_only\n7174                         or np.issubdtype(var.dtype, np.number)\n7175                         or var.dtype == np.bool_\n7176                     ):\n7177                         variables[name] = var.quantile(\n7178                             q,\n7179                             dim=reduce_dims,\n7180                             method=method,\n7181                             keep_attrs=keep_attrs,\n7182                             skipna=skipna,\n7183                         )\n7184 \n7185             else:\n7186                 variables[name] = var\n7187 \n7188         # construct the new dataset\n7189         coord_names = {k for k in self.coords if k in variables}\n7190         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n7191         if keep_attrs is None:\n7192             keep_attrs = _get_keep_attrs(default=False)\n7193         attrs = self.attrs if keep_attrs else None\n7194         new = self._replace_with_new_dims(\n7195             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n7196         )\n7197         return new.assign_coords(quantile=q)\n7198 \n7199     def rank(\n7200         self: T_Dataset,\n7201         dim: Hashable,\n7202         pct: bool = False,\n7203         keep_attrs: bool | None = None,\n7204     ) -> T_Dataset:\n7205         \"\"\"Ranks the data.\n7206 \n7207         Equal values are assigned a rank that is the average of the ranks that\n7208         would have been otherwise assigned to all of the values within\n7209         that set.\n7210         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n7211 \n7212         NaNs in the input array are returned as NaNs.\n7213 \n7214         The `bottleneck` library is required.\n7215 \n7216         Parameters\n7217         ----------\n7218         dim : Hashable\n7219             Dimension over which to compute rank.\n7220         pct : bool, default: False\n7221             If True, compute percentage ranks, otherwise compute integer ranks.\n7222         keep_attrs : bool or None, optional\n7223             If True, the dataset's attributes (`attrs`) will be copied from\n7224             the original object to the new one.  If False, the new\n7225             object will be returned without attributes.\n7226 \n7227         Returns\n7228         -------\n7229         ranked : Dataset\n7230             Variables that do not depend on `dim` are dropped.\n7231         \"\"\"\n7232         if not OPTIONS[\"use_bottleneck\"]:\n7233             raise RuntimeError(\n7234                 \"rank requires bottleneck to be enabled.\"\n7235                 \" Call `xr.set_options(use_bottleneck=True)` to enable it.\"\n7236             )\n7237 \n7238         if dim not in self.dims:\n7239             raise ValueError(f\"Dataset does not contain the dimension: {dim}\")\n7240 \n7241         variables = {}\n7242         for name, var in self.variables.items():\n7243             if name in self.data_vars:\n7244                 if dim in var.dims:\n7245                     variables[name] = var.rank(dim, pct=pct)\n7246             else:\n7247                 variables[name] = var\n7248 \n7249         coord_names = set(self.coords)\n7250         if keep_attrs is None:\n7251             keep_attrs = _get_keep_attrs(default=False)\n7252         attrs = self.attrs if keep_attrs else None\n7253         return self._replace(variables, coord_names, attrs=attrs)\n7254 \n7255     def differentiate(\n7256         self: T_Dataset,\n7257         coord: Hashable,\n7258         edge_order: Literal[1, 2] = 1,\n7259         datetime_unit: DatetimeUnitOptions | None = None,\n7260     ) -> T_Dataset:\n7261         \"\"\" Differentiate with the second order accurate central\n7262         differences.\n7263 \n7264         .. note::\n7265             This feature is limited to simple cartesian geometry, i.e. coord\n7266             must be one dimensional.\n7267 \n7268         Parameters\n7269         ----------\n7270         coord : Hashable\n7271             The coordinate to be used to compute the gradient.\n7272         edge_order : {1, 2}, default: 1\n7273             N-th order accurate differences at the boundaries.\n7274         datetime_unit : None or {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n7275             \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, default: None\n7276             Unit to compute gradient. Only valid for datetime coordinate.\n7277 \n7278         Returns\n7279         -------\n7280         differentiated: Dataset\n7281 \n7282         See also\n7283         --------\n7284         numpy.gradient: corresponding numpy function\n7285         \"\"\"\n7286         from xarray.core.variable import Variable\n7287 \n7288         if coord not in self.variables and coord not in self.dims:\n7289             raise ValueError(f\"Coordinate {coord} does not exist.\")\n7290 \n7291         coord_var = self[coord].variable\n7292         if coord_var.ndim != 1:\n7293             raise ValueError(\n7294                 \"Coordinate {} must be 1 dimensional but is {}\"\n7295                 \" dimensional\".format(coord, coord_var.ndim)\n7296             )\n7297 \n7298         dim = coord_var.dims[0]\n7299         if _contains_datetime_like_objects(coord_var):\n7300             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n7301                 datetime_unit = cast(\n7302                     \"DatetimeUnitOptions\", np.datetime_data(coord_var.dtype)[0]\n7303                 )\n7304             elif datetime_unit is None:\n7305                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n7306             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)\n7307 \n7308         variables = {}\n7309         for k, v in self.variables.items():\n7310             if k in self.data_vars and dim in v.dims and k not in self.coords:\n7311                 if _contains_datetime_like_objects(v):\n7312                     v = v._to_numeric(datetime_unit=datetime_unit)\n7313                 grad = duck_array_ops.gradient(\n7314                     v.data,\n7315                     coord_var.data,\n7316                     edge_order=edge_order,\n7317                     axis=v.get_axis_num(dim),\n7318                 )\n7319                 variables[k] = Variable(v.dims, grad)\n7320             else:\n7321                 variables[k] = v\n7322         return self._replace(variables)\n7323 \n7324     def integrate(\n7325         self: T_Dataset,\n7326         coord: Hashable | Sequence[Hashable],\n7327         datetime_unit: DatetimeUnitOptions = None,\n7328     ) -> T_Dataset:\n7329         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n7330 \n7331         .. note::\n7332             This feature is limited to simple cartesian geometry, i.e. coord\n7333             must be one dimensional.\n7334 \n7335         Parameters\n7336         ----------\n7337         coord : hashable, or sequence of hashable\n7338             Coordinate(s) used for the integration.\n7339         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n7340                         'ps', 'fs', 'as', None}, optional\n7341             Specify the unit if datetime coordinate is used.\n7342 \n7343         Returns\n7344         -------\n7345         integrated : Dataset\n7346 \n7347         See also\n7348         --------\n7349         DataArray.integrate\n7350         numpy.trapz : corresponding numpy function\n7351 \n7352         Examples\n7353         --------\n7354         >>> ds = xr.Dataset(\n7355         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n7356         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n7357         ... )\n7358         >>> ds\n7359         <xarray.Dataset>\n7360         Dimensions:  (x: 4)\n7361         Coordinates:\n7362           * x        (x) int64 0 1 2 3\n7363             y        (x) int64 1 7 3 5\n7364         Data variables:\n7365             a        (x) int64 5 5 6 6\n7366             b        (x) int64 1 2 1 0\n7367         >>> ds.integrate(\"x\")\n7368         <xarray.Dataset>\n7369         Dimensions:  ()\n7370         Data variables:\n7371             a        float64 16.5\n7372             b        float64 3.5\n7373         >>> ds.integrate(\"y\")\n7374         <xarray.Dataset>\n7375         Dimensions:  ()\n7376         Data variables:\n7377             a        float64 20.0\n7378             b        float64 4.0\n7379         \"\"\"\n7380         if not isinstance(coord, (list, tuple)):\n7381             coord = (coord,)\n7382         result = self\n7383         for c in coord:\n7384             result = result._integrate_one(c, datetime_unit=datetime_unit)\n7385         return result\n7386 \n7387     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):\n7388         from xarray.core.variable import Variable\n7389 \n7390         if coord not in self.variables and coord not in self.dims:\n7391             raise ValueError(f\"Coordinate {coord} does not exist.\")\n7392 \n7393         coord_var = self[coord].variable\n7394         if coord_var.ndim != 1:\n7395             raise ValueError(\n7396                 \"Coordinate {} must be 1 dimensional but is {}\"\n7397                 \" dimensional\".format(coord, coord_var.ndim)\n7398             )\n7399 \n7400         dim = coord_var.dims[0]\n7401         if _contains_datetime_like_objects(coord_var):\n7402             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n7403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)\n7404             elif datetime_unit is None:\n7405                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n7406             coord_var = coord_var._replace(\n7407                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)\n7408             )\n7409 \n7410         variables = {}\n7411         coord_names = set()\n7412         for k, v in self.variables.items():\n7413             if k in self.coords:\n7414                 if dim not in v.dims or cumulative:\n7415                     variables[k] = v\n7416                     coord_names.add(k)\n7417             else:\n7418                 if k in self.data_vars and dim in v.dims:\n7419                     if _contains_datetime_like_objects(v):\n7420                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)\n7421                     if cumulative:\n7422                         integ = duck_array_ops.cumulative_trapezoid(\n7423                             v.data, coord_var.data, axis=v.get_axis_num(dim)\n7424                         )\n7425                         v_dims = v.dims\n7426                     else:\n7427                         integ = duck_array_ops.trapz(\n7428                             v.data, coord_var.data, axis=v.get_axis_num(dim)\n7429                         )\n7430                         v_dims = list(v.dims)\n7431                         v_dims.remove(dim)\n7432                     variables[k] = Variable(v_dims, integ)\n7433                 else:\n7434                     variables[k] = v\n7435         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n7436         return self._replace_with_new_dims(\n7437             variables, coord_names=coord_names, indexes=indexes\n7438         )\n7439 \n7440     def cumulative_integrate(\n7441         self: T_Dataset,\n7442         coord: Hashable | Sequence[Hashable],\n7443         datetime_unit: DatetimeUnitOptions = None,\n7444     ) -> T_Dataset:\n7445         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n7446 \n7447         .. note::\n7448             This feature is limited to simple cartesian geometry, i.e. coord\n7449             must be one dimensional.\n7450 \n7451             The first entry of the cumulative integral of each variable is always 0, in\n7452             order to keep the length of the dimension unchanged between input and\n7453             output.\n7454 \n7455         Parameters\n7456         ----------\n7457         coord : hashable, or sequence of hashable\n7458             Coordinate(s) used for the integration.\n7459         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n7460                         'ps', 'fs', 'as', None}, optional\n7461             Specify the unit if datetime coordinate is used.\n7462 \n7463         Returns\n7464         -------\n7465         integrated : Dataset\n7466 \n7467         See also\n7468         --------\n7469         DataArray.cumulative_integrate\n7470         scipy.integrate.cumulative_trapezoid : corresponding scipy function\n7471 \n7472         Examples\n7473         --------\n7474         >>> ds = xr.Dataset(\n7475         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n7476         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n7477         ... )\n7478         >>> ds\n7479         <xarray.Dataset>\n7480         Dimensions:  (x: 4)\n7481         Coordinates:\n7482           * x        (x) int64 0 1 2 3\n7483             y        (x) int64 1 7 3 5\n7484         Data variables:\n7485             a        (x) int64 5 5 6 6\n7486             b        (x) int64 1 2 1 0\n7487         >>> ds.cumulative_integrate(\"x\")\n7488         <xarray.Dataset>\n7489         Dimensions:  (x: 4)\n7490         Coordinates:\n7491           * x        (x) int64 0 1 2 3\n7492             y        (x) int64 1 7 3 5\n7493         Data variables:\n7494             a        (x) float64 0.0 5.0 10.5 16.5\n7495             b        (x) float64 0.0 1.5 3.0 3.5\n7496         >>> ds.cumulative_integrate(\"y\")\n7497         <xarray.Dataset>\n7498         Dimensions:  (x: 4)\n7499         Coordinates:\n7500           * x        (x) int64 0 1 2 3\n7501             y        (x) int64 1 7 3 5\n7502         Data variables:\n7503             a        (x) float64 0.0 30.0 8.0 20.0\n7504             b        (x) float64 0.0 9.0 3.0 4.0\n7505         \"\"\"\n7506         if not isinstance(coord, (list, tuple)):\n7507             coord = (coord,)\n7508         result = self\n7509         for c in coord:\n7510             result = result._integrate_one(\n7511                 c, datetime_unit=datetime_unit, cumulative=True\n7512             )\n7513         return result\n7514 \n7515     @property\n7516     def real(self: T_Dataset) -> T_Dataset:\n7517         \"\"\"\n7518         The real part of each data variable.\n7519 \n7520         See Also\n7521         --------\n7522         numpy.ndarray.real\n7523         \"\"\"\n7524         return self.map(lambda x: x.real, keep_attrs=True)\n7525 \n7526     @property\n7527     def imag(self: T_Dataset) -> T_Dataset:\n7528         \"\"\"\n7529         The imaginary part of each data variable.\n7530 \n7531         See Also\n7532         --------\n7533         numpy.ndarray.imag\n7534         \"\"\"\n7535         return self.map(lambda x: x.imag, keep_attrs=True)\n7536 \n7537     plot = utils.UncachedAccessor(DatasetPlotAccessor)\n7538 \n7539     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:\n7540         \"\"\"Returns a ``Dataset`` with variables that match specific conditions.\n7541 \n7542         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n7543         containing only the variables for which all the filter tests pass.\n7544         These tests are either ``key=value`` for which the attribute ``key``\n7545         has the exact value ``value`` or the callable passed into\n7546         ``key=callable`` returns True. The callable will be passed a single\n7547         value, either the value of the attribute ``key`` or ``None`` if the\n7548         DataArray does not have an attribute with the name ``key``.\n7549 \n7550         Parameters\n7551         ----------\n7552         **kwargs\n7553             key : str\n7554                 Attribute name.\n7555             value : callable or obj\n7556                 If value is a callable, it should return a boolean in the form\n7557                 of bool = func(attr) where attr is da.attrs[key].\n7558                 Otherwise, value will be compared to the each\n7559                 DataArray's attrs[key].\n7560 \n7561         Returns\n7562         -------\n7563         new : Dataset\n7564             New dataset with variables filtered by attribute.\n7565 \n7566         Examples\n7567         --------\n7568         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n7569         >>> precip = 10 * np.random.rand(2, 2, 3)\n7570         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n7571         >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n7572         >>> dims = [\"x\", \"y\", \"time\"]\n7573         >>> temp_attr = dict(standard_name=\"air_potential_temperature\")\n7574         >>> precip_attr = dict(standard_name=\"convective_precipitation_flux\")\n7575 \n7576         >>> ds = xr.Dataset(\n7577         ...     dict(\n7578         ...         temperature=(dims, temp, temp_attr),\n7579         ...         precipitation=(dims, precip, precip_attr),\n7580         ...     ),\n7581         ...     coords=dict(\n7582         ...         lon=([\"x\", \"y\"], lon),\n7583         ...         lat=([\"x\", \"y\"], lat),\n7584         ...         time=pd.date_range(\"2014-09-06\", periods=3),\n7585         ...         reference_time=pd.Timestamp(\"2014-09-05\"),\n7586         ...     ),\n7587         ... )\n7588 \n7589         Get variables matching a specific standard_name:\n7590 \n7591         >>> ds.filter_by_attrs(standard_name=\"convective_precipitation_flux\")\n7592         <xarray.Dataset>\n7593         Dimensions:         (x: 2, y: 2, time: 3)\n7594         Coordinates:\n7595             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n7596             lat             (x, y) float64 42.25 42.21 42.63 42.59\n7597           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n7598             reference_time  datetime64[ns] 2014-09-05\n7599         Dimensions without coordinates: x, y\n7600         Data variables:\n7601             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n7602 \n7603         Get all variables that have a standard_name attribute:\n7604 \n7605         >>> standard_name = lambda v: v is not None\n7606         >>> ds.filter_by_attrs(standard_name=standard_name)\n7607         <xarray.Dataset>\n7608         Dimensions:         (x: 2, y: 2, time: 3)\n7609         Coordinates:\n7610             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n7611             lat             (x, y) float64 42.25 42.21 42.63 42.59\n7612           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n7613             reference_time  datetime64[ns] 2014-09-05\n7614         Dimensions without coordinates: x, y\n7615         Data variables:\n7616             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n7617             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n7618 \n7619         \"\"\"\n7620         selection = []\n7621         for var_name, variable in self.variables.items():\n7622             has_value_flag = False\n7623             for attr_name, pattern in kwargs.items():\n7624                 attr_value = variable.attrs.get(attr_name)\n7625                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:\n7626                     has_value_flag = True\n7627                 else:\n7628                     has_value_flag = False\n7629                     break\n7630             if has_value_flag is True:\n7631                 selection.append(var_name)\n7632         return self[selection]\n7633 \n7634     def unify_chunks(self: T_Dataset) -> T_Dataset:\n7635         \"\"\"Unify chunk size along all chunked dimensions of this Dataset.\n7636 \n7637         Returns\n7638         -------\n7639         Dataset with consistent chunk sizes for all dask-array variables\n7640 \n7641         See Also\n7642         --------\n7643         dask.array.core.unify_chunks\n7644         \"\"\"\n7645 \n7646         return unify_chunks(self)[0]\n7647 \n7648     def map_blocks(\n7649         self,\n7650         func: Callable[..., T_Xarray],\n7651         args: Sequence[Any] = (),\n7652         kwargs: Mapping[str, Any] | None = None,\n7653         template: DataArray | Dataset | None = None,\n7654     ) -> T_Xarray:\n7655         \"\"\"\n7656         Apply a function to each block of this Dataset.\n7657 \n7658         .. warning::\n7659             This method is experimental and its signature may change.\n7660 \n7661         Parameters\n7662         ----------\n7663         func : callable\n7664             User-provided function that accepts a Dataset as its first\n7665             parameter. The function will receive a subset or 'block' of this Dataset (see below),\n7666             corresponding to one chunk along each chunked dimension. ``func`` will be\n7667             executed as ``func(subset_dataset, *subset_args, **kwargs)``.\n7668 \n7669             This function must return either a single DataArray or a single Dataset.\n7670 \n7671             This function cannot add a new chunked dimension.\n7672         args : sequence\n7673             Passed to func after unpacking and subsetting any xarray objects by blocks.\n7674             xarray objects in args must be aligned with obj, otherwise an error is raised.\n7675         kwargs : Mapping or None\n7676             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n7677             subset to blocks. Passing dask collections in kwargs is not allowed.\n7678         template : DataArray, Dataset or None, optional\n7679             xarray object representing the final result after compute is called. If not provided,\n7680             the function will be first run on mocked-up data, that looks like this object but\n7681             has sizes 0, to determine properties of the returned object such as dtype,\n7682             variable names, attributes, new dimensions and new indexes (if any).\n7683             ``template`` must be provided if the function changes the size of existing dimensions.\n7684             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n7685             ``attrs`` set by ``func`` will be ignored.\n7686 \n7687         Returns\n7688         -------\n7689         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n7690         function.\n7691 \n7692         Notes\n7693         -----\n7694         This function is designed for when ``func`` needs to manipulate a whole xarray object\n7695         subset to each block. Each block is loaded into memory. In the more common case where\n7696         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n7697 \n7698         If none of the variables in this object is backed by dask arrays, calling this function is\n7699         equivalent to calling ``func(obj, *args, **kwargs)``.\n7700 \n7701         See Also\n7702         --------\n7703         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n7704         xarray.DataArray.map_blocks\n7705 \n7706         Examples\n7707         --------\n7708         Calculate an anomaly from climatology using ``.groupby()``. Using\n7709         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n7710         its indices, and its methods like ``.groupby()``.\n7711 \n7712         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n7713         ...     gb = da.groupby(groupby_type)\n7714         ...     clim = gb.mean(dim=\"time\")\n7715         ...     return gb - clim\n7716         ...\n7717         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n7718         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n7719         >>> np.random.seed(123)\n7720         >>> array = xr.DataArray(\n7721         ...     np.random.rand(len(time)),\n7722         ...     dims=[\"time\"],\n7723         ...     coords={\"time\": time, \"month\": month},\n7724         ... ).chunk()\n7725         >>> ds = xr.Dataset({\"a\": array})\n7726         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()\n7727         <xarray.Dataset>\n7728         Dimensions:  (time: 24)\n7729         Coordinates:\n7730           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n7731             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n7732         Data variables:\n7733             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901\n7734 \n7735         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n7736         to the function being applied in ``xr.map_blocks()``:\n7737 \n7738         >>> ds.map_blocks(\n7739         ...     calculate_anomaly,\n7740         ...     kwargs={\"groupby_type\": \"time.year\"},\n7741         ...     template=ds,\n7742         ... )\n7743         <xarray.Dataset>\n7744         Dimensions:  (time: 24)\n7745         Coordinates:\n7746           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n7747             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n7748         Data variables:\n7749             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>\n7750         \"\"\"\n7751         from xarray.core.parallel import map_blocks\n7752 \n7753         return map_blocks(func, self, args, kwargs, template)\n7754 \n7755     def polyfit(\n7756         self: T_Dataset,\n7757         dim: Hashable,\n7758         deg: int,\n7759         skipna: bool | None = None,\n7760         rcond: float | None = None,\n7761         w: Hashable | Any = None,\n7762         full: bool = False,\n7763         cov: bool | Literal[\"unscaled\"] = False,\n7764     ) -> T_Dataset:\n7765         \"\"\"\n7766         Least squares polynomial fit.\n7767 \n7768         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n7769         invalid values when `skipna = True`.\n7770 \n7771         Parameters\n7772         ----------\n7773         dim : hashable\n7774             Coordinate along which to fit the polynomials.\n7775         deg : int\n7776             Degree of the fitting polynomial.\n7777         skipna : bool or None, optional\n7778             If True, removes all invalid values before fitting each 1D slices of the array.\n7779             Default is True if data is stored in a dask.array or if there is any\n7780             invalid values, False otherwise.\n7781         rcond : float or None, optional\n7782             Relative condition number to the fit.\n7783         w : hashable or Any, optional\n7784             Weights to apply to the y-coordinate of the sample points.\n7785             Can be an array-like object or the name of a coordinate in the dataset.\n7786         full : bool, default: False\n7787             Whether to return the residuals, matrix rank and singular values in addition\n7788             to the coefficients.\n7789         cov : bool or \"unscaled\", default: False\n7790             Whether to return to the covariance matrix in addition to the coefficients.\n7791             The matrix is not scaled if `cov='unscaled'`.\n7792 \n7793         Returns\n7794         -------\n7795         polyfit_results : Dataset\n7796             A single dataset which contains (for each \"var\" in the input dataset):\n7797 \n7798             [var]_polyfit_coefficients\n7799                 The coefficients of the best fit for each variable in this dataset.\n7800             [var]_polyfit_residuals\n7801                 The residuals of the least-square computation for each variable (only included if `full=True`)\n7802                 When the matrix rank is deficient, np.nan is returned.\n7803             [dim]_matrix_rank\n7804                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n7805                 The rank is computed ignoring the NaN values that might be skipped.\n7806             [dim]_singular_values\n7807                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n7808             [var]_polyfit_covariance\n7809                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n7810 \n7811         Warns\n7812         -----\n7813         RankWarning\n7814             The rank of the coefficient matrix in the least-squares fit is deficient.\n7815             The warning is not raised with in-memory (not dask) data and `full=True`.\n7816 \n7817         See Also\n7818         --------\n7819         numpy.polyfit\n7820         numpy.polyval\n7821         xarray.polyval\n7822         \"\"\"\n7823         from xarray.core.dataarray import DataArray\n7824 \n7825         variables = {}\n7826         skipna_da = skipna\n7827 \n7828         x = get_clean_interp_index(self, dim, strict=False)\n7829         xname = f\"{self[dim].name}_\"\n7830         order = int(deg) + 1\n7831         lhs = np.vander(x, order)\n7832 \n7833         if rcond is None:\n7834             rcond = (\n7835                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]\n7836             )\n7837 \n7838         # Weights:\n7839         if w is not None:\n7840             if isinstance(w, Hashable):\n7841                 w = self.coords[w]\n7842             w = np.asarray(w)\n7843             if w.ndim != 1:\n7844                 raise TypeError(\"Expected a 1-d array for weights.\")\n7845             if w.shape[0] != lhs.shape[0]:\n7846                 raise TypeError(f\"Expected w and {dim} to have the same length\")\n7847             lhs *= w[:, np.newaxis]\n7848 \n7849         # Scaling\n7850         scale = np.sqrt((lhs * lhs).sum(axis=0))\n7851         lhs /= scale\n7852 \n7853         degree_dim = utils.get_temp_dimname(self.dims, \"degree\")\n7854 \n7855         rank = np.linalg.matrix_rank(lhs)\n7856 \n7857         if full:\n7858             rank = DataArray(rank, name=xname + \"matrix_rank\")\n7859             variables[rank.name] = rank\n7860             _sing = np.linalg.svd(lhs, compute_uv=False)\n7861             sing = DataArray(\n7862                 _sing,\n7863                 dims=(degree_dim,),\n7864                 coords={degree_dim: np.arange(rank - 1, -1, -1)},\n7865                 name=xname + \"singular_values\",\n7866             )\n7867             variables[sing.name] = sing\n7868 \n7869         for name, da in self.data_vars.items():\n7870             if dim not in da.dims:\n7871                 continue\n7872 \n7873             if is_duck_dask_array(da.data) and (\n7874                 rank != order or full or skipna is None\n7875             ):\n7876                 # Current algorithm with dask and skipna=False neither supports\n7877                 # deficient ranks nor does it output the \"full\" info (issue dask/dask#6516)\n7878                 skipna_da = True\n7879             elif skipna is None:\n7880                 skipna_da = bool(np.any(da.isnull()))\n7881 \n7882             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]\n7883             stacked_coords: dict[Hashable, DataArray] = {}\n7884             if dims_to_stack:\n7885                 stacked_dim = utils.get_temp_dimname(dims_to_stack, \"stacked\")\n7886                 rhs = da.transpose(dim, *dims_to_stack).stack(\n7887                     {stacked_dim: dims_to_stack}\n7888                 )\n7889                 stacked_coords = {stacked_dim: rhs[stacked_dim]}\n7890                 scale_da = scale[:, np.newaxis]\n7891             else:\n7892                 rhs = da\n7893                 scale_da = scale\n7894 \n7895             if w is not None:\n7896                 rhs *= w[:, np.newaxis]\n7897 \n7898             with warnings.catch_warnings():\n7899                 if full:  # Copy np.polyfit behavior\n7900                     warnings.simplefilter(\"ignore\", np.RankWarning)\n7901                 else:  # Raise only once per variable\n7902                     warnings.simplefilter(\"once\", np.RankWarning)\n7903 \n7904                 coeffs, residuals = duck_array_ops.least_squares(\n7905                     lhs, rhs.data, rcond=rcond, skipna=skipna_da\n7906                 )\n7907 \n7908             if isinstance(name, str):\n7909                 name = f\"{name}_\"\n7910             else:\n7911                 # Thus a ReprObject => polyfit was called on a DataArray\n7912                 name = \"\"\n7913 \n7914             coeffs = DataArray(\n7915                 coeffs / scale_da,\n7916                 dims=[degree_dim] + list(stacked_coords.keys()),\n7917                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},\n7918                 name=name + \"polyfit_coefficients\",\n7919             )\n7920             if dims_to_stack:\n7921                 coeffs = coeffs.unstack(stacked_dim)\n7922             variables[coeffs.name] = coeffs\n7923 \n7924             if full or (cov is True):\n7925                 residuals = DataArray(\n7926                     residuals if dims_to_stack else residuals.squeeze(),\n7927                     dims=list(stacked_coords.keys()),\n7928                     coords=stacked_coords,\n7929                     name=name + \"polyfit_residuals\",\n7930                 )\n7931                 if dims_to_stack:\n7932                     residuals = residuals.unstack(stacked_dim)\n7933                 variables[residuals.name] = residuals\n7934 \n7935             if cov:\n7936                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))\n7937                 Vbase /= np.outer(scale, scale)\n7938                 if cov == \"unscaled\":\n7939                     fac = 1\n7940                 else:\n7941                     if x.shape[0] <= order:\n7942                         raise ValueError(\n7943                             \"The number of data points must exceed order to scale the covariance matrix.\"\n7944                         )\n7945                     fac = residuals / (x.shape[0] - order)\n7946                 covariance = DataArray(Vbase, dims=(\"cov_i\", \"cov_j\")) * fac\n7947                 variables[name + \"polyfit_covariance\"] = covariance\n7948 \n7949         return type(self)(data_vars=variables, attrs=self.attrs.copy())\n7950 \n7951     def pad(\n7952         self: T_Dataset,\n7953         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,\n7954         mode: PadModeOptions = \"constant\",\n7955         stat_length: int\n7956         | tuple[int, int]\n7957         | Mapping[Any, tuple[int, int]]\n7958         | None = None,\n7959         constant_values: (\n7960             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None\n7961         ) = None,\n7962         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,\n7963         reflect_type: PadReflectOptions = None,\n7964         keep_attrs: bool | None = None,\n7965         **pad_width_kwargs: Any,\n7966     ) -> T_Dataset:\n7967         \"\"\"Pad this dataset along one or more dimensions.\n7968 \n7969         .. warning::\n7970             This function is experimental and its behaviour is likely to change\n7971             especially regarding padding of dimension coordinates (or IndexVariables).\n7972 \n7973         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n7974         coordinates will be padded with the same mode, otherwise coordinates\n7975         are padded using the \"constant\" mode with fill_value dtypes.NA.\n7976 \n7977         Parameters\n7978         ----------\n7979         pad_width : mapping of hashable to tuple of int\n7980             Mapping with the form of {dim: (pad_before, pad_after)}\n7981             describing the number of values padded along each dimension.\n7982             {dim: pad} is a shortcut for pad_before = pad_after = pad\n7983         mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \\\n7984             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n7985             How to pad the DataArray (taken from numpy docs):\n7986 \n7987             - \"constant\": Pads with a constant value.\n7988             - \"edge\": Pads with the edge values of array.\n7989             - \"linear_ramp\": Pads with the linear ramp between end_value and the\n7990               array edge value.\n7991             - \"maximum\": Pads with the maximum value of all or part of the\n7992               vector along each axis.\n7993             - \"mean\": Pads with the mean value of all or part of the\n7994               vector along each axis.\n7995             - \"median\": Pads with the median value of all or part of the\n7996               vector along each axis.\n7997             - \"minimum\": Pads with the minimum value of all or part of the\n7998               vector along each axis.\n7999             - \"reflect\": Pads with the reflection of the vector mirrored on\n8000               the first and last values of the vector along each axis.\n8001             - \"symmetric\": Pads with the reflection of the vector mirrored\n8002               along the edge of the array.\n8003             - \"wrap\": Pads with the wrap of the vector along the axis.\n8004               The first values are used to pad the end and the\n8005               end values are used to pad the beginning.\n8006 \n8007         stat_length : int, tuple or mapping of hashable to tuple, default: None\n8008             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n8009             values at edge of each axis used to calculate the statistic value.\n8010             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n8011             statistic lengths along each dimension.\n8012             ((before, after),) yields same before and after statistic lengths\n8013             for each dimension.\n8014             (stat_length,) or int is a shortcut for before = after = statistic\n8015             length for all axes.\n8016             Default is ``None``, to use the entire axis.\n8017         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0\n8018             Used in 'constant'.  The values to set the padded values for each\n8019             axis.\n8020             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n8021             pad constants along each dimension.\n8022             ``((before, after),)`` yields same before and after constants for each\n8023             dimension.\n8024             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n8025             all dimensions.\n8026             Default is 0.\n8027         end_values : scalar, tuple or mapping of hashable to tuple, default: 0\n8028             Used in 'linear_ramp'.  The values used for the ending value of the\n8029             linear_ramp and that will form the edge of the padded array.\n8030             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n8031             end values along each dimension.\n8032             ``((before, after),)`` yields same before and after end values for each\n8033             axis.\n8034             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n8035             all axes.\n8036             Default is 0.\n8037         reflect_type : {\"even\", \"odd\", None}, optional\n8038             Used in \"reflect\", and \"symmetric\".  The \"even\" style is the\n8039             default with an unaltered reflection around the edge value.  For\n8040             the \"odd\" style, the extended part of the array is created by\n8041             subtracting the reflected values from two times the edge value.\n8042         keep_attrs : bool or None, optional\n8043             If True, the attributes (``attrs``) will be copied from the\n8044             original object to the new one. If False, the new object\n8045             will be returned without attributes.\n8046         **pad_width_kwargs\n8047             The keyword arguments form of ``pad_width``.\n8048             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n8049 \n8050         Returns\n8051         -------\n8052         padded : Dataset\n8053             Dataset with the padded coordinates and data.\n8054 \n8055         See Also\n8056         --------\n8057         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad\n8058 \n8059         Notes\n8060         -----\n8061         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n8062         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n8063         specify ``constant_values=np.nan``\n8064 \n8065         Padding coordinates will drop their corresponding index (if any) and will reset default\n8066         indexes for dimension coordinates.\n8067 \n8068         Examples\n8069         --------\n8070         >>> ds = xr.Dataset({\"foo\": (\"x\", range(5))})\n8071         >>> ds.pad(x=(1, 2))\n8072         <xarray.Dataset>\n8073         Dimensions:  (x: 8)\n8074         Dimensions without coordinates: x\n8075         Data variables:\n8076             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan\n8077         \"\"\"\n8078         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n8079 \n8080         if mode in (\"edge\", \"reflect\", \"symmetric\", \"wrap\"):\n8081             coord_pad_mode = mode\n8082             coord_pad_options = {\n8083                 \"stat_length\": stat_length,\n8084                 \"constant_values\": constant_values,\n8085                 \"end_values\": end_values,\n8086                 \"reflect_type\": reflect_type,\n8087             }\n8088         else:\n8089             coord_pad_mode = \"constant\"\n8090             coord_pad_options = {}\n8091 \n8092         if keep_attrs is None:\n8093             keep_attrs = _get_keep_attrs(default=True)\n8094 \n8095         variables = {}\n8096 \n8097         # keep indexes that won't be affected by pad and drop all other indexes\n8098         xindexes = self.xindexes\n8099         pad_dims = set(pad_width)\n8100         indexes = {}\n8101         for k, idx in xindexes.items():\n8102             if not pad_dims.intersection(xindexes.get_all_dims(k)):\n8103                 indexes[k] = idx\n8104 \n8105         for name, var in self.variables.items():\n8106             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}\n8107             if not var_pad_width:\n8108                 variables[name] = var\n8109             elif name in self.data_vars:\n8110                 variables[name] = var.pad(\n8111                     pad_width=var_pad_width,\n8112                     mode=mode,\n8113                     stat_length=stat_length,\n8114                     constant_values=constant_values,\n8115                     end_values=end_values,\n8116                     reflect_type=reflect_type,\n8117                     keep_attrs=keep_attrs,\n8118                 )\n8119             else:\n8120                 variables[name] = var.pad(\n8121                     pad_width=var_pad_width,\n8122                     mode=coord_pad_mode,\n8123                     keep_attrs=keep_attrs,\n8124                     **coord_pad_options,  # type: ignore[arg-type]\n8125                 )\n8126                 # reset default index of dimension coordinates\n8127                 if (name,) == var.dims:\n8128                     dim_var = {name: variables[name]}\n8129                     index = PandasIndex.from_variables(dim_var, options={})\n8130                     index_vars = index.create_variables(dim_var)\n8131                     indexes[name] = index\n8132                     variables[name] = index_vars[name]\n8133 \n8134         attrs = self._attrs if keep_attrs else None\n8135         return self._replace_with_new_dims(variables, indexes=indexes, attrs=attrs)\n8136 \n8137     def idxmin(\n8138         self: T_Dataset,\n8139         dim: Hashable | None = None,\n8140         skipna: bool | None = None,\n8141         fill_value: Any = xrdtypes.NA,\n8142         keep_attrs: bool | None = None,\n8143     ) -> T_Dataset:\n8144         \"\"\"Return the coordinate label of the minimum value along a dimension.\n8145 \n8146         Returns a new `Dataset` named after the dimension with the values of\n8147         the coordinate labels along that dimension corresponding to minimum\n8148         values along that dimension.\n8149 \n8150         In comparison to :py:meth:`~Dataset.argmin`, this returns the\n8151         coordinate label while :py:meth:`~Dataset.argmin` returns the index.\n8152 \n8153         Parameters\n8154         ----------\n8155         dim : Hashable, optional\n8156             Dimension over which to apply `idxmin`.  This is optional for 1D\n8157             variables, but required for variables with 2 or more dimensions.\n8158         skipna : bool or None, optional\n8159             If True, skip missing values (as marked by NaN). By default, only\n8160             skips missing values for ``float``, ``complex``, and ``object``\n8161             dtypes; other dtypes either do not have a sentinel missing value\n8162             (``int``) or ``skipna=True`` has not been implemented\n8163             (``datetime64`` or ``timedelta64``).\n8164         fill_value : Any, default: NaN\n8165             Value to be filled in case all of the values along a dimension are\n8166             null.  By default this is NaN.  The fill value and result are\n8167             automatically converted to a compatible dtype if possible.\n8168             Ignored if ``skipna`` is False.\n8169         keep_attrs : bool or None, optional\n8170             If True, the attributes (``attrs``) will be copied from the\n8171             original object to the new one. If False, the new object\n8172             will be returned without attributes.\n8173 \n8174         Returns\n8175         -------\n8176         reduced : Dataset\n8177             New `Dataset` object with `idxmin` applied to its data and the\n8178             indicated dimension removed.\n8179 \n8180         See Also\n8181         --------\n8182         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin\n8183 \n8184         Examples\n8185         --------\n8186         >>> array1 = xr.DataArray(\n8187         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n8188         ... )\n8189         >>> array2 = xr.DataArray(\n8190         ...     [\n8191         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n8192         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n8193         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n8194         ...     ],\n8195         ...     dims=[\"y\", \"x\"],\n8196         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n8197         ... )\n8198         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n8199         >>> ds.min(dim=\"x\")\n8200         <xarray.Dataset>\n8201         Dimensions:  (y: 3)\n8202         Coordinates:\n8203           * y        (y) int64 -1 0 1\n8204         Data variables:\n8205             int      int64 -2\n8206             float    (y) float64 -2.0 -4.0 1.0\n8207         >>> ds.argmin(dim=\"x\")\n8208         <xarray.Dataset>\n8209         Dimensions:  (y: 3)\n8210         Coordinates:\n8211           * y        (y) int64 -1 0 1\n8212         Data variables:\n8213             int      int64 4\n8214             float    (y) int64 4 0 2\n8215         >>> ds.idxmin(dim=\"x\")\n8216         <xarray.Dataset>\n8217         Dimensions:  (y: 3)\n8218         Coordinates:\n8219           * y        (y) int64 -1 0 1\n8220         Data variables:\n8221             int      <U1 'e'\n8222             float    (y) object 'e' 'a' 'c'\n8223         \"\"\"\n8224         return self.map(\n8225             methodcaller(\n8226                 \"idxmin\",\n8227                 dim=dim,\n8228                 skipna=skipna,\n8229                 fill_value=fill_value,\n8230                 keep_attrs=keep_attrs,\n8231             )\n8232         )\n8233 \n8234     def idxmax(\n8235         self: T_Dataset,\n8236         dim: Hashable | None = None,\n8237         skipna: bool | None = None,\n8238         fill_value: Any = xrdtypes.NA,\n8239         keep_attrs: bool | None = None,\n8240     ) -> T_Dataset:\n8241         \"\"\"Return the coordinate label of the maximum value along a dimension.\n8242 \n8243         Returns a new `Dataset` named after the dimension with the values of\n8244         the coordinate labels along that dimension corresponding to maximum\n8245         values along that dimension.\n8246 \n8247         In comparison to :py:meth:`~Dataset.argmax`, this returns the\n8248         coordinate label while :py:meth:`~Dataset.argmax` returns the index.\n8249 \n8250         Parameters\n8251         ----------\n8252         dim : str, optional\n8253             Dimension over which to apply `idxmax`.  This is optional for 1D\n8254             variables, but required for variables with 2 or more dimensions.\n8255         skipna : bool or None, optional\n8256             If True, skip missing values (as marked by NaN). By default, only\n8257             skips missing values for ``float``, ``complex``, and ``object``\n8258             dtypes; other dtypes either do not have a sentinel missing value\n8259             (``int``) or ``skipna=True`` has not been implemented\n8260             (``datetime64`` or ``timedelta64``).\n8261         fill_value : Any, default: NaN\n8262             Value to be filled in case all of the values along a dimension are\n8263             null.  By default this is NaN.  The fill value and result are\n8264             automatically converted to a compatible dtype if possible.\n8265             Ignored if ``skipna`` is False.\n8266         keep_attrs : bool or None, optional\n8267             If True, the attributes (``attrs``) will be copied from the\n8268             original object to the new one. If False, the new object\n8269             will be returned without attributes.\n8270 \n8271         Returns\n8272         -------\n8273         reduced : Dataset\n8274             New `Dataset` object with `idxmax` applied to its data and the\n8275             indicated dimension removed.\n8276 \n8277         See Also\n8278         --------\n8279         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax\n8280 \n8281         Examples\n8282         --------\n8283         >>> array1 = xr.DataArray(\n8284         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n8285         ... )\n8286         >>> array2 = xr.DataArray(\n8287         ...     [\n8288         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n8289         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n8290         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n8291         ...     ],\n8292         ...     dims=[\"y\", \"x\"],\n8293         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n8294         ... )\n8295         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n8296         >>> ds.max(dim=\"x\")\n8297         <xarray.Dataset>\n8298         Dimensions:  (y: 3)\n8299         Coordinates:\n8300           * y        (y) int64 -1 0 1\n8301         Data variables:\n8302             int      int64 2\n8303             float    (y) float64 2.0 2.0 1.0\n8304         >>> ds.argmax(dim=\"x\")\n8305         <xarray.Dataset>\n8306         Dimensions:  (y: 3)\n8307         Coordinates:\n8308           * y        (y) int64 -1 0 1\n8309         Data variables:\n8310             int      int64 1\n8311             float    (y) int64 0 2 2\n8312         >>> ds.idxmax(dim=\"x\")\n8313         <xarray.Dataset>\n8314         Dimensions:  (y: 3)\n8315         Coordinates:\n8316           * y        (y) int64 -1 0 1\n8317         Data variables:\n8318             int      <U1 'b'\n8319             float    (y) object 'a' 'c' 'c'\n8320         \"\"\"\n8321         return self.map(\n8322             methodcaller(\n8323                 \"idxmax\",\n8324                 dim=dim,\n8325                 skipna=skipna,\n8326                 fill_value=fill_value,\n8327                 keep_attrs=keep_attrs,\n8328             )\n8329         )\n8330 \n8331     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:\n8332         \"\"\"Indices of the minima of the member variables.\n8333 \n8334         If there are multiple minima, the indices of the first one found will be\n8335         returned.\n8336 \n8337         Parameters\n8338         ----------\n8339         dim : Hashable, optional\n8340             The dimension over which to find the minimum. By default, finds minimum over\n8341             all dimensions - for now returning an int for backward compatibility, but\n8342             this is deprecated, in future will be an error, since DataArray.argmin will\n8343             return a dict with indices for all dimensions, which does not make sense for\n8344             a Dataset.\n8345         keep_attrs : bool, optional\n8346             If True, the attributes (`attrs`) will be copied from the original\n8347             object to the new one.  If False (default), the new object will be\n8348             returned without attributes.\n8349         skipna : bool, optional\n8350             If True, skip missing values (as marked by NaN). By default, only\n8351             skips missing values for float dtypes; other dtypes either do not\n8352             have a sentinel missing value (int) or skipna=True has not been\n8353             implemented (object, datetime64 or timedelta64).\n8354 \n8355         Returns\n8356         -------\n8357         result : Dataset\n8358 \n8359         See Also\n8360         --------\n8361         DataArray.argmin\n8362         \"\"\"\n8363         if dim is None:\n8364             warnings.warn(\n8365                 \"Once the behaviour of DataArray.argmin() and Variable.argmin() without \"\n8366                 \"dim changes to return a dict of indices of each dimension, for \"\n8367                 \"consistency it will be an error to call Dataset.argmin() with no argument,\"\n8368                 \"since we don't return a dict of Datasets.\",\n8369                 DeprecationWarning,\n8370                 stacklevel=2,\n8371             )\n8372         if (\n8373             dim is None\n8374             or (not isinstance(dim, Sequence) and dim is not ...)\n8375             or isinstance(dim, str)\n8376         ):\n8377             # Return int index if single dimension is passed, and is not part of a\n8378             # sequence\n8379             argmin_func = getattr(duck_array_ops, \"argmin\")\n8380             return self.reduce(\n8381                 argmin_func, dim=None if dim is None else [dim], **kwargs\n8382             )\n8383         else:\n8384             raise ValueError(\n8385                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n8386                 \"dicts cannot be contained in a Dataset, so cannot call \"\n8387                 \"Dataset.argmin() with a sequence or ... for dim\"\n8388             )\n8389 \n8390     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:\n8391         \"\"\"Indices of the maxima of the member variables.\n8392 \n8393         If there are multiple maxima, the indices of the first one found will be\n8394         returned.\n8395 \n8396         Parameters\n8397         ----------\n8398         dim : str, optional\n8399             The dimension over which to find the maximum. By default, finds maximum over\n8400             all dimensions - for now returning an int for backward compatibility, but\n8401             this is deprecated, in future will be an error, since DataArray.argmax will\n8402             return a dict with indices for all dimensions, which does not make sense for\n8403             a Dataset.\n8404         keep_attrs : bool, optional\n8405             If True, the attributes (`attrs`) will be copied from the original\n8406             object to the new one.  If False (default), the new object will be\n8407             returned without attributes.\n8408         skipna : bool, optional\n8409             If True, skip missing values (as marked by NaN). By default, only\n8410             skips missing values for float dtypes; other dtypes either do not\n8411             have a sentinel missing value (int) or skipna=True has not been\n8412             implemented (object, datetime64 or timedelta64).\n8413 \n8414         Returns\n8415         -------\n8416         result : Dataset\n8417 \n8418         See Also\n8419         --------\n8420         DataArray.argmax\n8421 \n8422         \"\"\"\n8423         if dim is None:\n8424             warnings.warn(\n8425                 \"Once the behaviour of DataArray.argmin() and Variable.argmin() without \"\n8426                 \"dim changes to return a dict of indices of each dimension, for \"\n8427                 \"consistency it will be an error to call Dataset.argmin() with no argument,\"\n8428                 \"since we don't return a dict of Datasets.\",\n8429                 DeprecationWarning,\n8430                 stacklevel=2,\n8431             )\n8432         if (\n8433             dim is None\n8434             or (not isinstance(dim, Sequence) and dim is not ...)\n8435             or isinstance(dim, str)\n8436         ):\n8437             # Return int index if single dimension is passed, and is not part of a\n8438             # sequence\n8439             argmax_func = getattr(duck_array_ops, \"argmax\")\n8440             return self.reduce(\n8441                 argmax_func, dim=None if dim is None else [dim], **kwargs\n8442             )\n8443         else:\n8444             raise ValueError(\n8445                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n8446                 \"dicts cannot be contained in a Dataset, so cannot call \"\n8447                 \"Dataset.argmin() with a sequence or ... for dim\"\n8448             )\n8449 \n8450     def query(\n8451         self: T_Dataset,\n8452         queries: Mapping[Any, Any] | None = None,\n8453         parser: QueryParserOptions = \"pandas\",\n8454         engine: QueryEngineOptions = None,\n8455         missing_dims: ErrorOptionsWithWarn = \"raise\",\n8456         **queries_kwargs: Any,\n8457     ) -> T_Dataset:\n8458         \"\"\"Return a new dataset with each array indexed along the specified\n8459         dimension(s), where the indexers are given as strings containing\n8460         Python expressions to be evaluated against the data variables in the\n8461         dataset.\n8462 \n8463         Parameters\n8464         ----------\n8465         queries : dict-like, optional\n8466             A dict-like with keys matching dimensions and values given by strings\n8467             containing Python expressions to be evaluated against the data variables\n8468             in the dataset. The expressions will be evaluated using the pandas\n8469             eval() function, and can contain any valid Python expressions but cannot\n8470             contain any Python statements.\n8471         parser : {\"pandas\", \"python\"}, default: \"pandas\"\n8472             The parser to use to construct the syntax tree from the expression.\n8473             The default of 'pandas' parses code slightly different than standard\n8474             Python. Alternatively, you can parse an expression using the 'python'\n8475             parser to retain strict Python semantics.\n8476         engine : {\"python\", \"numexpr\", None}, default: None\n8477             The engine used to evaluate the expression. Supported engines are:\n8478 \n8479             - None: tries to use numexpr, falls back to python\n8480             - \"numexpr\": evaluates expressions using numexpr\n8481             - \"python\": performs operations as if you had eval\u2019d in top level python\n8482 \n8483         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n8484             What to do if dimensions that should be selected from are not present in the\n8485             Dataset:\n8486 \n8487             - \"raise\": raise an exception\n8488             - \"warn\": raise a warning, and ignore the missing dimensions\n8489             - \"ignore\": ignore the missing dimensions\n8490 \n8491         **queries_kwargs : {dim: query, ...}, optional\n8492             The keyword arguments form of ``queries``.\n8493             One of queries or queries_kwargs must be provided.\n8494 \n8495         Returns\n8496         -------\n8497         obj : Dataset\n8498             A new Dataset with the same contents as this dataset, except each\n8499             array and dimension is indexed by the results of the appropriate\n8500             queries.\n8501 \n8502         See Also\n8503         --------\n8504         Dataset.isel\n8505         pandas.eval\n8506 \n8507         Examples\n8508         --------\n8509         >>> a = np.arange(0, 5, 1)\n8510         >>> b = np.linspace(0, 1, 5)\n8511         >>> ds = xr.Dataset({\"a\": (\"x\", a), \"b\": (\"x\", b)})\n8512         >>> ds\n8513         <xarray.Dataset>\n8514         Dimensions:  (x: 5)\n8515         Dimensions without coordinates: x\n8516         Data variables:\n8517             a        (x) int64 0 1 2 3 4\n8518             b        (x) float64 0.0 0.25 0.5 0.75 1.0\n8519         >>> ds.query(x=\"a > 2\")\n8520         <xarray.Dataset>\n8521         Dimensions:  (x: 2)\n8522         Dimensions without coordinates: x\n8523         Data variables:\n8524             a        (x) int64 3 4\n8525             b        (x) float64 0.75 1.0\n8526         \"\"\"\n8527 \n8528         # allow queries to be given either as a dict or as kwargs\n8529         queries = either_dict_or_kwargs(queries, queries_kwargs, \"query\")\n8530 \n8531         # check queries\n8532         for dim, expr in queries.items():\n8533             if not isinstance(expr, str):\n8534                 msg = f\"expr for dim {dim} must be a string to be evaluated, {type(expr)} given\"\n8535                 raise ValueError(msg)\n8536 \n8537         # evaluate the queries to create the indexers\n8538         indexers = {\n8539             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)\n8540             for dim, expr in queries.items()\n8541         }\n8542 \n8543         # apply the selection\n8544         return self.isel(indexers, missing_dims=missing_dims)\n8545 \n8546     def curvefit(\n8547         self: T_Dataset,\n8548         coords: str | DataArray | Iterable[str | DataArray],\n8549         func: Callable[..., Any],\n8550         reduce_dims: Dims = None,\n8551         skipna: bool = True,\n8552         p0: dict[str, Any] | None = None,\n8553         bounds: dict[str, Any] | None = None,\n8554         param_names: Sequence[str] | None = None,\n8555         kwargs: dict[str, Any] | None = None,\n8556     ) -> T_Dataset:\n8557         \"\"\"\n8558         Curve fitting optimization for arbitrary functions.\n8559 \n8560         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n8561 \n8562         Parameters\n8563         ----------\n8564         coords : hashable, DataArray, or sequence of hashable or DataArray\n8565             Independent coordinate(s) over which to perform the curve fitting. Must share\n8566             at least one dimension with the calling object. When fitting multi-dimensional\n8567             functions, supply `coords` as a sequence in the same order as arguments in\n8568             `func`. To fit along existing dimensions of the calling object, `coords` can\n8569             also be specified as a str or sequence of strs.\n8570         func : callable\n8571             User specified function in the form `f(x, *params)` which returns a numpy\n8572             array of length `len(x)`. `params` are the fittable parameters which are optimized\n8573             by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n8574             coordinates, e.g. `f((x0, x1), *params)`.\n8575         reduce_dims : str, Iterable of Hashable or None, optional\n8576             Additional dimension(s) over which to aggregate while fitting. For example,\n8577             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n8578             aggregate all lat and lon points and fit the specified function along the\n8579             time dimension.\n8580         skipna : bool, default: True\n8581             Whether to skip missing values when fitting. Default is True.\n8582         p0 : dict-like, optional\n8583             Optional dictionary of parameter names to initial guesses passed to the\n8584             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n8585             be assigned initial values following the default scipy behavior.\n8586         bounds : dict-like, optional\n8587             Optional dictionary of parameter names to bounding values passed to the\n8588             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n8589             will be unbounded following the default scipy behavior.\n8590         param_names : sequence of hashable, optional\n8591             Sequence of names for the fittable parameters of `func`. If not supplied,\n8592             this will be automatically determined by arguments of `func`. `param_names`\n8593             should be manually supplied when fitting a function that takes a variable\n8594             number of parameters.\n8595         **kwargs : optional\n8596             Additional keyword arguments to passed to scipy curve_fit.\n8597 \n8598         Returns\n8599         -------\n8600         curvefit_results : Dataset\n8601             A single dataset which contains:\n8602 \n8603             [var]_curvefit_coefficients\n8604                 The coefficients of the best fit.\n8605             [var]_curvefit_covariance\n8606                 The covariance matrix of the coefficient estimates.\n8607 \n8608         See Also\n8609         --------\n8610         Dataset.polyfit\n8611         scipy.optimize.curve_fit\n8612         \"\"\"\n8613         from scipy.optimize import curve_fit\n8614 \n8615         from xarray.core.alignment import broadcast\n8616         from xarray.core.computation import apply_ufunc\n8617         from xarray.core.dataarray import _THIS_ARRAY, DataArray\n8618 \n8619         if p0 is None:\n8620             p0 = {}\n8621         if bounds is None:\n8622             bounds = {}\n8623         if kwargs is None:\n8624             kwargs = {}\n8625 \n8626         reduce_dims_: list[Hashable]\n8627         if not reduce_dims:\n8628             reduce_dims_ = []\n8629         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):\n8630             reduce_dims_ = [reduce_dims]\n8631         else:\n8632             reduce_dims_ = list(reduce_dims)\n8633 \n8634         if (\n8635             isinstance(coords, str)\n8636             or isinstance(coords, DataArray)\n8637             or not isinstance(coords, Iterable)\n8638         ):\n8639             coords = [coords]\n8640         coords_: Sequence[DataArray] = [\n8641             self[coord] if isinstance(coord, str) else coord for coord in coords\n8642         ]\n8643 \n8644         # Determine whether any coords are dims on self\n8645         for coord in coords_:\n8646             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]\n8647         reduce_dims_ = list(set(reduce_dims_))\n8648         preserved_dims = list(set(self.dims) - set(reduce_dims_))\n8649         if not reduce_dims_:\n8650             raise ValueError(\n8651                 \"No arguments to `coords` were identified as a dimension on the calling \"\n8652                 \"object, and no dims were supplied to `reduce_dims`. This would result \"\n8653                 \"in fitting on scalar data.\"\n8654             )\n8655 \n8656         # Broadcast all coords with each other\n8657         coords_ = broadcast(*coords_)\n8658         coords_ = [\n8659             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_\n8660         ]\n8661 \n8662         params, func_args = _get_func_args(func, param_names)\n8663         param_defaults, bounds_defaults = _initialize_curvefit_params(\n8664             params, p0, bounds, func_args\n8665         )\n8666         n_params = len(params)\n8667         kwargs.setdefault(\"p0\", [param_defaults[p] for p in params])\n8668         kwargs.setdefault(\n8669             \"bounds\",\n8670             [\n8671                 [bounds_defaults[p][0] for p in params],\n8672                 [bounds_defaults[p][1] for p in params],\n8673             ],\n8674         )\n8675 \n8676         def _wrapper(Y, *coords_, **kwargs):\n8677             # Wrap curve_fit with raveled coordinates and pointwise NaN handling\n8678             x = np.vstack([c.ravel() for c in coords_])\n8679             y = Y.ravel()\n8680             if skipna:\n8681                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)\n8682                 x = x[:, mask]\n8683                 y = y[mask]\n8684                 if not len(y):\n8685                     popt = np.full([n_params], np.nan)\n8686                     pcov = np.full([n_params, n_params], np.nan)\n8687                     return popt, pcov\n8688             x = np.squeeze(x)\n8689             popt, pcov = curve_fit(func, x, y, **kwargs)\n8690             return popt, pcov\n8691 \n8692         result = type(self)()\n8693         for name, da in self.data_vars.items():\n8694             if name is _THIS_ARRAY:\n8695                 name = \"\"\n8696             else:\n8697                 name = f\"{str(name)}_\"\n8698 \n8699             popt, pcov = apply_ufunc(\n8700                 _wrapper,\n8701                 da,\n8702                 *coords_,\n8703                 vectorize=True,\n8704                 dask=\"parallelized\",\n8705                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],\n8706                 output_core_dims=[[\"param\"], [\"cov_i\", \"cov_j\"]],\n8707                 dask_gufunc_kwargs={\n8708                     \"output_sizes\": {\n8709                         \"param\": n_params,\n8710                         \"cov_i\": n_params,\n8711                         \"cov_j\": n_params,\n8712                     },\n8713                 },\n8714                 output_dtypes=(np.float64, np.float64),\n8715                 exclude_dims=set(reduce_dims_),\n8716                 kwargs=kwargs,\n8717             )\n8718             result[name + \"curvefit_coefficients\"] = popt\n8719             result[name + \"curvefit_covariance\"] = pcov\n8720 \n8721         result = result.assign_coords(\n8722             {\"param\": params, \"cov_i\": params, \"cov_j\": params}\n8723         )\n8724         result.attrs = self.attrs.copy()\n8725 \n8726         return result\n8727 \n8728     def drop_duplicates(\n8729         self: T_Dataset,\n8730         dim: Hashable | Iterable[Hashable],\n8731         keep: Literal[\"first\", \"last\", False] = \"first\",\n8732     ) -> T_Dataset:\n8733         \"\"\"Returns a new Dataset with duplicate dimension values removed.\n8734 \n8735         Parameters\n8736         ----------\n8737         dim : dimension label or labels\n8738             Pass `...` to drop duplicates along all dimensions.\n8739         keep : {\"first\", \"last\", False}, default: \"first\"\n8740             Determines which duplicates (if any) to keep.\n8741             - ``\"first\"`` : Drop duplicates except for the first occurrence.\n8742             - ``\"last\"`` : Drop duplicates except for the last occurrence.\n8743             - False : Drop all duplicates.\n8744 \n8745         Returns\n8746         -------\n8747         Dataset\n8748 \n8749         See Also\n8750         --------\n8751         DataArray.drop_duplicates\n8752         \"\"\"\n8753         if isinstance(dim, str):\n8754             dims: Iterable = (dim,)\n8755         elif dim is ...:\n8756             dims = self.dims\n8757         elif not isinstance(dim, Iterable):\n8758             dims = [dim]\n8759         else:\n8760             dims = dim\n8761 \n8762         missing_dims = set(dims) - set(self.dims)\n8763         if missing_dims:\n8764             raise ValueError(f\"'{missing_dims}' not found in dimensions\")\n8765 \n8766         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}\n8767         return self.isel(indexes)\n8768 \n8769     def convert_calendar(\n8770         self: T_Dataset,\n8771         calendar: CFCalendar,\n8772         dim: Hashable = \"time\",\n8773         align_on: Literal[\"date\", \"year\", None] = None,\n8774         missing: Any | None = None,\n8775         use_cftime: bool | None = None,\n8776     ) -> T_Dataset:\n8777         \"\"\"Convert the Dataset to another calendar.\n8778 \n8779         Only converts the individual timestamps, does not modify any data except\n8780         in dropping invalid/surplus dates or inserting missing dates.\n8781 \n8782         If the source and target calendars are either no_leap, all_leap or a\n8783         standard type, only the type of the time array is modified.\n8784         When converting to a leap year from a non-leap year, the 29th of February\n8785         is removed from the array. In the other direction the 29th of February\n8786         will be missing in the output, unless `missing` is specified,\n8787         in which case that value is inserted.\n8788 \n8789         For conversions involving `360_day` calendars, see Notes.\n8790 \n8791         This method is safe to use with sub-daily data as it doesn't touch the\n8792         time part of the timestamps.\n8793 \n8794         Parameters\n8795         ---------\n8796         calendar : str\n8797             The target calendar name.\n8798         dim : Hashable, default: \"time\"\n8799             Name of the time coordinate.\n8800         align_on : {None, 'date', 'year'}, optional\n8801             Must be specified when either source or target is a `360_day` calendar,\n8802             ignored otherwise. See Notes.\n8803         missing : Any or None, optional\n8804             By default, i.e. if the value is None, this method will simply attempt\n8805             to convert the dates in the source calendar to the same dates in the\n8806             target calendar, and drop any of those that are not possible to\n8807             represent.  If a value is provided, a new time coordinate will be\n8808             created in the target calendar with the same frequency as the original\n8809             time coordinate; for any dates that are not present in the source, the\n8810             data will be filled with this value.  Note that using this mode requires\n8811             that the source data have an inferable frequency; for more information\n8812             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n8813             target calendar combinations, this could result in many missing values, see notes.\n8814         use_cftime : bool or None, optional\n8815             Whether to use cftime objects in the output, only used if `calendar`\n8816             is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n8817             If True, the new time axis uses cftime objects.\n8818             If None (default), it uses :py:class:`numpy.datetime64` values if the\n8819             date range permits it, and :py:class:`cftime.datetime` objects if not.\n8820             If False, it uses :py:class:`numpy.datetime64`  or fails.\n8821 \n8822         Returns\n8823         -------\n8824         Dataset\n8825             Copy of the dataarray with the time coordinate converted to the\n8826             target calendar. If 'missing' was None (default), invalid dates in\n8827             the new calendar are dropped, but missing dates are not inserted.\n8828             If `missing` was given, the new data is reindexed to have a time axis\n8829             with the same frequency as the source, but in the new calendar; any\n8830             missing datapoints are filled with `missing`.\n8831 \n8832         Notes\n8833         -----\n8834         Passing a value to `missing` is only usable if the source's time coordinate as an\n8835         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n8836         if the target coordinate, generated from this frequency, has dates equivalent to the\n8837         source. It is usually **not** appropriate to use this mode with:\n8838 \n8839         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n8840         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n8841             or 'mH' where 24 % m != 0).\n8842 \n8843         If one of the source or target calendars is `\"360_day\"`, `align_on` must\n8844         be specified and two options are offered.\n8845 \n8846         - \"year\"\n8847             The dates are translated according to their relative position in the year,\n8848             ignoring their original month and day information, meaning that the\n8849             missing/surplus days are added/removed at regular intervals.\n8850 \n8851             From a `360_day` to a standard calendar, the output will be missing the\n8852             following dates (day of year in parentheses):\n8853 \n8854             To a leap year:\n8855                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n8856                 September 31st (275) and November 30th (335).\n8857             To a non-leap year:\n8858                 February 6th (36), April 19th (109), July 2nd (183),\n8859                 September 12th (255), November 25th (329).\n8860 \n8861             From a standard calendar to a `\"360_day\"`, the following dates in the\n8862             source array will be dropped:\n8863 \n8864             From a leap year:\n8865                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n8866                 September 31st (275), December 1st (336)\n8867             From a non-leap year:\n8868                 February 6th (37), April 20th (110), July 2nd (183),\n8869                 September 13th (256), November 25th (329)\n8870 \n8871             This option is best used on daily and subdaily data.\n8872 \n8873         - \"date\"\n8874             The month/day information is conserved and invalid dates are dropped\n8875             from the output. This means that when converting from a `\"360_day\"` to a\n8876             standard calendar, all 31st (Jan, March, May, July, August, October and\n8877             December) will be missing as there is no equivalent dates in the\n8878             `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n8879             will be dropped as there are no equivalent dates in a standard calendar.\n8880 \n8881             This option is best used with data on a frequency coarser than daily.\n8882         \"\"\"\n8883         return convert_calendar(\n8884             self,\n8885             calendar,\n8886             dim=dim,\n8887             align_on=align_on,\n8888             missing=missing,\n8889             use_cftime=use_cftime,\n8890         )\n8891 \n8892     def interp_calendar(\n8893         self: T_Dataset,\n8894         target: pd.DatetimeIndex | CFTimeIndex | DataArray,\n8895         dim: Hashable = \"time\",\n8896     ) -> T_Dataset:\n8897         \"\"\"Interpolates the Dataset to another calendar based on decimal year measure.\n8898 \n8899         Each timestamp in `source` and `target` are first converted to their decimal\n8900         year equivalent then `source` is interpolated on the target coordinate.\n8901         The decimal year of a timestamp is its year plus its sub-year component\n8902         converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n8903         2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n8904 \n8905         This method should only be used when the time (HH:MM:SS) information of\n8906         time coordinate is not important.\n8907 \n8908         Parameters\n8909         ----------\n8910         target: DataArray or DatetimeIndex or CFTimeIndex\n8911             The target time coordinate of a valid dtype\n8912             (np.datetime64 or cftime objects)\n8913         dim : Hashable, default: \"time\"\n8914             The time coordinate name.\n8915 \n8916         Return\n8917         ------\n8918         DataArray\n8919             The source interpolated on the decimal years of target,\n8920         \"\"\"\n8921         return interp_calendar(self, target, dim=dim)\n8922 \n8923     def groupby(\n8924         self,\n8925         group: Hashable | DataArray | IndexVariable,\n8926         squeeze: bool = True,\n8927         restore_coord_dims: bool = False,\n8928     ) -> DatasetGroupBy:\n8929         \"\"\"Returns a DatasetGroupBy object for performing grouped operations.\n8930 \n8931         Parameters\n8932         ----------\n8933         group : Hashable, DataArray or IndexVariable\n8934             Array whose unique values should be used to group this array. If a\n8935             string, must be the name of a variable contained in this dataset.\n8936         squeeze : bool, default: True\n8937             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n8938             controls whether the subarrays have a dimension of length 1 along\n8939             that dimension or if the dimension is squeezed out.\n8940         restore_coord_dims : bool, default: False\n8941             If True, also restore the dimension order of multi-dimensional\n8942             coordinates.\n8943 \n8944         Returns\n8945         -------\n8946         grouped : DatasetGroupBy\n8947             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n8948             iterated over in the form of `(unique_value, grouped_array)` pairs.\n8949 \n8950         See Also\n8951         --------\n8952         :ref:`groupby`\n8953             Users guide explanation of how to group and bin data.\n8954         Dataset.groupby_bins\n8955         DataArray.groupby\n8956         core.groupby.DatasetGroupBy\n8957         pandas.DataFrame.groupby\n8958         Dataset.resample\n8959         DataArray.resample\n8960         \"\"\"\n8961         from xarray.core.groupby import (\n8962             DatasetGroupBy,\n8963             ResolvedUniqueGrouper,\n8964             UniqueGrouper,\n8965             _validate_groupby_squeeze,\n8966         )\n8967 \n8968         _validate_groupby_squeeze(squeeze)\n8969         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)\n8970 \n8971         return DatasetGroupBy(\n8972             self,\n8973             (rgrouper,),\n8974             squeeze=squeeze,\n8975             restore_coord_dims=restore_coord_dims,\n8976         )\n8977 \n8978     def groupby_bins(\n8979         self,\n8980         group: Hashable | DataArray | IndexVariable,\n8981         bins: ArrayLike,\n8982         right: bool = True,\n8983         labels: ArrayLike | None = None,\n8984         precision: int = 3,\n8985         include_lowest: bool = False,\n8986         squeeze: bool = True,\n8987         restore_coord_dims: bool = False,\n8988     ) -> DatasetGroupBy:\n8989         \"\"\"Returns a DatasetGroupBy object for performing grouped operations.\n8990 \n8991         Rather than using all unique values of `group`, the values are discretized\n8992         first by applying `pandas.cut` [1]_ to `group`.\n8993 \n8994         Parameters\n8995         ----------\n8996         group : Hashable, DataArray or IndexVariable\n8997             Array whose binned values should be used to group this array. If a\n8998             string, must be the name of a variable contained in this dataset.\n8999         bins : int or array-like\n9000             If bins is an int, it defines the number of equal-width bins in the\n9001             range of x. However, in this case, the range of x is extended by .1%\n9002             on each side to include the min or max values of x. If bins is a\n9003             sequence it defines the bin edges allowing for non-uniform bin\n9004             width. No extension of the range of x is done in this case.\n9005         right : bool, default: True\n9006             Indicates whether the bins include the rightmost edge or not. If\n9007             right == True (the default), then the bins [1,2,3,4] indicate\n9008             (1,2], (2,3], (3,4].\n9009         labels : array-like or bool, default: None\n9010             Used as labels for the resulting bins. Must be of the same length as\n9011             the resulting bins. If False, string bin labels are assigned by\n9012             `pandas.cut`.\n9013         precision : int, default: 3\n9014             The precision at which to store and display the bins labels.\n9015         include_lowest : bool, default: False\n9016             Whether the first interval should be left-inclusive or not.\n9017         squeeze : bool, default: True\n9018             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n9019             controls whether the subarrays have a dimension of length 1 along\n9020             that dimension or if the dimension is squeezed out.\n9021         restore_coord_dims : bool, default: False\n9022             If True, also restore the dimension order of multi-dimensional\n9023             coordinates.\n9024 \n9025         Returns\n9026         -------\n9027         grouped : DatasetGroupBy\n9028             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n9029             iterated over in the form of `(unique_value, grouped_array)` pairs.\n9030             The name of the group has the added suffix `_bins` in order to\n9031             distinguish it from the original variable.\n9032 \n9033         See Also\n9034         --------\n9035         :ref:`groupby`\n9036             Users guide explanation of how to group and bin data.\n9037         Dataset.groupby\n9038         DataArray.groupby_bins\n9039         core.groupby.DatasetGroupBy\n9040         pandas.DataFrame.groupby\n9041 \n9042         References\n9043         ----------\n9044         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n9045         \"\"\"\n9046         from xarray.core.groupby import (\n9047             BinGrouper,\n9048             DatasetGroupBy,\n9049             ResolvedBinGrouper,\n9050             _validate_groupby_squeeze,\n9051         )\n9052 \n9053         _validate_groupby_squeeze(squeeze)\n9054         grouper = BinGrouper(\n9055             bins=bins,\n9056             cut_kwargs={\n9057                 \"right\": right,\n9058                 \"labels\": labels,\n9059                 \"precision\": precision,\n9060                 \"include_lowest\": include_lowest,\n9061             },\n9062         )\n9063         rgrouper = ResolvedBinGrouper(grouper, group, self)\n9064 \n9065         return DatasetGroupBy(\n9066             self,\n9067             (rgrouper,),\n9068             squeeze=squeeze,\n9069             restore_coord_dims=restore_coord_dims,\n9070         )\n9071 \n9072     def weighted(self, weights: DataArray) -> DatasetWeighted:\n9073         \"\"\"\n9074         Weighted Dataset operations.\n9075 \n9076         Parameters\n9077         ----------\n9078         weights : DataArray\n9079             An array of weights associated with the values in this Dataset.\n9080             Each value in the data contributes to the reduction operation\n9081             according to its associated weight.\n9082 \n9083         Notes\n9084         -----\n9085         ``weights`` must be a DataArray and cannot contain missing values.\n9086         Missing values can be replaced by ``weights.fillna(0)``.\n9087 \n9088         Returns\n9089         -------\n9090         core.weighted.DatasetWeighted\n9091 \n9092         See Also\n9093         --------\n9094         DataArray.weighted\n9095         \"\"\"\n9096         from xarray.core.weighted import DatasetWeighted\n9097 \n9098         return DatasetWeighted(self, weights)\n9099 \n9100     def rolling(\n9101         self,\n9102         dim: Mapping[Any, int] | None = None,\n9103         min_periods: int | None = None,\n9104         center: bool | Mapping[Any, bool] = False,\n9105         **window_kwargs: int,\n9106     ) -> DatasetRolling:\n9107         \"\"\"\n9108         Rolling window object for Datasets.\n9109 \n9110         Parameters\n9111         ----------\n9112         dim : dict, optional\n9113             Mapping from the dimension name to create the rolling iterator\n9114             along (e.g. `time`) to its moving window size.\n9115         min_periods : int or None, default: None\n9116             Minimum number of observations in window required to have a value\n9117             (otherwise result is NA). The default, None, is equivalent to\n9118             setting min_periods equal to the size of the window.\n9119         center : bool or Mapping to int, default: False\n9120             Set the labels at the center of the window.\n9121         **window_kwargs : optional\n9122             The keyword arguments form of ``dim``.\n9123             One of dim or window_kwargs must be provided.\n9124 \n9125         Returns\n9126         -------\n9127         core.rolling.DatasetRolling\n9128 \n9129         See Also\n9130         --------\n9131         core.rolling.DatasetRolling\n9132         DataArray.rolling\n9133         \"\"\"\n9134         from xarray.core.rolling import DatasetRolling\n9135 \n9136         dim = either_dict_or_kwargs(dim, window_kwargs, \"rolling\")\n9137         return DatasetRolling(self, dim, min_periods=min_periods, center=center)\n9138 \n9139     def coarsen(\n9140         self,\n9141         dim: Mapping[Any, int] | None = None,\n9142         boundary: CoarsenBoundaryOptions = \"exact\",\n9143         side: SideOptions | Mapping[Any, SideOptions] = \"left\",\n9144         coord_func: str | Callable | Mapping[Any, str | Callable] = \"mean\",\n9145         **window_kwargs: int,\n9146     ) -> DatasetCoarsen:\n9147         \"\"\"\n9148         Coarsen object for Datasets.\n9149 \n9150         Parameters\n9151         ----------\n9152         dim : mapping of hashable to int, optional\n9153             Mapping from the dimension name to the window size.\n9154         boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n9155             If 'exact', a ValueError will be raised if dimension size is not a\n9156             multiple of the window size. If 'trim', the excess entries are\n9157             dropped. If 'pad', NA will be padded.\n9158         side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n9159         coord_func : str or mapping of hashable to str, default: \"mean\"\n9160             function (name) that is applied to the coordinates,\n9161             or a mapping from coordinate name to function (name).\n9162 \n9163         Returns\n9164         -------\n9165         core.rolling.DatasetCoarsen\n9166 \n9167         See Also\n9168         --------\n9169         core.rolling.DatasetCoarsen\n9170         DataArray.coarsen\n9171         \"\"\"\n9172         from xarray.core.rolling import DatasetCoarsen\n9173 \n9174         dim = either_dict_or_kwargs(dim, window_kwargs, \"coarsen\")\n9175         return DatasetCoarsen(\n9176             self,\n9177             dim,\n9178             boundary=boundary,\n9179             side=side,\n9180             coord_func=coord_func,\n9181         )\n9182 \n9183     def resample(\n9184         self,\n9185         indexer: Mapping[Any, str] | None = None,\n9186         skipna: bool | None = None,\n9187         closed: SideOptions | None = None,\n9188         label: SideOptions | None = None,\n9189         base: int | None = None,\n9190         offset: pd.Timedelta | datetime.timedelta | str | None = None,\n9191         origin: str | DatetimeLike = \"start_day\",\n9192         keep_attrs: bool | None = None,\n9193         loffset: datetime.timedelta | str | None = None,\n9194         restore_coord_dims: bool | None = None,\n9195         **indexer_kwargs: str,\n9196     ) -> DatasetResample:\n9197         \"\"\"Returns a Resample object for performing resampling operations.\n9198 \n9199         Handles both downsampling and upsampling. The resampled\n9200         dimension must be a datetime-like coordinate. If any intervals\n9201         contain no values from the original object, they will be given\n9202         the value ``NaN``.\n9203 \n9204         Parameters\n9205         ----------\n9206         indexer : Mapping of Hashable to str, optional\n9207             Mapping from the dimension name to resample frequency [1]_. The\n9208             dimension must be datetime-like.\n9209         skipna : bool, optional\n9210             Whether to skip missing values when aggregating in downsampling.\n9211         closed : {\"left\", \"right\"}, optional\n9212             Side of each interval to treat as closed.\n9213         label : {\"left\", \"right\"}, optional\n9214             Side of each interval to use for labeling.\n9215         base : int, optional\n9216             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n9217             aggregated intervals. For example, for \"24H\" frequency, base could\n9218             range from 0 through 23.\n9219         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'\n9220             The datetime on which to adjust the grouping. The timezone of origin\n9221             must match the timezone of the index.\n9222 \n9223             If a datetime is not used, these values are also supported:\n9224             - 'epoch': `origin` is 1970-01-01\n9225             - 'start': `origin` is the first value of the timeseries\n9226             - 'start_day': `origin` is the first day at midnight of the timeseries\n9227             - 'end': `origin` is the last value of the timeseries\n9228             - 'end_day': `origin` is the ceiling midnight of the last day\n9229         offset : pd.Timedelta, datetime.timedelta, or str, default is None\n9230             An offset timedelta added to the origin.\n9231         loffset : timedelta or str, optional\n9232             Offset used to adjust the resampled time labels. Some pandas date\n9233             offset strings are supported.\n9234         restore_coord_dims : bool, optional\n9235             If True, also restore the dimension order of multi-dimensional\n9236             coordinates.\n9237         **indexer_kwargs : str\n9238             The keyword arguments form of ``indexer``.\n9239             One of indexer or indexer_kwargs must be provided.\n9240 \n9241         Returns\n9242         -------\n9243         resampled : core.resample.DataArrayResample\n9244             This object resampled.\n9245 \n9246         See Also\n9247         --------\n9248         DataArray.resample\n9249         pandas.Series.resample\n9250         pandas.DataFrame.resample\n9251         Dataset.groupby\n9252         DataArray.groupby\n9253 \n9254         References\n9255         ----------\n9256         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n9257         \"\"\"\n9258         from xarray.core.resample import DatasetResample\n9259 \n9260         return self._resample(\n9261             resample_cls=DatasetResample,\n9262             indexer=indexer,\n9263             skipna=skipna,\n9264             closed=closed,\n9265             label=label,\n9266             base=base,\n9267             offset=offset,\n9268             origin=origin,\n9269             keep_attrs=keep_attrs,\n9270             loffset=loffset,\n9271             restore_coord_dims=restore_coord_dims,\n9272             **indexer_kwargs,\n9273         )\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 1080,
        "end": 1100,
        "class_name": "DataArray",
        "method_name": "compute",
        "code": "1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:\n1081         \"\"\"Manually trigger loading of this array's data from disk or a\n1082         remote source into memory and return a new array. The original is\n1083         left unaltered.\n1084 \n1085         Normally, it should not be necessary to call this method in user code,\n1086         because all xarray functions should either work on deferred data or\n1087         load data automatically. However, this method can be necessary when\n1088         working with many file objects on disk.\n1089 \n1090         Parameters\n1091         ----------\n1092         **kwargs : dict\n1093             Additional keyword arguments passed on to ``dask.compute``.\n1094 \n1095         See Also\n1096         --------\n1097         dask.compute\n1098         \"\"\"\n1099         new = self.copy(deep=False)\n1100         return new.load(**kwargs)\n",
        "intended_behavior": "The `compute` method needs to be adapted to support execution using the specified parallel computation backend. This might involve dispatching the compute operation to different handlers based on the backend configuration, ensuring compatibility with both `dask` and potential new backends like `cubed`."
    },
    {
        "rel_file_path": "xarray/core/dataarray.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataarray.py",
        "start": 229,
        "end": 6971,
        "class_name": "DataArray",
        "method_name": null,
        "code": "229 class DataArray(\n230     AbstractArray,\n231     DataWithCoords,\n232     DataArrayArithmetic,\n233     DataArrayAggregations,\n234 ):\n235     \"\"\"N-dimensional array with labeled coordinates and dimensions.\n236 \n237     DataArray provides a wrapper around numpy ndarrays that uses\n238     labeled dimensions and coordinates to support metadata aware\n239     operations. The API is similar to that for the pandas Series or\n240     DataFrame, but DataArray objects can have any number of dimensions,\n241     and their contents have fixed data types.\n242 \n243     Additional features over raw numpy arrays:\n244 \n245     - Apply operations over dimensions by name: ``x.sum('time')``.\n246     - Select or assign values by integer location (like numpy):\n247       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or\n248       ``x.sel(time='2014-01-01')``.\n249     - Mathematical operations (e.g., ``x - y``) vectorize across\n250       multiple dimensions (known in numpy as \"broadcasting\") based on\n251       dimension names, regardless of their original order.\n252     - Keep track of arbitrary metadata in the form of a Python\n253       dictionary: ``x.attrs``\n254     - Convert to a pandas Series: ``x.to_series()``.\n255 \n256     Getting items from or doing mathematical operations with a\n257     DataArray always returns another DataArray.\n258 \n259     Parameters\n260     ----------\n261     data : array_like\n262         Values for this array. Must be an ``numpy.ndarray``, ndarray\n263         like, or castable to an ``ndarray``. If a self-described xarray\n264         or pandas object, attempts are made to use this array's\n265         metadata to fill in other unspecified arguments. A view of the\n266         array's data is used instead of a copy if possible.\n267     coords : sequence or dict of array_like, optional\n268         Coordinates (tick labels) to use for indexing along each\n269         dimension. The following notations are accepted:\n270 \n271         - mapping {dimension name: array-like}\n272         - sequence of tuples that are valid arguments for\n273           ``xarray.Variable()``\n274           - (dims, data)\n275           - (dims, data, attrs)\n276           - (dims, data, attrs, encoding)\n277 \n278         Additionally, it is possible to define a coord whose name\n279         does not match the dimension name, or a coord based on multiple\n280         dimensions, with one of the following notations:\n281 \n282         - mapping {coord name: DataArray}\n283         - mapping {coord name: Variable}\n284         - mapping {coord name: (dimension name, array-like)}\n285         - mapping {coord name: (tuple of dimension names, array-like)}\n286 \n287     dims : Hashable or sequence of Hashable, optional\n288         Name(s) of the data dimension(s). Must be either a Hashable\n289         (only for 1D data) or a sequence of Hashables with length equal\n290         to the number of dimensions. If this argument is omitted,\n291         dimension names are taken from ``coords`` (if possible) and\n292         otherwise default to ``['dim_0', ... 'dim_n']``.\n293     name : str or None, optional\n294         Name of this array.\n295     attrs : dict_like or None, optional\n296         Attributes to assign to the new instance. By default, an empty\n297         attribute dictionary is initialized.\n298 \n299     Examples\n300     --------\n301     Create data:\n302 \n303     >>> np.random.seed(0)\n304     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n305     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n306     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n307     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n308     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n309 \n310     Initialize a dataarray with multiple dimensions:\n311 \n312     >>> da = xr.DataArray(\n313     ...     data=temperature,\n314     ...     dims=[\"x\", \"y\", \"time\"],\n315     ...     coords=dict(\n316     ...         lon=([\"x\", \"y\"], lon),\n317     ...         lat=([\"x\", \"y\"], lat),\n318     ...         time=time,\n319     ...         reference_time=reference_time,\n320     ...     ),\n321     ...     attrs=dict(\n322     ...         description=\"Ambient temperature.\",\n323     ...         units=\"degC\",\n324     ...     ),\n325     ... )\n326     >>> da\n327     <xarray.DataArray (x: 2, y: 2, time: 3)>\n328     array([[[29.11241877, 18.20125767, 22.82990387],\n329             [32.92714559, 29.94046392,  7.18177696]],\n330     <BLANKLINE>\n331            [[22.60070734, 13.78914233, 14.17424919],\n332             [18.28478802, 16.15234857, 26.63418806]]])\n333     Coordinates:\n334         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n335         lat             (x, y) float64 42.25 42.21 42.63 42.59\n336       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n337         reference_time  datetime64[ns] 2014-09-05\n338     Dimensions without coordinates: x, y\n339     Attributes:\n340         description:  Ambient temperature.\n341         units:        degC\n342 \n343     Find out where the coldest temperature was:\n344 \n345     >>> da.isel(da.argmin(...))\n346     <xarray.DataArray ()>\n347     array(7.18177696)\n348     Coordinates:\n349         lon             float64 -99.32\n350         lat             float64 42.21\n351         time            datetime64[ns] 2014-09-08\n352         reference_time  datetime64[ns] 2014-09-05\n353     Attributes:\n354         description:  Ambient temperature.\n355         units:        degC\n356     \"\"\"\n357 \n358     _cache: dict[str, Any]\n359     _coords: dict[Any, Variable]\n360     _close: Callable[[], None] | None\n361     _indexes: dict[Hashable, Index]\n362     _name: Hashable | None\n363     _variable: Variable\n364 \n365     __slots__ = (\n366         \"_cache\",\n367         \"_coords\",\n368         \"_close\",\n369         \"_indexes\",\n370         \"_name\",\n371         \"_variable\",\n372         \"__weakref__\",\n373     )\n374 \n375     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor[\"DataArray\"])\n376 \n377     def __init__(\n378         self,\n379         data: Any = dtypes.NA,\n380         coords: Sequence[Sequence[Any] | pd.Index | DataArray]\n381         | Mapping[Any, Any]\n382         | None = None,\n383         dims: Hashable | Sequence[Hashable] | None = None,\n384         name: Hashable | None = None,\n385         attrs: Mapping | None = None,\n386         # internal parameters\n387         indexes: dict[Hashable, Index] | None = None,\n388         fastpath: bool = False,\n389     ) -> None:\n390         if fastpath:\n391             variable = data\n392             assert dims is None\n393             assert attrs is None\n394             assert indexes is not None\n395         else:\n396             # TODO: (benbovy - explicit indexes) remove\n397             # once it becomes part of the public interface\n398             if indexes is not None:\n399                 raise ValueError(\"Providing explicit indexes is not supported yet\")\n400 \n401             # try to fill in arguments from data if they weren't supplied\n402             if coords is None:\n403                 if isinstance(data, DataArray):\n404                     coords = data.coords\n405                 elif isinstance(data, pd.Series):\n406                     coords = [data.index]\n407                 elif isinstance(data, pd.DataFrame):\n408                     coords = [data.index, data.columns]\n409                 elif isinstance(data, (pd.Index, IndexVariable)):\n410                     coords = [data]\n411 \n412             if dims is None:\n413                 dims = getattr(data, \"dims\", getattr(coords, \"dims\", None))\n414             if name is None:\n415                 name = getattr(data, \"name\", None)\n416             if attrs is None and not isinstance(data, PANDAS_TYPES):\n417                 attrs = getattr(data, \"attrs\", None)\n418 \n419             data = _check_data_shape(data, coords, dims)\n420             data = as_compatible_data(data)\n421             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)\n422             variable = Variable(dims, data, attrs, fastpath=True)\n423             indexes, coords = _create_indexes_from_coords(coords)\n424 \n425         # These fully describe a DataArray\n426         self._variable = variable\n427         assert isinstance(coords, dict)\n428         self._coords = coords\n429         self._name = name\n430 \n431         # TODO(shoyer): document this argument, once it becomes part of the\n432         # public interface.\n433         self._indexes = indexes\n434 \n435         self._close = None\n436 \n437     @classmethod\n438     def _construct_direct(\n439         cls: type[T_DataArray],\n440         variable: Variable,\n441         coords: dict[Any, Variable],\n442         name: Hashable,\n443         indexes: dict[Hashable, Index],\n444     ) -> T_DataArray:\n445         \"\"\"Shortcut around __init__ for internal use when we want to skip\n446         costly validation\n447         \"\"\"\n448         obj = object.__new__(cls)\n449         obj._variable = variable\n450         obj._coords = coords\n451         obj._name = name\n452         obj._indexes = indexes\n453         obj._close = None\n454         return obj\n455 \n456     def _replace(\n457         self: T_DataArray,\n458         variable: Variable | None = None,\n459         coords=None,\n460         name: Hashable | None | Default = _default,\n461         indexes=None,\n462     ) -> T_DataArray:\n463         if variable is None:\n464             variable = self.variable\n465         if coords is None:\n466             coords = self._coords\n467         if indexes is None:\n468             indexes = self._indexes\n469         if name is _default:\n470             name = self.name\n471         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)\n472 \n473     def _replace_maybe_drop_dims(\n474         self: T_DataArray,\n475         variable: Variable,\n476         name: Hashable | None | Default = _default,\n477     ) -> T_DataArray:\n478         if variable.dims == self.dims and variable.shape == self.shape:\n479             coords = self._coords.copy()\n480             indexes = self._indexes\n481         elif variable.dims == self.dims:\n482             # Shape has changed (e.g. from reduce(..., keepdims=True)\n483             new_sizes = dict(zip(self.dims, variable.shape))\n484             coords = {\n485                 k: v\n486                 for k, v in self._coords.items()\n487                 if v.shape == tuple(new_sizes[d] for d in v.dims)\n488             }\n489             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n490         else:\n491             allowed_dims = set(variable.dims)\n492             coords = {\n493                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims\n494             }\n495             indexes = filter_indexes_from_coords(self._indexes, set(coords))\n496         return self._replace(variable, coords, name, indexes=indexes)\n497 \n498     def _overwrite_indexes(\n499         self: T_DataArray,\n500         indexes: Mapping[Any, Index],\n501         coords: Mapping[Any, Variable] | None = None,\n502         drop_coords: list[Hashable] | None = None,\n503         rename_dims: Mapping[Any, Any] | None = None,\n504     ) -> T_DataArray:\n505         \"\"\"Maybe replace indexes and their corresponding coordinates.\"\"\"\n506         if not indexes:\n507             return self\n508 \n509         if coords is None:\n510             coords = {}\n511         if drop_coords is None:\n512             drop_coords = []\n513 \n514         new_variable = self.variable.copy()\n515         new_coords = self._coords.copy()\n516         new_indexes = dict(self._indexes)\n517 \n518         for name in indexes:\n519             new_coords[name] = coords[name]\n520             new_indexes[name] = indexes[name]\n521 \n522         for name in drop_coords:\n523             new_coords.pop(name)\n524             new_indexes.pop(name)\n525 \n526         if rename_dims:\n527             new_variable.dims = tuple(rename_dims.get(d, d) for d in new_variable.dims)\n528 \n529         return self._replace(\n530             variable=new_variable, coords=new_coords, indexes=new_indexes\n531         )\n532 \n533     def _to_temp_dataset(self) -> Dataset:\n534         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)\n535 \n536     def _from_temp_dataset(\n537         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default\n538     ) -> T_DataArray:\n539         variable = dataset._variables.pop(_THIS_ARRAY)\n540         coords = dataset._variables\n541         indexes = dataset._indexes\n542         return self._replace(variable, coords, name, indexes=indexes)\n543 \n544     def _to_dataset_split(self, dim: Hashable) -> Dataset:\n545         \"\"\"splits dataarray along dimension 'dim'\"\"\"\n546 \n547         def subset(dim, label):\n548             array = self.loc[{dim: label}]\n549             array.attrs = {}\n550             return as_variable(array)\n551 \n552         variables = {label: subset(dim, label) for label in self.get_index(dim)}\n553         variables.update({k: v for k, v in self._coords.items() if k != dim})\n554         coord_names = set(self._coords) - {dim}\n555         indexes = filter_indexes_from_coords(self._indexes, coord_names)\n556         dataset = Dataset._construct_direct(\n557             variables, coord_names, indexes=indexes, attrs=self.attrs\n558         )\n559         return dataset\n560 \n561     def _to_dataset_whole(\n562         self, name: Hashable = None, shallow_copy: bool = True\n563     ) -> Dataset:\n564         if name is None:\n565             name = self.name\n566         if name is None:\n567             raise ValueError(\n568                 \"unable to convert unnamed DataArray to a \"\n569                 \"Dataset without providing an explicit name\"\n570             )\n571         if name in self.coords:\n572             raise ValueError(\n573                 \"cannot create a Dataset from a DataArray with \"\n574                 \"the same name as one of its coordinates\"\n575             )\n576         # use private APIs for speed: this is called by _to_temp_dataset(),\n577         # which is used in the guts of a lot of operations (e.g., reindex)\n578         variables = self._coords.copy()\n579         variables[name] = self.variable\n580         if shallow_copy:\n581             for k in variables:\n582                 variables[k] = variables[k].copy(deep=False)\n583         indexes = self._indexes\n584 \n585         coord_names = set(self._coords)\n586         return Dataset._construct_direct(variables, coord_names, indexes=indexes)\n587 \n588     def to_dataset(\n589         self,\n590         dim: Hashable = None,\n591         *,\n592         name: Hashable = None,\n593         promote_attrs: bool = False,\n594     ) -> Dataset:\n595         \"\"\"Convert a DataArray to a Dataset.\n596 \n597         Parameters\n598         ----------\n599         dim : Hashable, optional\n600             Name of the dimension on this array along which to split this array\n601             into separate variables. If not provided, this array is converted\n602             into a Dataset of one variable.\n603         name : Hashable, optional\n604             Name to substitute for this array's name. Only valid if ``dim`` is\n605             not provided.\n606         promote_attrs : bool, default: False\n607             Set to True to shallow copy attrs of DataArray to returned Dataset.\n608 \n609         Returns\n610         -------\n611         dataset : Dataset\n612         \"\"\"\n613         if dim is not None and dim not in self.dims:\n614             raise TypeError(\n615                 f\"{dim} is not a dim. If supplying a ``name``, pass as a kwarg.\"\n616             )\n617 \n618         if dim is not None:\n619             if name is not None:\n620                 raise TypeError(\"cannot supply both dim and name arguments\")\n621             result = self._to_dataset_split(dim)\n622         else:\n623             result = self._to_dataset_whole(name)\n624 \n625         if promote_attrs:\n626             result.attrs = dict(self.attrs)\n627 \n628         return result\n629 \n630     @property\n631     def name(self) -> Hashable | None:\n632         \"\"\"The name of this array.\"\"\"\n633         return self._name\n634 \n635     @name.setter\n636     def name(self, value: Hashable | None) -> None:\n637         self._name = value\n638 \n639     @property\n640     def variable(self) -> Variable:\n641         \"\"\"Low level interface to the Variable object for this DataArray.\"\"\"\n642         return self._variable\n643 \n644     @property\n645     def dtype(self) -> np.dtype:\n646         \"\"\"\n647         Data-type of the array\u2019s elements.\n648 \n649         See Also\n650         --------\n651         ndarray.dtype\n652         numpy.dtype\n653         \"\"\"\n654         return self.variable.dtype\n655 \n656     @property\n657     def shape(self) -> tuple[int, ...]:\n658         \"\"\"\n659         Tuple of array dimensions.\n660 \n661         See Also\n662         --------\n663         numpy.ndarray.shape\n664         \"\"\"\n665         return self.variable.shape\n666 \n667     @property\n668     def size(self) -> int:\n669         \"\"\"\n670         Number of elements in the array.\n671 \n672         Equal to ``np.prod(a.shape)``, i.e., the product of the array\u2019s dimensions.\n673 \n674         See Also\n675         --------\n676         numpy.ndarray.size\n677         \"\"\"\n678         return self.variable.size\n679 \n680     @property\n681     def nbytes(self) -> int:\n682         \"\"\"\n683         Total bytes consumed by the elements of this DataArray's data.\n684 \n685         If the underlying data array does not include ``nbytes``, estimates\n686         the bytes consumed based on the ``size`` and ``dtype``.\n687         \"\"\"\n688         return self.variable.nbytes\n689 \n690     @property\n691     def ndim(self) -> int:\n692         \"\"\"\n693         Number of array dimensions.\n694 \n695         See Also\n696         --------\n697         numpy.ndarray.ndim\n698         \"\"\"\n699         return self.variable.ndim\n700 \n701     def __len__(self) -> int:\n702         return len(self.variable)\n703 \n704     @property\n705     def data(self) -> Any:\n706         \"\"\"\n707         The DataArray's data as an array. The underlying array type\n708         (e.g. dask, sparse, pint) is preserved.\n709 \n710         See Also\n711         --------\n712         DataArray.to_numpy\n713         DataArray.as_numpy\n714         DataArray.values\n715         \"\"\"\n716         return self.variable.data\n717 \n718     @data.setter\n719     def data(self, value: Any) -> None:\n720         self.variable.data = value\n721 \n722     @property\n723     def values(self) -> np.ndarray:\n724         \"\"\"\n725         The array's data as a numpy.ndarray.\n726 \n727         If the array's data is not a numpy.ndarray this will attempt to convert\n728         it naively using np.array(), which will raise an error if the array\n729         type does not support coercion like this (e.g. cupy).\n730         \"\"\"\n731         return self.variable.values\n732 \n733     @values.setter\n734     def values(self, value: Any) -> None:\n735         self.variable.values = value\n736 \n737     def to_numpy(self) -> np.ndarray:\n738         \"\"\"\n739         Coerces wrapped data to numpy and returns a numpy.ndarray.\n740 \n741         See Also\n742         --------\n743         DataArray.as_numpy : Same but returns the surrounding DataArray instead.\n744         Dataset.as_numpy\n745         DataArray.values\n746         DataArray.data\n747         \"\"\"\n748         return self.variable.to_numpy()\n749 \n750     def as_numpy(self: T_DataArray) -> T_DataArray:\n751         \"\"\"\n752         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.\n753 \n754         See Also\n755         --------\n756         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.\n757         Dataset.as_numpy : Converts all variables in a Dataset.\n758         DataArray.values\n759         DataArray.data\n760         \"\"\"\n761         coords = {k: v.as_numpy() for k, v in self._coords.items()}\n762         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)\n763 \n764     @property\n765     def _in_memory(self) -> bool:\n766         return self.variable._in_memory\n767 \n768     def _to_index(self) -> pd.Index:\n769         return self.variable._to_index()\n770 \n771     def to_index(self) -> pd.Index:\n772         \"\"\"Convert this variable to a pandas.Index. Only possible for 1D\n773         arrays.\n774         \"\"\"\n775         return self.variable.to_index()\n776 \n777     @property\n778     def dims(self) -> tuple[Hashable, ...]:\n779         \"\"\"Tuple of dimension names associated with this array.\n780 \n781         Note that the type of this property is inconsistent with\n782         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for\n783         consistently named properties.\n784 \n785         See Also\n786         --------\n787         DataArray.sizes\n788         Dataset.dims\n789         \"\"\"\n790         return self.variable.dims\n791 \n792     @dims.setter\n793     def dims(self, value: Any) -> NoReturn:\n794         raise AttributeError(\n795             \"you cannot assign dims on a DataArray. Use \"\n796             \".rename() or .swap_dims() instead.\"\n797         )\n798 \n799     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:\n800         if utils.is_dict_like(key):\n801             return key\n802         key = indexing.expanded_indexer(key, self.ndim)\n803         return dict(zip(self.dims, key))\n804 \n805     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:\n806         from xarray.core.dataset import _get_virtual_variable\n807 \n808         try:\n809             var = self._coords[key]\n810         except KeyError:\n811             dim_sizes = dict(zip(self.dims, self.shape))\n812             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)\n813 \n814         return self._replace_maybe_drop_dims(var, name=key)\n815 \n816     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:\n817         if isinstance(key, str):\n818             return self._getitem_coord(key)\n819         else:\n820             # xarray-style array indexing\n821             return self.isel(indexers=self._item_key_to_dict(key))\n822 \n823     def __setitem__(self, key: Any, value: Any) -> None:\n824         if isinstance(key, str):\n825             self.coords[key] = value\n826         else:\n827             # Coordinates in key, value and self[key] should be consistent.\n828             # TODO Coordinate consistency in key is checked here, but it\n829             # causes unnecessary indexing. It should be optimized.\n830             obj = self[key]\n831             if isinstance(value, DataArray):\n832                 assert_coordinate_consistent(value, obj.coords.variables)\n833             # DataArray key -> Variable key\n834             key = {\n835                 k: v.variable if isinstance(v, DataArray) else v\n836                 for k, v in self._item_key_to_dict(key).items()\n837             }\n838             self.variable[key] = value\n839 \n840     def __delitem__(self, key: Any) -> None:\n841         del self.coords[key]\n842 \n843     @property\n844     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n845         \"\"\"Places to look-up items for attribute-style access\"\"\"\n846         yield from self._item_sources\n847         yield self.attrs\n848 \n849     @property\n850     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n851         \"\"\"Places to look-up items for key-completion\"\"\"\n852         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)\n853 \n854         # virtual coordinates\n855         # uses empty dict -- everything here can already be found in self.coords.\n856         yield HybridMappingProxy(keys=self.dims, mapping={})\n857 \n858     def __contains__(self, key: Any) -> bool:\n859         return key in self.data\n860 \n861     @property\n862     def loc(self) -> _LocIndexer:\n863         \"\"\"Attribute for location based indexing like pandas.\"\"\"\n864         return _LocIndexer(self)\n865 \n866     @property\n867     def attrs(self) -> dict[Any, Any]:\n868         \"\"\"Dictionary storing arbitrary metadata with this array.\"\"\"\n869         return self.variable.attrs\n870 \n871     @attrs.setter\n872     def attrs(self, value: Mapping[Any, Any]) -> None:\n873         self.variable.attrs = dict(value)\n874 \n875     @property\n876     def encoding(self) -> dict[Any, Any]:\n877         \"\"\"Dictionary of format-specific settings for how this array should be\n878         serialized.\"\"\"\n879         return self.variable.encoding\n880 \n881     @encoding.setter\n882     def encoding(self, value: Mapping[Any, Any]) -> None:\n883         self.variable.encoding = dict(value)\n884 \n885     def reset_encoding(self: T_DataArray) -> T_DataArray:\n886         \"\"\"Return a new DataArray without encoding on the array or any attached\n887         coords.\"\"\"\n888         ds = self._to_temp_dataset().reset_encoding()\n889         return self._from_temp_dataset(ds)\n890 \n891     @property\n892     def indexes(self) -> Indexes:\n893         \"\"\"Mapping of pandas.Index objects used for label based indexing.\n894 \n895         Raises an error if this Dataset has indexes that cannot be coerced\n896         to pandas.Index objects.\n897 \n898         See Also\n899         --------\n900         DataArray.xindexes\n901 \n902         \"\"\"\n903         return self.xindexes.to_pandas_indexes()\n904 \n905     @property\n906     def xindexes(self) -> Indexes:\n907         \"\"\"Mapping of xarray Index objects used for label based indexing.\"\"\"\n908         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})\n909 \n910     @property\n911     def coords(self) -> DataArrayCoordinates:\n912         \"\"\"Dictionary-like container of coordinate arrays.\"\"\"\n913         return DataArrayCoordinates(self)\n914 \n915     @overload\n916     def reset_coords(\n917         self: T_DataArray,\n918         names: Dims = None,\n919         drop: Literal[False] = False,\n920     ) -> Dataset:\n921         ...\n922 \n923     @overload\n924     def reset_coords(\n925         self: T_DataArray,\n926         names: Dims = None,\n927         *,\n928         drop: Literal[True],\n929     ) -> T_DataArray:\n930         ...\n931 \n932     def reset_coords(\n933         self: T_DataArray,\n934         names: Dims = None,\n935         drop: bool = False,\n936     ) -> T_DataArray | Dataset:\n937         \"\"\"Given names of coordinates, reset them to become variables.\n938 \n939         Parameters\n940         ----------\n941         names : str, Iterable of Hashable or None, optional\n942             Name(s) of non-index coordinates in this dataset to reset into\n943             variables. By default, all non-index coordinates are reset.\n944         drop : bool, default: False\n945             If True, remove coordinates instead of converting them into\n946             variables.\n947 \n948         Returns\n949         -------\n950         Dataset, or DataArray if ``drop == True``\n951 \n952         Examples\n953         --------\n954         >>> temperature = np.arange(25).reshape(5, 5)\n955         >>> pressure = np.arange(50, 75).reshape(5, 5)\n956         >>> da = xr.DataArray(\n957         ...     data=temperature,\n958         ...     dims=[\"x\", \"y\"],\n959         ...     coords=dict(\n960         ...         lon=(\"x\", np.arange(10, 15)),\n961         ...         lat=(\"y\", np.arange(20, 25)),\n962         ...         Pressure=([\"x\", \"y\"], pressure),\n963         ...     ),\n964         ...     name=\"Temperature\",\n965         ... )\n966         >>> da\n967         <xarray.DataArray 'Temperature' (x: 5, y: 5)>\n968         array([[ 0,  1,  2,  3,  4],\n969                [ 5,  6,  7,  8,  9],\n970                [10, 11, 12, 13, 14],\n971                [15, 16, 17, 18, 19],\n972                [20, 21, 22, 23, 24]])\n973         Coordinates:\n974             lon       (x) int64 10 11 12 13 14\n975             lat       (y) int64 20 21 22 23 24\n976             Pressure  (x, y) int64 50 51 52 53 54 55 56 57 ... 67 68 69 70 71 72 73 74\n977         Dimensions without coordinates: x, y\n978 \n979         Return Dataset with target coordinate as a data variable rather than a coordinate variable:\n980 \n981         >>> da.reset_coords(names=\"Pressure\")\n982         <xarray.Dataset>\n983         Dimensions:      (x: 5, y: 5)\n984         Coordinates:\n985             lon          (x) int64 10 11 12 13 14\n986             lat          (y) int64 20 21 22 23 24\n987         Dimensions without coordinates: x, y\n988         Data variables:\n989             Pressure     (x, y) int64 50 51 52 53 54 55 56 57 ... 68 69 70 71 72 73 74\n990             Temperature  (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24\n991 \n992         Return DataArray without targeted coordinate:\n993 \n994         >>> da.reset_coords(names=\"Pressure\", drop=True)\n995         <xarray.DataArray 'Temperature' (x: 5, y: 5)>\n996         array([[ 0,  1,  2,  3,  4],\n997                [ 5,  6,  7,  8,  9],\n998                [10, 11, 12, 13, 14],\n999                [15, 16, 17, 18, 19],\n1000                [20, 21, 22, 23, 24]])\n1001         Coordinates:\n1002             lon      (x) int64 10 11 12 13 14\n1003             lat      (y) int64 20 21 22 23 24\n1004         Dimensions without coordinates: x, y\n1005         \"\"\"\n1006         if names is None:\n1007             names = set(self.coords) - set(self._indexes)\n1008         dataset = self.coords.to_dataset().reset_coords(names, drop)\n1009         if drop:\n1010             return self._replace(coords=dataset._variables)\n1011         if self.name is None:\n1012             raise ValueError(\n1013                 \"cannot reset_coords with drop=False on an unnamed DataArrray\"\n1014             )\n1015         dataset[self.name] = self.variable\n1016         return dataset\n1017 \n1018     def __dask_tokenize__(self):\n1019         from dask.base import normalize_token\n1020 \n1021         return normalize_token((type(self), self._variable, self._coords, self._name))\n1022 \n1023     def __dask_graph__(self):\n1024         return self._to_temp_dataset().__dask_graph__()\n1025 \n1026     def __dask_keys__(self):\n1027         return self._to_temp_dataset().__dask_keys__()\n1028 \n1029     def __dask_layers__(self):\n1030         return self._to_temp_dataset().__dask_layers__()\n1031 \n1032     @property\n1033     def __dask_optimize__(self):\n1034         return self._to_temp_dataset().__dask_optimize__\n1035 \n1036     @property\n1037     def __dask_scheduler__(self):\n1038         return self._to_temp_dataset().__dask_scheduler__\n1039 \n1040     def __dask_postcompute__(self):\n1041         func, args = self._to_temp_dataset().__dask_postcompute__()\n1042         return self._dask_finalize, (self.name, func) + args\n1043 \n1044     def __dask_postpersist__(self):\n1045         func, args = self._to_temp_dataset().__dask_postpersist__()\n1046         return self._dask_finalize, (self.name, func) + args\n1047 \n1048     @staticmethod\n1049     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:\n1050         ds = func(results, *args, **kwargs)\n1051         variable = ds._variables.pop(_THIS_ARRAY)\n1052         coords = ds._variables\n1053         indexes = ds._indexes\n1054         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n1055 \n1056     def load(self: T_DataArray, **kwargs) -> T_DataArray:\n1057         \"\"\"Manually trigger loading of this array's data from disk or a\n1058         remote source into memory and return this array.\n1059 \n1060         Normally, it should not be necessary to call this method in user code,\n1061         because all xarray functions should either work on deferred data or\n1062         load data automatically. However, this method can be necessary when\n1063         working with many file objects on disk.\n1064 \n1065         Parameters\n1066         ----------\n1067         **kwargs : dict\n1068             Additional keyword arguments passed on to ``dask.compute``.\n1069 \n1070         See Also\n1071         --------\n1072         dask.compute\n1073         \"\"\"\n1074         ds = self._to_temp_dataset().load(**kwargs)\n1075         new = self._from_temp_dataset(ds)\n1076         self._variable = new._variable\n1077         self._coords = new._coords\n1078         return self\n1079 \n1080     def compute(self: T_DataArray, **kwargs) -> T_DataArray:\n1081         \"\"\"Manually trigger loading of this array's data from disk or a\n1082         remote source into memory and return a new array. The original is\n1083         left unaltered.\n1084 \n1085         Normally, it should not be necessary to call this method in user code,\n1086         because all xarray functions should either work on deferred data or\n1087         load data automatically. However, this method can be necessary when\n1088         working with many file objects on disk.\n1089 \n1090         Parameters\n1091         ----------\n1092         **kwargs : dict\n1093             Additional keyword arguments passed on to ``dask.compute``.\n1094 \n1095         See Also\n1096         --------\n1097         dask.compute\n1098         \"\"\"\n1099         new = self.copy(deep=False)\n1100         return new.load(**kwargs)\n1101 \n1102     def persist(self: T_DataArray, **kwargs) -> T_DataArray:\n1103         \"\"\"Trigger computation in constituent dask arrays\n1104 \n1105         This keeps them as dask arrays but encourages them to keep data in\n1106         memory.  This is particularly useful when on a distributed machine.\n1107         When on a single machine consider using ``.compute()`` instead.\n1108 \n1109         Parameters\n1110         ----------\n1111         **kwargs : dict\n1112             Additional keyword arguments passed on to ``dask.persist``.\n1113 \n1114         See Also\n1115         --------\n1116         dask.persist\n1117         \"\"\"\n1118         ds = self._to_temp_dataset().persist(**kwargs)\n1119         return self._from_temp_dataset(ds)\n1120 \n1121     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:\n1122         \"\"\"Returns a copy of this array.\n1123 \n1124         If `deep=True`, a deep copy is made of the data array.\n1125         Otherwise, a shallow copy is made, and the returned data array's\n1126         values are a new view of this data array's values.\n1127 \n1128         Use `data` to create a new object with the same structure as\n1129         original but entirely new data.\n1130 \n1131         Parameters\n1132         ----------\n1133         deep : bool, optional\n1134             Whether the data array and its coordinates are loaded into memory\n1135             and copied onto the new object. Default is True.\n1136         data : array_like, optional\n1137             Data to use in the new object. Must have same shape as original.\n1138             When `data` is used, `deep` is ignored for all data variables,\n1139             and only used for coords.\n1140 \n1141         Returns\n1142         -------\n1143         copy : DataArray\n1144             New object with dimensions, attributes, coordinates, name,\n1145             encoding, and optionally data copied from original.\n1146 \n1147         Examples\n1148         --------\n1149         Shallow versus deep copy\n1150 \n1151         >>> array = xr.DataArray([1, 2, 3], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\"]})\n1152         >>> array.copy()\n1153         <xarray.DataArray (x: 3)>\n1154         array([1, 2, 3])\n1155         Coordinates:\n1156           * x        (x) <U1 'a' 'b' 'c'\n1157         >>> array_0 = array.copy(deep=False)\n1158         >>> array_0[0] = 7\n1159         >>> array_0\n1160         <xarray.DataArray (x: 3)>\n1161         array([7, 2, 3])\n1162         Coordinates:\n1163           * x        (x) <U1 'a' 'b' 'c'\n1164         >>> array\n1165         <xarray.DataArray (x: 3)>\n1166         array([7, 2, 3])\n1167         Coordinates:\n1168           * x        (x) <U1 'a' 'b' 'c'\n1169 \n1170         Changing the data using the ``data`` argument maintains the\n1171         structure of the original object, but with the new data. Original\n1172         object is unaffected.\n1173 \n1174         >>> array.copy(data=[0.1, 0.2, 0.3])\n1175         <xarray.DataArray (x: 3)>\n1176         array([0.1, 0.2, 0.3])\n1177         Coordinates:\n1178           * x        (x) <U1 'a' 'b' 'c'\n1179         >>> array\n1180         <xarray.DataArray (x: 3)>\n1181         array([7, 2, 3])\n1182         Coordinates:\n1183           * x        (x) <U1 'a' 'b' 'c'\n1184 \n1185         See Also\n1186         --------\n1187         pandas.DataFrame.copy\n1188         \"\"\"\n1189         return self._copy(deep=deep, data=data)\n1190 \n1191     def _copy(\n1192         self: T_DataArray,\n1193         deep: bool = True,\n1194         data: Any = None,\n1195         memo: dict[int, Any] | None = None,\n1196     ) -> T_DataArray:\n1197         variable = self.variable._copy(deep=deep, data=data, memo=memo)\n1198         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n1199 \n1200         coords = {}\n1201         for k, v in self._coords.items():\n1202             if k in index_vars:\n1203                 coords[k] = index_vars[k]\n1204             else:\n1205                 coords[k] = v._copy(deep=deep, memo=memo)\n1206 \n1207         return self._replace(variable, coords, indexes=indexes)\n1208 \n1209     def __copy__(self: T_DataArray) -> T_DataArray:\n1210         return self._copy(deep=False)\n1211 \n1212     def __deepcopy__(\n1213         self: T_DataArray, memo: dict[int, Any] | None = None\n1214     ) -> T_DataArray:\n1215         return self._copy(deep=True, memo=memo)\n1216 \n1217     # mutable objects should not be Hashable\n1218     # https://github.com/python/mypy/issues/4266\n1219     __hash__ = None  # type: ignore[assignment]\n1220 \n1221     @property\n1222     def chunks(self) -> tuple[tuple[int, ...], ...] | None:\n1223         \"\"\"\n1224         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if\n1225         the underlying data is not a dask array.\n1226 \n1227         See Also\n1228         --------\n1229         DataArray.chunk\n1230         DataArray.chunksizes\n1231         xarray.unify_chunks\n1232         \"\"\"\n1233         return self.variable.chunks\n1234 \n1235     @property\n1236     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:\n1237         \"\"\"\n1238         Mapping from dimension names to block lengths for this dataarray's data, or None if\n1239         the underlying data is not a dask array.\n1240         Cannot be modified directly, but can be modified by calling .chunk().\n1241 \n1242         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes\n1243         instead of a tuple of chunk shapes.\n1244 \n1245         See Also\n1246         --------\n1247         DataArray.chunk\n1248         DataArray.chunks\n1249         xarray.unify_chunks\n1250         \"\"\"\n1251         all_variables = [self.variable] + [c.variable for c in self.coords.values()]\n1252         return get_chunksizes(all_variables)\n1253 \n1254     def chunk(\n1255         self: T_DataArray,\n1256         chunks: (\n1257             int\n1258             | Literal[\"auto\"]\n1259             | tuple[int, ...]\n1260             | tuple[tuple[int, ...], ...]\n1261             | Mapping[Any, None | int | tuple[int, ...]]\n1262         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n1263         name_prefix: str = \"xarray-\",\n1264         token: str | None = None,\n1265         lock: bool = False,\n1266         inline_array: bool = False,\n1267         **chunks_kwargs: Any,\n1268     ) -> T_DataArray:\n1269         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n1270 \n1271         If this variable is a non-dask array, it will be converted to dask\n1272         array. If it's a dask array, it will be rechunked to the given chunk\n1273         sizes.\n1274 \n1275         If neither chunks is not provided for one or more dimensions, chunk\n1276         sizes along that dimension will not be updated; non-dask arrays will be\n1277         converted into dask arrays with a single block.\n1278 \n1279         Parameters\n1280         ----------\n1281         chunks : int, \"auto\", tuple of int or mapping of Hashable to int, optional\n1282             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, ``(5, 5)`` or\n1283             ``{\"x\": 5, \"y\": 5}``.\n1284         name_prefix : str, optional\n1285             Prefix for the name of the new dask array.\n1286         token : str, optional\n1287             Token uniquely identifying this array.\n1288         lock : optional\n1289             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1290             already as dask array.\n1291         inline_array: optional\n1292             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1293             already as dask array.\n1294         **chunks_kwargs : {dim: chunks, ...}, optional\n1295             The keyword arguments form of ``chunks``.\n1296             One of chunks or chunks_kwargs must be provided.\n1297 \n1298         Returns\n1299         -------\n1300         chunked : xarray.DataArray\n1301 \n1302         See Also\n1303         --------\n1304         DataArray.chunks\n1305         DataArray.chunksizes\n1306         xarray.unify_chunks\n1307         dask.array.from_array\n1308         \"\"\"\n1309         if chunks is None:\n1310             warnings.warn(\n1311                 \"None value for 'chunks' is deprecated. \"\n1312                 \"It will raise an error in the future. Use instead '{}'\",\n1313                 category=FutureWarning,\n1314             )\n1315             chunks = {}\n1316 \n1317         if isinstance(chunks, (float, str, int)):\n1318             # ignoring type; unclear why it won't accept a Literal into the value.\n1319             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore\n1320         elif isinstance(chunks, (tuple, list)):\n1321             chunks = dict(zip(self.dims, chunks))\n1322         else:\n1323             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n1324 \n1325         ds = self._to_temp_dataset().chunk(\n1326             chunks,\n1327             name_prefix=name_prefix,\n1328             token=token,\n1329             lock=lock,\n1330             inline_array=inline_array,\n1331         )\n1332         return self._from_temp_dataset(ds)\n1333 \n1334     def isel(\n1335         self: T_DataArray,\n1336         indexers: Mapping[Any, Any] | None = None,\n1337         drop: bool = False,\n1338         missing_dims: ErrorOptionsWithWarn = \"raise\",\n1339         **indexers_kwargs: Any,\n1340     ) -> T_DataArray:\n1341         \"\"\"Return a new DataArray whose data is given by selecting indexes\n1342         along the specified dimension(s).\n1343 \n1344         Parameters\n1345         ----------\n1346         indexers : dict, optional\n1347             A dict with keys matching dimensions and values given\n1348             by integers, slice objects or arrays.\n1349             indexer can be a integer, slice, array-like or DataArray.\n1350             If DataArrays are passed as indexers, xarray-style indexing will be\n1351             carried out. See :ref:`indexing` for the details.\n1352             One of indexers or indexers_kwargs must be provided.\n1353         drop : bool, default: False\n1354             If ``drop=True``, drop coordinates variables indexed by integers\n1355             instead of making them scalar.\n1356         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n1357             What to do if dimensions that should be selected from are not present in the\n1358             DataArray:\n1359             - \"raise\": raise an exception\n1360             - \"warn\": raise a warning, and ignore the missing dimensions\n1361             - \"ignore\": ignore the missing dimensions\n1362         **indexers_kwargs : {dim: indexer, ...}, optional\n1363             The keyword arguments form of ``indexers``.\n1364 \n1365         Returns\n1366         -------\n1367         indexed : xarray.DataArray\n1368 \n1369         See Also\n1370         --------\n1371         Dataset.isel\n1372         DataArray.sel\n1373 \n1374         Examples\n1375         --------\n1376         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=(\"x\", \"y\"))\n1377         >>> da\n1378         <xarray.DataArray (x: 5, y: 5)>\n1379         array([[ 0,  1,  2,  3,  4],\n1380                [ 5,  6,  7,  8,  9],\n1381                [10, 11, 12, 13, 14],\n1382                [15, 16, 17, 18, 19],\n1383                [20, 21, 22, 23, 24]])\n1384         Dimensions without coordinates: x, y\n1385 \n1386         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1387         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims=\"points\")\n1388         >>> da = da.isel(x=tgt_x, y=tgt_y)\n1389         >>> da\n1390         <xarray.DataArray (points: 5)>\n1391         array([ 0,  6, 12, 18, 24])\n1392         Dimensions without coordinates: points\n1393         \"\"\"\n1394 \n1395         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1396 \n1397         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n1398             ds = self._to_temp_dataset()._isel_fancy(\n1399                 indexers, drop=drop, missing_dims=missing_dims\n1400             )\n1401             return self._from_temp_dataset(ds)\n1402 \n1403         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n1404         # lists, or zero or one-dimensional np.ndarray's\n1405 \n1406         variable = self._variable.isel(indexers, missing_dims=missing_dims)\n1407         indexes, index_variables = isel_indexes(self.xindexes, indexers)\n1408 \n1409         coords = {}\n1410         for coord_name, coord_value in self._coords.items():\n1411             if coord_name in index_variables:\n1412                 coord_value = index_variables[coord_name]\n1413             else:\n1414                 coord_indexers = {\n1415                     k: v for k, v in indexers.items() if k in coord_value.dims\n1416                 }\n1417                 if coord_indexers:\n1418                     coord_value = coord_value.isel(coord_indexers)\n1419                     if drop and coord_value.ndim == 0:\n1420                         continue\n1421             coords[coord_name] = coord_value\n1422 \n1423         return self._replace(variable=variable, coords=coords, indexes=indexes)\n1424 \n1425     def sel(\n1426         self: T_DataArray,\n1427         indexers: Mapping[Any, Any] | None = None,\n1428         method: str | None = None,\n1429         tolerance=None,\n1430         drop: bool = False,\n1431         **indexers_kwargs: Any,\n1432     ) -> T_DataArray:\n1433         \"\"\"Return a new DataArray whose data is given by selecting index\n1434         labels along the specified dimension(s).\n1435 \n1436         In contrast to `DataArray.isel`, indexers for this method should use\n1437         labels instead of integers.\n1438 \n1439         Under the hood, this method is powered by using pandas's powerful Index\n1440         objects. This makes label based indexing essentially just as fast as\n1441         using integer indexing.\n1442 \n1443         It also means this method uses pandas's (well documented) logic for\n1444         indexing. This means you can use string shortcuts for datetime indexes\n1445         (e.g., '2000-01' to select all values in January 2000). It also means\n1446         that slices are treated as inclusive of both the start and stop values,\n1447         unlike normal Python indexing.\n1448 \n1449         .. warning::\n1450 \n1451           Do not try to assign values when using any of the indexing methods\n1452           ``isel`` or ``sel``::\n1453 \n1454             da = xr.DataArray([0, 1, 2, 3], dims=['x'])\n1455             # DO NOT do this\n1456             da.isel(x=[0, 1, 2])[1] = -1\n1457 \n1458           Assigning values with the chained indexing using ``.sel`` or\n1459           ``.isel`` fails silently.\n1460 \n1461         Parameters\n1462         ----------\n1463         indexers : dict, optional\n1464             A dict with keys matching dimensions and values given\n1465             by scalars, slices or arrays of tick labels. For dimensions with\n1466             multi-index, the indexer may also be a dict-like object with keys\n1467             matching index level names.\n1468             If DataArrays are passed as indexers, xarray-style indexing will be\n1469             carried out. See :ref:`indexing` for the details.\n1470             One of indexers or indexers_kwargs must be provided.\n1471         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1472             Method to use for inexact matches:\n1473 \n1474             - None (default): only exact matches\n1475             - pad / ffill: propagate last valid index value forward\n1476             - backfill / bfill: propagate next valid index value backward\n1477             - nearest: use nearest valid index value\n1478 \n1479         tolerance : optional\n1480             Maximum distance between original and new labels for inexact\n1481             matches. The values of the index at the matching locations must\n1482             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1483         drop : bool, optional\n1484             If ``drop=True``, drop coordinates variables in `indexers` instead\n1485             of making them scalar.\n1486         **indexers_kwargs : {dim: indexer, ...}, optional\n1487             The keyword arguments form of ``indexers``.\n1488             One of indexers or indexers_kwargs must be provided.\n1489 \n1490         Returns\n1491         -------\n1492         obj : DataArray\n1493             A new DataArray with the same contents as this DataArray, except the\n1494             data and each dimension is indexed by the appropriate indexers.\n1495             If indexer DataArrays have coordinates that do not conflict with\n1496             this object, then these coordinates will be attached.\n1497             In general, each array's data will be a view of the array's data\n1498             in this DataArray, unless vectorized indexing was triggered by using\n1499             an array indexer, in which case the data will be a copy.\n1500 \n1501         See Also\n1502         --------\n1503         Dataset.sel\n1504         DataArray.isel\n1505 \n1506         Examples\n1507         --------\n1508         >>> da = xr.DataArray(\n1509         ...     np.arange(25).reshape(5, 5),\n1510         ...     coords={\"x\": np.arange(5), \"y\": np.arange(5)},\n1511         ...     dims=(\"x\", \"y\"),\n1512         ... )\n1513         >>> da\n1514         <xarray.DataArray (x: 5, y: 5)>\n1515         array([[ 0,  1,  2,  3,  4],\n1516                [ 5,  6,  7,  8,  9],\n1517                [10, 11, 12, 13, 14],\n1518                [15, 16, 17, 18, 19],\n1519                [20, 21, 22, 23, 24]])\n1520         Coordinates:\n1521           * x        (x) int64 0 1 2 3 4\n1522           * y        (y) int64 0 1 2 3 4\n1523 \n1524         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1525         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims=\"points\")\n1526         >>> da = da.sel(x=tgt_x, y=tgt_y, method=\"nearest\")\n1527         >>> da\n1528         <xarray.DataArray (points: 5)>\n1529         array([ 0,  6, 12, 18, 24])\n1530         Coordinates:\n1531             x        (points) int64 0 1 2 3 4\n1532             y        (points) int64 0 1 2 3 4\n1533         Dimensions without coordinates: points\n1534         \"\"\"\n1535         ds = self._to_temp_dataset().sel(\n1536             indexers=indexers,\n1537             drop=drop,\n1538             method=method,\n1539             tolerance=tolerance,\n1540             **indexers_kwargs,\n1541         )\n1542         return self._from_temp_dataset(ds)\n1543 \n1544     def head(\n1545         self: T_DataArray,\n1546         indexers: Mapping[Any, int] | int | None = None,\n1547         **indexers_kwargs: Any,\n1548     ) -> T_DataArray:\n1549         \"\"\"Return a new DataArray whose data is given by the the first `n`\n1550         values along the specified dimension(s). Default `n` = 5\n1551 \n1552         See Also\n1553         --------\n1554         Dataset.head\n1555         DataArray.tail\n1556         DataArray.thin\n1557 \n1558         Examples\n1559         --------\n1560         >>> da = xr.DataArray(\n1561         ...     np.arange(25).reshape(5, 5),\n1562         ...     dims=(\"x\", \"y\"),\n1563         ... )\n1564         >>> da\n1565         <xarray.DataArray (x: 5, y: 5)>\n1566         array([[ 0,  1,  2,  3,  4],\n1567                [ 5,  6,  7,  8,  9],\n1568                [10, 11, 12, 13, 14],\n1569                [15, 16, 17, 18, 19],\n1570                [20, 21, 22, 23, 24]])\n1571         Dimensions without coordinates: x, y\n1572 \n1573         >>> da.head(x=1)\n1574         <xarray.DataArray (x: 1, y: 5)>\n1575         array([[0, 1, 2, 3, 4]])\n1576         Dimensions without coordinates: x, y\n1577 \n1578         >>> da.head({\"x\": 2, \"y\": 2})\n1579         <xarray.DataArray (x: 2, y: 2)>\n1580         array([[0, 1],\n1581                [5, 6]])\n1582         Dimensions without coordinates: x, y\n1583         \"\"\"\n1584         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)\n1585         return self._from_temp_dataset(ds)\n1586 \n1587     def tail(\n1588         self: T_DataArray,\n1589         indexers: Mapping[Any, int] | int | None = None,\n1590         **indexers_kwargs: Any,\n1591     ) -> T_DataArray:\n1592         \"\"\"Return a new DataArray whose data is given by the the last `n`\n1593         values along the specified dimension(s). Default `n` = 5\n1594 \n1595         See Also\n1596         --------\n1597         Dataset.tail\n1598         DataArray.head\n1599         DataArray.thin\n1600 \n1601         Examples\n1602         --------\n1603         >>> da = xr.DataArray(\n1604         ...     np.arange(25).reshape(5, 5),\n1605         ...     dims=(\"x\", \"y\"),\n1606         ... )\n1607         >>> da\n1608         <xarray.DataArray (x: 5, y: 5)>\n1609         array([[ 0,  1,  2,  3,  4],\n1610                [ 5,  6,  7,  8,  9],\n1611                [10, 11, 12, 13, 14],\n1612                [15, 16, 17, 18, 19],\n1613                [20, 21, 22, 23, 24]])\n1614         Dimensions without coordinates: x, y\n1615 \n1616         >>> da.tail(y=1)\n1617         <xarray.DataArray (x: 5, y: 1)>\n1618         array([[ 4],\n1619                [ 9],\n1620                [14],\n1621                [19],\n1622                [24]])\n1623         Dimensions without coordinates: x, y\n1624 \n1625         >>> da.tail({\"x\": 2, \"y\": 2})\n1626         <xarray.DataArray (x: 2, y: 2)>\n1627         array([[18, 19],\n1628                [23, 24]])\n1629         Dimensions without coordinates: x, y\n1630         \"\"\"\n1631         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)\n1632         return self._from_temp_dataset(ds)\n1633 \n1634     def thin(\n1635         self: T_DataArray,\n1636         indexers: Mapping[Any, int] | int | None = None,\n1637         **indexers_kwargs: Any,\n1638     ) -> T_DataArray:\n1639         \"\"\"Return a new DataArray whose data is given by each `n` value\n1640         along the specified dimension(s).\n1641 \n1642         Examples\n1643         --------\n1644         >>> x_arr = np.arange(0, 26)\n1645         >>> x_arr\n1646         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n1647                17, 18, 19, 20, 21, 22, 23, 24, 25])\n1648         >>> x = xr.DataArray(\n1649         ...     np.reshape(x_arr, (2, 13)),\n1650         ...     dims=(\"x\", \"y\"),\n1651         ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n1652         ... )\n1653         >>> x\n1654         <xarray.DataArray (x: 2, y: 13)>\n1655         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],\n1656                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])\n1657         Coordinates:\n1658           * x        (x) int64 0 1\n1659           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n1660 \n1661         >>>\n1662         >>> x.thin(3)\n1663         <xarray.DataArray (x: 1, y: 5)>\n1664         array([[ 0,  3,  6,  9, 12]])\n1665         Coordinates:\n1666           * x        (x) int64 0\n1667           * y        (y) int64 0 3 6 9 12\n1668         >>> x.thin({\"x\": 2, \"y\": 5})\n1669         <xarray.DataArray (x: 1, y: 3)>\n1670         array([[ 0,  5, 10]])\n1671         Coordinates:\n1672           * x        (x) int64 0\n1673           * y        (y) int64 0 5 10\n1674 \n1675         See Also\n1676         --------\n1677         Dataset.thin\n1678         DataArray.head\n1679         DataArray.tail\n1680         \"\"\"\n1681         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)\n1682         return self._from_temp_dataset(ds)\n1683 \n1684     def broadcast_like(\n1685         self: T_DataArray,\n1686         other: DataArray | Dataset,\n1687         exclude: Iterable[Hashable] | None = None,\n1688     ) -> T_DataArray:\n1689         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n1690 \n1691         This is equivalent to xr.broadcast(other, self)[1]\n1692 \n1693         xarray objects are broadcast against each other in arithmetic\n1694         operations, so this method is not be necessary for most uses.\n1695 \n1696         If no change is needed, the input data is returned to the output\n1697         without being copied.\n1698 \n1699         If new coords are added by the broadcast, their values are\n1700         NaN filled.\n1701 \n1702         Parameters\n1703         ----------\n1704         other : Dataset or DataArray\n1705             Object against which to broadcast this array.\n1706         exclude : iterable of Hashable, optional\n1707             Dimensions that must not be broadcasted\n1708 \n1709         Returns\n1710         -------\n1711         new_da : DataArray\n1712             The caller broadcasted against ``other``.\n1713 \n1714         Examples\n1715         --------\n1716         >>> arr1 = xr.DataArray(\n1717         ...     np.random.randn(2, 3),\n1718         ...     dims=(\"x\", \"y\"),\n1719         ...     coords={\"x\": [\"a\", \"b\"], \"y\": [\"a\", \"b\", \"c\"]},\n1720         ... )\n1721         >>> arr2 = xr.DataArray(\n1722         ...     np.random.randn(3, 2),\n1723         ...     dims=(\"x\", \"y\"),\n1724         ...     coords={\"x\": [\"a\", \"b\", \"c\"], \"y\": [\"a\", \"b\"]},\n1725         ... )\n1726         >>> arr1\n1727         <xarray.DataArray (x: 2, y: 3)>\n1728         array([[ 1.76405235,  0.40015721,  0.97873798],\n1729                [ 2.2408932 ,  1.86755799, -0.97727788]])\n1730         Coordinates:\n1731           * x        (x) <U1 'a' 'b'\n1732           * y        (y) <U1 'a' 'b' 'c'\n1733         >>> arr2\n1734         <xarray.DataArray (x: 3, y: 2)>\n1735         array([[ 0.95008842, -0.15135721],\n1736                [-0.10321885,  0.4105985 ],\n1737                [ 0.14404357,  1.45427351]])\n1738         Coordinates:\n1739           * x        (x) <U1 'a' 'b' 'c'\n1740           * y        (y) <U1 'a' 'b'\n1741         >>> arr1.broadcast_like(arr2)\n1742         <xarray.DataArray (x: 3, y: 3)>\n1743         array([[ 1.76405235,  0.40015721,  0.97873798],\n1744                [ 2.2408932 ,  1.86755799, -0.97727788],\n1745                [        nan,         nan,         nan]])\n1746         Coordinates:\n1747           * x        (x) <U1 'a' 'b' 'c'\n1748           * y        (y) <U1 'a' 'b' 'c'\n1749         \"\"\"\n1750         if exclude is None:\n1751             exclude = set()\n1752         else:\n1753             exclude = set(exclude)\n1754         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n1755 \n1756         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n1757 \n1758         return _broadcast_helper(\n1759             cast(\"T_DataArray\", args[1]), exclude, dims_map, common_coords\n1760         )\n1761 \n1762     def _reindex_callback(\n1763         self: T_DataArray,\n1764         aligner: alignment.Aligner,\n1765         dim_pos_indexers: dict[Hashable, Any],\n1766         variables: dict[Hashable, Variable],\n1767         indexes: dict[Hashable, Index],\n1768         fill_value: Any,\n1769         exclude_dims: frozenset[Hashable],\n1770         exclude_vars: frozenset[Hashable],\n1771     ) -> T_DataArray:\n1772         \"\"\"Callback called from ``Aligner`` to create a new reindexed DataArray.\"\"\"\n1773 \n1774         if isinstance(fill_value, dict):\n1775             fill_value = fill_value.copy()\n1776             sentinel = object()\n1777             value = fill_value.pop(self.name, sentinel)\n1778             if value is not sentinel:\n1779                 fill_value[_THIS_ARRAY] = value\n1780 \n1781         ds = self._to_temp_dataset()\n1782         reindexed = ds._reindex_callback(\n1783             aligner,\n1784             dim_pos_indexers,\n1785             variables,\n1786             indexes,\n1787             fill_value,\n1788             exclude_dims,\n1789             exclude_vars,\n1790         )\n1791         return self._from_temp_dataset(reindexed)\n1792 \n1793     def reindex_like(\n1794         self: T_DataArray,\n1795         other: DataArray | Dataset,\n1796         method: ReindexMethodOptions = None,\n1797         tolerance: int | float | Iterable[int | float] | None = None,\n1798         copy: bool = True,\n1799         fill_value=dtypes.NA,\n1800     ) -> T_DataArray:\n1801         \"\"\"Conform this object onto the indexes of another object, filling in\n1802         missing values with ``fill_value``. The default fill value is NaN.\n1803 \n1804         Parameters\n1805         ----------\n1806         other : Dataset or DataArray\n1807             Object with an 'indexes' attribute giving a mapping from dimension\n1808             names to pandas.Index objects, which provides coordinates upon\n1809             which to index the variables in this dataset. The indexes on this\n1810             other object need not be the same as the indexes on this\n1811             dataset. Any mis-matched index values will be filled in with\n1812             NaN, and any mis-matched dimension names will simply be ignored.\n1813         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n1814             Method to use for filling index values from other not found on this\n1815             data array:\n1816 \n1817             - None (default): don't fill gaps\n1818             - pad / ffill: propagate last valid index value forward\n1819             - backfill / bfill: propagate next valid index value backward\n1820             - nearest: use nearest valid index value\n1821 \n1822         tolerance : optional\n1823             Maximum distance between original and new labels for inexact\n1824             matches. The values of the index at the matching locations must\n1825             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n1826             Tolerance may be a scalar value, which applies the same tolerance\n1827             to all values, or list-like, which applies variable tolerance per\n1828             element. List-like must be the same size as the index and its dtype\n1829             must exactly match the index\u2019s type.\n1830         copy : bool, default: True\n1831             If ``copy=True``, data in the return value is always copied. If\n1832             ``copy=False`` and reindexing is unnecessary, or can be performed\n1833             with only slice operations, then the output may share memory with\n1834             the input. In either case, a new xarray object is always returned.\n1835         fill_value : scalar or dict-like, optional\n1836             Value to use for newly missing values. If a dict-like, maps\n1837             variable names (including coordinates) to fill values. Use this\n1838             data array's name to refer to the data array's values.\n1839 \n1840         Returns\n1841         -------\n1842         reindexed : DataArray\n1843             Another dataset array, with this array's data but coordinates from\n1844             the other object.\n1845 \n1846         Examples\n1847         --------\n1848         >>> data = np.arange(12).reshape(4, 3)\n1849         >>> da1 = xr.DataArray(\n1850         ...     data=data,\n1851         ...     dims=[\"x\", \"y\"],\n1852         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n1853         ... )\n1854         >>> da1\n1855         <xarray.DataArray (x: 4, y: 3)>\n1856         array([[ 0,  1,  2],\n1857                [ 3,  4,  5],\n1858                [ 6,  7,  8],\n1859                [ 9, 10, 11]])\n1860         Coordinates:\n1861           * x        (x) int64 10 20 30 40\n1862           * y        (y) int64 70 80 90\n1863         >>> da2 = xr.DataArray(\n1864         ...     data=data,\n1865         ...     dims=[\"x\", \"y\"],\n1866         ...     coords={\"x\": [40, 30, 20, 10], \"y\": [90, 80, 70]},\n1867         ... )\n1868         >>> da2\n1869         <xarray.DataArray (x: 4, y: 3)>\n1870         array([[ 0,  1,  2],\n1871                [ 3,  4,  5],\n1872                [ 6,  7,  8],\n1873                [ 9, 10, 11]])\n1874         Coordinates:\n1875           * x        (x) int64 40 30 20 10\n1876           * y        (y) int64 90 80 70\n1877 \n1878         Reindexing with both DataArrays having the same coordinates set, but in different order:\n1879 \n1880         >>> da1.reindex_like(da2)\n1881         <xarray.DataArray (x: 4, y: 3)>\n1882         array([[11, 10,  9],\n1883                [ 8,  7,  6],\n1884                [ 5,  4,  3],\n1885                [ 2,  1,  0]])\n1886         Coordinates:\n1887           * x        (x) int64 40 30 20 10\n1888           * y        (y) int64 90 80 70\n1889 \n1890         Reindexing with the other array having coordinates which the source array doesn't have:\n1891 \n1892         >>> data = np.arange(12).reshape(4, 3)\n1893         >>> da1 = xr.DataArray(\n1894         ...     data=data,\n1895         ...     dims=[\"x\", \"y\"],\n1896         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n1897         ... )\n1898         >>> da2 = xr.DataArray(\n1899         ...     data=data,\n1900         ...     dims=[\"x\", \"y\"],\n1901         ...     coords={\"x\": [20, 10, 29, 39], \"y\": [70, 80, 90]},\n1902         ... )\n1903         >>> da1.reindex_like(da2)\n1904         <xarray.DataArray (x: 4, y: 3)>\n1905         array([[ 3.,  4.,  5.],\n1906                [ 0.,  1.,  2.],\n1907                [nan, nan, nan],\n1908                [nan, nan, nan]])\n1909         Coordinates:\n1910           * x        (x) int64 20 10 29 39\n1911           * y        (y) int64 70 80 90\n1912 \n1913         Filling missing values with the previous valid index with respect to the coordinates' value:\n1914 \n1915         >>> da1.reindex_like(da2, method=\"ffill\")\n1916         <xarray.DataArray (x: 4, y: 3)>\n1917         array([[3, 4, 5],\n1918                [0, 1, 2],\n1919                [3, 4, 5],\n1920                [6, 7, 8]])\n1921         Coordinates:\n1922           * x        (x) int64 20 10 29 39\n1923           * y        (y) int64 70 80 90\n1924 \n1925         Filling missing values while tolerating specified error for inexact matches:\n1926 \n1927         >>> da1.reindex_like(da2, method=\"ffill\", tolerance=5)\n1928         <xarray.DataArray (x: 4, y: 3)>\n1929         array([[ 3.,  4.,  5.],\n1930                [ 0.,  1.,  2.],\n1931                [nan, nan, nan],\n1932                [nan, nan, nan]])\n1933         Coordinates:\n1934           * x        (x) int64 20 10 29 39\n1935           * y        (y) int64 70 80 90\n1936 \n1937         Filling missing values with manually specified values:\n1938 \n1939         >>> da1.reindex_like(da2, fill_value=19)\n1940         <xarray.DataArray (x: 4, y: 3)>\n1941         array([[ 3,  4,  5],\n1942                [ 0,  1,  2],\n1943                [19, 19, 19],\n1944                [19, 19, 19]])\n1945         Coordinates:\n1946           * x        (x) int64 20 10 29 39\n1947           * y        (y) int64 70 80 90\n1948 \n1949         See Also\n1950         --------\n1951         DataArray.reindex\n1952         align\n1953         \"\"\"\n1954         return alignment.reindex_like(\n1955             self,\n1956             other=other,\n1957             method=method,\n1958             tolerance=tolerance,\n1959             copy=copy,\n1960             fill_value=fill_value,\n1961         )\n1962 \n1963     def reindex(\n1964         self: T_DataArray,\n1965         indexers: Mapping[Any, Any] | None = None,\n1966         method: ReindexMethodOptions = None,\n1967         tolerance: float | Iterable[float] | None = None,\n1968         copy: bool = True,\n1969         fill_value=dtypes.NA,\n1970         **indexers_kwargs: Any,\n1971     ) -> T_DataArray:\n1972         \"\"\"Conform this object onto the indexes of another object, filling in\n1973         missing values with ``fill_value``. The default fill value is NaN.\n1974 \n1975         Parameters\n1976         ----------\n1977         indexers : dict, optional\n1978             Dictionary with keys given by dimension names and values given by\n1979             arrays of coordinates tick labels. Any mis-matched coordinate\n1980             values will be filled in with NaN, and any mis-matched dimension\n1981             names will simply be ignored.\n1982             One of indexers or indexers_kwargs must be provided.\n1983         copy : bool, optional\n1984             If ``copy=True``, data in the return value is always copied. If\n1985             ``copy=False`` and reindexing is unnecessary, or can be performed\n1986             with only slice operations, then the output may share memory with\n1987             the input. In either case, a new xarray object is always returned.\n1988         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional\n1989             Method to use for filling index values in ``indexers`` not found on\n1990             this data array:\n1991 \n1992             - None (default): don't fill gaps\n1993             - pad / ffill: propagate last valid index value forward\n1994             - backfill / bfill: propagate next valid index value backward\n1995             - nearest: use nearest valid index value\n1996 \n1997         tolerance : float | Iterable[float] | None, default: None\n1998             Maximum distance between original and new labels for inexact\n1999             matches. The values of the index at the matching locations must\n2000             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2001             Tolerance may be a scalar value, which applies the same tolerance\n2002             to all values, or list-like, which applies variable tolerance per\n2003             element. List-like must be the same size as the index and its dtype\n2004             must exactly match the index\u2019s type.\n2005         fill_value : scalar or dict-like, optional\n2006             Value to use for newly missing values. If a dict-like, maps\n2007             variable names (including coordinates) to fill values. Use this\n2008             data array's name to refer to the data array's values.\n2009         **indexers_kwargs : {dim: indexer, ...}, optional\n2010             The keyword arguments form of ``indexers``.\n2011             One of indexers or indexers_kwargs must be provided.\n2012 \n2013         Returns\n2014         -------\n2015         reindexed : DataArray\n2016             Another dataset array, with this array's data but replaced\n2017             coordinates.\n2018 \n2019         Examples\n2020         --------\n2021         Reverse latitude:\n2022 \n2023         >>> da = xr.DataArray(\n2024         ...     np.arange(4),\n2025         ...     coords=[np.array([90, 89, 88, 87])],\n2026         ...     dims=\"lat\",\n2027         ... )\n2028         >>> da\n2029         <xarray.DataArray (lat: 4)>\n2030         array([0, 1, 2, 3])\n2031         Coordinates:\n2032           * lat      (lat) int64 90 89 88 87\n2033         >>> da.reindex(lat=da.lat[::-1])\n2034         <xarray.DataArray (lat: 4)>\n2035         array([3, 2, 1, 0])\n2036         Coordinates:\n2037           * lat      (lat) int64 87 88 89 90\n2038 \n2039         See Also\n2040         --------\n2041         DataArray.reindex_like\n2042         align\n2043         \"\"\"\n2044         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n2045         return alignment.reindex(\n2046             self,\n2047             indexers=indexers,\n2048             method=method,\n2049             tolerance=tolerance,\n2050             copy=copy,\n2051             fill_value=fill_value,\n2052         )\n2053 \n2054     def interp(\n2055         self: T_DataArray,\n2056         coords: Mapping[Any, Any] | None = None,\n2057         method: InterpOptions = \"linear\",\n2058         assume_sorted: bool = False,\n2059         kwargs: Mapping[str, Any] | None = None,\n2060         **coords_kwargs: Any,\n2061     ) -> T_DataArray:\n2062         \"\"\"Interpolate a DataArray onto new coordinates\n2063 \n2064         Performs univariate or multivariate interpolation of a DataArray onto\n2065         new coordinates using scipy's interpolation routines. If interpolating\n2066         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n2067         called. When interpolating along multiple existing dimensions, an\n2068         attempt is made to decompose the interpolation into multiple\n2069         1-dimensional interpolations. If this is possible,\n2070         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n2071         :py:func:`scipy.interpolate.interpn` is called.\n2072 \n2073         Parameters\n2074         ----------\n2075         coords : dict, optional\n2076             Mapping from dimension names to the new coordinates.\n2077             New coordinate can be a scalar, array-like or DataArray.\n2078             If DataArrays are passed as new coordinates, their dimensions are\n2079             used for the broadcasting. Missing values are skipped.\n2080         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n2081             The method used to interpolate. The method should be supported by\n2082             the scipy interpolator:\n2083 \n2084             - ``interp1d``: {\"linear\", \"nearest\", \"zero\", \"slinear\",\n2085               \"quadratic\", \"cubic\", \"polynomial\"}\n2086             - ``interpn``: {\"linear\", \"nearest\"}\n2087 \n2088             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n2089             also be provided.\n2090         assume_sorted : bool, default: False\n2091             If False, values of x can be in any order and they are sorted\n2092             first. If True, x has to be an array of monotonically increasing\n2093             values.\n2094         kwargs : dict-like or None, default: None\n2095             Additional keyword arguments passed to scipy's interpolator. Valid\n2096             options and their behavior depend whether ``interp1d`` or\n2097             ``interpn`` is used.\n2098         **coords_kwargs : {dim: coordinate, ...}, optional\n2099             The keyword arguments form of ``coords``.\n2100             One of coords or coords_kwargs must be provided.\n2101 \n2102         Returns\n2103         -------\n2104         interpolated : DataArray\n2105             New dataarray on the new coordinates.\n2106 \n2107         Notes\n2108         -----\n2109         scipy is required.\n2110 \n2111         See Also\n2112         --------\n2113         scipy.interpolate.interp1d\n2114         scipy.interpolate.interpn\n2115 \n2116         Examples\n2117         --------\n2118         >>> da = xr.DataArray(\n2119         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n2120         ...     dims=(\"x\", \"y\"),\n2121         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n2122         ... )\n2123         >>> da\n2124         <xarray.DataArray (x: 3, y: 4)>\n2125         array([[ 1.,  4.,  2.,  9.],\n2126                [ 2.,  7.,  6., nan],\n2127                [ 6., nan,  5.,  8.]])\n2128         Coordinates:\n2129           * x        (x) int64 0 1 2\n2130           * y        (y) int64 10 12 14 16\n2131 \n2132         1D linear interpolation (the default):\n2133 \n2134         >>> da.interp(x=[0, 0.75, 1.25, 1.75])\n2135         <xarray.DataArray (x: 4, y: 4)>\n2136         array([[1.  , 4.  , 2.  ,  nan],\n2137                [1.75, 6.25, 5.  ,  nan],\n2138                [3.  ,  nan, 5.75,  nan],\n2139                [5.  ,  nan, 5.25,  nan]])\n2140         Coordinates:\n2141           * y        (y) int64 10 12 14 16\n2142           * x        (x) float64 0.0 0.75 1.25 1.75\n2143 \n2144         1D nearest interpolation:\n2145 \n2146         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n2147         <xarray.DataArray (x: 4, y: 4)>\n2148         array([[ 1.,  4.,  2.,  9.],\n2149                [ 2.,  7.,  6., nan],\n2150                [ 2.,  7.,  6., nan],\n2151                [ 6., nan,  5.,  8.]])\n2152         Coordinates:\n2153           * y        (y) int64 10 12 14 16\n2154           * x        (x) float64 0.0 0.75 1.25 1.75\n2155 \n2156         1D linear extrapolation:\n2157 \n2158         >>> da.interp(\n2159         ...     x=[1, 1.5, 2.5, 3.5],\n2160         ...     method=\"linear\",\n2161         ...     kwargs={\"fill_value\": \"extrapolate\"},\n2162         ... )\n2163         <xarray.DataArray (x: 4, y: 4)>\n2164         array([[ 2. ,  7. ,  6. ,  nan],\n2165                [ 4. ,  nan,  5.5,  nan],\n2166                [ 8. ,  nan,  4.5,  nan],\n2167                [12. ,  nan,  3.5,  nan]])\n2168         Coordinates:\n2169           * y        (y) int64 10 12 14 16\n2170           * x        (x) float64 1.0 1.5 2.5 3.5\n2171 \n2172         2D linear interpolation:\n2173 \n2174         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n2175         <xarray.DataArray (x: 4, y: 3)>\n2176         array([[2.5  , 3.   ,   nan],\n2177                [4.   , 5.625,   nan],\n2178                [  nan,   nan,   nan],\n2179                [  nan,   nan,   nan]])\n2180         Coordinates:\n2181           * x        (x) float64 0.0 0.75 1.25 1.75\n2182           * y        (y) int64 11 13 15\n2183         \"\"\"\n2184         if self.dtype.kind not in \"uifc\":\n2185             raise TypeError(\n2186                 \"interp only works for a numeric type array. \"\n2187                 \"Given {}.\".format(self.dtype)\n2188             )\n2189         ds = self._to_temp_dataset().interp(\n2190             coords,\n2191             method=method,\n2192             kwargs=kwargs,\n2193             assume_sorted=assume_sorted,\n2194             **coords_kwargs,\n2195         )\n2196         return self._from_temp_dataset(ds)\n2197 \n2198     def interp_like(\n2199         self: T_DataArray,\n2200         other: DataArray | Dataset,\n2201         method: InterpOptions = \"linear\",\n2202         assume_sorted: bool = False,\n2203         kwargs: Mapping[str, Any] | None = None,\n2204     ) -> T_DataArray:\n2205         \"\"\"Interpolate this object onto the coordinates of another object,\n2206         filling out of range values with NaN.\n2207 \n2208         If interpolating along a single existing dimension,\n2209         :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n2210         along multiple existing dimensions, an attempt is made to decompose the\n2211         interpolation into multiple 1-dimensional interpolations. If this is\n2212         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n2213         :py:func:`scipy.interpolate.interpn` is called.\n2214 \n2215         Parameters\n2216         ----------\n2217         other : Dataset or DataArray\n2218             Object with an 'indexes' attribute giving a mapping from dimension\n2219             names to an 1d array-like, which provides coordinates upon\n2220             which to index the variables in this dataset. Missing values are skipped.\n2221         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\"}, default: \"linear\"\n2222             The method used to interpolate. The method should be supported by\n2223             the scipy interpolator:\n2224 \n2225             - {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\",\n2226               \"polynomial\"} when ``interp1d`` is called.\n2227             - {\"linear\", \"nearest\"} when ``interpn`` is called.\n2228 \n2229             If ``\"polynomial\"`` is passed, the ``order`` keyword argument must\n2230             also be provided.\n2231         assume_sorted : bool, default: False\n2232             If False, values of coordinates that are interpolated over can be\n2233             in any order and they are sorted first. If True, interpolated\n2234             coordinates are assumed to be an array of monotonically increasing\n2235             values.\n2236         kwargs : dict, optional\n2237             Additional keyword passed to scipy's interpolator.\n2238 \n2239         Returns\n2240         -------\n2241         interpolated : DataArray\n2242             Another dataarray by interpolating this dataarray's data along the\n2243             coordinates of the other object.\n2244 \n2245         Examples\n2246         --------\n2247         >>> data = np.arange(12).reshape(4, 3)\n2248         >>> da1 = xr.DataArray(\n2249         ...     data=data,\n2250         ...     dims=[\"x\", \"y\"],\n2251         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n2252         ... )\n2253         >>> da1\n2254         <xarray.DataArray (x: 4, y: 3)>\n2255         array([[ 0,  1,  2],\n2256                [ 3,  4,  5],\n2257                [ 6,  7,  8],\n2258                [ 9, 10, 11]])\n2259         Coordinates:\n2260           * x        (x) int64 10 20 30 40\n2261           * y        (y) int64 70 80 90\n2262         >>> da2 = xr.DataArray(\n2263         ...     data=data,\n2264         ...     dims=[\"x\", \"y\"],\n2265         ...     coords={\"x\": [10, 20, 29, 39], \"y\": [70, 80, 90]},\n2266         ... )\n2267         >>> da2\n2268         <xarray.DataArray (x: 4, y: 3)>\n2269         array([[ 0,  1,  2],\n2270                [ 3,  4,  5],\n2271                [ 6,  7,  8],\n2272                [ 9, 10, 11]])\n2273         Coordinates:\n2274           * x        (x) int64 10 20 29 39\n2275           * y        (y) int64 70 80 90\n2276 \n2277         Interpolate the values in the coordinates of the other DataArray with respect to the source's values:\n2278 \n2279         >>> da2.interp_like(da1)\n2280         <xarray.DataArray (x: 4, y: 3)>\n2281         array([[0. , 1. , 2. ],\n2282                [3. , 4. , 5. ],\n2283                [6.3, 7.3, 8.3],\n2284                [nan, nan, nan]])\n2285         Coordinates:\n2286           * x        (x) int64 10 20 30 40\n2287           * y        (y) int64 70 80 90\n2288 \n2289         Could also extrapolate missing values:\n2290 \n2291         >>> da2.interp_like(da1, kwargs={\"fill_value\": \"extrapolate\"})\n2292         <xarray.DataArray (x: 4, y: 3)>\n2293         array([[ 0. ,  1. ,  2. ],\n2294                [ 3. ,  4. ,  5. ],\n2295                [ 6.3,  7.3,  8.3],\n2296                [ 9.3, 10.3, 11.3]])\n2297         Coordinates:\n2298           * x        (x) int64 10 20 30 40\n2299           * y        (y) int64 70 80 90\n2300 \n2301         Notes\n2302         -----\n2303         scipy is required.\n2304         If the dataarray has object-type coordinates, reindex is used for these\n2305         coordinates instead of the interpolation.\n2306 \n2307         See Also\n2308         --------\n2309         DataArray.interp\n2310         DataArray.reindex_like\n2311         \"\"\"\n2312         if self.dtype.kind not in \"uifc\":\n2313             raise TypeError(\n2314                 \"interp only works for a numeric type array. \"\n2315                 \"Given {}.\".format(self.dtype)\n2316             )\n2317         ds = self._to_temp_dataset().interp_like(\n2318             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted\n2319         )\n2320         return self._from_temp_dataset(ds)\n2321 \n2322     # change type of self and return to T_DataArray once\n2323     # https://github.com/python/mypy/issues/12846 is resolved\n2324     def rename(\n2325         self,\n2326         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,\n2327         **names: Hashable,\n2328     ) -> DataArray:\n2329         \"\"\"Returns a new DataArray with renamed coordinates, dimensions or a new name.\n2330 \n2331         Parameters\n2332         ----------\n2333         new_name_or_name_dict : str or dict-like, optional\n2334             If the argument is dict-like, it used as a mapping from old\n2335             names to new names for coordinates or dimensions. Otherwise,\n2336             use the argument as the new name for this array.\n2337         **names : Hashable, optional\n2338             The keyword arguments form of a mapping from old names to\n2339             new names for coordinates or dimensions.\n2340             One of new_name_or_name_dict or names must be provided.\n2341 \n2342         Returns\n2343         -------\n2344         renamed : DataArray\n2345             Renamed array or array with renamed coordinates.\n2346 \n2347         See Also\n2348         --------\n2349         Dataset.rename\n2350         DataArray.swap_dims\n2351         \"\"\"\n2352         if new_name_or_name_dict is None and not names:\n2353             # change name to None?\n2354             return self._replace(name=None)\n2355         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:\n2356             # change dims/coords\n2357             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, \"rename\")\n2358             dataset = self._to_temp_dataset()._rename(name_dict)\n2359             return self._from_temp_dataset(dataset)\n2360         if utils.hashable(new_name_or_name_dict) and names:\n2361             # change name + dims/coords\n2362             dataset = self._to_temp_dataset()._rename(names)\n2363             dataarray = self._from_temp_dataset(dataset)\n2364             return dataarray._replace(name=new_name_or_name_dict)\n2365         # only change name\n2366         return self._replace(name=new_name_or_name_dict)\n2367 \n2368     def swap_dims(\n2369         self: T_DataArray,\n2370         dims_dict: Mapping[Any, Hashable] | None = None,\n2371         **dims_kwargs,\n2372     ) -> T_DataArray:\n2373         \"\"\"Returns a new DataArray with swapped dimensions.\n2374 \n2375         Parameters\n2376         ----------\n2377         dims_dict : dict-like\n2378             Dictionary whose keys are current dimension names and whose values\n2379             are new names.\n2380         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n2381             The keyword arguments form of ``dims_dict``.\n2382             One of dims_dict or dims_kwargs must be provided.\n2383 \n2384         Returns\n2385         -------\n2386         swapped : DataArray\n2387             DataArray with swapped dimensions.\n2388 \n2389         Examples\n2390         --------\n2391         >>> arr = xr.DataArray(\n2392         ...     data=[0, 1],\n2393         ...     dims=\"x\",\n2394         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n2395         ... )\n2396         >>> arr\n2397         <xarray.DataArray (x: 2)>\n2398         array([0, 1])\n2399         Coordinates:\n2400           * x        (x) <U1 'a' 'b'\n2401             y        (x) int64 0 1\n2402 \n2403         >>> arr.swap_dims({\"x\": \"y\"})\n2404         <xarray.DataArray (y: 2)>\n2405         array([0, 1])\n2406         Coordinates:\n2407             x        (y) <U1 'a' 'b'\n2408           * y        (y) int64 0 1\n2409 \n2410         >>> arr.swap_dims({\"x\": \"z\"})\n2411         <xarray.DataArray (z: 2)>\n2412         array([0, 1])\n2413         Coordinates:\n2414             x        (z) <U1 'a' 'b'\n2415             y        (z) int64 0 1\n2416         Dimensions without coordinates: z\n2417 \n2418         See Also\n2419         --------\n2420         DataArray.rename\n2421         Dataset.swap_dims\n2422         \"\"\"\n2423         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n2424         ds = self._to_temp_dataset().swap_dims(dims_dict)\n2425         return self._from_temp_dataset(ds)\n2426 \n2427     # change type of self and return to T_DataArray once\n2428     # https://github.com/python/mypy/issues/12846 is resolved\n2429     def expand_dims(\n2430         self,\n2431         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,\n2432         axis: None | int | Sequence[int] = None,\n2433         **dim_kwargs: Any,\n2434     ) -> DataArray:\n2435         \"\"\"Return a new object with an additional axis (or axes) inserted at\n2436         the corresponding position in the array shape. The new object is a\n2437         view into the underlying array, not a copy.\n2438 \n2439         If dim is already a scalar coordinate, it will be promoted to a 1D\n2440         coordinate consisting of a single value.\n2441 \n2442         Parameters\n2443         ----------\n2444         dim : Hashable, sequence of Hashable, dict, or None, optional\n2445             Dimensions to include on the new variable.\n2446             If provided as str or sequence of str, then dimensions are inserted\n2447             with length 1. If provided as a dict, then the keys are the new\n2448             dimensions and the values are either integers (giving the length of\n2449             the new dimensions) or sequence/ndarray (giving the coordinates of\n2450             the new dimensions).\n2451         axis : int, sequence of int, or None, default: None\n2452             Axis position(s) where new axis is to be inserted (position(s) on\n2453             the result array). If a sequence of integers is passed,\n2454             multiple axes are inserted. In this case, dim arguments should be\n2455             same length list. If axis=None is passed, all the axes will be\n2456             inserted to the start of the result array.\n2457         **dim_kwargs : int or sequence or ndarray\n2458             The keywords are arbitrary dimensions being inserted and the values\n2459             are either the lengths of the new dims (if int is given), or their\n2460             coordinates. Note, this is an alternative to passing a dict to the\n2461             dim kwarg and will only be used if dim is None.\n2462 \n2463         Returns\n2464         -------\n2465         expanded : DataArray\n2466             This object, but with additional dimension(s).\n2467 \n2468         See Also\n2469         --------\n2470         Dataset.expand_dims\n2471 \n2472         Examples\n2473         --------\n2474         >>> da = xr.DataArray(np.arange(5), dims=(\"x\"))\n2475         >>> da\n2476         <xarray.DataArray (x: 5)>\n2477         array([0, 1, 2, 3, 4])\n2478         Dimensions without coordinates: x\n2479 \n2480         Add new dimension of length 2:\n2481 \n2482         >>> da.expand_dims(dim={\"y\": 2})\n2483         <xarray.DataArray (y: 2, x: 5)>\n2484         array([[0, 1, 2, 3, 4],\n2485                [0, 1, 2, 3, 4]])\n2486         Dimensions without coordinates: y, x\n2487 \n2488         >>> da.expand_dims(dim={\"y\": 2}, axis=1)\n2489         <xarray.DataArray (x: 5, y: 2)>\n2490         array([[0, 0],\n2491                [1, 1],\n2492                [2, 2],\n2493                [3, 3],\n2494                [4, 4]])\n2495         Dimensions without coordinates: x, y\n2496 \n2497         Add a new dimension with coordinates from array:\n2498 \n2499         >>> da.expand_dims(dim={\"y\": np.arange(5)}, axis=0)\n2500         <xarray.DataArray (y: 5, x: 5)>\n2501         array([[0, 1, 2, 3, 4],\n2502                [0, 1, 2, 3, 4],\n2503                [0, 1, 2, 3, 4],\n2504                [0, 1, 2, 3, 4],\n2505                [0, 1, 2, 3, 4]])\n2506         Coordinates:\n2507           * y        (y) int64 0 1 2 3 4\n2508         Dimensions without coordinates: x\n2509         \"\"\"\n2510         if isinstance(dim, int):\n2511             raise TypeError(\"dim should be Hashable or sequence/mapping of Hashables\")\n2512         elif isinstance(dim, Sequence) and not isinstance(dim, str):\n2513             if len(dim) != len(set(dim)):\n2514                 raise ValueError(\"dims should not contain duplicate values.\")\n2515             dim = dict.fromkeys(dim, 1)\n2516         elif dim is not None and not isinstance(dim, Mapping):\n2517             dim = {cast(Hashable, dim): 1}\n2518 \n2519         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n2520         ds = self._to_temp_dataset().expand_dims(dim, axis)\n2521         return self._from_temp_dataset(ds)\n2522 \n2523     # change type of self and return to T_DataArray once\n2524     # https://github.com/python/mypy/issues/12846 is resolved\n2525     def set_index(\n2526         self,\n2527         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,\n2528         append: bool = False,\n2529         **indexes_kwargs: Hashable | Sequence[Hashable],\n2530     ) -> DataArray:\n2531         \"\"\"Set DataArray (multi-)indexes using one or more existing\n2532         coordinates.\n2533 \n2534         This legacy method is limited to pandas (multi-)indexes and\n2535         1-dimensional \"dimension\" coordinates. See\n2536         :py:meth:`~DataArray.set_xindex` for setting a pandas or a custom\n2537         Xarray-compatible index from one or more arbitrary coordinates.\n2538 \n2539         Parameters\n2540         ----------\n2541         indexes : {dim: index, ...}\n2542             Mapping from names matching dimensions and values given\n2543             by (lists of) the names of existing coordinates or variables to set\n2544             as new (multi-)index.\n2545         append : bool, default: False\n2546             If True, append the supplied index(es) to the existing index(es).\n2547             Otherwise replace the existing index(es).\n2548         **indexes_kwargs : optional\n2549             The keyword arguments form of ``indexes``.\n2550             One of indexes or indexes_kwargs must be provided.\n2551 \n2552         Returns\n2553         -------\n2554         obj : DataArray\n2555             Another DataArray, with this data but replaced coordinates.\n2556 \n2557         Examples\n2558         --------\n2559         >>> arr = xr.DataArray(\n2560         ...     data=np.ones((2, 3)),\n2561         ...     dims=[\"x\", \"y\"],\n2562         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n2563         ... )\n2564         >>> arr\n2565         <xarray.DataArray (x: 2, y: 3)>\n2566         array([[1., 1., 1.],\n2567                [1., 1., 1.]])\n2568         Coordinates:\n2569           * x        (x) int64 0 1\n2570           * y        (y) int64 0 1 2\n2571             a        (x) int64 3 4\n2572         >>> arr.set_index(x=\"a\")\n2573         <xarray.DataArray (x: 2, y: 3)>\n2574         array([[1., 1., 1.],\n2575                [1., 1., 1.]])\n2576         Coordinates:\n2577           * x        (x) int64 3 4\n2578           * y        (y) int64 0 1 2\n2579 \n2580         See Also\n2581         --------\n2582         DataArray.reset_index\n2583         DataArray.set_xindex\n2584         \"\"\"\n2585         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)\n2586         return self._from_temp_dataset(ds)\n2587 \n2588     # change type of self and return to T_DataArray once\n2589     # https://github.com/python/mypy/issues/12846 is resolved\n2590     def reset_index(\n2591         self,\n2592         dims_or_levels: Hashable | Sequence[Hashable],\n2593         drop: bool = False,\n2594     ) -> DataArray:\n2595         \"\"\"Reset the specified index(es) or multi-index level(s).\n2596 \n2597         This legacy method is specific to pandas (multi-)indexes and\n2598         1-dimensional \"dimension\" coordinates. See the more generic\n2599         :py:meth:`~DataArray.drop_indexes` and :py:meth:`~DataArray.set_xindex`\n2600         method to respectively drop and set pandas or custom indexes for\n2601         arbitrary coordinates.\n2602 \n2603         Parameters\n2604         ----------\n2605         dims_or_levels : Hashable or sequence of Hashable\n2606             Name(s) of the dimension(s) and/or multi-index level(s) that will\n2607             be reset.\n2608         drop : bool, default: False\n2609             If True, remove the specified indexes and/or multi-index levels\n2610             instead of extracting them as new coordinates (default: False).\n2611 \n2612         Returns\n2613         -------\n2614         obj : DataArray\n2615             Another dataarray, with this dataarray's data but replaced\n2616             coordinates.\n2617 \n2618         See Also\n2619         --------\n2620         DataArray.set_index\n2621         DataArray.set_xindex\n2622         DataArray.drop_indexes\n2623         \"\"\"\n2624         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)\n2625         return self._from_temp_dataset(ds)\n2626 \n2627     def set_xindex(\n2628         self: T_DataArray,\n2629         coord_names: str | Sequence[Hashable],\n2630         index_cls: type[Index] | None = None,\n2631         **options,\n2632     ) -> T_DataArray:\n2633         \"\"\"Set a new, Xarray-compatible index from one or more existing\n2634         coordinate(s).\n2635 \n2636         Parameters\n2637         ----------\n2638         coord_names : str or list\n2639             Name(s) of the coordinate(s) used to build the index.\n2640             If several names are given, their order matters.\n2641         index_cls : subclass of :class:`~xarray.indexes.Index`\n2642             The type of index to create. By default, try setting\n2643             a pandas (multi-)index from the supplied coordinates.\n2644         **options\n2645             Options passed to the index constructor.\n2646 \n2647         Returns\n2648         -------\n2649         obj : DataArray\n2650             Another dataarray, with this dataarray's data and with a new index.\n2651 \n2652         \"\"\"\n2653         ds = self._to_temp_dataset().set_xindex(coord_names, index_cls, **options)\n2654         return self._from_temp_dataset(ds)\n2655 \n2656     def reorder_levels(\n2657         self: T_DataArray,\n2658         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,\n2659         **dim_order_kwargs: Sequence[int | Hashable],\n2660     ) -> T_DataArray:\n2661         \"\"\"Rearrange index levels using input order.\n2662 \n2663         Parameters\n2664         ----------\n2665         dim_order dict-like of Hashable to int or Hashable: optional\n2666             Mapping from names matching dimensions and values given\n2667             by lists representing new level orders. Every given dimension\n2668             must have a multi-index.\n2669         **dim_order_kwargs : optional\n2670             The keyword arguments form of ``dim_order``.\n2671             One of dim_order or dim_order_kwargs must be provided.\n2672 \n2673         Returns\n2674         -------\n2675         obj : DataArray\n2676             Another dataarray, with this dataarray's data but replaced\n2677             coordinates.\n2678         \"\"\"\n2679         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)\n2680         return self._from_temp_dataset(ds)\n2681 \n2682     def stack(\n2683         self: T_DataArray,\n2684         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,\n2685         create_index: bool | None = True,\n2686         index_cls: type[Index] = PandasMultiIndex,\n2687         **dimensions_kwargs: Sequence[Hashable],\n2688     ) -> T_DataArray:\n2689         \"\"\"\n2690         Stack any number of existing dimensions into a single new dimension.\n2691 \n2692         New dimensions will be added at the end, and the corresponding\n2693         coordinate variables will be combined into a MultiIndex.\n2694 \n2695         Parameters\n2696         ----------\n2697         dimensions : mapping of Hashable to sequence of Hashable\n2698             Mapping of the form `new_name=(dim1, dim2, ...)`.\n2699             Names of new dimensions, and the existing dimensions that they\n2700             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.\n2701             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n2702             all dimensions.\n2703         create_index : bool or None, default: True\n2704             If True, create a multi-index for each of the stacked dimensions.\n2705             If False, don't create any index.\n2706             If None, create a multi-index only if exactly one single (1-d) coordinate\n2707             index is found for every dimension to stack.\n2708         index_cls: class, optional\n2709             Can be used to pass a custom multi-index type. Must be an Xarray index that\n2710             implements `.stack()`. By default, a pandas multi-index wrapper is used.\n2711         **dimensions_kwargs\n2712             The keyword arguments form of ``dimensions``.\n2713             One of dimensions or dimensions_kwargs must be provided.\n2714 \n2715         Returns\n2716         -------\n2717         stacked : DataArray\n2718             DataArray with stacked data.\n2719 \n2720         Examples\n2721         --------\n2722         >>> arr = xr.DataArray(\n2723         ...     np.arange(6).reshape(2, 3),\n2724         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2725         ... )\n2726         >>> arr\n2727         <xarray.DataArray (x: 2, y: 3)>\n2728         array([[0, 1, 2],\n2729                [3, 4, 5]])\n2730         Coordinates:\n2731           * x        (x) <U1 'a' 'b'\n2732           * y        (y) int64 0 1 2\n2733         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2734         >>> stacked.indexes[\"z\"]\n2735         MultiIndex([('a', 0),\n2736                     ('a', 1),\n2737                     ('a', 2),\n2738                     ('b', 0),\n2739                     ('b', 1),\n2740                     ('b', 2)],\n2741                    name='z')\n2742 \n2743         See Also\n2744         --------\n2745         DataArray.unstack\n2746         \"\"\"\n2747         ds = self._to_temp_dataset().stack(\n2748             dimensions,\n2749             create_index=create_index,\n2750             index_cls=index_cls,\n2751             **dimensions_kwargs,\n2752         )\n2753         return self._from_temp_dataset(ds)\n2754 \n2755     # change type of self and return to T_DataArray once\n2756     # https://github.com/python/mypy/issues/12846 is resolved\n2757     def unstack(\n2758         self,\n2759         dim: Dims = None,\n2760         fill_value: Any = dtypes.NA,\n2761         sparse: bool = False,\n2762     ) -> DataArray:\n2763         \"\"\"\n2764         Unstack existing dimensions corresponding to MultiIndexes into\n2765         multiple new dimensions.\n2766 \n2767         New dimensions will be added at the end.\n2768 \n2769         Parameters\n2770         ----------\n2771         dim : str, Iterable of Hashable or None, optional\n2772             Dimension(s) over which to unstack. By default unstacks all\n2773             MultiIndexes.\n2774         fill_value : scalar or dict-like, default: nan\n2775             Value to be filled. If a dict-like, maps variable names to\n2776             fill values. Use the data array's name to refer to its\n2777             name. If not provided or if the dict-like does not contain\n2778             all variables, the dtype's NA value will be used.\n2779         sparse : bool, default: False\n2780             Use sparse-array if True\n2781 \n2782         Returns\n2783         -------\n2784         unstacked : DataArray\n2785             Array with unstacked data.\n2786 \n2787         Examples\n2788         --------\n2789         >>> arr = xr.DataArray(\n2790         ...     np.arange(6).reshape(2, 3),\n2791         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2792         ... )\n2793         >>> arr\n2794         <xarray.DataArray (x: 2, y: 3)>\n2795         array([[0, 1, 2],\n2796                [3, 4, 5]])\n2797         Coordinates:\n2798           * x        (x) <U1 'a' 'b'\n2799           * y        (y) int64 0 1 2\n2800         >>> stacked = arr.stack(z=(\"x\", \"y\"))\n2801         >>> stacked.indexes[\"z\"]\n2802         MultiIndex([('a', 0),\n2803                     ('a', 1),\n2804                     ('a', 2),\n2805                     ('b', 0),\n2806                     ('b', 1),\n2807                     ('b', 2)],\n2808                    name='z')\n2809         >>> roundtripped = stacked.unstack()\n2810         >>> arr.identical(roundtripped)\n2811         True\n2812 \n2813         See Also\n2814         --------\n2815         DataArray.stack\n2816         \"\"\"\n2817         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)\n2818         return self._from_temp_dataset(ds)\n2819 \n2820     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:\n2821         \"\"\"Unstack DataArray expanding to Dataset along a given level of a\n2822         stacked coordinate.\n2823 \n2824         This is the inverse operation of Dataset.to_stacked_array.\n2825 \n2826         Parameters\n2827         ----------\n2828         dim : Hashable\n2829             Name of existing dimension to unstack\n2830         level : int or Hashable, default: 0\n2831             The MultiIndex level to expand to a dataset along. Can either be\n2832             the integer index of the level or its name.\n2833 \n2834         Returns\n2835         -------\n2836         unstacked: Dataset\n2837 \n2838         Examples\n2839         --------\n2840         >>> arr = xr.DataArray(\n2841         ...     np.arange(6).reshape(2, 3),\n2842         ...     coords=[(\"x\", [\"a\", \"b\"]), (\"y\", [0, 1, 2])],\n2843         ... )\n2844         >>> data = xr.Dataset({\"a\": arr, \"b\": arr.isel(y=0)})\n2845         >>> data\n2846         <xarray.Dataset>\n2847         Dimensions:  (x: 2, y: 3)\n2848         Coordinates:\n2849           * x        (x) <U1 'a' 'b'\n2850           * y        (y) int64 0 1 2\n2851         Data variables:\n2852             a        (x, y) int64 0 1 2 3 4 5\n2853             b        (x) int64 0 3\n2854         >>> stacked = data.to_stacked_array(\"z\", [\"x\"])\n2855         >>> stacked.indexes[\"z\"]\n2856         MultiIndex([('a', 0.0),\n2857                     ('a', 1.0),\n2858                     ('a', 2.0),\n2859                     ('b', nan)],\n2860                    name='z')\n2861         >>> roundtripped = stacked.to_unstacked_dataset(dim=\"z\")\n2862         >>> data.identical(roundtripped)\n2863         True\n2864 \n2865         See Also\n2866         --------\n2867         Dataset.to_stacked_array\n2868         \"\"\"\n2869         idx = self._indexes[dim].to_pandas_index()\n2870         if not isinstance(idx, pd.MultiIndex):\n2871             raise ValueError(f\"'{dim}' is not a stacked coordinate\")\n2872 \n2873         level_number = idx._get_level_number(level)\n2874         variables = idx.levels[level_number]\n2875         variable_dim = idx.names[level_number]\n2876 \n2877         # pull variables out of datarray\n2878         data_dict = {}\n2879         for k in variables:\n2880             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)\n2881 \n2882         # unstacked dataset\n2883         return Dataset(data_dict)\n2884 \n2885     def transpose(\n2886         self: T_DataArray,\n2887         *dims: Hashable,\n2888         transpose_coords: bool = True,\n2889         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2890     ) -> T_DataArray:\n2891         \"\"\"Return a new DataArray object with transposed dimensions.\n2892 \n2893         Parameters\n2894         ----------\n2895         *dims : Hashable, optional\n2896             By default, reverse the dimensions. Otherwise, reorder the\n2897             dimensions to this order.\n2898         transpose_coords : bool, default: True\n2899             If True, also transpose the coordinates of this DataArray.\n2900         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2901             What to do if dimensions that should be selected from are not present in the\n2902             DataArray:\n2903             - \"raise\": raise an exception\n2904             - \"warn\": raise a warning, and ignore the missing dimensions\n2905             - \"ignore\": ignore the missing dimensions\n2906 \n2907         Returns\n2908         -------\n2909         transposed : DataArray\n2910             The returned DataArray's array is transposed.\n2911 \n2912         Notes\n2913         -----\n2914         This operation returns a view of this array's data. It is\n2915         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n2916         -- the data will be fully loaded.\n2917 \n2918         See Also\n2919         --------\n2920         numpy.transpose\n2921         Dataset.transpose\n2922         \"\"\"\n2923         if dims:\n2924             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))\n2925         variable = self.variable.transpose(*dims)\n2926         if transpose_coords:\n2927             coords: dict[Hashable, Variable] = {}\n2928             for name, coord in self.coords.items():\n2929                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)\n2930                 coords[name] = coord.variable.transpose(*coord_dims)\n2931             return self._replace(variable, coords)\n2932         else:\n2933             return self._replace(variable)\n2934 \n2935     @property\n2936     def T(self: T_DataArray) -> T_DataArray:\n2937         return self.transpose()\n2938 \n2939     # change type of self and return to T_DataArray once\n2940     # https://github.com/python/mypy/issues/12846 is resolved\n2941     def drop_vars(\n2942         self,\n2943         names: Hashable | Iterable[Hashable],\n2944         *,\n2945         errors: ErrorOptions = \"raise\",\n2946     ) -> DataArray:\n2947         \"\"\"Returns an array with dropped variables.\n2948 \n2949         Parameters\n2950         ----------\n2951         names : Hashable or iterable of Hashable\n2952             Name(s) of variables to drop.\n2953         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n2954             If 'raise', raises a ValueError error if any of the variable\n2955             passed are not in the dataset. If 'ignore', any given names that are in the\n2956             DataArray are dropped and no error is raised.\n2957 \n2958         Returns\n2959         -------\n2960         dropped : Dataset\n2961             New Dataset copied from `self` with variables removed.\n2962 \n2963         Examples\n2964         -------\n2965         >>> data = np.arange(12).reshape(4, 3)\n2966         >>> da = xr.DataArray(\n2967         ...     data=data,\n2968         ...     dims=[\"x\", \"y\"],\n2969         ...     coords={\"x\": [10, 20, 30, 40], \"y\": [70, 80, 90]},\n2970         ... )\n2971         >>> da\n2972         <xarray.DataArray (x: 4, y: 3)>\n2973         array([[ 0,  1,  2],\n2974                [ 3,  4,  5],\n2975                [ 6,  7,  8],\n2976                [ 9, 10, 11]])\n2977         Coordinates:\n2978           * x        (x) int64 10 20 30 40\n2979           * y        (y) int64 70 80 90\n2980 \n2981         Removing a single variable:\n2982 \n2983         >>> da.drop_vars(\"x\")\n2984         <xarray.DataArray (x: 4, y: 3)>\n2985         array([[ 0,  1,  2],\n2986                [ 3,  4,  5],\n2987                [ 6,  7,  8],\n2988                [ 9, 10, 11]])\n2989         Coordinates:\n2990           * y        (y) int64 70 80 90\n2991         Dimensions without coordinates: x\n2992 \n2993         Removing a list of variables:\n2994 \n2995         >>> da.drop_vars([\"x\", \"y\"])\n2996         <xarray.DataArray (x: 4, y: 3)>\n2997         array([[ 0,  1,  2],\n2998                [ 3,  4,  5],\n2999                [ 6,  7,  8],\n3000                [ 9, 10, 11]])\n3001         Dimensions without coordinates: x, y\n3002         \"\"\"\n3003         ds = self._to_temp_dataset().drop_vars(names, errors=errors)\n3004         return self._from_temp_dataset(ds)\n3005 \n3006     def drop_indexes(\n3007         self: T_DataArray,\n3008         coord_names: Hashable | Iterable[Hashable],\n3009         *,\n3010         errors: ErrorOptions = \"raise\",\n3011     ) -> T_DataArray:\n3012         \"\"\"Drop the indexes assigned to the given coordinates.\n3013 \n3014         Parameters\n3015         ----------\n3016         coord_names : hashable or iterable of hashable\n3017             Name(s) of the coordinate(s) for which to drop the index.\n3018         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n3019             If 'raise', raises a ValueError error if any of the coordinates\n3020             passed have no index or are not in the dataset.\n3021             If 'ignore', no error is raised.\n3022 \n3023         Returns\n3024         -------\n3025         dropped : DataArray\n3026             A new dataarray with dropped indexes.\n3027         \"\"\"\n3028         ds = self._to_temp_dataset().drop_indexes(coord_names, errors=errors)\n3029         return self._from_temp_dataset(ds)\n3030 \n3031     def drop(\n3032         self: T_DataArray,\n3033         labels: Mapping[Any, Any] | None = None,\n3034         dim: Hashable | None = None,\n3035         *,\n3036         errors: ErrorOptions = \"raise\",\n3037         **labels_kwargs,\n3038     ) -> T_DataArray:\n3039         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n3040 \n3041         Using either `drop_vars` or `drop_sel` is encouraged\n3042 \n3043         See Also\n3044         --------\n3045         DataArray.drop_vars\n3046         DataArray.drop_sel\n3047         \"\"\"\n3048         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)\n3049         return self._from_temp_dataset(ds)\n3050 \n3051     def drop_sel(\n3052         self: T_DataArray,\n3053         labels: Mapping[Any, Any] | None = None,\n3054         *,\n3055         errors: ErrorOptions = \"raise\",\n3056         **labels_kwargs,\n3057     ) -> T_DataArray:\n3058         \"\"\"Drop index labels from this DataArray.\n3059 \n3060         Parameters\n3061         ----------\n3062         labels : mapping of Hashable to Any\n3063             Index labels to drop\n3064         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n3065             If 'raise', raises a ValueError error if\n3066             any of the index labels passed are not\n3067             in the dataset. If 'ignore', any given labels that are in the\n3068             dataset are dropped and no error is raised.\n3069         **labels_kwargs : {dim: label, ...}, optional\n3070             The keyword arguments form of ``dim`` and ``labels``\n3071 \n3072         Returns\n3073         -------\n3074         dropped : DataArray\n3075 \n3076         Examples\n3077         --------\n3078         >>> da = xr.DataArray(\n3079         ...     np.arange(25).reshape(5, 5),\n3080         ...     coords={\"x\": np.arange(0, 9, 2), \"y\": np.arange(0, 13, 3)},\n3081         ...     dims=(\"x\", \"y\"),\n3082         ... )\n3083         >>> da\n3084         <xarray.DataArray (x: 5, y: 5)>\n3085         array([[ 0,  1,  2,  3,  4],\n3086                [ 5,  6,  7,  8,  9],\n3087                [10, 11, 12, 13, 14],\n3088                [15, 16, 17, 18, 19],\n3089                [20, 21, 22, 23, 24]])\n3090         Coordinates:\n3091           * x        (x) int64 0 2 4 6 8\n3092           * y        (y) int64 0 3 6 9 12\n3093 \n3094         >>> da.drop_sel(x=[0, 2], y=9)\n3095         <xarray.DataArray (x: 3, y: 4)>\n3096         array([[10, 11, 12, 14],\n3097                [15, 16, 17, 19],\n3098                [20, 21, 22, 24]])\n3099         Coordinates:\n3100           * x        (x) int64 4 6 8\n3101           * y        (y) int64 0 3 6 12\n3102 \n3103         >>> da.drop_sel({\"x\": 6, \"y\": [0, 3]})\n3104         <xarray.DataArray (x: 4, y: 3)>\n3105         array([[ 2,  3,  4],\n3106                [ 7,  8,  9],\n3107                [12, 13, 14],\n3108                [22, 23, 24]])\n3109         Coordinates:\n3110           * x        (x) int64 0 2 4 8\n3111           * y        (y) int64 6 9 12\n3112         \"\"\"\n3113         if labels_kwargs or isinstance(labels, dict):\n3114             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n3115 \n3116         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)\n3117         return self._from_temp_dataset(ds)\n3118 \n3119     def drop_isel(\n3120         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs\n3121     ) -> T_DataArray:\n3122         \"\"\"Drop index positions from this DataArray.\n3123 \n3124         Parameters\n3125         ----------\n3126         indexers : mapping of Hashable to Any or None, default: None\n3127             Index locations to drop\n3128         **indexers_kwargs : {dim: position, ...}, optional\n3129             The keyword arguments form of ``dim`` and ``positions``\n3130 \n3131         Returns\n3132         -------\n3133         dropped : DataArray\n3134 \n3135         Raises\n3136         ------\n3137         IndexError\n3138 \n3139         Examples\n3140         --------\n3141         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=(\"X\", \"Y\"))\n3142         >>> da\n3143         <xarray.DataArray (X: 5, Y: 5)>\n3144         array([[ 0,  1,  2,  3,  4],\n3145                [ 5,  6,  7,  8,  9],\n3146                [10, 11, 12, 13, 14],\n3147                [15, 16, 17, 18, 19],\n3148                [20, 21, 22, 23, 24]])\n3149         Dimensions without coordinates: X, Y\n3150 \n3151         >>> da.drop_isel(X=[0, 4], Y=2)\n3152         <xarray.DataArray (X: 3, Y: 4)>\n3153         array([[ 5,  6,  8,  9],\n3154                [10, 11, 13, 14],\n3155                [15, 16, 18, 19]])\n3156         Dimensions without coordinates: X, Y\n3157 \n3158         >>> da.drop_isel({\"X\": 3, \"Y\": 3})\n3159         <xarray.DataArray (X: 4, Y: 4)>\n3160         array([[ 0,  1,  2,  4],\n3161                [ 5,  6,  7,  9],\n3162                [10, 11, 12, 14],\n3163                [20, 21, 22, 24]])\n3164         Dimensions without coordinates: X, Y\n3165         \"\"\"\n3166         dataset = self._to_temp_dataset()\n3167         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)\n3168         return self._from_temp_dataset(dataset)\n3169 \n3170     def dropna(\n3171         self: T_DataArray,\n3172         dim: Hashable,\n3173         how: Literal[\"any\", \"all\"] = \"any\",\n3174         thresh: int | None = None,\n3175     ) -> T_DataArray:\n3176         \"\"\"Returns a new array with dropped labels for missing values along\n3177         the provided dimension.\n3178 \n3179         Parameters\n3180         ----------\n3181         dim : Hashable\n3182             Dimension along which to drop missing values. Dropping along\n3183             multiple dimensions simultaneously is not yet supported.\n3184         how : {\"any\", \"all\"}, default: \"any\"\n3185             - any : if any NA values are present, drop that label\n3186             - all : if all values are NA, drop that label\n3187 \n3188         thresh : int or None, default: None\n3189             If supplied, require this many non-NA values.\n3190 \n3191         Returns\n3192         -------\n3193         dropped : DataArray\n3194 \n3195         Examples\n3196         --------\n3197         >>> temperature = [\n3198         ...     [0, 4, 2, 9],\n3199         ...     [np.nan, np.nan, np.nan, np.nan],\n3200         ...     [np.nan, 4, 2, 0],\n3201         ...     [3, 1, 0, 0],\n3202         ... ]\n3203         >>> da = xr.DataArray(\n3204         ...     data=temperature,\n3205         ...     dims=[\"Y\", \"X\"],\n3206         ...     coords=dict(\n3207         ...         lat=(\"Y\", np.array([-20.0, -20.25, -20.50, -20.75])),\n3208         ...         lon=(\"X\", np.array([10.0, 10.25, 10.5, 10.75])),\n3209         ...     ),\n3210         ... )\n3211         >>> da\n3212         <xarray.DataArray (Y: 4, X: 4)>\n3213         array([[ 0.,  4.,  2.,  9.],\n3214                [nan, nan, nan, nan],\n3215                [nan,  4.,  2.,  0.],\n3216                [ 3.,  1.,  0.,  0.]])\n3217         Coordinates:\n3218             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75\n3219             lon      (X) float64 10.0 10.25 10.5 10.75\n3220         Dimensions without coordinates: Y, X\n3221 \n3222         >>> da.dropna(dim=\"Y\", how=\"any\")\n3223         <xarray.DataArray (Y: 2, X: 4)>\n3224         array([[0., 4., 2., 9.],\n3225                [3., 1., 0., 0.]])\n3226         Coordinates:\n3227             lat      (Y) float64 -20.0 -20.75\n3228             lon      (X) float64 10.0 10.25 10.5 10.75\n3229         Dimensions without coordinates: Y, X\n3230 \n3231         Drop values only if all values along the dimension are NaN:\n3232 \n3233         >>> da.dropna(dim=\"Y\", how=\"all\")\n3234         <xarray.DataArray (Y: 3, X: 4)>\n3235         array([[ 0.,  4.,  2.,  9.],\n3236                [nan,  4.,  2.,  0.],\n3237                [ 3.,  1.,  0.,  0.]])\n3238         Coordinates:\n3239             lat      (Y) float64 -20.0 -20.5 -20.75\n3240             lon      (X) float64 10.0 10.25 10.5 10.75\n3241         Dimensions without coordinates: Y, X\n3242         \"\"\"\n3243         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)\n3244         return self._from_temp_dataset(ds)\n3245 \n3246     def fillna(self: T_DataArray, value: Any) -> T_DataArray:\n3247         \"\"\"Fill missing values in this object.\n3248 \n3249         This operation follows the normal broadcasting and alignment rules that\n3250         xarray uses for binary arithmetic, except the result is aligned to this\n3251         object (``join='left'``) instead of aligned to the intersection of\n3252         index coordinates (``join='inner'``).\n3253 \n3254         Parameters\n3255         ----------\n3256         value : scalar, ndarray or DataArray\n3257             Used to fill all matching missing values in this array. If the\n3258             argument is a DataArray, it is first aligned with (reindexed to)\n3259             this array.\n3260 \n3261         Returns\n3262         -------\n3263         filled : DataArray\n3264 \n3265         Examples\n3266         --------\n3267         >>> da = xr.DataArray(\n3268         ...     np.array([1, 4, np.nan, 0, 3, np.nan]),\n3269         ...     dims=\"Z\",\n3270         ...     coords=dict(\n3271         ...         Z=(\"Z\", np.arange(6)),\n3272         ...         height=(\"Z\", np.array([0, 10, 20, 30, 40, 50])),\n3273         ...     ),\n3274         ... )\n3275         >>> da\n3276         <xarray.DataArray (Z: 6)>\n3277         array([ 1.,  4., nan,  0.,  3., nan])\n3278         Coordinates:\n3279           * Z        (Z) int64 0 1 2 3 4 5\n3280             height   (Z) int64 0 10 20 30 40 50\n3281 \n3282         Fill all NaN values with 0:\n3283 \n3284         >>> da.fillna(0)\n3285         <xarray.DataArray (Z: 6)>\n3286         array([1., 4., 0., 0., 3., 0.])\n3287         Coordinates:\n3288           * Z        (Z) int64 0 1 2 3 4 5\n3289             height   (Z) int64 0 10 20 30 40 50\n3290 \n3291         Fill NaN values with corresponding values in array:\n3292 \n3293         >>> da.fillna(np.array([2, 9, 4, 2, 8, 9]))\n3294         <xarray.DataArray (Z: 6)>\n3295         array([1., 4., 4., 0., 3., 9.])\n3296         Coordinates:\n3297           * Z        (Z) int64 0 1 2 3 4 5\n3298             height   (Z) int64 0 10 20 30 40 50\n3299         \"\"\"\n3300         if utils.is_dict_like(value):\n3301             raise TypeError(\n3302                 \"cannot provide fill value as a dictionary with \"\n3303                 \"fillna on a DataArray\"\n3304             )\n3305         out = ops.fillna(self, value)\n3306         return out\n3307 \n3308     def interpolate_na(\n3309         self: T_DataArray,\n3310         dim: Hashable | None = None,\n3311         method: InterpOptions = \"linear\",\n3312         limit: int | None = None,\n3313         use_coordinate: bool | str = True,\n3314         max_gap: (\n3315             None\n3316             | int\n3317             | float\n3318             | str\n3319             | pd.Timedelta\n3320             | np.timedelta64\n3321             | datetime.timedelta\n3322         ) = None,\n3323         keep_attrs: bool | None = None,\n3324         **kwargs: Any,\n3325     ) -> T_DataArray:\n3326         \"\"\"Fill in NaNs by interpolating according to different methods.\n3327 \n3328         Parameters\n3329         ----------\n3330         dim : Hashable or None, optional\n3331             Specifies the dimension along which to interpolate.\n3332         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3333             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3334             String indicating which method to use for interpolation:\n3335 \n3336             - 'linear': linear interpolation. Additional keyword\n3337               arguments are passed to :py:func:`numpy.interp`\n3338             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3339               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3340               ``method='polynomial'``, the ``order`` keyword argument must also be\n3341               provided.\n3342             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3343               respective :py:class:`scipy.interpolate` classes.\n3344 \n3345         use_coordinate : bool or str, default: True\n3346             Specifies which index to use as the x values in the interpolation\n3347             formulated as `y = f(x)`. If False, values are treated as if\n3348             equally-spaced along ``dim``. If True, the IndexVariable `dim` is\n3349             used. If ``use_coordinate`` is a string, it specifies the name of a\n3350             coordinate variable to use as the index.\n3351         limit : int or None, default: None\n3352             Maximum number of consecutive NaNs to fill. Must be greater than 0\n3353             or None for no limit. This filling is done regardless of the size of\n3354             the gap in the data. To only interpolate over gaps less than a given length,\n3355             see ``max_gap``.\n3356         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n3357             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n3358             Use None for no limit. When interpolating along a datetime64 dimension\n3359             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n3360 \n3361             - a string that is valid input for pandas.to_timedelta\n3362             - a :py:class:`numpy.timedelta64` object\n3363             - a :py:class:`pandas.Timedelta` object\n3364             - a :py:class:`datetime.timedelta` object\n3365 \n3366             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n3367             dimensions has not been implemented yet. Gap length is defined as the difference\n3368             between coordinate values at the first data point after a gap and the last value\n3369             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n3370             between coordinate values at the first (last) valid data point and the first (last) NaN.\n3371             For example, consider::\n3372 \n3373                 <xarray.DataArray (x: 9)>\n3374                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n3375                 Coordinates:\n3376                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n3377 \n3378             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n3379         keep_attrs : bool or None, default: None\n3380             If True, the dataarray's attributes (`attrs`) will be copied from\n3381             the original object to the new one.  If False, the new\n3382             object will be returned without attributes.\n3383         **kwargs : dict, optional\n3384             parameters passed verbatim to the underlying interpolation function\n3385 \n3386         Returns\n3387         -------\n3388         interpolated: DataArray\n3389             Filled in DataArray.\n3390 \n3391         See Also\n3392         --------\n3393         numpy.interp\n3394         scipy.interpolate\n3395 \n3396         Examples\n3397         --------\n3398         >>> da = xr.DataArray(\n3399         ...     [np.nan, 2, 3, np.nan, 0], dims=\"x\", coords={\"x\": [0, 1, 2, 3, 4]}\n3400         ... )\n3401         >>> da\n3402         <xarray.DataArray (x: 5)>\n3403         array([nan,  2.,  3., nan,  0.])\n3404         Coordinates:\n3405           * x        (x) int64 0 1 2 3 4\n3406 \n3407         >>> da.interpolate_na(dim=\"x\", method=\"linear\")\n3408         <xarray.DataArray (x: 5)>\n3409         array([nan, 2. , 3. , 1.5, 0. ])\n3410         Coordinates:\n3411           * x        (x) int64 0 1 2 3 4\n3412 \n3413         >>> da.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n3414         <xarray.DataArray (x: 5)>\n3415         array([1. , 2. , 3. , 1.5, 0. ])\n3416         Coordinates:\n3417           * x        (x) int64 0 1 2 3 4\n3418         \"\"\"\n3419         from xarray.core.missing import interp_na\n3420 \n3421         return interp_na(\n3422             self,\n3423             dim=dim,\n3424             method=method,\n3425             limit=limit,\n3426             use_coordinate=use_coordinate,\n3427             max_gap=max_gap,\n3428             keep_attrs=keep_attrs,\n3429             **kwargs,\n3430         )\n3431 \n3432     def ffill(\n3433         self: T_DataArray, dim: Hashable, limit: int | None = None\n3434     ) -> T_DataArray:\n3435         \"\"\"Fill NaN values by propagating values forward\n3436 \n3437         *Requires bottleneck.*\n3438 \n3439         Parameters\n3440         ----------\n3441         dim : Hashable\n3442             Specifies the dimension along which to propagate values when\n3443             filling.\n3444         limit : int or None, default: None\n3445             The maximum number of consecutive NaN values to forward fill. In\n3446             other words, if there is a gap with more than this number of\n3447             consecutive NaNs, it will only be partially filled. Must be greater\n3448             than 0 or None for no limit. Must be None or greater than or equal\n3449             to axis length if filling along chunked axes (dimensions).\n3450 \n3451         Returns\n3452         -------\n3453         filled : DataArray\n3454 \n3455         Examples\n3456         --------\n3457         >>> temperature = np.array(\n3458         ...     [\n3459         ...         [np.nan, 1, 3],\n3460         ...         [0, np.nan, 5],\n3461         ...         [5, np.nan, np.nan],\n3462         ...         [3, np.nan, np.nan],\n3463         ...         [0, 2, 0],\n3464         ...     ]\n3465         ... )\n3466         >>> da = xr.DataArray(\n3467         ...     data=temperature,\n3468         ...     dims=[\"Y\", \"X\"],\n3469         ...     coords=dict(\n3470         ...         lat=(\"Y\", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),\n3471         ...         lon=(\"X\", np.array([10.0, 10.25, 10.5])),\n3472         ...     ),\n3473         ... )\n3474         >>> da\n3475         <xarray.DataArray (Y: 5, X: 3)>\n3476         array([[nan,  1.,  3.],\n3477                [ 0., nan,  5.],\n3478                [ 5., nan, nan],\n3479                [ 3., nan, nan],\n3480                [ 0.,  2.,  0.]])\n3481         Coordinates:\n3482             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3483             lon      (X) float64 10.0 10.25 10.5\n3484         Dimensions without coordinates: Y, X\n3485 \n3486         Fill all NaN values:\n3487 \n3488         >>> da.ffill(dim=\"Y\", limit=None)\n3489         <xarray.DataArray (Y: 5, X: 3)>\n3490         array([[nan,  1.,  3.],\n3491                [ 0.,  1.,  5.],\n3492                [ 5.,  1.,  5.],\n3493                [ 3.,  1.,  5.],\n3494                [ 0.,  2.,  0.]])\n3495         Coordinates:\n3496             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3497             lon      (X) float64 10.0 10.25 10.5\n3498         Dimensions without coordinates: Y, X\n3499 \n3500         Fill only the first of consecutive NaN values:\n3501 \n3502         >>> da.ffill(dim=\"Y\", limit=1)\n3503         <xarray.DataArray (Y: 5, X: 3)>\n3504         array([[nan,  1.,  3.],\n3505                [ 0.,  1.,  5.],\n3506                [ 5., nan,  5.],\n3507                [ 3., nan, nan],\n3508                [ 0.,  2.,  0.]])\n3509         Coordinates:\n3510             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3511             lon      (X) float64 10.0 10.25 10.5\n3512         Dimensions without coordinates: Y, X\n3513         \"\"\"\n3514         from xarray.core.missing import ffill\n3515 \n3516         return ffill(self, dim, limit=limit)\n3517 \n3518     def bfill(\n3519         self: T_DataArray, dim: Hashable, limit: int | None = None\n3520     ) -> T_DataArray:\n3521         \"\"\"Fill NaN values by propagating values backward\n3522 \n3523         *Requires bottleneck.*\n3524 \n3525         Parameters\n3526         ----------\n3527         dim : str\n3528             Specifies the dimension along which to propagate values when\n3529             filling.\n3530         limit : int or None, default: None\n3531             The maximum number of consecutive NaN values to backward fill. In\n3532             other words, if there is a gap with more than this number of\n3533             consecutive NaNs, it will only be partially filled. Must be greater\n3534             than 0 or None for no limit. Must be None or greater than or equal\n3535             to axis length if filling along chunked axes (dimensions).\n3536 \n3537         Returns\n3538         -------\n3539         filled : DataArray\n3540 \n3541         Examples\n3542         --------\n3543         >>> temperature = np.array(\n3544         ...     [\n3545         ...         [0, 1, 3],\n3546         ...         [0, np.nan, 5],\n3547         ...         [5, np.nan, np.nan],\n3548         ...         [3, np.nan, np.nan],\n3549         ...         [np.nan, 2, 0],\n3550         ...     ]\n3551         ... )\n3552         >>> da = xr.DataArray(\n3553         ...     data=temperature,\n3554         ...     dims=[\"Y\", \"X\"],\n3555         ...     coords=dict(\n3556         ...         lat=(\"Y\", np.array([-20.0, -20.25, -20.50, -20.75, -21.0])),\n3557         ...         lon=(\"X\", np.array([10.0, 10.25, 10.5])),\n3558         ...     ),\n3559         ... )\n3560         >>> da\n3561         <xarray.DataArray (Y: 5, X: 3)>\n3562         array([[ 0.,  1.,  3.],\n3563                [ 0., nan,  5.],\n3564                [ 5., nan, nan],\n3565                [ 3., nan, nan],\n3566                [nan,  2.,  0.]])\n3567         Coordinates:\n3568             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3569             lon      (X) float64 10.0 10.25 10.5\n3570         Dimensions without coordinates: Y, X\n3571 \n3572         Fill all NaN values:\n3573 \n3574         >>> da.bfill(dim=\"Y\", limit=None)\n3575         <xarray.DataArray (Y: 5, X: 3)>\n3576         array([[ 0.,  1.,  3.],\n3577                [ 0.,  2.,  5.],\n3578                [ 5.,  2.,  0.],\n3579                [ 3.,  2.,  0.],\n3580                [nan,  2.,  0.]])\n3581         Coordinates:\n3582             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3583             lon      (X) float64 10.0 10.25 10.5\n3584         Dimensions without coordinates: Y, X\n3585 \n3586         Fill only the first of consecutive NaN values:\n3587 \n3588         >>> da.bfill(dim=\"Y\", limit=1)\n3589         <xarray.DataArray (Y: 5, X: 3)>\n3590         array([[ 0.,  1.,  3.],\n3591                [ 0., nan,  5.],\n3592                [ 5., nan, nan],\n3593                [ 3.,  2.,  0.],\n3594                [nan,  2.,  0.]])\n3595         Coordinates:\n3596             lat      (Y) float64 -20.0 -20.25 -20.5 -20.75 -21.0\n3597             lon      (X) float64 10.0 10.25 10.5\n3598         Dimensions without coordinates: Y, X\n3599         \"\"\"\n3600         from xarray.core.missing import bfill\n3601 \n3602         return bfill(self, dim, limit=limit)\n3603 \n3604     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n3605         \"\"\"Combine two DataArray objects, with union of coordinates.\n3606 \n3607         This operation follows the normal broadcasting and alignment rules of\n3608         ``join='outer'``.  Default to non-null values of array calling the\n3609         method.  Use np.nan to fill in vacant cells after alignment.\n3610 \n3611         Parameters\n3612         ----------\n3613         other : DataArray\n3614             Used to fill all matching missing values in this array.\n3615 \n3616         Returns\n3617         -------\n3618         DataArray\n3619         \"\"\"\n3620         return ops.fillna(self, other, join=\"outer\")\n3621 \n3622     def reduce(\n3623         self: T_DataArray,\n3624         func: Callable[..., Any],\n3625         dim: Dims = None,\n3626         *,\n3627         axis: int | Sequence[int] | None = None,\n3628         keep_attrs: bool | None = None,\n3629         keepdims: bool = False,\n3630         **kwargs: Any,\n3631     ) -> T_DataArray:\n3632         \"\"\"Reduce this array by applying `func` along some dimension(s).\n3633 \n3634         Parameters\n3635         ----------\n3636         func : callable\n3637             Function which can be called in the form\n3638             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n3639             np.ndarray over an integer valued axis.\n3640         dim : \"...\", str, Iterable of Hashable or None, optional\n3641             Dimension(s) over which to apply `func`. By default `func` is\n3642             applied over all dimensions.\n3643         axis : int or sequence of int, optional\n3644             Axis(es) over which to repeatedly apply `func`. Only one of the\n3645             'dim' and 'axis' arguments can be supplied. If neither are\n3646             supplied, then the reduction is calculated over the flattened array\n3647             (by calling `f(x)` without an axis argument).\n3648         keep_attrs : bool or None, optional\n3649             If True, the variable's attributes (`attrs`) will be copied from\n3650             the original object to the new one.  If False (default), the new\n3651             object will be returned without attributes.\n3652         keepdims : bool, default: False\n3653             If True, the dimensions which are reduced are left in the result\n3654             as dimensions of size one. Coordinates that use these dimensions\n3655             are removed.\n3656         **kwargs : dict\n3657             Additional keyword arguments passed on to `func`.\n3658 \n3659         Returns\n3660         -------\n3661         reduced : DataArray\n3662             DataArray with this object's array replaced with an array with\n3663             summarized data and the indicated dimension(s) removed.\n3664         \"\"\"\n3665 \n3666         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)\n3667         return self._replace_maybe_drop_dims(var)\n3668 \n3669     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:\n3670         \"\"\"Convert this array into a pandas object with the same shape.\n3671 \n3672         The type of the returned object depends on the number of DataArray\n3673         dimensions:\n3674 \n3675         * 0D -> `xarray.DataArray`\n3676         * 1D -> `pandas.Series`\n3677         * 2D -> `pandas.DataFrame`\n3678 \n3679         Only works for arrays with 2 or fewer dimensions.\n3680 \n3681         The DataArray constructor performs the inverse transformation.\n3682 \n3683         Returns\n3684         -------\n3685         result : DataArray | Series | DataFrame\n3686             DataArray, pandas Series or pandas DataFrame.\n3687         \"\"\"\n3688         # TODO: consolidate the info about pandas constructors and the\n3689         # attributes that correspond to their indexes into a separate module?\n3690         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}\n3691         try:\n3692             constructor = constructors[self.ndim]\n3693         except KeyError:\n3694             raise ValueError(\n3695                 f\"Cannot convert arrays with {self.ndim} dimensions into \"\n3696                 \"pandas objects. Requires 2 or fewer dimensions.\"\n3697             )\n3698         indexes = [self.get_index(dim) for dim in self.dims]\n3699         return constructor(self.values, *indexes)\n3700 \n3701     def to_dataframe(\n3702         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None\n3703     ) -> pd.DataFrame:\n3704         \"\"\"Convert this array and its coordinates into a tidy pandas.DataFrame.\n3705 \n3706         The DataFrame is indexed by the Cartesian product of index coordinates\n3707         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are\n3708         included as columns in the DataFrame.\n3709 \n3710         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which\n3711         doesn't rely on a MultiIndex to build the DataFrame.\n3712 \n3713         Parameters\n3714         ----------\n3715         name: Hashable or None, optional\n3716             Name to give to this array (required if unnamed).\n3717         dim_order: Sequence of Hashable or None, optional\n3718             Hierarchical dimension order for the resulting dataframe.\n3719             Array content is transposed to this order and then written out as flat\n3720             vectors in contiguous order, so the last dimension in this list\n3721             will be contiguous in the resulting DataFrame. This has a major\n3722             influence on which operations are efficient on the resulting\n3723             dataframe.\n3724 \n3725             If provided, must include all dimensions of this DataArray. By default,\n3726             dimensions are sorted according to the DataArray dimensions order.\n3727 \n3728         Returns\n3729         -------\n3730         result: DataFrame\n3731             DataArray as a pandas DataFrame.\n3732 \n3733         See also\n3734         --------\n3735         DataArray.to_pandas\n3736         DataArray.to_series\n3737         \"\"\"\n3738         if name is None:\n3739             name = self.name\n3740         if name is None:\n3741             raise ValueError(\n3742                 \"cannot convert an unnamed DataArray to a \"\n3743                 \"DataFrame: use the ``name`` parameter\"\n3744             )\n3745         if self.ndim == 0:\n3746             raise ValueError(\"cannot convert a scalar to a DataFrame\")\n3747 \n3748         # By using a unique name, we can convert a DataArray into a DataFrame\n3749         # even if it shares a name with one of its coordinates.\n3750         # I would normally use unique_name = object() but that results in a\n3751         # dataframe with columns in the wrong order, for reasons I have not\n3752         # been able to debug (possibly a pandas bug?).\n3753         unique_name = \"__unique_name_identifier_z98xfz98xugfg73ho__\"\n3754         ds = self._to_dataset_whole(name=unique_name)\n3755 \n3756         if dim_order is None:\n3757             ordered_dims = dict(zip(self.dims, self.shape))\n3758         else:\n3759             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)\n3760 \n3761         df = ds._to_dataframe(ordered_dims)\n3762         df.columns = [name if c == unique_name else c for c in df.columns]\n3763         return df\n3764 \n3765     def to_series(self) -> pd.Series:\n3766         \"\"\"Convert this array into a pandas.Series.\n3767 \n3768         The Series is indexed by the Cartesian product of index coordinates\n3769         (in the form of a :py:class:`pandas.MultiIndex`).\n3770 \n3771         Returns\n3772         -------\n3773         result : Series\n3774             DataArray as a pandas Series.\n3775 \n3776         See also\n3777         --------\n3778         DataArray.to_pandas\n3779         DataArray.to_dataframe\n3780         \"\"\"\n3781         index = self.coords.to_index()\n3782         return pd.Series(self.values.reshape(-1), index=index, name=self.name)\n3783 \n3784     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:\n3785         \"\"\"Convert this array into a numpy.ma.MaskedArray\n3786 \n3787         Parameters\n3788         ----------\n3789         copy : bool, default: True\n3790             If True make a copy of the array in the result. If False,\n3791             a MaskedArray view of DataArray.values is returned.\n3792 \n3793         Returns\n3794         -------\n3795         result : MaskedArray\n3796             Masked where invalid values (nan or inf) occur.\n3797         \"\"\"\n3798         values = self.to_numpy()  # only compute lazy arrays once\n3799         isnull = pd.isnull(values)\n3800         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)\n3801 \n3802     # path=None writes to bytes\n3803     @overload\n3804     def to_netcdf(\n3805         self,\n3806         path: None = None,\n3807         mode: Literal[\"w\", \"a\"] = \"w\",\n3808         format: T_NetcdfTypes | None = None,\n3809         group: str | None = None,\n3810         engine: T_NetcdfEngine | None = None,\n3811         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3812         unlimited_dims: Iterable[Hashable] | None = None,\n3813         compute: bool = True,\n3814         invalid_netcdf: bool = False,\n3815     ) -> bytes:\n3816         ...\n3817 \n3818     # default return None\n3819     @overload\n3820     def to_netcdf(\n3821         self,\n3822         path: str | PathLike,\n3823         mode: Literal[\"w\", \"a\"] = \"w\",\n3824         format: T_NetcdfTypes | None = None,\n3825         group: str | None = None,\n3826         engine: T_NetcdfEngine | None = None,\n3827         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3828         unlimited_dims: Iterable[Hashable] | None = None,\n3829         compute: Literal[True] = True,\n3830         invalid_netcdf: bool = False,\n3831     ) -> None:\n3832         ...\n3833 \n3834     # compute=False returns dask.Delayed\n3835     @overload\n3836     def to_netcdf(\n3837         self,\n3838         path: str | PathLike,\n3839         mode: Literal[\"w\", \"a\"] = \"w\",\n3840         format: T_NetcdfTypes | None = None,\n3841         group: str | None = None,\n3842         engine: T_NetcdfEngine | None = None,\n3843         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3844         unlimited_dims: Iterable[Hashable] | None = None,\n3845         *,\n3846         compute: Literal[False],\n3847         invalid_netcdf: bool = False,\n3848     ) -> Delayed:\n3849         ...\n3850 \n3851     def to_netcdf(\n3852         self,\n3853         path: str | PathLike | None = None,\n3854         mode: Literal[\"w\", \"a\"] = \"w\",\n3855         format: T_NetcdfTypes | None = None,\n3856         group: str | None = None,\n3857         engine: T_NetcdfEngine | None = None,\n3858         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,\n3859         unlimited_dims: Iterable[Hashable] | None = None,\n3860         compute: bool = True,\n3861         invalid_netcdf: bool = False,\n3862     ) -> bytes | Delayed | None:\n3863         \"\"\"Write DataArray contents to a netCDF file.\n3864 \n3865         Parameters\n3866         ----------\n3867         path : str, path-like or None, optional\n3868             Path to which to save this dataset. File-like objects are only\n3869             supported by the scipy engine. If no path is provided, this\n3870             function returns the resulting netCDF file as bytes; in this case,\n3871             we need to use scipy, which does not support netCDF version 4 (the\n3872             default format becomes NETCDF3_64BIT).\n3873         mode : {\"w\", \"a\"}, default: \"w\"\n3874             Write ('w') or append ('a') mode. If mode='w', any existing file at\n3875             this location will be overwritten. If mode='a', existing variables\n3876             will be overwritten.\n3877         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n3878                   \"NETCDF3_CLASSIC\"}, optional\n3879             File format for the resulting netCDF file:\n3880 \n3881             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n3882               features.\n3883             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n3884               netCDF 3 compatible API features.\n3885             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n3886               which fully supports 2+ GB files, but is only compatible with\n3887               clients linked against netCDF version 3.6.0 or later.\n3888             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n3889               handle 2+ GB files very well.\n3890 \n3891             All formats are supported by the netCDF4-python library.\n3892             scipy.io.netcdf only supports the last two formats.\n3893 \n3894             The default format is NETCDF4 if you are saving a file to disk and\n3895             have the netCDF4-python library available. Otherwise, xarray falls\n3896             back to using scipy to write netCDF files and defaults to the\n3897             NETCDF3_64BIT format (scipy does not support netCDF4).\n3898         group : str, optional\n3899             Path to the netCDF4 group in the given file to open (only works for\n3900             format='NETCDF4'). The group(s) will be created if necessary.\n3901         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n3902             Engine to use when writing netCDF files. If not provided, the\n3903             default engine is chosen based on available dependencies, with a\n3904             preference for 'netcdf4' if writing to a file on disk.\n3905         encoding : dict, optional\n3906             Nested dictionary with variable names as keys and dictionaries of\n3907             variable specific encodings as values, e.g.,\n3908             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n3909             \"zlib\": True}, ...}``\n3910 \n3911             The `h5netcdf` engine supports both the NetCDF4-style compression\n3912             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n3913             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n3914             This allows using any compression plugin installed in the HDF5\n3915             library, e.g. LZF.\n3916 \n3917         unlimited_dims : iterable of Hashable, optional\n3918             Dimension(s) that should be serialized as unlimited dimensions.\n3919             By default, no dimensions are treated as unlimited dimensions.\n3920             Note that unlimited_dims may also be set via\n3921             ``dataset.encoding[\"unlimited_dims\"]``.\n3922         compute: bool, default: True\n3923             If true compute immediately, otherwise return a\n3924             ``dask.delayed.Delayed`` object that can be computed later.\n3925         invalid_netcdf: bool, default: False\n3926             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n3927             hdf5 files which are invalid netcdf as described in\n3928             https://github.com/h5netcdf/h5netcdf.\n3929 \n3930         Returns\n3931         -------\n3932         store: bytes or Delayed or None\n3933             * ``bytes`` if path is None\n3934             * ``dask.delayed.Delayed`` if compute is False\n3935             * None otherwise\n3936 \n3937         Notes\n3938         -----\n3939         Only xarray.Dataset objects can be written to netCDF files, so\n3940         the xarray.DataArray is converted to a xarray.Dataset object\n3941         containing a single variable. If the DataArray has no name, or if the\n3942         name is the same as a coordinate name, then it is given the name\n3943         ``\"__xarray_dataarray_variable__\"``.\n3944 \n3945         See Also\n3946         --------\n3947         Dataset.to_netcdf\n3948         \"\"\"\n3949         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf\n3950 \n3951         if self.name is None:\n3952             # If no name is set then use a generic xarray name\n3953             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3954         elif self.name in self.coords or self.name in self.dims:\n3955             # The name is the same as one of the coords names, which netCDF\n3956             # doesn't support, so rename it but keep track of the old name\n3957             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n3958             dataset.attrs[DATAARRAY_NAME] = self.name\n3959         else:\n3960             # No problems with the name - so we're fine!\n3961             dataset = self.to_dataset()\n3962 \n3963         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n3964             dataset,\n3965             path,\n3966             mode=mode,\n3967             format=format,\n3968             group=group,\n3969             engine=engine,\n3970             encoding=encoding,\n3971             unlimited_dims=unlimited_dims,\n3972             compute=compute,\n3973             multifile=False,\n3974             invalid_netcdf=invalid_netcdf,\n3975         )\n3976 \n3977     # compute=True (default) returns ZarrStore\n3978     @overload\n3979     def to_zarr(\n3980         self,\n3981         store: MutableMapping | str | PathLike[str] | None = None,\n3982         chunk_store: MutableMapping | str | PathLike | None = None,\n3983         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n3984         synchronizer=None,\n3985         group: str | None = None,\n3986         encoding: Mapping | None = None,\n3987         compute: Literal[True] = True,\n3988         consolidated: bool | None = None,\n3989         append_dim: Hashable | None = None,\n3990         region: Mapping[str, slice] | None = None,\n3991         safe_chunks: bool = True,\n3992         storage_options: dict[str, str] | None = None,\n3993         zarr_version: int | None = None,\n3994     ) -> ZarrStore:\n3995         ...\n3996 \n3997     # compute=False returns dask.Delayed\n3998     @overload\n3999     def to_zarr(\n4000         self,\n4001         store: MutableMapping | str | PathLike[str] | None = None,\n4002         chunk_store: MutableMapping | str | PathLike | None = None,\n4003         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n4004         synchronizer=None,\n4005         group: str | None = None,\n4006         encoding: Mapping | None = None,\n4007         *,\n4008         compute: Literal[False],\n4009         consolidated: bool | None = None,\n4010         append_dim: Hashable | None = None,\n4011         region: Mapping[str, slice] | None = None,\n4012         safe_chunks: bool = True,\n4013         storage_options: dict[str, str] | None = None,\n4014         zarr_version: int | None = None,\n4015     ) -> Delayed:\n4016         ...\n4017 \n4018     def to_zarr(\n4019         self,\n4020         store: MutableMapping | str | PathLike[str] | None = None,\n4021         chunk_store: MutableMapping | str | PathLike | None = None,\n4022         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n4023         synchronizer=None,\n4024         group: str | None = None,\n4025         encoding: Mapping | None = None,\n4026         compute: bool = True,\n4027         consolidated: bool | None = None,\n4028         append_dim: Hashable | None = None,\n4029         region: Mapping[str, slice] | None = None,\n4030         safe_chunks: bool = True,\n4031         storage_options: dict[str, str] | None = None,\n4032         zarr_version: int | None = None,\n4033     ) -> ZarrStore | Delayed:\n4034         \"\"\"Write DataArray contents to a Zarr store\n4035 \n4036         Zarr chunks are determined in the following way:\n4037 \n4038         - From the ``chunks`` attribute in each variable's ``encoding``\n4039           (can be set via `DataArray.chunk`).\n4040         - If the variable is a Dask array, from the dask chunks\n4041         - If neither Dask chunks nor encoding chunks are present, chunks will\n4042           be determined automatically by Zarr\n4043         - If both Dask chunks and encoding chunks are present, encoding chunks\n4044           will be used, provided that there is a many-to-one relationship between\n4045           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and\n4046           evenly divide encoding chunks); otherwise raise a ``ValueError``.\n4047           This restriction ensures that no synchronization / locks are required\n4048           when writing. To disable this restriction, use ``safe_chunks=False``.\n4049 \n4050         Parameters\n4051         ----------\n4052         store : MutableMapping, str or path-like, optional\n4053             Store or path to directory in local or remote file system.\n4054         chunk_store : MutableMapping, str or path-like, optional\n4055             Store or path to directory in local or remote file system only for Zarr\n4056             array chunks. Requires zarr-python v2.4.0 or later.\n4057         mode : {\"w\", \"w-\", \"a\", \"r+\", None}, optional\n4058             Persistence mode: \"w\" means create (overwrite if exists);\n4059             \"w-\" means create (fail if exists);\n4060             \"a\" means override existing variables (create if does not exist);\n4061             \"r+\" means modify existing array *values* only (raise an error if\n4062             any metadata or shapes would change).\n4063             The default mode is \"a\" if ``append_dim`` is set. Otherwise, it is\n4064             \"r+\" if ``region`` is set and ``w-`` otherwise.\n4065         synchronizer : object, optional\n4066             Zarr array synchronizer.\n4067         group : str, optional\n4068             Group path. (a.k.a. `path` in zarr terminology.)\n4069         encoding : dict, optional\n4070             Nested dictionary with variable names as keys and dictionaries of\n4071             variable specific encodings as values, e.g.,\n4072             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n4073         compute : bool, default: True\n4074             If True write array data immediately, otherwise return a\n4075             ``dask.delayed.Delayed`` object that can be computed to write\n4076             array data later. Metadata is always updated eagerly.\n4077         consolidated : bool, optional\n4078             If True, apply zarr's `consolidate_metadata` function to the store\n4079             after writing metadata and read existing stores with consolidated\n4080             metadata; if False, do not. The default (`consolidated=None`) means\n4081             write consolidated metadata and attempt to read consolidated\n4082             metadata for existing stores (falling back to non-consolidated).\n4083 \n4084             When the experimental ``zarr_version=3``, ``consolidated`` must be\n4085             either be ``None`` or ``False``.\n4086         append_dim : hashable, optional\n4087             If set, the dimension along which the data will be appended. All\n4088             other dimensions on overridden variables must remain the same size.\n4089         region : dict, optional\n4090             Optional mapping from dimension names to integer slices along\n4091             dataarray dimensions to indicate the region of existing zarr array(s)\n4092             in which to write this datarray's data. For example,\n4093             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate\n4094             that values should be written to the region ``0:1000`` along ``x``\n4095             and ``10000:11000`` along ``y``.\n4096 \n4097             Two restrictions apply to the use of ``region``:\n4098 \n4099             - If ``region`` is set, _all_ variables in a dataarray must have at\n4100               least one dimension in common with the region. Other variables\n4101               should be written in a separate call to ``to_zarr()``.\n4102             - Dimensions cannot be included in both ``region`` and\n4103               ``append_dim`` at the same time. To create empty arrays to fill\n4104               in with ``region``, use a separate call to ``to_zarr()`` with\n4105               ``compute=False``. See \"Appending to existing Zarr stores\" in\n4106               the reference documentation for full details.\n4107         safe_chunks : bool, default: True\n4108             If True, only allow writes to when there is a many-to-one relationship\n4109             between Zarr chunks (specified in encoding) and Dask chunks.\n4110             Set False to override this restriction; however, data may become corrupted\n4111             if Zarr arrays are written in parallel. This option may be useful in combination\n4112             with ``compute=False`` to initialize a Zarr store from an existing\n4113             DataArray with arbitrary chunk structure.\n4114         storage_options : dict, optional\n4115             Any additional parameters for the storage backend (ignored for local\n4116             paths).\n4117         zarr_version : int or None, optional\n4118             The desired zarr spec version to target (currently 2 or 3). The\n4119             default of None will attempt to determine the zarr version from\n4120             ``store`` when possible, otherwise defaulting to 2.\n4121 \n4122         Returns\n4123         -------\n4124             * ``dask.delayed.Delayed`` if compute is False\n4125             * ZarrStore otherwise\n4126 \n4127         References\n4128         ----------\n4129         https://zarr.readthedocs.io/\n4130 \n4131         Notes\n4132         -----\n4133         Zarr chunking behavior:\n4134             If chunks are found in the encoding argument or attribute\n4135             corresponding to any DataArray, those chunks are used.\n4136             If a DataArray is a dask array, it is written with those chunks.\n4137             If not other chunks are found, Zarr uses its own heuristics to\n4138             choose automatic chunk sizes.\n4139 \n4140         encoding:\n4141             The encoding attribute (if exists) of the DataArray(s) will be\n4142             used. Override any existing encodings by providing the ``encoding`` kwarg.\n4143 \n4144         See Also\n4145         --------\n4146         Dataset.to_zarr\n4147         :ref:`io.zarr`\n4148             The I/O user guide, with more details and examples.\n4149         \"\"\"\n4150         from xarray.backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_zarr\n4151 \n4152         if self.name is None:\n4153             # If no name is set then use a generic xarray name\n4154             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n4155         elif self.name in self.coords or self.name in self.dims:\n4156             # The name is the same as one of the coords names, which the netCDF data model\n4157             # does not support, so rename it but keep track of the old name\n4158             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)\n4159             dataset.attrs[DATAARRAY_NAME] = self.name\n4160         else:\n4161             # No problems with the name - so we're fine!\n4162             dataset = self.to_dataset()\n4163 \n4164         return to_zarr(  # type: ignore[call-overload,misc]\n4165             dataset,\n4166             store=store,\n4167             chunk_store=chunk_store,\n4168             mode=mode,\n4169             synchronizer=synchronizer,\n4170             group=group,\n4171             encoding=encoding,\n4172             compute=compute,\n4173             consolidated=consolidated,\n4174             append_dim=append_dim,\n4175             region=region,\n4176             safe_chunks=safe_chunks,\n4177             storage_options=storage_options,\n4178             zarr_version=zarr_version,\n4179         )\n4180 \n4181     def to_dict(\n4182         self, data: bool | Literal[\"list\", \"array\"] = \"list\", encoding: bool = False\n4183     ) -> dict[str, Any]:\n4184         \"\"\"\n4185         Convert this xarray.DataArray into a dictionary following xarray\n4186         naming conventions.\n4187 \n4188         Converts all variables and attributes to native Python objects.\n4189         Useful for converting to json. To avoid datetime incompatibility\n4190         use decode_times=False kwarg in xarray.open_dataset.\n4191 \n4192         Parameters\n4193         ----------\n4194         data : bool or {\"list\", \"array\"}, default: \"list\"\n4195             Whether to include the actual data in the dictionary. When set to\n4196             False, returns just the schema. If set to \"array\", returns data as\n4197             underlying array type. If set to \"list\" (or True for backwards\n4198             compatibility), returns data in lists of Python data types. Note\n4199             that for obtaining the \"list\" output efficiently, use\n4200             `da.compute().to_dict(data=\"list\")`.\n4201 \n4202         encoding : bool, default: False\n4203             Whether to include the Dataset's encoding in the dictionary.\n4204 \n4205         Returns\n4206         -------\n4207         dict: dict\n4208 \n4209         See Also\n4210         --------\n4211         DataArray.from_dict\n4212         Dataset.to_dict\n4213         \"\"\"\n4214         d = self.variable.to_dict(data=data)\n4215         d.update({\"coords\": {}, \"name\": self.name})\n4216         for k, coord in self.coords.items():\n4217             d[\"coords\"][k] = coord.variable.to_dict(data=data)\n4218         if encoding:\n4219             d[\"encoding\"] = dict(self.encoding)\n4220         return d\n4221 \n4222     @classmethod\n4223     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:\n4224         \"\"\"Convert a dictionary into an xarray.DataArray\n4225 \n4226         Parameters\n4227         ----------\n4228         d : dict\n4229             Mapping with a minimum structure of {\"dims\": [...], \"data\": [...]}\n4230 \n4231         Returns\n4232         -------\n4233         obj : xarray.DataArray\n4234 \n4235         See Also\n4236         --------\n4237         DataArray.to_dict\n4238         Dataset.from_dict\n4239 \n4240         Examples\n4241         --------\n4242         >>> d = {\"dims\": \"t\", \"data\": [1, 2, 3]}\n4243         >>> da = xr.DataArray.from_dict(d)\n4244         >>> da\n4245         <xarray.DataArray (t: 3)>\n4246         array([1, 2, 3])\n4247         Dimensions without coordinates: t\n4248 \n4249         >>> d = {\n4250         ...     \"coords\": {\n4251         ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n4252         ...     },\n4253         ...     \"attrs\": {\"title\": \"air temperature\"},\n4254         ...     \"dims\": \"t\",\n4255         ...     \"data\": [10, 20, 30],\n4256         ...     \"name\": \"a\",\n4257         ... }\n4258         >>> da = xr.DataArray.from_dict(d)\n4259         >>> da\n4260         <xarray.DataArray 'a' (t: 3)>\n4261         array([10, 20, 30])\n4262         Coordinates:\n4263           * t        (t) int64 0 1 2\n4264         Attributes:\n4265             title:    air temperature\n4266         \"\"\"\n4267         coords = None\n4268         if \"coords\" in d:\n4269             try:\n4270                 coords = {\n4271                     k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"))\n4272                     for k, v in d[\"coords\"].items()\n4273                 }\n4274             except KeyError as e:\n4275                 raise ValueError(\n4276                     \"cannot convert dict when coords are missing the key \"\n4277                     \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n4278                 )\n4279         try:\n4280             data = d[\"data\"]\n4281         except KeyError:\n4282             raise ValueError(\"cannot convert dict without the key 'data''\")\n4283         else:\n4284             obj = cls(data, coords, d.get(\"dims\"), d.get(\"name\"), d.get(\"attrs\"))\n4285 \n4286         obj.encoding.update(d.get(\"encoding\", {}))\n4287 \n4288         return obj\n4289 \n4290     @classmethod\n4291     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:\n4292         \"\"\"Convert a pandas.Series into an xarray.DataArray.\n4293 \n4294         If the series's index is a MultiIndex, it will be expanded into a\n4295         tensor product of one-dimensional coordinates (filling in missing\n4296         values with NaN). Thus this operation should be the inverse of the\n4297         `to_series` method.\n4298 \n4299         Parameters\n4300         ----------\n4301         series : Series\n4302             Pandas Series object to convert.\n4303         sparse : bool, default: False\n4304             If sparse=True, creates a sparse array instead of a dense NumPy array.\n4305             Requires the pydata/sparse package.\n4306 \n4307         See Also\n4308         --------\n4309         DataArray.to_series\n4310         Dataset.from_dataframe\n4311         \"\"\"\n4312         temp_name = \"__temporary_name\"\n4313         df = pd.DataFrame({temp_name: series})\n4314         ds = Dataset.from_dataframe(df, sparse=sparse)\n4315         result = cast(DataArray, ds[temp_name])\n4316         result.name = series.name\n4317         return result\n4318 \n4319     def to_cdms2(self) -> cdms2_Variable:\n4320         \"\"\"Convert this array into a cdms2.Variable\"\"\"\n4321         from xarray.convert import to_cdms2\n4322 \n4323         return to_cdms2(self)\n4324 \n4325     @classmethod\n4326     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:\n4327         \"\"\"Convert a cdms2.Variable into an xarray.DataArray\"\"\"\n4328         from xarray.convert import from_cdms2\n4329 \n4330         return from_cdms2(variable)\n4331 \n4332     def to_iris(self) -> iris_Cube:\n4333         \"\"\"Convert this array into a iris.cube.Cube\"\"\"\n4334         from xarray.convert import to_iris\n4335 \n4336         return to_iris(self)\n4337 \n4338     @classmethod\n4339     def from_iris(cls, cube: iris_Cube) -> DataArray:\n4340         \"\"\"Convert a iris.cube.Cube into an xarray.DataArray\"\"\"\n4341         from xarray.convert import from_iris\n4342 \n4343         return from_iris(cube)\n4344 \n4345     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:\n4346         \"\"\"Helper function for equals, broadcast_equals, and identical\"\"\"\n4347 \n4348         def compat(x, y):\n4349             return getattr(x.variable, compat_str)(y.variable)\n4350 \n4351         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(\n4352             self, other\n4353         )\n4354 \n4355     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:\n4356         \"\"\"Two DataArrays are broadcast equal if they are equal after\n4357         broadcasting them against each other such that they have the same\n4358         dimensions.\n4359 \n4360         Parameters\n4361         ----------\n4362         other : DataArray\n4363             DataArray to compare to.\n4364 \n4365         Returns\n4366         ----------\n4367         equal : bool\n4368             True if the two DataArrays are broadcast equal.\n4369 \n4370         See Also\n4371         --------\n4372         DataArray.equals\n4373         DataArray.identical\n4374 \n4375         Examples\n4376         --------\n4377         >>> a = xr.DataArray([1, 2], dims=\"X\")\n4378         >>> b = xr.DataArray([[1, 1], [2, 2]], dims=[\"X\", \"Y\"])\n4379         >>> a\n4380         <xarray.DataArray (X: 2)>\n4381         array([1, 2])\n4382         Dimensions without coordinates: X\n4383         >>> b\n4384         <xarray.DataArray (X: 2, Y: 2)>\n4385         array([[1, 1],\n4386                [2, 2]])\n4387         Dimensions without coordinates: X, Y\n4388 \n4389         .equals returns True if two DataArrays have the same values, dimensions, and coordinates. .broadcast_equals returns True if the results of broadcasting two DataArrays against eachother have the same values, dimensions, and coordinates.\n4390 \n4391         >>> a.equals(b)\n4392         False\n4393         >>> a2, b2 = xr.broadcast(a, b)\n4394         >>> a2.equals(b2)\n4395         True\n4396         >>> a.broadcast_equals(b)\n4397         True\n4398         \"\"\"\n4399         try:\n4400             return self._all_compat(other, \"broadcast_equals\")\n4401         except (TypeError, AttributeError):\n4402             return False\n4403 \n4404     def equals(self: T_DataArray, other: T_DataArray) -> bool:\n4405         \"\"\"True if two DataArrays have the same dimensions, coordinates and\n4406         values; otherwise False.\n4407 \n4408         DataArrays can still be equal (like pandas objects) if they have NaN\n4409         values in the same locations.\n4410 \n4411         This method is necessary because `v1 == v2` for ``DataArray``\n4412         does element-wise comparisons (like numpy.ndarrays).\n4413 \n4414         Parameters\n4415         ----------\n4416         other : DataArray\n4417             DataArray to compare to.\n4418 \n4419         Returns\n4420         ----------\n4421         equal : bool\n4422             True if the two DataArrays are equal.\n4423 \n4424         See Also\n4425         --------\n4426         DataArray.broadcast_equals\n4427         DataArray.identical\n4428 \n4429         Examples\n4430         --------\n4431         >>> a = xr.DataArray([1, 2, 3], dims=\"X\")\n4432         >>> b = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"m\"))\n4433         >>> c = xr.DataArray([1, 2, 3], dims=\"Y\")\n4434         >>> d = xr.DataArray([3, 2, 1], dims=\"X\")\n4435         >>> a\n4436         <xarray.DataArray (X: 3)>\n4437         array([1, 2, 3])\n4438         Dimensions without coordinates: X\n4439         >>> b\n4440         <xarray.DataArray (X: 3)>\n4441         array([1, 2, 3])\n4442         Dimensions without coordinates: X\n4443         Attributes:\n4444             units:    m\n4445         >>> c\n4446         <xarray.DataArray (Y: 3)>\n4447         array([1, 2, 3])\n4448         Dimensions without coordinates: Y\n4449         >>> d\n4450         <xarray.DataArray (X: 3)>\n4451         array([3, 2, 1])\n4452         Dimensions without coordinates: X\n4453 \n4454         >>> a.equals(b)\n4455         True\n4456         >>> a.equals(c)\n4457         False\n4458         >>> a.equals(d)\n4459         False\n4460         \"\"\"\n4461         try:\n4462             return self._all_compat(other, \"equals\")\n4463         except (TypeError, AttributeError):\n4464             return False\n4465 \n4466     def identical(self: T_DataArray, other: T_DataArray) -> bool:\n4467         \"\"\"Like equals, but also checks the array name and attributes, and\n4468         attributes on all coordinates.\n4469 \n4470         Parameters\n4471         ----------\n4472         other : DataArray\n4473             DataArray to compare to.\n4474 \n4475         Returns\n4476         ----------\n4477         equal : bool\n4478             True if the two DataArrays are identical.\n4479 \n4480         See Also\n4481         --------\n4482         DataArray.broadcast_equals\n4483         DataArray.equals\n4484 \n4485         Examples\n4486         --------\n4487         >>> a = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"m\"), name=\"Width\")\n4488         >>> b = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"m\"), name=\"Width\")\n4489         >>> c = xr.DataArray([1, 2, 3], dims=\"X\", attrs=dict(units=\"ft\"), name=\"Width\")\n4490         >>> a\n4491         <xarray.DataArray 'Width' (X: 3)>\n4492         array([1, 2, 3])\n4493         Dimensions without coordinates: X\n4494         Attributes:\n4495             units:    m\n4496         >>> b\n4497         <xarray.DataArray 'Width' (X: 3)>\n4498         array([1, 2, 3])\n4499         Dimensions without coordinates: X\n4500         Attributes:\n4501             units:    m\n4502         >>> c\n4503         <xarray.DataArray 'Width' (X: 3)>\n4504         array([1, 2, 3])\n4505         Dimensions without coordinates: X\n4506         Attributes:\n4507             units:    ft\n4508 \n4509         >>> a.equals(b)\n4510         True\n4511         >>> a.identical(b)\n4512         True\n4513 \n4514         >>> a.equals(c)\n4515         True\n4516         >>> a.identical(c)\n4517         False\n4518         \"\"\"\n4519         try:\n4520             return self.name == other.name and self._all_compat(other, \"identical\")\n4521         except (TypeError, AttributeError):\n4522             return False\n4523 \n4524     def _result_name(self, other: Any = None) -> Hashable | None:\n4525         # use the same naming heuristics as pandas:\n4526         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356\n4527         other_name = getattr(other, \"name\", _default)\n4528         if other_name is _default or other_name == self.name:\n4529             return self.name\n4530         else:\n4531             return None\n4532 \n4533     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:\n4534         new_var = self.variable.__array_wrap__(obj, context)\n4535         return self._replace(new_var)\n4536 \n4537     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:\n4538         return self.dot(obj)\n4539 \n4540     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:\n4541         # currently somewhat duplicative, as only other DataArrays are\n4542         # compatible with matmul\n4543         return computation.dot(other, self)\n4544 \n4545     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:\n4546         keep_attrs = kwargs.pop(\"keep_attrs\", None)\n4547         if keep_attrs is None:\n4548             keep_attrs = _get_keep_attrs(default=True)\n4549         with warnings.catch_warnings():\n4550             warnings.filterwarnings(\"ignore\", r\"All-NaN (slice|axis) encountered\")\n4551             warnings.filterwarnings(\n4552                 \"ignore\", r\"Mean of empty slice\", category=RuntimeWarning\n4553             )\n4554             with np.errstate(all=\"ignore\"):\n4555                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))\n4556             if keep_attrs:\n4557                 da.attrs = self.attrs\n4558             return da\n4559 \n4560     def _binary_op(\n4561         self: T_DataArray,\n4562         other: Any,\n4563         f: Callable,\n4564         reflexive: bool = False,\n4565     ) -> T_DataArray:\n4566         from xarray.core.groupby import GroupBy\n4567 \n4568         if isinstance(other, (Dataset, GroupBy)):\n4569             return NotImplemented\n4570         if isinstance(other, DataArray):\n4571             align_type = OPTIONS[\"arithmetic_join\"]\n4572             self, other = align(self, other, join=align_type, copy=False)  # type: ignore\n4573         other_variable = getattr(other, \"variable\", other)\n4574         other_coords = getattr(other, \"coords\", None)\n4575 \n4576         variable = (\n4577             f(self.variable, other_variable)\n4578             if not reflexive\n4579             else f(other_variable, self.variable)\n4580         )\n4581         coords, indexes = self.coords._merge_raw(other_coords, reflexive)\n4582         name = self._result_name(other)\n4583 \n4584         return self._replace(variable, coords, name, indexes=indexes)\n4585 \n4586     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:\n4587         from xarray.core.groupby import GroupBy\n4588 \n4589         if isinstance(other, GroupBy):\n4590             raise TypeError(\n4591                 \"in-place operations between a DataArray and \"\n4592                 \"a grouped object are not permitted\"\n4593             )\n4594         # n.b. we can't align other to self (with other.reindex_like(self))\n4595         # because `other` may be converted into floats, which would cause\n4596         # in-place arithmetic to fail unpredictably. Instead, we simply\n4597         # don't support automatic alignment with in-place arithmetic.\n4598         other_coords = getattr(other, \"coords\", None)\n4599         other_variable = getattr(other, \"variable\", other)\n4600         try:\n4601             with self.coords._merge_inplace(other_coords):\n4602                 f(self.variable, other_variable)\n4603         except MergeError as exc:\n4604             raise MergeError(\n4605                 \"Automatic alignment is not supported for in-place operations.\\n\"\n4606                 \"Consider aligning the indices manually or using a not-in-place operation.\\n\"\n4607                 \"See https://github.com/pydata/xarray/issues/3910 for more explanations.\"\n4608             ) from exc\n4609         return self\n4610 \n4611     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:\n4612         self.attrs = other.attrs\n4613 \n4614     plot = utils.UncachedAccessor(DataArrayPlotAccessor)\n4615 \n4616     def _title_for_slice(self, truncate: int = 50) -> str:\n4617         \"\"\"\n4618         If the dataarray has 1 dimensional coordinates or comes from a slice\n4619         we can show that info in the title\n4620 \n4621         Parameters\n4622         ----------\n4623         truncate : int, default: 50\n4624             maximum number of characters for title\n4625 \n4626         Returns\n4627         -------\n4628         title : string\n4629             Can be used for plot titles\n4630 \n4631         \"\"\"\n4632         one_dims = []\n4633         for dim, coord in self.coords.items():\n4634             if coord.size == 1:\n4635                 one_dims.append(\n4636                     \"{dim} = {v}{unit}\".format(\n4637                         dim=dim,\n4638                         v=format_item(coord.values),\n4639                         unit=_get_units_from_attrs(coord),\n4640                     )\n4641                 )\n4642 \n4643         title = \", \".join(one_dims)\n4644         if len(title) > truncate:\n4645             title = title[: (truncate - 3)] + \"...\"\n4646 \n4647         return title\n4648 \n4649     def diff(\n4650         self: T_DataArray,\n4651         dim: Hashable,\n4652         n: int = 1,\n4653         label: Literal[\"upper\", \"lower\"] = \"upper\",\n4654     ) -> T_DataArray:\n4655         \"\"\"Calculate the n-th order discrete difference along given axis.\n4656 \n4657         Parameters\n4658         ----------\n4659         dim : Hashable\n4660             Dimension over which to calculate the finite difference.\n4661         n : int, default: 1\n4662             The number of times values are differenced.\n4663         label : {\"upper\", \"lower\"}, default: \"upper\"\n4664             The new coordinate in dimension ``dim`` will have the\n4665             values of either the minuend's or subtrahend's coordinate\n4666             for values 'upper' and 'lower', respectively.\n4667 \n4668         Returns\n4669         -------\n4670         difference : DataArray\n4671             The n-th order finite difference of this object.\n4672 \n4673         Notes\n4674         -----\n4675         `n` matches numpy's behavior and is different from pandas' first argument named\n4676         `periods`.\n4677 \n4678         Examples\n4679         --------\n4680         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], [\"x\"])\n4681         >>> arr.diff(\"x\")\n4682         <xarray.DataArray (x: 3)>\n4683         array([0, 1, 0])\n4684         Coordinates:\n4685           * x        (x) int64 2 3 4\n4686         >>> arr.diff(\"x\", 2)\n4687         <xarray.DataArray (x: 2)>\n4688         array([ 1, -1])\n4689         Coordinates:\n4690           * x        (x) int64 3 4\n4691 \n4692         See Also\n4693         --------\n4694         DataArray.differentiate\n4695         \"\"\"\n4696         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)\n4697         return self._from_temp_dataset(ds)\n4698 \n4699     def shift(\n4700         self: T_DataArray,\n4701         shifts: Mapping[Any, int] | None = None,\n4702         fill_value: Any = dtypes.NA,\n4703         **shifts_kwargs: int,\n4704     ) -> T_DataArray:\n4705         \"\"\"Shift this DataArray by an offset along one or more dimensions.\n4706 \n4707         Only the data is moved; coordinates stay in place. This is consistent\n4708         with the behavior of ``shift`` in pandas.\n4709 \n4710         Values shifted from beyond array bounds will appear at one end of\n4711         each dimension, which are filled according to `fill_value`. For periodic\n4712         offsets instead see `roll`.\n4713 \n4714         Parameters\n4715         ----------\n4716         shifts : mapping of Hashable to int or None, optional\n4717             Integer offset to shift along each of the given dimensions.\n4718             Positive offsets shift to the right; negative offsets shift to the\n4719             left.\n4720         fill_value : scalar, optional\n4721             Value to use for newly missing values\n4722         **shifts_kwargs\n4723             The keyword arguments form of ``shifts``.\n4724             One of shifts or shifts_kwargs must be provided.\n4725 \n4726         Returns\n4727         -------\n4728         shifted : DataArray\n4729             DataArray with the same coordinates and attributes but shifted\n4730             data.\n4731 \n4732         See Also\n4733         --------\n4734         roll\n4735 \n4736         Examples\n4737         --------\n4738         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n4739         >>> arr.shift(x=1)\n4740         <xarray.DataArray (x: 3)>\n4741         array([nan,  5.,  6.])\n4742         Dimensions without coordinates: x\n4743         \"\"\"\n4744         variable = self.variable.shift(\n4745             shifts=shifts, fill_value=fill_value, **shifts_kwargs\n4746         )\n4747         return self._replace(variable=variable)\n4748 \n4749     def roll(\n4750         self: T_DataArray,\n4751         shifts: Mapping[Hashable, int] | None = None,\n4752         roll_coords: bool = False,\n4753         **shifts_kwargs: int,\n4754     ) -> T_DataArray:\n4755         \"\"\"Roll this array by an offset along one or more dimensions.\n4756 \n4757         Unlike shift, roll treats the given dimensions as periodic, so will not\n4758         create any missing values to be filled.\n4759 \n4760         Unlike shift, roll may rotate all variables, including coordinates\n4761         if specified. The direction of rotation is consistent with\n4762         :py:func:`numpy.roll`.\n4763 \n4764         Parameters\n4765         ----------\n4766         shifts : mapping of Hashable to int, optional\n4767             Integer offset to rotate each of the given dimensions.\n4768             Positive offsets roll to the right; negative offsets roll to the\n4769             left.\n4770         roll_coords : bool, default: False\n4771             Indicates whether to roll the coordinates by the offset too.\n4772         **shifts_kwargs : {dim: offset, ...}, optional\n4773             The keyword arguments form of ``shifts``.\n4774             One of shifts or shifts_kwargs must be provided.\n4775 \n4776         Returns\n4777         -------\n4778         rolled : DataArray\n4779             DataArray with the same attributes but rolled data and coordinates.\n4780 \n4781         See Also\n4782         --------\n4783         shift\n4784 \n4785         Examples\n4786         --------\n4787         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n4788         >>> arr.roll(x=1)\n4789         <xarray.DataArray (x: 3)>\n4790         array([7, 5, 6])\n4791         Dimensions without coordinates: x\n4792         \"\"\"\n4793         ds = self._to_temp_dataset().roll(\n4794             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs\n4795         )\n4796         return self._from_temp_dataset(ds)\n4797 \n4798     @property\n4799     def real(self: T_DataArray) -> T_DataArray:\n4800         \"\"\"\n4801         The real part of the array.\n4802 \n4803         See Also\n4804         --------\n4805         numpy.ndarray.real\n4806         \"\"\"\n4807         return self._replace(self.variable.real)\n4808 \n4809     @property\n4810     def imag(self: T_DataArray) -> T_DataArray:\n4811         \"\"\"\n4812         The imaginary part of the array.\n4813 \n4814         See Also\n4815         --------\n4816         numpy.ndarray.imag\n4817         \"\"\"\n4818         return self._replace(self.variable.imag)\n4819 \n4820     def dot(\n4821         self: T_DataArray,\n4822         other: T_DataArray,\n4823         dims: Dims = None,\n4824     ) -> T_DataArray:\n4825         \"\"\"Perform dot product of two DataArrays along their shared dims.\n4826 \n4827         Equivalent to taking taking tensordot over all shared dims.\n4828 \n4829         Parameters\n4830         ----------\n4831         other : DataArray\n4832             The other array with which the dot product is performed.\n4833         dims : ..., str, Iterable of Hashable or None, optional\n4834             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.\n4835             If not specified, then all the common dimensions are summed over.\n4836 \n4837         Returns\n4838         -------\n4839         result : DataArray\n4840             Array resulting from the dot product over all shared dimensions.\n4841 \n4842         See Also\n4843         --------\n4844         dot\n4845         numpy.tensordot\n4846 \n4847         Examples\n4848         --------\n4849         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))\n4850         >>> da = xr.DataArray(da_vals, dims=[\"x\", \"y\", \"z\"])\n4851         >>> dm_vals = np.arange(4)\n4852         >>> dm = xr.DataArray(dm_vals, dims=[\"z\"])\n4853 \n4854         >>> dm.dims\n4855         ('z',)\n4856 \n4857         >>> da.dims\n4858         ('x', 'y', 'z')\n4859 \n4860         >>> dot_result = da.dot(dm)\n4861         >>> dot_result.dims\n4862         ('x', 'y')\n4863 \n4864         \"\"\"\n4865         if isinstance(other, Dataset):\n4866             raise NotImplementedError(\n4867                 \"dot products are not yet supported with Dataset objects.\"\n4868             )\n4869         if not isinstance(other, DataArray):\n4870             raise TypeError(\"dot only operates on DataArrays.\")\n4871 \n4872         return computation.dot(self, other, dims=dims)\n4873 \n4874     # change type of self and return to T_DataArray once\n4875     # https://github.com/python/mypy/issues/12846 is resolved\n4876     def sortby(\n4877         self,\n4878         variables: Hashable | DataArray | Sequence[Hashable | DataArray],\n4879         ascending: bool = True,\n4880     ) -> DataArray:\n4881         \"\"\"Sort object by labels or values (along an axis).\n4882 \n4883         Sorts the dataarray, either along specified dimensions,\n4884         or according to values of 1-D dataarrays that share dimension\n4885         with calling object.\n4886 \n4887         If the input variables are dataarrays, then the dataarrays are aligned\n4888         (via left-join) to the calling object prior to sorting by cell values.\n4889         NaNs are sorted to the end, following Numpy convention.\n4890 \n4891         If multiple sorts along the same dimension is\n4892         given, numpy's lexsort is performed along that dimension:\n4893         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n4894         and the FIRST key in the sequence is used as the primary sort key,\n4895         followed by the 2nd key, etc.\n4896 \n4897         Parameters\n4898         ----------\n4899         variables : Hashable, DataArray, or sequence of Hashable or DataArray\n4900             1D DataArray objects or name(s) of 1D variable(s) in\n4901             coords whose values are used to sort this array.\n4902         ascending : bool, default: True\n4903             Whether to sort by ascending or descending order.\n4904 \n4905         Returns\n4906         -------\n4907         sorted : DataArray\n4908             A new dataarray where all the specified dims are sorted by dim\n4909             labels.\n4910 \n4911         See Also\n4912         --------\n4913         Dataset.sortby\n4914         numpy.sort\n4915         pandas.sort_values\n4916         pandas.sort_index\n4917 \n4918         Examples\n4919         --------\n4920         >>> da = xr.DataArray(\n4921         ...     np.random.rand(5),\n4922         ...     coords=[pd.date_range(\"1/1/2000\", periods=5)],\n4923         ...     dims=\"time\",\n4924         ... )\n4925         >>> da\n4926         <xarray.DataArray (time: 5)>\n4927         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])\n4928         Coordinates:\n4929           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05\n4930 \n4931         >>> da.sortby(da)\n4932         <xarray.DataArray (time: 5)>\n4933         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])\n4934         Coordinates:\n4935           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02\n4936         \"\"\"\n4937         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)\n4938         return self._from_temp_dataset(ds)\n4939 \n4940     def quantile(\n4941         self: T_DataArray,\n4942         q: ArrayLike,\n4943         dim: Dims = None,\n4944         method: QuantileMethods = \"linear\",\n4945         keep_attrs: bool | None = None,\n4946         skipna: bool | None = None,\n4947         interpolation: QuantileMethods | None = None,\n4948     ) -> T_DataArray:\n4949         \"\"\"Compute the qth quantile of the data along the specified dimension.\n4950 \n4951         Returns the qth quantiles(s) of the array elements.\n4952 \n4953         Parameters\n4954         ----------\n4955         q : float or array-like of float\n4956             Quantile to compute, which must be between 0 and 1 inclusive.\n4957         dim : str or Iterable of Hashable, optional\n4958             Dimension(s) over which to apply quantile.\n4959         method : str, default: \"linear\"\n4960             This optional parameter specifies the interpolation method to use when the\n4961             desired quantile lies between two data points. The options sorted by their R\n4962             type as summarized in the H&F paper [1]_ are:\n4963 \n4964                 1. \"inverted_cdf\" (*)\n4965                 2. \"averaged_inverted_cdf\" (*)\n4966                 3. \"closest_observation\" (*)\n4967                 4. \"interpolated_inverted_cdf\" (*)\n4968                 5. \"hazen\" (*)\n4969                 6. \"weibull\" (*)\n4970                 7. \"linear\"  (default)\n4971                 8. \"median_unbiased\" (*)\n4972                 9. \"normal_unbiased\" (*)\n4973 \n4974             The first three methods are discontiuous. The following discontinuous\n4975             variations of the default \"linear\" (7.) option are also available:\n4976 \n4977                 * \"lower\"\n4978                 * \"higher\"\n4979                 * \"midpoint\"\n4980                 * \"nearest\"\n4981 \n4982             See :py:func:`numpy.quantile` or [1]_ for details. The \"method\" argument\n4983             was previously called \"interpolation\", renamed in accordance with numpy\n4984             version 1.22.0.\n4985 \n4986             (*) These methods require numpy version 1.22 or newer.\n4987 \n4988         keep_attrs : bool or None, optional\n4989             If True, the dataset's attributes (`attrs`) will be copied from\n4990             the original object to the new one.  If False (default), the new\n4991             object will be returned without attributes.\n4992         skipna : bool or None, optional\n4993             If True, skip missing values (as marked by NaN). By default, only\n4994             skips missing values for float dtypes; other dtypes either do not\n4995             have a sentinel missing value (int) or skipna=True has not been\n4996             implemented (object, datetime64 or timedelta64).\n4997 \n4998         Returns\n4999         -------\n5000         quantiles : DataArray\n5001             If `q` is a single quantile, then the result\n5002             is a scalar. If multiple percentiles are given, first axis of\n5003             the result corresponds to the quantile and a quantile dimension\n5004             is added to the return array. The other dimensions are the\n5005             dimensions that remain after the reduction of the array.\n5006 \n5007         See Also\n5008         --------\n5009         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile\n5010 \n5011         Examples\n5012         --------\n5013         >>> da = xr.DataArray(\n5014         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],\n5015         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n5016         ...     dims=(\"x\", \"y\"),\n5017         ... )\n5018         >>> da.quantile(0)  # or da.quantile(0, dim=...)\n5019         <xarray.DataArray ()>\n5020         array(0.7)\n5021         Coordinates:\n5022             quantile  float64 0.0\n5023         >>> da.quantile(0, dim=\"x\")\n5024         <xarray.DataArray (y: 4)>\n5025         array([0.7, 4.2, 2.6, 1.5])\n5026         Coordinates:\n5027           * y         (y) float64 1.0 1.5 2.0 2.5\n5028             quantile  float64 0.0\n5029         >>> da.quantile([0, 0.5, 1])\n5030         <xarray.DataArray (quantile: 3)>\n5031         array([0.7, 3.4, 9.4])\n5032         Coordinates:\n5033           * quantile  (quantile) float64 0.0 0.5 1.0\n5034         >>> da.quantile([0, 0.5, 1], dim=\"x\")\n5035         <xarray.DataArray (quantile: 3, y: 4)>\n5036         array([[0.7 , 4.2 , 2.6 , 1.5 ],\n5037                [3.6 , 5.75, 6.  , 1.7 ],\n5038                [6.5 , 7.3 , 9.4 , 1.9 ]])\n5039         Coordinates:\n5040           * y         (y) float64 1.0 1.5 2.0 2.5\n5041           * quantile  (quantile) float64 0.0 0.5 1.0\n5042 \n5043         References\n5044         ----------\n5045         .. [1] R. J. Hyndman and Y. Fan,\n5046            \"Sample quantiles in statistical packages,\"\n5047            The American Statistician, 50(4), pp. 361-365, 1996\n5048         \"\"\"\n5049 \n5050         ds = self._to_temp_dataset().quantile(\n5051             q,\n5052             dim=dim,\n5053             keep_attrs=keep_attrs,\n5054             method=method,\n5055             skipna=skipna,\n5056             interpolation=interpolation,\n5057         )\n5058         return self._from_temp_dataset(ds)\n5059 \n5060     def rank(\n5061         self: T_DataArray,\n5062         dim: Hashable,\n5063         pct: bool = False,\n5064         keep_attrs: bool | None = None,\n5065     ) -> T_DataArray:\n5066         \"\"\"Ranks the data.\n5067 \n5068         Equal values are assigned a rank that is the average of the ranks that\n5069         would have been otherwise assigned to all of the values within that\n5070         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.\n5071 \n5072         NaNs in the input array are returned as NaNs.\n5073 \n5074         The `bottleneck` library is required.\n5075 \n5076         Parameters\n5077         ----------\n5078         dim : Hashable\n5079             Dimension over which to compute rank.\n5080         pct : bool, default: False\n5081             If True, compute percentage ranks, otherwise compute integer ranks.\n5082         keep_attrs : bool or None, optional\n5083             If True, the dataset's attributes (`attrs`) will be copied from\n5084             the original object to the new one.  If False (default), the new\n5085             object will be returned without attributes.\n5086 \n5087         Returns\n5088         -------\n5089         ranked : DataArray\n5090             DataArray with the same coordinates and dtype 'float64'.\n5091 \n5092         Examples\n5093         --------\n5094         >>> arr = xr.DataArray([5, 6, 7], dims=\"x\")\n5095         >>> arr.rank(\"x\")\n5096         <xarray.DataArray (x: 3)>\n5097         array([1., 2., 3.])\n5098         Dimensions without coordinates: x\n5099         \"\"\"\n5100 \n5101         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)\n5102         return self._from_temp_dataset(ds)\n5103 \n5104     def differentiate(\n5105         self: T_DataArray,\n5106         coord: Hashable,\n5107         edge_order: Literal[1, 2] = 1,\n5108         datetime_unit: DatetimeUnitOptions = None,\n5109     ) -> T_DataArray:\n5110         \"\"\" Differentiate the array with the second order accurate central\n5111         differences.\n5112 \n5113         .. note::\n5114             This feature is limited to simple cartesian geometry, i.e. coord\n5115             must be one dimensional.\n5116 \n5117         Parameters\n5118         ----------\n5119         coord : Hashable\n5120             The coordinate to be used to compute the gradient.\n5121         edge_order : {1, 2}, default: 1\n5122             N-th order accurate differences at the boundaries.\n5123         datetime_unit : {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n5124                          \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, optional\n5125             Unit to compute gradient. Only valid for datetime coordinate.\n5126 \n5127         Returns\n5128         -------\n5129         differentiated: DataArray\n5130 \n5131         See also\n5132         --------\n5133         numpy.gradient: corresponding numpy function\n5134 \n5135         Examples\n5136         --------\n5137 \n5138         >>> da = xr.DataArray(\n5139         ...     np.arange(12).reshape(4, 3),\n5140         ...     dims=[\"x\", \"y\"],\n5141         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n5142         ... )\n5143         >>> da\n5144         <xarray.DataArray (x: 4, y: 3)>\n5145         array([[ 0,  1,  2],\n5146                [ 3,  4,  5],\n5147                [ 6,  7,  8],\n5148                [ 9, 10, 11]])\n5149         Coordinates:\n5150           * x        (x) float64 0.0 0.1 1.1 1.2\n5151         Dimensions without coordinates: y\n5152         >>>\n5153         >>> da.differentiate(\"x\")\n5154         <xarray.DataArray (x: 4, y: 3)>\n5155         array([[30.        , 30.        , 30.        ],\n5156                [27.54545455, 27.54545455, 27.54545455],\n5157                [27.54545455, 27.54545455, 27.54545455],\n5158                [30.        , 30.        , 30.        ]])\n5159         Coordinates:\n5160           * x        (x) float64 0.0 0.1 1.1 1.2\n5161         Dimensions without coordinates: y\n5162         \"\"\"\n5163         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)\n5164         return self._from_temp_dataset(ds)\n5165 \n5166     # change type of self and return to T_DataArray once\n5167     # https://github.com/python/mypy/issues/12846 is resolved\n5168     def integrate(\n5169         self,\n5170         coord: Hashable | Sequence[Hashable] = None,\n5171         datetime_unit: DatetimeUnitOptions = None,\n5172     ) -> DataArray:\n5173         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n5174 \n5175         .. note::\n5176             This feature is limited to simple cartesian geometry, i.e. coord\n5177             must be one dimensional.\n5178 \n5179         Parameters\n5180         ----------\n5181         coord : Hashable, or sequence of Hashable\n5182             Coordinate(s) used for the integration.\n5183         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n5184                         'ps', 'fs', 'as', None}, optional\n5185             Specify the unit if a datetime coordinate is used.\n5186 \n5187         Returns\n5188         -------\n5189         integrated : DataArray\n5190 \n5191         See also\n5192         --------\n5193         Dataset.integrate\n5194         numpy.trapz : corresponding numpy function\n5195 \n5196         Examples\n5197         --------\n5198 \n5199         >>> da = xr.DataArray(\n5200         ...     np.arange(12).reshape(4, 3),\n5201         ...     dims=[\"x\", \"y\"],\n5202         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n5203         ... )\n5204         >>> da\n5205         <xarray.DataArray (x: 4, y: 3)>\n5206         array([[ 0,  1,  2],\n5207                [ 3,  4,  5],\n5208                [ 6,  7,  8],\n5209                [ 9, 10, 11]])\n5210         Coordinates:\n5211           * x        (x) float64 0.0 0.1 1.1 1.2\n5212         Dimensions without coordinates: y\n5213         >>>\n5214         >>> da.integrate(\"x\")\n5215         <xarray.DataArray (y: 3)>\n5216         array([5.4, 6.6, 7.8])\n5217         Dimensions without coordinates: y\n5218         \"\"\"\n5219         ds = self._to_temp_dataset().integrate(coord, datetime_unit)\n5220         return self._from_temp_dataset(ds)\n5221 \n5222     # change type of self and return to T_DataArray once\n5223     # https://github.com/python/mypy/issues/12846 is resolved\n5224     def cumulative_integrate(\n5225         self,\n5226         coord: Hashable | Sequence[Hashable] = None,\n5227         datetime_unit: DatetimeUnitOptions = None,\n5228     ) -> DataArray:\n5229         \"\"\"Integrate cumulatively along the given coordinate using the trapezoidal rule.\n5230 \n5231         .. note::\n5232             This feature is limited to simple cartesian geometry, i.e. coord\n5233             must be one dimensional.\n5234 \n5235             The first entry of the cumulative integral is always 0, in order to keep the\n5236             length of the dimension unchanged between input and output.\n5237 \n5238         Parameters\n5239         ----------\n5240         coord : Hashable, or sequence of Hashable\n5241             Coordinate(s) used for the integration.\n5242         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n5243                         'ps', 'fs', 'as', None}, optional\n5244             Specify the unit if a datetime coordinate is used.\n5245 \n5246         Returns\n5247         -------\n5248         integrated : DataArray\n5249 \n5250         See also\n5251         --------\n5252         Dataset.cumulative_integrate\n5253         scipy.integrate.cumulative_trapezoid : corresponding scipy function\n5254 \n5255         Examples\n5256         --------\n5257 \n5258         >>> da = xr.DataArray(\n5259         ...     np.arange(12).reshape(4, 3),\n5260         ...     dims=[\"x\", \"y\"],\n5261         ...     coords={\"x\": [0, 0.1, 1.1, 1.2]},\n5262         ... )\n5263         >>> da\n5264         <xarray.DataArray (x: 4, y: 3)>\n5265         array([[ 0,  1,  2],\n5266                [ 3,  4,  5],\n5267                [ 6,  7,  8],\n5268                [ 9, 10, 11]])\n5269         Coordinates:\n5270           * x        (x) float64 0.0 0.1 1.1 1.2\n5271         Dimensions without coordinates: y\n5272         >>>\n5273         >>> da.cumulative_integrate(\"x\")\n5274         <xarray.DataArray (x: 4, y: 3)>\n5275         array([[0.  , 0.  , 0.  ],\n5276                [0.15, 0.25, 0.35],\n5277                [4.65, 5.75, 6.85],\n5278                [5.4 , 6.6 , 7.8 ]])\n5279         Coordinates:\n5280           * x        (x) float64 0.0 0.1 1.1 1.2\n5281         Dimensions without coordinates: y\n5282         \"\"\"\n5283         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)\n5284         return self._from_temp_dataset(ds)\n5285 \n5286     def unify_chunks(self) -> DataArray:\n5287         \"\"\"Unify chunk size along all chunked dimensions of this DataArray.\n5288 \n5289         Returns\n5290         -------\n5291         DataArray with consistent chunk sizes for all dask-array variables\n5292 \n5293         See Also\n5294         --------\n5295         dask.array.core.unify_chunks\n5296         \"\"\"\n5297 \n5298         return unify_chunks(self)[0]\n5299 \n5300     def map_blocks(\n5301         self,\n5302         func: Callable[..., T_Xarray],\n5303         args: Sequence[Any] = (),\n5304         kwargs: Mapping[str, Any] | None = None,\n5305         template: DataArray | Dataset | None = None,\n5306     ) -> T_Xarray:\n5307         \"\"\"\n5308         Apply a function to each block of this DataArray.\n5309 \n5310         .. warning::\n5311             This method is experimental and its signature may change.\n5312 \n5313         Parameters\n5314         ----------\n5315         func : callable\n5316             User-provided function that accepts a DataArray as its first\n5317             parameter. The function will receive a subset or 'block' of this DataArray (see below),\n5318             corresponding to one chunk along each chunked dimension. ``func`` will be\n5319             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.\n5320 \n5321             This function must return either a single DataArray or a single Dataset.\n5322 \n5323             This function cannot add a new chunked dimension.\n5324         args : sequence\n5325             Passed to func after unpacking and subsetting any xarray objects by blocks.\n5326             xarray objects in args must be aligned with this object, otherwise an error is raised.\n5327         kwargs : mapping\n5328             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n5329             subset to blocks. Passing dask collections in kwargs is not allowed.\n5330         template : DataArray or Dataset, optional\n5331             xarray object representing the final result after compute is called. If not provided,\n5332             the function will be first run on mocked-up data, that looks like this object but\n5333             has sizes 0, to determine properties of the returned object such as dtype,\n5334             variable names, attributes, new dimensions and new indexes (if any).\n5335             ``template`` must be provided if the function changes the size of existing dimensions.\n5336             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n5337             ``attrs`` set by ``func`` will be ignored.\n5338 \n5339         Returns\n5340         -------\n5341         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n5342         function.\n5343 \n5344         Notes\n5345         -----\n5346         This function is designed for when ``func`` needs to manipulate a whole xarray object\n5347         subset to each block. Each block is loaded into memory. In the more common case where\n5348         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n5349 \n5350         If none of the variables in this object is backed by dask arrays, calling this function is\n5351         equivalent to calling ``func(obj, *args, **kwargs)``.\n5352 \n5353         See Also\n5354         --------\n5355         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n5356         xarray.DataArray.map_blocks\n5357 \n5358         Examples\n5359         --------\n5360         Calculate an anomaly from climatology using ``.groupby()``. Using\n5361         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n5362         its indices, and its methods like ``.groupby()``.\n5363 \n5364         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n5365         ...     gb = da.groupby(groupby_type)\n5366         ...     clim = gb.mean(dim=\"time\")\n5367         ...     return gb - clim\n5368         ...\n5369         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n5370         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n5371         >>> np.random.seed(123)\n5372         >>> array = xr.DataArray(\n5373         ...     np.random.rand(len(time)),\n5374         ...     dims=[\"time\"],\n5375         ...     coords={\"time\": time, \"month\": month},\n5376         ... ).chunk()\n5377         >>> array.map_blocks(calculate_anomaly, template=array).compute()\n5378         <xarray.DataArray (time: 24)>\n5379         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n5380                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n5381                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n5382                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n5383                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n5384         Coordinates:\n5385           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n5386             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n5387 \n5388         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n5389         to the function being applied in ``xr.map_blocks()``:\n5390 \n5391         >>> array.map_blocks(\n5392         ...     calculate_anomaly, kwargs={\"groupby_type\": \"time.year\"}, template=array\n5393         ... )  # doctest: +ELLIPSIS\n5394         <xarray.DataArray (time: 24)>\n5395         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>\n5396         Coordinates:\n5397           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n5398             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n5399         \"\"\"\n5400         from xarray.core.parallel import map_blocks\n5401 \n5402         return map_blocks(func, self, args, kwargs, template)\n5403 \n5404     def polyfit(\n5405         self,\n5406         dim: Hashable,\n5407         deg: int,\n5408         skipna: bool | None = None,\n5409         rcond: float | None = None,\n5410         w: Hashable | Any | None = None,\n5411         full: bool = False,\n5412         cov: bool | Literal[\"unscaled\"] = False,\n5413     ) -> Dataset:\n5414         \"\"\"\n5415         Least squares polynomial fit.\n5416 \n5417         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n5418         invalid values when `skipna = True`.\n5419 \n5420         Parameters\n5421         ----------\n5422         dim : Hashable\n5423             Coordinate along which to fit the polynomials.\n5424         deg : int\n5425             Degree of the fitting polynomial.\n5426         skipna : bool or None, optional\n5427             If True, removes all invalid values before fitting each 1D slices of the array.\n5428             Default is True if data is stored in a dask.array or if there is any\n5429             invalid values, False otherwise.\n5430         rcond : float or None, optional\n5431             Relative condition number to the fit.\n5432         w : Hashable, array-like or None, optional\n5433             Weights to apply to the y-coordinate of the sample points.\n5434             Can be an array-like object or the name of a coordinate in the dataset.\n5435         full : bool, default: False\n5436             Whether to return the residuals, matrix rank and singular values in addition\n5437             to the coefficients.\n5438         cov : bool or \"unscaled\", default: False\n5439             Whether to return to the covariance matrix in addition to the coefficients.\n5440             The matrix is not scaled if `cov='unscaled'`.\n5441 \n5442         Returns\n5443         -------\n5444         polyfit_results : Dataset\n5445             A single dataset which contains:\n5446 \n5447             polyfit_coefficients\n5448                 The coefficients of the best fit.\n5449             polyfit_residuals\n5450                 The residuals of the least-square computation (only included if `full=True`).\n5451                 When the matrix rank is deficient, np.nan is returned.\n5452             [dim]_matrix_rank\n5453                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n5454             [dim]_singular_value\n5455                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n5456             polyfit_covariance\n5457                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n5458 \n5459         See Also\n5460         --------\n5461         numpy.polyfit\n5462         numpy.polyval\n5463         xarray.polyval\n5464         \"\"\"\n5465         return self._to_temp_dataset().polyfit(\n5466             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov\n5467         )\n5468 \n5469     def pad(\n5470         self: T_DataArray,\n5471         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,\n5472         mode: PadModeOptions = \"constant\",\n5473         stat_length: int\n5474         | tuple[int, int]\n5475         | Mapping[Any, tuple[int, int]]\n5476         | None = None,\n5477         constant_values: float\n5478         | tuple[float, float]\n5479         | Mapping[Any, tuple[float, float]]\n5480         | None = None,\n5481         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,\n5482         reflect_type: PadReflectOptions = None,\n5483         keep_attrs: bool | None = None,\n5484         **pad_width_kwargs: Any,\n5485     ) -> T_DataArray:\n5486         \"\"\"Pad this array along one or more dimensions.\n5487 \n5488         .. warning::\n5489             This function is experimental and its behaviour is likely to change\n5490             especially regarding padding of dimension coordinates (or IndexVariables).\n5491 \n5492         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n5493         coordinates will be padded with the same mode, otherwise coordinates\n5494         are padded using the \"constant\" mode with fill_value dtypes.NA.\n5495 \n5496         Parameters\n5497         ----------\n5498         pad_width : mapping of Hashable to tuple of int\n5499             Mapping with the form of {dim: (pad_before, pad_after)}\n5500             describing the number of values padded along each dimension.\n5501             {dim: pad} is a shortcut for pad_before = pad_after = pad\n5502         mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \\\n5503             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n5504             How to pad the DataArray (taken from numpy docs):\n5505 \n5506             - \"constant\": Pads with a constant value.\n5507             - \"edge\": Pads with the edge values of array.\n5508             - \"linear_ramp\": Pads with the linear ramp between end_value and the\n5509               array edge value.\n5510             - \"maximum\": Pads with the maximum value of all or part of the\n5511               vector along each axis.\n5512             - \"mean\": Pads with the mean value of all or part of the\n5513               vector along each axis.\n5514             - \"median\": Pads with the median value of all or part of the\n5515               vector along each axis.\n5516             - \"minimum\": Pads with the minimum value of all or part of the\n5517               vector along each axis.\n5518             - \"reflect\": Pads with the reflection of the vector mirrored on\n5519               the first and last values of the vector along each axis.\n5520             - \"symmetric\": Pads with the reflection of the vector mirrored\n5521               along the edge of the array.\n5522             - \"wrap\": Pads with the wrap of the vector along the axis.\n5523               The first values are used to pad the end and the\n5524               end values are used to pad the beginning.\n5525 \n5526         stat_length : int, tuple or mapping of Hashable to tuple, default: None\n5527             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n5528             values at edge of each axis used to calculate the statistic value.\n5529             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n5530             statistic lengths along each dimension.\n5531             ((before, after),) yields same before and after statistic lengths\n5532             for each dimension.\n5533             (stat_length,) or int is a shortcut for before = after = statistic\n5534             length for all axes.\n5535             Default is ``None``, to use the entire axis.\n5536         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n5537             Used in 'constant'.  The values to set the padded values for each\n5538             axis.\n5539             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n5540             pad constants along each dimension.\n5541             ``((before, after),)`` yields same before and after constants for each\n5542             dimension.\n5543             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n5544             all dimensions.\n5545             Default is 0.\n5546         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0\n5547             Used in 'linear_ramp'.  The values used for the ending value of the\n5548             linear_ramp and that will form the edge of the padded array.\n5549             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n5550             end values along each dimension.\n5551             ``((before, after),)`` yields same before and after end values for each\n5552             axis.\n5553             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n5554             all axes.\n5555             Default is 0.\n5556         reflect_type : {\"even\", \"odd\", None}, optional\n5557             Used in \"reflect\", and \"symmetric\". The \"even\" style is the\n5558             default with an unaltered reflection around the edge value. For\n5559             the \"odd\" style, the extended part of the array is created by\n5560             subtracting the reflected values from two times the edge value.\n5561         keep_attrs : bool or None, optional\n5562             If True, the attributes (``attrs``) will be copied from the\n5563             original object to the new one. If False, the new object\n5564             will be returned without attributes.\n5565         **pad_width_kwargs\n5566             The keyword arguments form of ``pad_width``.\n5567             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n5568 \n5569         Returns\n5570         -------\n5571         padded : DataArray\n5572             DataArray with the padded coordinates and data.\n5573 \n5574         See Also\n5575         --------\n5576         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad\n5577 \n5578         Notes\n5579         -----\n5580         For ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n5581         promoted to ``float`` and padded with ``np.nan``.\n5582 \n5583         Padding coordinates will drop their corresponding index (if any) and will reset default\n5584         indexes for dimension coordinates.\n5585 \n5586         Examples\n5587         --------\n5588         >>> arr = xr.DataArray([5, 6, 7], coords=[(\"x\", [0, 1, 2])])\n5589         >>> arr.pad(x=(1, 2), constant_values=0)\n5590         <xarray.DataArray (x: 6)>\n5591         array([0, 5, 6, 7, 0, 0])\n5592         Coordinates:\n5593           * x        (x) float64 nan 0.0 1.0 2.0 nan nan\n5594 \n5595         >>> da = xr.DataArray(\n5596         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],\n5597         ...     dims=[\"x\", \"y\"],\n5598         ...     coords={\"x\": [0, 1], \"y\": [10, 20, 30, 40], \"z\": (\"x\", [100, 200])},\n5599         ... )\n5600         >>> da.pad(x=1)\n5601         <xarray.DataArray (x: 4, y: 4)>\n5602         array([[nan, nan, nan, nan],\n5603                [ 0.,  1.,  2.,  3.],\n5604                [10., 11., 12., 13.],\n5605                [nan, nan, nan, nan]])\n5606         Coordinates:\n5607           * x        (x) float64 nan 0.0 1.0 nan\n5608           * y        (y) int64 10 20 30 40\n5609             z        (x) float64 nan 100.0 200.0 nan\n5610 \n5611         Careful, ``constant_values`` are coerced to the data type of the array which may\n5612         lead to a loss of precision:\n5613 \n5614         >>> da.pad(x=1, constant_values=1.23456789)\n5615         <xarray.DataArray (x: 4, y: 4)>\n5616         array([[ 1,  1,  1,  1],\n5617                [ 0,  1,  2,  3],\n5618                [10, 11, 12, 13],\n5619                [ 1,  1,  1,  1]])\n5620         Coordinates:\n5621           * x        (x) float64 nan 0.0 1.0 nan\n5622           * y        (y) int64 10 20 30 40\n5623             z        (x) float64 nan 100.0 200.0 nan\n5624         \"\"\"\n5625         ds = self._to_temp_dataset().pad(\n5626             pad_width=pad_width,\n5627             mode=mode,\n5628             stat_length=stat_length,\n5629             constant_values=constant_values,\n5630             end_values=end_values,\n5631             reflect_type=reflect_type,\n5632             keep_attrs=keep_attrs,\n5633             **pad_width_kwargs,\n5634         )\n5635         return self._from_temp_dataset(ds)\n5636 \n5637     def idxmin(\n5638         self,\n5639         dim: Hashable | None = None,\n5640         skipna: bool | None = None,\n5641         fill_value: Any = dtypes.NA,\n5642         keep_attrs: bool | None = None,\n5643     ) -> DataArray:\n5644         \"\"\"Return the coordinate label of the minimum value along a dimension.\n5645 \n5646         Returns a new `DataArray` named after the dimension with the values of\n5647         the coordinate labels along that dimension corresponding to minimum\n5648         values along that dimension.\n5649 \n5650         In comparison to :py:meth:`~DataArray.argmin`, this returns the\n5651         coordinate label while :py:meth:`~DataArray.argmin` returns the index.\n5652 \n5653         Parameters\n5654         ----------\n5655         dim : str, optional\n5656             Dimension over which to apply `idxmin`.  This is optional for 1D\n5657             arrays, but required for arrays with 2 or more dimensions.\n5658         skipna : bool or None, default: None\n5659             If True, skip missing values (as marked by NaN). By default, only\n5660             skips missing values for ``float``, ``complex``, and ``object``\n5661             dtypes; other dtypes either do not have a sentinel missing value\n5662             (``int``) or ``skipna=True`` has not been implemented\n5663             (``datetime64`` or ``timedelta64``).\n5664         fill_value : Any, default: NaN\n5665             Value to be filled in case all of the values along a dimension are\n5666             null.  By default this is NaN.  The fill value and result are\n5667             automatically converted to a compatible dtype if possible.\n5668             Ignored if ``skipna`` is False.\n5669         keep_attrs : bool or None, optional\n5670             If True, the attributes (``attrs``) will be copied from the\n5671             original object to the new one. If False, the new object\n5672             will be returned without attributes.\n5673 \n5674         Returns\n5675         -------\n5676         reduced : DataArray\n5677             New `DataArray` object with `idxmin` applied to its data and the\n5678             indicated dimension removed.\n5679 \n5680         See Also\n5681         --------\n5682         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin\n5683 \n5684         Examples\n5685         --------\n5686         >>> array = xr.DataArray(\n5687         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n5688         ... )\n5689         >>> array.min()\n5690         <xarray.DataArray ()>\n5691         array(-2)\n5692         >>> array.argmin(...)\n5693         {'x': <xarray.DataArray ()>\n5694         array(4)}\n5695         >>> array.idxmin()\n5696         <xarray.DataArray 'x' ()>\n5697         array('e', dtype='<U1')\n5698 \n5699         >>> array = xr.DataArray(\n5700         ...     [\n5701         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n5702         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n5703         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n5704         ...     ],\n5705         ...     dims=[\"y\", \"x\"],\n5706         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n5707         ... )\n5708         >>> array.min(dim=\"x\")\n5709         <xarray.DataArray (y: 3)>\n5710         array([-2., -4.,  1.])\n5711         Coordinates:\n5712           * y        (y) int64 -1 0 1\n5713         >>> array.argmin(dim=\"x\")\n5714         <xarray.DataArray (y: 3)>\n5715         array([4, 0, 2])\n5716         Coordinates:\n5717           * y        (y) int64 -1 0 1\n5718         >>> array.idxmin(dim=\"x\")\n5719         <xarray.DataArray 'x' (y: 3)>\n5720         array([16.,  0.,  4.])\n5721         Coordinates:\n5722           * y        (y) int64 -1 0 1\n5723         \"\"\"\n5724         return computation._calc_idxminmax(\n5725             array=self,\n5726             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),\n5727             dim=dim,\n5728             skipna=skipna,\n5729             fill_value=fill_value,\n5730             keep_attrs=keep_attrs,\n5731         )\n5732 \n5733     def idxmax(\n5734         self,\n5735         dim: Hashable = None,\n5736         skipna: bool | None = None,\n5737         fill_value: Any = dtypes.NA,\n5738         keep_attrs: bool | None = None,\n5739     ) -> DataArray:\n5740         \"\"\"Return the coordinate label of the maximum value along a dimension.\n5741 \n5742         Returns a new `DataArray` named after the dimension with the values of\n5743         the coordinate labels along that dimension corresponding to maximum\n5744         values along that dimension.\n5745 \n5746         In comparison to :py:meth:`~DataArray.argmax`, this returns the\n5747         coordinate label while :py:meth:`~DataArray.argmax` returns the index.\n5748 \n5749         Parameters\n5750         ----------\n5751         dim : Hashable, optional\n5752             Dimension over which to apply `idxmax`.  This is optional for 1D\n5753             arrays, but required for arrays with 2 or more dimensions.\n5754         skipna : bool or None, default: None\n5755             If True, skip missing values (as marked by NaN). By default, only\n5756             skips missing values for ``float``, ``complex``, and ``object``\n5757             dtypes; other dtypes either do not have a sentinel missing value\n5758             (``int``) or ``skipna=True`` has not been implemented\n5759             (``datetime64`` or ``timedelta64``).\n5760         fill_value : Any, default: NaN\n5761             Value to be filled in case all of the values along a dimension are\n5762             null.  By default this is NaN.  The fill value and result are\n5763             automatically converted to a compatible dtype if possible.\n5764             Ignored if ``skipna`` is False.\n5765         keep_attrs : bool or None, optional\n5766             If True, the attributes (``attrs``) will be copied from the\n5767             original object to the new one. If False, the new object\n5768             will be returned without attributes.\n5769 \n5770         Returns\n5771         -------\n5772         reduced : DataArray\n5773             New `DataArray` object with `idxmax` applied to its data and the\n5774             indicated dimension removed.\n5775 \n5776         See Also\n5777         --------\n5778         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax\n5779 \n5780         Examples\n5781         --------\n5782         >>> array = xr.DataArray(\n5783         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n5784         ... )\n5785         >>> array.max()\n5786         <xarray.DataArray ()>\n5787         array(2)\n5788         >>> array.argmax(...)\n5789         {'x': <xarray.DataArray ()>\n5790         array(1)}\n5791         >>> array.idxmax()\n5792         <xarray.DataArray 'x' ()>\n5793         array('b', dtype='<U1')\n5794 \n5795         >>> array = xr.DataArray(\n5796         ...     [\n5797         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n5798         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n5799         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n5800         ...     ],\n5801         ...     dims=[\"y\", \"x\"],\n5802         ...     coords={\"y\": [-1, 0, 1], \"x\": np.arange(5.0) ** 2},\n5803         ... )\n5804         >>> array.max(dim=\"x\")\n5805         <xarray.DataArray (y: 3)>\n5806         array([2., 2., 1.])\n5807         Coordinates:\n5808           * y        (y) int64 -1 0 1\n5809         >>> array.argmax(dim=\"x\")\n5810         <xarray.DataArray (y: 3)>\n5811         array([0, 2, 2])\n5812         Coordinates:\n5813           * y        (y) int64 -1 0 1\n5814         >>> array.idxmax(dim=\"x\")\n5815         <xarray.DataArray 'x' (y: 3)>\n5816         array([0., 4., 4.])\n5817         Coordinates:\n5818           * y        (y) int64 -1 0 1\n5819         \"\"\"\n5820         return computation._calc_idxminmax(\n5821             array=self,\n5822             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),\n5823             dim=dim,\n5824             skipna=skipna,\n5825             fill_value=fill_value,\n5826             keep_attrs=keep_attrs,\n5827         )\n5828 \n5829     # change type of self and return to T_DataArray once\n5830     # https://github.com/python/mypy/issues/12846 is resolved\n5831     def argmin(\n5832         self,\n5833         dim: Dims = None,\n5834         axis: int | None = None,\n5835         keep_attrs: bool | None = None,\n5836         skipna: bool | None = None,\n5837     ) -> DataArray | dict[Hashable, DataArray]:\n5838         \"\"\"Index or indices of the minimum of the DataArray over one or more dimensions.\n5839 \n5840         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n5841         which can be passed directly to isel(). If a single str is passed to 'dim' then\n5842         returns a DataArray with dtype int.\n5843 \n5844         If there are multiple minima, the indices of the first one found will be\n5845         returned.\n5846 \n5847         Parameters\n5848         ----------\n5849         dim : \"...\", str, Iterable of Hashable or None, optional\n5850             The dimensions over which to find the minimum. By default, finds minimum over\n5851             all dimensions - for now returning an int for backward compatibility, but\n5852             this is deprecated, in future will return a dict with indices for all\n5853             dimensions; to return a dict with all dimensions now, pass '...'.\n5854         axis : int or None, optional\n5855             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n5856             can be supplied.\n5857         keep_attrs : bool or None, optional\n5858             If True, the attributes (`attrs`) will be copied from the original\n5859             object to the new one. If False, the new object will be\n5860             returned without attributes.\n5861         skipna : bool or None, optional\n5862             If True, skip missing values (as marked by NaN). By default, only\n5863             skips missing values for float dtypes; other dtypes either do not\n5864             have a sentinel missing value (int) or skipna=True has not been\n5865             implemented (object, datetime64 or timedelta64).\n5866 \n5867         Returns\n5868         -------\n5869         result : DataArray or dict of DataArray\n5870 \n5871         See Also\n5872         --------\n5873         Variable.argmin, DataArray.idxmin\n5874 \n5875         Examples\n5876         --------\n5877         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n5878         >>> array.min()\n5879         <xarray.DataArray ()>\n5880         array(-1)\n5881         >>> array.argmin(...)\n5882         {'x': <xarray.DataArray ()>\n5883         array(2)}\n5884         >>> array.isel(array.argmin(...))\n5885         <xarray.DataArray ()>\n5886         array(-1)\n5887 \n5888         >>> array = xr.DataArray(\n5889         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],\n5890         ...     dims=(\"x\", \"y\", \"z\"),\n5891         ... )\n5892         >>> array.min(dim=\"x\")\n5893         <xarray.DataArray (y: 3, z: 3)>\n5894         array([[ 1,  2,  1],\n5895                [ 2, -5,  1],\n5896                [ 2,  1,  1]])\n5897         Dimensions without coordinates: y, z\n5898         >>> array.argmin(dim=\"x\")\n5899         <xarray.DataArray (y: 3, z: 3)>\n5900         array([[1, 0, 0],\n5901                [1, 1, 1],\n5902                [0, 0, 1]])\n5903         Dimensions without coordinates: y, z\n5904         >>> array.argmin(dim=[\"x\"])\n5905         {'x': <xarray.DataArray (y: 3, z: 3)>\n5906         array([[1, 0, 0],\n5907                [1, 1, 1],\n5908                [0, 0, 1]])\n5909         Dimensions without coordinates: y, z}\n5910         >>> array.min(dim=(\"x\", \"z\"))\n5911         <xarray.DataArray (y: 3)>\n5912         array([ 1, -5,  1])\n5913         Dimensions without coordinates: y\n5914         >>> array.argmin(dim=[\"x\", \"z\"])\n5915         {'x': <xarray.DataArray (y: 3)>\n5916         array([0, 1, 0])\n5917         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n5918         array([2, 1, 1])\n5919         Dimensions without coordinates: y}\n5920         >>> array.isel(array.argmin(dim=[\"x\", \"z\"]))\n5921         <xarray.DataArray (y: 3)>\n5922         array([ 1, -5,  1])\n5923         Dimensions without coordinates: y\n5924         \"\"\"\n5925         result = self.variable.argmin(dim, axis, keep_attrs, skipna)\n5926         if isinstance(result, dict):\n5927             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n5928         else:\n5929             return self._replace_maybe_drop_dims(result)\n5930 \n5931     # change type of self and return to T_DataArray once\n5932     # https://github.com/python/mypy/issues/12846 is resolved\n5933     def argmax(\n5934         self,\n5935         dim: Dims = None,\n5936         axis: int | None = None,\n5937         keep_attrs: bool | None = None,\n5938         skipna: bool | None = None,\n5939     ) -> DataArray | dict[Hashable, DataArray]:\n5940         \"\"\"Index or indices of the maximum of the DataArray over one or more dimensions.\n5941 \n5942         If a sequence is passed to 'dim', then result returned as dict of DataArrays,\n5943         which can be passed directly to isel(). If a single str is passed to 'dim' then\n5944         returns a DataArray with dtype int.\n5945 \n5946         If there are multiple maxima, the indices of the first one found will be\n5947         returned.\n5948 \n5949         Parameters\n5950         ----------\n5951         dim : \"...\", str, Iterable of Hashable or None, optional\n5952             The dimensions over which to find the maximum. By default, finds maximum over\n5953             all dimensions - for now returning an int for backward compatibility, but\n5954             this is deprecated, in future will return a dict with indices for all\n5955             dimensions; to return a dict with all dimensions now, pass '...'.\n5956         axis : int or None, optional\n5957             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments\n5958             can be supplied.\n5959         keep_attrs : bool or None, optional\n5960             If True, the attributes (`attrs`) will be copied from the original\n5961             object to the new one. If False, the new object will be\n5962             returned without attributes.\n5963         skipna : bool or None, optional\n5964             If True, skip missing values (as marked by NaN). By default, only\n5965             skips missing values for float dtypes; other dtypes either do not\n5966             have a sentinel missing value (int) or skipna=True has not been\n5967             implemented (object, datetime64 or timedelta64).\n5968 \n5969         Returns\n5970         -------\n5971         result : DataArray or dict of DataArray\n5972 \n5973         See Also\n5974         --------\n5975         Variable.argmax, DataArray.idxmax\n5976 \n5977         Examples\n5978         --------\n5979         >>> array = xr.DataArray([0, 2, -1, 3], dims=\"x\")\n5980         >>> array.max()\n5981         <xarray.DataArray ()>\n5982         array(3)\n5983         >>> array.argmax(...)\n5984         {'x': <xarray.DataArray ()>\n5985         array(3)}\n5986         >>> array.isel(array.argmax(...))\n5987         <xarray.DataArray ()>\n5988         array(3)\n5989 \n5990         >>> array = xr.DataArray(\n5991         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],\n5992         ...     dims=(\"x\", \"y\", \"z\"),\n5993         ... )\n5994         >>> array.max(dim=\"x\")\n5995         <xarray.DataArray (y: 3, z: 3)>\n5996         array([[3, 3, 2],\n5997                [3, 5, 2],\n5998                [2, 3, 3]])\n5999         Dimensions without coordinates: y, z\n6000         >>> array.argmax(dim=\"x\")\n6001         <xarray.DataArray (y: 3, z: 3)>\n6002         array([[0, 1, 1],\n6003                [0, 1, 0],\n6004                [0, 1, 0]])\n6005         Dimensions without coordinates: y, z\n6006         >>> array.argmax(dim=[\"x\"])\n6007         {'x': <xarray.DataArray (y: 3, z: 3)>\n6008         array([[0, 1, 1],\n6009                [0, 1, 0],\n6010                [0, 1, 0]])\n6011         Dimensions without coordinates: y, z}\n6012         >>> array.max(dim=(\"x\", \"z\"))\n6013         <xarray.DataArray (y: 3)>\n6014         array([3, 5, 3])\n6015         Dimensions without coordinates: y\n6016         >>> array.argmax(dim=[\"x\", \"z\"])\n6017         {'x': <xarray.DataArray (y: 3)>\n6018         array([0, 1, 0])\n6019         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>\n6020         array([0, 1, 2])\n6021         Dimensions without coordinates: y}\n6022         >>> array.isel(array.argmax(dim=[\"x\", \"z\"]))\n6023         <xarray.DataArray (y: 3)>\n6024         array([3, 5, 3])\n6025         Dimensions without coordinates: y\n6026         \"\"\"\n6027         result = self.variable.argmax(dim, axis, keep_attrs, skipna)\n6028         if isinstance(result, dict):\n6029             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}\n6030         else:\n6031             return self._replace_maybe_drop_dims(result)\n6032 \n6033     def query(\n6034         self,\n6035         queries: Mapping[Any, Any] | None = None,\n6036         parser: QueryParserOptions = \"pandas\",\n6037         engine: QueryEngineOptions = None,\n6038         missing_dims: ErrorOptionsWithWarn = \"raise\",\n6039         **queries_kwargs: Any,\n6040     ) -> DataArray:\n6041         \"\"\"Return a new data array indexed along the specified\n6042         dimension(s), where the indexers are given as strings containing\n6043         Python expressions to be evaluated against the values in the array.\n6044 \n6045         Parameters\n6046         ----------\n6047         queries : dict-like or None, optional\n6048             A dict-like with keys matching dimensions and values given by strings\n6049             containing Python expressions to be evaluated against the data variables\n6050             in the dataset. The expressions will be evaluated using the pandas\n6051             eval() function, and can contain any valid Python expressions but cannot\n6052             contain any Python statements.\n6053         parser : {\"pandas\", \"python\"}, default: \"pandas\"\n6054             The parser to use to construct the syntax tree from the expression.\n6055             The default of 'pandas' parses code slightly different than standard\n6056             Python. Alternatively, you can parse an expression using the 'python'\n6057             parser to retain strict Python semantics.\n6058         engine : {\"python\", \"numexpr\", None}, default: None\n6059             The engine used to evaluate the expression. Supported engines are:\n6060 \n6061             - None: tries to use numexpr, falls back to python\n6062             - \"numexpr\": evaluates expressions using numexpr\n6063             - \"python\": performs operations as if you had eval\u2019d in top level python\n6064 \n6065         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n6066             What to do if dimensions that should be selected from are not present in the\n6067             DataArray:\n6068 \n6069             - \"raise\": raise an exception\n6070             - \"warn\": raise a warning, and ignore the missing dimensions\n6071             - \"ignore\": ignore the missing dimensions\n6072 \n6073         **queries_kwargs : {dim: query, ...}, optional\n6074             The keyword arguments form of ``queries``.\n6075             One of queries or queries_kwargs must be provided.\n6076 \n6077         Returns\n6078         -------\n6079         obj : DataArray\n6080             A new DataArray with the same contents as this dataset, indexed by\n6081             the results of the appropriate queries.\n6082 \n6083         See Also\n6084         --------\n6085         DataArray.isel\n6086         Dataset.query\n6087         pandas.eval\n6088 \n6089         Examples\n6090         --------\n6091         >>> da = xr.DataArray(np.arange(0, 5, 1), dims=\"x\", name=\"a\")\n6092         >>> da\n6093         <xarray.DataArray 'a' (x: 5)>\n6094         array([0, 1, 2, 3, 4])\n6095         Dimensions without coordinates: x\n6096         >>> da.query(x=\"a > 2\")\n6097         <xarray.DataArray 'a' (x: 2)>\n6098         array([3, 4])\n6099         Dimensions without coordinates: x\n6100         \"\"\"\n6101 \n6102         ds = self._to_dataset_whole(shallow_copy=True)\n6103         ds = ds.query(\n6104             queries=queries,\n6105             parser=parser,\n6106             engine=engine,\n6107             missing_dims=missing_dims,\n6108             **queries_kwargs,\n6109         )\n6110         return ds[self.name]\n6111 \n6112     def curvefit(\n6113         self,\n6114         coords: str | DataArray | Iterable[str | DataArray],\n6115         func: Callable[..., Any],\n6116         reduce_dims: Dims = None,\n6117         skipna: bool = True,\n6118         p0: dict[str, Any] | None = None,\n6119         bounds: dict[str, Any] | None = None,\n6120         param_names: Sequence[str] | None = None,\n6121         kwargs: dict[str, Any] | None = None,\n6122     ) -> Dataset:\n6123         \"\"\"\n6124         Curve fitting optimization for arbitrary functions.\n6125 \n6126         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n6127 \n6128         Parameters\n6129         ----------\n6130         coords : Hashable, DataArray, or sequence of DataArray or Hashable\n6131             Independent coordinate(s) over which to perform the curve fitting. Must share\n6132             at least one dimension with the calling object. When fitting multi-dimensional\n6133             functions, supply `coords` as a sequence in the same order as arguments in\n6134             `func`. To fit along existing dimensions of the calling object, `coords` can\n6135             also be specified as a str or sequence of strs.\n6136         func : callable\n6137             User specified function in the form `f(x, *params)` which returns a numpy\n6138             array of length `len(x)`. `params` are the fittable parameters which are optimized\n6139             by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n6140             coordinates, e.g. `f((x0, x1), *params)`.\n6141         reduce_dims : str, Iterable of Hashable or None, optional\n6142             Additional dimension(s) over which to aggregate while fitting. For example,\n6143             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n6144             aggregate all lat and lon points and fit the specified function along the\n6145             time dimension.\n6146         skipna : bool, default: True\n6147             Whether to skip missing values when fitting. Default is True.\n6148         p0 : dict-like or None, optional\n6149             Optional dictionary of parameter names to initial guesses passed to the\n6150             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n6151             be assigned initial values following the default scipy behavior.\n6152         bounds : dict-like or None, optional\n6153             Optional dictionary of parameter names to bounding values passed to the\n6154             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n6155             will be unbounded following the default scipy behavior.\n6156         param_names : sequence of Hashable or None, optional\n6157             Sequence of names for the fittable parameters of `func`. If not supplied,\n6158             this will be automatically determined by arguments of `func`. `param_names`\n6159             should be manually supplied when fitting a function that takes a variable\n6160             number of parameters.\n6161         **kwargs : optional\n6162             Additional keyword arguments to passed to scipy curve_fit.\n6163 \n6164         Returns\n6165         -------\n6166         curvefit_results : Dataset\n6167             A single dataset which contains:\n6168 \n6169             [var]_curvefit_coefficients\n6170                 The coefficients of the best fit.\n6171             [var]_curvefit_covariance\n6172                 The covariance matrix of the coefficient estimates.\n6173 \n6174         See Also\n6175         --------\n6176         DataArray.polyfit\n6177         scipy.optimize.curve_fit\n6178         \"\"\"\n6179         return self._to_temp_dataset().curvefit(\n6180             coords,\n6181             func,\n6182             reduce_dims=reduce_dims,\n6183             skipna=skipna,\n6184             p0=p0,\n6185             bounds=bounds,\n6186             param_names=param_names,\n6187             kwargs=kwargs,\n6188         )\n6189 \n6190     def drop_duplicates(\n6191         self: T_DataArray,\n6192         dim: Hashable | Iterable[Hashable],\n6193         keep: Literal[\"first\", \"last\", False] = \"first\",\n6194     ) -> T_DataArray:\n6195         \"\"\"Returns a new DataArray with duplicate dimension values removed.\n6196 \n6197         Parameters\n6198         ----------\n6199         dim : dimension label or labels\n6200             Pass `...` to drop duplicates along all dimensions.\n6201         keep : {\"first\", \"last\", False}, default: \"first\"\n6202             Determines which duplicates (if any) to keep.\n6203 \n6204             - ``\"first\"`` : Drop duplicates except for the first occurrence.\n6205             - ``\"last\"`` : Drop duplicates except for the last occurrence.\n6206             - False : Drop all duplicates.\n6207 \n6208         Returns\n6209         -------\n6210         DataArray\n6211 \n6212         See Also\n6213         --------\n6214         Dataset.drop_duplicates\n6215 \n6216         Examples\n6217         --------\n6218         >>> da = xr.DataArray(\n6219         ...     np.arange(25).reshape(5, 5),\n6220         ...     dims=(\"x\", \"y\"),\n6221         ...     coords={\"x\": np.array([0, 0, 1, 2, 3]), \"y\": np.array([0, 1, 2, 3, 3])},\n6222         ... )\n6223         >>> da\n6224         <xarray.DataArray (x: 5, y: 5)>\n6225         array([[ 0,  1,  2,  3,  4],\n6226                [ 5,  6,  7,  8,  9],\n6227                [10, 11, 12, 13, 14],\n6228                [15, 16, 17, 18, 19],\n6229                [20, 21, 22, 23, 24]])\n6230         Coordinates:\n6231           * x        (x) int64 0 0 1 2 3\n6232           * y        (y) int64 0 1 2 3 3\n6233 \n6234         >>> da.drop_duplicates(dim=\"x\")\n6235         <xarray.DataArray (x: 4, y: 5)>\n6236         array([[ 0,  1,  2,  3,  4],\n6237                [10, 11, 12, 13, 14],\n6238                [15, 16, 17, 18, 19],\n6239                [20, 21, 22, 23, 24]])\n6240         Coordinates:\n6241           * x        (x) int64 0 1 2 3\n6242           * y        (y) int64 0 1 2 3 3\n6243 \n6244         >>> da.drop_duplicates(dim=\"x\", keep=\"last\")\n6245         <xarray.DataArray (x: 4, y: 5)>\n6246         array([[ 5,  6,  7,  8,  9],\n6247                [10, 11, 12, 13, 14],\n6248                [15, 16, 17, 18, 19],\n6249                [20, 21, 22, 23, 24]])\n6250         Coordinates:\n6251           * x        (x) int64 0 1 2 3\n6252           * y        (y) int64 0 1 2 3 3\n6253 \n6254         Drop all duplicate dimension values:\n6255 \n6256         >>> da.drop_duplicates(dim=...)\n6257         <xarray.DataArray (x: 4, y: 4)>\n6258         array([[ 0,  1,  2,  3],\n6259                [10, 11, 12, 13],\n6260                [15, 16, 17, 18],\n6261                [20, 21, 22, 23]])\n6262         Coordinates:\n6263           * x        (x) int64 0 1 2 3\n6264           * y        (y) int64 0 1 2 3\n6265         \"\"\"\n6266         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)\n6267         return self._from_temp_dataset(deduplicated)\n6268 \n6269     def convert_calendar(\n6270         self,\n6271         calendar: str,\n6272         dim: str = \"time\",\n6273         align_on: str | None = None,\n6274         missing: Any | None = None,\n6275         use_cftime: bool | None = None,\n6276     ) -> DataArray:\n6277         \"\"\"Convert the DataArray to another calendar.\n6278 \n6279         Only converts the individual timestamps, does not modify any data except\n6280         in dropping invalid/surplus dates or inserting missing dates.\n6281 \n6282         If the source and target calendars are either no_leap, all_leap or a\n6283         standard type, only the type of the time array is modified.\n6284         When converting to a leap year from a non-leap year, the 29th of February\n6285         is removed from the array. In the other direction the 29th of February\n6286         will be missing in the output, unless `missing` is specified,\n6287         in which case that value is inserted.\n6288 \n6289         For conversions involving `360_day` calendars, see Notes.\n6290 \n6291         This method is safe to use with sub-daily data as it doesn't touch the\n6292         time part of the timestamps.\n6293 \n6294         Parameters\n6295         ---------\n6296         calendar : str\n6297             The target calendar name.\n6298         dim : str\n6299             Name of the time coordinate.\n6300         align_on : {None, 'date', 'year'}\n6301             Must be specified when either source or target is a `360_day` calendar,\n6302            ignored otherwise. See Notes.\n6303         missing : Optional[any]\n6304             By default, i.e. if the value is None, this method will simply attempt\n6305             to convert the dates in the source calendar to the same dates in the\n6306             target calendar, and drop any of those that are not possible to\n6307             represent.  If a value is provided, a new time coordinate will be\n6308             created in the target calendar with the same frequency as the original\n6309             time coordinate; for any dates that are not present in the source, the\n6310             data will be filled with this value.  Note that using this mode requires\n6311             that the source data have an inferable frequency; for more information\n6312             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n6313             target calendar combinations, this could result in many missing values, see notes.\n6314         use_cftime : boolean, optional\n6315             Whether to use cftime objects in the output, only used if `calendar`\n6316             is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n6317             If True, the new time axis uses cftime objects.\n6318             If None (default), it uses :py:class:`numpy.datetime64` values if the\n6319             date range permits it, and :py:class:`cftime.datetime` objects if not.\n6320             If False, it uses :py:class:`numpy.datetime64`  or fails.\n6321 \n6322         Returns\n6323         -------\n6324         DataArray\n6325             Copy of the dataarray with the time coordinate converted to the\n6326             target calendar. If 'missing' was None (default), invalid dates in\n6327             the new calendar are dropped, but missing dates are not inserted.\n6328             If `missing` was given, the new data is reindexed to have a time axis\n6329             with the same frequency as the source, but in the new calendar; any\n6330             missing datapoints are filled with `missing`.\n6331 \n6332         Notes\n6333         -----\n6334         Passing a value to `missing` is only usable if the source's time coordinate as an\n6335         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n6336         if the target coordinate, generated from this frequency, has dates equivalent to the\n6337         source. It is usually **not** appropriate to use this mode with:\n6338 \n6339         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n6340         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n6341             or 'mH' where 24 % m != 0).\n6342 \n6343         If one of the source or target calendars is `\"360_day\"`, `align_on` must\n6344         be specified and two options are offered.\n6345 \n6346         - \"year\"\n6347             The dates are translated according to their relative position in the year,\n6348             ignoring their original month and day information, meaning that the\n6349             missing/surplus days are added/removed at regular intervals.\n6350 \n6351             From a `360_day` to a standard calendar, the output will be missing the\n6352             following dates (day of year in parentheses):\n6353 \n6354             To a leap year:\n6355                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n6356                 September 31st (275) and November 30th (335).\n6357             To a non-leap year:\n6358                 February 6th (36), April 19th (109), July 2nd (183),\n6359                 September 12th (255), November 25th (329).\n6360 \n6361             From a standard calendar to a `\"360_day\"`, the following dates in the\n6362             source array will be dropped:\n6363 \n6364             From a leap year:\n6365                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n6366                 September 31st (275), December 1st (336)\n6367             From a non-leap year:\n6368                 February 6th (37), April 20th (110), July 2nd (183),\n6369                 September 13th (256), November 25th (329)\n6370 \n6371             This option is best used on daily and subdaily data.\n6372 \n6373         - \"date\"\n6374             The month/day information is conserved and invalid dates are dropped\n6375             from the output. This means that when converting from a `\"360_day\"` to a\n6376             standard calendar, all 31st (Jan, March, May, July, August, October and\n6377             December) will be missing as there is no equivalent dates in the\n6378             `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n6379             will be dropped as there are no equivalent dates in a standard calendar.\n6380 \n6381             This option is best used with data on a frequency coarser than daily.\n6382         \"\"\"\n6383         return convert_calendar(\n6384             self,\n6385             calendar,\n6386             dim=dim,\n6387             align_on=align_on,\n6388             missing=missing,\n6389             use_cftime=use_cftime,\n6390         )\n6391 \n6392     def interp_calendar(\n6393         self,\n6394         target: pd.DatetimeIndex | CFTimeIndex | DataArray,\n6395         dim: str = \"time\",\n6396     ) -> DataArray:\n6397         \"\"\"Interpolates the DataArray to another calendar based on decimal year measure.\n6398 \n6399         Each timestamp in `source` and `target` are first converted to their decimal\n6400         year equivalent then `source` is interpolated on the target coordinate.\n6401         The decimal year of a timestamp is its year plus its sub-year component\n6402         converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n6403         2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n6404 \n6405         This method should only be used when the time (HH:MM:SS) information of\n6406         time coordinate is not important.\n6407 \n6408         Parameters\n6409         ----------\n6410         target: DataArray or DatetimeIndex or CFTimeIndex\n6411             The target time coordinate of a valid dtype\n6412             (np.datetime64 or cftime objects)\n6413         dim : str\n6414             The time coordinate name.\n6415 \n6416         Return\n6417         ------\n6418         DataArray\n6419             The source interpolated on the decimal years of target,\n6420         \"\"\"\n6421         return interp_calendar(self, target, dim=dim)\n6422 \n6423     def groupby(\n6424         self,\n6425         group: Hashable | DataArray | IndexVariable,\n6426         squeeze: bool = True,\n6427         restore_coord_dims: bool = False,\n6428     ) -> DataArrayGroupBy:\n6429         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n6430 \n6431         Parameters\n6432         ----------\n6433         group : Hashable, DataArray or IndexVariable\n6434             Array whose unique values should be used to group this array. If a\n6435             Hashable, must be the name of a coordinate contained in this dataarray.\n6436         squeeze : bool, default: True\n6437             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n6438             controls whether the subarrays have a dimension of length 1 along\n6439             that dimension or if the dimension is squeezed out.\n6440         restore_coord_dims : bool, default: False\n6441             If True, also restore the dimension order of multi-dimensional\n6442             coordinates.\n6443 \n6444         Returns\n6445         -------\n6446         grouped : DataArrayGroupBy\n6447             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n6448             iterated over in the form of `(unique_value, grouped_array)` pairs.\n6449 \n6450         Examples\n6451         --------\n6452         Calculate daily anomalies for daily data:\n6453 \n6454         >>> da = xr.DataArray(\n6455         ...     np.linspace(0, 1826, num=1827),\n6456         ...     coords=[pd.date_range(\"2000-01-01\", \"2004-12-31\", freq=\"D\")],\n6457         ...     dims=\"time\",\n6458         ... )\n6459         >>> da\n6460         <xarray.DataArray (time: 1827)>\n6461         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,\n6462                1.826e+03])\n6463         Coordinates:\n6464           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n6465         >>> da.groupby(\"time.dayofyear\") - da.groupby(\"time.dayofyear\").mean(\"time\")\n6466         <xarray.DataArray (time: 1827)>\n6467         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])\n6468         Coordinates:\n6469           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31\n6470             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366\n6471 \n6472         See Also\n6473         --------\n6474         :ref:`groupby`\n6475             Users guide explanation of how to group and bin data.\n6476         DataArray.groupby_bins\n6477         Dataset.groupby\n6478         core.groupby.DataArrayGroupBy\n6479         pandas.DataFrame.groupby\n6480         \"\"\"\n6481         from xarray.core.groupby import (\n6482             DataArrayGroupBy,\n6483             ResolvedUniqueGrouper,\n6484             UniqueGrouper,\n6485             _validate_groupby_squeeze,\n6486         )\n6487 \n6488         _validate_groupby_squeeze(squeeze)\n6489         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)\n6490         return DataArrayGroupBy(\n6491             self,\n6492             (rgrouper,),\n6493             squeeze=squeeze,\n6494             restore_coord_dims=restore_coord_dims,\n6495         )\n6496 \n6497     def groupby_bins(\n6498         self,\n6499         group: Hashable | DataArray | IndexVariable,\n6500         bins: ArrayLike,\n6501         right: bool = True,\n6502         labels: ArrayLike | Literal[False] | None = None,\n6503         precision: int = 3,\n6504         include_lowest: bool = False,\n6505         squeeze: bool = True,\n6506         restore_coord_dims: bool = False,\n6507     ) -> DataArrayGroupBy:\n6508         \"\"\"Returns a DataArrayGroupBy object for performing grouped operations.\n6509 \n6510         Rather than using all unique values of `group`, the values are discretized\n6511         first by applying `pandas.cut` [1]_ to `group`.\n6512 \n6513         Parameters\n6514         ----------\n6515         group : Hashable, DataArray or IndexVariable\n6516             Array whose binned values should be used to group this array. If a\n6517             Hashable, must be the name of a coordinate contained in this dataarray.\n6518         bins : int or array-like\n6519             If bins is an int, it defines the number of equal-width bins in the\n6520             range of x. However, in this case, the range of x is extended by .1%\n6521             on each side to include the min or max values of x. If bins is a\n6522             sequence it defines the bin edges allowing for non-uniform bin\n6523             width. No extension of the range of x is done in this case.\n6524         right : bool, default: True\n6525             Indicates whether the bins include the rightmost edge or not. If\n6526             right == True (the default), then the bins [1,2,3,4] indicate\n6527             (1,2], (2,3], (3,4].\n6528         labels : array-like, False or None, default: None\n6529             Used as labels for the resulting bins. Must be of the same length as\n6530             the resulting bins. If False, string bin labels are assigned by\n6531             `pandas.cut`.\n6532         precision : int, default: 3\n6533             The precision at which to store and display the bins labels.\n6534         include_lowest : bool, default: False\n6535             Whether the first interval should be left-inclusive or not.\n6536         squeeze : bool, default: True\n6537             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n6538             controls whether the subarrays have a dimension of length 1 along\n6539             that dimension or if the dimension is squeezed out.\n6540         restore_coord_dims : bool, default: False\n6541             If True, also restore the dimension order of multi-dimensional\n6542             coordinates.\n6543 \n6544         Returns\n6545         -------\n6546         grouped : DataArrayGroupBy\n6547             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be\n6548             iterated over in the form of `(unique_value, grouped_array)` pairs.\n6549             The name of the group has the added suffix `_bins` in order to\n6550             distinguish it from the original variable.\n6551 \n6552         See Also\n6553         --------\n6554         :ref:`groupby`\n6555             Users guide explanation of how to group and bin data.\n6556         DataArray.groupby\n6557         Dataset.groupby_bins\n6558         core.groupby.DataArrayGroupBy\n6559         pandas.DataFrame.groupby\n6560 \n6561         References\n6562         ----------\n6563         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n6564         \"\"\"\n6565         from xarray.core.groupby import (\n6566             BinGrouper,\n6567             DataArrayGroupBy,\n6568             ResolvedBinGrouper,\n6569             _validate_groupby_squeeze,\n6570         )\n6571 \n6572         _validate_groupby_squeeze(squeeze)\n6573         grouper = BinGrouper(\n6574             bins=bins,\n6575             cut_kwargs={\n6576                 \"right\": right,\n6577                 \"labels\": labels,\n6578                 \"precision\": precision,\n6579                 \"include_lowest\": include_lowest,\n6580             },\n6581         )\n6582         rgrouper = ResolvedBinGrouper(grouper, group, self)\n6583 \n6584         return DataArrayGroupBy(\n6585             self,\n6586             (rgrouper,),\n6587             squeeze=squeeze,\n6588             restore_coord_dims=restore_coord_dims,\n6589         )\n6590 \n6591     def weighted(self, weights: DataArray) -> DataArrayWeighted:\n6592         \"\"\"\n6593         Weighted DataArray operations.\n6594 \n6595         Parameters\n6596         ----------\n6597         weights : DataArray\n6598             An array of weights associated with the values in this Dataset.\n6599             Each value in the data contributes to the reduction operation\n6600             according to its associated weight.\n6601 \n6602         Notes\n6603         -----\n6604         ``weights`` must be a DataArray and cannot contain missing values.\n6605         Missing values can be replaced by ``weights.fillna(0)``.\n6606 \n6607         Returns\n6608         -------\n6609         core.weighted.DataArrayWeighted\n6610 \n6611         See Also\n6612         --------\n6613         Dataset.weighted\n6614         \"\"\"\n6615         from xarray.core.weighted import DataArrayWeighted\n6616 \n6617         return DataArrayWeighted(self, weights)\n6618 \n6619     def rolling(\n6620         self,\n6621         dim: Mapping[Any, int] | None = None,\n6622         min_periods: int | None = None,\n6623         center: bool | Mapping[Any, bool] = False,\n6624         **window_kwargs: int,\n6625     ) -> DataArrayRolling:\n6626         \"\"\"\n6627         Rolling window object for DataArrays.\n6628 \n6629         Parameters\n6630         ----------\n6631         dim : dict, optional\n6632             Mapping from the dimension name to create the rolling iterator\n6633             along (e.g. `time`) to its moving window size.\n6634         min_periods : int or None, default: None\n6635             Minimum number of observations in window required to have a value\n6636             (otherwise result is NA). The default, None, is equivalent to\n6637             setting min_periods equal to the size of the window.\n6638         center : bool or Mapping to int, default: False\n6639             Set the labels at the center of the window.\n6640         **window_kwargs : optional\n6641             The keyword arguments form of ``dim``.\n6642             One of dim or window_kwargs must be provided.\n6643 \n6644         Returns\n6645         -------\n6646         core.rolling.DataArrayRolling\n6647 \n6648         Examples\n6649         --------\n6650         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:\n6651 \n6652         >>> da = xr.DataArray(\n6653         ...     np.linspace(0, 11, num=12),\n6654         ...     coords=[\n6655         ...         pd.date_range(\n6656         ...             \"1999-12-15\",\n6657         ...             periods=12,\n6658         ...             freq=pd.DateOffset(months=1),\n6659         ...         )\n6660         ...     ],\n6661         ...     dims=\"time\",\n6662         ... )\n6663         >>> da\n6664         <xarray.DataArray (time: 12)>\n6665         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n6666         Coordinates:\n6667           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n6668         >>> da.rolling(time=3, center=True).mean()\n6669         <xarray.DataArray (time: 12)>\n6670         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])\n6671         Coordinates:\n6672           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n6673 \n6674         Remove the NaNs using ``dropna()``:\n6675 \n6676         >>> da.rolling(time=3, center=True).mean().dropna(\"time\")\n6677         <xarray.DataArray (time: 10)>\n6678         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n6679         Coordinates:\n6680           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15\n6681 \n6682         See Also\n6683         --------\n6684         core.rolling.DataArrayRolling\n6685         Dataset.rolling\n6686         \"\"\"\n6687         from xarray.core.rolling import DataArrayRolling\n6688 \n6689         dim = either_dict_or_kwargs(dim, window_kwargs, \"rolling\")\n6690         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)\n6691 \n6692     def coarsen(\n6693         self,\n6694         dim: Mapping[Any, int] | None = None,\n6695         boundary: CoarsenBoundaryOptions = \"exact\",\n6696         side: SideOptions | Mapping[Any, SideOptions] = \"left\",\n6697         coord_func: str | Callable | Mapping[Any, str | Callable] = \"mean\",\n6698         **window_kwargs: int,\n6699     ) -> DataArrayCoarsen:\n6700         \"\"\"\n6701         Coarsen object for DataArrays.\n6702 \n6703         Parameters\n6704         ----------\n6705         dim : mapping of hashable to int, optional\n6706             Mapping from the dimension name to the window size.\n6707         boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n6708             If 'exact', a ValueError will be raised if dimension size is not a\n6709             multiple of the window size. If 'trim', the excess entries are\n6710             dropped. If 'pad', NA will be padded.\n6711         side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n6712         coord_func : str or mapping of hashable to str, default: \"mean\"\n6713             function (name) that is applied to the coordinates,\n6714             or a mapping from coordinate name to function (name).\n6715 \n6716         Returns\n6717         -------\n6718         core.rolling.DataArrayCoarsen\n6719 \n6720         Examples\n6721         --------\n6722         Coarsen the long time series by averaging over every three days.\n6723 \n6724         >>> da = xr.DataArray(\n6725         ...     np.linspace(0, 364, num=364),\n6726         ...     dims=\"time\",\n6727         ...     coords={\"time\": pd.date_range(\"1999-12-15\", periods=364)},\n6728         ... )\n6729         >>> da  # +doctest: ELLIPSIS\n6730         <xarray.DataArray (time: 364)>\n6731         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,\n6732                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,\n6733                  8.02203857,   9.02479339,  10.02754821,  11.03030303,\n6734         ...\n6735                356.98071625, 357.98347107, 358.9862259 , 359.98898072,\n6736                360.99173554, 361.99449036, 362.99724518, 364.        ])\n6737         Coordinates:\n6738           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12\n6739         >>> da.coarsen(time=3, boundary=\"trim\").mean()  # +doctest: ELLIPSIS\n6740         <xarray.DataArray (time: 121)>\n6741         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,\n6742                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,\n6743                 25.06887052,  28.07713499,  31.08539945,  34.09366391,\n6744         ...\n6745                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,\n6746                361.99449036])\n6747         Coordinates:\n6748           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10\n6749         >>>\n6750 \n6751         See Also\n6752         --------\n6753         core.rolling.DataArrayCoarsen\n6754         Dataset.coarsen\n6755         \"\"\"\n6756         from xarray.core.rolling import DataArrayCoarsen\n6757 \n6758         dim = either_dict_or_kwargs(dim, window_kwargs, \"coarsen\")\n6759         return DataArrayCoarsen(\n6760             self,\n6761             dim,\n6762             boundary=boundary,\n6763             side=side,\n6764             coord_func=coord_func,\n6765         )\n6766 \n6767     def resample(\n6768         self,\n6769         indexer: Mapping[Any, str] | None = None,\n6770         skipna: bool | None = None,\n6771         closed: SideOptions | None = None,\n6772         label: SideOptions | None = None,\n6773         base: int | None = None,\n6774         offset: pd.Timedelta | datetime.timedelta | str | None = None,\n6775         origin: str | DatetimeLike = \"start_day\",\n6776         keep_attrs: bool | None = None,\n6777         loffset: datetime.timedelta | str | None = None,\n6778         restore_coord_dims: bool | None = None,\n6779         **indexer_kwargs: str,\n6780     ) -> DataArrayResample:\n6781         \"\"\"Returns a Resample object for performing resampling operations.\n6782 \n6783         Handles both downsampling and upsampling. The resampled\n6784         dimension must be a datetime-like coordinate. If any intervals\n6785         contain no values from the original object, they will be given\n6786         the value ``NaN``.\n6787 \n6788         Parameters\n6789         ----------\n6790         indexer : Mapping of Hashable to str, optional\n6791             Mapping from the dimension name to resample frequency [1]_. The\n6792             dimension must be datetime-like.\n6793         skipna : bool, optional\n6794             Whether to skip missing values when aggregating in downsampling.\n6795         closed : {\"left\", \"right\"}, optional\n6796             Side of each interval to treat as closed.\n6797         label : {\"left\", \"right\"}, optional\n6798             Side of each interval to use for labeling.\n6799         base : int, optional\n6800             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n6801             aggregated intervals. For example, for \"24H\" frequency, base could\n6802             range from 0 through 23.\n6803         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'\n6804             The datetime on which to adjust the grouping. The timezone of origin\n6805             must match the timezone of the index.\n6806 \n6807             If a datetime is not used, these values are also supported:\n6808             - 'epoch': `origin` is 1970-01-01\n6809             - 'start': `origin` is the first value of the timeseries\n6810             - 'start_day': `origin` is the first day at midnight of the timeseries\n6811             - 'end': `origin` is the last value of the timeseries\n6812             - 'end_day': `origin` is the ceiling midnight of the last day\n6813         offset : pd.Timedelta, datetime.timedelta, or str, default is None\n6814             An offset timedelta added to the origin.\n6815         loffset : timedelta or str, optional\n6816             Offset used to adjust the resampled time labels. Some pandas date\n6817             offset strings are supported.\n6818         restore_coord_dims : bool, optional\n6819             If True, also restore the dimension order of multi-dimensional\n6820             coordinates.\n6821         **indexer_kwargs : str\n6822             The keyword arguments form of ``indexer``.\n6823             One of indexer or indexer_kwargs must be provided.\n6824 \n6825         Returns\n6826         -------\n6827         resampled : core.resample.DataArrayResample\n6828             This object resampled.\n6829 \n6830         Examples\n6831         --------\n6832         Downsample monthly time-series data to seasonal data:\n6833 \n6834         >>> da = xr.DataArray(\n6835         ...     np.linspace(0, 11, num=12),\n6836         ...     coords=[\n6837         ...         pd.date_range(\n6838         ...             \"1999-12-15\",\n6839         ...             periods=12,\n6840         ...             freq=pd.DateOffset(months=1),\n6841         ...         )\n6842         ...     ],\n6843         ...     dims=\"time\",\n6844         ... )\n6845         >>> da\n6846         <xarray.DataArray (time: 12)>\n6847         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])\n6848         Coordinates:\n6849           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15\n6850         >>> da.resample(time=\"QS-DEC\").mean()\n6851         <xarray.DataArray (time: 4)>\n6852         array([ 1.,  4.,  7., 10.])\n6853         Coordinates:\n6854           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01\n6855 \n6856         Upsample monthly time-series data to daily data:\n6857 \n6858         >>> da.resample(time=\"1D\").interpolate(\"linear\")  # +doctest: ELLIPSIS\n6859         <xarray.DataArray (time: 337)>\n6860         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,\n6861                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,\n6862                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,\n6863         ...\n6864                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,\n6865                10.96774194, 11.        ])\n6866         Coordinates:\n6867           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n6868 \n6869         Limit scope of upsampling method\n6870 \n6871         >>> da.resample(time=\"1D\").nearest(tolerance=\"1D\")\n6872         <xarray.DataArray (time: 337)>\n6873         array([ 0.,  0., nan, ..., nan, 11., 11.])\n6874         Coordinates:\n6875           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15\n6876 \n6877         See Also\n6878         --------\n6879         Dataset.resample\n6880         pandas.Series.resample\n6881         pandas.DataFrame.resample\n6882 \n6883         References\n6884         ----------\n6885         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n6886         \"\"\"\n6887         from xarray.core.resample import DataArrayResample\n6888 \n6889         return self._resample(\n6890             resample_cls=DataArrayResample,\n6891             indexer=indexer,\n6892             skipna=skipna,\n6893             closed=closed,\n6894             label=label,\n6895             base=base,\n6896             offset=offset,\n6897             origin=origin,\n6898             keep_attrs=keep_attrs,\n6899             loffset=loffset,\n6900             restore_coord_dims=restore_coord_dims,\n6901             **indexer_kwargs,\n6902         )\n6903 \n6904     def to_dask_dataframe(\n6905         self,\n6906         dim_order: Sequence[Hashable] | None = None,\n6907         set_index: bool = False,\n6908     ) -> DaskDataFrame:\n6909         \"\"\"Convert this array into a dask.dataframe.DataFrame.\n6910 \n6911         Parameters\n6912         ----------\n6913         dim_order : Sequence of Hashable or None , optional\n6914             Hierarchical dimension order for the resulting dataframe.\n6915             Array content is transposed to this order and then written out as flat\n6916             vectors in contiguous order, so the last dimension in this list\n6917             will be contiguous in the resulting DataFrame. This has a major influence\n6918             on which operations are efficient on the resulting dask dataframe.\n6919         set_index : bool, default: False\n6920             If set_index=True, the dask DataFrame is indexed by this dataset's\n6921             coordinate. Since dask DataFrames do not support multi-indexes,\n6922             set_index only works if the dataset only contains one dimension.\n6923 \n6924         Returns\n6925         -------\n6926         dask.dataframe.DataFrame\n6927 \n6928         Examples\n6929         --------\n6930         >>> da = xr.DataArray(\n6931         ...     np.arange(4 * 2 * 2).reshape(4, 2, 2),\n6932         ...     dims=(\"time\", \"lat\", \"lon\"),\n6933         ...     coords={\n6934         ...         \"time\": np.arange(4),\n6935         ...         \"lat\": [-30, -20],\n6936         ...         \"lon\": [120, 130],\n6937         ...     },\n6938         ...     name=\"eg_dataarray\",\n6939         ...     attrs={\"units\": \"Celsius\", \"description\": \"Random temperature data\"},\n6940         ... )\n6941         >>> da.to_dask_dataframe([\"lat\", \"lon\", \"time\"]).compute()\n6942             lat  lon  time  eg_dataarray\n6943         0   -30  120     0             0\n6944         1   -30  120     1             4\n6945         2   -30  120     2             8\n6946         3   -30  120     3            12\n6947         4   -30  130     0             1\n6948         5   -30  130     1             5\n6949         6   -30  130     2             9\n6950         7   -30  130     3            13\n6951         8   -20  120     0             2\n6952         9   -20  120     1             6\n6953         10  -20  120     2            10\n6954         11  -20  120     3            14\n6955         12  -20  130     0             3\n6956         13  -20  130     1             7\n6957         14  -20  130     2            11\n6958         15  -20  130     3            15\n6959         \"\"\"\n6960         if self.name is None:\n6961             raise ValueError(\n6962                 \"Cannot convert an unnamed DataArray to a \"\n6963                 \"dask dataframe : use the ``.rename`` method to assign a name.\"\n6964             )\n6965         name = self.name\n6966         ds = self._to_dataset_whole(name, shallow_copy=False)\n6967         return ds.to_dask_dataframe(dim_order, set_index)\n6968 \n6969     # this needs to be at the end, or mypy will confuse with `str`\n6970     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names\n6971     str = utils.UncachedAccessor(StringAccessor[\"DataArray\"])\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataset.py",
        "start": 898,
        "end": 918,
        "class_name": "Dataset",
        "method_name": "compute",
        "code": "898     def compute(self: T_Dataset, **kwargs) -> T_Dataset:\n899         \"\"\"Manually trigger loading and/or computation of this dataset's data\n900         from disk or a remote source into memory and return a new dataset.\n901         Unlike load, the original dataset is left unaltered.\n902 \n903         Normally, it should not be necessary to call this method in user code,\n904         because all xarray functions should either work on deferred data or\n905         load data automatically. However, this method can be necessary when\n906         working with many file objects on disk.\n907 \n908         Parameters\n909         ----------\n910         **kwargs : dict\n911             Additional keyword arguments passed on to ``dask.compute``.\n912 \n913         See Also\n914         --------\n915         dask.compute\n916         \"\"\"\n917         new = self.copy(deep=False)\n918         return new.load(**kwargs)\n",
        "intended_behavior": "Similarly, the `compute` method in the `Dataset` class should be enhanced to support flexible backend configuration. This enhancement should abstract the compute process to allow for the execution of parallel computations through the preferred backend, ensuring existing `dask` functionality is maintained while enabling support for other backends."
    },
    {
        "rel_file_path": "xarray/core/dataset.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/dataset.py",
        "start": 439,
        "end": 9273,
        "class_name": "Dataset",
        "method_name": null,
        "code": "439 class Dataset(\n440     DataWithCoords,\n441     DatasetAggregations,\n442     DatasetArithmetic,\n443     Mapping[Hashable, \"DataArray\"],\n444 ):\n445     \"\"\"A multi-dimensional, in memory, array database.\n446 \n447     A dataset resembles an in-memory representation of a NetCDF file,\n448     and consists of variables, coordinates and attributes which\n449     together form a self describing dataset.\n450 \n451     Dataset implements the mapping interface with keys given by variable\n452     names and values given by DataArray objects for each variable name.\n453 \n454     One dimensional variables with name equal to their dimension are\n455     index coordinates used for label based indexing.\n456 \n457     To load data from a file or file-like object, use the `open_dataset`\n458     function.\n459 \n460     Parameters\n461     ----------\n462     data_vars : dict-like, optional\n463         A mapping from variable names to :py:class:`~xarray.DataArray`\n464         objects, :py:class:`~xarray.Variable` objects or to tuples of\n465         the form ``(dims, data[, attrs])`` which can be used as\n466         arguments to create a new ``Variable``. Each dimension must\n467         have the same length in all variables in which it appears.\n468 \n469         The following notations are accepted:\n470 \n471         - mapping {var name: DataArray}\n472         - mapping {var name: Variable}\n473         - mapping {var name: (dimension name, array-like)}\n474         - mapping {var name: (tuple of dimension names, array-like)}\n475         - mapping {dimension name: array-like}\n476           (it will be automatically moved to coords, see below)\n477 \n478         Each dimension must have the same length in all variables in\n479         which it appears.\n480     coords : dict-like, optional\n481         Another mapping in similar form as the `data_vars` argument,\n482         except the each item is saved on the dataset as a \"coordinate\".\n483         These variables have an associated meaning: they describe\n484         constant/fixed/independent quantities, unlike the\n485         varying/measured/dependent quantities that belong in\n486         `variables`. Coordinates values may be given by 1-dimensional\n487         arrays or scalars, in which case `dims` do not need to be\n488         supplied: 1D arrays will be assumed to give index values along\n489         the dimension with the same name.\n490 \n491         The following notations are accepted:\n492 \n493         - mapping {coord name: DataArray}\n494         - mapping {coord name: Variable}\n495         - mapping {coord name: (dimension name, array-like)}\n496         - mapping {coord name: (tuple of dimension names, array-like)}\n497         - mapping {dimension name: array-like}\n498           (the dimension name is implicitly set to be the same as the\n499           coord name)\n500 \n501         The last notation implies that the coord name is the same as\n502         the dimension name.\n503 \n504     attrs : dict-like, optional\n505         Global attributes to save on this dataset.\n506 \n507     Examples\n508     --------\n509     Create data:\n510 \n511     >>> np.random.seed(0)\n512     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n513     >>> precipitation = 10 * np.random.rand(2, 2, 3)\n514     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n515     >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n516     >>> time = pd.date_range(\"2014-09-06\", periods=3)\n517     >>> reference_time = pd.Timestamp(\"2014-09-05\")\n518 \n519     Initialize a dataset with multiple dimensions:\n520 \n521     >>> ds = xr.Dataset(\n522     ...     data_vars=dict(\n523     ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n524     ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n525     ...     ),\n526     ...     coords=dict(\n527     ...         lon=([\"x\", \"y\"], lon),\n528     ...         lat=([\"x\", \"y\"], lat),\n529     ...         time=time,\n530     ...         reference_time=reference_time,\n531     ...     ),\n532     ...     attrs=dict(description=\"Weather related data.\"),\n533     ... )\n534     >>> ds\n535     <xarray.Dataset>\n536     Dimensions:         (x: 2, y: 2, time: 3)\n537     Coordinates:\n538         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n539         lat             (x, y) float64 42.25 42.21 42.63 42.59\n540       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n541         reference_time  datetime64[ns] 2014-09-05\n542     Dimensions without coordinates: x, y\n543     Data variables:\n544         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n545         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n546     Attributes:\n547         description:  Weather related data.\n548 \n549     Find out where the coldest temperature was and what values the\n550     other variables had:\n551 \n552     >>> ds.isel(ds.temperature.argmin(...))\n553     <xarray.Dataset>\n554     Dimensions:         ()\n555     Coordinates:\n556         lon             float64 -99.32\n557         lat             float64 42.21\n558         time            datetime64[ns] 2014-09-08\n559         reference_time  datetime64[ns] 2014-09-05\n560     Data variables:\n561         temperature     float64 7.182\n562         precipitation   float64 8.326\n563     Attributes:\n564         description:  Weather related data.\n565     \"\"\"\n566 \n567     _attrs: dict[Hashable, Any] | None\n568     _cache: dict[str, Any]\n569     _coord_names: set[Hashable]\n570     _dims: dict[Hashable, int]\n571     _encoding: dict[Hashable, Any] | None\n572     _close: Callable[[], None] | None\n573     _indexes: dict[Hashable, Index]\n574     _variables: dict[Hashable, Variable]\n575 \n576     __slots__ = (\n577         \"_attrs\",\n578         \"_cache\",\n579         \"_coord_names\",\n580         \"_dims\",\n581         \"_encoding\",\n582         \"_close\",\n583         \"_indexes\",\n584         \"_variables\",\n585         \"__weakref__\",\n586     )\n587 \n588     def __init__(\n589         self,\n590         # could make a VariableArgs to use more generally, and refine these\n591         # categories\n592         data_vars: Mapping[Any, Any] | None = None,\n593         coords: Mapping[Any, Any] | None = None,\n594         attrs: Mapping[Any, Any] | None = None,\n595     ) -> None:\n596         # TODO(shoyer): expose indexes as a public argument in __init__\n597 \n598         if data_vars is None:\n599             data_vars = {}\n600         if coords is None:\n601             coords = {}\n602 \n603         both_data_and_coords = set(data_vars) & set(coords)\n604         if both_data_and_coords:\n605             raise ValueError(\n606                 f\"variables {both_data_and_coords!r} are found in both data_vars and coords\"\n607             )\n608 \n609         if isinstance(coords, Dataset):\n610             coords = coords.variables\n611 \n612         variables, coord_names, dims, indexes, _ = merge_data_and_coords(\n613             data_vars, coords, compat=\"broadcast_equals\"\n614         )\n615 \n616         self._attrs = dict(attrs) if attrs is not None else None\n617         self._close = None\n618         self._encoding = None\n619         self._variables = variables\n620         self._coord_names = coord_names\n621         self._dims = dims\n622         self._indexes = indexes\n623 \n624     @classmethod\n625     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:\n626         \"\"\"Create a new dataset from the contents of a backends.*DataStore\n627         object\n628         \"\"\"\n629         variables, attributes = store.load()\n630         if decoder:\n631             variables, attributes = decoder(variables, attributes)\n632         obj = cls(variables, attrs=attributes)\n633         obj.set_close(store.close)\n634         return obj\n635 \n636     @property\n637     def variables(self) -> Frozen[Hashable, Variable]:\n638         \"\"\"Low level interface to Dataset contents as dict of Variable objects.\n639 \n640         This ordered dictionary is frozen to prevent mutation that could\n641         violate Dataset invariants. It contains all variable objects\n642         constituting the Dataset, including both data variables and\n643         coordinates.\n644         \"\"\"\n645         return Frozen(self._variables)\n646 \n647     @property\n648     def attrs(self) -> dict[Any, Any]:\n649         \"\"\"Dictionary of global attributes on this dataset\"\"\"\n650         if self._attrs is None:\n651             self._attrs = {}\n652         return self._attrs\n653 \n654     @attrs.setter\n655     def attrs(self, value: Mapping[Any, Any]) -> None:\n656         self._attrs = dict(value)\n657 \n658     @property\n659     def encoding(self) -> dict[Any, Any]:\n660         \"\"\"Dictionary of global encoding attributes on this dataset\"\"\"\n661         if self._encoding is None:\n662             self._encoding = {}\n663         return self._encoding\n664 \n665     @encoding.setter\n666     def encoding(self, value: Mapping[Any, Any]) -> None:\n667         self._encoding = dict(value)\n668 \n669     def reset_encoding(self: T_Dataset) -> T_Dataset:\n670         \"\"\"Return a new Dataset without encoding on the dataset or any of its\n671         variables/coords.\"\"\"\n672         variables = {k: v.reset_encoding() for k, v in self.variables.items()}\n673         return self._replace(variables=variables, encoding={})\n674 \n675     @property\n676     def dims(self) -> Frozen[Hashable, int]:\n677         \"\"\"Mapping from dimension names to lengths.\n678 \n679         Cannot be modified directly, but is updated when adding new variables.\n680 \n681         Note that type of this object differs from `DataArray.dims`.\n682         See `Dataset.sizes` and `DataArray.sizes` for consistently named\n683         properties.\n684 \n685         See Also\n686         --------\n687         Dataset.sizes\n688         DataArray.dims\n689         \"\"\"\n690         return Frozen(self._dims)\n691 \n692     @property\n693     def sizes(self) -> Frozen[Hashable, int]:\n694         \"\"\"Mapping from dimension names to lengths.\n695 \n696         Cannot be modified directly, but is updated when adding new variables.\n697 \n698         This is an alias for `Dataset.dims` provided for the benefit of\n699         consistency with `DataArray.sizes`.\n700 \n701         See Also\n702         --------\n703         DataArray.sizes\n704         \"\"\"\n705         return self.dims\n706 \n707     @property\n708     def dtypes(self) -> Frozen[Hashable, np.dtype]:\n709         \"\"\"Mapping from data variable names to dtypes.\n710 \n711         Cannot be modified directly, but is updated when adding new variables.\n712 \n713         See Also\n714         --------\n715         DataArray.dtype\n716         \"\"\"\n717         return Frozen(\n718             {\n719                 n: v.dtype\n720                 for n, v in self._variables.items()\n721                 if n not in self._coord_names\n722             }\n723         )\n724 \n725     def load(self: T_Dataset, **kwargs) -> T_Dataset:\n726         \"\"\"Manually trigger loading and/or computation of this dataset's data\n727         from disk or a remote source into memory and return this dataset.\n728         Unlike compute, the original dataset is modified and returned.\n729 \n730         Normally, it should not be necessary to call this method in user code,\n731         because all xarray functions should either work on deferred data or\n732         load data automatically. However, this method can be necessary when\n733         working with many file objects on disk.\n734 \n735         Parameters\n736         ----------\n737         **kwargs : dict\n738             Additional keyword arguments passed on to ``dask.compute``.\n739 \n740         See Also\n741         --------\n742         dask.compute\n743         \"\"\"\n744         # access .data to coerce everything to numpy or dask arrays\n745         lazy_data = {\n746             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n747         }\n748         if lazy_data:\n749             import dask.array as da\n750 \n751             # evaluate all the dask arrays simultaneously\n752             evaluated_data = da.compute(*lazy_data.values(), **kwargs)\n753 \n754             for k, data in zip(lazy_data, evaluated_data):\n755                 self.variables[k].data = data\n756 \n757         # load everything else sequentially\n758         for k, v in self.variables.items():\n759             if k not in lazy_data:\n760                 v.load()\n761 \n762         return self\n763 \n764     def __dask_tokenize__(self):\n765         from dask.base import normalize_token\n766 \n767         return normalize_token(\n768             (type(self), self._variables, self._coord_names, self._attrs)\n769         )\n770 \n771     def __dask_graph__(self):\n772         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}\n773         graphs = {k: v for k, v in graphs.items() if v is not None}\n774         if not graphs:\n775             return None\n776         else:\n777             try:\n778                 from dask.highlevelgraph import HighLevelGraph\n779 \n780                 return HighLevelGraph.merge(*graphs.values())\n781             except ImportError:\n782                 from dask import sharedict\n783 \n784                 return sharedict.merge(*graphs.values())\n785 \n786     def __dask_keys__(self):\n787         import dask\n788 \n789         return [\n790             v.__dask_keys__()\n791             for v in self.variables.values()\n792             if dask.is_dask_collection(v)\n793         ]\n794 \n795     def __dask_layers__(self):\n796         import dask\n797 \n798         return sum(\n799             (\n800                 v.__dask_layers__()\n801                 for v in self.variables.values()\n802                 if dask.is_dask_collection(v)\n803             ),\n804             (),\n805         )\n806 \n807     @property\n808     def __dask_optimize__(self):\n809         import dask.array as da\n810 \n811         return da.Array.__dask_optimize__\n812 \n813     @property\n814     def __dask_scheduler__(self):\n815         import dask.array as da\n816 \n817         return da.Array.__dask_scheduler__\n818 \n819     def __dask_postcompute__(self):\n820         return self._dask_postcompute, ()\n821 \n822     def __dask_postpersist__(self):\n823         return self._dask_postpersist, ()\n824 \n825     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:\n826         import dask\n827 \n828         variables = {}\n829         results_iter = iter(results)\n830 \n831         for k, v in self._variables.items():\n832             if dask.is_dask_collection(v):\n833                 rebuild, args = v.__dask_postcompute__()\n834                 v = rebuild(next(results_iter), *args)\n835             variables[k] = v\n836 \n837         return type(self)._construct_direct(\n838             variables,\n839             self._coord_names,\n840             self._dims,\n841             self._attrs,\n842             self._indexes,\n843             self._encoding,\n844             self._close,\n845         )\n846 \n847     def _dask_postpersist(\n848         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] | None = None\n849     ) -> T_Dataset:\n850         from dask import is_dask_collection\n851         from dask.highlevelgraph import HighLevelGraph\n852         from dask.optimization import cull\n853 \n854         variables = {}\n855 \n856         for k, v in self._variables.items():\n857             if not is_dask_collection(v):\n858                 variables[k] = v\n859                 continue\n860 \n861             if isinstance(dsk, HighLevelGraph):\n862                 # dask >= 2021.3\n863                 # __dask_postpersist__() was called by dask.highlevelgraph.\n864                 # Don't use dsk.cull(), as we need to prevent partial layers:\n865                 # https://github.com/dask/dask/issues/7137\n866                 layers = v.__dask_layers__()\n867                 if rename:\n868                     layers = [rename.get(k, k) for k in layers]\n869                 dsk2 = dsk.cull_layers(layers)\n870             elif rename:  # pragma: nocover\n871                 # At the moment of writing, this is only for forward compatibility.\n872                 # replace_name_in_key requires dask >= 2021.3.\n873                 from dask.base import flatten, replace_name_in_key\n874 \n875                 keys = [\n876                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())\n877                 ]\n878                 dsk2, _ = cull(dsk, keys)\n879             else:\n880                 # __dask_postpersist__() was called by dask.optimize or dask.persist\n881                 dsk2, _ = cull(dsk, v.__dask_keys__())\n882 \n883             rebuild, args = v.__dask_postpersist__()\n884             # rename was added in dask 2021.3\n885             kwargs = {\"rename\": rename} if rename else {}\n886             variables[k] = rebuild(dsk2, *args, **kwargs)\n887 \n888         return type(self)._construct_direct(\n889             variables,\n890             self._coord_names,\n891             self._dims,\n892             self._attrs,\n893             self._indexes,\n894             self._encoding,\n895             self._close,\n896         )\n897 \n898     def compute(self: T_Dataset, **kwargs) -> T_Dataset:\n899         \"\"\"Manually trigger loading and/or computation of this dataset's data\n900         from disk or a remote source into memory and return a new dataset.\n901         Unlike load, the original dataset is left unaltered.\n902 \n903         Normally, it should not be necessary to call this method in user code,\n904         because all xarray functions should either work on deferred data or\n905         load data automatically. However, this method can be necessary when\n906         working with many file objects on disk.\n907 \n908         Parameters\n909         ----------\n910         **kwargs : dict\n911             Additional keyword arguments passed on to ``dask.compute``.\n912 \n913         See Also\n914         --------\n915         dask.compute\n916         \"\"\"\n917         new = self.copy(deep=False)\n918         return new.load(**kwargs)\n919 \n920     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:\n921         \"\"\"Persist all Dask arrays in memory\"\"\"\n922         # access .data to coerce everything to numpy or dask arrays\n923         lazy_data = {\n924             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)\n925         }\n926         if lazy_data:\n927             import dask\n928 \n929             # evaluate all the dask arrays simultaneously\n930             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)\n931 \n932             for k, data in zip(lazy_data, evaluated_data):\n933                 self.variables[k].data = data\n934 \n935         return self\n936 \n937     def persist(self: T_Dataset, **kwargs) -> T_Dataset:\n938         \"\"\"Trigger computation, keeping data as dask arrays\n939 \n940         This operation can be used to trigger computation on underlying dask\n941         arrays, similar to ``.compute()`` or ``.load()``.  However this\n942         operation keeps the data as dask arrays. This is particularly useful\n943         when using the dask.distributed scheduler and you want to load a large\n944         amount of data into distributed memory.\n945 \n946         Parameters\n947         ----------\n948         **kwargs : dict\n949             Additional keyword arguments passed on to ``dask.persist``.\n950 \n951         See Also\n952         --------\n953         dask.persist\n954         \"\"\"\n955         new = self.copy(deep=False)\n956         return new._persist_inplace(**kwargs)\n957 \n958     @classmethod\n959     def _construct_direct(\n960         cls: type[T_Dataset],\n961         variables: dict[Any, Variable],\n962         coord_names: set[Hashable],\n963         dims: dict[Any, int] | None = None,\n964         attrs: dict | None = None,\n965         indexes: dict[Any, Index] | None = None,\n966         encoding: dict | None = None,\n967         close: Callable[[], None] | None = None,\n968     ) -> T_Dataset:\n969         \"\"\"Shortcut around __init__ for internal use when we want to skip\n970         costly validation\n971         \"\"\"\n972         if dims is None:\n973             dims = calculate_dimensions(variables)\n974         if indexes is None:\n975             indexes = {}\n976         obj = object.__new__(cls)\n977         obj._variables = variables\n978         obj._coord_names = coord_names\n979         obj._dims = dims\n980         obj._indexes = indexes\n981         obj._attrs = attrs\n982         obj._close = close\n983         obj._encoding = encoding\n984         return obj\n985 \n986     def _replace(\n987         self: T_Dataset,\n988         variables: dict[Hashable, Variable] | None = None,\n989         coord_names: set[Hashable] | None = None,\n990         dims: dict[Any, int] | None = None,\n991         attrs: dict[Hashable, Any] | None | Default = _default,\n992         indexes: dict[Hashable, Index] | None = None,\n993         encoding: dict | None | Default = _default,\n994         inplace: bool = False,\n995     ) -> T_Dataset:\n996         \"\"\"Fastpath constructor for internal use.\n997 \n998         Returns an object with optionally with replaced attributes.\n999 \n1000         Explicitly passed arguments are *not* copied when placed on the new\n1001         dataset. It is up to the caller to ensure that they have the right type\n1002         and are not used elsewhere.\n1003         \"\"\"\n1004         if inplace:\n1005             if variables is not None:\n1006                 self._variables = variables\n1007             if coord_names is not None:\n1008                 self._coord_names = coord_names\n1009             if dims is not None:\n1010                 self._dims = dims\n1011             if attrs is not _default:\n1012                 self._attrs = attrs\n1013             if indexes is not None:\n1014                 self._indexes = indexes\n1015             if encoding is not _default:\n1016                 self._encoding = encoding\n1017             obj = self\n1018         else:\n1019             if variables is None:\n1020                 variables = self._variables.copy()\n1021             if coord_names is None:\n1022                 coord_names = self._coord_names.copy()\n1023             if dims is None:\n1024                 dims = self._dims.copy()\n1025             if attrs is _default:\n1026                 attrs = copy.copy(self._attrs)\n1027             if indexes is None:\n1028                 indexes = self._indexes.copy()\n1029             if encoding is _default:\n1030                 encoding = copy.copy(self._encoding)\n1031             obj = self._construct_direct(\n1032                 variables, coord_names, dims, attrs, indexes, encoding\n1033             )\n1034         return obj\n1035 \n1036     def _replace_with_new_dims(\n1037         self: T_Dataset,\n1038         variables: dict[Hashable, Variable],\n1039         coord_names: set | None = None,\n1040         attrs: dict[Hashable, Any] | None | Default = _default,\n1041         indexes: dict[Hashable, Index] | None = None,\n1042         inplace: bool = False,\n1043     ) -> T_Dataset:\n1044         \"\"\"Replace variables with recalculated dimensions.\"\"\"\n1045         dims = calculate_dimensions(variables)\n1046         return self._replace(\n1047             variables, coord_names, dims, attrs, indexes, inplace=inplace\n1048         )\n1049 \n1050     def _replace_vars_and_dims(\n1051         self: T_Dataset,\n1052         variables: dict[Hashable, Variable],\n1053         coord_names: set | None = None,\n1054         dims: dict[Hashable, int] | None = None,\n1055         attrs: dict[Hashable, Any] | None | Default = _default,\n1056         inplace: bool = False,\n1057     ) -> T_Dataset:\n1058         \"\"\"Deprecated version of _replace_with_new_dims().\n1059 \n1060         Unlike _replace_with_new_dims(), this method always recalculates\n1061         indexes from variables.\n1062         \"\"\"\n1063         if dims is None:\n1064             dims = calculate_dimensions(variables)\n1065         return self._replace(\n1066             variables, coord_names, dims, attrs, indexes=None, inplace=inplace\n1067         )\n1068 \n1069     def _overwrite_indexes(\n1070         self: T_Dataset,\n1071         indexes: Mapping[Hashable, Index],\n1072         variables: Mapping[Hashable, Variable] | None = None,\n1073         drop_variables: list[Hashable] | None = None,\n1074         drop_indexes: list[Hashable] | None = None,\n1075         rename_dims: Mapping[Hashable, Hashable] | None = None,\n1076     ) -> T_Dataset:\n1077         \"\"\"Maybe replace indexes.\n1078 \n1079         This function may do a lot more depending on index query\n1080         results.\n1081 \n1082         \"\"\"\n1083         if not indexes:\n1084             return self\n1085 \n1086         if variables is None:\n1087             variables = {}\n1088         if drop_variables is None:\n1089             drop_variables = []\n1090         if drop_indexes is None:\n1091             drop_indexes = []\n1092 \n1093         new_variables = self._variables.copy()\n1094         new_coord_names = self._coord_names.copy()\n1095         new_indexes = dict(self._indexes)\n1096 \n1097         index_variables = {}\n1098         no_index_variables = {}\n1099         for name, var in variables.items():\n1100             old_var = self._variables.get(name)\n1101             if old_var is not None:\n1102                 var.attrs.update(old_var.attrs)\n1103                 var.encoding.update(old_var.encoding)\n1104             if name in indexes:\n1105                 index_variables[name] = var\n1106             else:\n1107                 no_index_variables[name] = var\n1108 \n1109         for name in indexes:\n1110             new_indexes[name] = indexes[name]\n1111 \n1112         for name, var in index_variables.items():\n1113             new_coord_names.add(name)\n1114             new_variables[name] = var\n1115 \n1116         # append no-index variables at the end\n1117         for k in no_index_variables:\n1118             new_variables.pop(k)\n1119         new_variables.update(no_index_variables)\n1120 \n1121         for name in drop_indexes:\n1122             new_indexes.pop(name)\n1123 \n1124         for name in drop_variables:\n1125             new_variables.pop(name)\n1126             new_indexes.pop(name, None)\n1127             new_coord_names.remove(name)\n1128 \n1129         replaced = self._replace(\n1130             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes\n1131         )\n1132 \n1133         if rename_dims:\n1134             # skip rename indexes: they should already have the right name(s)\n1135             dims = replaced._rename_dims(rename_dims)\n1136             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)\n1137             return replaced._replace(\n1138                 variables=new_variables, coord_names=new_coord_names, dims=dims\n1139             )\n1140         else:\n1141             return replaced\n1142 \n1143     def copy(\n1144         self: T_Dataset, deep: bool = False, data: Mapping[Any, ArrayLike] | None = None\n1145     ) -> T_Dataset:\n1146         \"\"\"Returns a copy of this dataset.\n1147 \n1148         If `deep=True`, a deep copy is made of each of the component variables.\n1149         Otherwise, a shallow copy of each of the component variable is made, so\n1150         that the underlying memory region of the new dataset is the same as in\n1151         the original dataset.\n1152 \n1153         Use `data` to create a new object with the same structure as\n1154         original but entirely new data.\n1155 \n1156         Parameters\n1157         ----------\n1158         deep : bool, default: False\n1159             Whether each component variable is loaded into memory and copied onto\n1160             the new object. Default is False.\n1161         data : dict-like or None, optional\n1162             Data to use in the new object. Each item in `data` must have same\n1163             shape as corresponding data variable in original. When `data` is\n1164             used, `deep` is ignored for the data variables and only used for\n1165             coords.\n1166 \n1167         Returns\n1168         -------\n1169         object : Dataset\n1170             New object with dimensions, attributes, coordinates, name, encoding,\n1171             and optionally data copied from original.\n1172 \n1173         Examples\n1174         --------\n1175         Shallow copy versus deep copy\n1176 \n1177         >>> da = xr.DataArray(np.random.randn(2, 3))\n1178         >>> ds = xr.Dataset(\n1179         ...     {\"foo\": da, \"bar\": (\"x\", [-1, 2])},\n1180         ...     coords={\"x\": [\"one\", \"two\"]},\n1181         ... )\n1182         >>> ds.copy()\n1183         <xarray.Dataset>\n1184         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1185         Coordinates:\n1186           * x        (x) <U3 'one' 'two'\n1187         Dimensions without coordinates: dim_0, dim_1\n1188         Data variables:\n1189             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n1190             bar      (x) int64 -1 2\n1191 \n1192         >>> ds_0 = ds.copy(deep=False)\n1193         >>> ds_0[\"foo\"][0, 0] = 7\n1194         >>> ds_0\n1195         <xarray.Dataset>\n1196         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1197         Coordinates:\n1198           * x        (x) <U3 'one' 'two'\n1199         Dimensions without coordinates: dim_0, dim_1\n1200         Data variables:\n1201             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1202             bar      (x) int64 -1 2\n1203 \n1204         >>> ds\n1205         <xarray.Dataset>\n1206         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1207         Coordinates:\n1208           * x        (x) <U3 'one' 'two'\n1209         Dimensions without coordinates: dim_0, dim_1\n1210         Data variables:\n1211             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1212             bar      (x) int64 -1 2\n1213 \n1214         Changing the data using the ``data`` argument maintains the\n1215         structure of the original object, but with the new data. Original\n1216         object is unaffected.\n1217 \n1218         >>> ds.copy(data={\"foo\": np.arange(6).reshape(2, 3), \"bar\": [\"a\", \"b\"]})\n1219         <xarray.Dataset>\n1220         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1221         Coordinates:\n1222           * x        (x) <U3 'one' 'two'\n1223         Dimensions without coordinates: dim_0, dim_1\n1224         Data variables:\n1225             foo      (dim_0, dim_1) int64 0 1 2 3 4 5\n1226             bar      (x) <U1 'a' 'b'\n1227 \n1228         >>> ds\n1229         <xarray.Dataset>\n1230         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n1231         Coordinates:\n1232           * x        (x) <U3 'one' 'two'\n1233         Dimensions without coordinates: dim_0, dim_1\n1234         Data variables:\n1235             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773\n1236             bar      (x) int64 -1 2\n1237 \n1238         See Also\n1239         --------\n1240         pandas.DataFrame.copy\n1241         \"\"\"\n1242         return self._copy(deep=deep, data=data)\n1243 \n1244     def _copy(\n1245         self: T_Dataset,\n1246         deep: bool = False,\n1247         data: Mapping[Any, ArrayLike] | None = None,\n1248         memo: dict[int, Any] | None = None,\n1249     ) -> T_Dataset:\n1250         if data is None:\n1251             data = {}\n1252         elif not utils.is_dict_like(data):\n1253             raise ValueError(\"Data must be dict-like\")\n1254 \n1255         if data:\n1256             var_keys = set(self.data_vars.keys())\n1257             data_keys = set(data.keys())\n1258             keys_not_in_vars = data_keys - var_keys\n1259             if keys_not_in_vars:\n1260                 raise ValueError(\n1261                     \"Data must only contain variables in original \"\n1262                     \"dataset. Extra variables: {}\".format(keys_not_in_vars)\n1263                 )\n1264             keys_missing_from_data = var_keys - data_keys\n1265             if keys_missing_from_data:\n1266                 raise ValueError(\n1267                     \"Data must contain all variables in original \"\n1268                     \"dataset. Data is missing {}\".format(keys_missing_from_data)\n1269                 )\n1270 \n1271         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)\n1272 \n1273         variables = {}\n1274         for k, v in self._variables.items():\n1275             if k in index_vars:\n1276                 variables[k] = index_vars[k]\n1277             else:\n1278                 variables[k] = v._copy(deep=deep, data=data.get(k), memo=memo)\n1279 \n1280         attrs = copy.deepcopy(self._attrs, memo) if deep else copy.copy(self._attrs)\n1281         encoding = (\n1282             copy.deepcopy(self._encoding, memo) if deep else copy.copy(self._encoding)\n1283         )\n1284 \n1285         return self._replace(variables, indexes=indexes, attrs=attrs, encoding=encoding)\n1286 \n1287     def __copy__(self: T_Dataset) -> T_Dataset:\n1288         return self._copy(deep=False)\n1289 \n1290     def __deepcopy__(self: T_Dataset, memo: dict[int, Any] | None = None) -> T_Dataset:\n1291         return self._copy(deep=True, memo=memo)\n1292 \n1293     def as_numpy(self: T_Dataset) -> T_Dataset:\n1294         \"\"\"\n1295         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.\n1296 \n1297         See also\n1298         --------\n1299         DataArray.as_numpy\n1300         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.\n1301         \"\"\"\n1302         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}\n1303         return self._replace(variables=numpy_variables)\n1304 \n1305     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:\n1306         \"\"\"Create a new Dataset with the listed variables from this dataset and\n1307         the all relevant coordinates. Skips all validation.\n1308         \"\"\"\n1309         variables: dict[Hashable, Variable] = {}\n1310         coord_names = set()\n1311         indexes: dict[Hashable, Index] = {}\n1312 \n1313         for name in names:\n1314             try:\n1315                 variables[name] = self._variables[name]\n1316             except KeyError:\n1317                 ref_name, var_name, var = _get_virtual_variable(\n1318                     self._variables, name, self.dims\n1319                 )\n1320                 variables[var_name] = var\n1321                 if ref_name in self._coord_names or ref_name in self.dims:\n1322                     coord_names.add(var_name)\n1323                 if (var_name,) == var.dims:\n1324                     index, index_vars = create_default_index_implicit(var, names)\n1325                     indexes.update({k: index for k in index_vars})\n1326                     variables.update(index_vars)\n1327                     coord_names.update(index_vars)\n1328 \n1329         needed_dims: OrderedSet[Hashable] = OrderedSet()\n1330         for v in variables.values():\n1331             needed_dims.update(v.dims)\n1332 \n1333         dims = {k: self.dims[k] for k in needed_dims}\n1334 \n1335         # preserves ordering of coordinates\n1336         for k in self._variables:\n1337             if k not in self._coord_names:\n1338                 continue\n1339 \n1340             if set(self.variables[k].dims) <= needed_dims:\n1341                 variables[k] = self._variables[k]\n1342                 coord_names.add(k)\n1343 \n1344         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))\n1345 \n1346         return self._replace(variables, coord_names, dims, indexes=indexes)\n1347 \n1348     def _construct_dataarray(self, name: Hashable) -> DataArray:\n1349         \"\"\"Construct a DataArray by indexing this dataset\"\"\"\n1350         from xarray.core.dataarray import DataArray\n1351 \n1352         try:\n1353             variable = self._variables[name]\n1354         except KeyError:\n1355             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)\n1356 \n1357         needed_dims = set(variable.dims)\n1358 \n1359         coords: dict[Hashable, Variable] = {}\n1360         # preserve ordering\n1361         for k in self._variables:\n1362             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:\n1363                 coords[k] = self.variables[k]\n1364 \n1365         indexes = filter_indexes_from_coords(self._indexes, set(coords))\n1366 \n1367         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)\n1368 \n1369     @property\n1370     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1371         \"\"\"Places to look-up items for attribute-style access\"\"\"\n1372         yield from self._item_sources\n1373         yield self.attrs\n1374 \n1375     @property\n1376     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:\n1377         \"\"\"Places to look-up items for key-completion\"\"\"\n1378         yield self.data_vars\n1379         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)\n1380 \n1381         # virtual coordinates\n1382         yield HybridMappingProxy(keys=self.dims, mapping=self)\n1383 \n1384     def __contains__(self, key: object) -> bool:\n1385         \"\"\"The 'in' operator will return true or false depending on whether\n1386         'key' is an array in the dataset or not.\n1387         \"\"\"\n1388         return key in self._variables\n1389 \n1390     def __len__(self) -> int:\n1391         return len(self.data_vars)\n1392 \n1393     def __bool__(self) -> bool:\n1394         return bool(self.data_vars)\n1395 \n1396     def __iter__(self) -> Iterator[Hashable]:\n1397         return iter(self.data_vars)\n1398 \n1399     def __array__(self, dtype=None):\n1400         raise TypeError(\n1401             \"cannot directly convert an xarray.Dataset into a \"\n1402             \"numpy array. Instead, create an xarray.DataArray \"\n1403             \"first, either with indexing on the Dataset or by \"\n1404             \"invoking the `to_array()` method.\"\n1405         )\n1406 \n1407     @property\n1408     def nbytes(self) -> int:\n1409         \"\"\"\n1410         Total bytes consumed by the data arrays of all variables in this dataset.\n1411 \n1412         If the backend array for any variable does not include ``nbytes``, estimates\n1413         the total bytes for that array based on the ``size`` and ``dtype``.\n1414         \"\"\"\n1415         return sum(v.nbytes for v in self.variables.values())\n1416 \n1417     @property\n1418     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:\n1419         \"\"\"Attribute for location based indexing. Only supports __getitem__,\n1420         and only when the key is a dict of the form {dim: labels}.\n1421         \"\"\"\n1422         return _LocIndexer(self)\n1423 \n1424     @overload\n1425     def __getitem__(self, key: Hashable) -> DataArray:\n1426         ...\n1427 \n1428     # Mapping is Iterable\n1429     @overload\n1430     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:\n1431         ...\n1432 \n1433     def __getitem__(\n1434         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]\n1435     ) -> T_Dataset | DataArray:\n1436         \"\"\"Access variables or coordinates of this dataset as a\n1437         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.\n1438 \n1439         Indexing with a list of names will return a new ``Dataset`` object.\n1440         \"\"\"\n1441         if utils.is_dict_like(key):\n1442             return self.isel(**key)\n1443         if utils.hashable(key):\n1444             return self._construct_dataarray(key)\n1445         if utils.iterable_of_hashable(key):\n1446             return self._copy_listed(key)\n1447         raise ValueError(f\"Unsupported key-type {type(key)}\")\n1448 \n1449     def __setitem__(\n1450         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any\n1451     ) -> None:\n1452         \"\"\"Add an array to this dataset.\n1453         Multiple arrays can be added at the same time, in which case each of\n1454         the following operations is applied to the respective value.\n1455 \n1456         If key is dict-like, update all variables in the dataset\n1457         one by one with the given value at the given location.\n1458         If the given value is also a dataset, select corresponding variables\n1459         in the given value and in the dataset to be changed.\n1460 \n1461         If value is a `\n1462         from .dataarray import DataArray`, call its `select_vars()` method, rename it\n1463         to `key` and merge the contents of the resulting dataset into this\n1464         dataset.\n1465 \n1466         If value is a `Variable` object (or tuple of form\n1467         ``(dims, data[, attrs])``), add it to this dataset as a new\n1468         variable.\n1469         \"\"\"\n1470         from xarray.core.dataarray import DataArray\n1471 \n1472         if utils.is_dict_like(key):\n1473             # check for consistency and convert value to dataset\n1474             value = self._setitem_check(key, value)\n1475             # loop over dataset variables and set new values\n1476             processed = []\n1477             for name, var in self.items():\n1478                 try:\n1479                     var[key] = value[name]\n1480                     processed.append(name)\n1481                 except Exception as e:\n1482                     if processed:\n1483                         raise RuntimeError(\n1484                             \"An error occurred while setting values of the\"\n1485                             f\" variable '{name}'. The following variables have\"\n1486                             f\" been successfully updated:\\n{processed}\"\n1487                         ) from e\n1488                     else:\n1489                         raise e\n1490 \n1491         elif utils.hashable(key):\n1492             if isinstance(value, Dataset):\n1493                 raise TypeError(\n1494                     \"Cannot assign a Dataset to a single key - only a DataArray or Variable \"\n1495                     \"object can be stored under a single key.\"\n1496                 )\n1497             self.update({key: value})\n1498 \n1499         elif utils.iterable_of_hashable(key):\n1500             keylist = list(key)\n1501             if len(keylist) == 0:\n1502                 raise ValueError(\"Empty list of variables to be set\")\n1503             if len(keylist) == 1:\n1504                 self.update({keylist[0]: value})\n1505             else:\n1506                 if len(keylist) != len(value):\n1507                     raise ValueError(\n1508                         f\"Different lengths of variables to be set \"\n1509                         f\"({len(keylist)}) and data used as input for \"\n1510                         f\"setting ({len(value)})\"\n1511                     )\n1512                 if isinstance(value, Dataset):\n1513                     self.update(dict(zip(keylist, value.data_vars.values())))\n1514                 elif isinstance(value, DataArray):\n1515                     raise ValueError(\"Cannot assign single DataArray to multiple keys\")\n1516                 else:\n1517                     self.update(dict(zip(keylist, value)))\n1518 \n1519         else:\n1520             raise ValueError(f\"Unsupported key-type {type(key)}\")\n1521 \n1522     def _setitem_check(self, key, value):\n1523         \"\"\"Consistency check for __setitem__\n1524 \n1525         When assigning values to a subset of a Dataset, do consistency check beforehand\n1526         to avoid leaving the dataset in a partially updated state when an error occurs.\n1527         \"\"\"\n1528         from xarray.core.alignment import align\n1529         from xarray.core.dataarray import DataArray\n1530 \n1531         if isinstance(value, Dataset):\n1532             missing_vars = [\n1533                 name for name in value.data_vars if name not in self.data_vars\n1534             ]\n1535             if missing_vars:\n1536                 raise ValueError(\n1537                     f\"Variables {missing_vars} in new values\"\n1538                     f\" not available in original dataset:\\n{self}\"\n1539                 )\n1540         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):\n1541             raise TypeError(\n1542                 \"Dataset assignment only accepts DataArrays, Datasets, and scalars.\"\n1543             )\n1544 \n1545         new_value = Dataset()\n1546         for name, var in self.items():\n1547             # test indexing\n1548             try:\n1549                 var_k = var[key]\n1550             except Exception as e:\n1551                 raise ValueError(\n1552                     f\"Variable '{name}': indexer {key} not available\"\n1553                 ) from e\n1554 \n1555             if isinstance(value, Dataset):\n1556                 val = value[name]\n1557             else:\n1558                 val = value\n1559 \n1560             if isinstance(val, DataArray):\n1561                 # check consistency of dimensions\n1562                 for dim in val.dims:\n1563                     if dim not in var_k.dims:\n1564                         raise KeyError(\n1565                             f\"Variable '{name}': dimension '{dim}' appears in new values \"\n1566                             f\"but not in the indexed original data\"\n1567                         )\n1568                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)\n1569                 if dims != val.dims:\n1570                     raise ValueError(\n1571                         f\"Variable '{name}': dimension order differs between\"\n1572                         f\" original and new data:\\n{dims}\\nvs.\\n{val.dims}\"\n1573                     )\n1574             else:\n1575                 val = np.array(val)\n1576 \n1577             # type conversion\n1578             new_value[name] = val.astype(var_k.dtype, copy=False)\n1579 \n1580         # check consistency of dimension sizes and dimension coordinates\n1581         if isinstance(value, DataArray) or isinstance(value, Dataset):\n1582             align(self[key], value, join=\"exact\", copy=False)\n1583 \n1584         return new_value\n1585 \n1586     def __delitem__(self, key: Hashable) -> None:\n1587         \"\"\"Remove a variable from this dataset.\"\"\"\n1588         assert_no_index_corrupted(self.xindexes, {key})\n1589 \n1590         if key in self._indexes:\n1591             del self._indexes[key]\n1592         del self._variables[key]\n1593         self._coord_names.discard(key)\n1594         self._dims = calculate_dimensions(self._variables)\n1595 \n1596     # mutable objects should not be hashable\n1597     # https://github.com/python/mypy/issues/4266\n1598     __hash__ = None  # type: ignore[assignment]\n1599 \n1600     def _all_compat(self, other: Dataset, compat_str: str) -> bool:\n1601         \"\"\"Helper function for equals and identical\"\"\"\n1602 \n1603         # some stores (e.g., scipy) do not seem to preserve order, so don't\n1604         # require matching order for equality\n1605         def compat(x: Variable, y: Variable) -> bool:\n1606             return getattr(x, compat_str)(y)\n1607 \n1608         return self._coord_names == other._coord_names and utils.dict_equiv(\n1609             self._variables, other._variables, compat=compat\n1610         )\n1611 \n1612     def broadcast_equals(self, other: Dataset) -> bool:\n1613         \"\"\"Two Datasets are broadcast equal if they are equal after\n1614         broadcasting all variables against each other.\n1615 \n1616         For example, variables that are scalar in one dataset but non-scalar in\n1617         the other dataset can still be broadcast equal if the the non-scalar\n1618         variable is a constant.\n1619 \n1620         See Also\n1621         --------\n1622         Dataset.equals\n1623         Dataset.identical\n1624         \"\"\"\n1625         try:\n1626             return self._all_compat(other, \"broadcast_equals\")\n1627         except (TypeError, AttributeError):\n1628             return False\n1629 \n1630     def equals(self, other: Dataset) -> bool:\n1631         \"\"\"Two Datasets are equal if they have matching variables and\n1632         coordinates, all of which are equal.\n1633 \n1634         Datasets can still be equal (like pandas objects) if they have NaN\n1635         values in the same locations.\n1636 \n1637         This method is necessary because `v1 == v2` for ``Dataset``\n1638         does element-wise comparisons (like numpy.ndarrays).\n1639 \n1640         See Also\n1641         --------\n1642         Dataset.broadcast_equals\n1643         Dataset.identical\n1644         \"\"\"\n1645         try:\n1646             return self._all_compat(other, \"equals\")\n1647         except (TypeError, AttributeError):\n1648             return False\n1649 \n1650     def identical(self, other: Dataset) -> bool:\n1651         \"\"\"Like equals, but also checks all dataset attributes and the\n1652         attributes on all variables and coordinates.\n1653 \n1654         See Also\n1655         --------\n1656         Dataset.broadcast_equals\n1657         Dataset.equals\n1658         \"\"\"\n1659         try:\n1660             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(\n1661                 other, \"identical\"\n1662             )\n1663         except (TypeError, AttributeError):\n1664             return False\n1665 \n1666     @property\n1667     def indexes(self) -> Indexes[pd.Index]:\n1668         \"\"\"Mapping of pandas.Index objects used for label based indexing.\n1669 \n1670         Raises an error if this Dataset has indexes that cannot be coerced\n1671         to pandas.Index objects.\n1672 \n1673         See Also\n1674         --------\n1675         Dataset.xindexes\n1676 \n1677         \"\"\"\n1678         return self.xindexes.to_pandas_indexes()\n1679 \n1680     @property\n1681     def xindexes(self) -> Indexes[Index]:\n1682         \"\"\"Mapping of xarray Index objects used for label based indexing.\"\"\"\n1683         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})\n1684 \n1685     @property\n1686     def coords(self) -> DatasetCoordinates:\n1687         \"\"\"Dictionary of xarray.DataArray objects corresponding to coordinate\n1688         variables\n1689         \"\"\"\n1690         return DatasetCoordinates(self)\n1691 \n1692     @property\n1693     def data_vars(self) -> DataVariables:\n1694         \"\"\"Dictionary of DataArray objects corresponding to data variables\"\"\"\n1695         return DataVariables(self)\n1696 \n1697     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:\n1698         \"\"\"Given names of one or more variables, set them as coordinates\n1699 \n1700         Parameters\n1701         ----------\n1702         names : hashable or iterable of hashable\n1703             Name(s) of variables in this dataset to convert into coordinates.\n1704 \n1705         Returns\n1706         -------\n1707         Dataset\n1708 \n1709         See Also\n1710         --------\n1711         Dataset.swap_dims\n1712         Dataset.assign_coords\n1713         \"\"\"\n1714         # TODO: allow inserting new coordinates with this method, like\n1715         # DataFrame.set_index?\n1716         # nb. check in self._variables, not self.data_vars to insure that the\n1717         # operation is idempotent\n1718         if isinstance(names, str) or not isinstance(names, Iterable):\n1719             names = [names]\n1720         else:\n1721             names = list(names)\n1722         self._assert_all_in_dataset(names)\n1723         obj = self.copy()\n1724         obj._coord_names.update(names)\n1725         return obj\n1726 \n1727     def reset_coords(\n1728         self: T_Dataset,\n1729         names: Dims = None,\n1730         drop: bool = False,\n1731     ) -> T_Dataset:\n1732         \"\"\"Given names of coordinates, reset them to become variables\n1733 \n1734         Parameters\n1735         ----------\n1736         names : str, Iterable of Hashable or None, optional\n1737             Name(s) of non-index coordinates in this dataset to reset into\n1738             variables. By default, all non-index coordinates are reset.\n1739         drop : bool, default: False\n1740             If True, remove coordinates instead of converting them into\n1741             variables.\n1742 \n1743         Returns\n1744         -------\n1745         Dataset\n1746         \"\"\"\n1747         if names is None:\n1748             names = self._coord_names - set(self._indexes)\n1749         else:\n1750             if isinstance(names, str) or not isinstance(names, Iterable):\n1751                 names = [names]\n1752             else:\n1753                 names = list(names)\n1754             self._assert_all_in_dataset(names)\n1755             bad_coords = set(names) & set(self._indexes)\n1756             if bad_coords:\n1757                 raise ValueError(\n1758                     f\"cannot remove index coordinates with reset_coords: {bad_coords}\"\n1759                 )\n1760         obj = self.copy()\n1761         obj._coord_names.difference_update(names)\n1762         if drop:\n1763             for name in names:\n1764                 del obj._variables[name]\n1765         return obj\n1766 \n1767     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:\n1768         \"\"\"Store dataset contents to a backends.*DataStore object.\"\"\"\n1769         from xarray.backends.api import dump_to_store\n1770 \n1771         # TODO: rename and/or cleanup this method to make it more consistent\n1772         # with to_netcdf()\n1773         dump_to_store(self, store, **kwargs)\n1774 \n1775     # path=None writes to bytes\n1776     @overload\n1777     def to_netcdf(\n1778         self,\n1779         path: None = None,\n1780         mode: Literal[\"w\", \"a\"] = \"w\",\n1781         format: T_NetcdfTypes | None = None,\n1782         group: str | None = None,\n1783         engine: T_NetcdfEngine | None = None,\n1784         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1785         unlimited_dims: Iterable[Hashable] | None = None,\n1786         compute: bool = True,\n1787         invalid_netcdf: bool = False,\n1788     ) -> bytes:\n1789         ...\n1790 \n1791     # default return None\n1792     @overload\n1793     def to_netcdf(\n1794         self,\n1795         path: str | PathLike,\n1796         mode: Literal[\"w\", \"a\"] = \"w\",\n1797         format: T_NetcdfTypes | None = None,\n1798         group: str | None = None,\n1799         engine: T_NetcdfEngine | None = None,\n1800         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1801         unlimited_dims: Iterable[Hashable] | None = None,\n1802         compute: Literal[True] = True,\n1803         invalid_netcdf: bool = False,\n1804     ) -> None:\n1805         ...\n1806 \n1807     # compute=False returns dask.Delayed\n1808     @overload\n1809     def to_netcdf(\n1810         self,\n1811         path: str | PathLike,\n1812         mode: Literal[\"w\", \"a\"] = \"w\",\n1813         format: T_NetcdfTypes | None = None,\n1814         group: str | None = None,\n1815         engine: T_NetcdfEngine | None = None,\n1816         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1817         unlimited_dims: Iterable[Hashable] | None = None,\n1818         *,\n1819         compute: Literal[False],\n1820         invalid_netcdf: bool = False,\n1821     ) -> Delayed:\n1822         ...\n1823 \n1824     def to_netcdf(\n1825         self,\n1826         path: str | PathLike | None = None,\n1827         mode: Literal[\"w\", \"a\"] = \"w\",\n1828         format: T_NetcdfTypes | None = None,\n1829         group: str | None = None,\n1830         engine: T_NetcdfEngine | None = None,\n1831         encoding: Mapping[Any, Mapping[str, Any]] | None = None,\n1832         unlimited_dims: Iterable[Hashable] | None = None,\n1833         compute: bool = True,\n1834         invalid_netcdf: bool = False,\n1835     ) -> bytes | Delayed | None:\n1836         \"\"\"Write dataset contents to a netCDF file.\n1837 \n1838         Parameters\n1839         ----------\n1840         path : str, path-like or file-like, optional\n1841             Path to which to save this dataset. File-like objects are only\n1842             supported by the scipy engine. If no path is provided, this\n1843             function returns the resulting netCDF file as bytes; in this case,\n1844             we need to use scipy, which does not support netCDF version 4 (the\n1845             default format becomes NETCDF3_64BIT).\n1846         mode : {\"w\", \"a\"}, default: \"w\"\n1847             Write ('w') or append ('a') mode. If mode='w', any existing file at\n1848             this location will be overwritten. If mode='a', existing variables\n1849             will be overwritten.\n1850         format : {\"NETCDF4\", \"NETCDF4_CLASSIC\", \"NETCDF3_64BIT\", \\\n1851                   \"NETCDF3_CLASSIC\"}, optional\n1852             File format for the resulting netCDF file:\n1853 \n1854             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API\n1855               features.\n1856             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only\n1857               netCDF 3 compatible API features.\n1858             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,\n1859               which fully supports 2+ GB files, but is only compatible with\n1860               clients linked against netCDF version 3.6.0 or later.\n1861             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not\n1862               handle 2+ GB files very well.\n1863 \n1864             All formats are supported by the netCDF4-python library.\n1865             scipy.io.netcdf only supports the last two formats.\n1866 \n1867             The default format is NETCDF4 if you are saving a file to disk and\n1868             have the netCDF4-python library available. Otherwise, xarray falls\n1869             back to using scipy to write netCDF files and defaults to the\n1870             NETCDF3_64BIT format (scipy does not support netCDF4).\n1871         group : str, optional\n1872             Path to the netCDF4 group in the given file to open (only works for\n1873             format='NETCDF4'). The group(s) will be created if necessary.\n1874         engine : {\"netcdf4\", \"scipy\", \"h5netcdf\"}, optional\n1875             Engine to use when writing netCDF files. If not provided, the\n1876             default engine is chosen based on available dependencies, with a\n1877             preference for 'netcdf4' if writing to a file on disk.\n1878         encoding : dict, optional\n1879             Nested dictionary with variable names as keys and dictionaries of\n1880             variable specific encodings as values, e.g.,\n1881             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,\n1882             \"zlib\": True}, ...}``\n1883 \n1884             The `h5netcdf` engine supports both the NetCDF4-style compression\n1885             encoding parameters ``{\"zlib\": True, \"complevel\": 9}`` and the h5py\n1886             ones ``{\"compression\": \"gzip\", \"compression_opts\": 9}``.\n1887             This allows using any compression plugin installed in the HDF5\n1888             library, e.g. LZF.\n1889 \n1890         unlimited_dims : iterable of hashable, optional\n1891             Dimension(s) that should be serialized as unlimited dimensions.\n1892             By default, no dimensions are treated as unlimited dimensions.\n1893             Note that unlimited_dims may also be set via\n1894             ``dataset.encoding[\"unlimited_dims\"]``.\n1895         compute: bool, default: True\n1896             If true compute immediately, otherwise return a\n1897             ``dask.delayed.Delayed`` object that can be computed later.\n1898         invalid_netcdf: bool, default: False\n1899             Only valid along with ``engine=\"h5netcdf\"``. If True, allow writing\n1900             hdf5 files which are invalid netcdf as described in\n1901             https://github.com/h5netcdf/h5netcdf.\n1902 \n1903         Returns\n1904         -------\n1905             * ``bytes`` if path is None\n1906             * ``dask.delayed.Delayed`` if compute is False\n1907             * None otherwise\n1908 \n1909         See Also\n1910         --------\n1911         DataArray.to_netcdf\n1912         \"\"\"\n1913         if encoding is None:\n1914             encoding = {}\n1915         from xarray.backends.api import to_netcdf\n1916 \n1917         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(\n1918             self,\n1919             path,\n1920             mode=mode,\n1921             format=format,\n1922             group=group,\n1923             engine=engine,\n1924             encoding=encoding,\n1925             unlimited_dims=unlimited_dims,\n1926             compute=compute,\n1927             multifile=False,\n1928             invalid_netcdf=invalid_netcdf,\n1929         )\n1930 \n1931     # compute=True (default) returns ZarrStore\n1932     @overload\n1933     def to_zarr(\n1934         self,\n1935         store: MutableMapping | str | PathLike[str] | None = None,\n1936         chunk_store: MutableMapping | str | PathLike | None = None,\n1937         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1938         synchronizer=None,\n1939         group: str | None = None,\n1940         encoding: Mapping | None = None,\n1941         compute: Literal[True] = True,\n1942         consolidated: bool | None = None,\n1943         append_dim: Hashable | None = None,\n1944         region: Mapping[str, slice] | None = None,\n1945         safe_chunks: bool = True,\n1946         storage_options: dict[str, str] | None = None,\n1947         zarr_version: int | None = None,\n1948     ) -> ZarrStore:\n1949         ...\n1950 \n1951     # compute=False returns dask.Delayed\n1952     @overload\n1953     def to_zarr(\n1954         self,\n1955         store: MutableMapping | str | PathLike[str] | None = None,\n1956         chunk_store: MutableMapping | str | PathLike | None = None,\n1957         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1958         synchronizer=None,\n1959         group: str | None = None,\n1960         encoding: Mapping | None = None,\n1961         *,\n1962         compute: Literal[False],\n1963         consolidated: bool | None = None,\n1964         append_dim: Hashable | None = None,\n1965         region: Mapping[str, slice] | None = None,\n1966         safe_chunks: bool = True,\n1967         storage_options: dict[str, str] | None = None,\n1968         zarr_version: int | None = None,\n1969     ) -> Delayed:\n1970         ...\n1971 \n1972     def to_zarr(\n1973         self,\n1974         store: MutableMapping | str | PathLike[str] | None = None,\n1975         chunk_store: MutableMapping | str | PathLike | None = None,\n1976         mode: Literal[\"w\", \"w-\", \"a\", \"r+\", None] = None,\n1977         synchronizer=None,\n1978         group: str | None = None,\n1979         encoding: Mapping | None = None,\n1980         compute: bool = True,\n1981         consolidated: bool | None = None,\n1982         append_dim: Hashable | None = None,\n1983         region: Mapping[str, slice] | None = None,\n1984         safe_chunks: bool = True,\n1985         storage_options: dict[str, str] | None = None,\n1986         zarr_version: int | None = None,\n1987     ) -> ZarrStore | Delayed:\n1988         \"\"\"Write dataset contents to a zarr group.\n1989 \n1990         Zarr chunks are determined in the following way:\n1991 \n1992         - From the ``chunks`` attribute in each variable's ``encoding``\n1993           (can be set via `Dataset.chunk`).\n1994         - If the variable is a Dask array, from the dask chunks\n1995         - If neither Dask chunks nor encoding chunks are present, chunks will\n1996           be determined automatically by Zarr\n1997         - If both Dask chunks and encoding chunks are present, encoding chunks\n1998           will be used, provided that there is a many-to-one relationship between\n1999           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and\n2000           evenly divide encoding chunks); otherwise raise a ``ValueError``.\n2001           This restriction ensures that no synchronization / locks are required\n2002           when writing. To disable this restriction, use ``safe_chunks=False``.\n2003 \n2004         Parameters\n2005         ----------\n2006         store : MutableMapping, str or path-like, optional\n2007             Store or path to directory in local or remote file system.\n2008         chunk_store : MutableMapping, str or path-like, optional\n2009             Store or path to directory in local or remote file system only for Zarr\n2010             array chunks. Requires zarr-python v2.4.0 or later.\n2011         mode : {\"w\", \"w-\", \"a\", \"r+\", None}, optional\n2012             Persistence mode: \"w\" means create (overwrite if exists);\n2013             \"w-\" means create (fail if exists);\n2014             \"a\" means override existing variables (create if does not exist);\n2015             \"r+\" means modify existing array *values* only (raise an error if\n2016             any metadata or shapes would change).\n2017             The default mode is \"a\" if ``append_dim`` is set. Otherwise, it is\n2018             \"r+\" if ``region`` is set and ``w-`` otherwise.\n2019         synchronizer : object, optional\n2020             Zarr array synchronizer.\n2021         group : str, optional\n2022             Group path. (a.k.a. `path` in zarr terminology.)\n2023         encoding : dict, optional\n2024             Nested dictionary with variable names as keys and dictionaries of\n2025             variable specific encodings as values, e.g.,\n2026             ``{\"my_variable\": {\"dtype\": \"int16\", \"scale_factor\": 0.1,}, ...}``\n2027         compute : bool, default: True\n2028             If True write array data immediately, otherwise return a\n2029             ``dask.delayed.Delayed`` object that can be computed to write\n2030             array data later. Metadata is always updated eagerly.\n2031         consolidated : bool, optional\n2032             If True, apply zarr's `consolidate_metadata` function to the store\n2033             after writing metadata and read existing stores with consolidated\n2034             metadata; if False, do not. The default (`consolidated=None`) means\n2035             write consolidated metadata and attempt to read consolidated\n2036             metadata for existing stores (falling back to non-consolidated).\n2037 \n2038             When the experimental ``zarr_version=3``, ``consolidated`` must be\n2039             either be ``None`` or ``False``.\n2040         append_dim : hashable, optional\n2041             If set, the dimension along which the data will be appended. All\n2042             other dimensions on overridden variables must remain the same size.\n2043         region : dict, optional\n2044             Optional mapping from dimension names to integer slices along\n2045             dataset dimensions to indicate the region of existing zarr array(s)\n2046             in which to write this dataset's data. For example,\n2047             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate\n2048             that values should be written to the region ``0:1000`` along ``x``\n2049             and ``10000:11000`` along ``y``.\n2050 \n2051             Two restrictions apply to the use of ``region``:\n2052 \n2053             - If ``region`` is set, _all_ variables in a dataset must have at\n2054               least one dimension in common with the region. Other variables\n2055               should be written in a separate call to ``to_zarr()``.\n2056             - Dimensions cannot be included in both ``region`` and\n2057               ``append_dim`` at the same time. To create empty arrays to fill\n2058               in with ``region``, use a separate call to ``to_zarr()`` with\n2059               ``compute=False``. See \"Appending to existing Zarr stores\" in\n2060               the reference documentation for full details.\n2061         safe_chunks : bool, default: True\n2062             If True, only allow writes to when there is a many-to-one relationship\n2063             between Zarr chunks (specified in encoding) and Dask chunks.\n2064             Set False to override this restriction; however, data may become corrupted\n2065             if Zarr arrays are written in parallel. This option may be useful in combination\n2066             with ``compute=False`` to initialize a Zarr from an existing\n2067             Dataset with arbitrary chunk structure.\n2068         storage_options : dict, optional\n2069             Any additional parameters for the storage backend (ignored for local\n2070             paths).\n2071         zarr_version : int or None, optional\n2072             The desired zarr spec version to target (currently 2 or 3). The\n2073             default of None will attempt to determine the zarr version from\n2074             ``store`` when possible, otherwise defaulting to 2.\n2075 \n2076         Returns\n2077         -------\n2078             * ``dask.delayed.Delayed`` if compute is False\n2079             * ZarrStore otherwise\n2080 \n2081         References\n2082         ----------\n2083         https://zarr.readthedocs.io/\n2084 \n2085         Notes\n2086         -----\n2087         Zarr chunking behavior:\n2088             If chunks are found in the encoding argument or attribute\n2089             corresponding to any DataArray, those chunks are used.\n2090             If a DataArray is a dask array, it is written with those chunks.\n2091             If not other chunks are found, Zarr uses its own heuristics to\n2092             choose automatic chunk sizes.\n2093 \n2094         encoding:\n2095             The encoding attribute (if exists) of the DataArray(s) will be\n2096             used. Override any existing encodings by providing the ``encoding`` kwarg.\n2097 \n2098         See Also\n2099         --------\n2100         :ref:`io.zarr`\n2101             The I/O user guide, with more details and examples.\n2102         \"\"\"\n2103         from xarray.backends.api import to_zarr\n2104 \n2105         return to_zarr(  # type: ignore[call-overload,misc]\n2106             self,\n2107             store=store,\n2108             chunk_store=chunk_store,\n2109             storage_options=storage_options,\n2110             mode=mode,\n2111             synchronizer=synchronizer,\n2112             group=group,\n2113             encoding=encoding,\n2114             compute=compute,\n2115             consolidated=consolidated,\n2116             append_dim=append_dim,\n2117             region=region,\n2118             safe_chunks=safe_chunks,\n2119             zarr_version=zarr_version,\n2120         )\n2121 \n2122     def __repr__(self) -> str:\n2123         return formatting.dataset_repr(self)\n2124 \n2125     def _repr_html_(self) -> str:\n2126         if OPTIONS[\"display_style\"] == \"text\":\n2127             return f\"<pre>{escape(repr(self))}</pre>\"\n2128         return formatting_html.dataset_repr(self)\n2129 \n2130     def info(self, buf: IO | None = None) -> None:\n2131         \"\"\"\n2132         Concise summary of a Dataset variables and attributes.\n2133 \n2134         Parameters\n2135         ----------\n2136         buf : file-like, default: sys.stdout\n2137             writable buffer\n2138 \n2139         See Also\n2140         --------\n2141         pandas.DataFrame.assign\n2142         ncdump : netCDF's ncdump\n2143         \"\"\"\n2144         if buf is None:  # pragma: no cover\n2145             buf = sys.stdout\n2146 \n2147         lines = []\n2148         lines.append(\"xarray.Dataset {\")\n2149         lines.append(\"dimensions:\")\n2150         for name, size in self.dims.items():\n2151             lines.append(f\"\\t{name} = {size} ;\")\n2152         lines.append(\"\\nvariables:\")\n2153         for name, da in self.variables.items():\n2154             dims = \", \".join(map(str, da.dims))\n2155             lines.append(f\"\\t{da.dtype} {name}({dims}) ;\")\n2156             for k, v in da.attrs.items():\n2157                 lines.append(f\"\\t\\t{name}:{k} = {v} ;\")\n2158         lines.append(\"\\n// global attributes:\")\n2159         for k, v in self.attrs.items():\n2160             lines.append(f\"\\t:{k} = {v} ;\")\n2161         lines.append(\"}\")\n2162 \n2163         buf.write(\"\\n\".join(lines))\n2164 \n2165     @property\n2166     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:\n2167         \"\"\"\n2168         Mapping from dimension names to block lengths for this dataset's data, or None if\n2169         the underlying data is not a dask array.\n2170         Cannot be modified directly, but can be modified by calling .chunk().\n2171 \n2172         Same as Dataset.chunksizes, but maintained for backwards compatibility.\n2173 \n2174         See Also\n2175         --------\n2176         Dataset.chunk\n2177         Dataset.chunksizes\n2178         xarray.unify_chunks\n2179         \"\"\"\n2180         return get_chunksizes(self.variables.values())\n2181 \n2182     @property\n2183     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:\n2184         \"\"\"\n2185         Mapping from dimension names to block lengths for this dataset's data, or None if\n2186         the underlying data is not a dask array.\n2187         Cannot be modified directly, but can be modified by calling .chunk().\n2188 \n2189         Same as Dataset.chunks.\n2190 \n2191         See Also\n2192         --------\n2193         Dataset.chunk\n2194         Dataset.chunks\n2195         xarray.unify_chunks\n2196         \"\"\"\n2197         return get_chunksizes(self.variables.values())\n2198 \n2199     def chunk(\n2200         self: T_Dataset,\n2201         chunks: (\n2202             int | Literal[\"auto\"] | Mapping[Any, None | int | str | tuple[int, ...]]\n2203         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)\n2204         name_prefix: str = \"xarray-\",\n2205         token: str | None = None,\n2206         lock: bool = False,\n2207         inline_array: bool = False,\n2208         **chunks_kwargs: None | int | str | tuple[int, ...],\n2209     ) -> T_Dataset:\n2210         \"\"\"Coerce all arrays in this dataset into dask arrays with the given\n2211         chunks.\n2212 \n2213         Non-dask arrays in this dataset will be converted to dask arrays. Dask\n2214         arrays will be rechunked to the given chunk sizes.\n2215 \n2216         If neither chunks is not provided for one or more dimensions, chunk\n2217         sizes along that dimension will not be updated; non-dask arrays will be\n2218         converted into dask arrays with a single block.\n2219 \n2220         Parameters\n2221         ----------\n2222         chunks : int, tuple of int, \"auto\" or mapping of hashable to int, optional\n2223             Chunk sizes along each dimension, e.g., ``5``, ``\"auto\"``, or\n2224             ``{\"x\": 5, \"y\": 5}``.\n2225         name_prefix : str, default: \"xarray-\"\n2226             Prefix for the name of any new dask arrays.\n2227         token : str, optional\n2228             Token uniquely identifying this dataset.\n2229         lock : bool, default: False\n2230             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2231             already as dask array.\n2232         inline_array: bool, default: False\n2233             Passed on to :py:func:`dask.array.from_array`, if the array is not\n2234             already as dask array.\n2235         **chunks_kwargs : {dim: chunks, ...}, optional\n2236             The keyword arguments form of ``chunks``.\n2237             One of chunks or chunks_kwargs must be provided\n2238 \n2239         Returns\n2240         -------\n2241         chunked : xarray.Dataset\n2242 \n2243         See Also\n2244         --------\n2245         Dataset.chunks\n2246         Dataset.chunksizes\n2247         xarray.unify_chunks\n2248         dask.array.from_array\n2249         \"\"\"\n2250         if chunks is None and chunks_kwargs is None:\n2251             warnings.warn(\n2252                 \"None value for 'chunks' is deprecated. \"\n2253                 \"It will raise an error in the future. Use instead '{}'\",\n2254                 category=FutureWarning,\n2255             )\n2256             chunks = {}\n2257 \n2258         if isinstance(chunks, (Number, str, int)):\n2259             chunks = dict.fromkeys(self.dims, chunks)\n2260         else:\n2261             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, \"chunk\")\n2262 \n2263         bad_dims = chunks.keys() - self.dims.keys()\n2264         if bad_dims:\n2265             raise ValueError(\n2266                 f\"some chunks keys are not dimensions on this object: {bad_dims}\"\n2267             )\n2268 \n2269         variables = {\n2270             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)\n2271             for k, v in self.variables.items()\n2272         }\n2273         return self._replace(variables)\n2274 \n2275     def _validate_indexers(\n2276         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = \"raise\"\n2277     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:\n2278         \"\"\"Here we make sure\n2279         + indexer has a valid keys\n2280         + indexer is in a valid data type\n2281         + string indexers are cast to the appropriate date type if the\n2282           associated index is a DatetimeIndex or CFTimeIndex\n2283         \"\"\"\n2284         from xarray.coding.cftimeindex import CFTimeIndex\n2285         from xarray.core.dataarray import DataArray\n2286 \n2287         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n2288 \n2289         # all indexers should be int, slice, np.ndarrays, or Variable\n2290         for k, v in indexers.items():\n2291             if isinstance(v, (int, slice, Variable)):\n2292                 yield k, v\n2293             elif isinstance(v, DataArray):\n2294                 yield k, v.variable\n2295             elif isinstance(v, tuple):\n2296                 yield k, as_variable(v)\n2297             elif isinstance(v, Dataset):\n2298                 raise TypeError(\"cannot use a Dataset as an indexer\")\n2299             elif isinstance(v, Sequence) and len(v) == 0:\n2300                 yield k, np.empty((0,), dtype=\"int64\")\n2301             else:\n2302                 if not is_duck_array(v):\n2303                     v = np.asarray(v)\n2304 \n2305                 if v.dtype.kind in \"US\":\n2306                     index = self._indexes[k].to_pandas_index()\n2307                     if isinstance(index, pd.DatetimeIndex):\n2308                         v = v.astype(\"datetime64[ns]\")\n2309                     elif isinstance(index, CFTimeIndex):\n2310                         v = _parse_array_of_cftime_strings(v, index.date_type)\n2311 \n2312                 if v.ndim > 1:\n2313                     raise IndexError(\n2314                         \"Unlabeled multi-dimensional array cannot be \"\n2315                         \"used for indexing: {}\".format(k)\n2316                     )\n2317                 yield k, v\n2318 \n2319     def _validate_interp_indexers(\n2320         self, indexers: Mapping[Any, Any]\n2321     ) -> Iterator[tuple[Hashable, Variable]]:\n2322         \"\"\"Variant of _validate_indexers to be used for interpolation\"\"\"\n2323         for k, v in self._validate_indexers(indexers):\n2324             if isinstance(v, Variable):\n2325                 if v.ndim == 1:\n2326                     yield k, v.to_index_variable()\n2327                 else:\n2328                     yield k, v\n2329             elif isinstance(v, int):\n2330                 yield k, Variable((), v, attrs=self.coords[k].attrs)\n2331             elif isinstance(v, np.ndarray):\n2332                 if v.ndim == 0:\n2333                     yield k, Variable((), v, attrs=self.coords[k].attrs)\n2334                 elif v.ndim == 1:\n2335                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)\n2336                 else:\n2337                     raise AssertionError()  # Already tested by _validate_indexers\n2338             else:\n2339                 raise TypeError(type(v))\n2340 \n2341     def _get_indexers_coords_and_indexes(self, indexers):\n2342         \"\"\"Extract coordinates and indexes from indexers.\n2343 \n2344         Only coordinate with a name different from any of self.variables will\n2345         be attached.\n2346         \"\"\"\n2347         from xarray.core.dataarray import DataArray\n2348 \n2349         coords_list = []\n2350         for k, v in indexers.items():\n2351             if isinstance(v, DataArray):\n2352                 if v.dtype.kind == \"b\":\n2353                     if v.ndim != 1:  # we only support 1-d boolean array\n2354                         raise ValueError(\n2355                             \"{:d}d-boolean array is used for indexing along \"\n2356                             \"dimension {!r}, but only 1d boolean arrays are \"\n2357                             \"supported.\".format(v.ndim, k)\n2358                         )\n2359                     # Make sure in case of boolean DataArray, its\n2360                     # coordinate also should be indexed.\n2361                     v_coords = v[v.values.nonzero()[0]].coords\n2362                 else:\n2363                     v_coords = v.coords\n2364                 coords_list.append(v_coords)\n2365 \n2366         # we don't need to call align() explicitly or check indexes for\n2367         # alignment, because merge_variables already checks for exact alignment\n2368         # between dimension coordinates\n2369         coords, indexes = merge_coordinates_without_align(coords_list)\n2370         assert_coordinate_consistent(self, coords)\n2371 \n2372         # silently drop the conflicted variables.\n2373         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}\n2374         attached_indexes = {\n2375             k: v for k, v in indexes.items() if k not in self._variables\n2376         }\n2377         return attached_coords, attached_indexes\n2378 \n2379     def isel(\n2380         self: T_Dataset,\n2381         indexers: Mapping[Any, Any] | None = None,\n2382         drop: bool = False,\n2383         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2384         **indexers_kwargs: Any,\n2385     ) -> T_Dataset:\n2386         \"\"\"Returns a new dataset with each array indexed along the specified\n2387         dimension(s).\n2388 \n2389         This method selects values from each array using its `__getitem__`\n2390         method, except this method does not require knowing the order of\n2391         each array's dimensions.\n2392 \n2393         Parameters\n2394         ----------\n2395         indexers : dict, optional\n2396             A dict with keys matching dimensions and values given\n2397             by integers, slice objects or arrays.\n2398             indexer can be a integer, slice, array-like or DataArray.\n2399             If DataArrays are passed as indexers, xarray-style indexing will be\n2400             carried out. See :ref:`indexing` for the details.\n2401             One of indexers or indexers_kwargs must be provided.\n2402         drop : bool, default: False\n2403             If ``drop=True``, drop coordinates variables indexed by integers\n2404             instead of making them scalar.\n2405         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n2406             What to do if dimensions that should be selected from are not present in the\n2407             Dataset:\n2408             - \"raise\": raise an exception\n2409             - \"warn\": raise a warning, and ignore the missing dimensions\n2410             - \"ignore\": ignore the missing dimensions\n2411 \n2412         **indexers_kwargs : {dim: indexer, ...}, optional\n2413             The keyword arguments form of ``indexers``.\n2414             One of indexers or indexers_kwargs must be provided.\n2415 \n2416         Returns\n2417         -------\n2418         obj : Dataset\n2419             A new Dataset with the same contents as this dataset, except each\n2420             array and dimension is indexed by the appropriate indexers.\n2421             If indexer DataArrays have coordinates that do not conflict with\n2422             this object, then these coordinates will be attached.\n2423             In general, each array's data will be a view of the array's data\n2424             in this dataset, unless vectorized indexing was triggered by using\n2425             an array indexer, in which case the data will be a copy.\n2426 \n2427         See Also\n2428         --------\n2429         Dataset.sel\n2430         DataArray.isel\n2431         \"\"\"\n2432         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n2433         if any(is_fancy_indexer(idx) for idx in indexers.values()):\n2434             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)\n2435 \n2436         # Much faster algorithm for when all indexers are ints, slices, one-dimensional\n2437         # lists, or zero or one-dimensional np.ndarray's\n2438         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n2439 \n2440         variables = {}\n2441         dims: dict[Hashable, int] = {}\n2442         coord_names = self._coord_names.copy()\n2443 \n2444         indexes, index_variables = isel_indexes(self.xindexes, indexers)\n2445 \n2446         for name, var in self._variables.items():\n2447             # preserve variable order\n2448             if name in index_variables:\n2449                 var = index_variables[name]\n2450             else:\n2451                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}\n2452                 if var_indexers:\n2453                     var = var.isel(var_indexers)\n2454                     if drop and var.ndim == 0 and name in coord_names:\n2455                         coord_names.remove(name)\n2456                         continue\n2457             variables[name] = var\n2458             dims.update(zip(var.dims, var.shape))\n2459 \n2460         return self._construct_direct(\n2461             variables=variables,\n2462             coord_names=coord_names,\n2463             dims=dims,\n2464             attrs=self._attrs,\n2465             indexes=indexes,\n2466             encoding=self._encoding,\n2467             close=self._close,\n2468         )\n2469 \n2470     def _isel_fancy(\n2471         self: T_Dataset,\n2472         indexers: Mapping[Any, Any],\n2473         *,\n2474         drop: bool,\n2475         missing_dims: ErrorOptionsWithWarn = \"raise\",\n2476     ) -> T_Dataset:\n2477         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))\n2478 \n2479         variables: dict[Hashable, Variable] = {}\n2480         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)\n2481 \n2482         for name, var in self.variables.items():\n2483             if name in index_variables:\n2484                 new_var = index_variables[name]\n2485             else:\n2486                 var_indexers = {\n2487                     k: v for k, v in valid_indexers.items() if k in var.dims\n2488                 }\n2489                 if var_indexers:\n2490                     new_var = var.isel(indexers=var_indexers)\n2491                     # drop scalar coordinates\n2492                     # https://github.com/pydata/xarray/issues/6554\n2493                     if name in self.coords and drop and new_var.ndim == 0:\n2494                         continue\n2495                 else:\n2496                     new_var = var.copy(deep=False)\n2497                 if name not in indexes:\n2498                     new_var = new_var.to_base_variable()\n2499             variables[name] = new_var\n2500 \n2501         coord_names = self._coord_names & variables.keys()\n2502         selected = self._replace_with_new_dims(variables, coord_names, indexes)\n2503 \n2504         # Extract coordinates from indexers\n2505         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)\n2506         variables.update(coord_vars)\n2507         indexes.update(new_indexes)\n2508         coord_names = self._coord_names & variables.keys() | coord_vars.keys()\n2509         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n2510 \n2511     def sel(\n2512         self: T_Dataset,\n2513         indexers: Mapping[Any, Any] | None = None,\n2514         method: str | None = None,\n2515         tolerance: int | float | Iterable[int | float] | None = None,\n2516         drop: bool = False,\n2517         **indexers_kwargs: Any,\n2518     ) -> T_Dataset:\n2519         \"\"\"Returns a new dataset with each array indexed by tick labels\n2520         along the specified dimension(s).\n2521 \n2522         In contrast to `Dataset.isel`, indexers for this method should use\n2523         labels instead of integers.\n2524 \n2525         Under the hood, this method is powered by using pandas's powerful Index\n2526         objects. This makes label based indexing essentially just as fast as\n2527         using integer indexing.\n2528 \n2529         It also means this method uses pandas's (well documented) logic for\n2530         indexing. This means you can use string shortcuts for datetime indexes\n2531         (e.g., '2000-01' to select all values in January 2000). It also means\n2532         that slices are treated as inclusive of both the start and stop values,\n2533         unlike normal Python indexing.\n2534 \n2535         Parameters\n2536         ----------\n2537         indexers : dict, optional\n2538             A dict with keys matching dimensions and values given\n2539             by scalars, slices or arrays of tick labels. For dimensions with\n2540             multi-index, the indexer may also be a dict-like object with keys\n2541             matching index level names.\n2542             If DataArrays are passed as indexers, xarray-style indexing will be\n2543             carried out. See :ref:`indexing` for the details.\n2544             One of indexers or indexers_kwargs must be provided.\n2545         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n2546             Method to use for inexact matches:\n2547 \n2548             * None (default): only exact matches\n2549             * pad / ffill: propagate last valid index value forward\n2550             * backfill / bfill: propagate next valid index value backward\n2551             * nearest: use nearest valid index value\n2552         tolerance : optional\n2553             Maximum distance between original and new labels for inexact\n2554             matches. The values of the index at the matching locations must\n2555             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2556         drop : bool, optional\n2557             If ``drop=True``, drop coordinates variables in `indexers` instead\n2558             of making them scalar.\n2559         **indexers_kwargs : {dim: indexer, ...}, optional\n2560             The keyword arguments form of ``indexers``.\n2561             One of indexers or indexers_kwargs must be provided.\n2562 \n2563         Returns\n2564         -------\n2565         obj : Dataset\n2566             A new Dataset with the same contents as this dataset, except each\n2567             variable and dimension is indexed by the appropriate indexers.\n2568             If indexer DataArrays have coordinates that do not conflict with\n2569             this object, then these coordinates will be attached.\n2570             In general, each array's data will be a view of the array's data\n2571             in this dataset, unless vectorized indexing was triggered by using\n2572             an array indexer, in which case the data will be a copy.\n2573 \n2574         See Also\n2575         --------\n2576         Dataset.isel\n2577         DataArray.sel\n2578         \"\"\"\n2579         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"sel\")\n2580         query_results = map_index_queries(\n2581             self, indexers=indexers, method=method, tolerance=tolerance\n2582         )\n2583 \n2584         if drop:\n2585             no_scalar_variables = {}\n2586             for k, v in query_results.variables.items():\n2587                 if v.dims:\n2588                     no_scalar_variables[k] = v\n2589                 else:\n2590                     if k in self._coord_names:\n2591                         query_results.drop_coords.append(k)\n2592             query_results.variables = no_scalar_variables\n2593 \n2594         result = self.isel(indexers=query_results.dim_indexers, drop=drop)\n2595         return result._overwrite_indexes(*query_results.as_tuple()[1:])\n2596 \n2597     def head(\n2598         self: T_Dataset,\n2599         indexers: Mapping[Any, int] | int | None = None,\n2600         **indexers_kwargs: Any,\n2601     ) -> T_Dataset:\n2602         \"\"\"Returns a new dataset with the first `n` values of each array\n2603         for the specified dimension(s).\n2604 \n2605         Parameters\n2606         ----------\n2607         indexers : dict or int, default: 5\n2608             A dict with keys matching dimensions and integer values `n`\n2609             or a single integer `n` applied over all dimensions.\n2610             One of indexers or indexers_kwargs must be provided.\n2611         **indexers_kwargs : {dim: n, ...}, optional\n2612             The keyword arguments form of ``indexers``.\n2613             One of indexers or indexers_kwargs must be provided.\n2614 \n2615         See Also\n2616         --------\n2617         Dataset.tail\n2618         Dataset.thin\n2619         DataArray.head\n2620         \"\"\"\n2621         if not indexers_kwargs:\n2622             if indexers is None:\n2623                 indexers = 5\n2624             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2625                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2626         if isinstance(indexers, int):\n2627             indexers = {dim: indexers for dim in self.dims}\n2628         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"head\")\n2629         for k, v in indexers.items():\n2630             if not isinstance(v, int):\n2631                 raise TypeError(\n2632                     \"expected integer type indexer for \"\n2633                     f\"dimension {k!r}, found {type(v)!r}\"\n2634                 )\n2635             elif v < 0:\n2636                 raise ValueError(\n2637                     \"expected positive integer as indexer \"\n2638                     f\"for dimension {k!r}, found {v}\"\n2639                 )\n2640         indexers_slices = {k: slice(val) for k, val in indexers.items()}\n2641         return self.isel(indexers_slices)\n2642 \n2643     def tail(\n2644         self: T_Dataset,\n2645         indexers: Mapping[Any, int] | int | None = None,\n2646         **indexers_kwargs: Any,\n2647     ) -> T_Dataset:\n2648         \"\"\"Returns a new dataset with the last `n` values of each array\n2649         for the specified dimension(s).\n2650 \n2651         Parameters\n2652         ----------\n2653         indexers : dict or int, default: 5\n2654             A dict with keys matching dimensions and integer values `n`\n2655             or a single integer `n` applied over all dimensions.\n2656             One of indexers or indexers_kwargs must be provided.\n2657         **indexers_kwargs : {dim: n, ...}, optional\n2658             The keyword arguments form of ``indexers``.\n2659             One of indexers or indexers_kwargs must be provided.\n2660 \n2661         See Also\n2662         --------\n2663         Dataset.head\n2664         Dataset.thin\n2665         DataArray.tail\n2666         \"\"\"\n2667         if not indexers_kwargs:\n2668             if indexers is None:\n2669                 indexers = 5\n2670             if not isinstance(indexers, int) and not is_dict_like(indexers):\n2671                 raise TypeError(\"indexers must be either dict-like or a single integer\")\n2672         if isinstance(indexers, int):\n2673             indexers = {dim: indexers for dim in self.dims}\n2674         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"tail\")\n2675         for k, v in indexers.items():\n2676             if not isinstance(v, int):\n2677                 raise TypeError(\n2678                     \"expected integer type indexer for \"\n2679                     f\"dimension {k!r}, found {type(v)!r}\"\n2680                 )\n2681             elif v < 0:\n2682                 raise ValueError(\n2683                     \"expected positive integer as indexer \"\n2684                     f\"for dimension {k!r}, found {v}\"\n2685                 )\n2686         indexers_slices = {\n2687             k: slice(-val, None) if val != 0 else slice(val)\n2688             for k, val in indexers.items()\n2689         }\n2690         return self.isel(indexers_slices)\n2691 \n2692     def thin(\n2693         self: T_Dataset,\n2694         indexers: Mapping[Any, int] | int | None = None,\n2695         **indexers_kwargs: Any,\n2696     ) -> T_Dataset:\n2697         \"\"\"Returns a new dataset with each array indexed along every `n`-th\n2698         value for the specified dimension(s)\n2699 \n2700         Parameters\n2701         ----------\n2702         indexers : dict or int\n2703             A dict with keys matching dimensions and integer values `n`\n2704             or a single integer `n` applied over all dimensions.\n2705             One of indexers or indexers_kwargs must be provided.\n2706         **indexers_kwargs : {dim: n, ...}, optional\n2707             The keyword arguments form of ``indexers``.\n2708             One of indexers or indexers_kwargs must be provided.\n2709 \n2710         Examples\n2711         --------\n2712         >>> x_arr = np.arange(0, 26)\n2713         >>> x_arr\n2714         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n2715                17, 18, 19, 20, 21, 22, 23, 24, 25])\n2716         >>> x = xr.DataArray(\n2717         ...     np.reshape(x_arr, (2, 13)),\n2718         ...     dims=(\"x\", \"y\"),\n2719         ...     coords={\"x\": [0, 1], \"y\": np.arange(0, 13)},\n2720         ... )\n2721         >>> x_ds = xr.Dataset({\"foo\": x})\n2722         >>> x_ds\n2723         <xarray.Dataset>\n2724         Dimensions:  (x: 2, y: 13)\n2725         Coordinates:\n2726           * x        (x) int64 0 1\n2727           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12\n2728         Data variables:\n2729             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25\n2730 \n2731         >>> x_ds.thin(3)\n2732         <xarray.Dataset>\n2733         Dimensions:  (x: 1, y: 5)\n2734         Coordinates:\n2735           * x        (x) int64 0\n2736           * y        (y) int64 0 3 6 9 12\n2737         Data variables:\n2738             foo      (x, y) int64 0 3 6 9 12\n2739         >>> x.thin({\"x\": 2, \"y\": 5})\n2740         <xarray.DataArray (x: 1, y: 3)>\n2741         array([[ 0,  5, 10]])\n2742         Coordinates:\n2743           * x        (x) int64 0\n2744           * y        (y) int64 0 5 10\n2745 \n2746         See Also\n2747         --------\n2748         Dataset.head\n2749         Dataset.tail\n2750         DataArray.thin\n2751         \"\"\"\n2752         if (\n2753             not indexers_kwargs\n2754             and not isinstance(indexers, int)\n2755             and not is_dict_like(indexers)\n2756         ):\n2757             raise TypeError(\"indexers must be either dict-like or a single integer\")\n2758         if isinstance(indexers, int):\n2759             indexers = {dim: indexers for dim in self.dims}\n2760         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"thin\")\n2761         for k, v in indexers.items():\n2762             if not isinstance(v, int):\n2763                 raise TypeError(\n2764                     \"expected integer type indexer for \"\n2765                     f\"dimension {k!r}, found {type(v)!r}\"\n2766                 )\n2767             elif v < 0:\n2768                 raise ValueError(\n2769                     \"expected positive integer as indexer \"\n2770                     f\"for dimension {k!r}, found {v}\"\n2771                 )\n2772             elif v == 0:\n2773                 raise ValueError(\"step cannot be zero\")\n2774         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}\n2775         return self.isel(indexers_slices)\n2776 \n2777     def broadcast_like(\n2778         self: T_Dataset,\n2779         other: Dataset | DataArray,\n2780         exclude: Iterable[Hashable] | None = None,\n2781     ) -> T_Dataset:\n2782         \"\"\"Broadcast this DataArray against another Dataset or DataArray.\n2783         This is equivalent to xr.broadcast(other, self)[1]\n2784 \n2785         Parameters\n2786         ----------\n2787         other : Dataset or DataArray\n2788             Object against which to broadcast this array.\n2789         exclude : iterable of hashable, optional\n2790             Dimensions that must not be broadcasted\n2791 \n2792         \"\"\"\n2793         if exclude is None:\n2794             exclude = set()\n2795         else:\n2796             exclude = set(exclude)\n2797         args = align(other, self, join=\"outer\", copy=False, exclude=exclude)\n2798 \n2799         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)\n2800 \n2801         return _broadcast_helper(\n2802             cast(\"T_Dataset\", args[1]), exclude, dims_map, common_coords\n2803         )\n2804 \n2805     def _reindex_callback(\n2806         self,\n2807         aligner: alignment.Aligner,\n2808         dim_pos_indexers: dict[Hashable, Any],\n2809         variables: dict[Hashable, Variable],\n2810         indexes: dict[Hashable, Index],\n2811         fill_value: Any,\n2812         exclude_dims: frozenset[Hashable],\n2813         exclude_vars: frozenset[Hashable],\n2814     ) -> Dataset:\n2815         \"\"\"Callback called from ``Aligner`` to create a new reindexed Dataset.\"\"\"\n2816 \n2817         new_variables = variables.copy()\n2818         new_indexes = indexes.copy()\n2819 \n2820         # re-assign variable metadata\n2821         for name, new_var in new_variables.items():\n2822             var = self._variables.get(name)\n2823             if var is not None:\n2824                 new_var.attrs = var.attrs\n2825                 new_var.encoding = var.encoding\n2826 \n2827         # pass through indexes from excluded dimensions\n2828         # no extra check needed for multi-coordinate indexes, potential conflicts\n2829         # should already have been detected when aligning the indexes\n2830         for name, idx in self._indexes.items():\n2831             var = self._variables[name]\n2832             if set(var.dims) <= exclude_dims:\n2833                 new_indexes[name] = idx\n2834                 new_variables[name] = var\n2835 \n2836         if not dim_pos_indexers:\n2837             # fast path for no reindexing necessary\n2838             if set(new_indexes) - set(self._indexes):\n2839                 # this only adds new indexes and their coordinate variables\n2840                 reindexed = self._overwrite_indexes(new_indexes, new_variables)\n2841             else:\n2842                 reindexed = self.copy(deep=aligner.copy)\n2843         else:\n2844             to_reindex = {\n2845                 k: v\n2846                 for k, v in self.variables.items()\n2847                 if k not in variables and k not in exclude_vars\n2848             }\n2849             reindexed_vars = alignment.reindex_variables(\n2850                 to_reindex,\n2851                 dim_pos_indexers,\n2852                 copy=aligner.copy,\n2853                 fill_value=fill_value,\n2854                 sparse=aligner.sparse,\n2855             )\n2856             new_variables.update(reindexed_vars)\n2857             new_coord_names = self._coord_names | set(new_indexes)\n2858             reindexed = self._replace_with_new_dims(\n2859                 new_variables, new_coord_names, indexes=new_indexes\n2860             )\n2861 \n2862         return reindexed\n2863 \n2864     def reindex_like(\n2865         self: T_Dataset,\n2866         other: Dataset | DataArray,\n2867         method: ReindexMethodOptions = None,\n2868         tolerance: int | float | Iterable[int | float] | None = None,\n2869         copy: bool = True,\n2870         fill_value: Any = xrdtypes.NA,\n2871     ) -> T_Dataset:\n2872         \"\"\"Conform this object onto the indexes of another object, filling in\n2873         missing values with ``fill_value``. The default fill value is NaN.\n2874 \n2875         Parameters\n2876         ----------\n2877         other : Dataset or DataArray\n2878             Object with an 'indexes' attribute giving a mapping from dimension\n2879             names to pandas.Index objects, which provides coordinates upon\n2880             which to index the variables in this dataset. The indexes on this\n2881             other object need not be the same as the indexes on this\n2882             dataset. Any mis-matched index values will be filled in with\n2883             NaN, and any mis-matched dimension names will simply be ignored.\n2884         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n2885             Method to use for filling index values from other not found in this\n2886             dataset:\n2887 \n2888             - None (default): don't fill gaps\n2889             - \"pad\" / \"ffill\": propagate last valid index value forward\n2890             - \"backfill\" / \"bfill\": propagate next valid index value backward\n2891             - \"nearest\": use nearest valid index value\n2892 \n2893         tolerance : optional\n2894             Maximum distance between original and new labels for inexact\n2895             matches. The values of the index at the matching locations must\n2896             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2897             Tolerance may be a scalar value, which applies the same tolerance\n2898             to all values, or list-like, which applies variable tolerance per\n2899             element. List-like must be the same size as the index and its dtype\n2900             must exactly match the index\u2019s type.\n2901         copy : bool, default: True\n2902             If ``copy=True``, data in the return value is always copied. If\n2903             ``copy=False`` and reindexing is unnecessary, or can be performed\n2904             with only slice operations, then the output may share memory with\n2905             the input. In either case, a new xarray object is always returned.\n2906         fill_value : scalar or dict-like, optional\n2907             Value to use for newly missing values. If a dict-like maps\n2908             variable names to fill values.\n2909 \n2910         Returns\n2911         -------\n2912         reindexed : Dataset\n2913             Another dataset, with this dataset's data but coordinates from the\n2914             other object.\n2915 \n2916         See Also\n2917         --------\n2918         Dataset.reindex\n2919         align\n2920         \"\"\"\n2921         return alignment.reindex_like(\n2922             self,\n2923             other=other,\n2924             method=method,\n2925             tolerance=tolerance,\n2926             copy=copy,\n2927             fill_value=fill_value,\n2928         )\n2929 \n2930     def reindex(\n2931         self: T_Dataset,\n2932         indexers: Mapping[Any, Any] | None = None,\n2933         method: ReindexMethodOptions = None,\n2934         tolerance: int | float | Iterable[int | float] | None = None,\n2935         copy: bool = True,\n2936         fill_value: Any = xrdtypes.NA,\n2937         **indexers_kwargs: Any,\n2938     ) -> T_Dataset:\n2939         \"\"\"Conform this object onto a new set of indexes, filling in\n2940         missing values with ``fill_value``. The default fill value is NaN.\n2941 \n2942         Parameters\n2943         ----------\n2944         indexers : dict, optional\n2945             Dictionary with keys given by dimension names and values given by\n2946             arrays of coordinates tick labels. Any mis-matched coordinate\n2947             values will be filled in with NaN, and any mis-matched dimension\n2948             names will simply be ignored.\n2949             One of indexers or indexers_kwargs must be provided.\n2950         method : {None, \"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\", None}, optional\n2951             Method to use for filling index values in ``indexers`` not found in\n2952             this dataset:\n2953 \n2954             - None (default): don't fill gaps\n2955             - \"pad\" / \"ffill\": propagate last valid index value forward\n2956             - \"backfill\" / \"bfill\": propagate next valid index value backward\n2957             - \"nearest\": use nearest valid index value\n2958 \n2959         tolerance : optional\n2960             Maximum distance between original and new labels for inexact\n2961             matches. The values of the index at the matching locations must\n2962             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.\n2963             Tolerance may be a scalar value, which applies the same tolerance\n2964             to all values, or list-like, which applies variable tolerance per\n2965             element. List-like must be the same size as the index and its dtype\n2966             must exactly match the index\u2019s type.\n2967         copy : bool, default: True\n2968             If ``copy=True``, data in the return value is always copied. If\n2969             ``copy=False`` and reindexing is unnecessary, or can be performed\n2970             with only slice operations, then the output may share memory with\n2971             the input. In either case, a new xarray object is always returned.\n2972         fill_value : scalar or dict-like, optional\n2973             Value to use for newly missing values. If a dict-like,\n2974             maps variable names (including coordinates) to fill values.\n2975         sparse : bool, default: False\n2976             use sparse-array.\n2977         **indexers_kwargs : {dim: indexer, ...}, optional\n2978             Keyword arguments in the same form as ``indexers``.\n2979             One of indexers or indexers_kwargs must be provided.\n2980 \n2981         Returns\n2982         -------\n2983         reindexed : Dataset\n2984             Another dataset, with this dataset's data but replaced coordinates.\n2985 \n2986         See Also\n2987         --------\n2988         Dataset.reindex_like\n2989         align\n2990         pandas.Index.get_indexer\n2991 \n2992         Examples\n2993         --------\n2994         Create a dataset with some fictional data.\n2995 \n2996         >>> x = xr.Dataset(\n2997         ...     {\n2998         ...         \"temperature\": (\"station\", 20 * np.random.rand(4)),\n2999         ...         \"pressure\": (\"station\", 500 * np.random.rand(4)),\n3000         ...     },\n3001         ...     coords={\"station\": [\"boston\", \"nyc\", \"seattle\", \"denver\"]},\n3002         ... )\n3003         >>> x\n3004         <xarray.Dataset>\n3005         Dimensions:      (station: 4)\n3006         Coordinates:\n3007           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'\n3008         Data variables:\n3009             temperature  (station) float64 10.98 14.3 12.06 10.9\n3010             pressure     (station) float64 211.8 322.9 218.8 445.9\n3011         >>> x.indexes\n3012         Indexes:\n3013             station  Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')\n3014 \n3015         Create a new index and reindex the dataset. By default values in the new index that\n3016         do not have corresponding records in the dataset are assigned `NaN`.\n3017 \n3018         >>> new_index = [\"boston\", \"austin\", \"seattle\", \"lincoln\"]\n3019         >>> x.reindex({\"station\": new_index})\n3020         <xarray.Dataset>\n3021         Dimensions:      (station: 4)\n3022         Coordinates:\n3023           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n3024         Data variables:\n3025             temperature  (station) float64 10.98 nan 12.06 nan\n3026             pressure     (station) float64 211.8 nan 218.8 nan\n3027 \n3028         We can fill in the missing values by passing a value to the keyword `fill_value`.\n3029 \n3030         >>> x.reindex({\"station\": new_index}, fill_value=0)\n3031         <xarray.Dataset>\n3032         Dimensions:      (station: 4)\n3033         Coordinates:\n3034           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n3035         Data variables:\n3036             temperature  (station) float64 10.98 0.0 12.06 0.0\n3037             pressure     (station) float64 211.8 0.0 218.8 0.0\n3038 \n3039         We can also use different fill values for each variable.\n3040 \n3041         >>> x.reindex(\n3042         ...     {\"station\": new_index}, fill_value={\"temperature\": 0, \"pressure\": 100}\n3043         ... )\n3044         <xarray.Dataset>\n3045         Dimensions:      (station: 4)\n3046         Coordinates:\n3047           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'\n3048         Data variables:\n3049             temperature  (station) float64 10.98 0.0 12.06 0.0\n3050             pressure     (station) float64 211.8 100.0 218.8 100.0\n3051 \n3052         Because the index is not monotonically increasing or decreasing, we cannot use arguments\n3053         to the keyword method to fill the `NaN` values.\n3054 \n3055         >>> x.reindex({\"station\": new_index}, method=\"nearest\")\n3056         Traceback (most recent call last):\n3057         ...\n3058             raise ValueError('index must be monotonic increasing or decreasing')\n3059         ValueError: index must be monotonic increasing or decreasing\n3060 \n3061         To further illustrate the filling functionality in reindex, we will create a\n3062         dataset with a monotonically increasing index (for example, a sequence of dates).\n3063 \n3064         >>> x2 = xr.Dataset(\n3065         ...     {\n3066         ...         \"temperature\": (\n3067         ...             \"time\",\n3068         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],\n3069         ...         ),\n3070         ...         \"pressure\": (\"time\", 500 * np.random.rand(6)),\n3071         ...     },\n3072         ...     coords={\"time\": pd.date_range(\"01/01/2019\", periods=6, freq=\"D\")},\n3073         ... )\n3074         >>> x2\n3075         <xarray.Dataset>\n3076         Dimensions:      (time: 6)\n3077         Coordinates:\n3078           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06\n3079         Data variables:\n3080             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12\n3081             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8\n3082 \n3083         Suppose we decide to expand the dataset to cover a wider date range.\n3084 \n3085         >>> time_index2 = pd.date_range(\"12/29/2018\", periods=10, freq=\"D\")\n3086         >>> x2.reindex({\"time\": time_index2})\n3087         <xarray.Dataset>\n3088         Dimensions:      (time: 10)\n3089         Coordinates:\n3090           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n3091         Data variables:\n3092             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan\n3093             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan\n3094 \n3095         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)\n3096         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.\n3097 \n3098         For example, to back-propagate the last valid value to fill the `NaN` values,\n3099         pass `bfill` as an argument to the `method` keyword.\n3100 \n3101         >>> x3 = x2.reindex({\"time\": time_index2}, method=\"bfill\")\n3102         >>> x3\n3103         <xarray.Dataset>\n3104         Dimensions:      (time: 10)\n3105         Coordinates:\n3106           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07\n3107         Data variables:\n3108             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan\n3109             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan\n3110 \n3111         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)\n3112         will not be filled by any of the value propagation schemes.\n3113 \n3114         >>> x2.where(x2.temperature.isnull(), drop=True)\n3115         <xarray.Dataset>\n3116         Dimensions:      (time: 1)\n3117         Coordinates:\n3118           * time         (time) datetime64[ns] 2019-01-03\n3119         Data variables:\n3120             temperature  (time) float64 nan\n3121             pressure     (time) float64 395.9\n3122         >>> x3.where(x3.temperature.isnull(), drop=True)\n3123         <xarray.Dataset>\n3124         Dimensions:      (time: 2)\n3125         Coordinates:\n3126           * time         (time) datetime64[ns] 2019-01-03 2019-01-07\n3127         Data variables:\n3128             temperature  (time) float64 nan nan\n3129             pressure     (time) float64 395.9 nan\n3130 \n3131         This is because filling while reindexing does not look at dataset values, but only compares\n3132         the original and desired indexes. If you do want to fill in the `NaN` values present in the\n3133         original dataset, use the :py:meth:`~Dataset.fillna()` method.\n3134 \n3135         \"\"\"\n3136         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n3137         return alignment.reindex(\n3138             self,\n3139             indexers=indexers,\n3140             method=method,\n3141             tolerance=tolerance,\n3142             copy=copy,\n3143             fill_value=fill_value,\n3144         )\n3145 \n3146     def _reindex(\n3147         self: T_Dataset,\n3148         indexers: Mapping[Any, Any] | None = None,\n3149         method: str | None = None,\n3150         tolerance: int | float | Iterable[int | float] | None = None,\n3151         copy: bool = True,\n3152         fill_value: Any = xrdtypes.NA,\n3153         sparse: bool = False,\n3154         **indexers_kwargs: Any,\n3155     ) -> T_Dataset:\n3156         \"\"\"\n3157         Same as reindex but supports sparse option.\n3158         \"\"\"\n3159         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, \"reindex\")\n3160         return alignment.reindex(\n3161             self,\n3162             indexers=indexers,\n3163             method=method,\n3164             tolerance=tolerance,\n3165             copy=copy,\n3166             fill_value=fill_value,\n3167             sparse=sparse,\n3168         )\n3169 \n3170     def interp(\n3171         self: T_Dataset,\n3172         coords: Mapping[Any, Any] | None = None,\n3173         method: InterpOptions = \"linear\",\n3174         assume_sorted: bool = False,\n3175         kwargs: Mapping[str, Any] | None = None,\n3176         method_non_numeric: str = \"nearest\",\n3177         **coords_kwargs: Any,\n3178     ) -> T_Dataset:\n3179         \"\"\"Interpolate a Dataset onto new coordinates\n3180 \n3181         Performs univariate or multivariate interpolation of a Dataset onto\n3182         new coordinates using scipy's interpolation routines. If interpolating\n3183         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is\n3184         called.  When interpolating along multiple existing dimensions, an\n3185         attempt is made to decompose the interpolation into multiple\n3186         1-dimensional interpolations. If this is possible,\n3187         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n3188         :py:func:`scipy.interpolate.interpn` is called.\n3189 \n3190         Parameters\n3191         ----------\n3192         coords : dict, optional\n3193             Mapping from dimension names to the new coordinates.\n3194             New coordinate can be a scalar, array-like or DataArray.\n3195             If DataArrays are passed as new coordinates, their dimensions are\n3196             used for the broadcasting. Missing values are skipped.\n3197         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3198             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3199             String indicating which method to use for interpolation:\n3200 \n3201             - 'linear': linear interpolation. Additional keyword\n3202               arguments are passed to :py:func:`numpy.interp`\n3203             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3204               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3205               ``method='polynomial'``, the ``order`` keyword argument must also be\n3206               provided.\n3207             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3208               respective :py:class:`scipy.interpolate` classes.\n3209 \n3210         assume_sorted : bool, default: False\n3211             If False, values of coordinates that are interpolated over can be\n3212             in any order and they are sorted first. If True, interpolated\n3213             coordinates are assumed to be an array of monotonically increasing\n3214             values.\n3215         kwargs : dict, optional\n3216             Additional keyword arguments passed to scipy's interpolator. Valid\n3217             options and their behavior depend whether ``interp1d`` or\n3218             ``interpn`` is used.\n3219         method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n3220             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n3221             ``\"nearest\"`` is used by default.\n3222         **coords_kwargs : {dim: coordinate, ...}, optional\n3223             The keyword arguments form of ``coords``.\n3224             One of coords or coords_kwargs must be provided.\n3225 \n3226         Returns\n3227         -------\n3228         interpolated : Dataset\n3229             New dataset on the new coordinates.\n3230 \n3231         Notes\n3232         -----\n3233         scipy is required.\n3234 \n3235         See Also\n3236         --------\n3237         scipy.interpolate.interp1d\n3238         scipy.interpolate.interpn\n3239 \n3240         Examples\n3241         --------\n3242         >>> ds = xr.Dataset(\n3243         ...     data_vars={\n3244         ...         \"a\": (\"x\", [5, 7, 4]),\n3245         ...         \"b\": (\n3246         ...             (\"x\", \"y\"),\n3247         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],\n3248         ...         ),\n3249         ...     },\n3250         ...     coords={\"x\": [0, 1, 2], \"y\": [10, 12, 14, 16]},\n3251         ... )\n3252         >>> ds\n3253         <xarray.Dataset>\n3254         Dimensions:  (x: 3, y: 4)\n3255         Coordinates:\n3256           * x        (x) int64 0 1 2\n3257           * y        (y) int64 10 12 14 16\n3258         Data variables:\n3259             a        (x) int64 5 7 4\n3260             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0\n3261 \n3262         1D interpolation with the default method (linear):\n3263 \n3264         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])\n3265         <xarray.Dataset>\n3266         Dimensions:  (x: 4, y: 4)\n3267         Coordinates:\n3268           * y        (y) int64 10 12 14 16\n3269           * x        (x) float64 0.0 0.75 1.25 1.75\n3270         Data variables:\n3271             a        (x) float64 5.0 6.5 6.25 4.75\n3272             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan\n3273 \n3274         1D interpolation with a different method:\n3275 \n3276         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method=\"nearest\")\n3277         <xarray.Dataset>\n3278         Dimensions:  (x: 4, y: 4)\n3279         Coordinates:\n3280           * y        (y) int64 10 12 14 16\n3281           * x        (x) float64 0.0 0.75 1.25 1.75\n3282         Data variables:\n3283             a        (x) float64 5.0 7.0 7.0 4.0\n3284             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0\n3285 \n3286         1D extrapolation:\n3287 \n3288         >>> ds.interp(\n3289         ...     x=[1, 1.5, 2.5, 3.5],\n3290         ...     method=\"linear\",\n3291         ...     kwargs={\"fill_value\": \"extrapolate\"},\n3292         ... )\n3293         <xarray.Dataset>\n3294         Dimensions:  (x: 4, y: 4)\n3295         Coordinates:\n3296           * y        (y) int64 10 12 14 16\n3297           * x        (x) float64 1.0 1.5 2.5 3.5\n3298         Data variables:\n3299             a        (x) float64 7.0 5.5 2.5 -0.5\n3300             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan\n3301 \n3302         2D interpolation:\n3303 \n3304         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method=\"linear\")\n3305         <xarray.Dataset>\n3306         Dimensions:  (x: 4, y: 3)\n3307         Coordinates:\n3308           * x        (x) float64 0.0 0.75 1.25 1.75\n3309           * y        (y) int64 11 13 15\n3310         Data variables:\n3311             a        (x) float64 5.0 6.5 6.25 4.75\n3312             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan\n3313         \"\"\"\n3314         from xarray.core import missing\n3315 \n3316         if kwargs is None:\n3317             kwargs = {}\n3318 \n3319         coords = either_dict_or_kwargs(coords, coords_kwargs, \"interp\")\n3320         indexers = dict(self._validate_interp_indexers(coords))\n3321 \n3322         if coords:\n3323             # This avoids broadcasting over coordinates that are both in\n3324             # the original array AND in the indexing array. It essentially\n3325             # forces interpolation along the shared coordinates.\n3326             sdims = (\n3327                 set(self.dims)\n3328                 .intersection(*[set(nx.dims) for nx in indexers.values()])\n3329                 .difference(coords.keys())\n3330             )\n3331             indexers.update({d: self.variables[d] for d in sdims})\n3332 \n3333         obj = self if assume_sorted else self.sortby([k for k in coords])\n3334 \n3335         def maybe_variable(obj, k):\n3336             # workaround to get variable for dimension without coordinate.\n3337             try:\n3338                 return obj._variables[k]\n3339             except KeyError:\n3340                 return as_variable((k, range(obj.dims[k])))\n3341 \n3342         def _validate_interp_indexer(x, new_x):\n3343             # In the case of datetimes, the restrictions placed on indexers\n3344             # used with interp are stronger than those which are placed on\n3345             # isel, so we need an additional check after _validate_indexers.\n3346             if _contains_datetime_like_objects(\n3347                 x\n3348             ) and not _contains_datetime_like_objects(new_x):\n3349                 raise TypeError(\n3350                     \"When interpolating over a datetime-like \"\n3351                     \"coordinate, the coordinates to \"\n3352                     \"interpolate to must be either datetime \"\n3353                     \"strings or datetimes. \"\n3354                     \"Instead got\\n{}\".format(new_x)\n3355                 )\n3356             return x, new_x\n3357 \n3358         validated_indexers = {\n3359             k: _validate_interp_indexer(maybe_variable(obj, k), v)\n3360             for k, v in indexers.items()\n3361         }\n3362 \n3363         # optimization: subset to coordinate range of the target index\n3364         if method in [\"linear\", \"nearest\"]:\n3365             for k, v in validated_indexers.items():\n3366                 obj, newidx = missing._localize(obj, {k: v})\n3367                 validated_indexers[k] = newidx[k]\n3368 \n3369         # optimization: create dask coordinate arrays once per Dataset\n3370         # rather than once per Variable when dask.array.unify_chunks is called later\n3371         # GH4739\n3372         if obj.__dask_graph__():\n3373             dask_indexers = {\n3374                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())\n3375                 for k, (index, dest) in validated_indexers.items()\n3376             }\n3377 \n3378         variables: dict[Hashable, Variable] = {}\n3379         reindex: bool = False\n3380         for name, var in obj._variables.items():\n3381             if name in indexers:\n3382                 continue\n3383 \n3384             if is_duck_dask_array(var.data):\n3385                 use_indexers = dask_indexers\n3386             else:\n3387                 use_indexers = validated_indexers\n3388 \n3389             dtype_kind = var.dtype.kind\n3390             if dtype_kind in \"uifc\":\n3391                 # For normal number types do the interpolation:\n3392                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}\n3393                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)\n3394             elif dtype_kind in \"ObU\" and (use_indexers.keys() & var.dims):\n3395                 # For types that we do not understand do stepwise\n3396                 # interpolation to avoid modifying the elements.\n3397                 # reindex the variable instead because it supports\n3398                 # booleans and objects and retains the dtype but inside\n3399                 # this loop there might be some duplicate code that slows it\n3400                 # down, therefore collect these signals and run it later:\n3401                 reindex = True\n3402             elif all(d not in indexers for d in var.dims):\n3403                 # For anything else we can only keep variables if they\n3404                 # are not dependent on any coords that are being\n3405                 # interpolated along:\n3406                 variables[name] = var\n3407 \n3408         if reindex:\n3409             reindex_indexers = {\n3410                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)\n3411             }\n3412             reindexed = alignment.reindex(\n3413                 obj,\n3414                 indexers=reindex_indexers,\n3415                 method=method_non_numeric,\n3416                 exclude_vars=variables.keys(),\n3417             )\n3418             indexes = dict(reindexed._indexes)\n3419             variables.update(reindexed.variables)\n3420         else:\n3421             # Get the indexes that are not being interpolated along\n3422             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}\n3423 \n3424         # Get the coords that also exist in the variables:\n3425         coord_names = obj._coord_names & variables.keys()\n3426         selected = self._replace_with_new_dims(\n3427             variables.copy(), coord_names, indexes=indexes\n3428         )\n3429 \n3430         # Attach indexer as coordinate\n3431         for k, v in indexers.items():\n3432             assert isinstance(v, Variable)\n3433             if v.dims == (k,):\n3434                 index = PandasIndex(v, k, coord_dtype=v.dtype)\n3435                 index_vars = index.create_variables({k: v})\n3436                 indexes[k] = index\n3437                 variables.update(index_vars)\n3438             else:\n3439                 variables[k] = v\n3440 \n3441         # Extract coordinates from indexers\n3442         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)\n3443         variables.update(coord_vars)\n3444         indexes.update(new_indexes)\n3445 \n3446         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()\n3447         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n3448 \n3449     def interp_like(\n3450         self,\n3451         other: Dataset | DataArray,\n3452         method: InterpOptions = \"linear\",\n3453         assume_sorted: bool = False,\n3454         kwargs: Mapping[str, Any] | None = None,\n3455         method_non_numeric: str = \"nearest\",\n3456     ) -> Dataset:\n3457         \"\"\"Interpolate this object onto the coordinates of another object,\n3458         filling the out of range values with NaN.\n3459 \n3460         If interpolating along a single existing dimension,\n3461         :py:class:`scipy.interpolate.interp1d` is called. When interpolating\n3462         along multiple existing dimensions, an attempt is made to decompose the\n3463         interpolation into multiple 1-dimensional interpolations. If this is\n3464         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,\n3465         :py:func:`scipy.interpolate.interpn` is called.\n3466 \n3467         Parameters\n3468         ----------\n3469         other : Dataset or DataArray\n3470             Object with an 'indexes' attribute giving a mapping from dimension\n3471             names to an 1d array-like, which provides coordinates upon\n3472             which to index the variables in this dataset. Missing values are skipped.\n3473         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n3474             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n3475             String indicating which method to use for interpolation:\n3476 \n3477             - 'linear': linear interpolation. Additional keyword\n3478               arguments are passed to :py:func:`numpy.interp`\n3479             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n3480               are passed to :py:func:`scipy.interpolate.interp1d`. If\n3481               ``method='polynomial'``, the ``order`` keyword argument must also be\n3482               provided.\n3483             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n3484               respective :py:class:`scipy.interpolate` classes.\n3485 \n3486         assume_sorted : bool, default: False\n3487             If False, values of coordinates that are interpolated over can be\n3488             in any order and they are sorted first. If True, interpolated\n3489             coordinates are assumed to be an array of monotonically increasing\n3490             values.\n3491         kwargs : dict, optional\n3492             Additional keyword passed to scipy's interpolator.\n3493         method_non_numeric : {\"nearest\", \"pad\", \"ffill\", \"backfill\", \"bfill\"}, optional\n3494             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.\n3495             ``\"nearest\"`` is used by default.\n3496 \n3497         Returns\n3498         -------\n3499         interpolated : Dataset\n3500             Another dataset by interpolating this dataset's data along the\n3501             coordinates of the other object.\n3502 \n3503         Notes\n3504         -----\n3505         scipy is required.\n3506         If the dataset has object-type coordinates, reindex is used for these\n3507         coordinates instead of the interpolation.\n3508 \n3509         See Also\n3510         --------\n3511         Dataset.interp\n3512         Dataset.reindex_like\n3513         \"\"\"\n3514         if kwargs is None:\n3515             kwargs = {}\n3516 \n3517         # pick only dimension coordinates with a single index\n3518         coords = {}\n3519         other_indexes = other.xindexes\n3520         for dim in self.dims:\n3521             other_dim_coords = other_indexes.get_all_coords(dim, errors=\"ignore\")\n3522             if len(other_dim_coords) == 1:\n3523                 coords[dim] = other_dim_coords[dim]\n3524 \n3525         numeric_coords: dict[Hashable, pd.Index] = {}\n3526         object_coords: dict[Hashable, pd.Index] = {}\n3527         for k, v in coords.items():\n3528             if v.dtype.kind in \"uifcMm\":\n3529                 numeric_coords[k] = v\n3530             else:\n3531                 object_coords[k] = v\n3532 \n3533         ds = self\n3534         if object_coords:\n3535             # We do not support interpolation along object coordinate.\n3536             # reindex instead.\n3537             ds = self.reindex(object_coords)\n3538         return ds.interp(\n3539             coords=numeric_coords,\n3540             method=method,\n3541             assume_sorted=assume_sorted,\n3542             kwargs=kwargs,\n3543             method_non_numeric=method_non_numeric,\n3544         )\n3545 \n3546     # Helper methods for rename()\n3547     def _rename_vars(\n3548         self, name_dict, dims_dict\n3549     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:\n3550         variables = {}\n3551         coord_names = set()\n3552         for k, v in self.variables.items():\n3553             var = v.copy(deep=False)\n3554             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n3555             name = name_dict.get(k, k)\n3556             if name in variables:\n3557                 raise ValueError(f\"the new name {name!r} conflicts\")\n3558             variables[name] = var\n3559             if k in self._coord_names:\n3560                 coord_names.add(name)\n3561         return variables, coord_names\n3562 \n3563     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:\n3564         return {name_dict.get(k, k): v for k, v in self.dims.items()}\n3565 \n3566     def _rename_indexes(\n3567         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]\n3568     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:\n3569         if not self._indexes:\n3570             return {}, {}\n3571 \n3572         indexes = {}\n3573         variables = {}\n3574 \n3575         for index, coord_names in self.xindexes.group_by_index():\n3576             new_index = index.rename(name_dict, dims_dict)\n3577             new_coord_names = [name_dict.get(k, k) for k in coord_names]\n3578             indexes.update({k: new_index for k in new_coord_names})\n3579             new_index_vars = new_index.create_variables(\n3580                 {\n3581                     new: self._variables[old]\n3582                     for old, new in zip(coord_names, new_coord_names)\n3583                 }\n3584             )\n3585             variables.update(new_index_vars)\n3586 \n3587         return indexes, variables\n3588 \n3589     def _rename_all(\n3590         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]\n3591     ) -> tuple[\n3592         dict[Hashable, Variable],\n3593         set[Hashable],\n3594         dict[Hashable, int],\n3595         dict[Hashable, Index],\n3596     ]:\n3597         variables, coord_names = self._rename_vars(name_dict, dims_dict)\n3598         dims = self._rename_dims(dims_dict)\n3599 \n3600         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)\n3601         variables = {k: index_vars.get(k, v) for k, v in variables.items()}\n3602 \n3603         return variables, coord_names, dims, indexes\n3604 \n3605     def _rename(\n3606         self: T_Dataset,\n3607         name_dict: Mapping[Any, Hashable] | None = None,\n3608         **names: Hashable,\n3609     ) -> T_Dataset:\n3610         \"\"\"Also used internally by DataArray so that the warning (if any)\n3611         is raised at the right stack level.\n3612         \"\"\"\n3613         name_dict = either_dict_or_kwargs(name_dict, names, \"rename\")\n3614         for k in name_dict.keys():\n3615             if k not in self and k not in self.dims:\n3616                 raise ValueError(\n3617                     f\"cannot rename {k!r} because it is not a \"\n3618                     \"variable or dimension in this dataset\"\n3619                 )\n3620 \n3621             create_dim_coord = False\n3622             new_k = name_dict[k]\n3623 \n3624             if k in self.dims and new_k in self._coord_names:\n3625                 coord_dims = self._variables[name_dict[k]].dims\n3626                 if coord_dims == (k,):\n3627                     create_dim_coord = True\n3628             elif k in self._coord_names and new_k in self.dims:\n3629                 coord_dims = self._variables[k].dims\n3630                 if coord_dims == (new_k,):\n3631                     create_dim_coord = True\n3632 \n3633             if create_dim_coord:\n3634                 warnings.warn(\n3635                     f\"rename {k!r} to {name_dict[k]!r} does not create an index \"\n3636                     \"anymore. Try using swap_dims instead or use set_index \"\n3637                     \"after rename to create an indexed coordinate.\",\n3638                     UserWarning,\n3639                     stacklevel=3,\n3640                 )\n3641 \n3642         variables, coord_names, dims, indexes = self._rename_all(\n3643             name_dict=name_dict, dims_dict=name_dict\n3644         )\n3645         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3646 \n3647     def rename(\n3648         self: T_Dataset,\n3649         name_dict: Mapping[Any, Hashable] | None = None,\n3650         **names: Hashable,\n3651     ) -> T_Dataset:\n3652         \"\"\"Returns a new object with renamed variables, coordinates and dimensions.\n3653 \n3654         Parameters\n3655         ----------\n3656         name_dict : dict-like, optional\n3657             Dictionary whose keys are current variable, coordinate or dimension names and\n3658             whose values are the desired names.\n3659         **names : optional\n3660             Keyword form of ``name_dict``.\n3661             One of name_dict or names must be provided.\n3662 \n3663         Returns\n3664         -------\n3665         renamed : Dataset\n3666             Dataset with renamed variables, coordinates and dimensions.\n3667 \n3668         See Also\n3669         --------\n3670         Dataset.swap_dims\n3671         Dataset.rename_vars\n3672         Dataset.rename_dims\n3673         DataArray.rename\n3674         \"\"\"\n3675         return self._rename(name_dict=name_dict, **names)\n3676 \n3677     def rename_dims(\n3678         self: T_Dataset,\n3679         dims_dict: Mapping[Any, Hashable] | None = None,\n3680         **dims: Hashable,\n3681     ) -> T_Dataset:\n3682         \"\"\"Returns a new object with renamed dimensions only.\n3683 \n3684         Parameters\n3685         ----------\n3686         dims_dict : dict-like, optional\n3687             Dictionary whose keys are current dimension names and\n3688             whose values are the desired names. The desired names must\n3689             not be the name of an existing dimension or Variable in the Dataset.\n3690         **dims : optional\n3691             Keyword form of ``dims_dict``.\n3692             One of dims_dict or dims must be provided.\n3693 \n3694         Returns\n3695         -------\n3696         renamed : Dataset\n3697             Dataset with renamed dimensions.\n3698 \n3699         See Also\n3700         --------\n3701         Dataset.swap_dims\n3702         Dataset.rename\n3703         Dataset.rename_vars\n3704         DataArray.rename\n3705         \"\"\"\n3706         dims_dict = either_dict_or_kwargs(dims_dict, dims, \"rename_dims\")\n3707         for k, v in dims_dict.items():\n3708             if k not in self.dims:\n3709                 raise ValueError(\n3710                     f\"cannot rename {k!r} because it is not a \"\n3711                     \"dimension in this dataset\"\n3712                 )\n3713             if v in self.dims or v in self:\n3714                 raise ValueError(\n3715                     f\"Cannot rename {k} to {v} because {v} already exists. \"\n3716                     \"Try using swap_dims instead.\"\n3717                 )\n3718 \n3719         variables, coord_names, sizes, indexes = self._rename_all(\n3720             name_dict={}, dims_dict=dims_dict\n3721         )\n3722         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)\n3723 \n3724     def rename_vars(\n3725         self: T_Dataset,\n3726         name_dict: Mapping[Any, Hashable] | None = None,\n3727         **names: Hashable,\n3728     ) -> T_Dataset:\n3729         \"\"\"Returns a new object with renamed variables including coordinates\n3730 \n3731         Parameters\n3732         ----------\n3733         name_dict : dict-like, optional\n3734             Dictionary whose keys are current variable or coordinate names and\n3735             whose values are the desired names.\n3736         **names : optional\n3737             Keyword form of ``name_dict``.\n3738             One of name_dict or names must be provided.\n3739 \n3740         Returns\n3741         -------\n3742         renamed : Dataset\n3743             Dataset with renamed variables including coordinates\n3744 \n3745         See Also\n3746         --------\n3747         Dataset.swap_dims\n3748         Dataset.rename\n3749         Dataset.rename_dims\n3750         DataArray.rename\n3751         \"\"\"\n3752         name_dict = either_dict_or_kwargs(name_dict, names, \"rename_vars\")\n3753         for k in name_dict:\n3754             if k not in self:\n3755                 raise ValueError(\n3756                     f\"cannot rename {k!r} because it is not a \"\n3757                     \"variable or coordinate in this dataset\"\n3758                 )\n3759         variables, coord_names, dims, indexes = self._rename_all(\n3760             name_dict=name_dict, dims_dict={}\n3761         )\n3762         return self._replace(variables, coord_names, dims=dims, indexes=indexes)\n3763 \n3764     def swap_dims(\n3765         self: T_Dataset, dims_dict: Mapping[Any, Hashable] | None = None, **dims_kwargs\n3766     ) -> T_Dataset:\n3767         \"\"\"Returns a new object with swapped dimensions.\n3768 \n3769         Parameters\n3770         ----------\n3771         dims_dict : dict-like\n3772             Dictionary whose keys are current dimension names and whose values\n3773             are new names.\n3774         **dims_kwargs : {existing_dim: new_dim, ...}, optional\n3775             The keyword arguments form of ``dims_dict``.\n3776             One of dims_dict or dims_kwargs must be provided.\n3777 \n3778         Returns\n3779         -------\n3780         swapped : Dataset\n3781             Dataset with swapped dimensions.\n3782 \n3783         Examples\n3784         --------\n3785         >>> ds = xr.Dataset(\n3786         ...     data_vars={\"a\": (\"x\", [5, 7]), \"b\": (\"x\", [0.1, 2.4])},\n3787         ...     coords={\"x\": [\"a\", \"b\"], \"y\": (\"x\", [0, 1])},\n3788         ... )\n3789         >>> ds\n3790         <xarray.Dataset>\n3791         Dimensions:  (x: 2)\n3792         Coordinates:\n3793           * x        (x) <U1 'a' 'b'\n3794             y        (x) int64 0 1\n3795         Data variables:\n3796             a        (x) int64 5 7\n3797             b        (x) float64 0.1 2.4\n3798 \n3799         >>> ds.swap_dims({\"x\": \"y\"})\n3800         <xarray.Dataset>\n3801         Dimensions:  (y: 2)\n3802         Coordinates:\n3803             x        (y) <U1 'a' 'b'\n3804           * y        (y) int64 0 1\n3805         Data variables:\n3806             a        (y) int64 5 7\n3807             b        (y) float64 0.1 2.4\n3808 \n3809         >>> ds.swap_dims({\"x\": \"z\"})\n3810         <xarray.Dataset>\n3811         Dimensions:  (z: 2)\n3812         Coordinates:\n3813             x        (z) <U1 'a' 'b'\n3814             y        (z) int64 0 1\n3815         Dimensions without coordinates: z\n3816         Data variables:\n3817             a        (z) int64 5 7\n3818             b        (z) float64 0.1 2.4\n3819 \n3820         See Also\n3821         --------\n3822         Dataset.rename\n3823         DataArray.swap_dims\n3824         \"\"\"\n3825         # TODO: deprecate this method in favor of a (less confusing)\n3826         # rename_dims() method that only renames dimensions.\n3827 \n3828         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, \"swap_dims\")\n3829         for k, v in dims_dict.items():\n3830             if k not in self.dims:\n3831                 raise ValueError(\n3832                     f\"cannot swap from dimension {k!r} because it is \"\n3833                     \"not an existing dimension\"\n3834                 )\n3835             if v in self.variables and self.variables[v].dims != (k,):\n3836                 raise ValueError(\n3837                     f\"replacement dimension {v!r} is not a 1D \"\n3838                     f\"variable along the old dimension {k!r}\"\n3839                 )\n3840 \n3841         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}\n3842 \n3843         coord_names = self._coord_names.copy()\n3844         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})\n3845 \n3846         variables: dict[Hashable, Variable] = {}\n3847         indexes: dict[Hashable, Index] = {}\n3848         for k, v in self.variables.items():\n3849             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)\n3850             var: Variable\n3851             if k in result_dims:\n3852                 var = v.to_index_variable()\n3853                 var.dims = dims\n3854                 if k in self._indexes:\n3855                     indexes[k] = self._indexes[k]\n3856                     variables[k] = var\n3857                 else:\n3858                     index, index_vars = create_default_index_implicit(var)\n3859                     indexes.update({name: index for name in index_vars})\n3860                     variables.update(index_vars)\n3861                     coord_names.update(index_vars)\n3862             else:\n3863                 var = v.to_base_variable()\n3864                 var.dims = dims\n3865                 variables[k] = var\n3866 \n3867         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)\n3868 \n3869     # change type of self and return to T_Dataset once\n3870     # https://github.com/python/mypy/issues/12846 is resolved\n3871     def expand_dims(\n3872         self,\n3873         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,\n3874         axis: None | int | Sequence[int] = None,\n3875         **dim_kwargs: Any,\n3876     ) -> Dataset:\n3877         \"\"\"Return a new object with an additional axis (or axes) inserted at\n3878         the corresponding position in the array shape.  The new object is a\n3879         view into the underlying array, not a copy.\n3880 \n3881         If dim is already a scalar coordinate, it will be promoted to a 1D\n3882         coordinate consisting of a single value.\n3883 \n3884         Parameters\n3885         ----------\n3886         dim : hashable, sequence of hashable, mapping, or None\n3887             Dimensions to include on the new variable. If provided as hashable\n3888             or sequence of hashable, then dimensions are inserted with length\n3889             1. If provided as a mapping, then the keys are the new dimensions\n3890             and the values are either integers (giving the length of the new\n3891             dimensions) or array-like (giving the coordinates of the new\n3892             dimensions).\n3893         axis : int, sequence of int, or None, default: None\n3894             Axis position(s) where new axis is to be inserted (position(s) on\n3895             the result array). If a sequence of integers is passed,\n3896             multiple axes are inserted. In this case, dim arguments should be\n3897             same length list. If axis=None is passed, all the axes will be\n3898             inserted to the start of the result array.\n3899         **dim_kwargs : int or sequence or ndarray\n3900             The keywords are arbitrary dimensions being inserted and the values\n3901             are either the lengths of the new dims (if int is given), or their\n3902             coordinates. Note, this is an alternative to passing a dict to the\n3903             dim kwarg and will only be used if dim is None.\n3904 \n3905         Returns\n3906         -------\n3907         expanded : Dataset\n3908             This object, but with additional dimension(s).\n3909 \n3910         See Also\n3911         --------\n3912         DataArray.expand_dims\n3913         \"\"\"\n3914         if dim is None:\n3915             pass\n3916         elif isinstance(dim, Mapping):\n3917             # We're later going to modify dim in place; don't tamper with\n3918             # the input\n3919             dim = dict(dim)\n3920         elif isinstance(dim, int):\n3921             raise TypeError(\n3922                 \"dim should be hashable or sequence of hashables or mapping\"\n3923             )\n3924         elif isinstance(dim, str) or not isinstance(dim, Sequence):\n3925             dim = {dim: 1}\n3926         elif isinstance(dim, Sequence):\n3927             if len(dim) != len(set(dim)):\n3928                 raise ValueError(\"dims should not contain duplicate values.\")\n3929             dim = {d: 1 for d in dim}\n3930 \n3931         dim = either_dict_or_kwargs(dim, dim_kwargs, \"expand_dims\")\n3932         assert isinstance(dim, MutableMapping)\n3933 \n3934         if axis is None:\n3935             axis = list(range(len(dim)))\n3936         elif not isinstance(axis, Sequence):\n3937             axis = [axis]\n3938 \n3939         if len(dim) != len(axis):\n3940             raise ValueError(\"lengths of dim and axis should be identical.\")\n3941         for d in dim:\n3942             if d in self.dims:\n3943                 raise ValueError(f\"Dimension {d} already exists.\")\n3944             if d in self._variables and not utils.is_scalar(self._variables[d]):\n3945                 raise ValueError(\n3946                     \"{dim} already exists as coordinate or\"\n3947                     \" variable name.\".format(dim=d)\n3948                 )\n3949 \n3950         variables: dict[Hashable, Variable] = {}\n3951         indexes: dict[Hashable, Index] = dict(self._indexes)\n3952         coord_names = self._coord_names.copy()\n3953         # If dim is a dict, then ensure that the values are either integers\n3954         # or iterables.\n3955         for k, v in dim.items():\n3956             if hasattr(v, \"__iter__\"):\n3957                 # If the value for the new dimension is an iterable, then\n3958                 # save the coordinates to the variables dict, and set the\n3959                 # value within the dim dict to the length of the iterable\n3960                 # for later use.\n3961                 index = PandasIndex(v, k)\n3962                 indexes[k] = index\n3963                 variables.update(index.create_variables())\n3964                 coord_names.add(k)\n3965                 dim[k] = variables[k].size\n3966             elif isinstance(v, int):\n3967                 pass  # Do nothing if the dimensions value is just an int\n3968             else:\n3969                 raise TypeError(\n3970                     \"The value of new dimension {k} must be \"\n3971                     \"an iterable or an int\".format(k=k)\n3972                 )\n3973 \n3974         for k, v in self._variables.items():\n3975             if k not in dim:\n3976                 if k in coord_names:  # Do not change coordinates\n3977                     variables[k] = v\n3978                 else:\n3979                     result_ndim = len(v.dims) + len(axis)\n3980                     for a in axis:\n3981                         if a < -result_ndim or result_ndim - 1 < a:\n3982                             raise IndexError(\n3983                                 f\"Axis {a} of variable {k} is out of bounds of the \"\n3984                                 f\"expanded dimension size {result_ndim}\"\n3985                             )\n3986 \n3987                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]\n3988                     if len(axis_pos) != len(set(axis_pos)):\n3989                         raise ValueError(\"axis should not contain duplicate values\")\n3990                     # We need to sort them to make sure `axis` equals to the\n3991                     # axis positions of the result array.\n3992                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))\n3993 \n3994                     all_dims = list(zip(v.dims, v.shape))\n3995                     for d, c in zip_axis_dim:\n3996                         all_dims.insert(d, c)\n3997                     variables[k] = v.set_dims(dict(all_dims))\n3998             else:\n3999                 if k not in variables:\n4000                     # If dims includes a label of a non-dimension coordinate,\n4001                     # it will be promoted to a 1D coordinate with a single value.\n4002                     index, index_vars = create_default_index_implicit(v.set_dims(k))\n4003                     indexes[k] = index\n4004                     variables.update(index_vars)\n4005 \n4006         return self._replace_with_new_dims(\n4007             variables, coord_names=coord_names, indexes=indexes\n4008         )\n4009 \n4010     # change type of self and return to T_Dataset once\n4011     # https://github.com/python/mypy/issues/12846 is resolved\n4012     def set_index(\n4013         self,\n4014         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,\n4015         append: bool = False,\n4016         **indexes_kwargs: Hashable | Sequence[Hashable],\n4017     ) -> Dataset:\n4018         \"\"\"Set Dataset (multi-)indexes using one or more existing coordinates\n4019         or variables.\n4020 \n4021         This legacy method is limited to pandas (multi-)indexes and\n4022         1-dimensional \"dimension\" coordinates. See\n4023         :py:meth:`~Dataset.set_xindex` for setting a pandas or a custom\n4024         Xarray-compatible index from one or more arbitrary coordinates.\n4025 \n4026         Parameters\n4027         ----------\n4028         indexes : {dim: index, ...}\n4029             Mapping from names matching dimensions and values given\n4030             by (lists of) the names of existing coordinates or variables to set\n4031             as new (multi-)index.\n4032         append : bool, default: False\n4033             If True, append the supplied index(es) to the existing index(es).\n4034             Otherwise replace the existing index(es) (default).\n4035         **indexes_kwargs : optional\n4036             The keyword arguments form of ``indexes``.\n4037             One of indexes or indexes_kwargs must be provided.\n4038 \n4039         Returns\n4040         -------\n4041         obj : Dataset\n4042             Another dataset, with this dataset's data but replaced coordinates.\n4043 \n4044         Examples\n4045         --------\n4046         >>> arr = xr.DataArray(\n4047         ...     data=np.ones((2, 3)),\n4048         ...     dims=[\"x\", \"y\"],\n4049         ...     coords={\"x\": range(2), \"y\": range(3), \"a\": (\"x\", [3, 4])},\n4050         ... )\n4051         >>> ds = xr.Dataset({\"v\": arr})\n4052         >>> ds\n4053         <xarray.Dataset>\n4054         Dimensions:  (x: 2, y: 3)\n4055         Coordinates:\n4056           * x        (x) int64 0 1\n4057           * y        (y) int64 0 1 2\n4058             a        (x) int64 3 4\n4059         Data variables:\n4060             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n4061         >>> ds.set_index(x=\"a\")\n4062         <xarray.Dataset>\n4063         Dimensions:  (x: 2, y: 3)\n4064         Coordinates:\n4065           * x        (x) int64 3 4\n4066           * y        (y) int64 0 1 2\n4067         Data variables:\n4068             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0\n4069 \n4070         See Also\n4071         --------\n4072         Dataset.reset_index\n4073         Dataset.set_xindex\n4074         Dataset.swap_dims\n4075         \"\"\"\n4076         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, \"set_index\")\n4077 \n4078         new_indexes: dict[Hashable, Index] = {}\n4079         new_variables: dict[Hashable, Variable] = {}\n4080         drop_indexes: set[Hashable] = set()\n4081         drop_variables: set[Hashable] = set()\n4082         replace_dims: dict[Hashable, Hashable] = {}\n4083         all_var_names: set[Hashable] = set()\n4084 \n4085         for dim, _var_names in dim_coords.items():\n4086             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):\n4087                 var_names = [_var_names]\n4088             else:\n4089                 var_names = list(_var_names)\n4090 \n4091             invalid_vars = set(var_names) - set(self._variables)\n4092             if invalid_vars:\n4093                 raise ValueError(\n4094                     \", \".join([str(v) for v in invalid_vars])\n4095                     + \" variable(s) do not exist\"\n4096                 )\n4097 \n4098             all_var_names.update(var_names)\n4099             drop_variables.update(var_names)\n4100 \n4101             # drop any pre-existing index involved and its corresponding coordinates\n4102             index_coord_names = self.xindexes.get_all_coords(dim, errors=\"ignore\")\n4103             all_index_coord_names = set(index_coord_names)\n4104             for k in var_names:\n4105                 all_index_coord_names.update(\n4106                     self.xindexes.get_all_coords(k, errors=\"ignore\")\n4107                 )\n4108 \n4109             drop_indexes.update(all_index_coord_names)\n4110             drop_variables.update(all_index_coord_names)\n4111 \n4112             if len(var_names) == 1 and (not append or dim not in self._indexes):\n4113                 var_name = var_names[0]\n4114                 var = self._variables[var_name]\n4115                 if var.dims != (dim,):\n4116                     raise ValueError(\n4117                         f\"dimension mismatch: try setting an index for dimension {dim!r} with \"\n4118                         f\"variable {var_name!r} that has dimensions {var.dims}\"\n4119                     )\n4120                 idx = PandasIndex.from_variables({dim: var}, options={})\n4121                 idx_vars = idx.create_variables({var_name: var})\n4122 \n4123                 # trick to preserve coordinate order in this case\n4124                 if dim in self._coord_names:\n4125                     drop_variables.remove(dim)\n4126             else:\n4127                 if append:\n4128                     current_variables = {\n4129                         k: self._variables[k] for k in index_coord_names\n4130                     }\n4131                 else:\n4132                     current_variables = {}\n4133                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(\n4134                     dim,\n4135                     current_variables,\n4136                     {k: self._variables[k] for k in var_names},\n4137                 )\n4138                 for n in idx.index.names:\n4139                     replace_dims[n] = dim\n4140 \n4141             new_indexes.update({k: idx for k in idx_vars})\n4142             new_variables.update(idx_vars)\n4143 \n4144         # re-add deindexed coordinates (convert to base variables)\n4145         for k in drop_variables:\n4146             if (\n4147                 k not in new_variables\n4148                 and k not in all_var_names\n4149                 and k in self._coord_names\n4150             ):\n4151                 new_variables[k] = self._variables[k].to_base_variable()\n4152 \n4153         indexes_: dict[Any, Index] = {\n4154             k: v for k, v in self._indexes.items() if k not in drop_indexes\n4155         }\n4156         indexes_.update(new_indexes)\n4157 \n4158         variables = {\n4159             k: v for k, v in self._variables.items() if k not in drop_variables\n4160         }\n4161         variables.update(new_variables)\n4162 \n4163         # update dimensions if necessary, GH: 3512\n4164         for k, v in variables.items():\n4165             if any(d in replace_dims for d in v.dims):\n4166                 new_dims = [replace_dims.get(d, d) for d in v.dims]\n4167                 variables[k] = v._replace(dims=new_dims)\n4168 \n4169         coord_names = self._coord_names - drop_variables | set(new_variables)\n4170 \n4171         return self._replace_with_new_dims(\n4172             variables, coord_names=coord_names, indexes=indexes_\n4173         )\n4174 \n4175     def reset_index(\n4176         self: T_Dataset,\n4177         dims_or_levels: Hashable | Sequence[Hashable],\n4178         drop: bool = False,\n4179     ) -> T_Dataset:\n4180         \"\"\"Reset the specified index(es) or multi-index level(s).\n4181 \n4182         This legacy method is specific to pandas (multi-)indexes and\n4183         1-dimensional \"dimension\" coordinates. See the more generic\n4184         :py:meth:`~Dataset.drop_indexes` and :py:meth:`~Dataset.set_xindex`\n4185         method to respectively drop and set pandas or custom indexes for\n4186         arbitrary coordinates.\n4187 \n4188         Parameters\n4189         ----------\n4190         dims_or_levels : Hashable or Sequence of Hashable\n4191             Name(s) of the dimension(s) and/or multi-index level(s) that will\n4192             be reset.\n4193         drop : bool, default: False\n4194             If True, remove the specified indexes and/or multi-index levels\n4195             instead of extracting them as new coordinates (default: False).\n4196 \n4197         Returns\n4198         -------\n4199         obj : Dataset\n4200             Another dataset, with this dataset's data but replaced coordinates.\n4201 \n4202         See Also\n4203         --------\n4204         Dataset.set_index\n4205         Dataset.set_xindex\n4206         Dataset.drop_indexes\n4207         \"\"\"\n4208         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):\n4209             dims_or_levels = [dims_or_levels]\n4210 \n4211         invalid_coords = set(dims_or_levels) - set(self._indexes)\n4212         if invalid_coords:\n4213             raise ValueError(\n4214                 f\"{tuple(invalid_coords)} are not coordinates with an index\"\n4215             )\n4216 \n4217         drop_indexes: set[Hashable] = set()\n4218         drop_variables: set[Hashable] = set()\n4219         seen: set[Index] = set()\n4220         new_indexes: dict[Hashable, Index] = {}\n4221         new_variables: dict[Hashable, Variable] = {}\n4222 \n4223         def drop_or_convert(var_names):\n4224             if drop:\n4225                 drop_variables.update(var_names)\n4226             else:\n4227                 base_vars = {\n4228                     k: self._variables[k].to_base_variable() for k in var_names\n4229                 }\n4230                 new_variables.update(base_vars)\n4231 \n4232         for name in dims_or_levels:\n4233             index = self._indexes[name]\n4234 \n4235             if index in seen:\n4236                 continue\n4237             seen.add(index)\n4238 \n4239             idx_var_names = set(self.xindexes.get_all_coords(name))\n4240             drop_indexes.update(idx_var_names)\n4241 \n4242             if isinstance(index, PandasMultiIndex):\n4243                 # special case for pd.MultiIndex\n4244                 level_names = index.index.names\n4245                 keep_level_vars = {\n4246                     k: self._variables[k]\n4247                     for k in level_names\n4248                     if k not in dims_or_levels\n4249                 }\n4250 \n4251                 if index.dim not in dims_or_levels and keep_level_vars:\n4252                     # do not drop the multi-index completely\n4253                     # instead replace it by a new (multi-)index with dropped level(s)\n4254                     idx = index.keep_levels(keep_level_vars)\n4255                     idx_vars = idx.create_variables(keep_level_vars)\n4256                     new_indexes.update({k: idx for k in idx_vars})\n4257                     new_variables.update(idx_vars)\n4258                     if not isinstance(idx, PandasMultiIndex):\n4259                         # multi-index reduced to single index\n4260                         # backward compatibility: unique level coordinate renamed to dimension\n4261                         drop_variables.update(keep_level_vars)\n4262                     drop_or_convert(\n4263                         [k for k in level_names if k not in keep_level_vars]\n4264                     )\n4265                 else:\n4266                     # always drop the multi-index dimension variable\n4267                     drop_variables.add(index.dim)\n4268                     drop_or_convert(level_names)\n4269             else:\n4270                 drop_or_convert(idx_var_names)\n4271 \n4272         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}\n4273         indexes.update(new_indexes)\n4274 \n4275         variables = {\n4276             k: v for k, v in self._variables.items() if k not in drop_variables\n4277         }\n4278         variables.update(new_variables)\n4279 \n4280         coord_names = self._coord_names - drop_variables\n4281 \n4282         return self._replace_with_new_dims(\n4283             variables, coord_names=coord_names, indexes=indexes\n4284         )\n4285 \n4286     def set_xindex(\n4287         self: T_Dataset,\n4288         coord_names: str | Sequence[Hashable],\n4289         index_cls: type[Index] | None = None,\n4290         **options,\n4291     ) -> T_Dataset:\n4292         \"\"\"Set a new, Xarray-compatible index from one or more existing\n4293         coordinate(s).\n4294 \n4295         Parameters\n4296         ----------\n4297         coord_names : str or list\n4298             Name(s) of the coordinate(s) used to build the index.\n4299             If several names are given, their order matters.\n4300         index_cls : subclass of :class:`~xarray.indexes.Index`, optional\n4301             The type of index to create. By default, try setting\n4302             a ``PandasIndex`` if ``len(coord_names) == 1``,\n4303             otherwise a ``PandasMultiIndex``.\n4304         **options\n4305             Options passed to the index constructor.\n4306 \n4307         Returns\n4308         -------\n4309         obj : Dataset\n4310             Another dataset, with this dataset's data and with a new index.\n4311 \n4312         \"\"\"\n4313         # the Sequence check is required for mypy\n4314         if is_scalar(coord_names) or not isinstance(coord_names, Sequence):\n4315             coord_names = [coord_names]\n4316 \n4317         if index_cls is None:\n4318             if len(coord_names) == 1:\n4319                 index_cls = PandasIndex\n4320             else:\n4321                 index_cls = PandasMultiIndex\n4322         else:\n4323             if not issubclass(index_cls, Index):\n4324                 raise TypeError(f\"{index_cls} is not a subclass of xarray.Index\")\n4325 \n4326         invalid_coords = set(coord_names) - self._coord_names\n4327 \n4328         if invalid_coords:\n4329             msg = [\"invalid coordinate(s)\"]\n4330             no_vars = invalid_coords - set(self._variables)\n4331             data_vars = invalid_coords - no_vars\n4332             if no_vars:\n4333                 msg.append(f\"those variables don't exist: {no_vars}\")\n4334             if data_vars:\n4335                 msg.append(\n4336                     f\"those variables are data variables: {data_vars}, use `set_coords` first\"\n4337                 )\n4338             raise ValueError(\"\\n\".join(msg))\n4339 \n4340         # we could be more clever here (e.g., drop-in index replacement if index\n4341         # coordinates do not conflict), but let's not allow this for now\n4342         indexed_coords = set(coord_names) & set(self._indexes)\n4343 \n4344         if indexed_coords:\n4345             raise ValueError(\n4346                 f\"those coordinates already have an index: {indexed_coords}\"\n4347             )\n4348 \n4349         coord_vars = {name: self._variables[name] for name in coord_names}\n4350 \n4351         index = index_cls.from_variables(coord_vars, options=options)\n4352 \n4353         new_coord_vars = index.create_variables(coord_vars)\n4354 \n4355         # special case for setting a pandas multi-index from level coordinates\n4356         # TODO: remove it once we depreciate pandas multi-index dimension (tuple\n4357         # elements) coordinate\n4358         if isinstance(index, PandasMultiIndex):\n4359             coord_names = [index.dim] + list(coord_names)\n4360 \n4361         variables: dict[Hashable, Variable]\n4362         indexes: dict[Hashable, Index]\n4363 \n4364         if len(coord_names) == 1:\n4365             variables = self._variables.copy()\n4366             indexes = self._indexes.copy()\n4367 \n4368             name = list(coord_names).pop()\n4369             if name in new_coord_vars:\n4370                 variables[name] = new_coord_vars[name]\n4371             indexes[name] = index\n4372         else:\n4373             # reorder variables and indexes so that coordinates having the same\n4374             # index are next to each other\n4375             variables = {}\n4376             for name, var in self._variables.items():\n4377                 if name not in coord_names:\n4378                     variables[name] = var\n4379 \n4380             indexes = {}\n4381             for name, idx in self._indexes.items():\n4382                 if name not in coord_names:\n4383                     indexes[name] = idx\n4384 \n4385             for name in coord_names:\n4386                 try:\n4387                     variables[name] = new_coord_vars[name]\n4388                 except KeyError:\n4389                     variables[name] = self._variables[name]\n4390                 indexes[name] = index\n4391 \n4392         return self._replace(\n4393             variables=variables,\n4394             coord_names=self._coord_names | set(coord_names),\n4395             indexes=indexes,\n4396         )\n4397 \n4398     def reorder_levels(\n4399         self: T_Dataset,\n4400         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,\n4401         **dim_order_kwargs: Sequence[int | Hashable],\n4402     ) -> T_Dataset:\n4403         \"\"\"Rearrange index levels using input order.\n4404 \n4405         Parameters\n4406         ----------\n4407         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional\n4408             Mapping from names matching dimensions and values given\n4409             by lists representing new level orders. Every given dimension\n4410             must have a multi-index.\n4411         **dim_order_kwargs : Sequence of int or Hashable, optional\n4412             The keyword arguments form of ``dim_order``.\n4413             One of dim_order or dim_order_kwargs must be provided.\n4414 \n4415         Returns\n4416         -------\n4417         obj : Dataset\n4418             Another dataset, with this dataset's data but replaced\n4419             coordinates.\n4420         \"\"\"\n4421         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, \"reorder_levels\")\n4422         variables = self._variables.copy()\n4423         indexes = dict(self._indexes)\n4424         new_indexes: dict[Hashable, Index] = {}\n4425         new_variables: dict[Hashable, IndexVariable] = {}\n4426 \n4427         for dim, order in dim_order.items():\n4428             index = self._indexes[dim]\n4429 \n4430             if not isinstance(index, PandasMultiIndex):\n4431                 raise ValueError(f\"coordinate {dim} has no MultiIndex\")\n4432 \n4433             level_vars = {k: self._variables[k] for k in order}\n4434             idx = index.reorder_levels(level_vars)\n4435             idx_vars = idx.create_variables(level_vars)\n4436             new_indexes.update({k: idx for k in idx_vars})\n4437             new_variables.update(idx_vars)\n4438 \n4439         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}\n4440         indexes.update(new_indexes)\n4441 \n4442         variables = {k: v for k, v in self._variables.items() if k not in new_variables}\n4443         variables.update(new_variables)\n4444 \n4445         return self._replace(variables, indexes=indexes)\n4446 \n4447     def _get_stack_index(\n4448         self,\n4449         dim,\n4450         multi=False,\n4451         create_index=False,\n4452     ) -> tuple[Index | None, dict[Hashable, Variable]]:\n4453         \"\"\"Used by stack and unstack to get one pandas (multi-)index among\n4454         the indexed coordinates along dimension `dim`.\n4455 \n4456         If exactly one index is found, return it with its corresponding\n4457         coordinate variables(s), otherwise return None and an empty dict.\n4458 \n4459         If `create_index=True`, create a new index if none is found or raise\n4460         an error if multiple indexes are found.\n4461 \n4462         \"\"\"\n4463         stack_index: Index | None = None\n4464         stack_coords: dict[Hashable, Variable] = {}\n4465 \n4466         for name, index in self._indexes.items():\n4467             var = self._variables[name]\n4468             if (\n4469                 var.ndim == 1\n4470                 and var.dims[0] == dim\n4471                 and (\n4472                     # stack: must be a single coordinate index\n4473                     not multi\n4474                     and not self.xindexes.is_multi(name)\n4475                     # unstack: must be an index that implements .unstack\n4476                     or multi\n4477                     and type(index).unstack is not Index.unstack\n4478                 )\n4479             ):\n4480                 if stack_index is not None and index is not stack_index:\n4481                     # more than one index found, stop\n4482                     if create_index:\n4483                         raise ValueError(\n4484                             f\"cannot stack dimension {dim!r} with `create_index=True` \"\n4485                             \"and with more than one index found along that dimension\"\n4486                         )\n4487                     return None, {}\n4488                 stack_index = index\n4489                 stack_coords[name] = var\n4490 \n4491         if create_index and stack_index is None:\n4492             if dim in self._variables:\n4493                 var = self._variables[dim]\n4494             else:\n4495                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)\n4496             # dummy index (only `stack_coords` will be used to construct the multi-index)\n4497             stack_index = PandasIndex([0], dim)\n4498             stack_coords = {dim: var}\n4499 \n4500         return stack_index, stack_coords\n4501 \n4502     def _stack_once(\n4503         self: T_Dataset,\n4504         dims: Sequence[Hashable | ellipsis],\n4505         new_dim: Hashable,\n4506         index_cls: type[Index],\n4507         create_index: bool | None = True,\n4508     ) -> T_Dataset:\n4509         if dims == ...:\n4510             raise ValueError(\"Please use [...] for dims, rather than just ...\")\n4511         if ... in dims:\n4512             dims = list(infix_dims(dims, self.dims))\n4513 \n4514         new_variables: dict[Hashable, Variable] = {}\n4515         stacked_var_names: list[Hashable] = []\n4516         drop_indexes: list[Hashable] = []\n4517 \n4518         for name, var in self.variables.items():\n4519             if any(d in var.dims for d in dims):\n4520                 add_dims = [d for d in dims if d not in var.dims]\n4521                 vdims = list(var.dims) + add_dims\n4522                 shape = [self.dims[d] for d in vdims]\n4523                 exp_var = var.set_dims(vdims, shape)\n4524                 stacked_var = exp_var.stack(**{new_dim: dims})\n4525                 new_variables[name] = stacked_var\n4526                 stacked_var_names.append(name)\n4527             else:\n4528                 new_variables[name] = var.copy(deep=False)\n4529 \n4530         # drop indexes of stacked coordinates (if any)\n4531         for name in stacked_var_names:\n4532             drop_indexes += list(self.xindexes.get_all_coords(name, errors=\"ignore\"))\n4533 \n4534         new_indexes = {}\n4535         new_coord_names = set(self._coord_names)\n4536         if create_index or create_index is None:\n4537             product_vars: dict[Any, Variable] = {}\n4538             for dim in dims:\n4539                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)\n4540                 if idx is not None:\n4541                     product_vars.update(idx_vars)\n4542 \n4543             if len(product_vars) == len(dims):\n4544                 idx = index_cls.stack(product_vars, new_dim)\n4545                 new_indexes[new_dim] = idx\n4546                 new_indexes.update({k: idx for k in product_vars})\n4547                 idx_vars = idx.create_variables(product_vars)\n4548                 # keep consistent multi-index coordinate order\n4549                 for k in idx_vars:\n4550                     new_variables.pop(k, None)\n4551                 new_variables.update(idx_vars)\n4552                 new_coord_names.update(idx_vars)\n4553 \n4554         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}\n4555         indexes.update(new_indexes)\n4556 \n4557         return self._replace_with_new_dims(\n4558             new_variables, coord_names=new_coord_names, indexes=indexes\n4559         )\n4560 \n4561     def stack(\n4562         self: T_Dataset,\n4563         dimensions: Mapping[Any, Sequence[Hashable | ellipsis]] | None = None,\n4564         create_index: bool | None = True,\n4565         index_cls: type[Index] = PandasMultiIndex,\n4566         **dimensions_kwargs: Sequence[Hashable | ellipsis],\n4567     ) -> T_Dataset:\n4568         \"\"\"\n4569         Stack any number of existing dimensions into a single new dimension.\n4570 \n4571         New dimensions will be added at the end, and by default the corresponding\n4572         coordinate variables will be combined into a MultiIndex.\n4573 \n4574         Parameters\n4575         ----------\n4576         dimensions : mapping of hashable to sequence of hashable\n4577             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new\n4578             dimensions, and the existing dimensions that they replace. An\n4579             ellipsis (`...`) will be replaced by all unlisted dimensions.\n4580             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over\n4581             all dimensions.\n4582         create_index : bool or None, default: True\n4583 \n4584             - True: create a multi-index for each of the stacked dimensions.\n4585             - False: don't create any index.\n4586             - None. create a multi-index only if exactly one single (1-d) coordinate\n4587               index is found for every dimension to stack.\n4588 \n4589         index_cls: Index-class, default: PandasMultiIndex\n4590             Can be used to pass a custom multi-index type (must be an Xarray index that\n4591             implements `.stack()`). By default, a pandas multi-index wrapper is used.\n4592         **dimensions_kwargs\n4593             The keyword arguments form of ``dimensions``.\n4594             One of dimensions or dimensions_kwargs must be provided.\n4595 \n4596         Returns\n4597         -------\n4598         stacked : Dataset\n4599             Dataset with stacked data.\n4600 \n4601         See Also\n4602         --------\n4603         Dataset.unstack\n4604         \"\"\"\n4605         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n4606         result = self\n4607         for new_dim, dims in dimensions.items():\n4608             result = result._stack_once(dims, new_dim, index_cls, create_index)\n4609         return result\n4610 \n4611     def to_stacked_array(\n4612         self,\n4613         new_dim: Hashable,\n4614         sample_dims: Collection[Hashable],\n4615         variable_dim: Hashable = \"variable\",\n4616         name: Hashable | None = None,\n4617     ) -> DataArray:\n4618         \"\"\"Combine variables of differing dimensionality into a DataArray\n4619         without broadcasting.\n4620 \n4621         This method is similar to Dataset.to_array but does not broadcast the\n4622         variables.\n4623 \n4624         Parameters\n4625         ----------\n4626         new_dim : hashable\n4627             Name of the new stacked coordinate\n4628         sample_dims : Collection of hashables\n4629             List of dimensions that **will not** be stacked. Each array in the\n4630             dataset must share these dimensions. For machine learning\n4631             applications, these define the dimensions over which samples are\n4632             drawn.\n4633         variable_dim : hashable, default: \"variable\"\n4634             Name of the level in the stacked coordinate which corresponds to\n4635             the variables.\n4636         name : hashable, optional\n4637             Name of the new data array.\n4638 \n4639         Returns\n4640         -------\n4641         stacked : DataArray\n4642             DataArray with the specified dimensions and data variables\n4643             stacked together. The stacked coordinate is named ``new_dim``\n4644             and represented by a MultiIndex object with a level containing the\n4645             data variable names. The name of this level is controlled using\n4646             the ``variable_dim`` argument.\n4647 \n4648         See Also\n4649         --------\n4650         Dataset.to_array\n4651         Dataset.stack\n4652         DataArray.to_unstacked_dataset\n4653 \n4654         Examples\n4655         --------\n4656         >>> data = xr.Dataset(\n4657         ...     data_vars={\n4658         ...         \"a\": ((\"x\", \"y\"), [[0, 1, 2], [3, 4, 5]]),\n4659         ...         \"b\": (\"x\", [6, 7]),\n4660         ...     },\n4661         ...     coords={\"y\": [\"u\", \"v\", \"w\"]},\n4662         ... )\n4663 \n4664         >>> data\n4665         <xarray.Dataset>\n4666         Dimensions:  (x: 2, y: 3)\n4667         Coordinates:\n4668           * y        (y) <U1 'u' 'v' 'w'\n4669         Dimensions without coordinates: x\n4670         Data variables:\n4671             a        (x, y) int64 0 1 2 3 4 5\n4672             b        (x) int64 6 7\n4673 \n4674         >>> data.to_stacked_array(\"z\", sample_dims=[\"x\"])\n4675         <xarray.DataArray 'a' (x: 2, z: 4)>\n4676         array([[0, 1, 2, 6],\n4677                [3, 4, 5, 7]])\n4678         Coordinates:\n4679           * z         (z) object MultiIndex\n4680           * variable  (z) object 'a' 'a' 'a' 'b'\n4681           * y         (z) object 'u' 'v' 'w' nan\n4682         Dimensions without coordinates: x\n4683 \n4684         \"\"\"\n4685         from xarray.core.concat import concat\n4686 \n4687         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)\n4688 \n4689         for variable in self:\n4690             dims = self[variable].dims\n4691             dims_include_sample_dims = set(sample_dims) <= set(dims)\n4692             if not dims_include_sample_dims:\n4693                 raise ValueError(\n4694                     \"All variables in the dataset must contain the \"\n4695                     \"dimensions {}.\".format(dims)\n4696                 )\n4697 \n4698         def ensure_stackable(val):\n4699             assign_coords = {variable_dim: val.name}\n4700             for dim in stacking_dims:\n4701                 if dim not in val.dims:\n4702                     assign_coords[dim] = None\n4703 \n4704             expand_dims = set(stacking_dims).difference(set(val.dims))\n4705             expand_dims.add(variable_dim)\n4706             # must be list for .expand_dims\n4707             expand_dims = list(expand_dims)\n4708 \n4709             return (\n4710                 val.assign_coords(**assign_coords)\n4711                 .expand_dims(expand_dims)\n4712                 .stack({new_dim: (variable_dim,) + stacking_dims})\n4713             )\n4714 \n4715         # concatenate the arrays\n4716         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]\n4717         data_array = concat(stackable_vars, dim=new_dim)\n4718 \n4719         if name is not None:\n4720             data_array.name = name\n4721 \n4722         return data_array\n4723 \n4724     def _unstack_once(\n4725         self: T_Dataset,\n4726         dim: Hashable,\n4727         index_and_vars: tuple[Index, dict[Hashable, Variable]],\n4728         fill_value,\n4729         sparse: bool = False,\n4730     ) -> T_Dataset:\n4731         index, index_vars = index_and_vars\n4732         variables: dict[Hashable, Variable] = {}\n4733         indexes = {k: v for k, v in self._indexes.items() if k != dim}\n4734 \n4735         new_indexes, clean_index = index.unstack()\n4736         indexes.update(new_indexes)\n4737 \n4738         for name, idx in new_indexes.items():\n4739             variables.update(idx.create_variables(index_vars))\n4740 \n4741         for name, var in self.variables.items():\n4742             if name not in index_vars:\n4743                 if dim in var.dims:\n4744                     if isinstance(fill_value, Mapping):\n4745                         fill_value_ = fill_value[name]\n4746                     else:\n4747                         fill_value_ = fill_value\n4748 \n4749                     variables[name] = var._unstack_once(\n4750                         index=clean_index,\n4751                         dim=dim,\n4752                         fill_value=fill_value_,\n4753                         sparse=sparse,\n4754                     )\n4755                 else:\n4756                     variables[name] = var\n4757 \n4758         coord_names = set(self._coord_names) - {dim} | set(new_indexes)\n4759 \n4760         return self._replace_with_new_dims(\n4761             variables, coord_names=coord_names, indexes=indexes\n4762         )\n4763 \n4764     def _unstack_full_reindex(\n4765         self: T_Dataset,\n4766         dim: Hashable,\n4767         index_and_vars: tuple[Index, dict[Hashable, Variable]],\n4768         fill_value,\n4769         sparse: bool,\n4770     ) -> T_Dataset:\n4771         index, index_vars = index_and_vars\n4772         variables: dict[Hashable, Variable] = {}\n4773         indexes = {k: v for k, v in self._indexes.items() if k != dim}\n4774 \n4775         new_indexes, clean_index = index.unstack()\n4776         indexes.update(new_indexes)\n4777 \n4778         new_index_variables = {}\n4779         for name, idx in new_indexes.items():\n4780             new_index_variables.update(idx.create_variables(index_vars))\n4781 \n4782         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}\n4783         variables.update(new_index_variables)\n4784 \n4785         # take a shortcut in case the MultiIndex was not modified.\n4786         full_idx = pd.MultiIndex.from_product(\n4787             clean_index.levels, names=clean_index.names\n4788         )\n4789         if clean_index.equals(full_idx):\n4790             obj = self\n4791         else:\n4792             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex\n4793             xr_full_idx = PandasMultiIndex(full_idx, dim)\n4794             indexers = Indexes(\n4795                 {k: xr_full_idx for k in index_vars},\n4796                 xr_full_idx.create_variables(index_vars),\n4797             )\n4798             obj = self._reindex(\n4799                 indexers, copy=False, fill_value=fill_value, sparse=sparse\n4800             )\n4801 \n4802         for name, var in obj.variables.items():\n4803             if name not in index_vars:\n4804                 if dim in var.dims:\n4805                     variables[name] = var.unstack({dim: new_dim_sizes})\n4806                 else:\n4807                     variables[name] = var\n4808 \n4809         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)\n4810 \n4811         return self._replace_with_new_dims(\n4812             variables, coord_names=coord_names, indexes=indexes\n4813         )\n4814 \n4815     def unstack(\n4816         self: T_Dataset,\n4817         dim: Dims = None,\n4818         fill_value: Any = xrdtypes.NA,\n4819         sparse: bool = False,\n4820     ) -> T_Dataset:\n4821         \"\"\"\n4822         Unstack existing dimensions corresponding to MultiIndexes into\n4823         multiple new dimensions.\n4824 \n4825         New dimensions will be added at the end.\n4826 \n4827         Parameters\n4828         ----------\n4829         dim : str, Iterable of Hashable or None, optional\n4830             Dimension(s) over which to unstack. By default unstacks all\n4831             MultiIndexes.\n4832         fill_value : scalar or dict-like, default: nan\n4833             value to be filled. If a dict-like, maps variable names to\n4834             fill values. If not provided or if the dict-like does not\n4835             contain all variables, the dtype's NA value will be used.\n4836         sparse : bool, default: False\n4837             use sparse-array if True\n4838 \n4839         Returns\n4840         -------\n4841         unstacked : Dataset\n4842             Dataset with unstacked data.\n4843 \n4844         See Also\n4845         --------\n4846         Dataset.stack\n4847         \"\"\"\n4848 \n4849         if dim is None:\n4850             dims = list(self.dims)\n4851         else:\n4852             if isinstance(dim, str) or not isinstance(dim, Iterable):\n4853                 dims = [dim]\n4854             else:\n4855                 dims = list(dim)\n4856 \n4857             missing_dims = [d for d in dims if d not in self.dims]\n4858             if missing_dims:\n4859                 raise ValueError(\n4860                     f\"Dataset does not contain the dimensions: {missing_dims}\"\n4861                 )\n4862 \n4863         # each specified dimension must have exactly one multi-index\n4864         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}\n4865         for d in dims:\n4866             idx, idx_vars = self._get_stack_index(d, multi=True)\n4867             if idx is not None:\n4868                 stacked_indexes[d] = idx, idx_vars\n4869 \n4870         if dim is None:\n4871             dims = list(stacked_indexes)\n4872         else:\n4873             non_multi_dims = set(dims) - set(stacked_indexes)\n4874             if non_multi_dims:\n4875                 raise ValueError(\n4876                     \"cannot unstack dimensions that do not \"\n4877                     f\"have exactly one multi-index: {tuple(non_multi_dims)}\"\n4878                 )\n4879 \n4880         result = self.copy(deep=False)\n4881 \n4882         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,\n4883         # so we can't just access self.variables[v].data for every variable.\n4884         # We only check the non-index variables.\n4885         # https://github.com/pydata/xarray/issues/5902\n4886         nonindexes = [\n4887             self.variables[k] for k in set(self.variables) - set(self._indexes)\n4888         ]\n4889         # Notes for each of these cases:\n4890         # 1. Dask arrays don't support assignment by index, which the fast unstack\n4891         #    function requires.\n4892         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125\n4893         # 2. Sparse doesn't currently support (though we could special-case it)\n4894         #    https://github.com/pydata/sparse/issues/422\n4895         # 3. pint requires checking if it's a NumPy array until\n4896         #    https://github.com/pydata/xarray/pull/4751 is resolved,\n4897         #    Once that is resolved, explicitly exclude pint arrays.\n4898         #    pint doesn't implement `np.full_like` in a way that's\n4899         #    currently compatible.\n4900         sparse_array_type = array_type(\"sparse\")\n4901         needs_full_reindex = any(\n4902             is_duck_dask_array(v.data)\n4903             or isinstance(v.data, sparse_array_type)\n4904             or not isinstance(v.data, np.ndarray)\n4905             for v in nonindexes\n4906         )\n4907 \n4908         for d in dims:\n4909             if needs_full_reindex:\n4910                 result = result._unstack_full_reindex(\n4911                     d, stacked_indexes[d], fill_value, sparse\n4912                 )\n4913             else:\n4914                 result = result._unstack_once(d, stacked_indexes[d], fill_value, sparse)\n4915         return result\n4916 \n4917     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:\n4918         \"\"\"Update this dataset's variables with those from another dataset.\n4919 \n4920         Just like :py:meth:`dict.update` this is a in-place operation.\n4921         For a non-inplace version, see :py:meth:`Dataset.merge`.\n4922 \n4923         Parameters\n4924         ----------\n4925         other : Dataset or mapping\n4926             Variables with which to update this dataset. One of:\n4927 \n4928             - Dataset\n4929             - mapping {var name: DataArray}\n4930             - mapping {var name: Variable}\n4931             - mapping {var name: (dimension name, array-like)}\n4932             - mapping {var name: (tuple of dimension names, array-like)}\n4933 \n4934         Returns\n4935         -------\n4936         updated : Dataset\n4937             Updated dataset. Note that since the update is in-place this is the input\n4938             dataset.\n4939 \n4940             It is deprecated since version 0.17 and scheduled to be removed in 0.21.\n4941 \n4942         Raises\n4943         ------\n4944         ValueError\n4945             If any dimensions would have inconsistent sizes in the updated\n4946             dataset.\n4947 \n4948         See Also\n4949         --------\n4950         Dataset.assign\n4951         Dataset.merge\n4952         \"\"\"\n4953         merge_result = dataset_update_method(self, other)\n4954         return self._replace(inplace=True, **merge_result._asdict())\n4955 \n4956     def merge(\n4957         self: T_Dataset,\n4958         other: CoercibleMapping | DataArray,\n4959         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),\n4960         compat: CompatOptions = \"no_conflicts\",\n4961         join: JoinOptions = \"outer\",\n4962         fill_value: Any = xrdtypes.NA,\n4963         combine_attrs: CombineAttrsOptions = \"override\",\n4964     ) -> T_Dataset:\n4965         \"\"\"Merge the arrays of two datasets into a single dataset.\n4966 \n4967         This method generally does not allow for overriding data, with the\n4968         exception of attributes, which are ignored on the second dataset.\n4969         Variables with the same name are checked for conflicts via the equals\n4970         or identical methods.\n4971 \n4972         Parameters\n4973         ----------\n4974         other : Dataset or mapping\n4975             Dataset or variables to merge with this dataset.\n4976         overwrite_vars : hashable or iterable of hashable, optional\n4977             If provided, update variables of these name(s) without checking for\n4978             conflicts in this dataset.\n4979         compat : {\"identical\", \"equals\", \"broadcast_equals\", \\\n4980                   \"no_conflicts\", \"override\", \"minimal\"}, default: \"no_conflicts\"\n4981             String indicating how to compare variables of the same name for\n4982             potential conflicts:\n4983 \n4984             - 'identical': all values, dimensions and attributes must be the\n4985               same.\n4986             - 'equals': all values and dimensions must be the same.\n4987             - 'broadcast_equals': all values must be equal when variables are\n4988               broadcast against each other to ensure common dimensions.\n4989             - 'no_conflicts': only values which are not null in both datasets\n4990               must be equal. The returned dataset then contains the combination\n4991               of all non-null values.\n4992             - 'override': skip comparing and pick variable from first dataset\n4993             - 'minimal': drop conflicting coordinates\n4994 \n4995         join : {\"outer\", \"inner\", \"left\", \"right\", \"exact\", \"override\"}, \\\n4996                default: \"outer\"\n4997             Method for joining ``self`` and ``other`` along shared dimensions:\n4998 \n4999             - 'outer': use the union of the indexes\n5000             - 'inner': use the intersection of the indexes\n5001             - 'left': use indexes from ``self``\n5002             - 'right': use indexes from ``other``\n5003             - 'exact': error instead of aligning non-equal indexes\n5004             - 'override': use indexes from ``self`` that are the same size\n5005               as those of ``other`` in that dimension\n5006 \n5007         fill_value : scalar or dict-like, optional\n5008             Value to use for newly missing values. If a dict-like, maps\n5009             variable names (including coordinates) to fill values.\n5010         combine_attrs : {\"drop\", \"identical\", \"no_conflicts\", \"drop_conflicts\", \\\n5011                          \"override\"} or callable, default: \"override\"\n5012             A callable or a string indicating how to combine attrs of the objects being\n5013             merged:\n5014 \n5015             - \"drop\": empty attrs on returned Dataset.\n5016             - \"identical\": all attrs must be the same on every object.\n5017             - \"no_conflicts\": attrs from all objects are combined, any that have\n5018               the same name must also have the same value.\n5019             - \"drop_conflicts\": attrs from all objects are combined, any that have\n5020               the same name but different values are dropped.\n5021             - \"override\": skip comparing and copy attrs from the first dataset to\n5022               the result.\n5023 \n5024             If a callable, it must expect a sequence of ``attrs`` dicts and a context object\n5025             as its only parameters.\n5026 \n5027         Returns\n5028         -------\n5029         merged : Dataset\n5030             Merged dataset.\n5031 \n5032         Raises\n5033         ------\n5034         MergeError\n5035             If any variables conflict (see ``compat``).\n5036 \n5037         See Also\n5038         --------\n5039         Dataset.update\n5040         \"\"\"\n5041         from xarray.core.dataarray import DataArray\n5042 \n5043         other = other.to_dataset() if isinstance(other, DataArray) else other\n5044         merge_result = dataset_merge_method(\n5045             self,\n5046             other,\n5047             overwrite_vars=overwrite_vars,\n5048             compat=compat,\n5049             join=join,\n5050             fill_value=fill_value,\n5051             combine_attrs=combine_attrs,\n5052         )\n5053         return self._replace(**merge_result._asdict())\n5054 \n5055     def _assert_all_in_dataset(\n5056         self, names: Iterable[Hashable], virtual_okay: bool = False\n5057     ) -> None:\n5058         bad_names = set(names) - set(self._variables)\n5059         if virtual_okay:\n5060             bad_names -= self.virtual_variables\n5061         if bad_names:\n5062             ordered_bad_names = [name for name in names if name in bad_names]\n5063             raise ValueError(\n5064                 f\"These variables cannot be found in this dataset: {ordered_bad_names}\"\n5065             )\n5066 \n5067     def drop_vars(\n5068         self: T_Dataset,\n5069         names: Hashable | Iterable[Hashable],\n5070         *,\n5071         errors: ErrorOptions = \"raise\",\n5072     ) -> T_Dataset:\n5073         \"\"\"Drop variables from this dataset.\n5074 \n5075         Parameters\n5076         ----------\n5077         names : hashable or iterable of hashable\n5078             Name(s) of variables to drop.\n5079         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5080             If 'raise', raises a ValueError error if any of the variable\n5081             passed are not in the dataset. If 'ignore', any given names that are in the\n5082             dataset are dropped and no error is raised.\n5083 \n5084         Returns\n5085         -------\n5086         dropped : Dataset\n5087 \n5088         \"\"\"\n5089         # the Iterable check is required for mypy\n5090         if is_scalar(names) or not isinstance(names, Iterable):\n5091             names = {names}\n5092         else:\n5093             names = set(names)\n5094         if errors == \"raise\":\n5095             self._assert_all_in_dataset(names)\n5096 \n5097         # GH6505\n5098         other_names = set()\n5099         for var in names:\n5100             maybe_midx = self._indexes.get(var, None)\n5101             if isinstance(maybe_midx, PandasMultiIndex):\n5102                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])\n5103                 idx_other_names = idx_coord_names - set(names)\n5104                 other_names.update(idx_other_names)\n5105         if other_names:\n5106             names |= set(other_names)\n5107             warnings.warn(\n5108                 f\"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. \"\n5109                 f\"Please also drop the following variables: {other_names!r} to avoid an error in the future.\",\n5110                 DeprecationWarning,\n5111                 stacklevel=2,\n5112             )\n5113 \n5114         assert_no_index_corrupted(self.xindexes, names)\n5115 \n5116         variables = {k: v for k, v in self._variables.items() if k not in names}\n5117         coord_names = {k for k in self._coord_names if k in variables}\n5118         indexes = {k: v for k, v in self._indexes.items() if k not in names}\n5119         return self._replace_with_new_dims(\n5120             variables, coord_names=coord_names, indexes=indexes\n5121         )\n5122 \n5123     def drop_indexes(\n5124         self: T_Dataset,\n5125         coord_names: Hashable | Iterable[Hashable],\n5126         *,\n5127         errors: ErrorOptions = \"raise\",\n5128     ) -> T_Dataset:\n5129         \"\"\"Drop the indexes assigned to the given coordinates.\n5130 \n5131         Parameters\n5132         ----------\n5133         coord_names : hashable or iterable of hashable\n5134             Name(s) of the coordinate(s) for which to drop the index.\n5135         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5136             If 'raise', raises a ValueError error if any of the coordinates\n5137             passed have no index or are not in the dataset.\n5138             If 'ignore', no error is raised.\n5139 \n5140         Returns\n5141         -------\n5142         dropped : Dataset\n5143             A new dataset with dropped indexes.\n5144 \n5145         \"\"\"\n5146         # the Iterable check is required for mypy\n5147         if is_scalar(coord_names) or not isinstance(coord_names, Iterable):\n5148             coord_names = {coord_names}\n5149         else:\n5150             coord_names = set(coord_names)\n5151 \n5152         if errors == \"raise\":\n5153             invalid_coords = coord_names - self._coord_names\n5154             if invalid_coords:\n5155                 raise ValueError(f\"those coordinates don't exist: {invalid_coords}\")\n5156 \n5157             unindexed_coords = set(coord_names) - set(self._indexes)\n5158             if unindexed_coords:\n5159                 raise ValueError(\n5160                     f\"those coordinates do not have an index: {unindexed_coords}\"\n5161                 )\n5162 \n5163         assert_no_index_corrupted(self.xindexes, coord_names, action=\"remove index(es)\")\n5164 \n5165         variables = {}\n5166         for name, var in self._variables.items():\n5167             if name in coord_names:\n5168                 variables[name] = var.to_base_variable()\n5169             else:\n5170                 variables[name] = var\n5171 \n5172         indexes = {k: v for k, v in self._indexes.items() if k not in coord_names}\n5173 \n5174         return self._replace(variables=variables, indexes=indexes)\n5175 \n5176     def drop(\n5177         self: T_Dataset,\n5178         labels=None,\n5179         dim=None,\n5180         *,\n5181         errors: ErrorOptions = \"raise\",\n5182         **labels_kwargs,\n5183     ) -> T_Dataset:\n5184         \"\"\"Backward compatible method based on `drop_vars` and `drop_sel`\n5185 \n5186         Using either `drop_vars` or `drop_sel` is encouraged\n5187 \n5188         See Also\n5189         --------\n5190         Dataset.drop_vars\n5191         Dataset.drop_sel\n5192         \"\"\"\n5193         if errors not in [\"raise\", \"ignore\"]:\n5194             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n5195 \n5196         if is_dict_like(labels) and not isinstance(labels, dict):\n5197             warnings.warn(\n5198                 \"dropping coordinates using `drop` is be deprecated; use drop_vars.\",\n5199                 FutureWarning,\n5200                 stacklevel=2,\n5201             )\n5202             return self.drop_vars(labels, errors=errors)\n5203 \n5204         if labels_kwargs or isinstance(labels, dict):\n5205             if dim is not None:\n5206                 raise ValueError(\"cannot specify dim and dict-like arguments.\")\n5207             labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop\")\n5208 \n5209         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):\n5210             warnings.warn(\n5211                 \"dropping variables using `drop` will be deprecated; using drop_vars is encouraged.\",\n5212                 PendingDeprecationWarning,\n5213                 stacklevel=2,\n5214             )\n5215             return self.drop_vars(labels, errors=errors)\n5216         if dim is not None:\n5217             warnings.warn(\n5218                 \"dropping labels using list-like labels is deprecated; using \"\n5219                 \"dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).\",\n5220                 DeprecationWarning,\n5221                 stacklevel=2,\n5222             )\n5223             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)\n5224 \n5225         warnings.warn(\n5226             \"dropping labels using `drop` will be deprecated; using drop_sel is encouraged.\",\n5227             PendingDeprecationWarning,\n5228             stacklevel=2,\n5229         )\n5230         return self.drop_sel(labels, errors=errors)\n5231 \n5232     def drop_sel(\n5233         self: T_Dataset, labels=None, *, errors: ErrorOptions = \"raise\", **labels_kwargs\n5234     ) -> T_Dataset:\n5235         \"\"\"Drop index labels from this dataset.\n5236 \n5237         Parameters\n5238         ----------\n5239         labels : mapping of hashable to Any\n5240             Index labels to drop\n5241         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5242             If 'raise', raises a ValueError error if\n5243             any of the index labels passed are not\n5244             in the dataset. If 'ignore', any given labels that are in the\n5245             dataset are dropped and no error is raised.\n5246         **labels_kwargs : {dim: label, ...}, optional\n5247             The keyword arguments form of ``dim`` and ``labels``\n5248 \n5249         Returns\n5250         -------\n5251         dropped : Dataset\n5252 \n5253         Examples\n5254         --------\n5255         >>> data = np.arange(6).reshape(2, 3)\n5256         >>> labels = [\"a\", \"b\", \"c\"]\n5257         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n5258         >>> ds\n5259         <xarray.Dataset>\n5260         Dimensions:  (x: 2, y: 3)\n5261         Coordinates:\n5262           * y        (y) <U1 'a' 'b' 'c'\n5263         Dimensions without coordinates: x\n5264         Data variables:\n5265             A        (x, y) int64 0 1 2 3 4 5\n5266         >>> ds.drop_sel(y=[\"a\", \"c\"])\n5267         <xarray.Dataset>\n5268         Dimensions:  (x: 2, y: 1)\n5269         Coordinates:\n5270           * y        (y) <U1 'b'\n5271         Dimensions without coordinates: x\n5272         Data variables:\n5273             A        (x, y) int64 1 4\n5274         >>> ds.drop_sel(y=\"b\")\n5275         <xarray.Dataset>\n5276         Dimensions:  (x: 2, y: 2)\n5277         Coordinates:\n5278           * y        (y) <U1 'a' 'c'\n5279         Dimensions without coordinates: x\n5280         Data variables:\n5281             A        (x, y) int64 0 2 3 5\n5282         \"\"\"\n5283         if errors not in [\"raise\", \"ignore\"]:\n5284             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n5285 \n5286         labels = either_dict_or_kwargs(labels, labels_kwargs, \"drop_sel\")\n5287 \n5288         ds = self\n5289         for dim, labels_for_dim in labels.items():\n5290             # Don't cast to set, as it would harm performance when labels\n5291             # is a large numpy array\n5292             if utils.is_scalar(labels_for_dim):\n5293                 labels_for_dim = [labels_for_dim]\n5294             labels_for_dim = np.asarray(labels_for_dim)\n5295             try:\n5296                 index = self.get_index(dim)\n5297             except KeyError:\n5298                 raise ValueError(f\"dimension {dim!r} does not have coordinate labels\")\n5299             new_index = index.drop(labels_for_dim, errors=errors)\n5300             ds = ds.loc[{dim: new_index}]\n5301         return ds\n5302 \n5303     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:\n5304         \"\"\"Drop index positions from this Dataset.\n5305 \n5306         Parameters\n5307         ----------\n5308         indexers : mapping of hashable to Any\n5309             Index locations to drop\n5310         **indexers_kwargs : {dim: position, ...}, optional\n5311             The keyword arguments form of ``dim`` and ``positions``\n5312 \n5313         Returns\n5314         -------\n5315         dropped : Dataset\n5316 \n5317         Raises\n5318         ------\n5319         IndexError\n5320 \n5321         Examples\n5322         --------\n5323         >>> data = np.arange(6).reshape(2, 3)\n5324         >>> labels = [\"a\", \"b\", \"c\"]\n5325         >>> ds = xr.Dataset({\"A\": ([\"x\", \"y\"], data), \"y\": labels})\n5326         >>> ds\n5327         <xarray.Dataset>\n5328         Dimensions:  (x: 2, y: 3)\n5329         Coordinates:\n5330           * y        (y) <U1 'a' 'b' 'c'\n5331         Dimensions without coordinates: x\n5332         Data variables:\n5333             A        (x, y) int64 0 1 2 3 4 5\n5334         >>> ds.drop_isel(y=[0, 2])\n5335         <xarray.Dataset>\n5336         Dimensions:  (x: 2, y: 1)\n5337         Coordinates:\n5338           * y        (y) <U1 'b'\n5339         Dimensions without coordinates: x\n5340         Data variables:\n5341             A        (x, y) int64 1 4\n5342         >>> ds.drop_isel(y=1)\n5343         <xarray.Dataset>\n5344         Dimensions:  (x: 2, y: 2)\n5345         Coordinates:\n5346           * y        (y) <U1 'a' 'c'\n5347         Dimensions without coordinates: x\n5348         Data variables:\n5349             A        (x, y) int64 0 2 3 5\n5350         \"\"\"\n5351 \n5352         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"drop_isel\")\n5353 \n5354         ds = self\n5355         dimension_index = {}\n5356         for dim, pos_for_dim in indexers.items():\n5357             # Don't cast to set, as it would harm performance when labels\n5358             # is a large numpy array\n5359             if utils.is_scalar(pos_for_dim):\n5360                 pos_for_dim = [pos_for_dim]\n5361             pos_for_dim = np.asarray(pos_for_dim)\n5362             index = self.get_index(dim)\n5363             new_index = index.delete(pos_for_dim)\n5364             dimension_index[dim] = new_index\n5365         ds = ds.loc[dimension_index]\n5366         return ds\n5367 \n5368     def drop_dims(\n5369         self: T_Dataset,\n5370         drop_dims: str | Iterable[Hashable],\n5371         *,\n5372         errors: ErrorOptions = \"raise\",\n5373     ) -> T_Dataset:\n5374         \"\"\"Drop dimensions and associated variables from this dataset.\n5375 \n5376         Parameters\n5377         ----------\n5378         drop_dims : str or Iterable of Hashable\n5379             Dimension or dimensions to drop.\n5380         errors : {\"raise\", \"ignore\"}, default: \"raise\"\n5381             If 'raise', raises a ValueError error if any of the\n5382             dimensions passed are not in the dataset. If 'ignore', any given\n5383             dimensions that are in the dataset are dropped and no error is raised.\n5384 \n5385         Returns\n5386         -------\n5387         obj : Dataset\n5388             The dataset without the given dimensions (or any variables\n5389             containing those dimensions).\n5390         \"\"\"\n5391         if errors not in [\"raise\", \"ignore\"]:\n5392             raise ValueError('errors must be either \"raise\" or \"ignore\"')\n5393 \n5394         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):\n5395             drop_dims = {drop_dims}\n5396         else:\n5397             drop_dims = set(drop_dims)\n5398 \n5399         if errors == \"raise\":\n5400             missing_dims = drop_dims - set(self.dims)\n5401             if missing_dims:\n5402                 raise ValueError(\n5403                     f\"Dataset does not contain the dimensions: {missing_dims}\"\n5404                 )\n5405 \n5406         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}\n5407         return self.drop_vars(drop_vars)\n5408 \n5409     def transpose(\n5410         self: T_Dataset,\n5411         *dims: Hashable,\n5412         missing_dims: ErrorOptionsWithWarn = \"raise\",\n5413     ) -> T_Dataset:\n5414         \"\"\"Return a new Dataset object with all array dimensions transposed.\n5415 \n5416         Although the order of dimensions on each array will change, the dataset\n5417         dimensions themselves will remain in fixed (sorted) order.\n5418 \n5419         Parameters\n5420         ----------\n5421         *dims : hashable, optional\n5422             By default, reverse the dimensions on each array. Otherwise,\n5423             reorder the dimensions to this order.\n5424         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n5425             What to do if dimensions that should be selected from are not present in the\n5426             Dataset:\n5427             - \"raise\": raise an exception\n5428             - \"warn\": raise a warning, and ignore the missing dimensions\n5429             - \"ignore\": ignore the missing dimensions\n5430 \n5431         Returns\n5432         -------\n5433         transposed : Dataset\n5434             Each array in the dataset (including) coordinates will be\n5435             transposed to the given order.\n5436 \n5437         Notes\n5438         -----\n5439         This operation returns a view of each array's data. It is\n5440         lazy for dask-backed DataArrays but not for numpy-backed DataArrays\n5441         -- the data will be fully loaded into memory.\n5442 \n5443         See Also\n5444         --------\n5445         numpy.transpose\n5446         DataArray.transpose\n5447         \"\"\"\n5448         # Raise error if list is passed as dims\n5449         if (len(dims) > 0) and (isinstance(dims[0], list)):\n5450             list_fix = [f\"{repr(x)}\" if isinstance(x, str) else f\"{x}\" for x in dims[0]]\n5451             raise TypeError(\n5452                 f'transpose requires dims to be passed as multiple arguments. Expected `{\", \".join(list_fix)}`. Received `{dims[0]}` instead'\n5453             )\n5454 \n5455         # Use infix_dims to check once for missing dimensions\n5456         if len(dims) != 0:\n5457             _ = list(infix_dims(dims, self.dims, missing_dims))\n5458 \n5459         ds = self.copy()\n5460         for name, var in self._variables.items():\n5461             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))\n5462             ds._variables[name] = var.transpose(*var_dims)\n5463         return ds\n5464 \n5465     def dropna(\n5466         self: T_Dataset,\n5467         dim: Hashable,\n5468         how: Literal[\"any\", \"all\"] = \"any\",\n5469         thresh: int | None = None,\n5470         subset: Iterable[Hashable] | None = None,\n5471     ) -> T_Dataset:\n5472         \"\"\"Returns a new dataset with dropped labels for missing values along\n5473         the provided dimension.\n5474 \n5475         Parameters\n5476         ----------\n5477         dim : hashable\n5478             Dimension along which to drop missing values. Dropping along\n5479             multiple dimensions simultaneously is not yet supported.\n5480         how : {\"any\", \"all\"}, default: \"any\"\n5481             - any : if any NA values are present, drop that label\n5482             - all : if all values are NA, drop that label\n5483 \n5484         thresh : int or None, optional\n5485             If supplied, require this many non-NA values (summed over all the subset variables).\n5486         subset : iterable of hashable or None, optional\n5487             Which variables to check for missing values. By default, all\n5488             variables in the dataset are checked.\n5489 \n5490         Returns\n5491         -------\n5492         Dataset\n5493         \"\"\"\n5494         # TODO: consider supporting multiple dimensions? Or not, given that\n5495         # there are some ugly edge cases, e.g., pandas's dropna differs\n5496         # depending on the order of the supplied axes.\n5497 \n5498         if dim not in self.dims:\n5499             raise ValueError(f\"{dim} must be a single dataset dimension\")\n5500 \n5501         if subset is None:\n5502             subset = iter(self.data_vars)\n5503 \n5504         count = np.zeros(self.dims[dim], dtype=np.int64)\n5505         size = np.int_(0)  # for type checking\n5506 \n5507         for k in subset:\n5508             array = self._variables[k]\n5509             if dim in array.dims:\n5510                 dims = [d for d in array.dims if d != dim]\n5511                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]\n5512                 size += math.prod([self.dims[d] for d in dims])\n5513 \n5514         if thresh is not None:\n5515             mask = count >= thresh\n5516         elif how == \"any\":\n5517             mask = count == size\n5518         elif how == \"all\":\n5519             mask = count > 0\n5520         elif how is not None:\n5521             raise ValueError(f\"invalid how option: {how}\")\n5522         else:\n5523             raise TypeError(\"must specify how or thresh\")\n5524 \n5525         return self.isel({dim: mask})\n5526 \n5527     def fillna(self: T_Dataset, value: Any) -> T_Dataset:\n5528         \"\"\"Fill missing values in this object.\n5529 \n5530         This operation follows the normal broadcasting and alignment rules that\n5531         xarray uses for binary arithmetic, except the result is aligned to this\n5532         object (``join='left'``) instead of aligned to the intersection of\n5533         index coordinates (``join='inner'``).\n5534 \n5535         Parameters\n5536         ----------\n5537         value : scalar, ndarray, DataArray, dict or Dataset\n5538             Used to fill all matching missing values in this dataset's data\n5539             variables. Scalars, ndarrays or DataArrays arguments are used to\n5540             fill all data with aligned coordinates (for DataArrays).\n5541             Dictionaries or datasets match data variables and then align\n5542             coordinates if necessary.\n5543 \n5544         Returns\n5545         -------\n5546         Dataset\n5547 \n5548         Examples\n5549         --------\n5550         >>> ds = xr.Dataset(\n5551         ...     {\n5552         ...         \"A\": (\"x\", [np.nan, 2, np.nan, 0]),\n5553         ...         \"B\": (\"x\", [3, 4, np.nan, 1]),\n5554         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5]),\n5555         ...         \"D\": (\"x\", [np.nan, 3, np.nan, 4]),\n5556         ...     },\n5557         ...     coords={\"x\": [0, 1, 2, 3]},\n5558         ... )\n5559         >>> ds\n5560         <xarray.Dataset>\n5561         Dimensions:  (x: 4)\n5562         Coordinates:\n5563           * x        (x) int64 0 1 2 3\n5564         Data variables:\n5565             A        (x) float64 nan 2.0 nan 0.0\n5566             B        (x) float64 3.0 4.0 nan 1.0\n5567             C        (x) float64 nan nan nan 5.0\n5568             D        (x) float64 nan 3.0 nan 4.0\n5569 \n5570         Replace all `NaN` values with 0s.\n5571 \n5572         >>> ds.fillna(0)\n5573         <xarray.Dataset>\n5574         Dimensions:  (x: 4)\n5575         Coordinates:\n5576           * x        (x) int64 0 1 2 3\n5577         Data variables:\n5578             A        (x) float64 0.0 2.0 0.0 0.0\n5579             B        (x) float64 3.0 4.0 0.0 1.0\n5580             C        (x) float64 0.0 0.0 0.0 5.0\n5581             D        (x) float64 0.0 3.0 0.0 4.0\n5582 \n5583         Replace all `NaN` elements in column \u2018A\u2019, \u2018B\u2019, \u2018C\u2019, and \u2018D\u2019, with 0, 1, 2, and 3 respectively.\n5584 \n5585         >>> values = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\n5586         >>> ds.fillna(value=values)\n5587         <xarray.Dataset>\n5588         Dimensions:  (x: 4)\n5589         Coordinates:\n5590           * x        (x) int64 0 1 2 3\n5591         Data variables:\n5592             A        (x) float64 0.0 2.0 0.0 0.0\n5593             B        (x) float64 3.0 4.0 1.0 1.0\n5594             C        (x) float64 2.0 2.0 2.0 5.0\n5595             D        (x) float64 3.0 3.0 3.0 4.0\n5596         \"\"\"\n5597         if utils.is_dict_like(value):\n5598             value_keys = getattr(value, \"data_vars\", value).keys()\n5599             if not set(value_keys) <= set(self.data_vars.keys()):\n5600                 raise ValueError(\n5601                     \"all variables in the argument to `fillna` \"\n5602                     \"must be contained in the original dataset\"\n5603                 )\n5604         out = ops.fillna(self, value)\n5605         return out\n5606 \n5607     def interpolate_na(\n5608         self: T_Dataset,\n5609         dim: Hashable | None = None,\n5610         method: InterpOptions = \"linear\",\n5611         limit: int | None = None,\n5612         use_coordinate: bool | Hashable = True,\n5613         max_gap: (\n5614             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta\n5615         ) = None,\n5616         **kwargs: Any,\n5617     ) -> T_Dataset:\n5618         \"\"\"Fill in NaNs by interpolating according to different methods.\n5619 \n5620         Parameters\n5621         ----------\n5622         dim : Hashable or None, optional\n5623             Specifies the dimension along which to interpolate.\n5624         method : {\"linear\", \"nearest\", \"zero\", \"slinear\", \"quadratic\", \"cubic\", \"polynomial\", \\\n5625             \"barycentric\", \"krog\", \"pchip\", \"spline\", \"akima\"}, default: \"linear\"\n5626             String indicating which method to use for interpolation:\n5627 \n5628             - 'linear': linear interpolation. Additional keyword\n5629               arguments are passed to :py:func:`numpy.interp`\n5630             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':\n5631               are passed to :py:func:`scipy.interpolate.interp1d`. If\n5632               ``method='polynomial'``, the ``order`` keyword argument must also be\n5633               provided.\n5634             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their\n5635               respective :py:class:`scipy.interpolate` classes.\n5636 \n5637         use_coordinate : bool or Hashable, default: True\n5638             Specifies which index to use as the x values in the interpolation\n5639             formulated as `y = f(x)`. If False, values are treated as if\n5640             equally-spaced along ``dim``. If True, the IndexVariable `dim` is\n5641             used. If ``use_coordinate`` is a string, it specifies the name of a\n5642             coordinate variable to use as the index.\n5643         limit : int, default: None\n5644             Maximum number of consecutive NaNs to fill. Must be greater than 0\n5645             or None for no limit. This filling is done regardless of the size of\n5646             the gap in the data. To only interpolate over gaps less than a given length,\n5647             see ``max_gap``.\n5648         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None\n5649             Maximum size of gap, a continuous sequence of NaNs, that will be filled.\n5650             Use None for no limit. When interpolating along a datetime64 dimension\n5651             and ``use_coordinate=True``, ``max_gap`` can be one of the following:\n5652 \n5653             - a string that is valid input for pandas.to_timedelta\n5654             - a :py:class:`numpy.timedelta64` object\n5655             - a :py:class:`pandas.Timedelta` object\n5656             - a :py:class:`datetime.timedelta` object\n5657 \n5658             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled\n5659             dimensions has not been implemented yet. Gap length is defined as the difference\n5660             between coordinate values at the first data point after a gap and the last value\n5661             before a gap. For gaps at the beginning (end), gap length is defined as the difference\n5662             between coordinate values at the first (last) valid data point and the first (last) NaN.\n5663             For example, consider::\n5664 \n5665                 <xarray.DataArray (x: 9)>\n5666                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])\n5667                 Coordinates:\n5668                   * x        (x) int64 0 1 2 3 4 5 6 7 8\n5669 \n5670             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively\n5671         **kwargs : dict, optional\n5672             parameters passed verbatim to the underlying interpolation function\n5673 \n5674         Returns\n5675         -------\n5676         interpolated: Dataset\n5677             Filled in Dataset.\n5678 \n5679         See Also\n5680         --------\n5681         numpy.interp\n5682         scipy.interpolate\n5683 \n5684         Examples\n5685         --------\n5686         >>> ds = xr.Dataset(\n5687         ...     {\n5688         ...         \"A\": (\"x\", [np.nan, 2, 3, np.nan, 0]),\n5689         ...         \"B\": (\"x\", [3, 4, np.nan, 1, 7]),\n5690         ...         \"C\": (\"x\", [np.nan, np.nan, np.nan, 5, 0]),\n5691         ...         \"D\": (\"x\", [np.nan, 3, np.nan, -1, 4]),\n5692         ...     },\n5693         ...     coords={\"x\": [0, 1, 2, 3, 4]},\n5694         ... )\n5695         >>> ds\n5696         <xarray.Dataset>\n5697         Dimensions:  (x: 5)\n5698         Coordinates:\n5699           * x        (x) int64 0 1 2 3 4\n5700         Data variables:\n5701             A        (x) float64 nan 2.0 3.0 nan 0.0\n5702             B        (x) float64 3.0 4.0 nan 1.0 7.0\n5703             C        (x) float64 nan nan nan 5.0 0.0\n5704             D        (x) float64 nan 3.0 nan -1.0 4.0\n5705 \n5706         >>> ds.interpolate_na(dim=\"x\", method=\"linear\")\n5707         <xarray.Dataset>\n5708         Dimensions:  (x: 5)\n5709         Coordinates:\n5710           * x        (x) int64 0 1 2 3 4\n5711         Data variables:\n5712             A        (x) float64 nan 2.0 3.0 1.5 0.0\n5713             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n5714             C        (x) float64 nan nan nan 5.0 0.0\n5715             D        (x) float64 nan 3.0 1.0 -1.0 4.0\n5716 \n5717         >>> ds.interpolate_na(dim=\"x\", method=\"linear\", fill_value=\"extrapolate\")\n5718         <xarray.Dataset>\n5719         Dimensions:  (x: 5)\n5720         Coordinates:\n5721           * x        (x) int64 0 1 2 3 4\n5722         Data variables:\n5723             A        (x) float64 1.0 2.0 3.0 1.5 0.0\n5724             B        (x) float64 3.0 4.0 2.5 1.0 7.0\n5725             C        (x) float64 20.0 15.0 10.0 5.0 0.0\n5726             D        (x) float64 5.0 3.0 1.0 -1.0 4.0\n5727         \"\"\"\n5728         from xarray.core.missing import _apply_over_vars_with_dim, interp_na\n5729 \n5730         new = _apply_over_vars_with_dim(\n5731             interp_na,\n5732             self,\n5733             dim=dim,\n5734             method=method,\n5735             limit=limit,\n5736             use_coordinate=use_coordinate,\n5737             max_gap=max_gap,\n5738             **kwargs,\n5739         )\n5740         return new\n5741 \n5742     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:\n5743         \"\"\"Fill NaN values by propagating values forward\n5744 \n5745         *Requires bottleneck.*\n5746 \n5747         Parameters\n5748         ----------\n5749         dim : Hashable\n5750             Specifies the dimension along which to propagate values when\n5751             filling.\n5752         limit : int or None, optional\n5753             The maximum number of consecutive NaN values to forward fill. In\n5754             other words, if there is a gap with more than this number of\n5755             consecutive NaNs, it will only be partially filled. Must be greater\n5756             than 0 or None for no limit. Must be None or greater than or equal\n5757             to axis length if filling along chunked axes (dimensions).\n5758 \n5759         Returns\n5760         -------\n5761         Dataset\n5762         \"\"\"\n5763         from xarray.core.missing import _apply_over_vars_with_dim, ffill\n5764 \n5765         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)\n5766         return new\n5767 \n5768     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:\n5769         \"\"\"Fill NaN values by propagating values backward\n5770 \n5771         *Requires bottleneck.*\n5772 \n5773         Parameters\n5774         ----------\n5775         dim : Hashable\n5776             Specifies the dimension along which to propagate values when\n5777             filling.\n5778         limit : int or None, optional\n5779             The maximum number of consecutive NaN values to backward fill. In\n5780             other words, if there is a gap with more than this number of\n5781             consecutive NaNs, it will only be partially filled. Must be greater\n5782             than 0 or None for no limit. Must be None or greater than or equal\n5783             to axis length if filling along chunked axes (dimensions).\n5784 \n5785         Returns\n5786         -------\n5787         Dataset\n5788         \"\"\"\n5789         from xarray.core.missing import _apply_over_vars_with_dim, bfill\n5790 \n5791         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n5792         return new\n5793 \n5794     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:\n5795         \"\"\"Combine two Datasets, default to data_vars of self.\n5796 \n5797         The new coordinates follow the normal broadcasting and alignment rules\n5798         of ``join='outer'``.  Vacant cells in the expanded coordinates are\n5799         filled with np.nan.\n5800 \n5801         Parameters\n5802         ----------\n5803         other : Dataset\n5804             Used to fill all matching missing values in this array.\n5805 \n5806         Returns\n5807         -------\n5808         Dataset\n5809         \"\"\"\n5810         out = ops.fillna(self, other, join=\"outer\", dataset_join=\"outer\")\n5811         return out\n5812 \n5813     def reduce(\n5814         self: T_Dataset,\n5815         func: Callable,\n5816         dim: Dims = None,\n5817         *,\n5818         keep_attrs: bool | None = None,\n5819         keepdims: bool = False,\n5820         numeric_only: bool = False,\n5821         **kwargs: Any,\n5822     ) -> T_Dataset:\n5823         \"\"\"Reduce this dataset by applying `func` along some dimension(s).\n5824 \n5825         Parameters\n5826         ----------\n5827         func : callable\n5828             Function which can be called in the form\n5829             `f(x, axis=axis, **kwargs)` to return the result of reducing an\n5830             np.ndarray over an integer valued axis.\n5831         dim : str, Iterable of Hashable or None, optional\n5832             Dimension(s) over which to apply `func`. By default `func` is\n5833             applied over all dimensions.\n5834         keep_attrs : bool or None, optional\n5835             If True, the dataset's attributes (`attrs`) will be copied from\n5836             the original object to the new one.  If False (default), the new\n5837             object will be returned without attributes.\n5838         keepdims : bool, default: False\n5839             If True, the dimensions which are reduced are left in the result\n5840             as dimensions of size one. Coordinates that use these dimensions\n5841             are removed.\n5842         numeric_only : bool, default: False\n5843             If True, only apply ``func`` to variables with a numeric dtype.\n5844         **kwargs : Any\n5845             Additional keyword arguments passed on to ``func``.\n5846 \n5847         Returns\n5848         -------\n5849         reduced : Dataset\n5850             Dataset with this object's DataArrays replaced with new DataArrays\n5851             of summarized data and the indicated dimension(s) removed.\n5852         \"\"\"\n5853         if kwargs.get(\"axis\", None) is not None:\n5854             raise ValueError(\n5855                 \"passing 'axis' to Dataset reduce methods is ambiguous.\"\n5856                 \" Please use 'dim' instead.\"\n5857             )\n5858 \n5859         if dim is None or dim is ...:\n5860             dims = set(self.dims)\n5861         elif isinstance(dim, str) or not isinstance(dim, Iterable):\n5862             dims = {dim}\n5863         else:\n5864             dims = set(dim)\n5865 \n5866         missing_dimensions = [d for d in dims if d not in self.dims]\n5867         if missing_dimensions:\n5868             raise ValueError(\n5869                 f\"Dataset does not contain the dimensions: {missing_dimensions}\"\n5870             )\n5871 \n5872         if keep_attrs is None:\n5873             keep_attrs = _get_keep_attrs(default=False)\n5874 \n5875         variables: dict[Hashable, Variable] = {}\n5876         for name, var in self._variables.items():\n5877             reduce_dims = [d for d in var.dims if d in dims]\n5878             if name in self.coords:\n5879                 if not reduce_dims:\n5880                     variables[name] = var\n5881             else:\n5882                 if (\n5883                     # Some reduction functions (e.g. std, var) need to run on variables\n5884                     # that don't have the reduce dims: PR5393\n5885                     not reduce_dims\n5886                     or not numeric_only\n5887                     or np.issubdtype(var.dtype, np.number)\n5888                     or (var.dtype == np.bool_)\n5889                 ):\n5890                     # prefer to aggregate over axis=None rather than\n5891                     # axis=(0, 1) if they will be equivalent, because\n5892                     # the former is often more efficient\n5893                     # keep single-element dims as list, to support Hashables\n5894                     reduce_maybe_single = (\n5895                         None\n5896                         if len(reduce_dims) == var.ndim and var.ndim != 1\n5897                         else reduce_dims\n5898                     )\n5899                     variables[name] = var.reduce(\n5900                         func,\n5901                         dim=reduce_maybe_single,\n5902                         keep_attrs=keep_attrs,\n5903                         keepdims=keepdims,\n5904                         **kwargs,\n5905                     )\n5906 \n5907         coord_names = {k for k in self.coords if k in variables}\n5908         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n5909         attrs = self.attrs if keep_attrs else None\n5910         return self._replace_with_new_dims(\n5911             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n5912         )\n5913 \n5914     def map(\n5915         self: T_Dataset,\n5916         func: Callable,\n5917         keep_attrs: bool | None = None,\n5918         args: Iterable[Any] = (),\n5919         **kwargs: Any,\n5920     ) -> T_Dataset:\n5921         \"\"\"Apply a function to each data variable in this dataset\n5922 \n5923         Parameters\n5924         ----------\n5925         func : callable\n5926             Function which can be called in the form `func(x, *args, **kwargs)`\n5927             to transform each DataArray `x` in this dataset into another\n5928             DataArray.\n5929         keep_attrs : bool or None, optional\n5930             If True, both the dataset's and variables' attributes (`attrs`) will be\n5931             copied from the original objects to the new ones. If False, the new dataset\n5932             and variables will be returned without copying the attributes.\n5933         args : iterable, optional\n5934             Positional arguments passed on to `func`.\n5935         **kwargs : Any\n5936             Keyword arguments passed on to `func`.\n5937 \n5938         Returns\n5939         -------\n5940         applied : Dataset\n5941             Resulting dataset from applying ``func`` to each data variable.\n5942 \n5943         Examples\n5944         --------\n5945         >>> da = xr.DataArray(np.random.randn(2, 3))\n5946         >>> ds = xr.Dataset({\"foo\": da, \"bar\": (\"x\", [-1, 2])})\n5947         >>> ds\n5948         <xarray.Dataset>\n5949         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n5950         Dimensions without coordinates: dim_0, dim_1, x\n5951         Data variables:\n5952             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773\n5953             bar      (x) int64 -1 2\n5954         >>> ds.map(np.fabs)\n5955         <xarray.Dataset>\n5956         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)\n5957         Dimensions without coordinates: dim_0, dim_1, x\n5958         Data variables:\n5959             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773\n5960             bar      (x) float64 1.0 2.0\n5961         \"\"\"\n5962         if keep_attrs is None:\n5963             keep_attrs = _get_keep_attrs(default=False)\n5964         variables = {\n5965             k: maybe_wrap_array(v, func(v, *args, **kwargs))\n5966             for k, v in self.data_vars.items()\n5967         }\n5968         if keep_attrs:\n5969             for k, v in variables.items():\n5970                 v._copy_attrs_from(self.data_vars[k])\n5971         attrs = self.attrs if keep_attrs else None\n5972         return type(self)(variables, attrs=attrs)\n5973 \n5974     def apply(\n5975         self: T_Dataset,\n5976         func: Callable,\n5977         keep_attrs: bool | None = None,\n5978         args: Iterable[Any] = (),\n5979         **kwargs: Any,\n5980     ) -> T_Dataset:\n5981         \"\"\"\n5982         Backward compatible implementation of ``map``\n5983 \n5984         See Also\n5985         --------\n5986         Dataset.map\n5987         \"\"\"\n5988         warnings.warn(\n5989             \"Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged\",\n5990             PendingDeprecationWarning,\n5991             stacklevel=2,\n5992         )\n5993         return self.map(func, keep_attrs, args, **kwargs)\n5994 \n5995     def assign(\n5996         self: T_Dataset,\n5997         variables: Mapping[Any, Any] | None = None,\n5998         **variables_kwargs: Any,\n5999     ) -> T_Dataset:\n6000         \"\"\"Assign new data variables to a Dataset, returning a new object\n6001         with all the original variables in addition to the new ones.\n6002 \n6003         Parameters\n6004         ----------\n6005         variables : mapping of hashable to Any\n6006             Mapping from variables names to the new values. If the new values\n6007             are callable, they are computed on the Dataset and assigned to new\n6008             data variables. If the values are not callable, (e.g. a DataArray,\n6009             scalar, or array), they are simply assigned.\n6010         **variables_kwargs\n6011             The keyword arguments form of ``variables``.\n6012             One of variables or variables_kwargs must be provided.\n6013 \n6014         Returns\n6015         -------\n6016         ds : Dataset\n6017             A new Dataset with the new variables in addition to all the\n6018             existing variables.\n6019 \n6020         Notes\n6021         -----\n6022         Since ``kwargs`` is a dictionary, the order of your arguments may not\n6023         be preserved, and so the order of the new variables is not well\n6024         defined. Assigning multiple variables within the same ``assign`` is\n6025         possible, but you cannot reference other variables created within the\n6026         same ``assign`` call.\n6027 \n6028         See Also\n6029         --------\n6030         pandas.DataFrame.assign\n6031 \n6032         Examples\n6033         --------\n6034         >>> x = xr.Dataset(\n6035         ...     {\n6036         ...         \"temperature_c\": (\n6037         ...             (\"lat\", \"lon\"),\n6038         ...             20 * np.random.rand(4).reshape(2, 2),\n6039         ...         ),\n6040         ...         \"precipitation\": ((\"lat\", \"lon\"), np.random.rand(4).reshape(2, 2)),\n6041         ...     },\n6042         ...     coords={\"lat\": [10, 20], \"lon\": [150, 160]},\n6043         ... )\n6044         >>> x\n6045         <xarray.Dataset>\n6046         Dimensions:        (lat: 2, lon: 2)\n6047         Coordinates:\n6048           * lat            (lat) int64 10 20\n6049           * lon            (lon) int64 150 160\n6050         Data variables:\n6051             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n6052             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n6053 \n6054         Where the value is a callable, evaluated on dataset:\n6055 \n6056         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)\n6057         <xarray.Dataset>\n6058         Dimensions:        (lat: 2, lon: 2)\n6059         Coordinates:\n6060           * lat            (lat) int64 10 20\n6061           * lon            (lon) int64 150 160\n6062         Data variables:\n6063             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n6064             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n6065             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n6066 \n6067         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:\n6068 \n6069         >>> x.assign(temperature_f=x[\"temperature_c\"] * 9 / 5 + 32)\n6070         <xarray.Dataset>\n6071         Dimensions:        (lat: 2, lon: 2)\n6072         Coordinates:\n6073           * lat            (lat) int64 10 20\n6074           * lon            (lon) int64 150 160\n6075         Data variables:\n6076             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9\n6077             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918\n6078             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62\n6079 \n6080         \"\"\"\n6081         variables = either_dict_or_kwargs(variables, variables_kwargs, \"assign\")\n6082         data = self.copy()\n6083         # do all calculations first...\n6084         results: CoercibleMapping = data._calc_assign_results(variables)\n6085         data.coords._maybe_drop_multiindex_coords(set(results.keys()))\n6086         # ... and then assign\n6087         data.update(results)\n6088         return data\n6089 \n6090     def to_array(\n6091         self, dim: Hashable = \"variable\", name: Hashable | None = None\n6092     ) -> DataArray:\n6093         \"\"\"Convert this dataset into an xarray.DataArray\n6094 \n6095         The data variables of this dataset will be broadcast against each other\n6096         and stacked along the first axis of the new array. All coordinates of\n6097         this dataset will remain coordinates.\n6098 \n6099         Parameters\n6100         ----------\n6101         dim : Hashable, default: \"variable\"\n6102             Name of the new dimension.\n6103         name : Hashable or None, optional\n6104             Name of the new data array.\n6105 \n6106         Returns\n6107         -------\n6108         array : xarray.DataArray\n6109         \"\"\"\n6110         from xarray.core.dataarray import DataArray\n6111 \n6112         data_vars = [self.variables[k] for k in self.data_vars]\n6113         broadcast_vars = broadcast_variables(*data_vars)\n6114         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)\n6115 \n6116         dims = (dim,) + broadcast_vars[0].dims\n6117         variable = Variable(dims, data, self.attrs, fastpath=True)\n6118 \n6119         coords = {k: v.variable for k, v in self.coords.items()}\n6120         indexes = filter_indexes_from_coords(self._indexes, set(coords))\n6121         new_dim_index = PandasIndex(list(self.data_vars), dim)\n6122         indexes[dim] = new_dim_index\n6123         coords.update(new_dim_index.create_variables())\n6124 \n6125         return DataArray._construct_direct(variable, coords, name, indexes)\n6126 \n6127     def _normalize_dim_order(\n6128         self, dim_order: Sequence[Hashable] | None = None\n6129     ) -> dict[Hashable, int]:\n6130         \"\"\"\n6131         Check the validity of the provided dimensions if any and return the mapping\n6132         between dimension name and their size.\n6133 \n6134         Parameters\n6135         ----------\n6136         dim_order: Sequence of Hashable or None, optional\n6137             Dimension order to validate (default to the alphabetical order if None).\n6138 \n6139         Returns\n6140         -------\n6141         result : dict[Hashable, int]\n6142             Validated dimensions mapping.\n6143 \n6144         \"\"\"\n6145         if dim_order is None:\n6146             dim_order = list(self.dims)\n6147         elif set(dim_order) != set(self.dims):\n6148             raise ValueError(\n6149                 \"dim_order {} does not match the set of dimensions of this \"\n6150                 \"Dataset: {}\".format(dim_order, list(self.dims))\n6151             )\n6152 \n6153         ordered_dims = {k: self.dims[k] for k in dim_order}\n6154 \n6155         return ordered_dims\n6156 \n6157     def to_pandas(self) -> pd.Series | pd.DataFrame:\n6158         \"\"\"Convert this dataset into a pandas object without changing the number of dimensions.\n6159 \n6160         The type of the returned object depends on the number of Dataset\n6161         dimensions:\n6162 \n6163         * 0D -> `pandas.Series`\n6164         * 1D -> `pandas.DataFrame`\n6165 \n6166         Only works for Datasets with 1 or fewer dimensions.\n6167         \"\"\"\n6168         if len(self.dims) == 0:\n6169             return pd.Series({k: v.item() for k, v in self.items()})\n6170         if len(self.dims) == 1:\n6171             return self.to_dataframe()\n6172         raise ValueError(\n6173             \"cannot convert Datasets with %s dimensions into \"\n6174             \"pandas objects without changing the number of dimensions. \"\n6175             \"Please use Dataset.to_dataframe() instead.\" % len(self.dims)\n6176         )\n6177 \n6178     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):\n6179         columns = [k for k in self.variables if k not in self.dims]\n6180         data = [\n6181             self._variables[k].set_dims(ordered_dims).values.reshape(-1)\n6182             for k in columns\n6183         ]\n6184         index = self.coords.to_index([*ordered_dims])\n6185         return pd.DataFrame(dict(zip(columns, data)), index=index)\n6186 \n6187     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:\n6188         \"\"\"Convert this dataset into a pandas.DataFrame.\n6189 \n6190         Non-index variables in this dataset form the columns of the\n6191         DataFrame. The DataFrame is indexed by the Cartesian product of\n6192         this dataset's indices.\n6193 \n6194         Parameters\n6195         ----------\n6196         dim_order: Sequence of Hashable or None, optional\n6197             Hierarchical dimension order for the resulting dataframe. All\n6198             arrays are transposed to this order and then written out as flat\n6199             vectors in contiguous order, so the last dimension in this list\n6200             will be contiguous in the resulting DataFrame. This has a major\n6201             influence on which operations are efficient on the resulting\n6202             dataframe.\n6203 \n6204             If provided, must include all dimensions of this dataset. By\n6205             default, dimensions are sorted alphabetically.\n6206 \n6207         Returns\n6208         -------\n6209         result : DataFrame\n6210             Dataset as a pandas DataFrame.\n6211 \n6212         \"\"\"\n6213 \n6214         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n6215 \n6216         return self._to_dataframe(ordered_dims=ordered_dims)\n6217 \n6218     def _set_sparse_data_from_dataframe(\n6219         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple\n6220     ) -> None:\n6221         from sparse import COO\n6222 \n6223         if isinstance(idx, pd.MultiIndex):\n6224             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)\n6225             is_sorted = idx.is_monotonic_increasing\n6226             shape = tuple(lev.size for lev in idx.levels)\n6227         else:\n6228             coords = np.arange(idx.size).reshape(1, -1)\n6229             is_sorted = True\n6230             shape = (idx.size,)\n6231 \n6232         for name, values in arrays:\n6233             # In virtually all real use cases, the sparse array will now have\n6234             # missing values and needs a fill_value. For consistency, don't\n6235             # special case the rare exceptions (e.g., dtype=int without a\n6236             # MultiIndex).\n6237             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)\n6238             values = np.asarray(values, dtype=dtype)\n6239 \n6240             data = COO(\n6241                 coords,\n6242                 values,\n6243                 shape,\n6244                 has_duplicates=False,\n6245                 sorted=is_sorted,\n6246                 fill_value=fill_value,\n6247             )\n6248             self[name] = (dims, data)\n6249 \n6250     def _set_numpy_data_from_dataframe(\n6251         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple\n6252     ) -> None:\n6253         if not isinstance(idx, pd.MultiIndex):\n6254             for name, values in arrays:\n6255                 self[name] = (dims, values)\n6256             return\n6257 \n6258         # NB: similar, more general logic, now exists in\n6259         # variable.unstack_once; we could consider combining them at some\n6260         # point.\n6261 \n6262         shape = tuple(lev.size for lev in idx.levels)\n6263         indexer = tuple(idx.codes)\n6264 \n6265         # We already verified that the MultiIndex has all unique values, so\n6266         # there are missing values if and only if the size of output arrays is\n6267         # larger that the index.\n6268         missing_values = math.prod(shape) > idx.shape[0]\n6269 \n6270         for name, values in arrays:\n6271             # NumPy indexing is much faster than using DataFrame.reindex() to\n6272             # fill in missing values:\n6273             # https://stackoverflow.com/a/35049899/809705\n6274             if missing_values:\n6275                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)\n6276                 data = np.full(shape, fill_value, dtype)\n6277             else:\n6278                 # If there are no missing values, keep the existing dtype\n6279                 # instead of promoting to support NA, e.g., keep integer\n6280                 # columns as integers.\n6281                 # TODO: consider removing this special case, which doesn't\n6282                 # exist for sparse=True.\n6283                 data = np.zeros(shape, values.dtype)\n6284             data[indexer] = values\n6285             self[name] = (dims, data)\n6286 \n6287     @classmethod\n6288     def from_dataframe(\n6289         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False\n6290     ) -> T_Dataset:\n6291         \"\"\"Convert a pandas.DataFrame into an xarray.Dataset\n6292 \n6293         Each column will be converted into an independent variable in the\n6294         Dataset. If the dataframe's index is a MultiIndex, it will be expanded\n6295         into a tensor product of one-dimensional indices (filling in missing\n6296         values with NaN). This method will produce a Dataset very similar to\n6297         that on which the 'to_dataframe' method was called, except with\n6298         possibly redundant dimensions (since all dataset variables will have\n6299         the same dimensionality)\n6300 \n6301         Parameters\n6302         ----------\n6303         dataframe : DataFrame\n6304             DataFrame from which to copy data and indices.\n6305         sparse : bool, default: False\n6306             If true, create a sparse arrays instead of dense numpy arrays. This\n6307             can potentially save a large amount of memory if the DataFrame has\n6308             a MultiIndex. Requires the sparse package (sparse.pydata.org).\n6309 \n6310         Returns\n6311         -------\n6312         New Dataset.\n6313 \n6314         See Also\n6315         --------\n6316         xarray.DataArray.from_series\n6317         pandas.DataFrame.to_xarray\n6318         \"\"\"\n6319         # TODO: Add an option to remove dimensions along which the variables\n6320         # are constant, to enable consistent serialization to/from a dataframe,\n6321         # even if some variables have different dimensionality.\n6322 \n6323         if not dataframe.columns.is_unique:\n6324             raise ValueError(\"cannot convert DataFrame with non-unique columns\")\n6325 \n6326         idx = remove_unused_levels_categories(dataframe.index)\n6327 \n6328         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:\n6329             raise ValueError(\n6330                 \"cannot convert a DataFrame with a non-unique MultiIndex into xarray\"\n6331             )\n6332 \n6333         # Cast to a NumPy array first, in case the Series is a pandas Extension\n6334         # array (which doesn't have a valid NumPy dtype)\n6335         # TODO: allow users to control how this casting happens, e.g., by\n6336         # forwarding arguments to pandas.Series.to_numpy?\n6337         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]\n6338 \n6339         indexes: dict[Hashable, Index] = {}\n6340         index_vars: dict[Hashable, Variable] = {}\n6341 \n6342         if isinstance(idx, pd.MultiIndex):\n6343             dims = tuple(\n6344                 name if name is not None else \"level_%i\" % n\n6345                 for n, name in enumerate(idx.names)\n6346             )\n6347             for dim, lev in zip(dims, idx.levels):\n6348                 xr_idx = PandasIndex(lev, dim)\n6349                 indexes[dim] = xr_idx\n6350                 index_vars.update(xr_idx.create_variables())\n6351         else:\n6352             index_name = idx.name if idx.name is not None else \"index\"\n6353             dims = (index_name,)\n6354             xr_idx = PandasIndex(idx, index_name)\n6355             indexes[index_name] = xr_idx\n6356             index_vars.update(xr_idx.create_variables())\n6357 \n6358         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)\n6359 \n6360         if sparse:\n6361             obj._set_sparse_data_from_dataframe(idx, arrays, dims)\n6362         else:\n6363             obj._set_numpy_data_from_dataframe(idx, arrays, dims)\n6364         return obj\n6365 \n6366     def to_dask_dataframe(\n6367         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False\n6368     ) -> DaskDataFrame:\n6369         \"\"\"\n6370         Convert this dataset into a dask.dataframe.DataFrame.\n6371 \n6372         The dimensions, coordinates and data variables in this dataset form\n6373         the columns of the DataFrame.\n6374 \n6375         Parameters\n6376         ----------\n6377         dim_order : list, optional\n6378             Hierarchical dimension order for the resulting dataframe. All\n6379             arrays are transposed to this order and then written out as flat\n6380             vectors in contiguous order, so the last dimension in this list\n6381             will be contiguous in the resulting DataFrame. This has a major\n6382             influence on which operations are efficient on the resulting dask\n6383             dataframe.\n6384 \n6385             If provided, must include all dimensions of this dataset. By\n6386             default, dimensions are sorted alphabetically.\n6387         set_index : bool, default: False\n6388             If set_index=True, the dask DataFrame is indexed by this dataset's\n6389             coordinate. Since dask DataFrames do not support multi-indexes,\n6390             set_index only works if the dataset only contains one dimension.\n6391 \n6392         Returns\n6393         -------\n6394         dask.dataframe.DataFrame\n6395         \"\"\"\n6396 \n6397         import dask.array as da\n6398         import dask.dataframe as dd\n6399 \n6400         ordered_dims = self._normalize_dim_order(dim_order=dim_order)\n6401 \n6402         columns = list(ordered_dims)\n6403         columns.extend(k for k in self.coords if k not in self.dims)\n6404         columns.extend(self.data_vars)\n6405 \n6406         series_list = []\n6407         for name in columns:\n6408             try:\n6409                 var = self.variables[name]\n6410             except KeyError:\n6411                 # dimension without a matching coordinate\n6412                 size = self.dims[name]\n6413                 data = da.arange(size, chunks=size, dtype=np.int64)\n6414                 var = Variable((name,), data)\n6415 \n6416             # IndexVariable objects have a dummy .chunk() method\n6417             if isinstance(var, IndexVariable):\n6418                 var = var.to_base_variable()\n6419 \n6420             # Make sure var is a dask array, otherwise the array can become too large\n6421             # when it is broadcasted to several dimensions:\n6422             if not is_duck_dask_array(var._data):\n6423                 var = var.chunk()\n6424 \n6425             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data\n6426             series = dd.from_array(dask_array.reshape(-1), columns=[name])\n6427             series_list.append(series)\n6428 \n6429         df = dd.concat(series_list, axis=1)\n6430 \n6431         if set_index:\n6432             dim_order = [*ordered_dims]\n6433 \n6434             if len(dim_order) == 1:\n6435                 (dim,) = dim_order\n6436                 df = df.set_index(dim)\n6437             else:\n6438                 # triggers an error about multi-indexes, even if only one\n6439                 # dimension is passed\n6440                 df = df.set_index(dim_order)\n6441 \n6442         return df\n6443 \n6444     def to_dict(\n6445         self, data: bool | Literal[\"list\", \"array\"] = \"list\", encoding: bool = False\n6446     ) -> dict[str, Any]:\n6447         \"\"\"\n6448         Convert this dataset to a dictionary following xarray naming\n6449         conventions.\n6450 \n6451         Converts all variables and attributes to native Python objects\n6452         Useful for converting to json. To avoid datetime incompatibility\n6453         use decode_times=False kwarg in xarrray.open_dataset.\n6454 \n6455         Parameters\n6456         ----------\n6457         data : bool or {\"list\", \"array\"}, default: \"list\"\n6458             Whether to include the actual data in the dictionary. When set to\n6459             False, returns just the schema. If set to \"array\", returns data as\n6460             underlying array type. If set to \"list\" (or True for backwards\n6461             compatibility), returns data in lists of Python data types. Note\n6462             that for obtaining the \"list\" output efficiently, use\n6463             `ds.compute().to_dict(data=\"list\")`.\n6464 \n6465         encoding : bool, default: False\n6466             Whether to include the Dataset's encoding in the dictionary.\n6467 \n6468         Returns\n6469         -------\n6470         d : dict\n6471             Dict with keys: \"coords\", \"attrs\", \"dims\", \"data_vars\" and optionally\n6472             \"encoding\".\n6473 \n6474         See Also\n6475         --------\n6476         Dataset.from_dict\n6477         DataArray.to_dict\n6478         \"\"\"\n6479         d: dict = {\n6480             \"coords\": {},\n6481             \"attrs\": decode_numpy_dict_values(self.attrs),\n6482             \"dims\": dict(self.dims),\n6483             \"data_vars\": {},\n6484         }\n6485         for k in self.coords:\n6486             d[\"coords\"].update(\n6487                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}\n6488             )\n6489         for k in self.data_vars:\n6490             d[\"data_vars\"].update(\n6491                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}\n6492             )\n6493         if encoding:\n6494             d[\"encoding\"] = dict(self.encoding)\n6495         return d\n6496 \n6497     @classmethod\n6498     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:\n6499         \"\"\"Convert a dictionary into an xarray.Dataset.\n6500 \n6501         Parameters\n6502         ----------\n6503         d : dict-like\n6504             Mapping with a minimum structure of\n6505                 ``{\"var_0\": {\"dims\": [..], \"data\": [..]}, \\\n6506                             ...}``\n6507 \n6508         Returns\n6509         -------\n6510         obj : Dataset\n6511 \n6512         See also\n6513         --------\n6514         Dataset.to_dict\n6515         DataArray.from_dict\n6516 \n6517         Examples\n6518         --------\n6519         >>> d = {\n6520         ...     \"t\": {\"dims\": (\"t\"), \"data\": [0, 1, 2]},\n6521         ...     \"a\": {\"dims\": (\"t\"), \"data\": [\"a\", \"b\", \"c\"]},\n6522         ...     \"b\": {\"dims\": (\"t\"), \"data\": [10, 20, 30]},\n6523         ... }\n6524         >>> ds = xr.Dataset.from_dict(d)\n6525         >>> ds\n6526         <xarray.Dataset>\n6527         Dimensions:  (t: 3)\n6528         Coordinates:\n6529           * t        (t) int64 0 1 2\n6530         Data variables:\n6531             a        (t) <U1 'a' 'b' 'c'\n6532             b        (t) int64 10 20 30\n6533 \n6534         >>> d = {\n6535         ...     \"coords\": {\n6536         ...         \"t\": {\"dims\": \"t\", \"data\": [0, 1, 2], \"attrs\": {\"units\": \"s\"}}\n6537         ...     },\n6538         ...     \"attrs\": {\"title\": \"air temperature\"},\n6539         ...     \"dims\": \"t\",\n6540         ...     \"data_vars\": {\n6541         ...         \"a\": {\"dims\": \"t\", \"data\": [10, 20, 30]},\n6542         ...         \"b\": {\"dims\": \"t\", \"data\": [\"a\", \"b\", \"c\"]},\n6543         ...     },\n6544         ... }\n6545         >>> ds = xr.Dataset.from_dict(d)\n6546         >>> ds\n6547         <xarray.Dataset>\n6548         Dimensions:  (t: 3)\n6549         Coordinates:\n6550           * t        (t) int64 0 1 2\n6551         Data variables:\n6552             a        (t) int64 10 20 30\n6553             b        (t) <U1 'a' 'b' 'c'\n6554         Attributes:\n6555             title:    air temperature\n6556 \n6557         \"\"\"\n6558 \n6559         variables: Iterable[tuple[Hashable, Any]]\n6560         if not {\"coords\", \"data_vars\"}.issubset(set(d)):\n6561             variables = d.items()\n6562         else:\n6563             import itertools\n6564 \n6565             variables = itertools.chain(\n6566                 d.get(\"coords\", {}).items(), d.get(\"data_vars\", {}).items()\n6567             )\n6568         try:\n6569             variable_dict = {\n6570                 k: (v[\"dims\"], v[\"data\"], v.get(\"attrs\"), v.get(\"encoding\"))\n6571                 for k, v in variables\n6572             }\n6573         except KeyError as e:\n6574             raise ValueError(\n6575                 \"cannot convert dict without the key \"\n6576                 \"'{dims_data}'\".format(dims_data=str(e.args[0]))\n6577             )\n6578         obj = cls(variable_dict)\n6579 \n6580         # what if coords aren't dims?\n6581         coords = set(d.get(\"coords\", {})) - set(d.get(\"dims\", {}))\n6582         obj = obj.set_coords(coords)\n6583 \n6584         obj.attrs.update(d.get(\"attrs\", {}))\n6585         obj.encoding.update(d.get(\"encoding\", {}))\n6586 \n6587         return obj\n6588 \n6589     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:\n6590         variables = {}\n6591         keep_attrs = kwargs.pop(\"keep_attrs\", None)\n6592         if keep_attrs is None:\n6593             keep_attrs = _get_keep_attrs(default=True)\n6594         for k, v in self._variables.items():\n6595             if k in self._coord_names:\n6596                 variables[k] = v\n6597             else:\n6598                 variables[k] = f(v, *args, **kwargs)\n6599                 if keep_attrs:\n6600                     variables[k].attrs = v._attrs\n6601         attrs = self._attrs if keep_attrs else None\n6602         return self._replace_with_new_dims(variables, attrs=attrs)\n6603 \n6604     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:\n6605         from xarray.core.dataarray import DataArray\n6606         from xarray.core.groupby import GroupBy\n6607 \n6608         if isinstance(other, GroupBy):\n6609             return NotImplemented\n6610         align_type = OPTIONS[\"arithmetic_join\"] if join is None else join\n6611         if isinstance(other, (DataArray, Dataset)):\n6612             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]\n6613         g = f if not reflexive else lambda x, y: f(y, x)\n6614         ds = self._calculate_binary_op(g, other, join=align_type)\n6615         keep_attrs = _get_keep_attrs(default=False)\n6616         if keep_attrs:\n6617             ds.attrs = self.attrs\n6618         return ds\n6619 \n6620     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:\n6621         from xarray.core.dataarray import DataArray\n6622         from xarray.core.groupby import GroupBy\n6623 \n6624         if isinstance(other, GroupBy):\n6625             raise TypeError(\n6626                 \"in-place operations between a Dataset and \"\n6627                 \"a grouped object are not permitted\"\n6628             )\n6629         # we don't actually modify arrays in-place with in-place Dataset\n6630         # arithmetic -- this lets us automatically align things\n6631         if isinstance(other, (DataArray, Dataset)):\n6632             other = other.reindex_like(self, copy=False)\n6633         g = ops.inplace_to_noninplace_op(f)\n6634         ds = self._calculate_binary_op(g, other, inplace=True)\n6635         self._replace_with_new_dims(\n6636             ds._variables,\n6637             ds._coord_names,\n6638             attrs=ds._attrs,\n6639             indexes=ds._indexes,\n6640             inplace=True,\n6641         )\n6642         return self\n6643 \n6644     def _calculate_binary_op(\n6645         self, f, other, join=\"inner\", inplace: bool = False\n6646     ) -> Dataset:\n6647         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):\n6648             if inplace and set(lhs_data_vars) != set(rhs_data_vars):\n6649                 raise ValueError(\n6650                     \"datasets must have the same data variables \"\n6651                     f\"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}\"\n6652                 )\n6653 \n6654             dest_vars = {}\n6655 \n6656             for k in lhs_data_vars:\n6657                 if k in rhs_data_vars:\n6658                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])\n6659                 elif join in [\"left\", \"outer\"]:\n6660                     dest_vars[k] = f(lhs_vars[k], np.nan)\n6661             for k in rhs_data_vars:\n6662                 if k not in dest_vars and join in [\"right\", \"outer\"]:\n6663                     dest_vars[k] = f(rhs_vars[k], np.nan)\n6664             return dest_vars\n6665 \n6666         if utils.is_dict_like(other) and not isinstance(other, Dataset):\n6667             # can't use our shortcut of doing the binary operation with\n6668             # Variable objects, so apply over our data vars instead.\n6669             new_data_vars = apply_over_both(\n6670                 self.data_vars, other, self.data_vars, other\n6671             )\n6672             return type(self)(new_data_vars)\n6673 \n6674         other_coords: Coordinates | None = getattr(other, \"coords\", None)\n6675         ds = self.coords.merge(other_coords)\n6676 \n6677         if isinstance(other, Dataset):\n6678             new_vars = apply_over_both(\n6679                 self.data_vars, other.data_vars, self.variables, other.variables\n6680             )\n6681         else:\n6682             other_variable = getattr(other, \"variable\", other)\n6683             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}\n6684         ds._variables.update(new_vars)\n6685         ds._dims = calculate_dimensions(ds._variables)\n6686         return ds\n6687 \n6688     def _copy_attrs_from(self, other):\n6689         self.attrs = other.attrs\n6690         for v in other.variables:\n6691             if v in self.variables:\n6692                 self.variables[v].attrs = other.variables[v].attrs\n6693 \n6694     def diff(\n6695         self: T_Dataset,\n6696         dim: Hashable,\n6697         n: int = 1,\n6698         label: Literal[\"upper\", \"lower\"] = \"upper\",\n6699     ) -> T_Dataset:\n6700         \"\"\"Calculate the n-th order discrete difference along given axis.\n6701 \n6702         Parameters\n6703         ----------\n6704         dim : Hashable\n6705             Dimension over which to calculate the finite difference.\n6706         n : int, default: 1\n6707             The number of times values are differenced.\n6708         label : {\"upper\", \"lower\"}, default: \"upper\"\n6709             The new coordinate in dimension ``dim`` will have the\n6710             values of either the minuend's or subtrahend's coordinate\n6711             for values 'upper' and 'lower', respectively.\n6712 \n6713         Returns\n6714         -------\n6715         difference : Dataset\n6716             The n-th order finite difference of this object.\n6717 \n6718         Notes\n6719         -----\n6720         `n` matches numpy's behavior and is different from pandas' first argument named\n6721         `periods`.\n6722 \n6723         Examples\n6724         --------\n6725         >>> ds = xr.Dataset({\"foo\": (\"x\", [5, 5, 6, 6])})\n6726         >>> ds.diff(\"x\")\n6727         <xarray.Dataset>\n6728         Dimensions:  (x: 3)\n6729         Dimensions without coordinates: x\n6730         Data variables:\n6731             foo      (x) int64 0 1 0\n6732         >>> ds.diff(\"x\", 2)\n6733         <xarray.Dataset>\n6734         Dimensions:  (x: 2)\n6735         Dimensions without coordinates: x\n6736         Data variables:\n6737             foo      (x) int64 1 -1\n6738 \n6739         See Also\n6740         --------\n6741         Dataset.differentiate\n6742         \"\"\"\n6743         if n == 0:\n6744             return self\n6745         if n < 0:\n6746             raise ValueError(f\"order `n` must be non-negative but got {n}\")\n6747 \n6748         # prepare slices\n6749         slice_start = {dim: slice(None, -1)}\n6750         slice_end = {dim: slice(1, None)}\n6751 \n6752         # prepare new coordinate\n6753         if label == \"upper\":\n6754             slice_new = slice_end\n6755         elif label == \"lower\":\n6756             slice_new = slice_start\n6757         else:\n6758             raise ValueError(\"The 'label' argument has to be either 'upper' or 'lower'\")\n6759 \n6760         indexes, index_vars = isel_indexes(self.xindexes, slice_new)\n6761         variables = {}\n6762 \n6763         for name, var in self.variables.items():\n6764             if name in index_vars:\n6765                 variables[name] = index_vars[name]\n6766             elif dim in var.dims:\n6767                 if name in self.data_vars:\n6768                     variables[name] = var.isel(slice_end) - var.isel(slice_start)\n6769                 else:\n6770                     variables[name] = var.isel(slice_new)\n6771             else:\n6772                 variables[name] = var\n6773 \n6774         difference = self._replace_with_new_dims(variables, indexes=indexes)\n6775 \n6776         if n > 1:\n6777             return difference.diff(dim, n - 1)\n6778         else:\n6779             return difference\n6780 \n6781     def shift(\n6782         self: T_Dataset,\n6783         shifts: Mapping[Any, int] | None = None,\n6784         fill_value: Any = xrdtypes.NA,\n6785         **shifts_kwargs: int,\n6786     ) -> T_Dataset:\n6787         \"\"\"Shift this dataset by an offset along one or more dimensions.\n6788 \n6789         Only data variables are moved; coordinates stay in place. This is\n6790         consistent with the behavior of ``shift`` in pandas.\n6791 \n6792         Values shifted from beyond array bounds will appear at one end of\n6793         each dimension, which are filled according to `fill_value`. For periodic\n6794         offsets instead see `roll`.\n6795 \n6796         Parameters\n6797         ----------\n6798         shifts : mapping of hashable to int\n6799             Integer offset to shift along each of the given dimensions.\n6800             Positive offsets shift to the right; negative offsets shift to the\n6801             left.\n6802         fill_value : scalar or dict-like, optional\n6803             Value to use for newly missing values. If a dict-like, maps\n6804             variable names (including coordinates) to fill values.\n6805         **shifts_kwargs\n6806             The keyword arguments form of ``shifts``.\n6807             One of shifts or shifts_kwargs must be provided.\n6808 \n6809         Returns\n6810         -------\n6811         shifted : Dataset\n6812             Dataset with the same coordinates and attributes but shifted data\n6813             variables.\n6814 \n6815         See Also\n6816         --------\n6817         roll\n6818 \n6819         Examples\n6820         --------\n6821         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))})\n6822         >>> ds.shift(x=2)\n6823         <xarray.Dataset>\n6824         Dimensions:  (x: 5)\n6825         Dimensions without coordinates: x\n6826         Data variables:\n6827             foo      (x) object nan nan 'a' 'b' 'c'\n6828         \"\"\"\n6829         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n6830         invalid = [k for k in shifts if k not in self.dims]\n6831         if invalid:\n6832             raise ValueError(f\"dimensions {invalid!r} do not exist\")\n6833 \n6834         variables = {}\n6835         for name, var in self.variables.items():\n6836             if name in self.data_vars:\n6837                 fill_value_ = (\n6838                     fill_value.get(name, xrdtypes.NA)\n6839                     if isinstance(fill_value, dict)\n6840                     else fill_value\n6841                 )\n6842 \n6843                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}\n6844                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)\n6845             else:\n6846                 variables[name] = var\n6847 \n6848         return self._replace(variables)\n6849 \n6850     def roll(\n6851         self: T_Dataset,\n6852         shifts: Mapping[Any, int] | None = None,\n6853         roll_coords: bool = False,\n6854         **shifts_kwargs: int,\n6855     ) -> T_Dataset:\n6856         \"\"\"Roll this dataset by an offset along one or more dimensions.\n6857 \n6858         Unlike shift, roll treats the given dimensions as periodic, so will not\n6859         create any missing values to be filled.\n6860 \n6861         Also unlike shift, roll may rotate all variables, including coordinates\n6862         if specified. The direction of rotation is consistent with\n6863         :py:func:`numpy.roll`.\n6864 \n6865         Parameters\n6866         ----------\n6867         shifts : mapping of hashable to int, optional\n6868             A dict with keys matching dimensions and values given\n6869             by integers to rotate each of the given dimensions. Positive\n6870             offsets roll to the right; negative offsets roll to the left.\n6871         roll_coords : bool, default: False\n6872             Indicates whether to roll the coordinates by the offset too.\n6873         **shifts_kwargs : {dim: offset, ...}, optional\n6874             The keyword arguments form of ``shifts``.\n6875             One of shifts or shifts_kwargs must be provided.\n6876 \n6877         Returns\n6878         -------\n6879         rolled : Dataset\n6880             Dataset with the same attributes but rolled data and coordinates.\n6881 \n6882         See Also\n6883         --------\n6884         shift\n6885 \n6886         Examples\n6887         --------\n6888         >>> ds = xr.Dataset({\"foo\": (\"x\", list(\"abcde\"))}, coords={\"x\": np.arange(5)})\n6889         >>> ds.roll(x=2)\n6890         <xarray.Dataset>\n6891         Dimensions:  (x: 5)\n6892         Coordinates:\n6893           * x        (x) int64 0 1 2 3 4\n6894         Data variables:\n6895             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n6896 \n6897         >>> ds.roll(x=2, roll_coords=True)\n6898         <xarray.Dataset>\n6899         Dimensions:  (x: 5)\n6900         Coordinates:\n6901           * x        (x) int64 3 4 0 1 2\n6902         Data variables:\n6903             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'\n6904 \n6905         \"\"\"\n6906         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n6907         invalid = [k for k in shifts if k not in self.dims]\n6908         if invalid:\n6909             raise ValueError(f\"dimensions {invalid!r} do not exist\")\n6910 \n6911         unrolled_vars: tuple[Hashable, ...]\n6912 \n6913         if roll_coords:\n6914             indexes, index_vars = roll_indexes(self.xindexes, shifts)\n6915             unrolled_vars = ()\n6916         else:\n6917             indexes = dict(self._indexes)\n6918             index_vars = dict(self.xindexes.variables)\n6919             unrolled_vars = tuple(self.coords)\n6920 \n6921         variables = {}\n6922         for k, var in self.variables.items():\n6923             if k in index_vars:\n6924                 variables[k] = index_vars[k]\n6925             elif k not in unrolled_vars:\n6926                 variables[k] = var.roll(\n6927                     shifts={k: s for k, s in shifts.items() if k in var.dims}\n6928                 )\n6929             else:\n6930                 variables[k] = var\n6931 \n6932         return self._replace(variables, indexes=indexes)\n6933 \n6934     def sortby(\n6935         self: T_Dataset,\n6936         variables: Hashable | DataArray | list[Hashable | DataArray],\n6937         ascending: bool = True,\n6938     ) -> T_Dataset:\n6939         \"\"\"\n6940         Sort object by labels or values (along an axis).\n6941 \n6942         Sorts the dataset, either along specified dimensions,\n6943         or according to values of 1-D dataarrays that share dimension\n6944         with calling object.\n6945 \n6946         If the input variables are dataarrays, then the dataarrays are aligned\n6947         (via left-join) to the calling object prior to sorting by cell values.\n6948         NaNs are sorted to the end, following Numpy convention.\n6949 \n6950         If multiple sorts along the same dimension is\n6951         given, numpy's lexsort is performed along that dimension:\n6952         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html\n6953         and the FIRST key in the sequence is used as the primary sort key,\n6954         followed by the 2nd key, etc.\n6955 \n6956         Parameters\n6957         ----------\n6958         variables : Hashable, DataArray, or list of hashable or DataArray\n6959             1D DataArray objects or name(s) of 1D variable(s) in\n6960             coords/data_vars whose values are used to sort the dataset.\n6961         ascending : bool, default: True\n6962             Whether to sort by ascending or descending order.\n6963 \n6964         Returns\n6965         -------\n6966         sorted : Dataset\n6967             A new dataset where all the specified dims are sorted by dim\n6968             labels.\n6969 \n6970         See Also\n6971         --------\n6972         DataArray.sortby\n6973         numpy.sort\n6974         pandas.sort_values\n6975         pandas.sort_index\n6976 \n6977         Examples\n6978         --------\n6979         >>> ds = xr.Dataset(\n6980         ...     {\n6981         ...         \"A\": ((\"x\", \"y\"), [[1, 2], [3, 4]]),\n6982         ...         \"B\": ((\"x\", \"y\"), [[5, 6], [7, 8]]),\n6983         ...     },\n6984         ...     coords={\"x\": [\"b\", \"a\"], \"y\": [1, 0]},\n6985         ... )\n6986         >>> ds = ds.sortby(\"x\")\n6987         >>> ds\n6988         <xarray.Dataset>\n6989         Dimensions:  (x: 2, y: 2)\n6990         Coordinates:\n6991           * x        (x) <U1 'a' 'b'\n6992           * y        (y) int64 1 0\n6993         Data variables:\n6994             A        (x, y) int64 3 4 1 2\n6995             B        (x, y) int64 7 8 5 6\n6996         \"\"\"\n6997         from xarray.core.dataarray import DataArray\n6998 \n6999         if not isinstance(variables, list):\n7000             variables = [variables]\n7001         else:\n7002             variables = variables\n7003         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]\n7004         aligned_vars = align(self, *arrays, join=\"left\")  # type: ignore[type-var]\n7005         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]\n7006         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]\n7007         vars_by_dim = defaultdict(list)\n7008         for data_array in aligned_other_vars:\n7009             if data_array.ndim != 1:\n7010                 raise ValueError(\"Input DataArray is not 1-D.\")\n7011             (key,) = data_array.dims\n7012             vars_by_dim[key].append(data_array)\n7013 \n7014         indices = {}\n7015         for key, arrays in vars_by_dim.items():\n7016             order = np.lexsort(tuple(reversed(arrays)))\n7017             indices[key] = order if ascending else order[::-1]\n7018         return aligned_self.isel(indices)\n7019 \n7020     def quantile(\n7021         self: T_Dataset,\n7022         q: ArrayLike,\n7023         dim: Dims = None,\n7024         method: QuantileMethods = \"linear\",\n7025         numeric_only: bool = False,\n7026         keep_attrs: bool | None = None,\n7027         skipna: bool | None = None,\n7028         interpolation: QuantileMethods | None = None,\n7029     ) -> T_Dataset:\n7030         \"\"\"Compute the qth quantile of the data along the specified dimension.\n7031 \n7032         Returns the qth quantiles(s) of the array elements for each variable\n7033         in the Dataset.\n7034 \n7035         Parameters\n7036         ----------\n7037         q : float or array-like of float\n7038             Quantile to compute, which must be between 0 and 1 inclusive.\n7039         dim : str or Iterable of Hashable, optional\n7040             Dimension(s) over which to apply quantile.\n7041         method : str, default: \"linear\"\n7042             This optional parameter specifies the interpolation method to use when the\n7043             desired quantile lies between two data points. The options sorted by their R\n7044             type as summarized in the H&F paper [1]_ are:\n7045 \n7046                 1. \"inverted_cdf\" (*)\n7047                 2. \"averaged_inverted_cdf\" (*)\n7048                 3. \"closest_observation\" (*)\n7049                 4. \"interpolated_inverted_cdf\" (*)\n7050                 5. \"hazen\" (*)\n7051                 6. \"weibull\" (*)\n7052                 7. \"linear\"  (default)\n7053                 8. \"median_unbiased\" (*)\n7054                 9. \"normal_unbiased\" (*)\n7055 \n7056             The first three methods are discontiuous.  The following discontinuous\n7057             variations of the default \"linear\" (7.) option are also available:\n7058 \n7059                 * \"lower\"\n7060                 * \"higher\"\n7061                 * \"midpoint\"\n7062                 * \"nearest\"\n7063 \n7064             See :py:func:`numpy.quantile` or [1]_ for details. The \"method\" argument\n7065             was previously called \"interpolation\", renamed in accordance with numpy\n7066             version 1.22.0.\n7067 \n7068             (*) These methods require numpy version 1.22 or newer.\n7069 \n7070         keep_attrs : bool, optional\n7071             If True, the dataset's attributes (`attrs`) will be copied from\n7072             the original object to the new one.  If False (default), the new\n7073             object will be returned without attributes.\n7074         numeric_only : bool, optional\n7075             If True, only apply ``func`` to variables with a numeric dtype.\n7076         skipna : bool, optional\n7077             If True, skip missing values (as marked by NaN). By default, only\n7078             skips missing values for float dtypes; other dtypes either do not\n7079             have a sentinel missing value (int) or skipna=True has not been\n7080             implemented (object, datetime64 or timedelta64).\n7081 \n7082         Returns\n7083         -------\n7084         quantiles : Dataset\n7085             If `q` is a single quantile, then the result is a scalar for each\n7086             variable in data_vars. If multiple percentiles are given, first\n7087             axis of the result corresponds to the quantile and a quantile\n7088             dimension is added to the return Dataset. The other dimensions are\n7089             the dimensions that remain after the reduction of the array.\n7090 \n7091         See Also\n7092         --------\n7093         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile\n7094 \n7095         Examples\n7096         --------\n7097         >>> ds = xr.Dataset(\n7098         ...     {\"a\": ((\"x\", \"y\"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},\n7099         ...     coords={\"x\": [7, 9], \"y\": [1, 1.5, 2, 2.5]},\n7100         ... )\n7101         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)\n7102         <xarray.Dataset>\n7103         Dimensions:   ()\n7104         Coordinates:\n7105             quantile  float64 0.0\n7106         Data variables:\n7107             a         float64 0.7\n7108         >>> ds.quantile(0, dim=\"x\")\n7109         <xarray.Dataset>\n7110         Dimensions:   (y: 4)\n7111         Coordinates:\n7112           * y         (y) float64 1.0 1.5 2.0 2.5\n7113             quantile  float64 0.0\n7114         Data variables:\n7115             a         (y) float64 0.7 4.2 2.6 1.5\n7116         >>> ds.quantile([0, 0.5, 1])\n7117         <xarray.Dataset>\n7118         Dimensions:   (quantile: 3)\n7119         Coordinates:\n7120           * quantile  (quantile) float64 0.0 0.5 1.0\n7121         Data variables:\n7122             a         (quantile) float64 0.7 3.4 9.4\n7123         >>> ds.quantile([0, 0.5, 1], dim=\"x\")\n7124         <xarray.Dataset>\n7125         Dimensions:   (quantile: 3, y: 4)\n7126         Coordinates:\n7127           * y         (y) float64 1.0 1.5 2.0 2.5\n7128           * quantile  (quantile) float64 0.0 0.5 1.0\n7129         Data variables:\n7130             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9\n7131 \n7132         References\n7133         ----------\n7134         .. [1] R. J. Hyndman and Y. Fan,\n7135            \"Sample quantiles in statistical packages,\"\n7136            The American Statistician, 50(4), pp. 361-365, 1996\n7137         \"\"\"\n7138 \n7139         # interpolation renamed to method in version 0.21.0\n7140         # check here and in variable to avoid repeated warnings\n7141         if interpolation is not None:\n7142             warnings.warn(\n7143                 \"The `interpolation` argument to quantile was renamed to `method`.\",\n7144                 FutureWarning,\n7145             )\n7146 \n7147             if method != \"linear\":\n7148                 raise TypeError(\"Cannot pass interpolation and method keywords!\")\n7149 \n7150             method = interpolation\n7151 \n7152         dims: set[Hashable]\n7153         if isinstance(dim, str):\n7154             dims = {dim}\n7155         elif dim is None or dim is ...:\n7156             dims = set(self.dims)\n7157         else:\n7158             dims = set(dim)\n7159 \n7160         _assert_empty(\n7161             tuple(d for d in dims if d not in self.dims),\n7162             \"Dataset does not contain the dimensions: %s\",\n7163         )\n7164 \n7165         q = np.asarray(q, dtype=np.float64)\n7166 \n7167         variables = {}\n7168         for name, var in self.variables.items():\n7169             reduce_dims = [d for d in var.dims if d in dims]\n7170             if reduce_dims or not var.dims:\n7171                 if name not in self.coords:\n7172                     if (\n7173                         not numeric_only\n7174                         or np.issubdtype(var.dtype, np.number)\n7175                         or var.dtype == np.bool_\n7176                     ):\n7177                         variables[name] = var.quantile(\n7178                             q,\n7179                             dim=reduce_dims,\n7180                             method=method,\n7181                             keep_attrs=keep_attrs,\n7182                             skipna=skipna,\n7183                         )\n7184 \n7185             else:\n7186                 variables[name] = var\n7187 \n7188         # construct the new dataset\n7189         coord_names = {k for k in self.coords if k in variables}\n7190         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n7191         if keep_attrs is None:\n7192             keep_attrs = _get_keep_attrs(default=False)\n7193         attrs = self.attrs if keep_attrs else None\n7194         new = self._replace_with_new_dims(\n7195             variables, coord_names=coord_names, attrs=attrs, indexes=indexes\n7196         )\n7197         return new.assign_coords(quantile=q)\n7198 \n7199     def rank(\n7200         self: T_Dataset,\n7201         dim: Hashable,\n7202         pct: bool = False,\n7203         keep_attrs: bool | None = None,\n7204     ) -> T_Dataset:\n7205         \"\"\"Ranks the data.\n7206 \n7207         Equal values are assigned a rank that is the average of the ranks that\n7208         would have been otherwise assigned to all of the values within\n7209         that set.\n7210         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.\n7211 \n7212         NaNs in the input array are returned as NaNs.\n7213 \n7214         The `bottleneck` library is required.\n7215 \n7216         Parameters\n7217         ----------\n7218         dim : Hashable\n7219             Dimension over which to compute rank.\n7220         pct : bool, default: False\n7221             If True, compute percentage ranks, otherwise compute integer ranks.\n7222         keep_attrs : bool or None, optional\n7223             If True, the dataset's attributes (`attrs`) will be copied from\n7224             the original object to the new one.  If False, the new\n7225             object will be returned without attributes.\n7226 \n7227         Returns\n7228         -------\n7229         ranked : Dataset\n7230             Variables that do not depend on `dim` are dropped.\n7231         \"\"\"\n7232         if not OPTIONS[\"use_bottleneck\"]:\n7233             raise RuntimeError(\n7234                 \"rank requires bottleneck to be enabled.\"\n7235                 \" Call `xr.set_options(use_bottleneck=True)` to enable it.\"\n7236             )\n7237 \n7238         if dim not in self.dims:\n7239             raise ValueError(f\"Dataset does not contain the dimension: {dim}\")\n7240 \n7241         variables = {}\n7242         for name, var in self.variables.items():\n7243             if name in self.data_vars:\n7244                 if dim in var.dims:\n7245                     variables[name] = var.rank(dim, pct=pct)\n7246             else:\n7247                 variables[name] = var\n7248 \n7249         coord_names = set(self.coords)\n7250         if keep_attrs is None:\n7251             keep_attrs = _get_keep_attrs(default=False)\n7252         attrs = self.attrs if keep_attrs else None\n7253         return self._replace(variables, coord_names, attrs=attrs)\n7254 \n7255     def differentiate(\n7256         self: T_Dataset,\n7257         coord: Hashable,\n7258         edge_order: Literal[1, 2] = 1,\n7259         datetime_unit: DatetimeUnitOptions | None = None,\n7260     ) -> T_Dataset:\n7261         \"\"\" Differentiate with the second order accurate central\n7262         differences.\n7263 \n7264         .. note::\n7265             This feature is limited to simple cartesian geometry, i.e. coord\n7266             must be one dimensional.\n7267 \n7268         Parameters\n7269         ----------\n7270         coord : Hashable\n7271             The coordinate to be used to compute the gradient.\n7272         edge_order : {1, 2}, default: 1\n7273             N-th order accurate differences at the boundaries.\n7274         datetime_unit : None or {\"Y\", \"M\", \"W\", \"D\", \"h\", \"m\", \"s\", \"ms\", \\\n7275             \"us\", \"ns\", \"ps\", \"fs\", \"as\", None}, default: None\n7276             Unit to compute gradient. Only valid for datetime coordinate.\n7277 \n7278         Returns\n7279         -------\n7280         differentiated: Dataset\n7281 \n7282         See also\n7283         --------\n7284         numpy.gradient: corresponding numpy function\n7285         \"\"\"\n7286         from xarray.core.variable import Variable\n7287 \n7288         if coord not in self.variables and coord not in self.dims:\n7289             raise ValueError(f\"Coordinate {coord} does not exist.\")\n7290 \n7291         coord_var = self[coord].variable\n7292         if coord_var.ndim != 1:\n7293             raise ValueError(\n7294                 \"Coordinate {} must be 1 dimensional but is {}\"\n7295                 \" dimensional\".format(coord, coord_var.ndim)\n7296             )\n7297 \n7298         dim = coord_var.dims[0]\n7299         if _contains_datetime_like_objects(coord_var):\n7300             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n7301                 datetime_unit = cast(\n7302                     \"DatetimeUnitOptions\", np.datetime_data(coord_var.dtype)[0]\n7303                 )\n7304             elif datetime_unit is None:\n7305                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n7306             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)\n7307 \n7308         variables = {}\n7309         for k, v in self.variables.items():\n7310             if k in self.data_vars and dim in v.dims and k not in self.coords:\n7311                 if _contains_datetime_like_objects(v):\n7312                     v = v._to_numeric(datetime_unit=datetime_unit)\n7313                 grad = duck_array_ops.gradient(\n7314                     v.data,\n7315                     coord_var.data,\n7316                     edge_order=edge_order,\n7317                     axis=v.get_axis_num(dim),\n7318                 )\n7319                 variables[k] = Variable(v.dims, grad)\n7320             else:\n7321                 variables[k] = v\n7322         return self._replace(variables)\n7323 \n7324     def integrate(\n7325         self: T_Dataset,\n7326         coord: Hashable | Sequence[Hashable],\n7327         datetime_unit: DatetimeUnitOptions = None,\n7328     ) -> T_Dataset:\n7329         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n7330 \n7331         .. note::\n7332             This feature is limited to simple cartesian geometry, i.e. coord\n7333             must be one dimensional.\n7334 \n7335         Parameters\n7336         ----------\n7337         coord : hashable, or sequence of hashable\n7338             Coordinate(s) used for the integration.\n7339         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n7340                         'ps', 'fs', 'as', None}, optional\n7341             Specify the unit if datetime coordinate is used.\n7342 \n7343         Returns\n7344         -------\n7345         integrated : Dataset\n7346 \n7347         See also\n7348         --------\n7349         DataArray.integrate\n7350         numpy.trapz : corresponding numpy function\n7351 \n7352         Examples\n7353         --------\n7354         >>> ds = xr.Dataset(\n7355         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n7356         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n7357         ... )\n7358         >>> ds\n7359         <xarray.Dataset>\n7360         Dimensions:  (x: 4)\n7361         Coordinates:\n7362           * x        (x) int64 0 1 2 3\n7363             y        (x) int64 1 7 3 5\n7364         Data variables:\n7365             a        (x) int64 5 5 6 6\n7366             b        (x) int64 1 2 1 0\n7367         >>> ds.integrate(\"x\")\n7368         <xarray.Dataset>\n7369         Dimensions:  ()\n7370         Data variables:\n7371             a        float64 16.5\n7372             b        float64 3.5\n7373         >>> ds.integrate(\"y\")\n7374         <xarray.Dataset>\n7375         Dimensions:  ()\n7376         Data variables:\n7377             a        float64 20.0\n7378             b        float64 4.0\n7379         \"\"\"\n7380         if not isinstance(coord, (list, tuple)):\n7381             coord = (coord,)\n7382         result = self\n7383         for c in coord:\n7384             result = result._integrate_one(c, datetime_unit=datetime_unit)\n7385         return result\n7386 \n7387     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):\n7388         from xarray.core.variable import Variable\n7389 \n7390         if coord not in self.variables and coord not in self.dims:\n7391             raise ValueError(f\"Coordinate {coord} does not exist.\")\n7392 \n7393         coord_var = self[coord].variable\n7394         if coord_var.ndim != 1:\n7395             raise ValueError(\n7396                 \"Coordinate {} must be 1 dimensional but is {}\"\n7397                 \" dimensional\".format(coord, coord_var.ndim)\n7398             )\n7399 \n7400         dim = coord_var.dims[0]\n7401         if _contains_datetime_like_objects(coord_var):\n7402             if coord_var.dtype.kind in \"mM\" and datetime_unit is None:\n7403                 datetime_unit, _ = np.datetime_data(coord_var.dtype)\n7404             elif datetime_unit is None:\n7405                 datetime_unit = \"s\"  # Default to seconds for cftime objects\n7406             coord_var = coord_var._replace(\n7407                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)\n7408             )\n7409 \n7410         variables = {}\n7411         coord_names = set()\n7412         for k, v in self.variables.items():\n7413             if k in self.coords:\n7414                 if dim not in v.dims or cumulative:\n7415                     variables[k] = v\n7416                     coord_names.add(k)\n7417             else:\n7418                 if k in self.data_vars and dim in v.dims:\n7419                     if _contains_datetime_like_objects(v):\n7420                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)\n7421                     if cumulative:\n7422                         integ = duck_array_ops.cumulative_trapezoid(\n7423                             v.data, coord_var.data, axis=v.get_axis_num(dim)\n7424                         )\n7425                         v_dims = v.dims\n7426                     else:\n7427                         integ = duck_array_ops.trapz(\n7428                             v.data, coord_var.data, axis=v.get_axis_num(dim)\n7429                         )\n7430                         v_dims = list(v.dims)\n7431                         v_dims.remove(dim)\n7432                     variables[k] = Variable(v_dims, integ)\n7433                 else:\n7434                     variables[k] = v\n7435         indexes = {k: v for k, v in self._indexes.items() if k in variables}\n7436         return self._replace_with_new_dims(\n7437             variables, coord_names=coord_names, indexes=indexes\n7438         )\n7439 \n7440     def cumulative_integrate(\n7441         self: T_Dataset,\n7442         coord: Hashable | Sequence[Hashable],\n7443         datetime_unit: DatetimeUnitOptions = None,\n7444     ) -> T_Dataset:\n7445         \"\"\"Integrate along the given coordinate using the trapezoidal rule.\n7446 \n7447         .. note::\n7448             This feature is limited to simple cartesian geometry, i.e. coord\n7449             must be one dimensional.\n7450 \n7451             The first entry of the cumulative integral of each variable is always 0, in\n7452             order to keep the length of the dimension unchanged between input and\n7453             output.\n7454 \n7455         Parameters\n7456         ----------\n7457         coord : hashable, or sequence of hashable\n7458             Coordinate(s) used for the integration.\n7459         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \\\n7460                         'ps', 'fs', 'as', None}, optional\n7461             Specify the unit if datetime coordinate is used.\n7462 \n7463         Returns\n7464         -------\n7465         integrated : Dataset\n7466 \n7467         See also\n7468         --------\n7469         DataArray.cumulative_integrate\n7470         scipy.integrate.cumulative_trapezoid : corresponding scipy function\n7471 \n7472         Examples\n7473         --------\n7474         >>> ds = xr.Dataset(\n7475         ...     data_vars={\"a\": (\"x\", [5, 5, 6, 6]), \"b\": (\"x\", [1, 2, 1, 0])},\n7476         ...     coords={\"x\": [0, 1, 2, 3], \"y\": (\"x\", [1, 7, 3, 5])},\n7477         ... )\n7478         >>> ds\n7479         <xarray.Dataset>\n7480         Dimensions:  (x: 4)\n7481         Coordinates:\n7482           * x        (x) int64 0 1 2 3\n7483             y        (x) int64 1 7 3 5\n7484         Data variables:\n7485             a        (x) int64 5 5 6 6\n7486             b        (x) int64 1 2 1 0\n7487         >>> ds.cumulative_integrate(\"x\")\n7488         <xarray.Dataset>\n7489         Dimensions:  (x: 4)\n7490         Coordinates:\n7491           * x        (x) int64 0 1 2 3\n7492             y        (x) int64 1 7 3 5\n7493         Data variables:\n7494             a        (x) float64 0.0 5.0 10.5 16.5\n7495             b        (x) float64 0.0 1.5 3.0 3.5\n7496         >>> ds.cumulative_integrate(\"y\")\n7497         <xarray.Dataset>\n7498         Dimensions:  (x: 4)\n7499         Coordinates:\n7500           * x        (x) int64 0 1 2 3\n7501             y        (x) int64 1 7 3 5\n7502         Data variables:\n7503             a        (x) float64 0.0 30.0 8.0 20.0\n7504             b        (x) float64 0.0 9.0 3.0 4.0\n7505         \"\"\"\n7506         if not isinstance(coord, (list, tuple)):\n7507             coord = (coord,)\n7508         result = self\n7509         for c in coord:\n7510             result = result._integrate_one(\n7511                 c, datetime_unit=datetime_unit, cumulative=True\n7512             )\n7513         return result\n7514 \n7515     @property\n7516     def real(self: T_Dataset) -> T_Dataset:\n7517         \"\"\"\n7518         The real part of each data variable.\n7519 \n7520         See Also\n7521         --------\n7522         numpy.ndarray.real\n7523         \"\"\"\n7524         return self.map(lambda x: x.real, keep_attrs=True)\n7525 \n7526     @property\n7527     def imag(self: T_Dataset) -> T_Dataset:\n7528         \"\"\"\n7529         The imaginary part of each data variable.\n7530 \n7531         See Also\n7532         --------\n7533         numpy.ndarray.imag\n7534         \"\"\"\n7535         return self.map(lambda x: x.imag, keep_attrs=True)\n7536 \n7537     plot = utils.UncachedAccessor(DatasetPlotAccessor)\n7538 \n7539     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:\n7540         \"\"\"Returns a ``Dataset`` with variables that match specific conditions.\n7541 \n7542         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned\n7543         containing only the variables for which all the filter tests pass.\n7544         These tests are either ``key=value`` for which the attribute ``key``\n7545         has the exact value ``value`` or the callable passed into\n7546         ``key=callable`` returns True. The callable will be passed a single\n7547         value, either the value of the attribute ``key`` or ``None`` if the\n7548         DataArray does not have an attribute with the name ``key``.\n7549 \n7550         Parameters\n7551         ----------\n7552         **kwargs\n7553             key : str\n7554                 Attribute name.\n7555             value : callable or obj\n7556                 If value is a callable, it should return a boolean in the form\n7557                 of bool = func(attr) where attr is da.attrs[key].\n7558                 Otherwise, value will be compared to the each\n7559                 DataArray's attrs[key].\n7560 \n7561         Returns\n7562         -------\n7563         new : Dataset\n7564             New dataset with variables filtered by attribute.\n7565 \n7566         Examples\n7567         --------\n7568         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)\n7569         >>> precip = 10 * np.random.rand(2, 2, 3)\n7570         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n7571         >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n7572         >>> dims = [\"x\", \"y\", \"time\"]\n7573         >>> temp_attr = dict(standard_name=\"air_potential_temperature\")\n7574         >>> precip_attr = dict(standard_name=\"convective_precipitation_flux\")\n7575 \n7576         >>> ds = xr.Dataset(\n7577         ...     dict(\n7578         ...         temperature=(dims, temp, temp_attr),\n7579         ...         precipitation=(dims, precip, precip_attr),\n7580         ...     ),\n7581         ...     coords=dict(\n7582         ...         lon=([\"x\", \"y\"], lon),\n7583         ...         lat=([\"x\", \"y\"], lat),\n7584         ...         time=pd.date_range(\"2014-09-06\", periods=3),\n7585         ...         reference_time=pd.Timestamp(\"2014-09-05\"),\n7586         ...     ),\n7587         ... )\n7588 \n7589         Get variables matching a specific standard_name:\n7590 \n7591         >>> ds.filter_by_attrs(standard_name=\"convective_precipitation_flux\")\n7592         <xarray.Dataset>\n7593         Dimensions:         (x: 2, y: 2, time: 3)\n7594         Coordinates:\n7595             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n7596             lat             (x, y) float64 42.25 42.21 42.63 42.59\n7597           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n7598             reference_time  datetime64[ns] 2014-09-05\n7599         Dimensions without coordinates: x, y\n7600         Data variables:\n7601             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n7602 \n7603         Get all variables that have a standard_name attribute:\n7604 \n7605         >>> standard_name = lambda v: v is not None\n7606         >>> ds.filter_by_attrs(standard_name=standard_name)\n7607         <xarray.Dataset>\n7608         Dimensions:         (x: 2, y: 2, time: 3)\n7609         Coordinates:\n7610             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n7611             lat             (x, y) float64 42.25 42.21 42.63 42.59\n7612           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n7613             reference_time  datetime64[ns] 2014-09-05\n7614         Dimensions without coordinates: x, y\n7615         Data variables:\n7616             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n7617             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n7618 \n7619         \"\"\"\n7620         selection = []\n7621         for var_name, variable in self.variables.items():\n7622             has_value_flag = False\n7623             for attr_name, pattern in kwargs.items():\n7624                 attr_value = variable.attrs.get(attr_name)\n7625                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:\n7626                     has_value_flag = True\n7627                 else:\n7628                     has_value_flag = False\n7629                     break\n7630             if has_value_flag is True:\n7631                 selection.append(var_name)\n7632         return self[selection]\n7633 \n7634     def unify_chunks(self: T_Dataset) -> T_Dataset:\n7635         \"\"\"Unify chunk size along all chunked dimensions of this Dataset.\n7636 \n7637         Returns\n7638         -------\n7639         Dataset with consistent chunk sizes for all dask-array variables\n7640 \n7641         See Also\n7642         --------\n7643         dask.array.core.unify_chunks\n7644         \"\"\"\n7645 \n7646         return unify_chunks(self)[0]\n7647 \n7648     def map_blocks(\n7649         self,\n7650         func: Callable[..., T_Xarray],\n7651         args: Sequence[Any] = (),\n7652         kwargs: Mapping[str, Any] | None = None,\n7653         template: DataArray | Dataset | None = None,\n7654     ) -> T_Xarray:\n7655         \"\"\"\n7656         Apply a function to each block of this Dataset.\n7657 \n7658         .. warning::\n7659             This method is experimental and its signature may change.\n7660 \n7661         Parameters\n7662         ----------\n7663         func : callable\n7664             User-provided function that accepts a Dataset as its first\n7665             parameter. The function will receive a subset or 'block' of this Dataset (see below),\n7666             corresponding to one chunk along each chunked dimension. ``func`` will be\n7667             executed as ``func(subset_dataset, *subset_args, **kwargs)``.\n7668 \n7669             This function must return either a single DataArray or a single Dataset.\n7670 \n7671             This function cannot add a new chunked dimension.\n7672         args : sequence\n7673             Passed to func after unpacking and subsetting any xarray objects by blocks.\n7674             xarray objects in args must be aligned with obj, otherwise an error is raised.\n7675         kwargs : Mapping or None\n7676             Passed verbatim to func after unpacking. xarray objects, if any, will not be\n7677             subset to blocks. Passing dask collections in kwargs is not allowed.\n7678         template : DataArray, Dataset or None, optional\n7679             xarray object representing the final result after compute is called. If not provided,\n7680             the function will be first run on mocked-up data, that looks like this object but\n7681             has sizes 0, to determine properties of the returned object such as dtype,\n7682             variable names, attributes, new dimensions and new indexes (if any).\n7683             ``template`` must be provided if the function changes the size of existing dimensions.\n7684             When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n7685             ``attrs`` set by ``func`` will be ignored.\n7686 \n7687         Returns\n7688         -------\n7689         A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n7690         function.\n7691 \n7692         Notes\n7693         -----\n7694         This function is designed for when ``func`` needs to manipulate a whole xarray object\n7695         subset to each block. Each block is loaded into memory. In the more common case where\n7696         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n7697 \n7698         If none of the variables in this object is backed by dask arrays, calling this function is\n7699         equivalent to calling ``func(obj, *args, **kwargs)``.\n7700 \n7701         See Also\n7702         --------\n7703         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n7704         xarray.DataArray.map_blocks\n7705 \n7706         Examples\n7707         --------\n7708         Calculate an anomaly from climatology using ``.groupby()``. Using\n7709         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n7710         its indices, and its methods like ``.groupby()``.\n7711 \n7712         >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n7713         ...     gb = da.groupby(groupby_type)\n7714         ...     clim = gb.mean(dim=\"time\")\n7715         ...     return gb - clim\n7716         ...\n7717         >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n7718         >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n7719         >>> np.random.seed(123)\n7720         >>> array = xr.DataArray(\n7721         ...     np.random.rand(len(time)),\n7722         ...     dims=[\"time\"],\n7723         ...     coords={\"time\": time, \"month\": month},\n7724         ... ).chunk()\n7725         >>> ds = xr.Dataset({\"a\": array})\n7726         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()\n7727         <xarray.Dataset>\n7728         Dimensions:  (time: 24)\n7729         Coordinates:\n7730           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n7731             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n7732         Data variables:\n7733             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901\n7734 \n7735         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n7736         to the function being applied in ``xr.map_blocks()``:\n7737 \n7738         >>> ds.map_blocks(\n7739         ...     calculate_anomaly,\n7740         ...     kwargs={\"groupby_type\": \"time.year\"},\n7741         ...     template=ds,\n7742         ... )\n7743         <xarray.Dataset>\n7744         Dimensions:  (time: 24)\n7745         Coordinates:\n7746           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n7747             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n7748         Data variables:\n7749             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>\n7750         \"\"\"\n7751         from xarray.core.parallel import map_blocks\n7752 \n7753         return map_blocks(func, self, args, kwargs, template)\n7754 \n7755     def polyfit(\n7756         self: T_Dataset,\n7757         dim: Hashable,\n7758         deg: int,\n7759         skipna: bool | None = None,\n7760         rcond: float | None = None,\n7761         w: Hashable | Any = None,\n7762         full: bool = False,\n7763         cov: bool | Literal[\"unscaled\"] = False,\n7764     ) -> T_Dataset:\n7765         \"\"\"\n7766         Least squares polynomial fit.\n7767 \n7768         This replicates the behaviour of `numpy.polyfit` but differs by skipping\n7769         invalid values when `skipna = True`.\n7770 \n7771         Parameters\n7772         ----------\n7773         dim : hashable\n7774             Coordinate along which to fit the polynomials.\n7775         deg : int\n7776             Degree of the fitting polynomial.\n7777         skipna : bool or None, optional\n7778             If True, removes all invalid values before fitting each 1D slices of the array.\n7779             Default is True if data is stored in a dask.array or if there is any\n7780             invalid values, False otherwise.\n7781         rcond : float or None, optional\n7782             Relative condition number to the fit.\n7783         w : hashable or Any, optional\n7784             Weights to apply to the y-coordinate of the sample points.\n7785             Can be an array-like object or the name of a coordinate in the dataset.\n7786         full : bool, default: False\n7787             Whether to return the residuals, matrix rank and singular values in addition\n7788             to the coefficients.\n7789         cov : bool or \"unscaled\", default: False\n7790             Whether to return to the covariance matrix in addition to the coefficients.\n7791             The matrix is not scaled if `cov='unscaled'`.\n7792 \n7793         Returns\n7794         -------\n7795         polyfit_results : Dataset\n7796             A single dataset which contains (for each \"var\" in the input dataset):\n7797 \n7798             [var]_polyfit_coefficients\n7799                 The coefficients of the best fit for each variable in this dataset.\n7800             [var]_polyfit_residuals\n7801                 The residuals of the least-square computation for each variable (only included if `full=True`)\n7802                 When the matrix rank is deficient, np.nan is returned.\n7803             [dim]_matrix_rank\n7804                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n7805                 The rank is computed ignoring the NaN values that might be skipped.\n7806             [dim]_singular_values\n7807                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)\n7808             [var]_polyfit_covariance\n7809                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)\n7810 \n7811         Warns\n7812         -----\n7813         RankWarning\n7814             The rank of the coefficient matrix in the least-squares fit is deficient.\n7815             The warning is not raised with in-memory (not dask) data and `full=True`.\n7816 \n7817         See Also\n7818         --------\n7819         numpy.polyfit\n7820         numpy.polyval\n7821         xarray.polyval\n7822         \"\"\"\n7823         from xarray.core.dataarray import DataArray\n7824 \n7825         variables = {}\n7826         skipna_da = skipna\n7827 \n7828         x = get_clean_interp_index(self, dim, strict=False)\n7829         xname = f\"{self[dim].name}_\"\n7830         order = int(deg) + 1\n7831         lhs = np.vander(x, order)\n7832 \n7833         if rcond is None:\n7834             rcond = (\n7835                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]\n7836             )\n7837 \n7838         # Weights:\n7839         if w is not None:\n7840             if isinstance(w, Hashable):\n7841                 w = self.coords[w]\n7842             w = np.asarray(w)\n7843             if w.ndim != 1:\n7844                 raise TypeError(\"Expected a 1-d array for weights.\")\n7845             if w.shape[0] != lhs.shape[0]:\n7846                 raise TypeError(f\"Expected w and {dim} to have the same length\")\n7847             lhs *= w[:, np.newaxis]\n7848 \n7849         # Scaling\n7850         scale = np.sqrt((lhs * lhs).sum(axis=0))\n7851         lhs /= scale\n7852 \n7853         degree_dim = utils.get_temp_dimname(self.dims, \"degree\")\n7854 \n7855         rank = np.linalg.matrix_rank(lhs)\n7856 \n7857         if full:\n7858             rank = DataArray(rank, name=xname + \"matrix_rank\")\n7859             variables[rank.name] = rank\n7860             _sing = np.linalg.svd(lhs, compute_uv=False)\n7861             sing = DataArray(\n7862                 _sing,\n7863                 dims=(degree_dim,),\n7864                 coords={degree_dim: np.arange(rank - 1, -1, -1)},\n7865                 name=xname + \"singular_values\",\n7866             )\n7867             variables[sing.name] = sing\n7868 \n7869         for name, da in self.data_vars.items():\n7870             if dim not in da.dims:\n7871                 continue\n7872 \n7873             if is_duck_dask_array(da.data) and (\n7874                 rank != order or full or skipna is None\n7875             ):\n7876                 # Current algorithm with dask and skipna=False neither supports\n7877                 # deficient ranks nor does it output the \"full\" info (issue dask/dask#6516)\n7878                 skipna_da = True\n7879             elif skipna is None:\n7880                 skipna_da = bool(np.any(da.isnull()))\n7881 \n7882             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]\n7883             stacked_coords: dict[Hashable, DataArray] = {}\n7884             if dims_to_stack:\n7885                 stacked_dim = utils.get_temp_dimname(dims_to_stack, \"stacked\")\n7886                 rhs = da.transpose(dim, *dims_to_stack).stack(\n7887                     {stacked_dim: dims_to_stack}\n7888                 )\n7889                 stacked_coords = {stacked_dim: rhs[stacked_dim]}\n7890                 scale_da = scale[:, np.newaxis]\n7891             else:\n7892                 rhs = da\n7893                 scale_da = scale\n7894 \n7895             if w is not None:\n7896                 rhs *= w[:, np.newaxis]\n7897 \n7898             with warnings.catch_warnings():\n7899                 if full:  # Copy np.polyfit behavior\n7900                     warnings.simplefilter(\"ignore\", np.RankWarning)\n7901                 else:  # Raise only once per variable\n7902                     warnings.simplefilter(\"once\", np.RankWarning)\n7903 \n7904                 coeffs, residuals = duck_array_ops.least_squares(\n7905                     lhs, rhs.data, rcond=rcond, skipna=skipna_da\n7906                 )\n7907 \n7908             if isinstance(name, str):\n7909                 name = f\"{name}_\"\n7910             else:\n7911                 # Thus a ReprObject => polyfit was called on a DataArray\n7912                 name = \"\"\n7913 \n7914             coeffs = DataArray(\n7915                 coeffs / scale_da,\n7916                 dims=[degree_dim] + list(stacked_coords.keys()),\n7917                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},\n7918                 name=name + \"polyfit_coefficients\",\n7919             )\n7920             if dims_to_stack:\n7921                 coeffs = coeffs.unstack(stacked_dim)\n7922             variables[coeffs.name] = coeffs\n7923 \n7924             if full or (cov is True):\n7925                 residuals = DataArray(\n7926                     residuals if dims_to_stack else residuals.squeeze(),\n7927                     dims=list(stacked_coords.keys()),\n7928                     coords=stacked_coords,\n7929                     name=name + \"polyfit_residuals\",\n7930                 )\n7931                 if dims_to_stack:\n7932                     residuals = residuals.unstack(stacked_dim)\n7933                 variables[residuals.name] = residuals\n7934 \n7935             if cov:\n7936                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))\n7937                 Vbase /= np.outer(scale, scale)\n7938                 if cov == \"unscaled\":\n7939                     fac = 1\n7940                 else:\n7941                     if x.shape[0] <= order:\n7942                         raise ValueError(\n7943                             \"The number of data points must exceed order to scale the covariance matrix.\"\n7944                         )\n7945                     fac = residuals / (x.shape[0] - order)\n7946                 covariance = DataArray(Vbase, dims=(\"cov_i\", \"cov_j\")) * fac\n7947                 variables[name + \"polyfit_covariance\"] = covariance\n7948 \n7949         return type(self)(data_vars=variables, attrs=self.attrs.copy())\n7950 \n7951     def pad(\n7952         self: T_Dataset,\n7953         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,\n7954         mode: PadModeOptions = \"constant\",\n7955         stat_length: int\n7956         | tuple[int, int]\n7957         | Mapping[Any, tuple[int, int]]\n7958         | None = None,\n7959         constant_values: (\n7960             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None\n7961         ) = None,\n7962         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,\n7963         reflect_type: PadReflectOptions = None,\n7964         keep_attrs: bool | None = None,\n7965         **pad_width_kwargs: Any,\n7966     ) -> T_Dataset:\n7967         \"\"\"Pad this dataset along one or more dimensions.\n7968 \n7969         .. warning::\n7970             This function is experimental and its behaviour is likely to change\n7971             especially regarding padding of dimension coordinates (or IndexVariables).\n7972 \n7973         When using one of the modes (\"edge\", \"reflect\", \"symmetric\", \"wrap\"),\n7974         coordinates will be padded with the same mode, otherwise coordinates\n7975         are padded using the \"constant\" mode with fill_value dtypes.NA.\n7976 \n7977         Parameters\n7978         ----------\n7979         pad_width : mapping of hashable to tuple of int\n7980             Mapping with the form of {dim: (pad_before, pad_after)}\n7981             describing the number of values padded along each dimension.\n7982             {dim: pad} is a shortcut for pad_before = pad_after = pad\n7983         mode : {\"constant\", \"edge\", \"linear_ramp\", \"maximum\", \"mean\", \"median\", \\\n7984             \"minimum\", \"reflect\", \"symmetric\", \"wrap\"}, default: \"constant\"\n7985             How to pad the DataArray (taken from numpy docs):\n7986 \n7987             - \"constant\": Pads with a constant value.\n7988             - \"edge\": Pads with the edge values of array.\n7989             - \"linear_ramp\": Pads with the linear ramp between end_value and the\n7990               array edge value.\n7991             - \"maximum\": Pads with the maximum value of all or part of the\n7992               vector along each axis.\n7993             - \"mean\": Pads with the mean value of all or part of the\n7994               vector along each axis.\n7995             - \"median\": Pads with the median value of all or part of the\n7996               vector along each axis.\n7997             - \"minimum\": Pads with the minimum value of all or part of the\n7998               vector along each axis.\n7999             - \"reflect\": Pads with the reflection of the vector mirrored on\n8000               the first and last values of the vector along each axis.\n8001             - \"symmetric\": Pads with the reflection of the vector mirrored\n8002               along the edge of the array.\n8003             - \"wrap\": Pads with the wrap of the vector along the axis.\n8004               The first values are used to pad the end and the\n8005               end values are used to pad the beginning.\n8006 \n8007         stat_length : int, tuple or mapping of hashable to tuple, default: None\n8008             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n8009             values at edge of each axis used to calculate the statistic value.\n8010             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique\n8011             statistic lengths along each dimension.\n8012             ((before, after),) yields same before and after statistic lengths\n8013             for each dimension.\n8014             (stat_length,) or int is a shortcut for before = after = statistic\n8015             length for all axes.\n8016             Default is ``None``, to use the entire axis.\n8017         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0\n8018             Used in 'constant'.  The values to set the padded values for each\n8019             axis.\n8020             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n8021             pad constants along each dimension.\n8022             ``((before, after),)`` yields same before and after constants for each\n8023             dimension.\n8024             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n8025             all dimensions.\n8026             Default is 0.\n8027         end_values : scalar, tuple or mapping of hashable to tuple, default: 0\n8028             Used in 'linear_ramp'.  The values used for the ending value of the\n8029             linear_ramp and that will form the edge of the padded array.\n8030             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique\n8031             end values along each dimension.\n8032             ``((before, after),)`` yields same before and after end values for each\n8033             axis.\n8034             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for\n8035             all axes.\n8036             Default is 0.\n8037         reflect_type : {\"even\", \"odd\", None}, optional\n8038             Used in \"reflect\", and \"symmetric\".  The \"even\" style is the\n8039             default with an unaltered reflection around the edge value.  For\n8040             the \"odd\" style, the extended part of the array is created by\n8041             subtracting the reflected values from two times the edge value.\n8042         keep_attrs : bool or None, optional\n8043             If True, the attributes (``attrs``) will be copied from the\n8044             original object to the new one. If False, the new object\n8045             will be returned without attributes.\n8046         **pad_width_kwargs\n8047             The keyword arguments form of ``pad_width``.\n8048             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.\n8049 \n8050         Returns\n8051         -------\n8052         padded : Dataset\n8053             Dataset with the padded coordinates and data.\n8054 \n8055         See Also\n8056         --------\n8057         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad\n8058 \n8059         Notes\n8060         -----\n8061         By default when ``mode=\"constant\"`` and ``constant_values=None``, integer types will be\n8062         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion\n8063         specify ``constant_values=np.nan``\n8064 \n8065         Padding coordinates will drop their corresponding index (if any) and will reset default\n8066         indexes for dimension coordinates.\n8067 \n8068         Examples\n8069         --------\n8070         >>> ds = xr.Dataset({\"foo\": (\"x\", range(5))})\n8071         >>> ds.pad(x=(1, 2))\n8072         <xarray.Dataset>\n8073         Dimensions:  (x: 8)\n8074         Dimensions without coordinates: x\n8075         Data variables:\n8076             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan\n8077         \"\"\"\n8078         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n8079 \n8080         if mode in (\"edge\", \"reflect\", \"symmetric\", \"wrap\"):\n8081             coord_pad_mode = mode\n8082             coord_pad_options = {\n8083                 \"stat_length\": stat_length,\n8084                 \"constant_values\": constant_values,\n8085                 \"end_values\": end_values,\n8086                 \"reflect_type\": reflect_type,\n8087             }\n8088         else:\n8089             coord_pad_mode = \"constant\"\n8090             coord_pad_options = {}\n8091 \n8092         if keep_attrs is None:\n8093             keep_attrs = _get_keep_attrs(default=True)\n8094 \n8095         variables = {}\n8096 \n8097         # keep indexes that won't be affected by pad and drop all other indexes\n8098         xindexes = self.xindexes\n8099         pad_dims = set(pad_width)\n8100         indexes = {}\n8101         for k, idx in xindexes.items():\n8102             if not pad_dims.intersection(xindexes.get_all_dims(k)):\n8103                 indexes[k] = idx\n8104 \n8105         for name, var in self.variables.items():\n8106             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}\n8107             if not var_pad_width:\n8108                 variables[name] = var\n8109             elif name in self.data_vars:\n8110                 variables[name] = var.pad(\n8111                     pad_width=var_pad_width,\n8112                     mode=mode,\n8113                     stat_length=stat_length,\n8114                     constant_values=constant_values,\n8115                     end_values=end_values,\n8116                     reflect_type=reflect_type,\n8117                     keep_attrs=keep_attrs,\n8118                 )\n8119             else:\n8120                 variables[name] = var.pad(\n8121                     pad_width=var_pad_width,\n8122                     mode=coord_pad_mode,\n8123                     keep_attrs=keep_attrs,\n8124                     **coord_pad_options,  # type: ignore[arg-type]\n8125                 )\n8126                 # reset default index of dimension coordinates\n8127                 if (name,) == var.dims:\n8128                     dim_var = {name: variables[name]}\n8129                     index = PandasIndex.from_variables(dim_var, options={})\n8130                     index_vars = index.create_variables(dim_var)\n8131                     indexes[name] = index\n8132                     variables[name] = index_vars[name]\n8133 \n8134         attrs = self._attrs if keep_attrs else None\n8135         return self._replace_with_new_dims(variables, indexes=indexes, attrs=attrs)\n8136 \n8137     def idxmin(\n8138         self: T_Dataset,\n8139         dim: Hashable | None = None,\n8140         skipna: bool | None = None,\n8141         fill_value: Any = xrdtypes.NA,\n8142         keep_attrs: bool | None = None,\n8143     ) -> T_Dataset:\n8144         \"\"\"Return the coordinate label of the minimum value along a dimension.\n8145 \n8146         Returns a new `Dataset` named after the dimension with the values of\n8147         the coordinate labels along that dimension corresponding to minimum\n8148         values along that dimension.\n8149 \n8150         In comparison to :py:meth:`~Dataset.argmin`, this returns the\n8151         coordinate label while :py:meth:`~Dataset.argmin` returns the index.\n8152 \n8153         Parameters\n8154         ----------\n8155         dim : Hashable, optional\n8156             Dimension over which to apply `idxmin`.  This is optional for 1D\n8157             variables, but required for variables with 2 or more dimensions.\n8158         skipna : bool or None, optional\n8159             If True, skip missing values (as marked by NaN). By default, only\n8160             skips missing values for ``float``, ``complex``, and ``object``\n8161             dtypes; other dtypes either do not have a sentinel missing value\n8162             (``int``) or ``skipna=True`` has not been implemented\n8163             (``datetime64`` or ``timedelta64``).\n8164         fill_value : Any, default: NaN\n8165             Value to be filled in case all of the values along a dimension are\n8166             null.  By default this is NaN.  The fill value and result are\n8167             automatically converted to a compatible dtype if possible.\n8168             Ignored if ``skipna`` is False.\n8169         keep_attrs : bool or None, optional\n8170             If True, the attributes (``attrs``) will be copied from the\n8171             original object to the new one. If False, the new object\n8172             will be returned without attributes.\n8173 \n8174         Returns\n8175         -------\n8176         reduced : Dataset\n8177             New `Dataset` object with `idxmin` applied to its data and the\n8178             indicated dimension removed.\n8179 \n8180         See Also\n8181         --------\n8182         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin\n8183 \n8184         Examples\n8185         --------\n8186         >>> array1 = xr.DataArray(\n8187         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n8188         ... )\n8189         >>> array2 = xr.DataArray(\n8190         ...     [\n8191         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n8192         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n8193         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n8194         ...     ],\n8195         ...     dims=[\"y\", \"x\"],\n8196         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n8197         ... )\n8198         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n8199         >>> ds.min(dim=\"x\")\n8200         <xarray.Dataset>\n8201         Dimensions:  (y: 3)\n8202         Coordinates:\n8203           * y        (y) int64 -1 0 1\n8204         Data variables:\n8205             int      int64 -2\n8206             float    (y) float64 -2.0 -4.0 1.0\n8207         >>> ds.argmin(dim=\"x\")\n8208         <xarray.Dataset>\n8209         Dimensions:  (y: 3)\n8210         Coordinates:\n8211           * y        (y) int64 -1 0 1\n8212         Data variables:\n8213             int      int64 4\n8214             float    (y) int64 4 0 2\n8215         >>> ds.idxmin(dim=\"x\")\n8216         <xarray.Dataset>\n8217         Dimensions:  (y: 3)\n8218         Coordinates:\n8219           * y        (y) int64 -1 0 1\n8220         Data variables:\n8221             int      <U1 'e'\n8222             float    (y) object 'e' 'a' 'c'\n8223         \"\"\"\n8224         return self.map(\n8225             methodcaller(\n8226                 \"idxmin\",\n8227                 dim=dim,\n8228                 skipna=skipna,\n8229                 fill_value=fill_value,\n8230                 keep_attrs=keep_attrs,\n8231             )\n8232         )\n8233 \n8234     def idxmax(\n8235         self: T_Dataset,\n8236         dim: Hashable | None = None,\n8237         skipna: bool | None = None,\n8238         fill_value: Any = xrdtypes.NA,\n8239         keep_attrs: bool | None = None,\n8240     ) -> T_Dataset:\n8241         \"\"\"Return the coordinate label of the maximum value along a dimension.\n8242 \n8243         Returns a new `Dataset` named after the dimension with the values of\n8244         the coordinate labels along that dimension corresponding to maximum\n8245         values along that dimension.\n8246 \n8247         In comparison to :py:meth:`~Dataset.argmax`, this returns the\n8248         coordinate label while :py:meth:`~Dataset.argmax` returns the index.\n8249 \n8250         Parameters\n8251         ----------\n8252         dim : str, optional\n8253             Dimension over which to apply `idxmax`.  This is optional for 1D\n8254             variables, but required for variables with 2 or more dimensions.\n8255         skipna : bool or None, optional\n8256             If True, skip missing values (as marked by NaN). By default, only\n8257             skips missing values for ``float``, ``complex``, and ``object``\n8258             dtypes; other dtypes either do not have a sentinel missing value\n8259             (``int``) or ``skipna=True`` has not been implemented\n8260             (``datetime64`` or ``timedelta64``).\n8261         fill_value : Any, default: NaN\n8262             Value to be filled in case all of the values along a dimension are\n8263             null.  By default this is NaN.  The fill value and result are\n8264             automatically converted to a compatible dtype if possible.\n8265             Ignored if ``skipna`` is False.\n8266         keep_attrs : bool or None, optional\n8267             If True, the attributes (``attrs``) will be copied from the\n8268             original object to the new one. If False, the new object\n8269             will be returned without attributes.\n8270 \n8271         Returns\n8272         -------\n8273         reduced : Dataset\n8274             New `Dataset` object with `idxmax` applied to its data and the\n8275             indicated dimension removed.\n8276 \n8277         See Also\n8278         --------\n8279         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax\n8280 \n8281         Examples\n8282         --------\n8283         >>> array1 = xr.DataArray(\n8284         ...     [0, 2, 1, 0, -2], dims=\"x\", coords={\"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]}\n8285         ... )\n8286         >>> array2 = xr.DataArray(\n8287         ...     [\n8288         ...         [2.0, 1.0, 2.0, 0.0, -2.0],\n8289         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],\n8290         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],\n8291         ...     ],\n8292         ...     dims=[\"y\", \"x\"],\n8293         ...     coords={\"y\": [-1, 0, 1], \"x\": [\"a\", \"b\", \"c\", \"d\", \"e\"]},\n8294         ... )\n8295         >>> ds = xr.Dataset({\"int\": array1, \"float\": array2})\n8296         >>> ds.max(dim=\"x\")\n8297         <xarray.Dataset>\n8298         Dimensions:  (y: 3)\n8299         Coordinates:\n8300           * y        (y) int64 -1 0 1\n8301         Data variables:\n8302             int      int64 2\n8303             float    (y) float64 2.0 2.0 1.0\n8304         >>> ds.argmax(dim=\"x\")\n8305         <xarray.Dataset>\n8306         Dimensions:  (y: 3)\n8307         Coordinates:\n8308           * y        (y) int64 -1 0 1\n8309         Data variables:\n8310             int      int64 1\n8311             float    (y) int64 0 2 2\n8312         >>> ds.idxmax(dim=\"x\")\n8313         <xarray.Dataset>\n8314         Dimensions:  (y: 3)\n8315         Coordinates:\n8316           * y        (y) int64 -1 0 1\n8317         Data variables:\n8318             int      <U1 'b'\n8319             float    (y) object 'a' 'c' 'c'\n8320         \"\"\"\n8321         return self.map(\n8322             methodcaller(\n8323                 \"idxmax\",\n8324                 dim=dim,\n8325                 skipna=skipna,\n8326                 fill_value=fill_value,\n8327                 keep_attrs=keep_attrs,\n8328             )\n8329         )\n8330 \n8331     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:\n8332         \"\"\"Indices of the minima of the member variables.\n8333 \n8334         If there are multiple minima, the indices of the first one found will be\n8335         returned.\n8336 \n8337         Parameters\n8338         ----------\n8339         dim : Hashable, optional\n8340             The dimension over which to find the minimum. By default, finds minimum over\n8341             all dimensions - for now returning an int for backward compatibility, but\n8342             this is deprecated, in future will be an error, since DataArray.argmin will\n8343             return a dict with indices for all dimensions, which does not make sense for\n8344             a Dataset.\n8345         keep_attrs : bool, optional\n8346             If True, the attributes (`attrs`) will be copied from the original\n8347             object to the new one.  If False (default), the new object will be\n8348             returned without attributes.\n8349         skipna : bool, optional\n8350             If True, skip missing values (as marked by NaN). By default, only\n8351             skips missing values for float dtypes; other dtypes either do not\n8352             have a sentinel missing value (int) or skipna=True has not been\n8353             implemented (object, datetime64 or timedelta64).\n8354 \n8355         Returns\n8356         -------\n8357         result : Dataset\n8358 \n8359         See Also\n8360         --------\n8361         DataArray.argmin\n8362         \"\"\"\n8363         if dim is None:\n8364             warnings.warn(\n8365                 \"Once the behaviour of DataArray.argmin() and Variable.argmin() without \"\n8366                 \"dim changes to return a dict of indices of each dimension, for \"\n8367                 \"consistency it will be an error to call Dataset.argmin() with no argument,\"\n8368                 \"since we don't return a dict of Datasets.\",\n8369                 DeprecationWarning,\n8370                 stacklevel=2,\n8371             )\n8372         if (\n8373             dim is None\n8374             or (not isinstance(dim, Sequence) and dim is not ...)\n8375             or isinstance(dim, str)\n8376         ):\n8377             # Return int index if single dimension is passed, and is not part of a\n8378             # sequence\n8379             argmin_func = getattr(duck_array_ops, \"argmin\")\n8380             return self.reduce(\n8381                 argmin_func, dim=None if dim is None else [dim], **kwargs\n8382             )\n8383         else:\n8384             raise ValueError(\n8385                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n8386                 \"dicts cannot be contained in a Dataset, so cannot call \"\n8387                 \"Dataset.argmin() with a sequence or ... for dim\"\n8388             )\n8389 \n8390     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:\n8391         \"\"\"Indices of the maxima of the member variables.\n8392 \n8393         If there are multiple maxima, the indices of the first one found will be\n8394         returned.\n8395 \n8396         Parameters\n8397         ----------\n8398         dim : str, optional\n8399             The dimension over which to find the maximum. By default, finds maximum over\n8400             all dimensions - for now returning an int for backward compatibility, but\n8401             this is deprecated, in future will be an error, since DataArray.argmax will\n8402             return a dict with indices for all dimensions, which does not make sense for\n8403             a Dataset.\n8404         keep_attrs : bool, optional\n8405             If True, the attributes (`attrs`) will be copied from the original\n8406             object to the new one.  If False (default), the new object will be\n8407             returned without attributes.\n8408         skipna : bool, optional\n8409             If True, skip missing values (as marked by NaN). By default, only\n8410             skips missing values for float dtypes; other dtypes either do not\n8411             have a sentinel missing value (int) or skipna=True has not been\n8412             implemented (object, datetime64 or timedelta64).\n8413 \n8414         Returns\n8415         -------\n8416         result : Dataset\n8417 \n8418         See Also\n8419         --------\n8420         DataArray.argmax\n8421 \n8422         \"\"\"\n8423         if dim is None:\n8424             warnings.warn(\n8425                 \"Once the behaviour of DataArray.argmin() and Variable.argmin() without \"\n8426                 \"dim changes to return a dict of indices of each dimension, for \"\n8427                 \"consistency it will be an error to call Dataset.argmin() with no argument,\"\n8428                 \"since we don't return a dict of Datasets.\",\n8429                 DeprecationWarning,\n8430                 stacklevel=2,\n8431             )\n8432         if (\n8433             dim is None\n8434             or (not isinstance(dim, Sequence) and dim is not ...)\n8435             or isinstance(dim, str)\n8436         ):\n8437             # Return int index if single dimension is passed, and is not part of a\n8438             # sequence\n8439             argmax_func = getattr(duck_array_ops, \"argmax\")\n8440             return self.reduce(\n8441                 argmax_func, dim=None if dim is None else [dim], **kwargs\n8442             )\n8443         else:\n8444             raise ValueError(\n8445                 \"When dim is a sequence or ..., DataArray.argmin() returns a dict. \"\n8446                 \"dicts cannot be contained in a Dataset, so cannot call \"\n8447                 \"Dataset.argmin() with a sequence or ... for dim\"\n8448             )\n8449 \n8450     def query(\n8451         self: T_Dataset,\n8452         queries: Mapping[Any, Any] | None = None,\n8453         parser: QueryParserOptions = \"pandas\",\n8454         engine: QueryEngineOptions = None,\n8455         missing_dims: ErrorOptionsWithWarn = \"raise\",\n8456         **queries_kwargs: Any,\n8457     ) -> T_Dataset:\n8458         \"\"\"Return a new dataset with each array indexed along the specified\n8459         dimension(s), where the indexers are given as strings containing\n8460         Python expressions to be evaluated against the data variables in the\n8461         dataset.\n8462 \n8463         Parameters\n8464         ----------\n8465         queries : dict-like, optional\n8466             A dict-like with keys matching dimensions and values given by strings\n8467             containing Python expressions to be evaluated against the data variables\n8468             in the dataset. The expressions will be evaluated using the pandas\n8469             eval() function, and can contain any valid Python expressions but cannot\n8470             contain any Python statements.\n8471         parser : {\"pandas\", \"python\"}, default: \"pandas\"\n8472             The parser to use to construct the syntax tree from the expression.\n8473             The default of 'pandas' parses code slightly different than standard\n8474             Python. Alternatively, you can parse an expression using the 'python'\n8475             parser to retain strict Python semantics.\n8476         engine : {\"python\", \"numexpr\", None}, default: None\n8477             The engine used to evaluate the expression. Supported engines are:\n8478 \n8479             - None: tries to use numexpr, falls back to python\n8480             - \"numexpr\": evaluates expressions using numexpr\n8481             - \"python\": performs operations as if you had eval\u2019d in top level python\n8482 \n8483         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n8484             What to do if dimensions that should be selected from are not present in the\n8485             Dataset:\n8486 \n8487             - \"raise\": raise an exception\n8488             - \"warn\": raise a warning, and ignore the missing dimensions\n8489             - \"ignore\": ignore the missing dimensions\n8490 \n8491         **queries_kwargs : {dim: query, ...}, optional\n8492             The keyword arguments form of ``queries``.\n8493             One of queries or queries_kwargs must be provided.\n8494 \n8495         Returns\n8496         -------\n8497         obj : Dataset\n8498             A new Dataset with the same contents as this dataset, except each\n8499             array and dimension is indexed by the results of the appropriate\n8500             queries.\n8501 \n8502         See Also\n8503         --------\n8504         Dataset.isel\n8505         pandas.eval\n8506 \n8507         Examples\n8508         --------\n8509         >>> a = np.arange(0, 5, 1)\n8510         >>> b = np.linspace(0, 1, 5)\n8511         >>> ds = xr.Dataset({\"a\": (\"x\", a), \"b\": (\"x\", b)})\n8512         >>> ds\n8513         <xarray.Dataset>\n8514         Dimensions:  (x: 5)\n8515         Dimensions without coordinates: x\n8516         Data variables:\n8517             a        (x) int64 0 1 2 3 4\n8518             b        (x) float64 0.0 0.25 0.5 0.75 1.0\n8519         >>> ds.query(x=\"a > 2\")\n8520         <xarray.Dataset>\n8521         Dimensions:  (x: 2)\n8522         Dimensions without coordinates: x\n8523         Data variables:\n8524             a        (x) int64 3 4\n8525             b        (x) float64 0.75 1.0\n8526         \"\"\"\n8527 \n8528         # allow queries to be given either as a dict or as kwargs\n8529         queries = either_dict_or_kwargs(queries, queries_kwargs, \"query\")\n8530 \n8531         # check queries\n8532         for dim, expr in queries.items():\n8533             if not isinstance(expr, str):\n8534                 msg = f\"expr for dim {dim} must be a string to be evaluated, {type(expr)} given\"\n8535                 raise ValueError(msg)\n8536 \n8537         # evaluate the queries to create the indexers\n8538         indexers = {\n8539             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)\n8540             for dim, expr in queries.items()\n8541         }\n8542 \n8543         # apply the selection\n8544         return self.isel(indexers, missing_dims=missing_dims)\n8545 \n8546     def curvefit(\n8547         self: T_Dataset,\n8548         coords: str | DataArray | Iterable[str | DataArray],\n8549         func: Callable[..., Any],\n8550         reduce_dims: Dims = None,\n8551         skipna: bool = True,\n8552         p0: dict[str, Any] | None = None,\n8553         bounds: dict[str, Any] | None = None,\n8554         param_names: Sequence[str] | None = None,\n8555         kwargs: dict[str, Any] | None = None,\n8556     ) -> T_Dataset:\n8557         \"\"\"\n8558         Curve fitting optimization for arbitrary functions.\n8559 \n8560         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.\n8561 \n8562         Parameters\n8563         ----------\n8564         coords : hashable, DataArray, or sequence of hashable or DataArray\n8565             Independent coordinate(s) over which to perform the curve fitting. Must share\n8566             at least one dimension with the calling object. When fitting multi-dimensional\n8567             functions, supply `coords` as a sequence in the same order as arguments in\n8568             `func`. To fit along existing dimensions of the calling object, `coords` can\n8569             also be specified as a str or sequence of strs.\n8570         func : callable\n8571             User specified function in the form `f(x, *params)` which returns a numpy\n8572             array of length `len(x)`. `params` are the fittable parameters which are optimized\n8573             by scipy curve_fit. `x` can also be specified as a sequence containing multiple\n8574             coordinates, e.g. `f((x0, x1), *params)`.\n8575         reduce_dims : str, Iterable of Hashable or None, optional\n8576             Additional dimension(s) over which to aggregate while fitting. For example,\n8577             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will\n8578             aggregate all lat and lon points and fit the specified function along the\n8579             time dimension.\n8580         skipna : bool, default: True\n8581             Whether to skip missing values when fitting. Default is True.\n8582         p0 : dict-like, optional\n8583             Optional dictionary of parameter names to initial guesses passed to the\n8584             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will\n8585             be assigned initial values following the default scipy behavior.\n8586         bounds : dict-like, optional\n8587             Optional dictionary of parameter names to bounding values passed to the\n8588             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest\n8589             will be unbounded following the default scipy behavior.\n8590         param_names : sequence of hashable, optional\n8591             Sequence of names for the fittable parameters of `func`. If not supplied,\n8592             this will be automatically determined by arguments of `func`. `param_names`\n8593             should be manually supplied when fitting a function that takes a variable\n8594             number of parameters.\n8595         **kwargs : optional\n8596             Additional keyword arguments to passed to scipy curve_fit.\n8597 \n8598         Returns\n8599         -------\n8600         curvefit_results : Dataset\n8601             A single dataset which contains:\n8602 \n8603             [var]_curvefit_coefficients\n8604                 The coefficients of the best fit.\n8605             [var]_curvefit_covariance\n8606                 The covariance matrix of the coefficient estimates.\n8607 \n8608         See Also\n8609         --------\n8610         Dataset.polyfit\n8611         scipy.optimize.curve_fit\n8612         \"\"\"\n8613         from scipy.optimize import curve_fit\n8614 \n8615         from xarray.core.alignment import broadcast\n8616         from xarray.core.computation import apply_ufunc\n8617         from xarray.core.dataarray import _THIS_ARRAY, DataArray\n8618 \n8619         if p0 is None:\n8620             p0 = {}\n8621         if bounds is None:\n8622             bounds = {}\n8623         if kwargs is None:\n8624             kwargs = {}\n8625 \n8626         reduce_dims_: list[Hashable]\n8627         if not reduce_dims:\n8628             reduce_dims_ = []\n8629         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):\n8630             reduce_dims_ = [reduce_dims]\n8631         else:\n8632             reduce_dims_ = list(reduce_dims)\n8633 \n8634         if (\n8635             isinstance(coords, str)\n8636             or isinstance(coords, DataArray)\n8637             or not isinstance(coords, Iterable)\n8638         ):\n8639             coords = [coords]\n8640         coords_: Sequence[DataArray] = [\n8641             self[coord] if isinstance(coord, str) else coord for coord in coords\n8642         ]\n8643 \n8644         # Determine whether any coords are dims on self\n8645         for coord in coords_:\n8646             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]\n8647         reduce_dims_ = list(set(reduce_dims_))\n8648         preserved_dims = list(set(self.dims) - set(reduce_dims_))\n8649         if not reduce_dims_:\n8650             raise ValueError(\n8651                 \"No arguments to `coords` were identified as a dimension on the calling \"\n8652                 \"object, and no dims were supplied to `reduce_dims`. This would result \"\n8653                 \"in fitting on scalar data.\"\n8654             )\n8655 \n8656         # Broadcast all coords with each other\n8657         coords_ = broadcast(*coords_)\n8658         coords_ = [\n8659             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_\n8660         ]\n8661 \n8662         params, func_args = _get_func_args(func, param_names)\n8663         param_defaults, bounds_defaults = _initialize_curvefit_params(\n8664             params, p0, bounds, func_args\n8665         )\n8666         n_params = len(params)\n8667         kwargs.setdefault(\"p0\", [param_defaults[p] for p in params])\n8668         kwargs.setdefault(\n8669             \"bounds\",\n8670             [\n8671                 [bounds_defaults[p][0] for p in params],\n8672                 [bounds_defaults[p][1] for p in params],\n8673             ],\n8674         )\n8675 \n8676         def _wrapper(Y, *coords_, **kwargs):\n8677             # Wrap curve_fit with raveled coordinates and pointwise NaN handling\n8678             x = np.vstack([c.ravel() for c in coords_])\n8679             y = Y.ravel()\n8680             if skipna:\n8681                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)\n8682                 x = x[:, mask]\n8683                 y = y[mask]\n8684                 if not len(y):\n8685                     popt = np.full([n_params], np.nan)\n8686                     pcov = np.full([n_params, n_params], np.nan)\n8687                     return popt, pcov\n8688             x = np.squeeze(x)\n8689             popt, pcov = curve_fit(func, x, y, **kwargs)\n8690             return popt, pcov\n8691 \n8692         result = type(self)()\n8693         for name, da in self.data_vars.items():\n8694             if name is _THIS_ARRAY:\n8695                 name = \"\"\n8696             else:\n8697                 name = f\"{str(name)}_\"\n8698 \n8699             popt, pcov = apply_ufunc(\n8700                 _wrapper,\n8701                 da,\n8702                 *coords_,\n8703                 vectorize=True,\n8704                 dask=\"parallelized\",\n8705                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],\n8706                 output_core_dims=[[\"param\"], [\"cov_i\", \"cov_j\"]],\n8707                 dask_gufunc_kwargs={\n8708                     \"output_sizes\": {\n8709                         \"param\": n_params,\n8710                         \"cov_i\": n_params,\n8711                         \"cov_j\": n_params,\n8712                     },\n8713                 },\n8714                 output_dtypes=(np.float64, np.float64),\n8715                 exclude_dims=set(reduce_dims_),\n8716                 kwargs=kwargs,\n8717             )\n8718             result[name + \"curvefit_coefficients\"] = popt\n8719             result[name + \"curvefit_covariance\"] = pcov\n8720 \n8721         result = result.assign_coords(\n8722             {\"param\": params, \"cov_i\": params, \"cov_j\": params}\n8723         )\n8724         result.attrs = self.attrs.copy()\n8725 \n8726         return result\n8727 \n8728     def drop_duplicates(\n8729         self: T_Dataset,\n8730         dim: Hashable | Iterable[Hashable],\n8731         keep: Literal[\"first\", \"last\", False] = \"first\",\n8732     ) -> T_Dataset:\n8733         \"\"\"Returns a new Dataset with duplicate dimension values removed.\n8734 \n8735         Parameters\n8736         ----------\n8737         dim : dimension label or labels\n8738             Pass `...` to drop duplicates along all dimensions.\n8739         keep : {\"first\", \"last\", False}, default: \"first\"\n8740             Determines which duplicates (if any) to keep.\n8741             - ``\"first\"`` : Drop duplicates except for the first occurrence.\n8742             - ``\"last\"`` : Drop duplicates except for the last occurrence.\n8743             - False : Drop all duplicates.\n8744 \n8745         Returns\n8746         -------\n8747         Dataset\n8748 \n8749         See Also\n8750         --------\n8751         DataArray.drop_duplicates\n8752         \"\"\"\n8753         if isinstance(dim, str):\n8754             dims: Iterable = (dim,)\n8755         elif dim is ...:\n8756             dims = self.dims\n8757         elif not isinstance(dim, Iterable):\n8758             dims = [dim]\n8759         else:\n8760             dims = dim\n8761 \n8762         missing_dims = set(dims) - set(self.dims)\n8763         if missing_dims:\n8764             raise ValueError(f\"'{missing_dims}' not found in dimensions\")\n8765 \n8766         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}\n8767         return self.isel(indexes)\n8768 \n8769     def convert_calendar(\n8770         self: T_Dataset,\n8771         calendar: CFCalendar,\n8772         dim: Hashable = \"time\",\n8773         align_on: Literal[\"date\", \"year\", None] = None,\n8774         missing: Any | None = None,\n8775         use_cftime: bool | None = None,\n8776     ) -> T_Dataset:\n8777         \"\"\"Convert the Dataset to another calendar.\n8778 \n8779         Only converts the individual timestamps, does not modify any data except\n8780         in dropping invalid/surplus dates or inserting missing dates.\n8781 \n8782         If the source and target calendars are either no_leap, all_leap or a\n8783         standard type, only the type of the time array is modified.\n8784         When converting to a leap year from a non-leap year, the 29th of February\n8785         is removed from the array. In the other direction the 29th of February\n8786         will be missing in the output, unless `missing` is specified,\n8787         in which case that value is inserted.\n8788 \n8789         For conversions involving `360_day` calendars, see Notes.\n8790 \n8791         This method is safe to use with sub-daily data as it doesn't touch the\n8792         time part of the timestamps.\n8793 \n8794         Parameters\n8795         ---------\n8796         calendar : str\n8797             The target calendar name.\n8798         dim : Hashable, default: \"time\"\n8799             Name of the time coordinate.\n8800         align_on : {None, 'date', 'year'}, optional\n8801             Must be specified when either source or target is a `360_day` calendar,\n8802             ignored otherwise. See Notes.\n8803         missing : Any or None, optional\n8804             By default, i.e. if the value is None, this method will simply attempt\n8805             to convert the dates in the source calendar to the same dates in the\n8806             target calendar, and drop any of those that are not possible to\n8807             represent.  If a value is provided, a new time coordinate will be\n8808             created in the target calendar with the same frequency as the original\n8809             time coordinate; for any dates that are not present in the source, the\n8810             data will be filled with this value.  Note that using this mode requires\n8811             that the source data have an inferable frequency; for more information\n8812             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and\n8813             target calendar combinations, this could result in many missing values, see notes.\n8814         use_cftime : bool or None, optional\n8815             Whether to use cftime objects in the output, only used if `calendar`\n8816             is one of {\"proleptic_gregorian\", \"gregorian\" or \"standard\"}.\n8817             If True, the new time axis uses cftime objects.\n8818             If None (default), it uses :py:class:`numpy.datetime64` values if the\n8819             date range permits it, and :py:class:`cftime.datetime` objects if not.\n8820             If False, it uses :py:class:`numpy.datetime64`  or fails.\n8821 \n8822         Returns\n8823         -------\n8824         Dataset\n8825             Copy of the dataarray with the time coordinate converted to the\n8826             target calendar. If 'missing' was None (default), invalid dates in\n8827             the new calendar are dropped, but missing dates are not inserted.\n8828             If `missing` was given, the new data is reindexed to have a time axis\n8829             with the same frequency as the source, but in the new calendar; any\n8830             missing datapoints are filled with `missing`.\n8831 \n8832         Notes\n8833         -----\n8834         Passing a value to `missing` is only usable if the source's time coordinate as an\n8835         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate\n8836         if the target coordinate, generated from this frequency, has dates equivalent to the\n8837         source. It is usually **not** appropriate to use this mode with:\n8838 \n8839         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'\n8840         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`\n8841             or 'mH' where 24 % m != 0).\n8842 \n8843         If one of the source or target calendars is `\"360_day\"`, `align_on` must\n8844         be specified and two options are offered.\n8845 \n8846         - \"year\"\n8847             The dates are translated according to their relative position in the year,\n8848             ignoring their original month and day information, meaning that the\n8849             missing/surplus days are added/removed at regular intervals.\n8850 \n8851             From a `360_day` to a standard calendar, the output will be missing the\n8852             following dates (day of year in parentheses):\n8853 \n8854             To a leap year:\n8855                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),\n8856                 September 31st (275) and November 30th (335).\n8857             To a non-leap year:\n8858                 February 6th (36), April 19th (109), July 2nd (183),\n8859                 September 12th (255), November 25th (329).\n8860 \n8861             From a standard calendar to a `\"360_day\"`, the following dates in the\n8862             source array will be dropped:\n8863 \n8864             From a leap year:\n8865                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),\n8866                 September 31st (275), December 1st (336)\n8867             From a non-leap year:\n8868                 February 6th (37), April 20th (110), July 2nd (183),\n8869                 September 13th (256), November 25th (329)\n8870 \n8871             This option is best used on daily and subdaily data.\n8872 \n8873         - \"date\"\n8874             The month/day information is conserved and invalid dates are dropped\n8875             from the output. This means that when converting from a `\"360_day\"` to a\n8876             standard calendar, all 31st (Jan, March, May, July, August, October and\n8877             December) will be missing as there is no equivalent dates in the\n8878             `\"360_day\"` calendar and the 29th (on non-leap years) and 30th of February\n8879             will be dropped as there are no equivalent dates in a standard calendar.\n8880 \n8881             This option is best used with data on a frequency coarser than daily.\n8882         \"\"\"\n8883         return convert_calendar(\n8884             self,\n8885             calendar,\n8886             dim=dim,\n8887             align_on=align_on,\n8888             missing=missing,\n8889             use_cftime=use_cftime,\n8890         )\n8891 \n8892     def interp_calendar(\n8893         self: T_Dataset,\n8894         target: pd.DatetimeIndex | CFTimeIndex | DataArray,\n8895         dim: Hashable = \"time\",\n8896     ) -> T_Dataset:\n8897         \"\"\"Interpolates the Dataset to another calendar based on decimal year measure.\n8898 \n8899         Each timestamp in `source` and `target` are first converted to their decimal\n8900         year equivalent then `source` is interpolated on the target coordinate.\n8901         The decimal year of a timestamp is its year plus its sub-year component\n8902         converted to the fraction of its year. For example \"2000-03-01 12:00\" is\n8903         2000.1653 in a standard calendar or 2000.16301 in a `\"noleap\"` calendar.\n8904 \n8905         This method should only be used when the time (HH:MM:SS) information of\n8906         time coordinate is not important.\n8907 \n8908         Parameters\n8909         ----------\n8910         target: DataArray or DatetimeIndex or CFTimeIndex\n8911             The target time coordinate of a valid dtype\n8912             (np.datetime64 or cftime objects)\n8913         dim : Hashable, default: \"time\"\n8914             The time coordinate name.\n8915 \n8916         Return\n8917         ------\n8918         DataArray\n8919             The source interpolated on the decimal years of target,\n8920         \"\"\"\n8921         return interp_calendar(self, target, dim=dim)\n8922 \n8923     def groupby(\n8924         self,\n8925         group: Hashable | DataArray | IndexVariable,\n8926         squeeze: bool = True,\n8927         restore_coord_dims: bool = False,\n8928     ) -> DatasetGroupBy:\n8929         \"\"\"Returns a DatasetGroupBy object for performing grouped operations.\n8930 \n8931         Parameters\n8932         ----------\n8933         group : Hashable, DataArray or IndexVariable\n8934             Array whose unique values should be used to group this array. If a\n8935             string, must be the name of a variable contained in this dataset.\n8936         squeeze : bool, default: True\n8937             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n8938             controls whether the subarrays have a dimension of length 1 along\n8939             that dimension or if the dimension is squeezed out.\n8940         restore_coord_dims : bool, default: False\n8941             If True, also restore the dimension order of multi-dimensional\n8942             coordinates.\n8943 \n8944         Returns\n8945         -------\n8946         grouped : DatasetGroupBy\n8947             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n8948             iterated over in the form of `(unique_value, grouped_array)` pairs.\n8949 \n8950         See Also\n8951         --------\n8952         :ref:`groupby`\n8953             Users guide explanation of how to group and bin data.\n8954         Dataset.groupby_bins\n8955         DataArray.groupby\n8956         core.groupby.DatasetGroupBy\n8957         pandas.DataFrame.groupby\n8958         Dataset.resample\n8959         DataArray.resample\n8960         \"\"\"\n8961         from xarray.core.groupby import (\n8962             DatasetGroupBy,\n8963             ResolvedUniqueGrouper,\n8964             UniqueGrouper,\n8965             _validate_groupby_squeeze,\n8966         )\n8967 \n8968         _validate_groupby_squeeze(squeeze)\n8969         rgrouper = ResolvedUniqueGrouper(UniqueGrouper(), group, self)\n8970 \n8971         return DatasetGroupBy(\n8972             self,\n8973             (rgrouper,),\n8974             squeeze=squeeze,\n8975             restore_coord_dims=restore_coord_dims,\n8976         )\n8977 \n8978     def groupby_bins(\n8979         self,\n8980         group: Hashable | DataArray | IndexVariable,\n8981         bins: ArrayLike,\n8982         right: bool = True,\n8983         labels: ArrayLike | None = None,\n8984         precision: int = 3,\n8985         include_lowest: bool = False,\n8986         squeeze: bool = True,\n8987         restore_coord_dims: bool = False,\n8988     ) -> DatasetGroupBy:\n8989         \"\"\"Returns a DatasetGroupBy object for performing grouped operations.\n8990 \n8991         Rather than using all unique values of `group`, the values are discretized\n8992         first by applying `pandas.cut` [1]_ to `group`.\n8993 \n8994         Parameters\n8995         ----------\n8996         group : Hashable, DataArray or IndexVariable\n8997             Array whose binned values should be used to group this array. If a\n8998             string, must be the name of a variable contained in this dataset.\n8999         bins : int or array-like\n9000             If bins is an int, it defines the number of equal-width bins in the\n9001             range of x. However, in this case, the range of x is extended by .1%\n9002             on each side to include the min or max values of x. If bins is a\n9003             sequence it defines the bin edges allowing for non-uniform bin\n9004             width. No extension of the range of x is done in this case.\n9005         right : bool, default: True\n9006             Indicates whether the bins include the rightmost edge or not. If\n9007             right == True (the default), then the bins [1,2,3,4] indicate\n9008             (1,2], (2,3], (3,4].\n9009         labels : array-like or bool, default: None\n9010             Used as labels for the resulting bins. Must be of the same length as\n9011             the resulting bins. If False, string bin labels are assigned by\n9012             `pandas.cut`.\n9013         precision : int, default: 3\n9014             The precision at which to store and display the bins labels.\n9015         include_lowest : bool, default: False\n9016             Whether the first interval should be left-inclusive or not.\n9017         squeeze : bool, default: True\n9018             If \"group\" is a dimension of any arrays in this dataset, `squeeze`\n9019             controls whether the subarrays have a dimension of length 1 along\n9020             that dimension or if the dimension is squeezed out.\n9021         restore_coord_dims : bool, default: False\n9022             If True, also restore the dimension order of multi-dimensional\n9023             coordinates.\n9024 \n9025         Returns\n9026         -------\n9027         grouped : DatasetGroupBy\n9028             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be\n9029             iterated over in the form of `(unique_value, grouped_array)` pairs.\n9030             The name of the group has the added suffix `_bins` in order to\n9031             distinguish it from the original variable.\n9032 \n9033         See Also\n9034         --------\n9035         :ref:`groupby`\n9036             Users guide explanation of how to group and bin data.\n9037         Dataset.groupby\n9038         DataArray.groupby_bins\n9039         core.groupby.DatasetGroupBy\n9040         pandas.DataFrame.groupby\n9041 \n9042         References\n9043         ----------\n9044         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\n9045         \"\"\"\n9046         from xarray.core.groupby import (\n9047             BinGrouper,\n9048             DatasetGroupBy,\n9049             ResolvedBinGrouper,\n9050             _validate_groupby_squeeze,\n9051         )\n9052 \n9053         _validate_groupby_squeeze(squeeze)\n9054         grouper = BinGrouper(\n9055             bins=bins,\n9056             cut_kwargs={\n9057                 \"right\": right,\n9058                 \"labels\": labels,\n9059                 \"precision\": precision,\n9060                 \"include_lowest\": include_lowest,\n9061             },\n9062         )\n9063         rgrouper = ResolvedBinGrouper(grouper, group, self)\n9064 \n9065         return DatasetGroupBy(\n9066             self,\n9067             (rgrouper,),\n9068             squeeze=squeeze,\n9069             restore_coord_dims=restore_coord_dims,\n9070         )\n9071 \n9072     def weighted(self, weights: DataArray) -> DatasetWeighted:\n9073         \"\"\"\n9074         Weighted Dataset operations.\n9075 \n9076         Parameters\n9077         ----------\n9078         weights : DataArray\n9079             An array of weights associated with the values in this Dataset.\n9080             Each value in the data contributes to the reduction operation\n9081             according to its associated weight.\n9082 \n9083         Notes\n9084         -----\n9085         ``weights`` must be a DataArray and cannot contain missing values.\n9086         Missing values can be replaced by ``weights.fillna(0)``.\n9087 \n9088         Returns\n9089         -------\n9090         core.weighted.DatasetWeighted\n9091 \n9092         See Also\n9093         --------\n9094         DataArray.weighted\n9095         \"\"\"\n9096         from xarray.core.weighted import DatasetWeighted\n9097 \n9098         return DatasetWeighted(self, weights)\n9099 \n9100     def rolling(\n9101         self,\n9102         dim: Mapping[Any, int] | None = None,\n9103         min_periods: int | None = None,\n9104         center: bool | Mapping[Any, bool] = False,\n9105         **window_kwargs: int,\n9106     ) -> DatasetRolling:\n9107         \"\"\"\n9108         Rolling window object for Datasets.\n9109 \n9110         Parameters\n9111         ----------\n9112         dim : dict, optional\n9113             Mapping from the dimension name to create the rolling iterator\n9114             along (e.g. `time`) to its moving window size.\n9115         min_periods : int or None, default: None\n9116             Minimum number of observations in window required to have a value\n9117             (otherwise result is NA). The default, None, is equivalent to\n9118             setting min_periods equal to the size of the window.\n9119         center : bool or Mapping to int, default: False\n9120             Set the labels at the center of the window.\n9121         **window_kwargs : optional\n9122             The keyword arguments form of ``dim``.\n9123             One of dim or window_kwargs must be provided.\n9124 \n9125         Returns\n9126         -------\n9127         core.rolling.DatasetRolling\n9128 \n9129         See Also\n9130         --------\n9131         core.rolling.DatasetRolling\n9132         DataArray.rolling\n9133         \"\"\"\n9134         from xarray.core.rolling import DatasetRolling\n9135 \n9136         dim = either_dict_or_kwargs(dim, window_kwargs, \"rolling\")\n9137         return DatasetRolling(self, dim, min_periods=min_periods, center=center)\n9138 \n9139     def coarsen(\n9140         self,\n9141         dim: Mapping[Any, int] | None = None,\n9142         boundary: CoarsenBoundaryOptions = \"exact\",\n9143         side: SideOptions | Mapping[Any, SideOptions] = \"left\",\n9144         coord_func: str | Callable | Mapping[Any, str | Callable] = \"mean\",\n9145         **window_kwargs: int,\n9146     ) -> DatasetCoarsen:\n9147         \"\"\"\n9148         Coarsen object for Datasets.\n9149 \n9150         Parameters\n9151         ----------\n9152         dim : mapping of hashable to int, optional\n9153             Mapping from the dimension name to the window size.\n9154         boundary : {\"exact\", \"trim\", \"pad\"}, default: \"exact\"\n9155             If 'exact', a ValueError will be raised if dimension size is not a\n9156             multiple of the window size. If 'trim', the excess entries are\n9157             dropped. If 'pad', NA will be padded.\n9158         side : {\"left\", \"right\"} or mapping of str to {\"left\", \"right\"}, default: \"left\"\n9159         coord_func : str or mapping of hashable to str, default: \"mean\"\n9160             function (name) that is applied to the coordinates,\n9161             or a mapping from coordinate name to function (name).\n9162 \n9163         Returns\n9164         -------\n9165         core.rolling.DatasetCoarsen\n9166 \n9167         See Also\n9168         --------\n9169         core.rolling.DatasetCoarsen\n9170         DataArray.coarsen\n9171         \"\"\"\n9172         from xarray.core.rolling import DatasetCoarsen\n9173 \n9174         dim = either_dict_or_kwargs(dim, window_kwargs, \"coarsen\")\n9175         return DatasetCoarsen(\n9176             self,\n9177             dim,\n9178             boundary=boundary,\n9179             side=side,\n9180             coord_func=coord_func,\n9181         )\n9182 \n9183     def resample(\n9184         self,\n9185         indexer: Mapping[Any, str] | None = None,\n9186         skipna: bool | None = None,\n9187         closed: SideOptions | None = None,\n9188         label: SideOptions | None = None,\n9189         base: int | None = None,\n9190         offset: pd.Timedelta | datetime.timedelta | str | None = None,\n9191         origin: str | DatetimeLike = \"start_day\",\n9192         keep_attrs: bool | None = None,\n9193         loffset: datetime.timedelta | str | None = None,\n9194         restore_coord_dims: bool | None = None,\n9195         **indexer_kwargs: str,\n9196     ) -> DatasetResample:\n9197         \"\"\"Returns a Resample object for performing resampling operations.\n9198 \n9199         Handles both downsampling and upsampling. The resampled\n9200         dimension must be a datetime-like coordinate. If any intervals\n9201         contain no values from the original object, they will be given\n9202         the value ``NaN``.\n9203 \n9204         Parameters\n9205         ----------\n9206         indexer : Mapping of Hashable to str, optional\n9207             Mapping from the dimension name to resample frequency [1]_. The\n9208             dimension must be datetime-like.\n9209         skipna : bool, optional\n9210             Whether to skip missing values when aggregating in downsampling.\n9211         closed : {\"left\", \"right\"}, optional\n9212             Side of each interval to treat as closed.\n9213         label : {\"left\", \"right\"}, optional\n9214             Side of each interval to use for labeling.\n9215         base : int, optional\n9216             For frequencies that evenly subdivide 1 day, the \"origin\" of the\n9217             aggregated intervals. For example, for \"24H\" frequency, base could\n9218             range from 0 through 23.\n9219         origin : {'epoch', 'start', 'start_day', 'end', 'end_day'}, pd.Timestamp, datetime.datetime, np.datetime64, or cftime.datetime, default 'start_day'\n9220             The datetime on which to adjust the grouping. The timezone of origin\n9221             must match the timezone of the index.\n9222 \n9223             If a datetime is not used, these values are also supported:\n9224             - 'epoch': `origin` is 1970-01-01\n9225             - 'start': `origin` is the first value of the timeseries\n9226             - 'start_day': `origin` is the first day at midnight of the timeseries\n9227             - 'end': `origin` is the last value of the timeseries\n9228             - 'end_day': `origin` is the ceiling midnight of the last day\n9229         offset : pd.Timedelta, datetime.timedelta, or str, default is None\n9230             An offset timedelta added to the origin.\n9231         loffset : timedelta or str, optional\n9232             Offset used to adjust the resampled time labels. Some pandas date\n9233             offset strings are supported.\n9234         restore_coord_dims : bool, optional\n9235             If True, also restore the dimension order of multi-dimensional\n9236             coordinates.\n9237         **indexer_kwargs : str\n9238             The keyword arguments form of ``indexer``.\n9239             One of indexer or indexer_kwargs must be provided.\n9240 \n9241         Returns\n9242         -------\n9243         resampled : core.resample.DataArrayResample\n9244             This object resampled.\n9245 \n9246         See Also\n9247         --------\n9248         DataArray.resample\n9249         pandas.Series.resample\n9250         pandas.DataFrame.resample\n9251         Dataset.groupby\n9252         DataArray.groupby\n9253 \n9254         References\n9255         ----------\n9256         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\n9257         \"\"\"\n9258         from xarray.core.resample import DatasetResample\n9259 \n9260         return self._resample(\n9261             resample_cls=DatasetResample,\n9262             indexer=indexer,\n9263             skipna=skipna,\n9264             closed=closed,\n9265             label=label,\n9266             base=base,\n9267             offset=offset,\n9268             origin=origin,\n9269             keep_attrs=keep_attrs,\n9270             loffset=loffset,\n9271             restore_coord_dims=restore_coord_dims,\n9272             **indexer_kwargs,\n9273         )\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "xarray/core/parallel.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__2022.06/xarray/core/parallel.py",
        "start": 1,
        "end": 582,
        "class_name": null,
        "method_name": null,
        "code": "1 from __future__ import annotations\n2 \n3 import collections\n4 import itertools\n5 import operator\n6 from collections.abc import Hashable, Iterable, Mapping, Sequence\n7 from typing import TYPE_CHECKING, Any, Callable\n8 \n9 import numpy as np\n10 \n11 from xarray.core.alignment import align\n12 from xarray.core.dataarray import DataArray\n13 from xarray.core.dataset import Dataset\n14 from xarray.core.pycompat import is_dask_collection\n15 \n16 if TYPE_CHECKING:\n17     from xarray.core.types import T_Xarray\n18 \n19 \n20 def unzip(iterable):\n21     return zip(*iterable)\n22 \n23 \n24 def assert_chunks_compatible(a: Dataset, b: Dataset):\n25     a = a.unify_chunks()\n26     b = b.unify_chunks()\n27 \n28     for dim in set(a.chunks).intersection(set(b.chunks)):\n29         if a.chunks[dim] != b.chunks[dim]:\n30             raise ValueError(f\"Chunk sizes along dimension {dim!r} are not equal.\")\n31 \n32 \n33 def check_result_variables(\n34     result: DataArray | Dataset, expected: Mapping[str, Any], kind: str\n35 ):\n36     if kind == \"coords\":\n37         nice_str = \"coordinate\"\n38     elif kind == \"data_vars\":\n39         nice_str = \"data\"\n40 \n41     # check that coords and data variables are as expected\n42     missing = expected[kind] - set(getattr(result, kind))\n43     if missing:\n44         raise ValueError(\n45             \"Result from applying user function does not contain \"\n46             f\"{nice_str} variables {missing}.\"\n47         )\n48     extra = set(getattr(result, kind)) - expected[kind]\n49     if extra:\n50         raise ValueError(\n51             \"Result from applying user function has unexpected \"\n52             f\"{nice_str} variables {extra}.\"\n53         )\n54 \n55 \n56 def dataset_to_dataarray(obj: Dataset) -> DataArray:\n57     if not isinstance(obj, Dataset):\n58         raise TypeError(f\"Expected Dataset, got {type(obj)}\")\n59 \n60     if len(obj.data_vars) > 1:\n61         raise TypeError(\n62             \"Trying to convert Dataset with more than one data variable to DataArray\"\n63         )\n64 \n65     return next(iter(obj.data_vars.values()))\n66 \n67 \n68 def dataarray_to_dataset(obj: DataArray) -> Dataset:\n69     # only using _to_temp_dataset would break\n70     # func = lambda x: x.to_dataset()\n71     # since that relies on preserving name.\n72     if obj.name is None:\n73         dataset = obj._to_temp_dataset()\n74     else:\n75         dataset = obj.to_dataset()\n76     return dataset\n77 \n78 \n79 def make_meta(obj):\n80     \"\"\"If obj is a DataArray or Dataset, return a new object of the same type and with\n81     the same variables and dtypes, but where all variables have size 0 and numpy\n82     backend.\n83     If obj is neither a DataArray nor Dataset, return it unaltered.\n84     \"\"\"\n85     if isinstance(obj, DataArray):\n86         obj_array = obj\n87         obj = dataarray_to_dataset(obj)\n88     elif isinstance(obj, Dataset):\n89         obj_array = None\n90     else:\n91         return obj\n92 \n93     from dask.array.utils import meta_from_array\n94 \n95     meta = Dataset()\n96     for name, variable in obj.variables.items():\n97         meta_obj = meta_from_array(variable.data, ndim=variable.ndim)\n98         meta[name] = (variable.dims, meta_obj, variable.attrs)\n99     meta.attrs = obj.attrs\n100     meta = meta.set_coords(obj.coords)\n101 \n102     if obj_array is not None:\n103         return dataset_to_dataarray(meta)\n104     return meta\n105 \n106 \n107 def infer_template(\n108     func: Callable[..., T_Xarray], obj: DataArray | Dataset, *args, **kwargs\n109 ) -> T_Xarray:\n110     \"\"\"Infer return object by running the function on meta objects.\"\"\"\n111     meta_args = [make_meta(arg) for arg in (obj,) + args]\n112 \n113     try:\n114         template = func(*meta_args, **kwargs)\n115     except Exception as e:\n116         raise Exception(\n117             \"Cannot infer object returned from running user provided function. \"\n118             \"Please supply the 'template' kwarg to map_blocks.\"\n119         ) from e\n120 \n121     if not isinstance(template, (Dataset, DataArray)):\n122         raise TypeError(\n123             \"Function must return an xarray DataArray or Dataset. Instead it returned \"\n124             f\"{type(template)}\"\n125         )\n126 \n127     return template\n128 \n129 \n130 def make_dict(x: DataArray | Dataset) -> dict[Hashable, Any]:\n131     \"\"\"Map variable name to numpy(-like) data\n132     (Dataset.to_dict() is too complicated).\n133     \"\"\"\n134     if isinstance(x, DataArray):\n135         x = x._to_temp_dataset()\n136 \n137     return {k: v.data for k, v in x.variables.items()}\n138 \n139 \n140 def _get_chunk_slicer(dim: Hashable, chunk_index: Mapping, chunk_bounds: Mapping):\n141     if dim in chunk_index:\n142         which_chunk = chunk_index[dim]\n143         return slice(chunk_bounds[dim][which_chunk], chunk_bounds[dim][which_chunk + 1])\n144     return slice(None)\n145 \n146 \n147 def map_blocks(\n148     func: Callable[..., T_Xarray],\n149     obj: DataArray | Dataset,\n150     args: Sequence[Any] = (),\n151     kwargs: Mapping[str, Any] | None = None,\n152     template: DataArray | Dataset | None = None,\n153 ) -> T_Xarray:\n154     \"\"\"Apply a function to each block of a DataArray or Dataset.\n155 \n156     .. warning::\n157         This function is experimental and its signature may change.\n158 \n159     Parameters\n160     ----------\n161     func : callable\n162         User-provided function that accepts a DataArray or Dataset as its first\n163         parameter ``obj``. The function will receive a subset or 'block' of ``obj`` (see below),\n164         corresponding to one chunk along each chunked dimension. ``func`` will be\n165         executed as ``func(subset_obj, *subset_args, **kwargs)``.\n166 \n167         This function must return either a single DataArray or a single Dataset.\n168 \n169         This function cannot add a new chunked dimension.\n170     obj : DataArray, Dataset\n171         Passed to the function as its first argument, one block at a time.\n172     args : sequence\n173         Passed to func after unpacking and subsetting any xarray objects by blocks.\n174         xarray objects in args must be aligned with obj, otherwise an error is raised.\n175     kwargs : mapping\n176         Passed verbatim to func after unpacking. xarray objects, if any, will not be\n177         subset to blocks. Passing dask collections in kwargs is not allowed.\n178     template : DataArray or Dataset, optional\n179         xarray object representing the final result after compute is called. If not provided,\n180         the function will be first run on mocked-up data, that looks like ``obj`` but\n181         has sizes 0, to determine properties of the returned object such as dtype,\n182         variable names, attributes, new dimensions and new indexes (if any).\n183         ``template`` must be provided if the function changes the size of existing dimensions.\n184         When provided, ``attrs`` on variables in `template` are copied over to the result. Any\n185         ``attrs`` set by ``func`` will be ignored.\n186 \n187     Returns\n188     -------\n189     A single DataArray or Dataset with dask backend, reassembled from the outputs of the\n190     function.\n191 \n192     Notes\n193     -----\n194     This function is designed for when ``func`` needs to manipulate a whole xarray object\n195     subset to each block. Each block is loaded into memory. In the more common case where\n196     ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.\n197 \n198     If none of the variables in ``obj`` is backed by dask arrays, calling this function is\n199     equivalent to calling ``func(obj, *args, **kwargs)``.\n200 \n201     See Also\n202     --------\n203     dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks\n204     xarray.DataArray.map_blocks\n205 \n206     Examples\n207     --------\n208     Calculate an anomaly from climatology using ``.groupby()``. Using\n209     ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,\n210     its indices, and its methods like ``.groupby()``.\n211 \n212     >>> def calculate_anomaly(da, groupby_type=\"time.month\"):\n213     ...     gb = da.groupby(groupby_type)\n214     ...     clim = gb.mean(dim=\"time\")\n215     ...     return gb - clim\n216     ...\n217     >>> time = xr.cftime_range(\"1990-01\", \"1992-01\", freq=\"M\")\n218     >>> month = xr.DataArray(time.month, coords={\"time\": time}, dims=[\"time\"])\n219     >>> np.random.seed(123)\n220     >>> array = xr.DataArray(\n221     ...     np.random.rand(len(time)),\n222     ...     dims=[\"time\"],\n223     ...     coords={\"time\": time, \"month\": month},\n224     ... ).chunk()\n225     >>> array.map_blocks(calculate_anomaly, template=array).compute()\n226     <xarray.DataArray (time: 24)>\n227     array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,\n228             0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,\n229            -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,\n230             0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,\n231             0.07673453,  0.22865714,  0.19063865, -0.0590131 ])\n232     Coordinates:\n233       * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n234         month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12\n235 \n236     Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments\n237     to the function being applied in ``xr.map_blocks()``:\n238 \n239     >>> array.map_blocks(\n240     ...     calculate_anomaly,\n241     ...     kwargs={\"groupby_type\": \"time.year\"},\n242     ...     template=array,\n243     ... )  # doctest: +ELLIPSIS\n244     <xarray.DataArray (time: 24)>\n245     dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>\n246     Coordinates:\n247       * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00\n248         month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>\n249     \"\"\"\n250 \n251     def _wrapper(\n252         func: Callable,\n253         args: list,\n254         kwargs: dict,\n255         arg_is_array: Iterable[bool],\n256         expected: dict,\n257     ):\n258         \"\"\"\n259         Wrapper function that receives datasets in args; converts to dataarrays when necessary;\n260         passes these to the user function `func` and checks returned objects for expected shapes/sizes/etc.\n261         \"\"\"\n262 \n263         converted_args = [\n264             dataset_to_dataarray(arg) if is_array else arg\n265             for is_array, arg in zip(arg_is_array, args)\n266         ]\n267 \n268         result = func(*converted_args, **kwargs)\n269 \n270         # check all dims are present\n271         missing_dimensions = set(expected[\"shapes\"]) - set(result.sizes)\n272         if missing_dimensions:\n273             raise ValueError(\n274                 f\"Dimensions {missing_dimensions} missing on returned object.\"\n275             )\n276 \n277         # check that index lengths and values are as expected\n278         for name, index in result._indexes.items():\n279             if name in expected[\"shapes\"]:\n280                 if result.sizes[name] != expected[\"shapes\"][name]:\n281                     raise ValueError(\n282                         f\"Received dimension {name!r} of length {result.sizes[name]}. \"\n283                         f\"Expected length {expected['shapes'][name]}.\"\n284                     )\n285             if name in expected[\"indexes\"]:\n286                 expected_index = expected[\"indexes\"][name]\n287                 if not index.equals(expected_index):\n288                     raise ValueError(\n289                         f\"Expected index {name!r} to be {expected_index!r}. Received {index!r} instead.\"\n290                     )\n291 \n292         # check that all expected variables were returned\n293         check_result_variables(result, expected, \"coords\")\n294         if isinstance(result, Dataset):\n295             check_result_variables(result, expected, \"data_vars\")\n296 \n297         return make_dict(result)\n298 \n299     if template is not None and not isinstance(template, (DataArray, Dataset)):\n300         raise TypeError(\n301             f\"template must be a DataArray or Dataset. Received {type(template).__name__} instead.\"\n302         )\n303     if not isinstance(args, Sequence):\n304         raise TypeError(\"args must be a sequence (for example, a list or tuple).\")\n305     if kwargs is None:\n306         kwargs = {}\n307     elif not isinstance(kwargs, Mapping):\n308         raise TypeError(\"kwargs must be a mapping (for example, a dict)\")\n309 \n310     for value in kwargs.values():\n311         if is_dask_collection(value):\n312             raise TypeError(\n313                 \"Cannot pass dask collections in kwargs yet. Please compute or \"\n314                 \"load values before passing to map_blocks.\"\n315             )\n316 \n317     if not is_dask_collection(obj):\n318         return func(obj, *args, **kwargs)\n319 \n320     try:\n321         import dask\n322         import dask.array\n323         from dask.highlevelgraph import HighLevelGraph\n324 \n325     except ImportError:\n326         pass\n327 \n328     all_args = [obj] + list(args)\n329     is_xarray = [isinstance(arg, (Dataset, DataArray)) for arg in all_args]\n330     is_array = [isinstance(arg, DataArray) for arg in all_args]\n331 \n332     # there should be a better way to group this. partition?\n333     xarray_indices, xarray_objs = unzip(\n334         (index, arg) for index, arg in enumerate(all_args) if is_xarray[index]\n335     )\n336     others = [\n337         (index, arg) for index, arg in enumerate(all_args) if not is_xarray[index]\n338     ]\n339 \n340     # all xarray objects must be aligned. This is consistent with apply_ufunc.\n341     aligned = align(*xarray_objs, join=\"exact\")\n342     xarray_objs = tuple(\n343         dataarray_to_dataset(arg) if isinstance(arg, DataArray) else arg\n344         for arg in aligned\n345     )\n346 \n347     _, npargs = unzip(\n348         sorted(list(zip(xarray_indices, xarray_objs)) + others, key=lambda x: x[0])\n349     )\n350 \n351     # check that chunk sizes are compatible\n352     input_chunks = dict(npargs[0].chunks)\n353     input_indexes = dict(npargs[0]._indexes)\n354     for arg in xarray_objs[1:]:\n355         assert_chunks_compatible(npargs[0], arg)\n356         input_chunks.update(arg.chunks)\n357         input_indexes.update(arg._indexes)\n358 \n359     if template is None:\n360         # infer template by providing zero-shaped arrays\n361         template = infer_template(func, aligned[0], *args, **kwargs)\n362         template_indexes = set(template._indexes)\n363         preserved_indexes = template_indexes & set(input_indexes)\n364         new_indexes = template_indexes - set(input_indexes)\n365         indexes = {dim: input_indexes[dim] for dim in preserved_indexes}\n366         indexes.update({k: template._indexes[k] for k in new_indexes})\n367         output_chunks: Mapping[Hashable, tuple[int, ...]] = {\n368             dim: input_chunks[dim] for dim in template.dims if dim in input_chunks\n369         }\n370 \n371     else:\n372         # template xarray object has been provided with proper sizes and chunk shapes\n373         indexes = dict(template._indexes)\n374         output_chunks = template.chunksizes\n375         if not output_chunks:\n376             raise ValueError(\n377                 \"Provided template has no dask arrays. \"\n378                 \" Please construct a template with appropriately chunked dask arrays.\"\n379             )\n380 \n381     for dim in output_chunks:\n382         if dim in input_chunks and len(input_chunks[dim]) != len(output_chunks[dim]):\n383             raise ValueError(\n384                 \"map_blocks requires that one block of the input maps to one block of output. \"\n385                 f\"Expected number of output chunks along dimension {dim!r} to be {len(input_chunks[dim])}. \"\n386                 f\"Received {len(output_chunks[dim])} instead. Please provide template if not provided, or \"\n387                 \"fix the provided template.\"\n388             )\n389 \n390     if isinstance(template, DataArray):\n391         result_is_array = True\n392         template_name = template.name\n393         template = template._to_temp_dataset()\n394     elif isinstance(template, Dataset):\n395         result_is_array = False\n396     else:\n397         raise TypeError(\n398             f\"func output must be DataArray or Dataset; got {type(template)}\"\n399         )\n400 \n401     # We're building a new HighLevelGraph hlg. We'll have one new layer\n402     # for each variable in the dataset, which is the result of the\n403     # func applied to the values.\n404 \n405     graph: dict[Any, Any] = {}\n406     new_layers: collections.defaultdict[str, dict[Any, Any]] = collections.defaultdict(\n407         dict\n408     )\n409     gname = \"{}-{}\".format(\n410         dask.utils.funcname(func), dask.base.tokenize(npargs[0], args, kwargs)\n411     )\n412 \n413     # map dims to list of chunk indexes\n414     ichunk = {dim: range(len(chunks_v)) for dim, chunks_v in input_chunks.items()}\n415     # mapping from chunk index to slice bounds\n416     input_chunk_bounds = {\n417         dim: np.cumsum((0,) + chunks_v) for dim, chunks_v in input_chunks.items()\n418     }\n419     output_chunk_bounds = {\n420         dim: np.cumsum((0,) + chunks_v) for dim, chunks_v in output_chunks.items()\n421     }\n422 \n423     def subset_dataset_to_block(\n424         graph: dict, gname: str, dataset: Dataset, input_chunk_bounds, chunk_index\n425     ):\n426         \"\"\"\n427         Creates a task that subsets an xarray dataset to a block determined by chunk_index.\n428         Block extents are determined by input_chunk_bounds.\n429         Also subtasks that subset the constituent variables of a dataset.\n430         \"\"\"\n431 \n432         # this will become [[name1, variable1],\n433         #                   [name2, variable2],\n434         #                   ...]\n435         # which is passed to dict and then to Dataset\n436         data_vars = []\n437         coords = []\n438 \n439         chunk_tuple = tuple(chunk_index.values())\n440         for name, variable in dataset.variables.items():\n441             # make a task that creates tuple of (dims, chunk)\n442             if dask.is_dask_collection(variable.data):\n443                 # recursively index into dask_keys nested list to get chunk\n444                 chunk = variable.__dask_keys__()\n445                 for dim in variable.dims:\n446                     chunk = chunk[chunk_index[dim]]\n447 \n448                 chunk_variable_task = (f\"{name}-{gname}-{chunk[0]}\",) + chunk_tuple\n449                 graph[chunk_variable_task] = (\n450                     tuple,\n451                     [variable.dims, chunk, variable.attrs],\n452                 )\n453             else:\n454                 # non-dask array possibly with dimensions chunked on other variables\n455                 # index into variable appropriately\n456                 subsetter = {\n457                     dim: _get_chunk_slicer(dim, chunk_index, input_chunk_bounds)\n458                     for dim in variable.dims\n459                 }\n460                 subset = variable.isel(subsetter)\n461                 chunk_variable_task = (\n462                     f\"{name}-{gname}-{dask.base.tokenize(subset)}\",\n463                 ) + chunk_tuple\n464                 graph[chunk_variable_task] = (\n465                     tuple,\n466                     [subset.dims, subset, subset.attrs],\n467                 )\n468 \n469             # this task creates dict mapping variable name to above tuple\n470             if name in dataset._coord_names:\n471                 coords.append([name, chunk_variable_task])\n472             else:\n473                 data_vars.append([name, chunk_variable_task])\n474 \n475         return (Dataset, (dict, data_vars), (dict, coords), dataset.attrs)\n476 \n477     # iterate over all possible chunk combinations\n478     for chunk_tuple in itertools.product(*ichunk.values()):\n479         # mapping from dimension name to chunk index\n480         chunk_index = dict(zip(ichunk.keys(), chunk_tuple))\n481 \n482         blocked_args = [\n483             subset_dataset_to_block(graph, gname, arg, input_chunk_bounds, chunk_index)\n484             if isxr\n485             else arg\n486             for isxr, arg in zip(is_xarray, npargs)\n487         ]\n488 \n489         # expected[\"shapes\", \"coords\", \"data_vars\", \"indexes\"] are used to\n490         # raise nice error messages in _wrapper\n491         expected = {}\n492         # input chunk 0 along a dimension maps to output chunk 0 along the same dimension\n493         # even if length of dimension is changed by the applied function\n494         expected[\"shapes\"] = {\n495             k: output_chunks[k][v] for k, v in chunk_index.items() if k in output_chunks\n496         }\n497         expected[\"data_vars\"] = set(template.data_vars.keys())  # type: ignore[assignment]\n498         expected[\"coords\"] = set(template.coords.keys())  # type: ignore[assignment]\n499         expected[\"indexes\"] = {\n500             dim: indexes[dim][_get_chunk_slicer(dim, chunk_index, output_chunk_bounds)]\n501             for dim in indexes\n502         }\n503 \n504         from_wrapper = (gname,) + chunk_tuple\n505         graph[from_wrapper] = (_wrapper, func, blocked_args, kwargs, is_array, expected)\n506 \n507         # mapping from variable name to dask graph key\n508         var_key_map: dict[Hashable, str] = {}\n509         for name, variable in template.variables.items():\n510             if name in indexes:\n511                 continue\n512             gname_l = f\"{name}-{gname}\"\n513             var_key_map[name] = gname_l\n514 \n515             key: tuple[Any, ...] = (gname_l,)\n516             for dim in variable.dims:\n517                 if dim in chunk_index:\n518                     key += (chunk_index[dim],)\n519                 else:\n520                     # unchunked dimensions in the input have one chunk in the result\n521                     # output can have new dimensions with exactly one chunk\n522                     key += (0,)\n523 \n524             # We're adding multiple new layers to the graph:\n525             # The first new layer is the result of the computation on\n526             # the array.\n527             # Then we add one layer per variable, which extracts the\n528             # result for that variable, and depends on just the first new\n529             # layer.\n530             new_layers[gname_l][key] = (operator.getitem, from_wrapper, name)\n531 \n532     hlg = HighLevelGraph.from_collections(\n533         gname,\n534         graph,\n535         dependencies=[arg for arg in npargs if dask.is_dask_collection(arg)],\n536     )\n537 \n538     # This adds in the getitems for each variable in the dataset.\n539     hlg = HighLevelGraph(\n540         {**hlg.layers, **new_layers},\n541         dependencies={\n542             **hlg.dependencies,\n543             **{name: {gname} for name in new_layers.keys()},\n544         },\n545     )\n546 \n547     # TODO: benbovy - flexible indexes: make it work with custom indexes\n548     # this will need to pass both indexes and coords to the Dataset constructor\n549     result = Dataset(\n550         coords={k: idx.to_pandas_index() for k, idx in indexes.items()},\n551         attrs=template.attrs,\n552     )\n553 \n554     for index in result._indexes:\n555         result[index].attrs = template[index].attrs\n556         result[index].encoding = template[index].encoding\n557 \n558     for name, gname_l in var_key_map.items():\n559         dims = template[name].dims\n560         var_chunks = []\n561         for dim in dims:\n562             if dim in output_chunks:\n563                 var_chunks.append(output_chunks[dim])\n564             elif dim in result._indexes:\n565                 var_chunks.append((result.sizes[dim],))\n566             elif dim in template.dims:\n567                 # new unindexed dimension\n568                 var_chunks.append((template.sizes[dim],))\n569 \n570         data = dask.array.Array(\n571             hlg, name=gname_l, chunks=var_chunks, dtype=template[name].dtype\n572         )\n573         result[name] = (dims, data, template[name].attrs)\n574         result[name].encoding = template[name].encoding\n575 \n576     result = result.set_coords(template._coord_names)\n577 \n578     if result_is_array:\n579         da = dataset_to_dataarray(result)\n580         da.name = template_name\n581         return da  # type: ignore[return-value]\n582     return result  # type: ignore[return-value]\n",
        "intended_behavior": "As this file likely deals with parallel computation directly, it might be beneficial to review and potentially refactor the integration points with `dask` to establish a more generalized parallel computation interface. This could involve creating an abstraction layer or interface that can interact with any parallel computation backend. Implementing such an interface would allow `darray` to leverage functionalities like `map_blocks`, `blockwise` operations seamlessly across different computation backends."
    }
]