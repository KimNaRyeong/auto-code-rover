[
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 2220,
        "end": 2552,
        "class_name": "GradientBoostingRegressor",
        "method_name": null,
        "code": "2220 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):\n2221     \"\"\"Gradient Boosting for regression.\n2222 \n2223     GB builds an additive model in a forward stage-wise fashion;\n2224     it allows for the optimization of arbitrary differentiable loss functions.\n2225     In each stage a regression tree is fit on the negative gradient of the\n2226     given loss function.\n2227 \n2228     Read more in the :ref:`User Guide <gradient_boosting>`.\n2229 \n2230     Parameters\n2231     ----------\n2232     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n2233         loss function to be optimized. 'ls' refers to least squares\n2234         regression. 'lad' (least absolute deviation) is a highly robust\n2235         loss function solely based on order information of the input\n2236         variables. 'huber' is a combination of the two. 'quantile'\n2237         allows quantile regression (use `alpha` to specify the quantile).\n2238 \n2239     learning_rate : float, optional (default=0.1)\n2240         learning rate shrinks the contribution of each tree by `learning_rate`.\n2241         There is a trade-off between learning_rate and n_estimators.\n2242 \n2243     n_estimators : int (default=100)\n2244         The number of boosting stages to perform. Gradient boosting\n2245         is fairly robust to over-fitting so a large number usually\n2246         results in better performance.\n2247 \n2248     subsample : float, optional (default=1.0)\n2249         The fraction of samples to be used for fitting the individual base\n2250         learners. If smaller than 1.0 this results in Stochastic Gradient\n2251         Boosting. `subsample` interacts with the parameter `n_estimators`.\n2252         Choosing `subsample < 1.0` leads to a reduction of variance\n2253         and an increase in bias.\n2254 \n2255     criterion : string, optional (default=\"friedman_mse\")\n2256         The function to measure the quality of a split. Supported criteria\n2257         are \"friedman_mse\" for the mean squared error with improvement\n2258         score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n2259         the mean absolute error. The default value of \"friedman_mse\" is\n2260         generally the best as it can provide a better approximation in\n2261         some cases.\n2262 \n2263         .. versionadded:: 0.18\n2264 \n2265     min_samples_split : int, float, optional (default=2)\n2266         The minimum number of samples required to split an internal node:\n2267 \n2268         - If int, then consider `min_samples_split` as the minimum number.\n2269         - If float, then `min_samples_split` is a fraction and\n2270           `ceil(min_samples_split * n_samples)` are the minimum\n2271           number of samples for each split.\n2272 \n2273         .. versionchanged:: 0.18\n2274            Added float values for fractions.\n2275 \n2276     min_samples_leaf : int, float, optional (default=1)\n2277         The minimum number of samples required to be at a leaf node.\n2278         A split point at any depth will only be considered if it leaves at\n2279         least ``min_samples_leaf`` training samples in each of the left and\n2280         right branches.  This may have the effect of smoothing the model,\n2281         especially in regression.\n2282 \n2283         - If int, then consider `min_samples_leaf` as the minimum number.\n2284         - If float, then `min_samples_leaf` is a fraction and\n2285           `ceil(min_samples_leaf * n_samples)` are the minimum\n2286           number of samples for each node.\n2287 \n2288         .. versionchanged:: 0.18\n2289            Added float values for fractions.\n2290 \n2291     min_weight_fraction_leaf : float, optional (default=0.)\n2292         The minimum weighted fraction of the sum total of weights (of all\n2293         the input samples) required to be at a leaf node. Samples have\n2294         equal weight when sample_weight is not provided.\n2295 \n2296     max_depth : integer, optional (default=3)\n2297         maximum depth of the individual regression estimators. The maximum\n2298         depth limits the number of nodes in the tree. Tune this parameter\n2299         for best performance; the best value depends on the interaction\n2300         of the input variables.\n2301 \n2302     min_impurity_decrease : float, optional (default=0.)\n2303         A node will be split if this split induces a decrease of the impurity\n2304         greater than or equal to this value.\n2305 \n2306         The weighted impurity decrease equation is the following::\n2307 \n2308             N_t / N * (impurity - N_t_R / N_t * right_impurity\n2309                                 - N_t_L / N_t * left_impurity)\n2310 \n2311         where ``N`` is the total number of samples, ``N_t`` is the number of\n2312         samples at the current node, ``N_t_L`` is the number of samples in the\n2313         left child, and ``N_t_R`` is the number of samples in the right child.\n2314 \n2315         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n2316         if ``sample_weight`` is passed.\n2317 \n2318         .. versionadded:: 0.19\n2319 \n2320     min_impurity_split : float, (default=1e-7)\n2321         Threshold for early stopping in tree growth. A node will split\n2322         if its impurity is above the threshold, otherwise it is a leaf.\n2323 \n2324         .. deprecated:: 0.19\n2325            ``min_impurity_split`` has been deprecated in favor of\n2326            ``min_impurity_decrease`` in 0.19. The default value of\n2327            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n2328            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n2329 \n2330     init : estimator or 'zero', optional (default=None)\n2331         An estimator object that is used to compute the initial predictions.\n2332         ``init`` has to provide `fit` and `predict`. If 'zero', the initial\n2333         raw predictions are set to zero. By default a ``DummyEstimator`` is\n2334         used, predicting either the average target value (for loss='ls'), or\n2335         a quantile for the other losses.\n2336 \n2337     random_state : int, RandomState instance or None, optional (default=None)\n2338         If int, random_state is the seed used by the random number generator;\n2339         If RandomState instance, random_state is the random number generator;\n2340         If None, the random number generator is the RandomState instance used\n2341         by `np.random`.\n2342 \n2343     max_features : int, float, string or None, optional (default=None)\n2344         The number of features to consider when looking for the best split:\n2345 \n2346         - If int, then consider `max_features` features at each split.\n2347         - If float, then `max_features` is a fraction and\n2348           `int(max_features * n_features)` features are considered at each\n2349           split.\n2350         - If \"auto\", then `max_features=n_features`.\n2351         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n2352         - If \"log2\", then `max_features=log2(n_features)`.\n2353         - If None, then `max_features=n_features`.\n2354 \n2355         Choosing `max_features < n_features` leads to a reduction of variance\n2356         and an increase in bias.\n2357 \n2358         Note: the search for a split does not stop until at least one\n2359         valid partition of the node samples is found, even if it requires to\n2360         effectively inspect more than ``max_features`` features.\n2361 \n2362     alpha : float (default=0.9)\n2363         The alpha-quantile of the huber loss function and the quantile\n2364         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n2365 \n2366     verbose : int, default: 0\n2367         Enable verbose output. If 1 then it prints progress and performance\n2368         once in a while (the more trees the lower the frequency). If greater\n2369         than 1 then it prints progress and performance for every tree.\n2370 \n2371     max_leaf_nodes : int or None, optional (default=None)\n2372         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n2373         Best nodes are defined as relative reduction in impurity.\n2374         If None then unlimited number of leaf nodes.\n2375 \n2376     warm_start : bool, default: False\n2377         When set to ``True``, reuse the solution of the previous call to fit\n2378         and add more estimators to the ensemble, otherwise, just erase the\n2379         previous solution. See :term:`the Glossary <warm_start>`.\n2380 \n2381     presort : bool or 'auto', optional (default='auto')\n2382         Whether to presort the data to speed up the finding of best splits in\n2383         fitting. Auto mode by default will use presorting on dense data and\n2384         default to normal sorting on sparse data. Setting presort to true on\n2385         sparse data will raise an error.\n2386 \n2387         .. versionadded:: 0.17\n2388            optional parameter *presort*.\n2389 \n2390     validation_fraction : float, optional, default 0.1\n2391         The proportion of training data to set aside as validation set for\n2392         early stopping. Must be between 0 and 1.\n2393         Only used if ``n_iter_no_change`` is set to an integer.\n2394 \n2395         .. versionadded:: 0.20\n2396 \n2397     n_iter_no_change : int, default None\n2398         ``n_iter_no_change`` is used to decide if early stopping will be used\n2399         to terminate training when validation score is not improving. By\n2400         default it is set to None to disable early stopping. If set to a\n2401         number, it will set aside ``validation_fraction`` size of the training\n2402         data as validation and terminate training when validation score is not\n2403         improving in all of the previous ``n_iter_no_change`` numbers of\n2404         iterations.\n2405 \n2406         .. versionadded:: 0.20\n2407 \n2408     tol : float, optional, default 1e-4\n2409         Tolerance for the early stopping. When the loss is not improving\n2410         by at least tol for ``n_iter_no_change`` iterations (if set to a\n2411         number), the training stops.\n2412 \n2413         .. versionadded:: 0.20\n2414 \n2415 \n2416     Attributes\n2417     ----------\n2418     feature_importances_ : array, shape (n_features,)\n2419         The feature importances (the higher, the more important the feature).\n2420 \n2421     oob_improvement_ : array, shape (n_estimators,)\n2422         The improvement in loss (= deviance) on the out-of-bag samples\n2423         relative to the previous iteration.\n2424         ``oob_improvement_[0]`` is the improvement in\n2425         loss of the first stage over the ``init`` estimator.\n2426 \n2427     train_score_ : array, shape (n_estimators,)\n2428         The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n2429         model at iteration ``i`` on the in-bag sample.\n2430         If ``subsample == 1`` this is the deviance on the training data.\n2431 \n2432     loss_ : LossFunction\n2433         The concrete ``LossFunction`` object.\n2434 \n2435     init_ : estimator\n2436         The estimator that provides the initial predictions.\n2437         Set via the ``init`` argument or ``loss.init_estimator``.\n2438 \n2439     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n2440         The collection of fitted sub-estimators.\n2441 \n2442     Notes\n2443     -----\n2444     The features are always randomly permuted at each split. Therefore,\n2445     the best found split may vary, even with the same training data and\n2446     ``max_features=n_features``, if the improvement of the criterion is\n2447     identical for several splits enumerated during the search of the best\n2448     split. To obtain a deterministic behaviour during fitting,\n2449     ``random_state`` has to be fixed.\n2450 \n2451     See also\n2452     --------\n2453     DecisionTreeRegressor, RandomForestRegressor\n2454 \n2455     References\n2456     ----------\n2457     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n2458     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n2459 \n2460     J. Friedman, Stochastic Gradient Boosting, 1999\n2461 \n2462     T. Hastie, R. Tibshirani and J. Friedman.\n2463     Elements of Statistical Learning Ed. 2, Springer, 2009.\n2464     \"\"\"\n2465 \n2466     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')\n2467 \n2468     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,\n2469                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n2470                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n2471                  max_depth=3, min_impurity_decrease=0.,\n2472                  min_impurity_split=None, init=None, random_state=None,\n2473                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n2474                  warm_start=False, presort='auto', validation_fraction=0.1,\n2475                  n_iter_no_change=None, tol=1e-4):\n2476 \n2477         super().__init__(\n2478             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n2479             criterion=criterion, min_samples_split=min_samples_split,\n2480             min_samples_leaf=min_samples_leaf,\n2481             min_weight_fraction_leaf=min_weight_fraction_leaf,\n2482             max_depth=max_depth, init=init, subsample=subsample,\n2483             max_features=max_features,\n2484             min_impurity_decrease=min_impurity_decrease,\n2485             min_impurity_split=min_impurity_split,\n2486             random_state=random_state, alpha=alpha, verbose=verbose,\n2487             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n2488             presort=presort, validation_fraction=validation_fraction,\n2489             n_iter_no_change=n_iter_no_change, tol=tol)\n2490 \n2491     def predict(self, X):\n2492         \"\"\"Predict regression target for X.\n2493 \n2494         Parameters\n2495         ----------\n2496         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2497             The input samples. Internally, it will be converted to\n2498             ``dtype=np.float32`` and if a sparse matrix is provided\n2499             to a sparse ``csr_matrix``.\n2500 \n2501         Returns\n2502         -------\n2503         y : array, shape (n_samples,)\n2504             The predicted values.\n2505         \"\"\"\n2506         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n2507         # In regression we can directly return the raw value from the trees.\n2508         return self._raw_predict(X).ravel()\n2509 \n2510     def staged_predict(self, X):\n2511         \"\"\"Predict regression target at each stage for X.\n2512 \n2513         This method allows monitoring (i.e. determine error on testing set)\n2514         after each stage.\n2515 \n2516         Parameters\n2517         ----------\n2518         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2519             The input samples. Internally, it will be converted to\n2520             ``dtype=np.float32`` and if a sparse matrix is provided\n2521             to a sparse ``csr_matrix``.\n2522 \n2523         Returns\n2524         -------\n2525         y : generator of array of shape (n_samples,)\n2526             The predicted value of the input samples.\n2527         \"\"\"\n2528         for raw_predictions in self._staged_raw_predict(X):\n2529             yield raw_predictions.ravel()\n2530 \n2531     def apply(self, X):\n2532         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n2533 \n2534         .. versionadded:: 0.17\n2535 \n2536         Parameters\n2537         ----------\n2538         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2539             The input samples. Internally, its dtype will be converted to\n2540             ``dtype=np.float32``. If a sparse matrix is provided, it will\n2541             be converted to a sparse ``csr_matrix``.\n2542 \n2543         Returns\n2544         -------\n2545         X_leaves : array-like, shape (n_samples, n_estimators)\n2546             For each datapoint x in X and for each tree in the ensemble,\n2547             return the index of the leaf x ends up in each estimator.\n2548         \"\"\"\n2549 \n2550         leaves = super().apply(X)\n2551         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n2552         return leaves\n",
        "intended_behavior": "The code within this location, specifically where it calls self.init_.fit(X, y, sample_weight), is intended to correctly initialize the regressor using the provided initial estimator. To resolve the issue, the method should be modified to ensure compatibility with the signature of Pipeline.fit. This means adjusting the calling convention to pass sample_weight as a named argument within **fit_params if self.init_ is indeed a pipeline (or supports **fit_params). This change would allow users to use a Pipeline as the initial estimator without encountering a TypeError. The modified implementation should check if self.init_ can accept sample_weight directly or as part of **fit_params and then adapt the call accordingly. This ensures that the functionality is preserved when using custom estimators as the initial model while adding support for Pipeline objects. The check could involve inspecting the fit method signature of self.init_ or safely attempting to pass sample_weight and adjusting based on the outcome. This solution seeks to maintain existing functionalities for all current users of GradientBoostingRegressor while extending its flexibility to allow Pipeline objects as initial estimators, thus solving the reported issue without breaking compatibility with other parts of the scikit-learn framework."
    }
]