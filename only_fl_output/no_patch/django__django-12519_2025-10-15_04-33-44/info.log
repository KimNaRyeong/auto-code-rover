2025-10-15 04:33:44.910 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 04:33:47.913 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1759, output_tokens=12, cost=0.017950
2025-10-15 04:33:47.914 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 04:33:47.918 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 04:33:58.720 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1870, output_tokens=413, cost=0.031090
2025-10-15 04:33:58.746 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 04:34:01.284 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=767, output_tokens=50, cost=0.009170
2025-10-15 04:34:01.285 | DEBUG    | Extracted a valid json.
2025-10-15 04:34:01.285 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"annotate\")",
        "search_method(\"aggregate\")",
        "search_class(\"SQLCompiler\")",
        "search_code(\"group by\")"
    ],
    "bug_locations": []
}
2025-10-15 04:34:01.345 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 04:34:01.345 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 04:34:22.966 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=4538, output_tokens=968, cost=0.074420
2025-10-15 04:34:22.984 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 04:34:32.236 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5722, output_tokens=424, cost=0.069940
2025-10-15 04:34:32.244 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 04:34:40.988 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=778, output_tokens=386, cost=0.019360
2025-10-15 04:34:40.989 | DEBUG    | Extracted a valid json.
2025-10-15 04:34:40.990 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "The `annotate` method should ensure that when generating the SQL for a queryset with multiple annotations, especially those that include subqueries, these annotations are correctly included in the GROUP BY section of the SQL query. This inclusion is critical to prevent duplicate results due to the omission of essential fields from the GROUP BY clause. To resolve the issue, the method may require adjustments in how it constructs the queryset's annotations, ensuring that subquery annotations are not omitted in any scenario. The fix should preserve the functionality of allowing complex annotations and aggregates within the same query, without introducing duplicates in the returned results."
        },
        {
            "file": "django/db/models/sql/compiler.py",
            "class": "SQLCompiler",
            "method": "",
            "intended_behavior": "The SQLCompiler class may need a review or enhancement in the methods responsible for constructing the GROUP BY clause, such as `get_group_by`, ensuring that all conditions leading to the correct inclusion of annotations, particularly subquery annotations, are met. The intended behavior is to enhance the logic so that it accurately recognizes when annotations (especially those involving subqueries) necessitate inclusion in the GROUP BY clause to avoid result set duplication and maintain consistency with previous Django versions while still allowing for optimizations introduced in newer versions. This might include refining how the compiler assesses and decides the fields and expressions to include in the GROUP BY clause based on the annotations present in the queryset. The adjustments described aim to correct the regression, ensuring that the ORM's handling of subquery annotations in conjunction with GROUP BY clauses is both accurate and consistent with expected behavior, thus preserving the robustness and reliability of the Django ORM across versions."
        }
    ]
}
2025-10-15 04:34:41.006 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1061     def annotate(self, *args, **kwargs):
1062         """
1063         Return a query set in which the returned objects have been annotated
1064         with extra data or aggregations.
1065         """
1066         self._not_support_combined_queries('annotate')
1067         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1068         annotations = {}
1069         for arg in args:
1070             # The default_alias property may raise a TypeError.
1071             try:
1072                 if arg.default_alias in kwargs:
1073                     raise ValueError("The named annotation '%s' conflicts with the "
1074                                      "default name for another annotation."
1075                                      % arg.default_alias)
1076             except TypeError:
1077                 raise TypeError("Complex annotations require an alias")
1078             annotations[arg.default_alias] = arg
1079         annotations.update(kwargs)
1080 
1081         clone = self._chain()
1082         names = self._fields
1083         if names is None:
1084             names = set(chain.from_iterable(
1085                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1086                 for field in self.model._meta.get_fields()
1087             ))
1088 
1089         for alias, annotation in annotations.items():
1090             if alias in names:
1091                 raise ValueError("The annotation '%s' conflicts with a field on "
1092                                  "the model." % alias)
1093             if isinstance(annotation, FilteredRelation):
1094                 clone.query.add_filtered_relation(annotation, alias)
1095             else:
1096                 clone.query.add_annotation(annotation, alias, is_summary=False)
1097 
1098         for alias, annotation in clone.query.annotations.items():
1099             if alias in annotations and annotation.contains_aggregate:
1100                 if clone._fields is None:
1101                     clone.query.group_by = True
1102                 else:
1103                     clone.query.set_group_by()
1104                 break
1105 
1106         return clone

</code><intended_behavior>The `annotate` method should ensure that when generating the SQL for a queryset with multiple annotations, especially those that include subqueries, these annotations are correctly included in the GROUP BY section of the SQL query. This inclusion is critical to prevent duplicate results due to the omission of essential fields from the GROUP BY clause. To resolve the issue, the method may require adjustments in how it constructs the queryset's annotations, ensuring that subquery annotations are not omitted in any scenario. The fix should preserve the functionality of allowing complex annotations and aggregates within the same query, without introducing duplicates in the returned results.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
242 
243     def __setstate__(self, state):
244         msg = None
245         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
246         if pickled_version:
247             current_version = get_version()
248             if current_version != pickled_version:
249                 msg = (
250                     "Pickled queryset instance's Django version %s does not "
251                     "match the current version %s." % (pickled_version, current_version)
252                 )
253         else:
254             msg = "Pickled queryset instance's Django version is not specified."
255 
256         if msg:
257             warnings.warn(msg, RuntimeWarning, stacklevel=2)
258 
259         self.__dict__.update(state)
260 
261     def __repr__(self):
262         data = list(self[:REPR_OUTPUT_SIZE + 1])
263         if len(data) > REPR_OUTPUT_SIZE:
264             data[-1] = "...(remaining elements truncated)..."
265         return '<%s %r>' % (self.__class__.__name__, data)
266 
267     def __len__(self):
268         self._fetch_all()
269         return len(self._result_cache)
270 
271     def __iter__(self):
272         """
273         The queryset iterator protocol uses three nested iterators in the
274         default case:
275             1. sql.compiler.execute_sql()
276                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
277                  using cursor.fetchmany(). This part is responsible for
278                  doing some column masking, and returning the rows in chunks.
279             2. sql.compiler.results_iter()
280                - Returns one row at time. At this point the rows are still just
281                  tuples. In some cases the return values are converted to
282                  Python values at this location.
283             3. self.iterator()
284                - Responsible for turning the rows into model objects.
285         """
286         self._fetch_all()
287         return iter(self._result_cache)
288 
289     def __bool__(self):
290         self._fetch_all()
291         return bool(self._result_cache)
292 
293     def __getitem__(self, k):
294         """Retrieve an item or slice from the set of results."""
295         if not isinstance(k, (int, slice)):
296             raise TypeError(
297                 'QuerySet indices must be integers or slices, not %s.'
298                 % type(k).__name__
299             )
300         assert ((not isinstance(k, slice) and (k >= 0)) or
301                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
302                  (k.stop is None or k.stop >= 0))), \
303             "Negative indexing is not supported."
304 
305         if self._result_cache is not None:
306             return self._result_cache[k]
307 
308         if isinstance(k, slice):
309             qs = self._chain()
310             if k.start is not None:
311                 start = int(k.start)
312             else:
313                 start = None
314             if k.stop is not None:
315                 stop = int(k.stop)
316             else:
317                 stop = None
318             qs.query.set_limits(start, stop)
319             return list(qs)[::k.step] if k.step else qs
320 
321         qs = self._chain()
322         qs.query.set_limits(k, k + 1)
323         qs._fetch_all()
324         return qs._result_cache[0]
325 
326     def __and__(self, other):
327         self._merge_sanity_check(other)
328         if isinstance(other, EmptyQuerySet):
329             return other
330         if isinstance(self, EmptyQuerySet):
331             return self
332         combined = self._chain()
333         combined._merge_known_related_objects(other)
334         combined.query.combine(other.query, sql.AND)
335         return combined
336 
337     def __or__(self, other):
338         self._merge_sanity_check(other)
339         if isinstance(self, EmptyQuerySet):
340             return other
341         if isinstance(other, EmptyQuerySet):
342             return self
343         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
344         combined = query._chain()
345         combined._merge_known_related_objects(other)
346         if not other.query.can_filter():
347             other = other.model._base_manager.filter(pk__in=other.values('pk'))
348         combined.query.combine(other.query, sql.OR)
349         return combined
350 
351     ####################################
352     # METHODS THAT DO DATABASE QUERIES #
353     ####################################
354 
355     def _iterator(self, use_chunked_fetch, chunk_size):
356         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
357 
358     def iterator(self, chunk_size=2000):
359         """
360         An iterator over the results from applying this QuerySet to the
361         database.
362         """
363         if chunk_size <= 0:
364             raise ValueError('Chunk size must be strictly positive.')
365         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
366         return self._iterator(use_chunked_fetch, chunk_size)
367 
368     def aggregate(self, *args, **kwargs):
369         """
370         Return a dictionary containing the calculations (aggregation)
371         over the current queryset.
372 
373         If args is present the expression is passed as a kwarg using
374         the Aggregate object's default alias.
375         """
376         if self.query.distinct_fields:
377             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
378         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
379         for arg in args:
380             # The default_alias property raises TypeError if default_alias
381             # can't be set automatically or AttributeError if it isn't an
382             # attribute.
383             try:
384                 arg.default_alias
385             except (AttributeError, TypeError):
386                 raise TypeError("Complex aggregates require an alias")
387             kwargs[arg.default_alias] = arg
388 
389         query = self.query.chain()
390         for (alias, aggregate_expr) in kwargs.items():
391             query.add_annotation(aggregate_expr, alias, is_summary=True)
392             if not query.annotations[alias].contains_aggregate:
393                 raise TypeError("%s is not an aggregate expression" % alias)
394         return query.get_aggregation(self.db, kwargs)
395 
396     def count(self):
397         """
398         Perform a SELECT COUNT() and return the number of records as an
399         integer.
400 
401         If the QuerySet is already fully cached, return the length of the
402         cached results set to avoid multiple SELECT COUNT(*) calls.
403         """
404         if self._result_cache is not None:
405             return len(self._result_cache)
406 
407         return self.query.get_count(using=self.db)
408 
409     def get(self, *args, **kwargs):
410         """
411         Perform the query and return a single object matching the given
412         keyword arguments.
413         """
414         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
415         if self.query.can_filter() and not self.query.distinct_fields:
416             clone = clone.order_by()
417         limit = None
418         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
419             limit = MAX_GET_RESULTS
420             clone.query.set_limits(high=limit)
421         num = len(clone)
422         if num == 1:
423             return clone._result_cache[0]
424         if not num:
425             raise self.model.DoesNotExist(
426                 "%s matching query does not exist." %
427                 self.model._meta.object_name
428             )
429         raise self.model.MultipleObjectsReturned(
430             'get() returned more than one %s -- it returned %s!' % (
431                 self.model._meta.object_name,
432                 num if not limit or num < limit else 'more than %s' % (limit - 1),
433             )
434         )
435 
436     def create(self, **kwargs):
437         """
438         Create a new object with the given kwargs, saving it to the database
439         and returning the created object.
440         """
441         obj = self.model(**kwargs)
442         self._for_write = True
443         obj.save(force_insert=True, using=self.db)
444         return obj
445 
446     def _populate_pk_values(self, objs):
447         for obj in objs:
448             if obj.pk is None:
449                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
450 
451     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
452         """
453         Insert each of the instances into the database. Do *not* call
454         save() on each of the instances, do not send any pre/post_save
455         signals, and do not set the primary key attribute if it is an
456         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
457         Multi-table models are not supported.
458         """
459         # When you bulk insert you don't get the primary keys back (if it's an
460         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
461         # you can't insert into the child tables which references this. There
462         # are two workarounds:
463         # 1) This could be implemented if you didn't have an autoincrement pk
464         # 2) You could do it by doing O(n) normal inserts into the parent
465         #    tables to get the primary keys back and then doing a single bulk
466         #    insert into the childmost table.
467         # We currently set the primary keys on the objects when using
468         # PostgreSQL via the RETURNING ID clause. It should be possible for
469         # Oracle as well, but the semantics for extracting the primary keys is
470         # trickier so it's not done yet.
471         assert batch_size is None or batch_size > 0
472         # Check that the parents share the same concrete model with the our
473         # model to detect the inheritance pattern ConcreteGrandParent ->
474         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
475         # would not identify that case as involving multiple tables.
476         for parent in self.model._meta.get_parent_list():
477             if parent._meta.concrete_model is not self.model._meta.concrete_model:
478                 raise ValueError("Can't bulk create a multi-table inherited model")
479         if not objs:
480             return objs
481         self._for_write = True
482         connection = connections[self.db]
483         opts = self.model._meta
484         fields = opts.concrete_fields
485         objs = list(objs)
486         self._populate_pk_values(objs)
487         with transaction.atomic(using=self.db, savepoint=False):
488             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
489             if objs_with_pk:
490                 returned_columns = self._batched_insert(
491                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
492                 )
493                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
494                     for result, field in zip(results, opts.db_returning_fields):
495                         if field != opts.pk:
496                             setattr(obj_with_pk, field.attname, result)
497                 for obj_with_pk in objs_with_pk:
498                     obj_with_pk._state.adding = False
499                     obj_with_pk._state.db = self.db
500             if objs_without_pk:
501                 fields = [f for f in fields if not isinstance(f, AutoField)]
502                 returned_columns = self._batched_insert(
503                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
506                     assert len(returned_columns) == len(objs_without_pk)
507                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
508                     for result, field in zip(results, opts.db_returning_fields):
509                         setattr(obj_without_pk, field.attname, result)
510                     obj_without_pk._state.adding = False
511                     obj_without_pk._state.db = self.db
512 
513         return objs
514 
515     def bulk_update(self, objs, fields, batch_size=None):
516         """
517         Update the given fields in each of the given objects in the database.
518         """
519         if batch_size is not None and batch_size < 0:
520             raise ValueError('Batch size must be a positive integer.')
521         if not fields:
522             raise ValueError('Field names must be given to bulk_update().')
523         objs = tuple(objs)
524         if any(obj.pk is None for obj in objs):
525             raise ValueError('All bulk_update() objects must have a primary key set.')
526         fields = [self.model._meta.get_field(name) for name in fields]
527         if any(not f.concrete or f.many_to_many for f in fields):
528             raise ValueError('bulk_update() can only be used with concrete fields.')
529         if any(f.primary_key for f in fields):
530             raise ValueError('bulk_update() cannot be used with primary key fields.')
531         if not objs:
532             return
533         # PK is used twice in the resulting update query, once in the filter
534         # and once in the WHEN. Each field will also have one CAST.
535         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
536         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
537         requires_casting = connections[self.db].features.requires_casted_case_in_updates
538         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
539         updates = []
540         for batch_objs in batches:
541             update_kwargs = {}
542             for field in fields:
543                 when_statements = []
544                 for obj in batch_objs:
545                     attr = getattr(obj, field.attname)
546                     if not isinstance(attr, Expression):
547                         attr = Value(attr, output_field=field)
548                     when_statements.append(When(pk=obj.pk, then=attr))
549                 case_statement = Case(*when_statements, output_field=field)
550                 if requires_casting:
551                     case_statement = Cast(case_statement, output_field=field)
552                 update_kwargs[field.attname] = case_statement
553             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
554         with transaction.atomic(using=self.db, savepoint=False):
555             for pks, update_kwargs in updates:
556                 self.filter(pk__in=pks).update(**update_kwargs)
557     bulk_update.alters_data = True
558 
559     def get_or_create(self, defaults=None, **kwargs):
560         """
561         Look up an object with the given kwargs, creating one if necessary.
562         Return a tuple of (object, created), where created is a boolean
563         specifying whether an object was created.
564         """
565         # The get() needs to be targeted at the write database in order
566         # to avoid potential transaction consistency problems.
567         self._for_write = True
568         try:
569             return self.get(**kwargs), False
570         except self.model.DoesNotExist:
571             params = self._extract_model_params(defaults, **kwargs)
572             return self._create_object_from_params(kwargs, params)
573 
574     def update_or_create(self, defaults=None, **kwargs):
575         """
576         Look up an object with the given kwargs, updating one with defaults
577         if it exists, otherwise create a new one.
578         Return a tuple (object, created), where created is a boolean
579         specifying whether an object was created.
580         """
581         defaults = defaults or {}
582         self._for_write = True
583         with transaction.atomic(using=self.db):
584             try:
585                 obj = self.select_for_update().get(**kwargs)
586             except self.model.DoesNotExist:
587                 params = self._extract_model_params(defaults, **kwargs)
588                 # Lock the row so that a concurrent update is blocked until
589                 # after update_or_create() has performed its save.
590                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
591                 if created:
592                     return obj, created
593             for k, v in resolve_callables(defaults):
594                 setattr(obj, k, v)
595             obj.save(using=self.db)
596         return obj, False
597 
598     def _create_object_from_params(self, lookup, params, lock=False):
599         """
600         Try to create an object using passed params. Used by get_or_create()
601         and update_or_create().
602         """
603         try:
604             with transaction.atomic(using=self.db):
605                 params = dict(resolve_callables(params))
606                 obj = self.create(**params)
607             return obj, True
608         except IntegrityError:
609             try:
610                 qs = self.select_for_update() if lock else self
611                 return qs.get(**lookup), False
612             except self.model.DoesNotExist:
613                 pass
614             raise
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create() and update_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657 
658         assert not self.query.is_sliced, \
659             "Cannot change a query once a slice has been taken."
660         obj = self._chain()
661         obj.query.set_limits(high=1)
662         obj.query.clear_ordering(force_empty=True)
663         obj.query.add_ordering(*order_by)
664         return obj.get()
665 
666     def earliest(self, *fields):
667         return self._earliest(*fields)
668 
669     def latest(self, *fields):
670         return self.reverse()._earliest(*fields)
671 
672     def first(self):
673         """Return the first object of a query or None if no match is found."""
674         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
675             return obj
676 
677     def last(self):
678         """Return the last object of a query or None if no match is found."""
679         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
680             return obj
681 
682     def in_bulk(self, id_list=None, *, field_name='pk'):
683         """
684         Return a dictionary mapping each of the given IDs to the object with
685         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
686         """
687         assert not self.query.is_sliced, \
688             "Cannot use 'limit' or 'offset' with in_bulk"
689         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
690             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
691         if id_list is not None:
692             if not id_list:
693                 return {}
694             filter_key = '{}__in'.format(field_name)
695             batch_size = connections[self.db].features.max_query_params
696             id_list = tuple(id_list)
697             # If the database has a limit on the number of query parameters
698             # (e.g. SQLite), retrieve objects in batches if necessary.
699             if batch_size and batch_size < len(id_list):
700                 qs = ()
701                 for offset in range(0, len(id_list), batch_size):
702                     batch = id_list[offset:offset + batch_size]
703                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
704             else:
705                 qs = self.filter(**{filter_key: id_list}).order_by()
706         else:
707             qs = self._chain()
708         return {getattr(obj, field_name): obj for obj in qs}
709 
710     def delete(self):
711         """Delete the records in the current QuerySet."""
712         self._not_support_combined_queries('delete')
713         assert not self.query.is_sliced, \
714             "Cannot use 'limit' or 'offset' with delete."
715 
716         if self._fields is not None:
717             raise TypeError("Cannot call delete() after .values() or .values_list()")
718 
719         del_query = self._chain()
720 
721         # The delete is actually 2 queries - one to find related objects,
722         # and one to delete. Make sure that the discovery of related
723         # objects is performed on the same database as the deletion.
724         del_query._for_write = True
725 
726         # Disable non-supported fields.
727         del_query.query.select_for_update = False
728         del_query.query.select_related = False
729         del_query.query.clear_ordering(force_empty=True)
730 
731         collector = Collector(using=del_query.db)
732         collector.collect(del_query)
733         deleted, _rows_count = collector.delete()
734 
735         # Clear the result cache, in case this QuerySet gets reused.
736         self._result_cache = None
737         return deleted, _rows_count
738 
739     delete.alters_data = True
740     delete.queryset_only = True
741 
742     def _raw_delete(self, using):
743         """
744         Delete objects found from the given queryset in single direct SQL
745         query. No signals are sent and there is no protection for cascades.
746         """
747         query = self.query.clone()
748         query.__class__ = sql.DeleteQuery
749         cursor = query.get_compiler(using).execute_sql(CURSOR)
750         if cursor:
751             with cursor:
752                 return cursor.rowcount
753         return 0
754     _raw_delete.alters_data = True
755 
756     def update(self, **kwargs):
757         """
758         Update all elements in the current QuerySet, setting all the given
759         fields to the appropriate values.
760         """
761         self._not_support_combined_queries('update')
762         assert not self.query.is_sliced, \
763             "Cannot update a query once a slice has been taken."
764         self._for_write = True
765         query = self.query.chain(sql.UpdateQuery)
766         query.add_update_values(kwargs)
767         # Clear any annotations so that they won't be present in subqueries.
768         query.annotations = {}
769         with transaction.mark_for_rollback_on_error(using=self.db):
770             rows = query.get_compiler(self.db).execute_sql(CURSOR)
771         self._result_cache = None
772         return rows
773     update.alters_data = True
774 
775     def _update(self, values):
776         """
777         A version of update() that accepts field objects instead of field names.
778         Used primarily for model saving and not intended for use by general
779         code (it requires too much poking around at model internals to be
780         useful at that level).
781         """
782         assert not self.query.is_sliced, \
783             "Cannot update a query once a slice has been taken."
784         query = self.query.chain(sql.UpdateQuery)
785         query.add_update_fields(values)
786         # Clear any annotations so that they won't be present in subqueries.
787         query.annotations = {}
788         self._result_cache = None
789         return query.get_compiler(self.db).execute_sql(CURSOR)
790     _update.alters_data = True
791     _update.queryset_only = False
792 
793     def exists(self):
794         if self._result_cache is None:
795             return self.query.has_results(using=self.db)
796         return bool(self._result_cache)
797 
798     def _prefetch_related_objects(self):
799         # This method can only be called once the result cache has been filled.
800         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
801         self._prefetch_done = True
802 
803     def explain(self, *, format=None, **options):
804         return self.query.explain(using=self.db, format=format, **options)
805 
806     ##################################################
807     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
808     ##################################################
809 
810     def raw(self, raw_query, params=None, translations=None, using=None):
811         if using is None:
812             using = self.db
813         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
814         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
815         return qs
816 
817     def _values(self, *fields, **expressions):
818         clone = self._chain()
819         if expressions:
820             clone = clone.annotate(**expressions)
821         clone._fields = fields
822         clone.query.set_values(fields)
823         return clone
824 
825     def values(self, *fields, **expressions):
826         fields += tuple(expressions)
827         clone = self._values(*fields, **expressions)
828         clone._iterable_class = ValuesIterable
829         return clone
830 
831     def values_list(self, *fields, flat=False, named=False):
832         if flat and named:
833             raise TypeError("'flat' and 'named' can't be used together.")
834         if flat and len(fields) > 1:
835             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
836 
837         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
838         _fields = []
839         expressions = {}
840         counter = 1
841         for field in fields:
842             if hasattr(field, 'resolve_expression'):
843                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
844                 while True:
845                     field_id = field_id_prefix + str(counter)
846                     counter += 1
847                     if field_id not in field_names:
848                         break
849                 expressions[field_id] = field
850                 _fields.append(field_id)
851             else:
852                 _fields.append(field)
853 
854         clone = self._values(*_fields, **expressions)
855         clone._iterable_class = (
856             NamedValuesListIterable if named
857             else FlatValuesListIterable if flat
858             else ValuesListIterable
859         )
860         return clone
861 
862     def dates(self, field_name, kind, order='ASC'):
863         """
864         Return a list of date objects representing all available dates for
865         the given field_name, scoped to 'kind'.
866         """
867         assert kind in ('year', 'month', 'week', 'day'), \
868             "'kind' must be one of 'year', 'month', 'week', or 'day'."
869         assert order in ('ASC', 'DESC'), \
870             "'order' must be either 'ASC' or 'DESC'."
871         return self.annotate(
872             datefield=Trunc(field_name, kind, output_field=DateField()),
873             plain_field=F(field_name)
874         ).values_list(
875             'datefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
877 
878     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
879         """
880         Return a list of datetime objects representing all available
881         datetimes for the given field_name, scoped to 'kind'.
882         """
883         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
884             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
885         assert order in ('ASC', 'DESC'), \
886             "'order' must be either 'ASC' or 'DESC'."
887         if settings.USE_TZ:
888             if tzinfo is None:
889                 tzinfo = timezone.get_current_timezone()
890         else:
891             tzinfo = None
892         return self.annotate(
893             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
894             plain_field=F(field_name)
895         ).values_list(
896             'datetimefield', flat=True
897         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
898 
899     def none(self):
900         """Return an empty QuerySet."""
901         clone = self._chain()
902         clone.query.set_empty()
903         return clone
904 
905     ##################################################################
906     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
907     ##################################################################
908 
909     def all(self):
910         """
911         Return a new QuerySet that is a copy of the current one. This allows a
912         QuerySet to proxy for a model manager in some cases.
913         """
914         return self._chain()
915 
916     def filter(self, *args, **kwargs):
917         """
918         Return a new QuerySet instance with the args ANDed to the existing
919         set.
920         """
921         self._not_support_combined_queries('filter')
922         return self._filter_or_exclude(False, *args, **kwargs)
923 
924     def exclude(self, *args, **kwargs):
925         """
926         Return a new QuerySet instance with NOT (args) ANDed to the existing
927         set.
928         """
929         self._not_support_combined_queries('exclude')
930         return self._filter_or_exclude(True, *args, **kwargs)
931 
932     def _filter_or_exclude(self, negate, *args, **kwargs):
933         if args or kwargs:
934             assert not self.query.is_sliced, \
935                 "Cannot filter a query once a slice has been taken."
936 
937         clone = self._chain()
938         if self._defer_next_filter:
939             self._defer_next_filter = False
940             clone._deferred_filter = negate, args, kwargs
941         else:
942             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
943         return clone
944 
945     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
946         if negate:
947             self._query.add_q(~Q(*args, **kwargs))
948         else:
949             self._query.add_q(Q(*args, **kwargs))
950 
951     def complex_filter(self, filter_obj):
952         """
953         Return a new QuerySet instance with filter_obj added to the filters.
954 
955         filter_obj can be a Q object or a dictionary of keyword lookup
956         arguments.
957 
958         This exists to support framework features such as 'limit_choices_to',
959         and usually it will be more natural to use other methods.
960         """
961         if isinstance(filter_obj, Q):
962             clone = self._chain()
963             clone.query.add_q(filter_obj)
964             return clone
965         else:
966             return self._filter_or_exclude(False, **filter_obj)
967 
968     def _combinator_query(self, combinator, *other_qs, all=False):
969         # Clone the query to inherit the select list and everything
970         clone = self._chain()
971         # Clear limits and ordering so they can be reapplied
972         clone.query.clear_ordering(True)
973         clone.query.clear_limits()
974         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
975         clone.query.combinator = combinator
976         clone.query.combinator_all = all
977         return clone
978 
979     def union(self, *other_qs, all=False):
980         # If the query is an EmptyQuerySet, combine all nonempty querysets.
981         if isinstance(self, EmptyQuerySet):
982             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
983             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
984         return self._combinator_query('union', *other_qs, all=all)
985 
986     def intersection(self, *other_qs):
987         # If any query is an EmptyQuerySet, return it.
988         if isinstance(self, EmptyQuerySet):
989             return self
990         for other in other_qs:
991             if isinstance(other, EmptyQuerySet):
992                 return other
993         return self._combinator_query('intersection', *other_qs)
994 
995     def difference(self, *other_qs):
996         # If the query is an EmptyQuerySet, return it.
997         if isinstance(self, EmptyQuerySet):
998             return self
999         return self._combinator_query('difference', *other_qs)
1000 
1001     def select_for_update(self, nowait=False, skip_locked=False, of=()):
1002         """
1003         Return a new QuerySet instance that will select objects with a
1004         FOR UPDATE lock.
1005         """
1006         if nowait and skip_locked:
1007             raise ValueError('The nowait option cannot be used with skip_locked.')
1008         obj = self._chain()
1009         obj._for_write = True
1010         obj.query.select_for_update = True
1011         obj.query.select_for_update_nowait = nowait
1012         obj.query.select_for_update_skip_locked = skip_locked
1013         obj.query.select_for_update_of = of
1014         return obj
1015 
1016     def select_related(self, *fields):
1017         """
1018         Return a new QuerySet instance that will select related objects.
1019 
1020         If fields are specified, they must be ForeignKey fields and only those
1021         related objects are included in the selection.
1022 
1023         If select_related(None) is called, clear the list.
1024         """
1025         self._not_support_combined_queries('select_related')
1026         if self._fields is not None:
1027             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1028 
1029         obj = self._chain()
1030         if fields == (None,):
1031             obj.query.select_related = False
1032         elif fields:
1033             obj.query.add_select_related(fields)
1034         else:
1035             obj.query.select_related = True
1036         return obj
1037 
1038     def prefetch_related(self, *lookups):
1039         """
1040         Return a new QuerySet instance that will prefetch the specified
1041         Many-To-One and Many-To-Many related objects when the QuerySet is
1042         evaluated.
1043 
1044         When prefetch_related() is called more than once, append to the list of
1045         prefetch lookups. If prefetch_related(None) is called, clear the list.
1046         """
1047         self._not_support_combined_queries('prefetch_related')
1048         clone = self._chain()
1049         if lookups == (None,):
1050             clone._prefetch_related_lookups = ()
1051         else:
1052             for lookup in lookups:
1053                 if isinstance(lookup, Prefetch):
1054                     lookup = lookup.prefetch_to
1055                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1056                 if lookup in self.query._filtered_relations:
1057                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1058             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1059         return clone
1060 
1061     def annotate(self, *args, **kwargs):
1062         """
1063         Return a query set in which the returned objects have been annotated
1064         with extra data or aggregations.
1065         """
1066         self._not_support_combined_queries('annotate')
1067         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1068         annotations = {}
1069         for arg in args:
1070             # The default_alias property may raise a TypeError.
1071             try:
1072                 if arg.default_alias in kwargs:
1073                     raise ValueError("The named annotation '%s' conflicts with the "
1074                                      "default name for another annotation."
1075                                      % arg.default_alias)
1076             except TypeError:
1077                 raise TypeError("Complex annotations require an alias")
1078             annotations[arg.default_alias] = arg
1079         annotations.update(kwargs)
1080 
1081         clone = self._chain()
1082         names = self._fields
1083         if names is None:
1084             names = set(chain.from_iterable(
1085                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1086                 for field in self.model._meta.get_fields()
1087             ))
1088 
1089         for alias, annotation in annotations.items():
1090             if alias in names:
1091                 raise ValueError("The annotation '%s' conflicts with a field on "
1092                                  "the model." % alias)
1093             if isinstance(annotation, FilteredRelation):
1094                 clone.query.add_filtered_relation(annotation, alias)
1095             else:
1096                 clone.query.add_annotation(annotation, alias, is_summary=False)
1097 
1098         for alias, annotation in clone.query.annotations.items():
1099             if alias in annotations and annotation.contains_aggregate:
1100                 if clone._fields is None:
1101                     clone.query.group_by = True
1102                 else:
1103                     clone.query.set_group_by()
1104                 break
1105 
1106         return clone
1107 
1108     def order_by(self, *field_names):
1109         """Return a new QuerySet instance with the ordering changed."""
1110         assert not self.query.is_sliced, \
1111             "Cannot reorder a query once a slice has been taken."
1112         obj = self._chain()
1113         obj.query.clear_ordering(force_empty=False)
1114         obj.query.add_ordering(*field_names)
1115         return obj
1116 
1117     def distinct(self, *field_names):
1118         """
1119         Return a new QuerySet instance that will select only distinct results.
1120         """
1121         assert not self.query.is_sliced, \
1122             "Cannot create distinct fields once a slice has been taken."
1123         obj = self._chain()
1124         obj.query.add_distinct_fields(*field_names)
1125         return obj
1126 
1127     def extra(self, select=None, where=None, params=None, tables=None,
1128               order_by=None, select_params=None):
1129         """Add extra SQL fragments to the query."""
1130         self._not_support_combined_queries('extra')
1131         assert not self.query.is_sliced, \
1132             "Cannot change a query once a slice has been taken"
1133         clone = self._chain()
1134         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1135         return clone
1136 
1137     def reverse(self):
1138         """Reverse the ordering of the QuerySet."""
1139         if self.query.is_sliced:
1140             raise TypeError('Cannot reverse a query once a slice has been taken.')
1141         clone = self._chain()
1142         clone.query.standard_ordering = not clone.query.standard_ordering
1143         return clone
1144 
1145     def defer(self, *fields):
1146         """
1147         Defer the loading of data for certain fields until they are accessed.
1148         Add the set of deferred fields to any existing set of deferred fields.
1149         The only exception to this is if None is passed in as the only
1150         parameter, in which case removal all deferrals.
1151         """
1152         self._not_support_combined_queries('defer')
1153         if self._fields is not None:
1154             raise TypeError("Cannot call defer() after .values() or .values_list()")
1155         clone = self._chain()
1156         if fields == (None,):
1157             clone.query.clear_deferred_loading()
1158         else:
1159             clone.query.add_deferred_loading(fields)
1160         return clone
1161 
1162     def only(self, *fields):
1163         """
1164         Essentially, the opposite of defer(). Only the fields passed into this
1165         method and that are not already specified as deferred are loaded
1166         immediately when the queryset is evaluated.
1167         """
1168         self._not_support_combined_queries('only')
1169         if self._fields is not None:
1170             raise TypeError("Cannot call only() after .values() or .values_list()")
1171         if fields == (None,):
1172             # Can only pass None to defer(), not only(), as the rest option.
1173             # That won't stop people trying to do this, so let's be explicit.
1174             raise TypeError("Cannot pass None as an argument to only().")
1175         for field in fields:
1176             field = field.split(LOOKUP_SEP, 1)[0]
1177             if field in self.query._filtered_relations:
1178                 raise ValueError('only() is not supported with FilteredRelation.')
1179         clone = self._chain()
1180         clone.query.add_immediate_loading(fields)
1181         return clone
1182 
1183     def using(self, alias):
1184         """Select which database this QuerySet should execute against."""
1185         clone = self._chain()
1186         clone._db = alias
1187         return clone
1188 
1189     ###################################
1190     # PUBLIC INTROSPECTION ATTRIBUTES #
1191     ###################################
1192 
1193     @property
1194     def ordered(self):
1195         """
1196         Return True if the QuerySet is ordered -- i.e. has an order_by()
1197         clause or a default ordering on the model (or is empty).
1198         """
1199         if isinstance(self, EmptyQuerySet):
1200             return True
1201         if self.query.extra_order_by or self.query.order_by:
1202             return True
1203         elif self.query.default_ordering and self.query.get_meta().ordering:
1204             return True
1205         else:
1206             return False
1207 
1208     @property
1209     def db(self):
1210         """Return the database used if this query is executed now."""
1211         if self._for_write:
1212             return self._db or router.db_for_write(self.model, **self._hints)
1213         return self._db or router.db_for_read(self.model, **self._hints)
1214 
1215     ###################
1216     # PRIVATE METHODS #
1217     ###################
1218 
1219     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1220         """
1221         Insert a new record for the given model. This provides an interface to
1222         the InsertQuery class and is how Model.save() is implemented.
1223         """
1224         self._for_write = True
1225         if using is None:
1226             using = self.db
1227         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1228         query.insert_values(fields, objs, raw=raw)
1229         return query.get_compiler(using=using).execute_sql(returning_fields)
1230     _insert.alters_data = True
1231     _insert.queryset_only = False
1232 
1233     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1234         """
1235         Helper method for bulk_create() to insert objs one batch at a time.
1236         """
1237         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1238             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1239         ops = connections[self.db].ops
1240         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1241         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1242         inserted_rows = []
1243         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1244         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1245             if bulk_return and not ignore_conflicts:
1246                 inserted_columns = self._insert(
1247                     item, fields=fields, using=self.db,
1248                     returning_fields=self.model._meta.db_returning_fields,
1249                     ignore_conflicts=ignore_conflicts,
1250                 )
1251                 if isinstance(inserted_columns, list):
1252                     inserted_rows.extend(inserted_columns)
1253                 else:
1254                     inserted_rows.append(inserted_columns)
1255             else:
1256                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1257         return inserted_rows
1258 
1259     def _chain(self, **kwargs):
1260         """
1261         Return a copy of the current QuerySet that's ready for another
1262         operation.
1263         """
1264         obj = self._clone()
1265         if obj._sticky_filter:
1266             obj.query.filter_is_sticky = True
1267             obj._sticky_filter = False
1268         obj.__dict__.update(kwargs)
1269         return obj
1270 
1271     def _clone(self):
1272         """
1273         Return a copy of the current QuerySet. A lightweight alternative
1274         to deepcopy().
1275         """
1276         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1277         c._sticky_filter = self._sticky_filter
1278         c._for_write = self._for_write
1279         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1280         c._known_related_objects = self._known_related_objects
1281         c._iterable_class = self._iterable_class
1282         c._fields = self._fields
1283         return c
1284 
1285     def _fetch_all(self):
1286         if self._result_cache is None:
1287             self._result_cache = list(self._iterable_class(self))
1288         if self._prefetch_related_lookups and not self._prefetch_done:
1289             self._prefetch_related_objects()
1290 
1291     def _next_is_sticky(self):
1292         """
1293         Indicate that the next filter call and the one following that should
1294         be treated as a single filter. This is only important when it comes to
1295         determining when to reuse tables for many-to-many filters. Required so
1296         that we can filter naturally on the results of related managers.
1297 
1298         This doesn't return a clone of the current QuerySet (it returns
1299         "self"). The method is only used internally and should be immediately
1300         followed by a filter() that does create a clone.
1301         """
1302         self._sticky_filter = True
1303         return self
1304 
1305     def _merge_sanity_check(self, other):
1306         """Check that two QuerySet classes may be merged."""
1307         if self._fields is not None and (
1308                 set(self.query.values_select) != set(other.query.values_select) or
1309                 set(self.query.extra_select) != set(other.query.extra_select) or
1310                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1311             raise TypeError(
1312                 "Merging '%s' classes must involve the same values in each case."
1313                 % self.__class__.__name__
1314             )
1315 
1316     def _merge_known_related_objects(self, other):
1317         """
1318         Keep track of all known related objects from either QuerySet instance.
1319         """
1320         for field, objects in other._known_related_objects.items():
1321             self._known_related_objects.setdefault(field, {}).update(objects)
1322 
1323     def resolve_expression(self, *args, **kwargs):
1324         if self._fields and len(self._fields) > 1:
1325             # values() queryset can only be used as nested queries
1326             # if they are set up to select only a single field.
1327             raise TypeError('Cannot use multi-field values as a filter value.')
1328         query = self.query.resolve_expression(*args, **kwargs)
1329         query._db = self._db
1330         return query
1331     resolve_expression.queryset_only = True
1332 
1333     def _add_hints(self, **hints):
1334         """
1335         Update hinting information for use by routers. Add new key/values or
1336         overwrite existing key/values.
1337         """
1338         self._hints.update(hints)
1339 
1340     def _has_filters(self):
1341         """
1342         Check if this QuerySet has any filtering going on. This isn't
1343         equivalent with checking if all objects are present in results, for
1344         example, qs[1:]._has_filters() -> False.
1345         """
1346         return self.query.has_filters()
1347 
1348     @staticmethod
1349     def _validate_values_are_expressions(values, method_name):
1350         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1351         if invalid_args:
1352             raise TypeError(
1353                 'QuerySet.%s() received non-expression(s): %s.' % (
1354                     method_name,
1355                     ', '.join(invalid_args),
1356                 )
1357             )
1358 
1359     def _not_support_combined_queries(self, operation_name):
1360         if self.query.combinator:
1361             raise NotSupportedError(
1362                 'Calling QuerySet.%s() after %s() is not supported.'
1363                 % (operation_name, self.query.combinator)
1364             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/compiler.py</file>
<class>SQLCompiler</class>
<method>None</method>
<code>
21 class SQLCompiler:
22     def __init__(self, query, connection, using):
23         self.query = query
24         self.connection = connection
25         self.using = using
26         self.quote_cache = {'*': '*'}
27         # The select, klass_info, and annotations are needed by QuerySet.iterator()
28         # these are set as a side-effect of executing the query. Note that we calculate
29         # separately a list of extra select columns needed for grammatical correctness
30         # of the query, but these columns are not included in self.select.
31         self.select = None
32         self.annotation_col_map = None
33         self.klass_info = None
34         # Multiline ordering SQL clause may appear from RawSQL.
35         self.ordering_parts = re.compile(r'^(.*)\s(ASC|DESC)(.*)', re.MULTILINE | re.DOTALL)
36         self._meta_ordering = None
37 
38     def setup_query(self):
39         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):
40             self.query.get_initial_alias()
41         self.select, self.klass_info, self.annotation_col_map = self.get_select()
42         self.col_count = len(self.select)
43 
44     def pre_sql_setup(self):
45         """
46         Do any necessary class setup immediately prior to producing SQL. This
47         is for things that can't necessarily be done in __init__ because we
48         might not have all the pieces in place at that time.
49         """
50         self.setup_query()
51         order_by = self.get_order_by()
52         self.where, self.having = self.query.where.split_having()
53         extra_select = self.get_extra_select(order_by, self.select)
54         self.has_extra_select = bool(extra_select)
55         group_by = self.get_group_by(self.select + extra_select, order_by)
56         return extra_select, order_by, group_by
57 
58     def get_group_by(self, select, order_by):
59         """
60         Return a list of 2-tuples of form (sql, params).
61 
62         The logic of what exactly the GROUP BY clause contains is hard
63         to describe in other words than "if it passes the test suite,
64         then it is correct".
65         """
66         # Some examples:
67         #     SomeModel.objects.annotate(Count('somecol'))
68         #     GROUP BY: all fields of the model
69         #
70         #    SomeModel.objects.values('name').annotate(Count('somecol'))
71         #    GROUP BY: name
72         #
73         #    SomeModel.objects.annotate(Count('somecol')).values('name')
74         #    GROUP BY: all cols of the model
75         #
76         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')
77         #    GROUP BY: name, pk
78         #
79         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')
80         #    GROUP BY: name, pk
81         #
82         # In fact, the self.query.group_by is the minimal set to GROUP BY. It
83         # can't be ever restricted to a smaller set, but additional columns in
84         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately
85         # the end result is that it is impossible to force the query to have
86         # a chosen GROUP BY clause - you can almost do this by using the form:
87         #     .values(*wanted_cols).annotate(AnAggregate())
88         # but any later annotations, extra selects, values calls that
89         # refer some column outside of the wanted_cols, order_by, or even
90         # filter calls can alter the GROUP BY clause.
91 
92         # The query.group_by is either None (no GROUP BY at all), True
93         # (group by select fields), or a list of expressions to be added
94         # to the group by.
95         if self.query.group_by is None:
96             return []
97         expressions = []
98         if self.query.group_by is not True:
99             # If the group by is set to a list (by .values() call most likely),
100             # then we need to add everything in it to the GROUP BY clause.
101             # Backwards compatibility hack for setting query.group_by. Remove
102             # when  we have public API way of forcing the GROUP BY clause.
103             # Converts string references to expressions.
104             for expr in self.query.group_by:
105                 if not hasattr(expr, 'as_sql'):
106                     expressions.append(self.query.resolve_ref(expr))
107                 else:
108                     expressions.append(expr)
109         # Note that even if the group_by is set, it is only the minimal
110         # set to group by. So, we need to add cols in select, order_by, and
111         # having into the select in any case.
112         ref_sources = {
113             expr.source for expr in expressions if isinstance(expr, Ref)
114         }
115         for expr, _, _ in select:
116             # Skip members of the select clause that are already included
117             # by reference.
118             if expr in ref_sources:
119                 continue
120             cols = expr.get_group_by_cols()
121             for col in cols:
122                 expressions.append(col)
123         for expr, (sql, params, is_ref) in order_by:
124             # Skip References to the select clause, as all expressions in the
125             # select clause are already part of the group by.
126             if not is_ref:
127                 expressions.extend(expr.get_group_by_cols())
128         having_group_by = self.having.get_group_by_cols() if self.having else ()
129         for expr in having_group_by:
130             expressions.append(expr)
131         result = []
132         seen = set()
133         expressions = self.collapse_group_by(expressions, having_group_by)
134 
135         for expr in expressions:
136             sql, params = self.compile(expr)
137             params_hash = make_hashable(params)
138             if (sql, params_hash) not in seen:
139                 result.append((sql, params))
140                 seen.add((sql, params_hash))
141         return result
142 
143     def collapse_group_by(self, expressions, having):
144         # If the DB can group by primary key, then group by the primary key of
145         # query's main model. Note that for PostgreSQL the GROUP BY clause must
146         # include the primary key of every table, but for MySQL it is enough to
147         # have the main table's primary key.
148         if self.connection.features.allows_group_by_pk:
149             # Determine if the main model's primary key is in the query.
150             pk = None
151             for expr in expressions:
152                 # Is this a reference to query's base table primary key? If the
153                 # expression isn't a Col-like, then skip the expression.
154                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and
155                         getattr(expr, 'alias', None) == self.query.base_table):
156                     pk = expr
157                     break
158             # If the main model's primary key is in the query, group by that
159             # field, HAVING expressions, and expressions associated with tables
160             # that don't have a primary key included in the grouped columns.
161             if pk:
162                 pk_aliases = {
163                     expr.alias for expr in expressions
164                     if hasattr(expr, 'target') and expr.target.primary_key
165                 }
166                 expressions = [pk] + [
167                     expr for expr in expressions
168                     if expr in having or (
169                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases
170                     )
171                 ]
172         elif self.connection.features.allows_group_by_selected_pks:
173             # Filter out all expressions associated with a table's primary key
174             # present in the grouped columns. This is done by identifying all
175             # tables that have their primary key included in the grouped
176             # columns and removing non-primary key columns referring to them.
177             # Unmanaged models are excluded because they could be representing
178             # database views on which the optimization might not be allowed.
179             pks = {
180                 expr for expr in expressions
181                 if (
182                     hasattr(expr, 'target') and
183                     expr.target.primary_key and
184                     self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
185                 )
186             }
187             aliases = {expr.alias for expr in pks}
188             expressions = [
189                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases
190             ]
191         return expressions
192 
193     def get_select(self):
194         """
195         Return three values:
196         - a list of 3-tuples of (expression, (sql, params), alias)
197         - a klass_info structure,
198         - a dictionary of annotations
199 
200         The (sql, params) is what the expression will produce, and alias is the
201         "AS alias" for the column (possibly None).
202 
203         The klass_info structure contains the following information:
204         - The base model of the query.
205         - Which columns for that model are present in the query (by
206           position of the select clause).
207         - related_klass_infos: [f, klass_info] to descent into
208 
209         The annotations is a dictionary of {'attname': column position} values.
210         """
211         select = []
212         klass_info = None
213         annotations = {}
214         select_idx = 0
215         for alias, (sql, params) in self.query.extra_select.items():
216             annotations[alias] = select_idx
217             select.append((RawSQL(sql, params), alias))
218             select_idx += 1
219         assert not (self.query.select and self.query.default_cols)
220         if self.query.default_cols:
221             cols = self.get_default_columns()
222         else:
223             # self.query.select is a special case. These columns never go to
224             # any model.
225             cols = self.query.select
226         if cols:
227             select_list = []
228             for col in cols:
229                 select_list.append(select_idx)
230                 select.append((col, None))
231                 select_idx += 1
232             klass_info = {
233                 'model': self.query.model,
234                 'select_fields': select_list,
235             }
236         for alias, annotation in self.query.annotation_select.items():
237             annotations[alias] = select_idx
238             select.append((annotation, alias))
239             select_idx += 1
240 
241         if self.query.select_related:
242             related_klass_infos = self.get_related_selections(select)
243             klass_info['related_klass_infos'] = related_klass_infos
244 
245             def get_select_from_parent(klass_info):
246                 for ki in klass_info['related_klass_infos']:
247                     if ki['from_parent']:
248                         ki['select_fields'] = (klass_info['select_fields'] +
249                                                ki['select_fields'])
250                     get_select_from_parent(ki)
251             get_select_from_parent(klass_info)
252 
253         ret = []
254         for col, alias in select:
255             try:
256                 sql, params = self.compile(col)
257             except EmptyResultSet:
258                 # Select a predicate that's always False.
259                 sql, params = '0', ()
260             else:
261                 sql, params = col.select_format(self, sql, params)
262             ret.append((col, (sql, params), alias))
263         return ret, klass_info, annotations
264 
265     def get_order_by(self):
266         """
267         Return a list of 2-tuples of form (expr, (sql, params, is_ref)) for the
268         ORDER BY clause.
269 
270         The order_by clause can alter the select clause (for example it
271         can add aliases to clauses that do not yet have one, or it can
272         add totally new select clauses).
273         """
274         if self.query.extra_order_by:
275             ordering = self.query.extra_order_by
276         elif not self.query.default_ordering:
277             ordering = self.query.order_by
278         elif self.query.order_by:
279             ordering = self.query.order_by
280         elif self.query.get_meta().ordering:
281             ordering = self.query.get_meta().ordering
282             self._meta_ordering = ordering
283         else:
284             ordering = []
285         if self.query.standard_ordering:
286             asc, desc = ORDER_DIR['ASC']
287         else:
288             asc, desc = ORDER_DIR['DESC']
289 
290         order_by = []
291         for field in ordering:
292             if hasattr(field, 'resolve_expression'):
293                 if isinstance(field, Value):
294                     # output_field must be resolved for constants.
295                     field = Cast(field, field.output_field)
296                 if not isinstance(field, OrderBy):
297                     field = field.asc()
298                 if not self.query.standard_ordering:
299                     field = field.copy()
300                     field.reverse_ordering()
301                 order_by.append((field, False))
302                 continue
303             if field == '?':  # random
304                 order_by.append((OrderBy(Random()), False))
305                 continue
306 
307             col, order = get_order_dir(field, asc)
308             descending = order == 'DESC'
309 
310             if col in self.query.annotation_select:
311                 # Reference to expression in SELECT clause
312                 order_by.append((
313                     OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending),
314                     True))
315                 continue
316             if col in self.query.annotations:
317                 # References to an expression which is masked out of the SELECT
318                 # clause.
319                 expr = self.query.annotations[col]
320                 if isinstance(expr, Value):
321                     # output_field must be resolved for constants.
322                     expr = Cast(expr, expr.output_field)
323                 order_by.append((OrderBy(expr, descending=descending), False))
324                 continue
325 
326             if '.' in field:
327                 # This came in through an extra(order_by=...) addition. Pass it
328                 # on verbatim.
329                 table, col = col.split('.', 1)
330                 order_by.append((
331                     OrderBy(
332                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),
333                         descending=descending
334                     ), False))
335                 continue
336 
337             if not self.query.extra or col not in self.query.extra:
338                 # 'col' is of the form 'field' or 'field1__field2' or
339                 # '-field1__field2__field', etc.
340                 order_by.extend(self.find_ordering_name(
341                     field, self.query.get_meta(), default_order=asc))
342             else:
343                 if col not in self.query.extra_select:
344                     order_by.append((
345                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),
346                         False))
347                 else:
348                     order_by.append((
349                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),
350                         True))
351         result = []
352         seen = set()
353 
354         for expr, is_ref in order_by:
355             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)
356             if self.query.combinator:
357                 src = resolved.get_source_expressions()[0]
358                 # Relabel order by columns to raw numbers if this is a combined
359                 # query; necessary since the columns can't be referenced by the
360                 # fully qualified name and the simple column names may collide.
361                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):
362                     if is_ref and col_alias == src.refs:
363                         src = src.source
364                     elif col_alias:
365                         continue
366                     if src == sel_expr:
367                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])
368                         break
369                 else:
370                     if col_alias:
371                         raise DatabaseError('ORDER BY term does not match any column in the result set.')
372                     # Add column used in ORDER BY clause without an alias to
373                     # the selected columns.
374                     self.query.add_select_col(src)
375                     resolved.set_source_expressions([RawSQL('%d' % len(self.query.select), ())])
376             sql, params = self.compile(resolved)
377             # Don't add the same column twice, but the order direction is
378             # not taken into account so we strip it. When this entire method
379             # is refactored into expressions, then we can check each part as we
380             # generate it.
381             without_ordering = self.ordering_parts.search(sql).group(1)
382             params_hash = make_hashable(params)
383             if (without_ordering, params_hash) in seen:
384                 continue
385             seen.add((without_ordering, params_hash))
386             result.append((resolved, (sql, params, is_ref)))
387         return result
388 
389     def get_extra_select(self, order_by, select):
390         extra_select = []
391         if self.query.distinct and not self.query.distinct_fields:
392             select_sql = [t[1] for t in select]
393             for expr, (sql, params, is_ref) in order_by:
394                 without_ordering = self.ordering_parts.search(sql).group(1)
395                 if not is_ref and (without_ordering, params) not in select_sql:
396                     extra_select.append((expr, (without_ordering, params), None))
397         return extra_select
398 
399     def quote_name_unless_alias(self, name):
400         """
401         A wrapper around connection.ops.quote_name that doesn't quote aliases
402         for table names. This avoids problems with some SQL dialects that treat
403         quoted strings specially (e.g. PostgreSQL).
404         """
405         if name in self.quote_cache:
406             return self.quote_cache[name]
407         if ((name in self.query.alias_map and name not in self.query.table_map) or
408                 name in self.query.extra_select or (
409                     self.query.external_aliases.get(name) and name not in self.query.table_map)):
410             self.quote_cache[name] = name
411             return name
412         r = self.connection.ops.quote_name(name)
413         self.quote_cache[name] = r
414         return r
415 
416     def compile(self, node):
417         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)
418         if vendor_impl:
419             sql, params = vendor_impl(self, self.connection)
420         else:
421             sql, params = node.as_sql(self, self.connection)
422         return sql, params
423 
424     def get_combinator_sql(self, combinator, all):
425         features = self.connection.features
426         compilers = [
427             query.get_compiler(self.using, self.connection)
428             for query in self.query.combined_queries if not query.is_empty()
429         ]
430         if not features.supports_slicing_ordering_in_compound:
431             for query, compiler in zip(self.query.combined_queries, compilers):
432                 if query.low_mark or query.high_mark:
433                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')
434                 if compiler.get_order_by():
435                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')
436         parts = ()
437         for compiler in compilers:
438             try:
439                 # If the columns list is limited, then all combined queries
440                 # must have the same columns list. Set the selects defined on
441                 # the query on all combined queries, if not already set.
442                 if not compiler.query.values_select and self.query.values_select:
443                     compiler.query = compiler.query.clone()
444                     compiler.query.set_values((
445                         *self.query.extra_select,
446                         *self.query.values_select,
447                         *self.query.annotation_select,
448                     ))
449                 part_sql, part_args = compiler.as_sql()
450                 if compiler.query.combinator:
451                     # Wrap in a subquery if wrapping in parentheses isn't
452                     # supported.
453                     if not features.supports_parentheses_in_compound:
454                         part_sql = 'SELECT * FROM ({})'.format(part_sql)
455                     # Add parentheses when combining with compound query if not
456                     # already added for all compound queries.
457                     elif not features.supports_slicing_ordering_in_compound:
458                         part_sql = '({})'.format(part_sql)
459                 parts += ((part_sql, part_args),)
460             except EmptyResultSet:
461                 # Omit the empty queryset with UNION and with DIFFERENCE if the
462                 # first queryset is nonempty.
463                 if combinator == 'union' or (combinator == 'difference' and parts):
464                     continue
465                 raise
466         if not parts:
467             raise EmptyResultSet
468         combinator_sql = self.connection.ops.set_operators[combinator]
469         if all and combinator == 'union':
470             combinator_sql += ' ALL'
471         braces = '({})' if features.supports_slicing_ordering_in_compound else '{}'
472         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))
473         result = [' {} '.format(combinator_sql).join(sql_parts)]
474         params = []
475         for part in args_parts:
476             params.extend(part)
477         return result, params
478 
479     def as_sql(self, with_limits=True, with_col_aliases=False):
480         """
481         Create the SQL for this query. Return the SQL string and list of
482         parameters.
483 
484         If 'with_limits' is False, any limit/offset information is not included
485         in the query.
486         """
487         refcounts_before = self.query.alias_refcount.copy()
488         try:
489             extra_select, order_by, group_by = self.pre_sql_setup()
490             for_update_part = None
491             # Is a LIMIT/OFFSET clause needed?
492             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)
493             combinator = self.query.combinator
494             features = self.connection.features
495             if combinator:
496                 if not getattr(features, 'supports_select_{}'.format(combinator)):
497                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))
498                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)
499             else:
500                 distinct_fields, distinct_params = self.get_distinct()
501                 # This must come after 'select', 'ordering', and 'distinct'
502                 # (see docstring of get_from_clause() for details).
503                 from_, f_params = self.get_from_clause()
504                 where, w_params = self.compile(self.where) if self.where is not None else ("", [])
505                 having, h_params = self.compile(self.having) if self.having is not None else ("", [])
506                 result = ['SELECT']
507                 params = []
508 
509                 if self.query.distinct:
510                     distinct_result, distinct_params = self.connection.ops.distinct_sql(
511                         distinct_fields,
512                         distinct_params,
513                     )
514                     result += distinct_result
515                     params += distinct_params
516 
517                 out_cols = []
518                 col_idx = 1
519                 for _, (s_sql, s_params), alias in self.select + extra_select:
520                     if alias:
521                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))
522                     elif with_col_aliases:
523                         s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)
524                         col_idx += 1
525                     params.extend(s_params)
526                     out_cols.append(s_sql)
527 
528                 result += [', '.join(out_cols), 'FROM', *from_]
529                 params.extend(f_params)
530 
531                 if self.query.select_for_update and self.connection.features.has_select_for_update:
532                     if self.connection.get_autocommit():
533                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')
534 
535                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:
536                         raise NotSupportedError(
537                             'LIMIT/OFFSET is not supported with '
538                             'select_for_update on this database backend.'
539                         )
540                     nowait = self.query.select_for_update_nowait
541                     skip_locked = self.query.select_for_update_skip_locked
542                     of = self.query.select_for_update_of
543                     # If it's a NOWAIT/SKIP LOCKED/OF query but the backend
544                     # doesn't support it, raise NotSupportedError to prevent a
545                     # possible deadlock.
546                     if nowait and not self.connection.features.has_select_for_update_nowait:
547                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
548                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
549                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
550                     elif of and not self.connection.features.has_select_for_update_of:
551                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
552                     for_update_part = self.connection.ops.for_update_sql(
553                         nowait=nowait,
554                         skip_locked=skip_locked,
555                         of=self.get_select_for_update_of_arguments(),
556                     )
557 
558                 if for_update_part and self.connection.features.for_update_after_from:
559                     result.append(for_update_part)
560 
561                 if where:
562                     result.append('WHERE %s' % where)
563                     params.extend(w_params)
564 
565                 grouping = []
566                 for g_sql, g_params in group_by:
567                     grouping.append(g_sql)
568                     params.extend(g_params)
569                 if grouping:
570                     if distinct_fields:
571                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')
572                     order_by = order_by or self.connection.ops.force_no_ordering()
573                     result.append('GROUP BY %s' % ', '.join(grouping))
574                     if self._meta_ordering:
575                         order_by = None
576                 if having:
577                     result.append('HAVING %s' % having)
578                     params.extend(h_params)
579 
580             if self.query.explain_query:
581                 result.insert(0, self.connection.ops.explain_query_prefix(
582                     self.query.explain_format,
583                     **self.query.explain_options
584                 ))
585 
586             if order_by:
587                 ordering = []
588                 for _, (o_sql, o_params, _) in order_by:
589                     ordering.append(o_sql)
590                     params.extend(o_params)
591                 result.append('ORDER BY %s' % ', '.join(ordering))
592 
593             if with_limit_offset:
594                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))
595 
596             if for_update_part and not self.connection.features.for_update_after_from:
597                 result.append(for_update_part)
598 
599             if self.query.subquery and extra_select:
600                 # If the query is used as a subquery, the extra selects would
601                 # result in more columns than the left-hand side expression is
602                 # expecting. This can happen when a subquery uses a combination
603                 # of order_by() and distinct(), forcing the ordering expressions
604                 # to be selected as well. Wrap the query in another subquery
605                 # to exclude extraneous selects.
606                 sub_selects = []
607                 sub_params = []
608                 for index, (select, _, alias) in enumerate(self.select, start=1):
609                     if not alias and with_col_aliases:
610                         alias = 'col%d' % index
611                     if alias:
612                         sub_selects.append("%s.%s" % (
613                             self.connection.ops.quote_name('subquery'),
614                             self.connection.ops.quote_name(alias),
615                         ))
616                     else:
617                         select_clone = select.relabeled_clone({select.alias: 'subquery'})
618                         subselect, subparams = select_clone.as_sql(self, self.connection)
619                         sub_selects.append(subselect)
620                         sub_params.extend(subparams)
621                 return 'SELECT %s FROM (%s) subquery' % (
622                     ', '.join(sub_selects),
623                     ' '.join(result),
624                 ), tuple(sub_params + params)
625 
626             return ' '.join(result), tuple(params)
627         finally:
628             # Finally do cleanup - get rid of the joins we created above.
629             self.query.reset_refcounts(refcounts_before)
630 
631     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):
632         """
633         Compute the default columns for selecting every field in the base
634         model. Will sometimes be called to pull in related models (e.g. via
635         select_related), in which case "opts" and "start_alias" will be given
636         to provide a starting point for the traversal.
637 
638         Return a list of strings, quoted appropriately for use in SQL
639         directly, as well as a set of aliases used in the select statement (if
640         'as_pairs' is True, return a list of (alias, col_name) pairs instead
641         of strings as the first component and None as the second component).
642         """
643         result = []
644         if opts is None:
645             opts = self.query.get_meta()
646         only_load = self.deferred_to_columns()
647         start_alias = start_alias or self.query.get_initial_alias()
648         # The 'seen_models' is used to optimize checking the needed parent
649         # alias for a given field. This also includes None -> start_alias to
650         # be used by local fields.
651         seen_models = {None: start_alias}
652 
653         for field in opts.concrete_fields:
654             model = field.model._meta.concrete_model
655             # A proxy model will have a different model and concrete_model. We
656             # will assign None if the field belongs to this model.
657             if model == opts.model:
658                 model = None
659             if from_parent and model is not None and issubclass(
660                     from_parent._meta.concrete_model, model._meta.concrete_model):
661                 # Avoid loading data for already loaded parents.
662                 # We end up here in the case select_related() resolution
663                 # proceeds from parent model to child model. In that case the
664                 # parent model data is already present in the SELECT clause,
665                 # and we want to avoid reloading the same data again.
666                 continue
667             if field.model in only_load and field.attname not in only_load[field.model]:
668                 continue
669             alias = self.query.join_parent_model(opts, model, start_alias,
670                                                  seen_models)
671             column = field.get_col(alias)
672             result.append(column)
673         return result
674 
675     def get_distinct(self):
676         """
677         Return a quoted list of fields to use in DISTINCT ON part of the query.
678 
679         This method can alter the tables in the query, and thus it must be
680         called before get_from_clause().
681         """
682         result = []
683         params = []
684         opts = self.query.get_meta()
685 
686         for name in self.query.distinct_fields:
687             parts = name.split(LOOKUP_SEP)
688             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)
689             targets, alias, _ = self.query.trim_joins(targets, joins, path)
690             for target in targets:
691                 if name in self.query.annotation_select:
692                     result.append(name)
693                 else:
694                     r, p = self.compile(transform_function(target, alias))
695                     result.append(r)
696                     params.append(p)
697         return result, params
698 
699     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',
700                            already_seen=None):
701         """
702         Return the table alias (the name might be ambiguous, the alias will
703         not be) and column name for ordering by the given 'name' parameter.
704         The 'name' is of the form 'field1__field2__...__fieldN'.
705         """
706         name, order = get_order_dir(name, default_order)
707         descending = order == 'DESC'
708         pieces = name.split(LOOKUP_SEP)
709         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)
710 
711         # If we get to this point and the field is a relation to another model,
712         # append the default ordering for that model unless the attribute name
713         # of the field is specified.
714         if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:
715             # Firstly, avoid infinite loops.
716             already_seen = already_seen or set()
717             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)
718             if join_tuple in already_seen:
719                 raise FieldError('Infinite loop caused by ordering.')
720             already_seen.add(join_tuple)
721 
722             results = []
723             for item in opts.ordering:
724                 if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):
725                     item = item.desc() if descending else item.asc()
726                 if isinstance(item, OrderBy):
727                     results.append((item, False))
728                     continue
729                 results.extend(self.find_ordering_name(item, opts, alias,
730                                                        order, already_seen))
731             return results
732         targets, alias, _ = self.query.trim_joins(targets, joins, path)
733         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]
734 
735     def _setup_joins(self, pieces, opts, alias):
736         """
737         Helper method for get_order_by() and get_distinct().
738 
739         get_ordering() and get_distinct() must produce same target columns on
740         same input, as the prefixes of get_ordering() and get_distinct() must
741         match. Executing SQL where this is not true is an error.
742         """
743         alias = alias or self.query.get_initial_alias()
744         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)
745         alias = joins[-1]
746         return field, targets, alias, joins, path, opts, transform_function
747 
748     def get_from_clause(self):
749         """
750         Return a list of strings that are joined together to go after the
751         "FROM" part of the query, as well as a list any extra parameters that
752         need to be included. Subclasses, can override this to create a
753         from-clause via a "select".
754 
755         This should only be called after any SQL construction methods that
756         might change the tables that are needed. This means the select columns,
757         ordering, and distinct must be done first.
758         """
759         result = []
760         params = []
761         for alias in tuple(self.query.alias_map):
762             if not self.query.alias_refcount[alias]:
763                 continue
764             try:
765                 from_clause = self.query.alias_map[alias]
766             except KeyError:
767                 # Extra tables can end up in self.tables, but not in the
768                 # alias_map if they aren't in a join. That's OK. We skip them.
769                 continue
770             clause_sql, clause_params = self.compile(from_clause)
771             result.append(clause_sql)
772             params.extend(clause_params)
773         for t in self.query.extra_tables:
774             alias, _ = self.query.table_alias(t)
775             # Only add the alias if it's not already present (the table_alias()
776             # call increments the refcount, so an alias refcount of one means
777             # this is the only reference).
778             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:
779                 result.append(', %s' % self.quote_name_unless_alias(alias))
780         return result, params
781 
782     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,
783                                requested=None, restricted=None):
784         """
785         Fill in the information needed for a select_related query. The current
786         depth is measured as the number of connections away from the root model
787         (for example, cur_depth=1 means we are looking at models with direct
788         connections to the root model).
789         """
790         def _get_field_choices():
791             direct_choices = (f.name for f in opts.fields if f.is_relation)
792             reverse_choices = (
793                 f.field.related_query_name()
794                 for f in opts.related_objects if f.field.unique
795             )
796             return chain(direct_choices, reverse_choices, self.query._filtered_relations)
797 
798         related_klass_infos = []
799         if not restricted and cur_depth > self.query.max_depth:
800             # We've recursed far enough; bail out.
801             return related_klass_infos
802 
803         if not opts:
804             opts = self.query.get_meta()
805             root_alias = self.query.get_initial_alias()
806         only_load = self.query.get_loaded_field_names()
807 
808         # Setup for the case when only particular related fields should be
809         # included in the related selection.
810         fields_found = set()
811         if requested is None:
812             restricted = isinstance(self.query.select_related, dict)
813             if restricted:
814                 requested = self.query.select_related
815 
816         def get_related_klass_infos(klass_info, related_klass_infos):
817             klass_info['related_klass_infos'] = related_klass_infos
818 
819         for f in opts.fields:
820             field_model = f.model._meta.concrete_model
821             fields_found.add(f.name)
822 
823             if restricted:
824                 next = requested.get(f.name, {})
825                 if not f.is_relation:
826                     # If a non-related field is used like a relation,
827                     # or if a single non-relational field is given.
828                     if next or f.name in requested:
829                         raise FieldError(
830                             "Non-relational field given in select_related: '%s'. "
831                             "Choices are: %s" % (
832                                 f.name,
833                                 ", ".join(_get_field_choices()) or '(none)',
834                             )
835                         )
836             else:
837                 next = False
838 
839             if not select_related_descend(f, restricted, requested,
840                                           only_load.get(field_model)):
841                 continue
842             klass_info = {
843                 'model': f.remote_field.model,
844                 'field': f,
845                 'reverse': False,
846                 'local_setter': f.set_cached_value,
847                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,
848                 'from_parent': False,
849             }
850             related_klass_infos.append(klass_info)
851             select_fields = []
852             _, _, _, joins, _, _ = self.query.setup_joins(
853                 [f.name], opts, root_alias)
854             alias = joins[-1]
855             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)
856             for col in columns:
857                 select_fields.append(len(select))
858                 select.append((col, None))
859             klass_info['select_fields'] = select_fields
860             next_klass_infos = self.get_related_selections(
861                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)
862             get_related_klass_infos(klass_info, next_klass_infos)
863 
864         if restricted:
865             related_fields = [
866                 (o.field, o.related_model)
867                 for o in opts.related_objects
868                 if o.field.unique and not o.many_to_many
869             ]
870             for f, model in related_fields:
871                 if not select_related_descend(f, restricted, requested,
872                                               only_load.get(model), reverse=True):
873                     continue
874 
875                 related_field_name = f.related_query_name()
876                 fields_found.add(related_field_name)
877 
878                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)
879                 alias = join_info.joins[-1]
880                 from_parent = issubclass(model, opts.model) and model is not opts.model
881                 klass_info = {
882                     'model': model,
883                     'field': f,
884                     'reverse': True,
885                     'local_setter': f.remote_field.set_cached_value,
886                     'remote_setter': f.set_cached_value,
887                     'from_parent': from_parent,
888                 }
889                 related_klass_infos.append(klass_info)
890                 select_fields = []
891                 columns = self.get_default_columns(
892                     start_alias=alias, opts=model._meta, from_parent=opts.model)
893                 for col in columns:
894                     select_fields.append(len(select))
895                     select.append((col, None))
896                 klass_info['select_fields'] = select_fields
897                 next = requested.get(f.related_query_name(), {})
898                 next_klass_infos = self.get_related_selections(
899                     select, model._meta, alias, cur_depth + 1,
900                     next, restricted)
901                 get_related_klass_infos(klass_info, next_klass_infos)
902 
903             def local_setter(obj, from_obj):
904                 # Set a reverse fk object when relation is non-empty.
905                 if from_obj:
906                     f.remote_field.set_cached_value(from_obj, obj)
907 
908             def remote_setter(name, obj, from_obj):
909                 setattr(from_obj, name, obj)
910 
911             for name in list(requested):
912                 # Filtered relations work only on the topmost level.
913                 if cur_depth > 1:
914                     break
915                 if name in self.query._filtered_relations:
916                     fields_found.add(name)
917                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)
918                     model = join_opts.model
919                     alias = joins[-1]
920                     from_parent = issubclass(model, opts.model) and model is not opts.model
921                     klass_info = {
922                         'model': model,
923                         'field': f,
924                         'reverse': True,
925                         'local_setter': local_setter,
926                         'remote_setter': partial(remote_setter, name),
927                         'from_parent': from_parent,
928                     }
929                     related_klass_infos.append(klass_info)
930                     select_fields = []
931                     columns = self.get_default_columns(
932                         start_alias=alias, opts=model._meta,
933                         from_parent=opts.model,
934                     )
935                     for col in columns:
936                         select_fields.append(len(select))
937                         select.append((col, None))
938                     klass_info['select_fields'] = select_fields
939                     next_requested = requested.get(name, {})
940                     next_klass_infos = self.get_related_selections(
941                         select, opts=model._meta, root_alias=alias,
942                         cur_depth=cur_depth + 1, requested=next_requested,
943                         restricted=restricted,
944                     )
945                     get_related_klass_infos(klass_info, next_klass_infos)
946             fields_not_found = set(requested).difference(fields_found)
947             if fields_not_found:
948                 invalid_fields = ("'%s'" % s for s in fields_not_found)
949                 raise FieldError(
950                     'Invalid field name(s) given in select_related: %s. '
951                     'Choices are: %s' % (
952                         ', '.join(invalid_fields),
953                         ', '.join(_get_field_choices()) or '(none)',
954                     )
955                 )
956         return related_klass_infos
957 
958     def get_select_for_update_of_arguments(self):
959         """
960         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of
961         the query.
962         """
963         def _get_parent_klass_info(klass_info):
964             for parent_model, parent_link in klass_info['model']._meta.parents.items():
965                 parent_list = parent_model._meta.get_parent_list()
966                 yield {
967                     'model': parent_model,
968                     'field': parent_link,
969                     'reverse': False,
970                     'select_fields': [
971                         select_index
972                         for select_index in klass_info['select_fields']
973                         # Selected columns from a model or its parents.
974                         if (
975                             self.select[select_index][0].target.model == parent_model or
976                             self.select[select_index][0].target.model in parent_list
977                         )
978                     ],
979                 }
980 
981         def _get_first_selected_col_from_model(klass_info):
982             """
983             Find the first selected column from a model. If it doesn't exist,
984             don't lock a model.
985 
986             select_fields is filled recursively, so it also contains fields
987             from the parent models.
988             """
989             for select_index in klass_info['select_fields']:
990                 if self.select[select_index][0].target.model == klass_info['model']:
991                     return self.select[select_index][0]
992 
993         def _get_field_choices():
994             """Yield all allowed field paths in breadth-first search order."""
995             queue = collections.deque([(None, self.klass_info)])
996             while queue:
997                 parent_path, klass_info = queue.popleft()
998                 if parent_path is None:
999                     path = []
1000                     yield 'self'
1001                 else:
1002                     field = klass_info['field']
1003                     if klass_info['reverse']:
1004                         field = field.remote_field
1005                     path = parent_path + [field.name]
1006                     yield LOOKUP_SEP.join(path)
1007                 queue.extend(
1008                     (path, klass_info)
1009                     for klass_info in _get_parent_klass_info(klass_info)
1010                 )
1011                 queue.extend(
1012                     (path, klass_info)
1013                     for klass_info in klass_info.get('related_klass_infos', [])
1014                 )
1015         result = []
1016         invalid_names = []
1017         for name in self.query.select_for_update_of:
1018             klass_info = self.klass_info
1019             if name == 'self':
1020                 col = _get_first_selected_col_from_model(klass_info)
1021             else:
1022                 for part in name.split(LOOKUP_SEP):
1023                     klass_infos = (
1024                         *klass_info.get('related_klass_infos', []),
1025                         *_get_parent_klass_info(klass_info),
1026                     )
1027                     for related_klass_info in klass_infos:
1028                         field = related_klass_info['field']
1029                         if related_klass_info['reverse']:
1030                             field = field.remote_field
1031                         if field.name == part:
1032                             klass_info = related_klass_info
1033                             break
1034                     else:
1035                         klass_info = None
1036                         break
1037                 if klass_info is None:
1038                     invalid_names.append(name)
1039                     continue
1040                 col = _get_first_selected_col_from_model(klass_info)
1041             if col is not None:
1042                 if self.connection.features.select_for_update_of_column:
1043                     result.append(self.compile(col)[0])
1044                 else:
1045                     result.append(self.quote_name_unless_alias(col.alias))
1046         if invalid_names:
1047             raise FieldError(
1048                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '
1049                 'Only relational fields followed in the query are allowed. '
1050                 'Choices are: %s.' % (
1051                     ', '.join(invalid_names),
1052                     ', '.join(_get_field_choices()),
1053                 )
1054             )
1055         return result
1056 
1057     def deferred_to_columns(self):
1058         """
1059         Convert the self.deferred_loading data structure to mapping of table
1060         names to sets of column names which are to be loaded. Return the
1061         dictionary.
1062         """
1063         columns = {}
1064         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)
1065         return columns
1066 
1067     def get_converters(self, expressions):
1068         converters = {}
1069         for i, expression in enumerate(expressions):
1070             if expression:
1071                 backend_converters = self.connection.ops.get_db_converters(expression)
1072                 field_converters = expression.get_db_converters(self.connection)
1073                 if backend_converters or field_converters:
1074                     converters[i] = (backend_converters + field_converters, expression)
1075         return converters
1076 
1077     def apply_converters(self, rows, converters):
1078         connection = self.connection
1079         converters = list(converters.items())
1080         for row in map(list, rows):
1081             for pos, (convs, expression) in converters:
1082                 value = row[pos]
1083                 for converter in convs:
1084                     value = converter(value, expression, connection)
1085                 row[pos] = value
1086             yield row
1087 
1088     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,
1089                      chunk_size=GET_ITERATOR_CHUNK_SIZE):
1090         """Return an iterator over the results from executing this query."""
1091         if results is None:
1092             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)
1093         fields = [s[0] for s in self.select[0:self.col_count]]
1094         converters = self.get_converters(fields)
1095         rows = chain.from_iterable(results)
1096         if converters:
1097             rows = self.apply_converters(rows, converters)
1098             if tuple_expected:
1099                 rows = map(tuple, rows)
1100         return rows
1101 
1102     def has_results(self):
1103         """
1104         Backends (e.g. NoSQL) can override this in order to use optimized
1105         versions of "query has any results."
1106         """
1107         # This is always executed on a query clone, so we can modify self.query
1108         self.query.add_extra({'a': 1}, None, None, None, None, None)
1109         self.query.set_extra_mask(['a'])
1110         return bool(self.execute_sql(SINGLE))
1111 
1112     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
1113         """
1114         Run the query against the database and return the result(s). The
1115         return value is a single data item if result_type is SINGLE, or an
1116         iterator over the results if the result_type is MULTI.
1117 
1118         result_type is either MULTI (use fetchmany() to retrieve all rows),
1119         SINGLE (only retrieve a single row), or None. In this last case, the
1120         cursor is returned if any query is executed, since it's used by
1121         subclasses such as InsertQuery). It's possible, however, that no query
1122         is needed, as the filters describe an empty set. In that case, None is
1123         returned, to avoid any unnecessary database interaction.
1124         """
1125         result_type = result_type or NO_RESULTS
1126         try:
1127             sql, params = self.as_sql()
1128             if not sql:
1129                 raise EmptyResultSet
1130         except EmptyResultSet:
1131             if result_type == MULTI:
1132                 return iter([])
1133             else:
1134                 return
1135         if chunked_fetch:
1136             cursor = self.connection.chunked_cursor()
1137         else:
1138             cursor = self.connection.cursor()
1139         try:
1140             cursor.execute(sql, params)
1141         except Exception:
1142             # Might fail for server-side cursors (e.g. connection closed)
1143             cursor.close()
1144             raise
1145 
1146         if result_type == CURSOR:
1147             # Give the caller the cursor to process and close.
1148             return cursor
1149         if result_type == SINGLE:
1150             try:
1151                 val = cursor.fetchone()
1152                 if val:
1153                     return val[0:self.col_count]
1154                 return val
1155             finally:
1156                 # done with the cursor
1157                 cursor.close()
1158         if result_type == NO_RESULTS:
1159             cursor.close()
1160             return
1161 
1162         result = cursor_iter(
1163             cursor, self.connection.features.empty_fetchmany_value,
1164             self.col_count if self.has_extra_select else None,
1165             chunk_size,
1166         )
1167         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:
1168             try:
1169                 # If we are using non-chunked reads, we return the same data
1170                 # structure as normally, but ensure it is all read into memory
1171                 # before going any further. Use chunked_fetch if requested,
1172                 # unless the database doesn't support it.
1173                 return list(result)
1174             finally:
1175                 # done with the cursor
1176                 cursor.close()
1177         return result
1178 
1179     def as_subquery_condition(self, alias, columns, compiler):
1180         qn = compiler.quote_name_unless_alias
1181         qn2 = self.connection.ops.quote_name
1182 
1183         for index, select_col in enumerate(self.query.select):
1184             lhs_sql, lhs_params = self.compile(select_col)
1185             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))
1186             self.query.where.add(
1187                 RawSQL('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')
1188 
1189         sql, params = self.as_sql()
1190         return 'EXISTS (%s)' % sql, params
1191 
1192     def explain_query(self):
1193         result = list(self.execute_sql())
1194         # Some backends return 1 item tuples with strings, and others return
1195         # tuples with integers and strings. Flatten them out into strings.
1196         for row in result[0]:
1197             if not isinstance(row, str):
1198                 yield ' '.join(str(c) for c in row)
1199             else:
1200                 yield row

</code><intended_behavior>The SQLCompiler class may need a review or enhancement in the methods responsible for constructing the GROUP BY clause, such as `get_group_by`, ensuring that all conditions leading to the correct inclusion of annotations, particularly subquery annotations, are met. The intended behavior is to enhance the logic so that it accurately recognizes when annotations (especially those involving subqueries) necessitate inclusion in the GROUP BY clause to avoid result set duplication and maintain consistency with previous Django versions while still allowing for optimizations introduced in newer versions. This might include refining how the compiler assesses and decides the fields and expressions to include in the GROUP BY clause based on the annotations present in the queryset. The adjustments described aim to correct the regression, ensuring that the ORM's handling of subquery annotations in conjunction with GROUP BY clauses is both accurate and consistent with expected behavior, thus preserving the robustness and reliability of the Django ORM across versions.</intended_behavior>]
2025-10-15 04:34:41.006 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1061     def annotate(self, *args, **kwargs):
1062         """
1063         Return a query set in which the returned objects have been annotated
1064         with extra data or aggregations.
1065         """
1066         self._not_support_combined_queries('annotate')
1067         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1068         annotations = {}
1069         for arg in args:
1070             # The default_alias property may raise a TypeError.
1071             try:
1072                 if arg.default_alias in kwargs:
1073                     raise ValueError("The named annotation '%s' conflicts with the "
1074                                      "default name for another annotation."
1075                                      % arg.default_alias)
1076             except TypeError:
1077                 raise TypeError("Complex annotations require an alias")
1078             annotations[arg.default_alias] = arg
1079         annotations.update(kwargs)
1080 
1081         clone = self._chain()
1082         names = self._fields
1083         if names is None:
1084             names = set(chain.from_iterable(
1085                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1086                 for field in self.model._meta.get_fields()
1087             ))
1088 
1089         for alias, annotation in annotations.items():
1090             if alias in names:
1091                 raise ValueError("The annotation '%s' conflicts with a field on "
1092                                  "the model." % alias)
1093             if isinstance(annotation, FilteredRelation):
1094                 clone.query.add_filtered_relation(annotation, alias)
1095             else:
1096                 clone.query.add_annotation(annotation, alias, is_summary=False)
1097 
1098         for alias, annotation in clone.query.annotations.items():
1099             if alias in annotations and annotation.contains_aggregate:
1100                 if clone._fields is None:
1101                     clone.query.group_by = True
1102                 else:
1103                     clone.query.set_group_by()
1104                 break
1105 
1106         return clone

</code><intended_behavior>The `annotate` method should ensure that when generating the SQL for a queryset with multiple annotations, especially those that include subqueries, these annotations are correctly included in the GROUP BY section of the SQL query. This inclusion is critical to prevent duplicate results due to the omission of essential fields from the GROUP BY clause. To resolve the issue, the method may require adjustments in how it constructs the queryset's annotations, ensuring that subquery annotations are not omitted in any scenario. The fix should preserve the functionality of allowing complex annotations and aggregates within the same query, without introducing duplicates in the returned results.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
242 
243     def __setstate__(self, state):
244         msg = None
245         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
246         if pickled_version:
247             current_version = get_version()
248             if current_version != pickled_version:
249                 msg = (
250                     "Pickled queryset instance's Django version %s does not "
251                     "match the current version %s." % (pickled_version, current_version)
252                 )
253         else:
254             msg = "Pickled queryset instance's Django version is not specified."
255 
256         if msg:
257             warnings.warn(msg, RuntimeWarning, stacklevel=2)
258 
259         self.__dict__.update(state)
260 
261     def __repr__(self):
262         data = list(self[:REPR_OUTPUT_SIZE + 1])
263         if len(data) > REPR_OUTPUT_SIZE:
264             data[-1] = "...(remaining elements truncated)..."
265         return '<%s %r>' % (self.__class__.__name__, data)
266 
267     def __len__(self):
268         self._fetch_all()
269         return len(self._result_cache)
270 
271     def __iter__(self):
272         """
273         The queryset iterator protocol uses three nested iterators in the
274         default case:
275             1. sql.compiler.execute_sql()
276                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
277                  using cursor.fetchmany(). This part is responsible for
278                  doing some column masking, and returning the rows in chunks.
279             2. sql.compiler.results_iter()
280                - Returns one row at time. At this point the rows are still just
281                  tuples. In some cases the return values are converted to
282                  Python values at this location.
283             3. self.iterator()
284                - Responsible for turning the rows into model objects.
285         """
286         self._fetch_all()
287         return iter(self._result_cache)
288 
289     def __bool__(self):
290         self._fetch_all()
291         return bool(self._result_cache)
292 
293     def __getitem__(self, k):
294         """Retrieve an item or slice from the set of results."""
295         if not isinstance(k, (int, slice)):
296             raise TypeError(
297                 'QuerySet indices must be integers or slices, not %s.'
298                 % type(k).__name__
299             )
300         assert ((not isinstance(k, slice) and (k >= 0)) or
301                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
302                  (k.stop is None or k.stop >= 0))), \
303             "Negative indexing is not supported."
304 
305         if self._result_cache is not None:
306             return self._result_cache[k]
307 
308         if isinstance(k, slice):
309             qs = self._chain()
310             if k.start is not None:
311                 start = int(k.start)
312             else:
313                 start = None
314             if k.stop is not None:
315                 stop = int(k.stop)
316             else:
317                 stop = None
318             qs.query.set_limits(start, stop)
319             return list(qs)[::k.step] if k.step else qs
320 
321         qs = self._chain()
322         qs.query.set_limits(k, k + 1)
323         qs._fetch_all()
324         return qs._result_cache[0]
325 
326     def __and__(self, other):
327         self._merge_sanity_check(other)
328         if isinstance(other, EmptyQuerySet):
329             return other
330         if isinstance(self, EmptyQuerySet):
331             return self
332         combined = self._chain()
333         combined._merge_known_related_objects(other)
334         combined.query.combine(other.query, sql.AND)
335         return combined
336 
337     def __or__(self, other):
338         self._merge_sanity_check(other)
339         if isinstance(self, EmptyQuerySet):
340             return other
341         if isinstance(other, EmptyQuerySet):
342             return self
343         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
344         combined = query._chain()
345         combined._merge_known_related_objects(other)
346         if not other.query.can_filter():
347             other = other.model._base_manager.filter(pk__in=other.values('pk'))
348         combined.query.combine(other.query, sql.OR)
349         return combined
350 
351     ####################################
352     # METHODS THAT DO DATABASE QUERIES #
353     ####################################
354 
355     def _iterator(self, use_chunked_fetch, chunk_size):
356         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
357 
358     def iterator(self, chunk_size=2000):
359         """
360         An iterator over the results from applying this QuerySet to the
361         database.
362         """
363         if chunk_size <= 0:
364             raise ValueError('Chunk size must be strictly positive.')
365         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
366         return self._iterator(use_chunked_fetch, chunk_size)
367 
368     def aggregate(self, *args, **kwargs):
369         """
370         Return a dictionary containing the calculations (aggregation)
371         over the current queryset.
372 
373         If args is present the expression is passed as a kwarg using
374         the Aggregate object's default alias.
375         """
376         if self.query.distinct_fields:
377             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
378         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
379         for arg in args:
380             # The default_alias property raises TypeError if default_alias
381             # can't be set automatically or AttributeError if it isn't an
382             # attribute.
383             try:
384                 arg.default_alias
385             except (AttributeError, TypeError):
386                 raise TypeError("Complex aggregates require an alias")
387             kwargs[arg.default_alias] = arg
388 
389         query = self.query.chain()
390         for (alias, aggregate_expr) in kwargs.items():
391             query.add_annotation(aggregate_expr, alias, is_summary=True)
392             if not query.annotations[alias].contains_aggregate:
393                 raise TypeError("%s is not an aggregate expression" % alias)
394         return query.get_aggregation(self.db, kwargs)
395 
396     def count(self):
397         """
398         Perform a SELECT COUNT() and return the number of records as an
399         integer.
400 
401         If the QuerySet is already fully cached, return the length of the
402         cached results set to avoid multiple SELECT COUNT(*) calls.
403         """
404         if self._result_cache is not None:
405             return len(self._result_cache)
406 
407         return self.query.get_count(using=self.db)
408 
409     def get(self, *args, **kwargs):
410         """
411         Perform the query and return a single object matching the given
412         keyword arguments.
413         """
414         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
415         if self.query.can_filter() and not self.query.distinct_fields:
416             clone = clone.order_by()
417         limit = None
418         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
419             limit = MAX_GET_RESULTS
420             clone.query.set_limits(high=limit)
421         num = len(clone)
422         if num == 1:
423             return clone._result_cache[0]
424         if not num:
425             raise self.model.DoesNotExist(
426                 "%s matching query does not exist." %
427                 self.model._meta.object_name
428             )
429         raise self.model.MultipleObjectsReturned(
430             'get() returned more than one %s -- it returned %s!' % (
431                 self.model._meta.object_name,
432                 num if not limit or num < limit else 'more than %s' % (limit - 1),
433             )
434         )
435 
436     def create(self, **kwargs):
437         """
438         Create a new object with the given kwargs, saving it to the database
439         and returning the created object.
440         """
441         obj = self.model(**kwargs)
442         self._for_write = True
443         obj.save(force_insert=True, using=self.db)
444         return obj
445 
446     def _populate_pk_values(self, objs):
447         for obj in objs:
448             if obj.pk is None:
449                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
450 
451     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
452         """
453         Insert each of the instances into the database. Do *not* call
454         save() on each of the instances, do not send any pre/post_save
455         signals, and do not set the primary key attribute if it is an
456         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
457         Multi-table models are not supported.
458         """
459         # When you bulk insert you don't get the primary keys back (if it's an
460         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
461         # you can't insert into the child tables which references this. There
462         # are two workarounds:
463         # 1) This could be implemented if you didn't have an autoincrement pk
464         # 2) You could do it by doing O(n) normal inserts into the parent
465         #    tables to get the primary keys back and then doing a single bulk
466         #    insert into the childmost table.
467         # We currently set the primary keys on the objects when using
468         # PostgreSQL via the RETURNING ID clause. It should be possible for
469         # Oracle as well, but the semantics for extracting the primary keys is
470         # trickier so it's not done yet.
471         assert batch_size is None or batch_size > 0
472         # Check that the parents share the same concrete model with the our
473         # model to detect the inheritance pattern ConcreteGrandParent ->
474         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
475         # would not identify that case as involving multiple tables.
476         for parent in self.model._meta.get_parent_list():
477             if parent._meta.concrete_model is not self.model._meta.concrete_model:
478                 raise ValueError("Can't bulk create a multi-table inherited model")
479         if not objs:
480             return objs
481         self._for_write = True
482         connection = connections[self.db]
483         opts = self.model._meta
484         fields = opts.concrete_fields
485         objs = list(objs)
486         self._populate_pk_values(objs)
487         with transaction.atomic(using=self.db, savepoint=False):
488             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
489             if objs_with_pk:
490                 returned_columns = self._batched_insert(
491                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
492                 )
493                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
494                     for result, field in zip(results, opts.db_returning_fields):
495                         if field != opts.pk:
496                             setattr(obj_with_pk, field.attname, result)
497                 for obj_with_pk in objs_with_pk:
498                     obj_with_pk._state.adding = False
499                     obj_with_pk._state.db = self.db
500             if objs_without_pk:
501                 fields = [f for f in fields if not isinstance(f, AutoField)]
502                 returned_columns = self._batched_insert(
503                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
504                 )
505                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
506                     assert len(returned_columns) == len(objs_without_pk)
507                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
508                     for result, field in zip(results, opts.db_returning_fields):
509                         setattr(obj_without_pk, field.attname, result)
510                     obj_without_pk._state.adding = False
511                     obj_without_pk._state.db = self.db
512 
513         return objs
514 
515     def bulk_update(self, objs, fields, batch_size=None):
516         """
517         Update the given fields in each of the given objects in the database.
518         """
519         if batch_size is not None and batch_size < 0:
520             raise ValueError('Batch size must be a positive integer.')
521         if not fields:
522             raise ValueError('Field names must be given to bulk_update().')
523         objs = tuple(objs)
524         if any(obj.pk is None for obj in objs):
525             raise ValueError('All bulk_update() objects must have a primary key set.')
526         fields = [self.model._meta.get_field(name) for name in fields]
527         if any(not f.concrete or f.many_to_many for f in fields):
528             raise ValueError('bulk_update() can only be used with concrete fields.')
529         if any(f.primary_key for f in fields):
530             raise ValueError('bulk_update() cannot be used with primary key fields.')
531         if not objs:
532             return
533         # PK is used twice in the resulting update query, once in the filter
534         # and once in the WHEN. Each field will also have one CAST.
535         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
536         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
537         requires_casting = connections[self.db].features.requires_casted_case_in_updates
538         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
539         updates = []
540         for batch_objs in batches:
541             update_kwargs = {}
542             for field in fields:
543                 when_statements = []
544                 for obj in batch_objs:
545                     attr = getattr(obj, field.attname)
546                     if not isinstance(attr, Expression):
547                         attr = Value(attr, output_field=field)
548                     when_statements.append(When(pk=obj.pk, then=attr))
549                 case_statement = Case(*when_statements, output_field=field)
550                 if requires_casting:
551                     case_statement = Cast(case_statement, output_field=field)
552                 update_kwargs[field.attname] = case_statement
553             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
554         with transaction.atomic(using=self.db, savepoint=False):
555             for pks, update_kwargs in updates:
556                 self.filter(pk__in=pks).update(**update_kwargs)
557     bulk_update.alters_data = True
558 
559     def get_or_create(self, defaults=None, **kwargs):
560         """
561         Look up an object with the given kwargs, creating one if necessary.
562         Return a tuple of (object, created), where created is a boolean
563         specifying whether an object was created.
564         """
565         # The get() needs to be targeted at the write database in order
566         # to avoid potential transaction consistency problems.
567         self._for_write = True
568         try:
569             return self.get(**kwargs), False
570         except self.model.DoesNotExist:
571             params = self._extract_model_params(defaults, **kwargs)
572             return self._create_object_from_params(kwargs, params)
573 
574     def update_or_create(self, defaults=None, **kwargs):
575         """
576         Look up an object with the given kwargs, updating one with defaults
577         if it exists, otherwise create a new one.
578         Return a tuple (object, created), where created is a boolean
579         specifying whether an object was created.
580         """
581         defaults = defaults or {}
582         self._for_write = True
583         with transaction.atomic(using=self.db):
584             try:
585                 obj = self.select_for_update().get(**kwargs)
586             except self.model.DoesNotExist:
587                 params = self._extract_model_params(defaults, **kwargs)
588                 # Lock the row so that a concurrent update is blocked until
589                 # after update_or_create() has performed its save.
590                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
591                 if created:
592                     return obj, created
593             for k, v in resolve_callables(defaults):
594                 setattr(obj, k, v)
595             obj.save(using=self.db)
596         return obj, False
597 
598     def _create_object_from_params(self, lookup, params, lock=False):
599         """
600         Try to create an object using passed params. Used by get_or_create()
601         and update_or_create().
602         """
603         try:
604             with transaction.atomic(using=self.db):
605                 params = dict(resolve_callables(params))
606                 obj = self.create(**params)
607             return obj, True
608         except IntegrityError:
609             try:
610                 qs = self.select_for_update() if lock else self
611                 return qs.get(**lookup), False
612             except self.model.DoesNotExist:
613                 pass
614             raise
615 
616     def _extract_model_params(self, defaults, **kwargs):
617         """
618         Prepare `params` for creating a model instance based on the given
619         kwargs; for use by get_or_create() and update_or_create().
620         """
621         defaults = defaults or {}
622         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
623         params.update(defaults)
624         property_names = self.model._meta._property_names
625         invalid_params = []
626         for param in params:
627             try:
628                 self.model._meta.get_field(param)
629             except exceptions.FieldDoesNotExist:
630                 # It's okay to use a model's property if it has a setter.
631                 if not (param in property_names and getattr(self.model, param).fset):
632                     invalid_params.append(param)
633         if invalid_params:
634             raise exceptions.FieldError(
635                 "Invalid field name(s) for model %s: '%s'." % (
636                     self.model._meta.object_name,
637                     "', '".join(sorted(invalid_params)),
638                 ))
639         return params
640 
641     def _earliest(self, *fields):
642         """
643         Return the earliest object according to fields (if given) or by the
644         model's Meta.get_latest_by.
645         """
646         if fields:
647             order_by = fields
648         else:
649             order_by = getattr(self.model._meta, 'get_latest_by')
650             if order_by and not isinstance(order_by, (tuple, list)):
651                 order_by = (order_by,)
652         if order_by is None:
653             raise ValueError(
654                 "earliest() and latest() require either fields as positional "
655                 "arguments or 'get_latest_by' in the model's Meta."
656             )
657 
658         assert not self.query.is_sliced, \
659             "Cannot change a query once a slice has been taken."
660         obj = self._chain()
661         obj.query.set_limits(high=1)
662         obj.query.clear_ordering(force_empty=True)
663         obj.query.add_ordering(*order_by)
664         return obj.get()
665 
666     def earliest(self, *fields):
667         return self._earliest(*fields)
668 
669     def latest(self, *fields):
670         return self.reverse()._earliest(*fields)
671 
672     def first(self):
673         """Return the first object of a query or None if no match is found."""
674         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
675             return obj
676 
677     def last(self):
678         """Return the last object of a query or None if no match is found."""
679         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
680             return obj
681 
682     def in_bulk(self, id_list=None, *, field_name='pk'):
683         """
684         Return a dictionary mapping each of the given IDs to the object with
685         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
686         """
687         assert not self.query.is_sliced, \
688             "Cannot use 'limit' or 'offset' with in_bulk"
689         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
690             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
691         if id_list is not None:
692             if not id_list:
693                 return {}
694             filter_key = '{}__in'.format(field_name)
695             batch_size = connections[self.db].features.max_query_params
696             id_list = tuple(id_list)
697             # If the database has a limit on the number of query parameters
698             # (e.g. SQLite), retrieve objects in batches if necessary.
699             if batch_size and batch_size < len(id_list):
700                 qs = ()
701                 for offset in range(0, len(id_list), batch_size):
702                     batch = id_list[offset:offset + batch_size]
703                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
704             else:
705                 qs = self.filter(**{filter_key: id_list}).order_by()
706         else:
707             qs = self._chain()
708         return {getattr(obj, field_name): obj for obj in qs}
709 
710     def delete(self):
711         """Delete the records in the current QuerySet."""
712         self._not_support_combined_queries('delete')
713         assert not self.query.is_sliced, \
714             "Cannot use 'limit' or 'offset' with delete."
715 
716         if self._fields is not None:
717             raise TypeError("Cannot call delete() after .values() or .values_list()")
718 
719         del_query = self._chain()
720 
721         # The delete is actually 2 queries - one to find related objects,
722         # and one to delete. Make sure that the discovery of related
723         # objects is performed on the same database as the deletion.
724         del_query._for_write = True
725 
726         # Disable non-supported fields.
727         del_query.query.select_for_update = False
728         del_query.query.select_related = False
729         del_query.query.clear_ordering(force_empty=True)
730 
731         collector = Collector(using=del_query.db)
732         collector.collect(del_query)
733         deleted, _rows_count = collector.delete()
734 
735         # Clear the result cache, in case this QuerySet gets reused.
736         self._result_cache = None
737         return deleted, _rows_count
738 
739     delete.alters_data = True
740     delete.queryset_only = True
741 
742     def _raw_delete(self, using):
743         """
744         Delete objects found from the given queryset in single direct SQL
745         query. No signals are sent and there is no protection for cascades.
746         """
747         query = self.query.clone()
748         query.__class__ = sql.DeleteQuery
749         cursor = query.get_compiler(using).execute_sql(CURSOR)
750         if cursor:
751             with cursor:
752                 return cursor.rowcount
753         return 0
754     _raw_delete.alters_data = True
755 
756     def update(self, **kwargs):
757         """
758         Update all elements in the current QuerySet, setting all the given
759         fields to the appropriate values.
760         """
761         self._not_support_combined_queries('update')
762         assert not self.query.is_sliced, \
763             "Cannot update a query once a slice has been taken."
764         self._for_write = True
765         query = self.query.chain(sql.UpdateQuery)
766         query.add_update_values(kwargs)
767         # Clear any annotations so that they won't be present in subqueries.
768         query.annotations = {}
769         with transaction.mark_for_rollback_on_error(using=self.db):
770             rows = query.get_compiler(self.db).execute_sql(CURSOR)
771         self._result_cache = None
772         return rows
773     update.alters_data = True
774 
775     def _update(self, values):
776         """
777         A version of update() that accepts field objects instead of field names.
778         Used primarily for model saving and not intended for use by general
779         code (it requires too much poking around at model internals to be
780         useful at that level).
781         """
782         assert not self.query.is_sliced, \
783             "Cannot update a query once a slice has been taken."
784         query = self.query.chain(sql.UpdateQuery)
785         query.add_update_fields(values)
786         # Clear any annotations so that they won't be present in subqueries.
787         query.annotations = {}
788         self._result_cache = None
789         return query.get_compiler(self.db).execute_sql(CURSOR)
790     _update.alters_data = True
791     _update.queryset_only = False
792 
793     def exists(self):
794         if self._result_cache is None:
795             return self.query.has_results(using=self.db)
796         return bool(self._result_cache)
797 
798     def _prefetch_related_objects(self):
799         # This method can only be called once the result cache has been filled.
800         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
801         self._prefetch_done = True
802 
803     def explain(self, *, format=None, **options):
804         return self.query.explain(using=self.db, format=format, **options)
805 
806     ##################################################
807     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
808     ##################################################
809 
810     def raw(self, raw_query, params=None, translations=None, using=None):
811         if using is None:
812             using = self.db
813         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
814         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
815         return qs
816 
817     def _values(self, *fields, **expressions):
818         clone = self._chain()
819         if expressions:
820             clone = clone.annotate(**expressions)
821         clone._fields = fields
822         clone.query.set_values(fields)
823         return clone
824 
825     def values(self, *fields, **expressions):
826         fields += tuple(expressions)
827         clone = self._values(*fields, **expressions)
828         clone._iterable_class = ValuesIterable
829         return clone
830 
831     def values_list(self, *fields, flat=False, named=False):
832         if flat and named:
833             raise TypeError("'flat' and 'named' can't be used together.")
834         if flat and len(fields) > 1:
835             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
836 
837         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
838         _fields = []
839         expressions = {}
840         counter = 1
841         for field in fields:
842             if hasattr(field, 'resolve_expression'):
843                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
844                 while True:
845                     field_id = field_id_prefix + str(counter)
846                     counter += 1
847                     if field_id not in field_names:
848                         break
849                 expressions[field_id] = field
850                 _fields.append(field_id)
851             else:
852                 _fields.append(field)
853 
854         clone = self._values(*_fields, **expressions)
855         clone._iterable_class = (
856             NamedValuesListIterable if named
857             else FlatValuesListIterable if flat
858             else ValuesListIterable
859         )
860         return clone
861 
862     def dates(self, field_name, kind, order='ASC'):
863         """
864         Return a list of date objects representing all available dates for
865         the given field_name, scoped to 'kind'.
866         """
867         assert kind in ('year', 'month', 'week', 'day'), \
868             "'kind' must be one of 'year', 'month', 'week', or 'day'."
869         assert order in ('ASC', 'DESC'), \
870             "'order' must be either 'ASC' or 'DESC'."
871         return self.annotate(
872             datefield=Trunc(field_name, kind, output_field=DateField()),
873             plain_field=F(field_name)
874         ).values_list(
875             'datefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
877 
878     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
879         """
880         Return a list of datetime objects representing all available
881         datetimes for the given field_name, scoped to 'kind'.
882         """
883         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
884             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
885         assert order in ('ASC', 'DESC'), \
886             "'order' must be either 'ASC' or 'DESC'."
887         if settings.USE_TZ:
888             if tzinfo is None:
889                 tzinfo = timezone.get_current_timezone()
890         else:
891             tzinfo = None
892         return self.annotate(
893             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
894             plain_field=F(field_name)
895         ).values_list(
896             'datetimefield', flat=True
897         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
898 
899     def none(self):
900         """Return an empty QuerySet."""
901         clone = self._chain()
902         clone.query.set_empty()
903         return clone
904 
905     ##################################################################
906     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
907     ##################################################################
908 
909     def all(self):
910         """
911         Return a new QuerySet that is a copy of the current one. This allows a
912         QuerySet to proxy for a model manager in some cases.
913         """
914         return self._chain()
915 
916     def filter(self, *args, **kwargs):
917         """
918         Return a new QuerySet instance with the args ANDed to the existing
919         set.
920         """
921         self._not_support_combined_queries('filter')
922         return self._filter_or_exclude(False, *args, **kwargs)
923 
924     def exclude(self, *args, **kwargs):
925         """
926         Return a new QuerySet instance with NOT (args) ANDed to the existing
927         set.
928         """
929         self._not_support_combined_queries('exclude')
930         return self._filter_or_exclude(True, *args, **kwargs)
931 
932     def _filter_or_exclude(self, negate, *args, **kwargs):
933         if args or kwargs:
934             assert not self.query.is_sliced, \
935                 "Cannot filter a query once a slice has been taken."
936 
937         clone = self._chain()
938         if self._defer_next_filter:
939             self._defer_next_filter = False
940             clone._deferred_filter = negate, args, kwargs
941         else:
942             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
943         return clone
944 
945     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
946         if negate:
947             self._query.add_q(~Q(*args, **kwargs))
948         else:
949             self._query.add_q(Q(*args, **kwargs))
950 
951     def complex_filter(self, filter_obj):
952         """
953         Return a new QuerySet instance with filter_obj added to the filters.
954 
955         filter_obj can be a Q object or a dictionary of keyword lookup
956         arguments.
957 
958         This exists to support framework features such as 'limit_choices_to',
959         and usually it will be more natural to use other methods.
960         """
961         if isinstance(filter_obj, Q):
962             clone = self._chain()
963             clone.query.add_q(filter_obj)
964             return clone
965         else:
966             return self._filter_or_exclude(False, **filter_obj)
967 
968     def _combinator_query(self, combinator, *other_qs, all=False):
969         # Clone the query to inherit the select list and everything
970         clone = self._chain()
971         # Clear limits and ordering so they can be reapplied
972         clone.query.clear_ordering(True)
973         clone.query.clear_limits()
974         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
975         clone.query.combinator = combinator
976         clone.query.combinator_all = all
977         return clone
978 
979     def union(self, *other_qs, all=False):
980         # If the query is an EmptyQuerySet, combine all nonempty querysets.
981         if isinstance(self, EmptyQuerySet):
982             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
983             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
984         return self._combinator_query('union', *other_qs, all=all)
985 
986     def intersection(self, *other_qs):
987         # If any query is an EmptyQuerySet, return it.
988         if isinstance(self, EmptyQuerySet):
989             return self
990         for other in other_qs:
991             if isinstance(other, EmptyQuerySet):
992                 return other
993         return self._combinator_query('intersection', *other_qs)
994 
995     def difference(self, *other_qs):
996         # If the query is an EmptyQuerySet, return it.
997         if isinstance(self, EmptyQuerySet):
998             return self
999         return self._combinator_query('difference', *other_qs)
1000 
1001     def select_for_update(self, nowait=False, skip_locked=False, of=()):
1002         """
1003         Return a new QuerySet instance that will select objects with a
1004         FOR UPDATE lock.
1005         """
1006         if nowait and skip_locked:
1007             raise ValueError('The nowait option cannot be used with skip_locked.')
1008         obj = self._chain()
1009         obj._for_write = True
1010         obj.query.select_for_update = True
1011         obj.query.select_for_update_nowait = nowait
1012         obj.query.select_for_update_skip_locked = skip_locked
1013         obj.query.select_for_update_of = of
1014         return obj
1015 
1016     def select_related(self, *fields):
1017         """
1018         Return a new QuerySet instance that will select related objects.
1019 
1020         If fields are specified, they must be ForeignKey fields and only those
1021         related objects are included in the selection.
1022 
1023         If select_related(None) is called, clear the list.
1024         """
1025         self._not_support_combined_queries('select_related')
1026         if self._fields is not None:
1027             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1028 
1029         obj = self._chain()
1030         if fields == (None,):
1031             obj.query.select_related = False
1032         elif fields:
1033             obj.query.add_select_related(fields)
1034         else:
1035             obj.query.select_related = True
1036         return obj
1037 
1038     def prefetch_related(self, *lookups):
1039         """
1040         Return a new QuerySet instance that will prefetch the specified
1041         Many-To-One and Many-To-Many related objects when the QuerySet is
1042         evaluated.
1043 
1044         When prefetch_related() is called more than once, append to the list of
1045         prefetch lookups. If prefetch_related(None) is called, clear the list.
1046         """
1047         self._not_support_combined_queries('prefetch_related')
1048         clone = self._chain()
1049         if lookups == (None,):
1050             clone._prefetch_related_lookups = ()
1051         else:
1052             for lookup in lookups:
1053                 if isinstance(lookup, Prefetch):
1054                     lookup = lookup.prefetch_to
1055                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1056                 if lookup in self.query._filtered_relations:
1057                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1058             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1059         return clone
1060 
1061     def annotate(self, *args, **kwargs):
1062         """
1063         Return a query set in which the returned objects have been annotated
1064         with extra data or aggregations.
1065         """
1066         self._not_support_combined_queries('annotate')
1067         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1068         annotations = {}
1069         for arg in args:
1070             # The default_alias property may raise a TypeError.
1071             try:
1072                 if arg.default_alias in kwargs:
1073                     raise ValueError("The named annotation '%s' conflicts with the "
1074                                      "default name for another annotation."
1075                                      % arg.default_alias)
1076             except TypeError:
1077                 raise TypeError("Complex annotations require an alias")
1078             annotations[arg.default_alias] = arg
1079         annotations.update(kwargs)
1080 
1081         clone = self._chain()
1082         names = self._fields
1083         if names is None:
1084             names = set(chain.from_iterable(
1085                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1086                 for field in self.model._meta.get_fields()
1087             ))
1088 
1089         for alias, annotation in annotations.items():
1090             if alias in names:
1091                 raise ValueError("The annotation '%s' conflicts with a field on "
1092                                  "the model." % alias)
1093             if isinstance(annotation, FilteredRelation):
1094                 clone.query.add_filtered_relation(annotation, alias)
1095             else:
1096                 clone.query.add_annotation(annotation, alias, is_summary=False)
1097 
1098         for alias, annotation in clone.query.annotations.items():
1099             if alias in annotations and annotation.contains_aggregate:
1100                 if clone._fields is None:
1101                     clone.query.group_by = True
1102                 else:
1103                     clone.query.set_group_by()
1104                 break
1105 
1106         return clone
1107 
1108     def order_by(self, *field_names):
1109         """Return a new QuerySet instance with the ordering changed."""
1110         assert not self.query.is_sliced, \
1111             "Cannot reorder a query once a slice has been taken."
1112         obj = self._chain()
1113         obj.query.clear_ordering(force_empty=False)
1114         obj.query.add_ordering(*field_names)
1115         return obj
1116 
1117     def distinct(self, *field_names):
1118         """
1119         Return a new QuerySet instance that will select only distinct results.
1120         """
1121         assert not self.query.is_sliced, \
1122             "Cannot create distinct fields once a slice has been taken."
1123         obj = self._chain()
1124         obj.query.add_distinct_fields(*field_names)
1125         return obj
1126 
1127     def extra(self, select=None, where=None, params=None, tables=None,
1128               order_by=None, select_params=None):
1129         """Add extra SQL fragments to the query."""
1130         self._not_support_combined_queries('extra')
1131         assert not self.query.is_sliced, \
1132             "Cannot change a query once a slice has been taken"
1133         clone = self._chain()
1134         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1135         return clone
1136 
1137     def reverse(self):
1138         """Reverse the ordering of the QuerySet."""
1139         if self.query.is_sliced:
1140             raise TypeError('Cannot reverse a query once a slice has been taken.')
1141         clone = self._chain()
1142         clone.query.standard_ordering = not clone.query.standard_ordering
1143         return clone
1144 
1145     def defer(self, *fields):
1146         """
1147         Defer the loading of data for certain fields until they are accessed.
1148         Add the set of deferred fields to any existing set of deferred fields.
1149         The only exception to this is if None is passed in as the only
1150         parameter, in which case removal all deferrals.
1151         """
1152         self._not_support_combined_queries('defer')
1153         if self._fields is not None:
1154             raise TypeError("Cannot call defer() after .values() or .values_list()")
1155         clone = self._chain()
1156         if fields == (None,):
1157             clone.query.clear_deferred_loading()
1158         else:
1159             clone.query.add_deferred_loading(fields)
1160         return clone
1161 
1162     def only(self, *fields):
1163         """
1164         Essentially, the opposite of defer(). Only the fields passed into this
1165         method and that are not already specified as deferred are loaded
1166         immediately when the queryset is evaluated.
1167         """
1168         self._not_support_combined_queries('only')
1169         if self._fields is not None:
1170             raise TypeError("Cannot call only() after .values() or .values_list()")
1171         if fields == (None,):
1172             # Can only pass None to defer(), not only(), as the rest option.
1173             # That won't stop people trying to do this, so let's be explicit.
1174             raise TypeError("Cannot pass None as an argument to only().")
1175         for field in fields:
1176             field = field.split(LOOKUP_SEP, 1)[0]
1177             if field in self.query._filtered_relations:
1178                 raise ValueError('only() is not supported with FilteredRelation.')
1179         clone = self._chain()
1180         clone.query.add_immediate_loading(fields)
1181         return clone
1182 
1183     def using(self, alias):
1184         """Select which database this QuerySet should execute against."""
1185         clone = self._chain()
1186         clone._db = alias
1187         return clone
1188 
1189     ###################################
1190     # PUBLIC INTROSPECTION ATTRIBUTES #
1191     ###################################
1192 
1193     @property
1194     def ordered(self):
1195         """
1196         Return True if the QuerySet is ordered -- i.e. has an order_by()
1197         clause or a default ordering on the model (or is empty).
1198         """
1199         if isinstance(self, EmptyQuerySet):
1200             return True
1201         if self.query.extra_order_by or self.query.order_by:
1202             return True
1203         elif self.query.default_ordering and self.query.get_meta().ordering:
1204             return True
1205         else:
1206             return False
1207 
1208     @property
1209     def db(self):
1210         """Return the database used if this query is executed now."""
1211         if self._for_write:
1212             return self._db or router.db_for_write(self.model, **self._hints)
1213         return self._db or router.db_for_read(self.model, **self._hints)
1214 
1215     ###################
1216     # PRIVATE METHODS #
1217     ###################
1218 
1219     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1220         """
1221         Insert a new record for the given model. This provides an interface to
1222         the InsertQuery class and is how Model.save() is implemented.
1223         """
1224         self._for_write = True
1225         if using is None:
1226             using = self.db
1227         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1228         query.insert_values(fields, objs, raw=raw)
1229         return query.get_compiler(using=using).execute_sql(returning_fields)
1230     _insert.alters_data = True
1231     _insert.queryset_only = False
1232 
1233     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1234         """
1235         Helper method for bulk_create() to insert objs one batch at a time.
1236         """
1237         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1238             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1239         ops = connections[self.db].ops
1240         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1241         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1242         inserted_rows = []
1243         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1244         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1245             if bulk_return and not ignore_conflicts:
1246                 inserted_columns = self._insert(
1247                     item, fields=fields, using=self.db,
1248                     returning_fields=self.model._meta.db_returning_fields,
1249                     ignore_conflicts=ignore_conflicts,
1250                 )
1251                 if isinstance(inserted_columns, list):
1252                     inserted_rows.extend(inserted_columns)
1253                 else:
1254                     inserted_rows.append(inserted_columns)
1255             else:
1256                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1257         return inserted_rows
1258 
1259     def _chain(self, **kwargs):
1260         """
1261         Return a copy of the current QuerySet that's ready for another
1262         operation.
1263         """
1264         obj = self._clone()
1265         if obj._sticky_filter:
1266             obj.query.filter_is_sticky = True
1267             obj._sticky_filter = False
1268         obj.__dict__.update(kwargs)
1269         return obj
1270 
1271     def _clone(self):
1272         """
1273         Return a copy of the current QuerySet. A lightweight alternative
1274         to deepcopy().
1275         """
1276         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1277         c._sticky_filter = self._sticky_filter
1278         c._for_write = self._for_write
1279         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1280         c._known_related_objects = self._known_related_objects
1281         c._iterable_class = self._iterable_class
1282         c._fields = self._fields
1283         return c
1284 
1285     def _fetch_all(self):
1286         if self._result_cache is None:
1287             self._result_cache = list(self._iterable_class(self))
1288         if self._prefetch_related_lookups and not self._prefetch_done:
1289             self._prefetch_related_objects()
1290 
1291     def _next_is_sticky(self):
1292         """
1293         Indicate that the next filter call and the one following that should
1294         be treated as a single filter. This is only important when it comes to
1295         determining when to reuse tables for many-to-many filters. Required so
1296         that we can filter naturally on the results of related managers.
1297 
1298         This doesn't return a clone of the current QuerySet (it returns
1299         "self"). The method is only used internally and should be immediately
1300         followed by a filter() that does create a clone.
1301         """
1302         self._sticky_filter = True
1303         return self
1304 
1305     def _merge_sanity_check(self, other):
1306         """Check that two QuerySet classes may be merged."""
1307         if self._fields is not None and (
1308                 set(self.query.values_select) != set(other.query.values_select) or
1309                 set(self.query.extra_select) != set(other.query.extra_select) or
1310                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1311             raise TypeError(
1312                 "Merging '%s' classes must involve the same values in each case."
1313                 % self.__class__.__name__
1314             )
1315 
1316     def _merge_known_related_objects(self, other):
1317         """
1318         Keep track of all known related objects from either QuerySet instance.
1319         """
1320         for field, objects in other._known_related_objects.items():
1321             self._known_related_objects.setdefault(field, {}).update(objects)
1322 
1323     def resolve_expression(self, *args, **kwargs):
1324         if self._fields and len(self._fields) > 1:
1325             # values() queryset can only be used as nested queries
1326             # if they are set up to select only a single field.
1327             raise TypeError('Cannot use multi-field values as a filter value.')
1328         query = self.query.resolve_expression(*args, **kwargs)
1329         query._db = self._db
1330         return query
1331     resolve_expression.queryset_only = True
1332 
1333     def _add_hints(self, **hints):
1334         """
1335         Update hinting information for use by routers. Add new key/values or
1336         overwrite existing key/values.
1337         """
1338         self._hints.update(hints)
1339 
1340     def _has_filters(self):
1341         """
1342         Check if this QuerySet has any filtering going on. This isn't
1343         equivalent with checking if all objects are present in results, for
1344         example, qs[1:]._has_filters() -> False.
1345         """
1346         return self.query.has_filters()
1347 
1348     @staticmethod
1349     def _validate_values_are_expressions(values, method_name):
1350         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1351         if invalid_args:
1352             raise TypeError(
1353                 'QuerySet.%s() received non-expression(s): %s.' % (
1354                     method_name,
1355                     ', '.join(invalid_args),
1356                 )
1357             )
1358 
1359     def _not_support_combined_queries(self, operation_name):
1360         if self.query.combinator:
1361             raise NotSupportedError(
1362                 'Calling QuerySet.%s() after %s() is not supported.'
1363                 % (operation_name, self.query.combinator)
1364             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/compiler.py</file>
<class>SQLCompiler</class>
<method>None</method>
<code>
21 class SQLCompiler:
22     def __init__(self, query, connection, using):
23         self.query = query
24         self.connection = connection
25         self.using = using
26         self.quote_cache = {'*': '*'}
27         # The select, klass_info, and annotations are needed by QuerySet.iterator()
28         # these are set as a side-effect of executing the query. Note that we calculate
29         # separately a list of extra select columns needed for grammatical correctness
30         # of the query, but these columns are not included in self.select.
31         self.select = None
32         self.annotation_col_map = None
33         self.klass_info = None
34         # Multiline ordering SQL clause may appear from RawSQL.
35         self.ordering_parts = re.compile(r'^(.*)\s(ASC|DESC)(.*)', re.MULTILINE | re.DOTALL)
36         self._meta_ordering = None
37 
38     def setup_query(self):
39         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):
40             self.query.get_initial_alias()
41         self.select, self.klass_info, self.annotation_col_map = self.get_select()
42         self.col_count = len(self.select)
43 
44     def pre_sql_setup(self):
45         """
46         Do any necessary class setup immediately prior to producing SQL. This
47         is for things that can't necessarily be done in __init__ because we
48         might not have all the pieces in place at that time.
49         """
50         self.setup_query()
51         order_by = self.get_order_by()
52         self.where, self.having = self.query.where.split_having()
53         extra_select = self.get_extra_select(order_by, self.select)
54         self.has_extra_select = bool(extra_select)
55         group_by = self.get_group_by(self.select + extra_select, order_by)
56         return extra_select, order_by, group_by
57 
58     def get_group_by(self, select, order_by):
59         """
60         Return a list of 2-tuples of form (sql, params).
61 
62         The logic of what exactly the GROUP BY clause contains is hard
63         to describe in other words than "if it passes the test suite,
64         then it is correct".
65         """
66         # Some examples:
67         #     SomeModel.objects.annotate(Count('somecol'))
68         #     GROUP BY: all fields of the model
69         #
70         #    SomeModel.objects.values('name').annotate(Count('somecol'))
71         #    GROUP BY: name
72         #
73         #    SomeModel.objects.annotate(Count('somecol')).values('name')
74         #    GROUP BY: all cols of the model
75         #
76         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')
77         #    GROUP BY: name, pk
78         #
79         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')
80         #    GROUP BY: name, pk
81         #
82         # In fact, the self.query.group_by is the minimal set to GROUP BY. It
83         # can't be ever restricted to a smaller set, but additional columns in
84         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately
85         # the end result is that it is impossible to force the query to have
86         # a chosen GROUP BY clause - you can almost do this by using the form:
87         #     .values(*wanted_cols).annotate(AnAggregate())
88         # but any later annotations, extra selects, values calls that
89         # refer some column outside of the wanted_cols, order_by, or even
90         # filter calls can alter the GROUP BY clause.
91 
92         # The query.group_by is either None (no GROUP BY at all), True
93         # (group by select fields), or a list of expressions to be added
94         # to the group by.
95         if self.query.group_by is None:
96             return []
97         expressions = []
98         if self.query.group_by is not True:
99             # If the group by is set to a list (by .values() call most likely),
100             # then we need to add everything in it to the GROUP BY clause.
101             # Backwards compatibility hack for setting query.group_by. Remove
102             # when  we have public API way of forcing the GROUP BY clause.
103             # Converts string references to expressions.
104             for expr in self.query.group_by:
105                 if not hasattr(expr, 'as_sql'):
106                     expressions.append(self.query.resolve_ref(expr))
107                 else:
108                     expressions.append(expr)
109         # Note that even if the group_by is set, it is only the minimal
110         # set to group by. So, we need to add cols in select, order_by, and
111         # having into the select in any case.
112         ref_sources = {
113             expr.source for expr in expressions if isinstance(expr, Ref)
114         }
115         for expr, _, _ in select:
116             # Skip members of the select clause that are already included
117             # by reference.
118             if expr in ref_sources:
119                 continue
120             cols = expr.get_group_by_cols()
121             for col in cols:
122                 expressions.append(col)
123         for expr, (sql, params, is_ref) in order_by:
124             # Skip References to the select clause, as all expressions in the
125             # select clause are already part of the group by.
126             if not is_ref:
127                 expressions.extend(expr.get_group_by_cols())
128         having_group_by = self.having.get_group_by_cols() if self.having else ()
129         for expr in having_group_by:
130             expressions.append(expr)
131         result = []
132         seen = set()
133         expressions = self.collapse_group_by(expressions, having_group_by)
134 
135         for expr in expressions:
136             sql, params = self.compile(expr)
137             params_hash = make_hashable(params)
138             if (sql, params_hash) not in seen:
139                 result.append((sql, params))
140                 seen.add((sql, params_hash))
141         return result
142 
143     def collapse_group_by(self, expressions, having):
144         # If the DB can group by primary key, then group by the primary key of
145         # query's main model. Note that for PostgreSQL the GROUP BY clause must
146         # include the primary key of every table, but for MySQL it is enough to
147         # have the main table's primary key.
148         if self.connection.features.allows_group_by_pk:
149             # Determine if the main model's primary key is in the query.
150             pk = None
151             for expr in expressions:
152                 # Is this a reference to query's base table primary key? If the
153                 # expression isn't a Col-like, then skip the expression.
154                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and
155                         getattr(expr, 'alias', None) == self.query.base_table):
156                     pk = expr
157                     break
158             # If the main model's primary key is in the query, group by that
159             # field, HAVING expressions, and expressions associated with tables
160             # that don't have a primary key included in the grouped columns.
161             if pk:
162                 pk_aliases = {
163                     expr.alias for expr in expressions
164                     if hasattr(expr, 'target') and expr.target.primary_key
165                 }
166                 expressions = [pk] + [
167                     expr for expr in expressions
168                     if expr in having or (
169                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases
170                     )
171                 ]
172         elif self.connection.features.allows_group_by_selected_pks:
173             # Filter out all expressions associated with a table's primary key
174             # present in the grouped columns. This is done by identifying all
175             # tables that have their primary key included in the grouped
176             # columns and removing non-primary key columns referring to them.
177             # Unmanaged models are excluded because they could be representing
178             # database views on which the optimization might not be allowed.
179             pks = {
180                 expr for expr in expressions
181                 if (
182                     hasattr(expr, 'target') and
183                     expr.target.primary_key and
184                     self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
185                 )
186             }
187             aliases = {expr.alias for expr in pks}
188             expressions = [
189                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases
190             ]
191         return expressions
192 
193     def get_select(self):
194         """
195         Return three values:
196         - a list of 3-tuples of (expression, (sql, params), alias)
197         - a klass_info structure,
198         - a dictionary of annotations
199 
200         The (sql, params) is what the expression will produce, and alias is the
201         "AS alias" for the column (possibly None).
202 
203         The klass_info structure contains the following information:
204         - The base model of the query.
205         - Which columns for that model are present in the query (by
206           position of the select clause).
207         - related_klass_infos: [f, klass_info] to descent into
208 
209         The annotations is a dictionary of {'attname': column position} values.
210         """
211         select = []
212         klass_info = None
213         annotations = {}
214         select_idx = 0
215         for alias, (sql, params) in self.query.extra_select.items():
216             annotations[alias] = select_idx
217             select.append((RawSQL(sql, params), alias))
218             select_idx += 1
219         assert not (self.query.select and self.query.default_cols)
220         if self.query.default_cols:
221             cols = self.get_default_columns()
222         else:
223             # self.query.select is a special case. These columns never go to
224             # any model.
225             cols = self.query.select
226         if cols:
227             select_list = []
228             for col in cols:
229                 select_list.append(select_idx)
230                 select.append((col, None))
231                 select_idx += 1
232             klass_info = {
233                 'model': self.query.model,
234                 'select_fields': select_list,
235             }
236         for alias, annotation in self.query.annotation_select.items():
237             annotations[alias] = select_idx
238             select.append((annotation, alias))
239             select_idx += 1
240 
241         if self.query.select_related:
242             related_klass_infos = self.get_related_selections(select)
243             klass_info['related_klass_infos'] = related_klass_infos
244 
245             def get_select_from_parent(klass_info):
246                 for ki in klass_info['related_klass_infos']:
247                     if ki['from_parent']:
248                         ki['select_fields'] = (klass_info['select_fields'] +
249                                                ki['select_fields'])
250                     get_select_from_parent(ki)
251             get_select_from_parent(klass_info)
252 
253         ret = []
254         for col, alias in select:
255             try:
256                 sql, params = self.compile(col)
257             except EmptyResultSet:
258                 # Select a predicate that's always False.
259                 sql, params = '0', ()
260             else:
261                 sql, params = col.select_format(self, sql, params)
262             ret.append((col, (sql, params), alias))
263         return ret, klass_info, annotations
264 
265     def get_order_by(self):
266         """
267         Return a list of 2-tuples of form (expr, (sql, params, is_ref)) for the
268         ORDER BY clause.
269 
270         The order_by clause can alter the select clause (for example it
271         can add aliases to clauses that do not yet have one, or it can
272         add totally new select clauses).
273         """
274         if self.query.extra_order_by:
275             ordering = self.query.extra_order_by
276         elif not self.query.default_ordering:
277             ordering = self.query.order_by
278         elif self.query.order_by:
279             ordering = self.query.order_by
280         elif self.query.get_meta().ordering:
281             ordering = self.query.get_meta().ordering
282             self._meta_ordering = ordering
283         else:
284             ordering = []
285         if self.query.standard_ordering:
286             asc, desc = ORDER_DIR['ASC']
287         else:
288             asc, desc = ORDER_DIR['DESC']
289 
290         order_by = []
291         for field in ordering:
292             if hasattr(field, 'resolve_expression'):
293                 if isinstance(field, Value):
294                     # output_field must be resolved for constants.
295                     field = Cast(field, field.output_field)
296                 if not isinstance(field, OrderBy):
297                     field = field.asc()
298                 if not self.query.standard_ordering:
299                     field = field.copy()
300                     field.reverse_ordering()
301                 order_by.append((field, False))
302                 continue
303             if field == '?':  # random
304                 order_by.append((OrderBy(Random()), False))
305                 continue
306 
307             col, order = get_order_dir(field, asc)
308             descending = order == 'DESC'
309 
310             if col in self.query.annotation_select:
311                 # Reference to expression in SELECT clause
312                 order_by.append((
313                     OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending),
314                     True))
315                 continue
316             if col in self.query.annotations:
317                 # References to an expression which is masked out of the SELECT
318                 # clause.
319                 expr = self.query.annotations[col]
320                 if isinstance(expr, Value):
321                     # output_field must be resolved for constants.
322                     expr = Cast(expr, expr.output_field)
323                 order_by.append((OrderBy(expr, descending=descending), False))
324                 continue
325 
326             if '.' in field:
327                 # This came in through an extra(order_by=...) addition. Pass it
328                 # on verbatim.
329                 table, col = col.split('.', 1)
330                 order_by.append((
331                     OrderBy(
332                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),
333                         descending=descending
334                     ), False))
335                 continue
336 
337             if not self.query.extra or col not in self.query.extra:
338                 # 'col' is of the form 'field' or 'field1__field2' or
339                 # '-field1__field2__field', etc.
340                 order_by.extend(self.find_ordering_name(
341                     field, self.query.get_meta(), default_order=asc))
342             else:
343                 if col not in self.query.extra_select:
344                     order_by.append((
345                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),
346                         False))
347                 else:
348                     order_by.append((
349                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),
350                         True))
351         result = []
352         seen = set()
353 
354         for expr, is_ref in order_by:
355             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)
356             if self.query.combinator:
357                 src = resolved.get_source_expressions()[0]
358                 # Relabel order by columns to raw numbers if this is a combined
359                 # query; necessary since the columns can't be referenced by the
360                 # fully qualified name and the simple column names may collide.
361                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):
362                     if is_ref and col_alias == src.refs:
363                         src = src.source
364                     elif col_alias:
365                         continue
366                     if src == sel_expr:
367                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])
368                         break
369                 else:
370                     if col_alias:
371                         raise DatabaseError('ORDER BY term does not match any column in the result set.')
372                     # Add column used in ORDER BY clause without an alias to
373                     # the selected columns.
374                     self.query.add_select_col(src)
375                     resolved.set_source_expressions([RawSQL('%d' % len(self.query.select), ())])
376             sql, params = self.compile(resolved)
377             # Don't add the same column twice, but the order direction is
378             # not taken into account so we strip it. When this entire method
379             # is refactored into expressions, then we can check each part as we
380             # generate it.
381             without_ordering = self.ordering_parts.search(sql).group(1)
382             params_hash = make_hashable(params)
383             if (without_ordering, params_hash) in seen:
384                 continue
385             seen.add((without_ordering, params_hash))
386             result.append((resolved, (sql, params, is_ref)))
387         return result
388 
389     def get_extra_select(self, order_by, select):
390         extra_select = []
391         if self.query.distinct and not self.query.distinct_fields:
392             select_sql = [t[1] for t in select]
393             for expr, (sql, params, is_ref) in order_by:
394                 without_ordering = self.ordering_parts.search(sql).group(1)
395                 if not is_ref and (without_ordering, params) not in select_sql:
396                     extra_select.append((expr, (without_ordering, params), None))
397         return extra_select
398 
399     def quote_name_unless_alias(self, name):
400         """
401         A wrapper around connection.ops.quote_name that doesn't quote aliases
402         for table names. This avoids problems with some SQL dialects that treat
403         quoted strings specially (e.g. PostgreSQL).
404         """
405         if name in self.quote_cache:
406             return self.quote_cache[name]
407         if ((name in self.query.alias_map and name not in self.query.table_map) or
408                 name in self.query.extra_select or (
409                     self.query.external_aliases.get(name) and name not in self.query.table_map)):
410             self.quote_cache[name] = name
411             return name
412         r = self.connection.ops.quote_name(name)
413         self.quote_cache[name] = r
414         return r
415 
416     def compile(self, node):
417         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)
418         if vendor_impl:
419             sql, params = vendor_impl(self, self.connection)
420         else:
421             sql, params = node.as_sql(self, self.connection)
422         return sql, params
423 
424     def get_combinator_sql(self, combinator, all):
425         features = self.connection.features
426         compilers = [
427             query.get_compiler(self.using, self.connection)
428             for query in self.query.combined_queries if not query.is_empty()
429         ]
430         if not features.supports_slicing_ordering_in_compound:
431             for query, compiler in zip(self.query.combined_queries, compilers):
432                 if query.low_mark or query.high_mark:
433                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')
434                 if compiler.get_order_by():
435                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')
436         parts = ()
437         for compiler in compilers:
438             try:
439                 # If the columns list is limited, then all combined queries
440                 # must have the same columns list. Set the selects defined on
441                 # the query on all combined queries, if not already set.
442                 if not compiler.query.values_select and self.query.values_select:
443                     compiler.query = compiler.query.clone()
444                     compiler.query.set_values((
445                         *self.query.extra_select,
446                         *self.query.values_select,
447                         *self.query.annotation_select,
448                     ))
449                 part_sql, part_args = compiler.as_sql()
450                 if compiler.query.combinator:
451                     # Wrap in a subquery if wrapping in parentheses isn't
452                     # supported.
453                     if not features.supports_parentheses_in_compound:
454                         part_sql = 'SELECT * FROM ({})'.format(part_sql)
455                     # Add parentheses when combining with compound query if not
456                     # already added for all compound queries.
457                     elif not features.supports_slicing_ordering_in_compound:
458                         part_sql = '({})'.format(part_sql)
459                 parts += ((part_sql, part_args),)
460             except EmptyResultSet:
461                 # Omit the empty queryset with UNION and with DIFFERENCE if the
462                 # first queryset is nonempty.
463                 if combinator == 'union' or (combinator == 'difference' and parts):
464                     continue
465                 raise
466         if not parts:
467             raise EmptyResultSet
468         combinator_sql = self.connection.ops.set_operators[combinator]
469         if all and combinator == 'union':
470             combinator_sql += ' ALL'
471         braces = '({})' if features.supports_slicing_ordering_in_compound else '{}'
472         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))
473         result = [' {} '.format(combinator_sql).join(sql_parts)]
474         params = []
475         for part in args_parts:
476             params.extend(part)
477         return result, params
478 
479     def as_sql(self, with_limits=True, with_col_aliases=False):
480         """
481         Create the SQL for this query. Return the SQL string and list of
482         parameters.
483 
484         If 'with_limits' is False, any limit/offset information is not included
485         in the query.
486         """
487         refcounts_before = self.query.alias_refcount.copy()
488         try:
489             extra_select, order_by, group_by = self.pre_sql_setup()
490             for_update_part = None
491             # Is a LIMIT/OFFSET clause needed?
492             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)
493             combinator = self.query.combinator
494             features = self.connection.features
495             if combinator:
496                 if not getattr(features, 'supports_select_{}'.format(combinator)):
497                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))
498                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)
499             else:
500                 distinct_fields, distinct_params = self.get_distinct()
501                 # This must come after 'select', 'ordering', and 'distinct'
502                 # (see docstring of get_from_clause() for details).
503                 from_, f_params = self.get_from_clause()
504                 where, w_params = self.compile(self.where) if self.where is not None else ("", [])
505                 having, h_params = self.compile(self.having) if self.having is not None else ("", [])
506                 result = ['SELECT']
507                 params = []
508 
509                 if self.query.distinct:
510                     distinct_result, distinct_params = self.connection.ops.distinct_sql(
511                         distinct_fields,
512                         distinct_params,
513                     )
514                     result += distinct_result
515                     params += distinct_params
516 
517                 out_cols = []
518                 col_idx = 1
519                 for _, (s_sql, s_params), alias in self.select + extra_select:
520                     if alias:
521                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))
522                     elif with_col_aliases:
523                         s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)
524                         col_idx += 1
525                     params.extend(s_params)
526                     out_cols.append(s_sql)
527 
528                 result += [', '.join(out_cols), 'FROM', *from_]
529                 params.extend(f_params)
530 
531                 if self.query.select_for_update and self.connection.features.has_select_for_update:
532                     if self.connection.get_autocommit():
533                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')
534 
535                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:
536                         raise NotSupportedError(
537                             'LIMIT/OFFSET is not supported with '
538                             'select_for_update on this database backend.'
539                         )
540                     nowait = self.query.select_for_update_nowait
541                     skip_locked = self.query.select_for_update_skip_locked
542                     of = self.query.select_for_update_of
543                     # If it's a NOWAIT/SKIP LOCKED/OF query but the backend
544                     # doesn't support it, raise NotSupportedError to prevent a
545                     # possible deadlock.
546                     if nowait and not self.connection.features.has_select_for_update_nowait:
547                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
548                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
549                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
550                     elif of and not self.connection.features.has_select_for_update_of:
551                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
552                     for_update_part = self.connection.ops.for_update_sql(
553                         nowait=nowait,
554                         skip_locked=skip_locked,
555                         of=self.get_select_for_update_of_arguments(),
556                     )
557 
558                 if for_update_part and self.connection.features.for_update_after_from:
559                     result.append(for_update_part)
560 
561                 if where:
562                     result.append('WHERE %s' % where)
563                     params.extend(w_params)
564 
565                 grouping = []
566                 for g_sql, g_params in group_by:
567                     grouping.append(g_sql)
568                     params.extend(g_params)
569                 if grouping:
570                     if distinct_fields:
571                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')
572                     order_by = order_by or self.connection.ops.force_no_ordering()
573                     result.append('GROUP BY %s' % ', '.join(grouping))
574                     if self._meta_ordering:
575                         order_by = None
576                 if having:
577                     result.append('HAVING %s' % having)
578                     params.extend(h_params)
579 
580             if self.query.explain_query:
581                 result.insert(0, self.connection.ops.explain_query_prefix(
582                     self.query.explain_format,
583                     **self.query.explain_options
584                 ))
585 
586             if order_by:
587                 ordering = []
588                 for _, (o_sql, o_params, _) in order_by:
589                     ordering.append(o_sql)
590                     params.extend(o_params)
591                 result.append('ORDER BY %s' % ', '.join(ordering))
592 
593             if with_limit_offset:
594                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))
595 
596             if for_update_part and not self.connection.features.for_update_after_from:
597                 result.append(for_update_part)
598 
599             if self.query.subquery and extra_select:
600                 # If the query is used as a subquery, the extra selects would
601                 # result in more columns than the left-hand side expression is
602                 # expecting. This can happen when a subquery uses a combination
603                 # of order_by() and distinct(), forcing the ordering expressions
604                 # to be selected as well. Wrap the query in another subquery
605                 # to exclude extraneous selects.
606                 sub_selects = []
607                 sub_params = []
608                 for index, (select, _, alias) in enumerate(self.select, start=1):
609                     if not alias and with_col_aliases:
610                         alias = 'col%d' % index
611                     if alias:
612                         sub_selects.append("%s.%s" % (
613                             self.connection.ops.quote_name('subquery'),
614                             self.connection.ops.quote_name(alias),
615                         ))
616                     else:
617                         select_clone = select.relabeled_clone({select.alias: 'subquery'})
618                         subselect, subparams = select_clone.as_sql(self, self.connection)
619                         sub_selects.append(subselect)
620                         sub_params.extend(subparams)
621                 return 'SELECT %s FROM (%s) subquery' % (
622                     ', '.join(sub_selects),
623                     ' '.join(result),
624                 ), tuple(sub_params + params)
625 
626             return ' '.join(result), tuple(params)
627         finally:
628             # Finally do cleanup - get rid of the joins we created above.
629             self.query.reset_refcounts(refcounts_before)
630 
631     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):
632         """
633         Compute the default columns for selecting every field in the base
634         model. Will sometimes be called to pull in related models (e.g. via
635         select_related), in which case "opts" and "start_alias" will be given
636         to provide a starting point for the traversal.
637 
638         Return a list of strings, quoted appropriately for use in SQL
639         directly, as well as a set of aliases used in the select statement (if
640         'as_pairs' is True, return a list of (alias, col_name) pairs instead
641         of strings as the first component and None as the second component).
642         """
643         result = []
644         if opts is None:
645             opts = self.query.get_meta()
646         only_load = self.deferred_to_columns()
647         start_alias = start_alias or self.query.get_initial_alias()
648         # The 'seen_models' is used to optimize checking the needed parent
649         # alias for a given field. This also includes None -> start_alias to
650         # be used by local fields.
651         seen_models = {None: start_alias}
652 
653         for field in opts.concrete_fields:
654             model = field.model._meta.concrete_model
655             # A proxy model will have a different model and concrete_model. We
656             # will assign None if the field belongs to this model.
657             if model == opts.model:
658                 model = None
659             if from_parent and model is not None and issubclass(
660                     from_parent._meta.concrete_model, model._meta.concrete_model):
661                 # Avoid loading data for already loaded parents.
662                 # We end up here in the case select_related() resolution
663                 # proceeds from parent model to child model. In that case the
664                 # parent model data is already present in the SELECT clause,
665                 # and we want to avoid reloading the same data again.
666                 continue
667             if field.model in only_load and field.attname not in only_load[field.model]:
668                 continue
669             alias = self.query.join_parent_model(opts, model, start_alias,
670                                                  seen_models)
671             column = field.get_col(alias)
672             result.append(column)
673         return result
674 
675     def get_distinct(self):
676         """
677         Return a quoted list of fields to use in DISTINCT ON part of the query.
678 
679         This method can alter the tables in the query, and thus it must be
680         called before get_from_clause().
681         """
682         result = []
683         params = []
684         opts = self.query.get_meta()
685 
686         for name in self.query.distinct_fields:
687             parts = name.split(LOOKUP_SEP)
688             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)
689             targets, alias, _ = self.query.trim_joins(targets, joins, path)
690             for target in targets:
691                 if name in self.query.annotation_select:
692                     result.append(name)
693                 else:
694                     r, p = self.compile(transform_function(target, alias))
695                     result.append(r)
696                     params.append(p)
697         return result, params
698 
699     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',
700                            already_seen=None):
701         """
702         Return the table alias (the name might be ambiguous, the alias will
703         not be) and column name for ordering by the given 'name' parameter.
704         The 'name' is of the form 'field1__field2__...__fieldN'.
705         """
706         name, order = get_order_dir(name, default_order)
707         descending = order == 'DESC'
708         pieces = name.split(LOOKUP_SEP)
709         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)
710 
711         # If we get to this point and the field is a relation to another model,
712         # append the default ordering for that model unless the attribute name
713         # of the field is specified.
714         if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:
715             # Firstly, avoid infinite loops.
716             already_seen = already_seen or set()
717             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)
718             if join_tuple in already_seen:
719                 raise FieldError('Infinite loop caused by ordering.')
720             already_seen.add(join_tuple)
721 
722             results = []
723             for item in opts.ordering:
724                 if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):
725                     item = item.desc() if descending else item.asc()
726                 if isinstance(item, OrderBy):
727                     results.append((item, False))
728                     continue
729                 results.extend(self.find_ordering_name(item, opts, alias,
730                                                        order, already_seen))
731             return results
732         targets, alias, _ = self.query.trim_joins(targets, joins, path)
733         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]
734 
735     def _setup_joins(self, pieces, opts, alias):
736         """
737         Helper method for get_order_by() and get_distinct().
738 
739         get_ordering() and get_distinct() must produce same target columns on
740         same input, as the prefixes of get_ordering() and get_distinct() must
741         match. Executing SQL where this is not true is an error.
742         """
743         alias = alias or self.query.get_initial_alias()
744         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)
745         alias = joins[-1]
746         return field, targets, alias, joins, path, opts, transform_function
747 
748     def get_from_clause(self):
749         """
750         Return a list of strings that are joined together to go after the
751         "FROM" part of the query, as well as a list any extra parameters that
752         need to be included. Subclasses, can override this to create a
753         from-clause via a "select".
754 
755         This should only be called after any SQL construction methods that
756         might change the tables that are needed. This means the select columns,
757         ordering, and distinct must be done first.
758         """
759         result = []
760         params = []
761         for alias in tuple(self.query.alias_map):
762             if not self.query.alias_refcount[alias]:
763                 continue
764             try:
765                 from_clause = self.query.alias_map[alias]
766             except KeyError:
767                 # Extra tables can end up in self.tables, but not in the
768                 # alias_map if they aren't in a join. That's OK. We skip them.
769                 continue
770             clause_sql, clause_params = self.compile(from_clause)
771             result.append(clause_sql)
772             params.extend(clause_params)
773         for t in self.query.extra_tables:
774             alias, _ = self.query.table_alias(t)
775             # Only add the alias if it's not already present (the table_alias()
776             # call increments the refcount, so an alias refcount of one means
777             # this is the only reference).
778             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:
779                 result.append(', %s' % self.quote_name_unless_alias(alias))
780         return result, params
781 
782     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,
783                                requested=None, restricted=None):
784         """
785         Fill in the information needed for a select_related query. The current
786         depth is measured as the number of connections away from the root model
787         (for example, cur_depth=1 means we are looking at models with direct
788         connections to the root model).
789         """
790         def _get_field_choices():
791             direct_choices = (f.name for f in opts.fields if f.is_relation)
792             reverse_choices = (
793                 f.field.related_query_name()
794                 for f in opts.related_objects if f.field.unique
795             )
796             return chain(direct_choices, reverse_choices, self.query._filtered_relations)
797 
798         related_klass_infos = []
799         if not restricted and cur_depth > self.query.max_depth:
800             # We've recursed far enough; bail out.
801             return related_klass_infos
802 
803         if not opts:
804             opts = self.query.get_meta()
805             root_alias = self.query.get_initial_alias()
806         only_load = self.query.get_loaded_field_names()
807 
808         # Setup for the case when only particular related fields should be
809         # included in the related selection.
810         fields_found = set()
811         if requested is None:
812             restricted = isinstance(self.query.select_related, dict)
813             if restricted:
814                 requested = self.query.select_related
815 
816         def get_related_klass_infos(klass_info, related_klass_infos):
817             klass_info['related_klass_infos'] = related_klass_infos
818 
819         for f in opts.fields:
820             field_model = f.model._meta.concrete_model
821             fields_found.add(f.name)
822 
823             if restricted:
824                 next = requested.get(f.name, {})
825                 if not f.is_relation:
826                     # If a non-related field is used like a relation,
827                     # or if a single non-relational field is given.
828                     if next or f.name in requested:
829                         raise FieldError(
830                             "Non-relational field given in select_related: '%s'. "
831                             "Choices are: %s" % (
832                                 f.name,
833                                 ", ".join(_get_field_choices()) or '(none)',
834                             )
835                         )
836             else:
837                 next = False
838 
839             if not select_related_descend(f, restricted, requested,
840                                           only_load.get(field_model)):
841                 continue
842             klass_info = {
843                 'model': f.remote_field.model,
844                 'field': f,
845                 'reverse': False,
846                 'local_setter': f.set_cached_value,
847                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,
848                 'from_parent': False,
849             }
850             related_klass_infos.append(klass_info)
851             select_fields = []
852             _, _, _, joins, _, _ = self.query.setup_joins(
853                 [f.name], opts, root_alias)
854             alias = joins[-1]
855             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)
856             for col in columns:
857                 select_fields.append(len(select))
858                 select.append((col, None))
859             klass_info['select_fields'] = select_fields
860             next_klass_infos = self.get_related_selections(
861                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)
862             get_related_klass_infos(klass_info, next_klass_infos)
863 
864         if restricted:
865             related_fields = [
866                 (o.field, o.related_model)
867                 for o in opts.related_objects
868                 if o.field.unique and not o.many_to_many
869             ]
870             for f, model in related_fields:
871                 if not select_related_descend(f, restricted, requested,
872                                               only_load.get(model), reverse=True):
873                     continue
874 
875                 related_field_name = f.related_query_name()
876                 fields_found.add(related_field_name)
877 
878                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)
879                 alias = join_info.joins[-1]
880                 from_parent = issubclass(model, opts.model) and model is not opts.model
881                 klass_info = {
882                     'model': model,
883                     'field': f,
884                     'reverse': True,
885                     'local_setter': f.remote_field.set_cached_value,
886                     'remote_setter': f.set_cached_value,
887                     'from_parent': from_parent,
888                 }
889                 related_klass_infos.append(klass_info)
890                 select_fields = []
891                 columns = self.get_default_columns(
892                     start_alias=alias, opts=model._meta, from_parent=opts.model)
893                 for col in columns:
894                     select_fields.append(len(select))
895                     select.append((col, None))
896                 klass_info['select_fields'] = select_fields
897                 next = requested.get(f.related_query_name(), {})
898                 next_klass_infos = self.get_related_selections(
899                     select, model._meta, alias, cur_depth + 1,
900                     next, restricted)
901                 get_related_klass_infos(klass_info, next_klass_infos)
902 
903             def local_setter(obj, from_obj):
904                 # Set a reverse fk object when relation is non-empty.
905                 if from_obj:
906                     f.remote_field.set_cached_value(from_obj, obj)
907 
908             def remote_setter(name, obj, from_obj):
909                 setattr(from_obj, name, obj)
910 
911             for name in list(requested):
912                 # Filtered relations work only on the topmost level.
913                 if cur_depth > 1:
914                     break
915                 if name in self.query._filtered_relations:
916                     fields_found.add(name)
917                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)
918                     model = join_opts.model
919                     alias = joins[-1]
920                     from_parent = issubclass(model, opts.model) and model is not opts.model
921                     klass_info = {
922                         'model': model,
923                         'field': f,
924                         'reverse': True,
925                         'local_setter': local_setter,
926                         'remote_setter': partial(remote_setter, name),
927                         'from_parent': from_parent,
928                     }
929                     related_klass_infos.append(klass_info)
930                     select_fields = []
931                     columns = self.get_default_columns(
932                         start_alias=alias, opts=model._meta,
933                         from_parent=opts.model,
934                     )
935                     for col in columns:
936                         select_fields.append(len(select))
937                         select.append((col, None))
938                     klass_info['select_fields'] = select_fields
939                     next_requested = requested.get(name, {})
940                     next_klass_infos = self.get_related_selections(
941                         select, opts=model._meta, root_alias=alias,
942                         cur_depth=cur_depth + 1, requested=next_requested,
943                         restricted=restricted,
944                     )
945                     get_related_klass_infos(klass_info, next_klass_infos)
946             fields_not_found = set(requested).difference(fields_found)
947             if fields_not_found:
948                 invalid_fields = ("'%s'" % s for s in fields_not_found)
949                 raise FieldError(
950                     'Invalid field name(s) given in select_related: %s. '
951                     'Choices are: %s' % (
952                         ', '.join(invalid_fields),
953                         ', '.join(_get_field_choices()) or '(none)',
954                     )
955                 )
956         return related_klass_infos
957 
958     def get_select_for_update_of_arguments(self):
959         """
960         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of
961         the query.
962         """
963         def _get_parent_klass_info(klass_info):
964             for parent_model, parent_link in klass_info['model']._meta.parents.items():
965                 parent_list = parent_model._meta.get_parent_list()
966                 yield {
967                     'model': parent_model,
968                     'field': parent_link,
969                     'reverse': False,
970                     'select_fields': [
971                         select_index
972                         for select_index in klass_info['select_fields']
973                         # Selected columns from a model or its parents.
974                         if (
975                             self.select[select_index][0].target.model == parent_model or
976                             self.select[select_index][0].target.model in parent_list
977                         )
978                     ],
979                 }
980 
981         def _get_first_selected_col_from_model(klass_info):
982             """
983             Find the first selected column from a model. If it doesn't exist,
984             don't lock a model.
985 
986             select_fields is filled recursively, so it also contains fields
987             from the parent models.
988             """
989             for select_index in klass_info['select_fields']:
990                 if self.select[select_index][0].target.model == klass_info['model']:
991                     return self.select[select_index][0]
992 
993         def _get_field_choices():
994             """Yield all allowed field paths in breadth-first search order."""
995             queue = collections.deque([(None, self.klass_info)])
996             while queue:
997                 parent_path, klass_info = queue.popleft()
998                 if parent_path is None:
999                     path = []
1000                     yield 'self'
1001                 else:
1002                     field = klass_info['field']
1003                     if klass_info['reverse']:
1004                         field = field.remote_field
1005                     path = parent_path + [field.name]
1006                     yield LOOKUP_SEP.join(path)
1007                 queue.extend(
1008                     (path, klass_info)
1009                     for klass_info in _get_parent_klass_info(klass_info)
1010                 )
1011                 queue.extend(
1012                     (path, klass_info)
1013                     for klass_info in klass_info.get('related_klass_infos', [])
1014                 )
1015         result = []
1016         invalid_names = []
1017         for name in self.query.select_for_update_of:
1018             klass_info = self.klass_info
1019             if name == 'self':
1020                 col = _get_first_selected_col_from_model(klass_info)
1021             else:
1022                 for part in name.split(LOOKUP_SEP):
1023                     klass_infos = (
1024                         *klass_info.get('related_klass_infos', []),
1025                         *_get_parent_klass_info(klass_info),
1026                     )
1027                     for related_klass_info in klass_infos:
1028                         field = related_klass_info['field']
1029                         if related_klass_info['reverse']:
1030                             field = field.remote_field
1031                         if field.name == part:
1032                             klass_info = related_klass_info
1033                             break
1034                     else:
1035                         klass_info = None
1036                         break
1037                 if klass_info is None:
1038                     invalid_names.append(name)
1039                     continue
1040                 col = _get_first_selected_col_from_model(klass_info)
1041             if col is not None:
1042                 if self.connection.features.select_for_update_of_column:
1043                     result.append(self.compile(col)[0])
1044                 else:
1045                     result.append(self.quote_name_unless_alias(col.alias))
1046         if invalid_names:
1047             raise FieldError(
1048                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '
1049                 'Only relational fields followed in the query are allowed. '
1050                 'Choices are: %s.' % (
1051                     ', '.join(invalid_names),
1052                     ', '.join(_get_field_choices()),
1053                 )
1054             )
1055         return result
1056 
1057     def deferred_to_columns(self):
1058         """
1059         Convert the self.deferred_loading data structure to mapping of table
1060         names to sets of column names which are to be loaded. Return the
1061         dictionary.
1062         """
1063         columns = {}
1064         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)
1065         return columns
1066 
1067     def get_converters(self, expressions):
1068         converters = {}
1069         for i, expression in enumerate(expressions):
1070             if expression:
1071                 backend_converters = self.connection.ops.get_db_converters(expression)
1072                 field_converters = expression.get_db_converters(self.connection)
1073                 if backend_converters or field_converters:
1074                     converters[i] = (backend_converters + field_converters, expression)
1075         return converters
1076 
1077     def apply_converters(self, rows, converters):
1078         connection = self.connection
1079         converters = list(converters.items())
1080         for row in map(list, rows):
1081             for pos, (convs, expression) in converters:
1082                 value = row[pos]
1083                 for converter in convs:
1084                     value = converter(value, expression, connection)
1085                 row[pos] = value
1086             yield row
1087 
1088     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,
1089                      chunk_size=GET_ITERATOR_CHUNK_SIZE):
1090         """Return an iterator over the results from executing this query."""
1091         if results is None:
1092             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)
1093         fields = [s[0] for s in self.select[0:self.col_count]]
1094         converters = self.get_converters(fields)
1095         rows = chain.from_iterable(results)
1096         if converters:
1097             rows = self.apply_converters(rows, converters)
1098             if tuple_expected:
1099                 rows = map(tuple, rows)
1100         return rows
1101 
1102     def has_results(self):
1103         """
1104         Backends (e.g. NoSQL) can override this in order to use optimized
1105         versions of "query has any results."
1106         """
1107         # This is always executed on a query clone, so we can modify self.query
1108         self.query.add_extra({'a': 1}, None, None, None, None, None)
1109         self.query.set_extra_mask(['a'])
1110         return bool(self.execute_sql(SINGLE))
1111 
1112     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
1113         """
1114         Run the query against the database and return the result(s). The
1115         return value is a single data item if result_type is SINGLE, or an
1116         iterator over the results if the result_type is MULTI.
1117 
1118         result_type is either MULTI (use fetchmany() to retrieve all rows),
1119         SINGLE (only retrieve a single row), or None. In this last case, the
1120         cursor is returned if any query is executed, since it's used by
1121         subclasses such as InsertQuery). It's possible, however, that no query
1122         is needed, as the filters describe an empty set. In that case, None is
1123         returned, to avoid any unnecessary database interaction.
1124         """
1125         result_type = result_type or NO_RESULTS
1126         try:
1127             sql, params = self.as_sql()
1128             if not sql:
1129                 raise EmptyResultSet
1130         except EmptyResultSet:
1131             if result_type == MULTI:
1132                 return iter([])
1133             else:
1134                 return
1135         if chunked_fetch:
1136             cursor = self.connection.chunked_cursor()
1137         else:
1138             cursor = self.connection.cursor()
1139         try:
1140             cursor.execute(sql, params)
1141         except Exception:
1142             # Might fail for server-side cursors (e.g. connection closed)
1143             cursor.close()
1144             raise
1145 
1146         if result_type == CURSOR:
1147             # Give the caller the cursor to process and close.
1148             return cursor
1149         if result_type == SINGLE:
1150             try:
1151                 val = cursor.fetchone()
1152                 if val:
1153                     return val[0:self.col_count]
1154                 return val
1155             finally:
1156                 # done with the cursor
1157                 cursor.close()
1158         if result_type == NO_RESULTS:
1159             cursor.close()
1160             return
1161 
1162         result = cursor_iter(
1163             cursor, self.connection.features.empty_fetchmany_value,
1164             self.col_count if self.has_extra_select else None,
1165             chunk_size,
1166         )
1167         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:
1168             try:
1169                 # If we are using non-chunked reads, we return the same data
1170                 # structure as normally, but ensure it is all read into memory
1171                 # before going any further. Use chunked_fetch if requested,
1172                 # unless the database doesn't support it.
1173                 return list(result)
1174             finally:
1175                 # done with the cursor
1176                 cursor.close()
1177         return result
1178 
1179     def as_subquery_condition(self, alias, columns, compiler):
1180         qn = compiler.quote_name_unless_alias
1181         qn2 = self.connection.ops.quote_name
1182 
1183         for index, select_col in enumerate(self.query.select):
1184             lhs_sql, lhs_params = self.compile(select_col)
1185             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))
1186             self.query.where.add(
1187                 RawSQL('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')
1188 
1189         sql, params = self.as_sql()
1190         return 'EXISTS (%s)' % sql, params
1191 
1192     def explain_query(self):
1193         result = list(self.execute_sql())
1194         # Some backends return 1 item tuples with strings, and others return
1195         # tuples with integers and strings. Flatten them out into strings.
1196         for row in result[0]:
1197             if not isinstance(row, str):
1198                 yield ' '.join(str(c) for c in row)
1199             else:
1200                 yield row

</code><intended_behavior>The SQLCompiler class may need a review or enhancement in the methods responsible for constructing the GROUP BY clause, such as `get_group_by`, ensuring that all conditions leading to the correct inclusion of annotations, particularly subquery annotations, are met. The intended behavior is to enhance the logic so that it accurately recognizes when annotations (especially those involving subqueries) necessitate inclusion in the GROUP BY clause to avoid result set duplication and maintain consistency with previous Django versions while still allowing for optimizations introduced in newer versions. This might include refining how the compiler assesses and decides the fields and expressions to include in the GROUP BY clause based on the annotations present in the queryset. The adjustments described aim to correct the regression, ensuring that the ORM's handling of subquery annotations in conjunction with GROUP BY clauses is both accurate and consistent with expected behavior, thus preserving the robustness and reliability of the Django ORM across versions.</intended_behavior>]
2025-10-15 04:34:41.007 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 04:34:41.007 | INFO     | Search-only: skipping patch selection
2025-10-15 04:34:41.182 | INFO     | Task django__django-12519 completed successfully.
2025-10-15 04:34:41.183 | INFO     | No patch generated. You can try running ACR again.
