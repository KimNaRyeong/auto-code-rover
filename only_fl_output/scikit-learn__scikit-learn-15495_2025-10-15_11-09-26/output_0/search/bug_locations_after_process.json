[
    {
        "rel_file_path": "sklearn/cluster/_dbscan.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/cluster/_dbscan.py",
        "start": 284,
        "end": 361,
        "class_name": "DBSCAN",
        "method_name": "fit",
        "code": "284     def fit(self, X, y=None, sample_weight=None):\n285         \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n286 \n287         Parameters\n288         ----------\n289         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n290             (n_samples, n_samples)\n291             Training instances to cluster, or distances between instances if\n292             ``metric='precomputed'``. If a sparse matrix is provided, it will\n293             be converted into a sparse ``csr_matrix``.\n294 \n295         sample_weight : array, shape (n_samples,), optional\n296             Weight of each sample, such that a sample with a weight of at least\n297             ``min_samples`` is by itself a core sample; a sample with a\n298             negative weight may inhibit its eps-neighbor from being core.\n299             Note that weights are absolute, and default to 1.\n300 \n301         y : Ignored\n302             Not used, present here for API consistency by convention.\n303 \n304         Returns\n305         -------\n306         self\n307 \n308         \"\"\"\n309         X = check_array(X, accept_sparse='csr')\n310 \n311         if not self.eps > 0.0:\n312             raise ValueError(\"eps must be positive.\")\n313 \n314         if sample_weight is not None:\n315             sample_weight = np.asarray(sample_weight)\n316             check_consistent_length(X, sample_weight)\n317 \n318         # Calculate neighborhood for all samples. This leaves the original\n319         # point in, which needs to be considered later (i.e. point i is in the\n320         # neighborhood of point i. While True, its useless information)\n321         if self.metric == 'precomputed' and sparse.issparse(X):\n322             # set the diagonal to explicit values, as a point is its own\n323             # neighbor\n324             with warnings.catch_warnings():\n325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n327 \n328         neighbors_model = NearestNeighbors(\n329             radius=self.eps, algorithm=self.algorithm,\n330             leaf_size=self.leaf_size, metric=self.metric,\n331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n332         neighbors_model.fit(X)\n333         # This has worst case O(n^2) memory complexity\n334         neighborhoods = neighbors_model.radius_neighbors(X,\n335                                                          return_distance=False)\n336 \n337         if sample_weight is None:\n338             n_neighbors = np.array([len(neighbors)\n339                                     for neighbors in neighborhoods])\n340         else:\n341             n_neighbors = np.array([np.sum(sample_weight[neighbors])\n342                                     for neighbors in neighborhoods])\n343 \n344         # Initially, all samples are noise.\n345         labels = np.full(X.shape[0], -1, dtype=np.intp)\n346 \n347         # A list of all core samples found.\n348         core_samples = np.asarray(n_neighbors >= self.min_samples,\n349                                   dtype=np.uint8)\n350         dbscan_inner(core_samples, neighborhoods, labels)\n351 \n352         self.core_sample_indices_ = np.where(core_samples)[0]\n353         self.labels_ = labels\n354 \n355         if len(self.core_sample_indices_):\n356             # fix for scipy sparse indexing issue\n357             self.components_ = X[self.core_sample_indices_].copy()\n358         else:\n359             # no core samples\n360             self.components_ = np.empty((0, X.shape[1]))\n361         return self\n",
        "intended_behavior": "Replace the manual array transformation and check_consistent_length validation of sample_weight with a call to _check_sample_weight(sample_weight). This will validate and prepare sample_weight for use while preserving existing functionality. The call to _check_sample_weight would ensure that sample_weight meets all necessary criteria for further processes within the method, thus maintaining the method's integrity and adhering to the library's standards for handling sample_weight."
    },
    {
        "rel_file_path": "sklearn/cluster/_dbscan.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/cluster/_dbscan.py",
        "start": 147,
        "end": 390,
        "class_name": "DBSCAN",
        "method_name": null,
        "code": "147 class DBSCAN(ClusterMixin, BaseEstimator):\n148     \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n149 \n150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n151     Finds core samples of high density and expands clusters from them.\n152     Good for data which contains clusters of similar density.\n153 \n154     Read more in the :ref:`User Guide <dbscan>`.\n155 \n156     Parameters\n157     ----------\n158     eps : float, optional\n159         The maximum distance between two samples for one to be considered\n160         as in the neighborhood of the other. This is not a maximum bound\n161         on the distances of points within a cluster. This is the most\n162         important DBSCAN parameter to choose appropriately for your data set\n163         and distance function.\n164 \n165     min_samples : int, optional\n166         The number of samples (or total weight) in a neighborhood for a point\n167         to be considered as a core point. This includes the point itself.\n168 \n169     metric : string, or callable\n170         The metric to use when calculating distance between instances in a\n171         feature array. If metric is a string or callable, it must be one of\n172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n173         its metric parameter.\n174         If metric is \"precomputed\", X is assumed to be a distance matrix and\n175         must be square. X may be a :term:`Glossary <sparse graph>`, in which\n176         case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n177 \n178         .. versionadded:: 0.17\n179            metric *precomputed* to accept precomputed sparse matrix.\n180 \n181     metric_params : dict, optional\n182         Additional keyword arguments for the metric function.\n183 \n184         .. versionadded:: 0.19\n185 \n186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n187         The algorithm to be used by the NearestNeighbors module\n188         to compute pointwise distances and find nearest neighbors.\n189         See NearestNeighbors module documentation for details.\n190 \n191     leaf_size : int, optional (default = 30)\n192         Leaf size passed to BallTree or cKDTree. This can affect the speed\n193         of the construction and query, as well as the memory required\n194         to store the tree. The optimal value depends\n195         on the nature of the problem.\n196 \n197     p : float, optional\n198         The power of the Minkowski metric to be used to calculate distance\n199         between points.\n200 \n201     n_jobs : int or None, optional (default=None)\n202         The number of parallel jobs to run.\n203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n205         for more details.\n206 \n207     Attributes\n208     ----------\n209     core_sample_indices_ : array, shape = [n_core_samples]\n210         Indices of core samples.\n211 \n212     components_ : array, shape = [n_core_samples, n_features]\n213         Copy of each core sample found by training.\n214 \n215     labels_ : array, shape = [n_samples]\n216         Cluster labels for each point in the dataset given to fit().\n217         Noisy samples are given the label -1.\n218 \n219     Examples\n220     --------\n221     >>> from sklearn.cluster import DBSCAN\n222     >>> import numpy as np\n223     >>> X = np.array([[1, 2], [2, 2], [2, 3],\n224     ...               [8, 7], [8, 8], [25, 80]])\n225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n226     >>> clustering.labels_\n227     array([ 0,  0,  0,  1,  1, -1])\n228     >>> clustering\n229     DBSCAN(eps=3, min_samples=2)\n230 \n231     See also\n232     --------\n233     OPTICS\n234         A similar clustering at multiple values of eps. Our implementation\n235         is optimized for memory usage.\n236 \n237     Notes\n238     -----\n239     For an example, see :ref:`examples/cluster/plot_dbscan.py\n240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n241 \n242     This implementation bulk-computes all neighborhood queries, which increases\n243     the memory complexity to O(n.d) where d is the average number of neighbors,\n244     while original DBSCAN had memory complexity O(n). It may attract a higher\n245     memory complexity when querying these nearest neighborhoods, depending\n246     on the ``algorithm``.\n247 \n248     One way to avoid the query complexity is to pre-compute sparse\n249     neighborhoods in chunks using\n250     :func:`NearestNeighbors.radius_neighbors_graph\n251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n252     ``mode='distance'``, then using ``metric='precomputed'`` here.\n253 \n254     Another way to reduce memory and computation time is to remove\n255     (near-)duplicate points and use ``sample_weight`` instead.\n256 \n257     :class:`cluster.OPTICS` provides a similar clustering with lower memory\n258     usage.\n259 \n260     References\n261     ----------\n262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, \"A Density-Based\n263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise\".\n264     In: Proceedings of the 2nd International Conference on Knowledge Discovery\n265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n266 \n267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\n269     ACM Transactions on Database Systems (TODS), 42(3), 19.\n270     \"\"\"\n271 \n272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',\n273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,\n274                  n_jobs=None):\n275         self.eps = eps\n276         self.min_samples = min_samples\n277         self.metric = metric\n278         self.metric_params = metric_params\n279         self.algorithm = algorithm\n280         self.leaf_size = leaf_size\n281         self.p = p\n282         self.n_jobs = n_jobs\n283 \n284     def fit(self, X, y=None, sample_weight=None):\n285         \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n286 \n287         Parameters\n288         ----------\n289         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n290             (n_samples, n_samples)\n291             Training instances to cluster, or distances between instances if\n292             ``metric='precomputed'``. If a sparse matrix is provided, it will\n293             be converted into a sparse ``csr_matrix``.\n294 \n295         sample_weight : array, shape (n_samples,), optional\n296             Weight of each sample, such that a sample with a weight of at least\n297             ``min_samples`` is by itself a core sample; a sample with a\n298             negative weight may inhibit its eps-neighbor from being core.\n299             Note that weights are absolute, and default to 1.\n300 \n301         y : Ignored\n302             Not used, present here for API consistency by convention.\n303 \n304         Returns\n305         -------\n306         self\n307 \n308         \"\"\"\n309         X = check_array(X, accept_sparse='csr')\n310 \n311         if not self.eps > 0.0:\n312             raise ValueError(\"eps must be positive.\")\n313 \n314         if sample_weight is not None:\n315             sample_weight = np.asarray(sample_weight)\n316             check_consistent_length(X, sample_weight)\n317 \n318         # Calculate neighborhood for all samples. This leaves the original\n319         # point in, which needs to be considered later (i.e. point i is in the\n320         # neighborhood of point i. While True, its useless information)\n321         if self.metric == 'precomputed' and sparse.issparse(X):\n322             # set the diagonal to explicit values, as a point is its own\n323             # neighbor\n324             with warnings.catch_warnings():\n325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)\n326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n327 \n328         neighbors_model = NearestNeighbors(\n329             radius=self.eps, algorithm=self.algorithm,\n330             leaf_size=self.leaf_size, metric=self.metric,\n331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)\n332         neighbors_model.fit(X)\n333         # This has worst case O(n^2) memory complexity\n334         neighborhoods = neighbors_model.radius_neighbors(X,\n335                                                          return_distance=False)\n336 \n337         if sample_weight is None:\n338             n_neighbors = np.array([len(neighbors)\n339                                     for neighbors in neighborhoods])\n340         else:\n341             n_neighbors = np.array([np.sum(sample_weight[neighbors])\n342                                     for neighbors in neighborhoods])\n343 \n344         # Initially, all samples are noise.\n345         labels = np.full(X.shape[0], -1, dtype=np.intp)\n346 \n347         # A list of all core samples found.\n348         core_samples = np.asarray(n_neighbors >= self.min_samples,\n349                                   dtype=np.uint8)\n350         dbscan_inner(core_samples, neighborhoods, labels)\n351 \n352         self.core_sample_indices_ = np.where(core_samples)[0]\n353         self.labels_ = labels\n354 \n355         if len(self.core_sample_indices_):\n356             # fix for scipy sparse indexing issue\n357             self.components_ = X[self.core_sample_indices_].copy()\n358         else:\n359             # no core samples\n360             self.components_ = np.empty((0, X.shape[1]))\n361         return self\n362 \n363     def fit_predict(self, X, y=None, sample_weight=None):\n364         \"\"\"Perform DBSCAN clustering from features or distance matrix,\n365         and return cluster labels.\n366 \n367         Parameters\n368         ----------\n369         X : array-like or sparse matrix, shape (n_samples, n_features), or \\\n370             (n_samples, n_samples)\n371             Training instances to cluster, or distances between instances if\n372             ``metric='precomputed'``. If a sparse matrix is provided, it will\n373             be converted into a sparse ``csr_matrix``.\n374 \n375         sample_weight : array, shape (n_samples,), optional\n376             Weight of each sample, such that a sample with a weight of at least\n377             ``min_samples`` is by itself a core sample; a sample with a\n378             negative weight may inhibit its eps-neighbor from being core.\n379             Note that weights are absolute, and default to 1.\n380 \n381         y : Ignored\n382             Not used, present here for API consistency by convention.\n383 \n384         Returns\n385         -------\n386         labels : ndarray, shape (n_samples,)\n387             Cluster labels. Noisy samples are given the label -1.\n388         \"\"\"\n389         self.fit(X, sample_weight=sample_weight)\n390         return self.labels_\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/_bagging.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_bagging.py",
        "start": 247,
        "end": 391,
        "class_name": "BaseBagging",
        "method_name": "_fit",
        "code": "247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):\n248         \"\"\"Build a Bagging ensemble of estimators from the training\n249            set (X, y).\n250 \n251         Parameters\n252         ----------\n253         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n254             The training input samples. Sparse matrices are accepted only if\n255             they are supported by the base estimator.\n256 \n257         y : array-like of shape (n_samples,)\n258             The target values (class labels in classification, real numbers in\n259             regression).\n260 \n261         max_samples : int or float, optional (default=None)\n262             Argument to use instead of self.max_samples.\n263 \n264         max_depth : int, optional (default=None)\n265             Override value used when constructing base estimator. Only\n266             supported if the base estimator has a max_depth parameter.\n267 \n268         sample_weight : array-like of shape (n_samples,), default=None\n269             Sample weights. If None, then samples are equally weighted.\n270             Note that this is supported only if the base estimator supports\n271             sample weighting.\n272 \n273         Returns\n274         -------\n275         self : object\n276         \"\"\"\n277         random_state = check_random_state(self.random_state)\n278 \n279         # Convert data (X is required to be 2d and indexable)\n280         X, y = check_X_y(\n281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,\n282             multi_output=True\n283         )\n284         if sample_weight is not None:\n285             sample_weight = check_array(sample_weight, ensure_2d=False)\n286             check_consistent_length(y, sample_weight)\n287 \n288         # Remap output\n289         n_samples, self.n_features_ = X.shape\n290         self._n_samples = n_samples\n291         y = self._validate_y(y)\n292 \n293         # Check parameters\n294         self._validate_estimator()\n295 \n296         if max_depth is not None:\n297             self.base_estimator_.max_depth = max_depth\n298 \n299         # Validate max_samples\n300         if max_samples is None:\n301             max_samples = self.max_samples\n302         elif not isinstance(max_samples, numbers.Integral):\n303             max_samples = int(max_samples * X.shape[0])\n304 \n305         if not (0 < max_samples <= X.shape[0]):\n306             raise ValueError(\"max_samples must be in (0, n_samples]\")\n307 \n308         # Store validated integer row sampling value\n309         self._max_samples = max_samples\n310 \n311         # Validate max_features\n312         if isinstance(self.max_features, numbers.Integral):\n313             max_features = self.max_features\n314         elif isinstance(self.max_features, np.float):\n315             max_features = self.max_features * self.n_features_\n316         else:\n317             raise ValueError(\"max_features must be int or float\")\n318 \n319         if not (0 < max_features <= self.n_features_):\n320             raise ValueError(\"max_features must be in (0, n_features]\")\n321 \n322         max_features = max(1, int(max_features))\n323 \n324         # Store validated integer feature sampling value\n325         self._max_features = max_features\n326 \n327         # Other checks\n328         if not self.bootstrap and self.oob_score:\n329             raise ValueError(\"Out of bag estimation only available\"\n330                              \" if bootstrap=True\")\n331 \n332         if self.warm_start and self.oob_score:\n333             raise ValueError(\"Out of bag estimate only available\"\n334                              \" if warm_start=False\")\n335 \n336         if hasattr(self, \"oob_score_\") and self.warm_start:\n337             del self.oob_score_\n338 \n339         if not self.warm_start or not hasattr(self, 'estimators_'):\n340             # Free allocated memory, if any\n341             self.estimators_ = []\n342             self.estimators_features_ = []\n343 \n344         n_more_estimators = self.n_estimators - len(self.estimators_)\n345 \n346         if n_more_estimators < 0:\n347             raise ValueError('n_estimators=%d must be larger or equal to '\n348                              'len(estimators_)=%d when warm_start==True'\n349                              % (self.n_estimators, len(self.estimators_)))\n350 \n351         elif n_more_estimators == 0:\n352             warn(\"Warm-start fitting without increasing n_estimators does not \"\n353                  \"fit new trees.\")\n354             return self\n355 \n356         # Parallel loop\n357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,\n358                                                              self.n_jobs)\n359         total_n_estimators = sum(n_estimators)\n360 \n361         # Advance random state to state after training\n362         # the first n_estimators\n363         if self.warm_start and len(self.estimators_) > 0:\n364             random_state.randint(MAX_INT, size=len(self.estimators_))\n365 \n366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n367         self._seeds = seeds\n368 \n369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n370                                **self._parallel_args())(\n371             delayed(_parallel_build_estimators)(\n372                 n_estimators[i],\n373                 self,\n374                 X,\n375                 y,\n376                 sample_weight,\n377                 seeds[starts[i]:starts[i + 1]],\n378                 total_n_estimators,\n379                 verbose=self.verbose)\n380             for i in range(n_jobs))\n381 \n382         # Reduce\n383         self.estimators_ += list(itertools.chain.from_iterable(\n384             t[0] for t in all_results))\n385         self.estimators_features_ += list(itertools.chain.from_iterable(\n386             t[1] for t in all_results))\n387 \n388         if self.oob_score:\n389             self._set_oob_score(X, y)\n390 \n391         return self\n",
        "intended_behavior": "Integrate _check_sample_weight(sample_weight) at the beginning of the method, before any operations that involve sample_weight, to validate and properly prepare the sample_weight array. This integration should replace or refactor explicit conversions and length checks currently performed on sample_weight. The key goal is to uphold the method's existing behavior\u2014managing sample weights during estimator fitting\u2014while encapsulating validation logic through the standardized _check_sample_weight, thereby improving code maintainability and alignment with project-wide handling of sample_weight."
    },
    {
        "rel_file_path": "sklearn/ensemble/_bagging.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/ensemble/_bagging.py",
        "start": 185,
        "end": 430,
        "class_name": "BaseBagging",
        "method_name": null,
        "code": "185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):\n186     \"\"\"Base class for Bagging meta-estimator.\n187 \n188     Warning: This class should not be used directly. Use derived classes\n189     instead.\n190     \"\"\"\n191 \n192     @abstractmethod\n193     def __init__(self,\n194                  base_estimator=None,\n195                  n_estimators=10,\n196                  max_samples=1.0,\n197                  max_features=1.0,\n198                  bootstrap=True,\n199                  bootstrap_features=False,\n200                  oob_score=False,\n201                  warm_start=False,\n202                  n_jobs=None,\n203                  random_state=None,\n204                  verbose=0):\n205         super().__init__(\n206             base_estimator=base_estimator,\n207             n_estimators=n_estimators)\n208 \n209         self.max_samples = max_samples\n210         self.max_features = max_features\n211         self.bootstrap = bootstrap\n212         self.bootstrap_features = bootstrap_features\n213         self.oob_score = oob_score\n214         self.warm_start = warm_start\n215         self.n_jobs = n_jobs\n216         self.random_state = random_state\n217         self.verbose = verbose\n218 \n219     def fit(self, X, y, sample_weight=None):\n220         \"\"\"Build a Bagging ensemble of estimators from the training\n221            set (X, y).\n222 \n223         Parameters\n224         ----------\n225         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n226             The training input samples. Sparse matrices are accepted only if\n227             they are supported by the base estimator.\n228 \n229         y : array-like of shape (n_samples,)\n230             The target values (class labels in classification, real numbers in\n231             regression).\n232 \n233         sample_weight : array-like of shape (n_samples,), default=None\n234             Sample weights. If None, then samples are equally weighted.\n235             Note that this is supported only if the base estimator supports\n236             sample weighting.\n237 \n238         Returns\n239         -------\n240         self : object\n241         \"\"\"\n242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)\n243 \n244     def _parallel_args(self):\n245         return {}\n246 \n247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):\n248         \"\"\"Build a Bagging ensemble of estimators from the training\n249            set (X, y).\n250 \n251         Parameters\n252         ----------\n253         X : {array-like, sparse matrix} of shape (n_samples, n_features)\n254             The training input samples. Sparse matrices are accepted only if\n255             they are supported by the base estimator.\n256 \n257         y : array-like of shape (n_samples,)\n258             The target values (class labels in classification, real numbers in\n259             regression).\n260 \n261         max_samples : int or float, optional (default=None)\n262             Argument to use instead of self.max_samples.\n263 \n264         max_depth : int, optional (default=None)\n265             Override value used when constructing base estimator. Only\n266             supported if the base estimator has a max_depth parameter.\n267 \n268         sample_weight : array-like of shape (n_samples,), default=None\n269             Sample weights. If None, then samples are equally weighted.\n270             Note that this is supported only if the base estimator supports\n271             sample weighting.\n272 \n273         Returns\n274         -------\n275         self : object\n276         \"\"\"\n277         random_state = check_random_state(self.random_state)\n278 \n279         # Convert data (X is required to be 2d and indexable)\n280         X, y = check_X_y(\n281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,\n282             multi_output=True\n283         )\n284         if sample_weight is not None:\n285             sample_weight = check_array(sample_weight, ensure_2d=False)\n286             check_consistent_length(y, sample_weight)\n287 \n288         # Remap output\n289         n_samples, self.n_features_ = X.shape\n290         self._n_samples = n_samples\n291         y = self._validate_y(y)\n292 \n293         # Check parameters\n294         self._validate_estimator()\n295 \n296         if max_depth is not None:\n297             self.base_estimator_.max_depth = max_depth\n298 \n299         # Validate max_samples\n300         if max_samples is None:\n301             max_samples = self.max_samples\n302         elif not isinstance(max_samples, numbers.Integral):\n303             max_samples = int(max_samples * X.shape[0])\n304 \n305         if not (0 < max_samples <= X.shape[0]):\n306             raise ValueError(\"max_samples must be in (0, n_samples]\")\n307 \n308         # Store validated integer row sampling value\n309         self._max_samples = max_samples\n310 \n311         # Validate max_features\n312         if isinstance(self.max_features, numbers.Integral):\n313             max_features = self.max_features\n314         elif isinstance(self.max_features, np.float):\n315             max_features = self.max_features * self.n_features_\n316         else:\n317             raise ValueError(\"max_features must be int or float\")\n318 \n319         if not (0 < max_features <= self.n_features_):\n320             raise ValueError(\"max_features must be in (0, n_features]\")\n321 \n322         max_features = max(1, int(max_features))\n323 \n324         # Store validated integer feature sampling value\n325         self._max_features = max_features\n326 \n327         # Other checks\n328         if not self.bootstrap and self.oob_score:\n329             raise ValueError(\"Out of bag estimation only available\"\n330                              \" if bootstrap=True\")\n331 \n332         if self.warm_start and self.oob_score:\n333             raise ValueError(\"Out of bag estimate only available\"\n334                              \" if warm_start=False\")\n335 \n336         if hasattr(self, \"oob_score_\") and self.warm_start:\n337             del self.oob_score_\n338 \n339         if not self.warm_start or not hasattr(self, 'estimators_'):\n340             # Free allocated memory, if any\n341             self.estimators_ = []\n342             self.estimators_features_ = []\n343 \n344         n_more_estimators = self.n_estimators - len(self.estimators_)\n345 \n346         if n_more_estimators < 0:\n347             raise ValueError('n_estimators=%d must be larger or equal to '\n348                              'len(estimators_)=%d when warm_start==True'\n349                              % (self.n_estimators, len(self.estimators_)))\n350 \n351         elif n_more_estimators == 0:\n352             warn(\"Warm-start fitting without increasing n_estimators does not \"\n353                  \"fit new trees.\")\n354             return self\n355 \n356         # Parallel loop\n357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,\n358                                                              self.n_jobs)\n359         total_n_estimators = sum(n_estimators)\n360 \n361         # Advance random state to state after training\n362         # the first n_estimators\n363         if self.warm_start and len(self.estimators_) > 0:\n364             random_state.randint(MAX_INT, size=len(self.estimators_))\n365 \n366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)\n367         self._seeds = seeds\n368 \n369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,\n370                                **self._parallel_args())(\n371             delayed(_parallel_build_estimators)(\n372                 n_estimators[i],\n373                 self,\n374                 X,\n375                 y,\n376                 sample_weight,\n377                 seeds[starts[i]:starts[i + 1]],\n378                 total_n_estimators,\n379                 verbose=self.verbose)\n380             for i in range(n_jobs))\n381 \n382         # Reduce\n383         self.estimators_ += list(itertools.chain.from_iterable(\n384             t[0] for t in all_results))\n385         self.estimators_features_ += list(itertools.chain.from_iterable(\n386             t[1] for t in all_results))\n387 \n388         if self.oob_score:\n389             self._set_oob_score(X, y)\n390 \n391         return self\n392 \n393     @abstractmethod\n394     def _set_oob_score(self, X, y):\n395         \"\"\"Calculate out of bag predictions and score.\"\"\"\n396 \n397     def _validate_y(self, y):\n398         if len(y.shape) == 1 or y.shape[1] == 1:\n399             return column_or_1d(y, warn=True)\n400         else:\n401             return y\n402 \n403     def _get_estimators_indices(self):\n404         # Get drawn indices along both sample and feature axes\n405         for seed in self._seeds:\n406             # Operations accessing random_state must be performed identically\n407             # to those in `_parallel_build_estimators()`\n408             random_state = np.random.RandomState(seed)\n409             feature_indices, sample_indices = _generate_bagging_indices(\n410                 random_state, self.bootstrap_features, self.bootstrap,\n411                 self.n_features_, self._n_samples, self._max_features,\n412                 self._max_samples)\n413 \n414             yield feature_indices, sample_indices\n415 \n416     @property\n417     def estimators_samples_(self):\n418         \"\"\"\n419         The subset of drawn samples for each base estimator.\n420 \n421         Returns a dynamically generated list of indices identifying\n422         the samples used for fitting each member of the ensemble, i.e.,\n423         the in-bag samples.\n424 \n425         Note: the list is re-created at each call to the property in order\n426         to reduce the object memory footprint by not storing the sampling\n427         data. Thus fetching the property may be slower than expected.\n428         \"\"\"\n429         return [sample_indices\n430                 for _, sample_indices in self._get_estimators_indices()]\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]