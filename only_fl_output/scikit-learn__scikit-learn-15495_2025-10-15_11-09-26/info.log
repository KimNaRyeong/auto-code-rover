2025-10-15 11:09:26.180 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 11:09:29.511 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=339, output_tokens=12, cost=0.003750
2025-10-15 11:09:29.512 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-15 11:09:29.516 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 11:09:40.278 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=647, output_tokens=480, cost=0.020870
2025-10-15 11:09:40.288 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 11:09:43.375 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=834, output_tokens=64, cost=0.010260
2025-10-15 11:09:43.377 | DEBUG    | Extracted a valid json.
2025-10-15 11:09:43.377 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"DBSCAN\")",
        "search_class(\"BaseBagging\")",
        "search_method_in_class(\"fit\", \"DBSCAN\")",
        "search_method_in_class(\"fit\", \"BaseBagging\")"
    ],
    "bug_locations": []
}
2025-10-15 11:09:43.425 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 11:09:43.426 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 11:09:59.429 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2728, output_tokens=640, cost=0.046480
2025-10-15 11:09:59.444 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 11:10:11.268 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3584, output_tokens=450, cost=0.049340
2025-10-15 11:10:11.278 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 11:10:19.503 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=804, output_tokens=334, cost=0.018060
2025-10-15 11:10:19.504 | DEBUG    | Extracted a valid json.
2025-10-15 11:10:19.504 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method_in_class(\"_fit\", \"BaseBagging\")"],
    "bug_locations":[
        {
            "file": "sklearn/cluster/_dbscan.py", 
            "class": "DBSCAN", 
            "method": "fit", 
            "intended_behavior": "Inside the `DBSCAN.fit` method, rather than manually validating `sample_weight` using `check_consistent_length` and converting it to a numpy array (if not `None`), we should delegate this task to the `_check_sample_weight` utility function. The rest of the functionality regarding how `sample_weight` influences the determination of core samples and the clustering process should remain unaffected. This change is aimed at standardizing how `sample_weight` is processed and validated across the library, improving code maintainability and consistency."
        },
        {
            "file": "sklearn/ensemble/_bagging.py", 
            "class": "BaseBagging", 
            "method": "fit", 
            "intended_behavior": "Although the direct manipulation of `sample_weight` is not observable within the `fit` method, this location is flagged for further investigation regarding how `sample_weight` is passed along to and handled by the `_fit` method. The ideal outcome here is to ensure that `_fit` (or any other downstream method that handles `sample_weight`) uses `_check_sample_weight` for consistent and standardized validation of `sample_weight`. This approach will help centralize the handling rules for `sample_weight`, making the codebase easier to maintain and adapt to future requirements."
        }
    ]
}
2025-10-15 11:10:19.541 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 11:10:19.543 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 11:10:39.145 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5486, output_tokens=605, cost=0.073010
2025-10-15 11:10:39.158 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 11:10:47.832 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6307, output_tokens=360, cost=0.073870
2025-10-15 11:10:47.841 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 11:10:56.497 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=714, output_tokens=278, cost=0.015480
2025-10-15 11:10:56.498 | DEBUG    | Extracted a valid json.
2025-10-15 11:10:56.498 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/cluster/_dbscan.py",
            "class": "DBSCAN",
            "method": "fit",
            "intended_behavior": "Replace the manual array transformation and check_consistent_length validation of sample_weight with a call to _check_sample_weight(sample_weight). This will validate and prepare sample_weight for use while preserving existing functionality. The call to _check_sample_weight would ensure that sample_weight meets all necessary criteria for further processes within the method, thus maintaining the method's integrity and adhering to the library's standards for handling sample_weight."
        },
        {
            "file": "sklearn/ensemble/_bagging.py",
            "class": "BaseBagging",
            "method": "_fit",
            "intended_behavior": "Integrate _check_sample_weight(sample_weight) at the beginning of the method, before any operations that involve sample_weight, to validate and properly prepare the sample_weight array. This integration should replace or refactor explicit conversions and length checks currently performed on sample_weight. The key goal is to uphold the method's existing behavior—managing sample weights during estimator fitting—while encapsulating validation logic through the standardized _check_sample_weight, thereby improving code maintainability and alignment with project-wide handling of sample_weight."
        }
    ]
}
2025-10-15 11:10:56.506 | DEBUG    | Bug location extracted successfully: [<file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>Replace the manual array transformation and check_consistent_length validation of sample_weight with a call to _check_sample_weight(sample_weight). This will validate and prepare sample_weight for use while preserving existing functionality. The call to _check_sample_weight would ensure that sample_weight meets all necessary criteria for further processes within the method, thus maintaining the method's integrity and adhering to the library's standards for handling sample_weight.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>_fit</method>
<code>
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self

</code><intended_behavior>Integrate _check_sample_weight(sample_weight) at the beginning of the method, before any operations that involve sample_weight, to validate and properly prepare the sample_weight array. This integration should replace or refactor explicit conversions and length checks currently performed on sample_weight. The key goal is to uphold the method's existing behavior—managing sample weights during estimator fitting—while encapsulating validation logic through the standardized _check_sample_weight, thereby improving code maintainability and alignment with project-wide handling of sample_weight.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>None</method>
<code>
185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):
186     """Base class for Bagging meta-estimator.
187 
188     Warning: This class should not be used directly. Use derived classes
189     instead.
190     """
191 
192     @abstractmethod
193     def __init__(self,
194                  base_estimator=None,
195                  n_estimators=10,
196                  max_samples=1.0,
197                  max_features=1.0,
198                  bootstrap=True,
199                  bootstrap_features=False,
200                  oob_score=False,
201                  warm_start=False,
202                  n_jobs=None,
203                  random_state=None,
204                  verbose=0):
205         super().__init__(
206             base_estimator=base_estimator,
207             n_estimators=n_estimators)
208 
209         self.max_samples = max_samples
210         self.max_features = max_features
211         self.bootstrap = bootstrap
212         self.bootstrap_features = bootstrap_features
213         self.oob_score = oob_score
214         self.warm_start = warm_start
215         self.n_jobs = n_jobs
216         self.random_state = random_state
217         self.verbose = verbose
218 
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
243 
244     def _parallel_args(self):
245         return {}
246 
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self
392 
393     @abstractmethod
394     def _set_oob_score(self, X, y):
395         """Calculate out of bag predictions and score."""
396 
397     def _validate_y(self, y):
398         if len(y.shape) == 1 or y.shape[1] == 1:
399             return column_or_1d(y, warn=True)
400         else:
401             return y
402 
403     def _get_estimators_indices(self):
404         # Get drawn indices along both sample and feature axes
405         for seed in self._seeds:
406             # Operations accessing random_state must be performed identically
407             # to those in `_parallel_build_estimators()`
408             random_state = np.random.RandomState(seed)
409             feature_indices, sample_indices = _generate_bagging_indices(
410                 random_state, self.bootstrap_features, self.bootstrap,
411                 self.n_features_, self._n_samples, self._max_features,
412                 self._max_samples)
413 
414             yield feature_indices, sample_indices
415 
416     @property
417     def estimators_samples_(self):
418         """
419         The subset of drawn samples for each base estimator.
420 
421         Returns a dynamically generated list of indices identifying
422         the samples used for fitting each member of the ensemble, i.e.,
423         the in-bag samples.
424 
425         Note: the list is re-created at each call to the property in order
426         to reduce the object memory footprint by not storing the sampling
427         data. Thus fetching the property may be slower than expected.
428         """
429         return [sample_indices
430                 for _, sample_indices in self._get_estimators_indices()]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 11:10:56.507 | INFO     | Search completed. Bug locations: [<file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>fit</method>
<code>
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self

</code><intended_behavior>Replace the manual array transformation and check_consistent_length validation of sample_weight with a call to _check_sample_weight(sample_weight). This will validate and prepare sample_weight for use while preserving existing functionality. The call to _check_sample_weight would ensure that sample_weight meets all necessary criteria for further processes within the method, thus maintaining the method's integrity and adhering to the library's standards for handling sample_weight.</intended_behavior>, <file>sklearn/cluster/_dbscan.py</file>
<class>DBSCAN</class>
<method>None</method>
<code>
147 class DBSCAN(ClusterMixin, BaseEstimator):
148     """Perform DBSCAN clustering from vector array or distance matrix.
149 
150     DBSCAN - Density-Based Spatial Clustering of Applications with Noise.
151     Finds core samples of high density and expands clusters from them.
152     Good for data which contains clusters of similar density.
153 
154     Read more in the :ref:`User Guide <dbscan>`.
155 
156     Parameters
157     ----------
158     eps : float, optional
159         The maximum distance between two samples for one to be considered
160         as in the neighborhood of the other. This is not a maximum bound
161         on the distances of points within a cluster. This is the most
162         important DBSCAN parameter to choose appropriately for your data set
163         and distance function.
164 
165     min_samples : int, optional
166         The number of samples (or total weight) in a neighborhood for a point
167         to be considered as a core point. This includes the point itself.
168 
169     metric : string, or callable
170         The metric to use when calculating distance between instances in a
171         feature array. If metric is a string or callable, it must be one of
172         the options allowed by :func:`sklearn.metrics.pairwise_distances` for
173         its metric parameter.
174         If metric is "precomputed", X is assumed to be a distance matrix and
175         must be square. X may be a :term:`Glossary <sparse graph>`, in which
176         case only "nonzero" elements may be considered neighbors for DBSCAN.
177 
178         .. versionadded:: 0.17
179            metric *precomputed* to accept precomputed sparse matrix.
180 
181     metric_params : dict, optional
182         Additional keyword arguments for the metric function.
183 
184         .. versionadded:: 0.19
185 
186     algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional
187         The algorithm to be used by the NearestNeighbors module
188         to compute pointwise distances and find nearest neighbors.
189         See NearestNeighbors module documentation for details.
190 
191     leaf_size : int, optional (default = 30)
192         Leaf size passed to BallTree or cKDTree. This can affect the speed
193         of the construction and query, as well as the memory required
194         to store the tree. The optimal value depends
195         on the nature of the problem.
196 
197     p : float, optional
198         The power of the Minkowski metric to be used to calculate distance
199         between points.
200 
201     n_jobs : int or None, optional (default=None)
202         The number of parallel jobs to run.
203         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
204         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
205         for more details.
206 
207     Attributes
208     ----------
209     core_sample_indices_ : array, shape = [n_core_samples]
210         Indices of core samples.
211 
212     components_ : array, shape = [n_core_samples, n_features]
213         Copy of each core sample found by training.
214 
215     labels_ : array, shape = [n_samples]
216         Cluster labels for each point in the dataset given to fit().
217         Noisy samples are given the label -1.
218 
219     Examples
220     --------
221     >>> from sklearn.cluster import DBSCAN
222     >>> import numpy as np
223     >>> X = np.array([[1, 2], [2, 2], [2, 3],
224     ...               [8, 7], [8, 8], [25, 80]])
225     >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)
226     >>> clustering.labels_
227     array([ 0,  0,  0,  1,  1, -1])
228     >>> clustering
229     DBSCAN(eps=3, min_samples=2)
230 
231     See also
232     --------
233     OPTICS
234         A similar clustering at multiple values of eps. Our implementation
235         is optimized for memory usage.
236 
237     Notes
238     -----
239     For an example, see :ref:`examples/cluster/plot_dbscan.py
240     <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.
241 
242     This implementation bulk-computes all neighborhood queries, which increases
243     the memory complexity to O(n.d) where d is the average number of neighbors,
244     while original DBSCAN had memory complexity O(n). It may attract a higher
245     memory complexity when querying these nearest neighborhoods, depending
246     on the ``algorithm``.
247 
248     One way to avoid the query complexity is to pre-compute sparse
249     neighborhoods in chunks using
250     :func:`NearestNeighbors.radius_neighbors_graph
251     <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with
252     ``mode='distance'``, then using ``metric='precomputed'`` here.
253 
254     Another way to reduce memory and computation time is to remove
255     (near-)duplicate points and use ``sample_weight`` instead.
256 
257     :class:`cluster.OPTICS` provides a similar clustering with lower memory
258     usage.
259 
260     References
261     ----------
262     Ester, M., H. P. Kriegel, J. Sander, and X. Xu, "A Density-Based
263     Algorithm for Discovering Clusters in Large Spatial Databases with Noise".
264     In: Proceedings of the 2nd International Conference on Knowledge Discovery
265     and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996
266 
267     Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
268     DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.
269     ACM Transactions on Database Systems (TODS), 42(3), 19.
270     """
271 
272     def __init__(self, eps=0.5, min_samples=5, metric='euclidean',
273                  metric_params=None, algorithm='auto', leaf_size=30, p=None,
274                  n_jobs=None):
275         self.eps = eps
276         self.min_samples = min_samples
277         self.metric = metric
278         self.metric_params = metric_params
279         self.algorithm = algorithm
280         self.leaf_size = leaf_size
281         self.p = p
282         self.n_jobs = n_jobs
283 
284     def fit(self, X, y=None, sample_weight=None):
285         """Perform DBSCAN clustering from features, or distance matrix.
286 
287         Parameters
288         ----------
289         X : array-like or sparse matrix, shape (n_samples, n_features), or \
290             (n_samples, n_samples)
291             Training instances to cluster, or distances between instances if
292             ``metric='precomputed'``. If a sparse matrix is provided, it will
293             be converted into a sparse ``csr_matrix``.
294 
295         sample_weight : array, shape (n_samples,), optional
296             Weight of each sample, such that a sample with a weight of at least
297             ``min_samples`` is by itself a core sample; a sample with a
298             negative weight may inhibit its eps-neighbor from being core.
299             Note that weights are absolute, and default to 1.
300 
301         y : Ignored
302             Not used, present here for API consistency by convention.
303 
304         Returns
305         -------
306         self
307 
308         """
309         X = check_array(X, accept_sparse='csr')
310 
311         if not self.eps > 0.0:
312             raise ValueError("eps must be positive.")
313 
314         if sample_weight is not None:
315             sample_weight = np.asarray(sample_weight)
316             check_consistent_length(X, sample_weight)
317 
318         # Calculate neighborhood for all samples. This leaves the original
319         # point in, which needs to be considered later (i.e. point i is in the
320         # neighborhood of point i. While True, its useless information)
321         if self.metric == 'precomputed' and sparse.issparse(X):
322             # set the diagonal to explicit values, as a point is its own
323             # neighbor
324             with warnings.catch_warnings():
325                 warnings.simplefilter('ignore', sparse.SparseEfficiencyWarning)
326                 X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place
327 
328         neighbors_model = NearestNeighbors(
329             radius=self.eps, algorithm=self.algorithm,
330             leaf_size=self.leaf_size, metric=self.metric,
331             metric_params=self.metric_params, p=self.p, n_jobs=self.n_jobs)
332         neighbors_model.fit(X)
333         # This has worst case O(n^2) memory complexity
334         neighborhoods = neighbors_model.radius_neighbors(X,
335                                                          return_distance=False)
336 
337         if sample_weight is None:
338             n_neighbors = np.array([len(neighbors)
339                                     for neighbors in neighborhoods])
340         else:
341             n_neighbors = np.array([np.sum(sample_weight[neighbors])
342                                     for neighbors in neighborhoods])
343 
344         # Initially, all samples are noise.
345         labels = np.full(X.shape[0], -1, dtype=np.intp)
346 
347         # A list of all core samples found.
348         core_samples = np.asarray(n_neighbors >= self.min_samples,
349                                   dtype=np.uint8)
350         dbscan_inner(core_samples, neighborhoods, labels)
351 
352         self.core_sample_indices_ = np.where(core_samples)[0]
353         self.labels_ = labels
354 
355         if len(self.core_sample_indices_):
356             # fix for scipy sparse indexing issue
357             self.components_ = X[self.core_sample_indices_].copy()
358         else:
359             # no core samples
360             self.components_ = np.empty((0, X.shape[1]))
361         return self
362 
363     def fit_predict(self, X, y=None, sample_weight=None):
364         """Perform DBSCAN clustering from features or distance matrix,
365         and return cluster labels.
366 
367         Parameters
368         ----------
369         X : array-like or sparse matrix, shape (n_samples, n_features), or \
370             (n_samples, n_samples)
371             Training instances to cluster, or distances between instances if
372             ``metric='precomputed'``. If a sparse matrix is provided, it will
373             be converted into a sparse ``csr_matrix``.
374 
375         sample_weight : array, shape (n_samples,), optional
376             Weight of each sample, such that a sample with a weight of at least
377             ``min_samples`` is by itself a core sample; a sample with a
378             negative weight may inhibit its eps-neighbor from being core.
379             Note that weights are absolute, and default to 1.
380 
381         y : Ignored
382             Not used, present here for API consistency by convention.
383 
384         Returns
385         -------
386         labels : ndarray, shape (n_samples,)
387             Cluster labels. Noisy samples are given the label -1.
388         """
389         self.fit(X, sample_weight=sample_weight)
390         return self.labels_

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>_fit</method>
<code>
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self

</code><intended_behavior>Integrate _check_sample_weight(sample_weight) at the beginning of the method, before any operations that involve sample_weight, to validate and properly prepare the sample_weight array. This integration should replace or refactor explicit conversions and length checks currently performed on sample_weight. The key goal is to uphold the method's existing behavior—managing sample weights during estimator fitting—while encapsulating validation logic through the standardized _check_sample_weight, thereby improving code maintainability and alignment with project-wide handling of sample_weight.</intended_behavior>, <file>sklearn/ensemble/_bagging.py</file>
<class>BaseBagging</class>
<method>None</method>
<code>
185 class BaseBagging(BaseEnsemble, metaclass=ABCMeta):
186     """Base class for Bagging meta-estimator.
187 
188     Warning: This class should not be used directly. Use derived classes
189     instead.
190     """
191 
192     @abstractmethod
193     def __init__(self,
194                  base_estimator=None,
195                  n_estimators=10,
196                  max_samples=1.0,
197                  max_features=1.0,
198                  bootstrap=True,
199                  bootstrap_features=False,
200                  oob_score=False,
201                  warm_start=False,
202                  n_jobs=None,
203                  random_state=None,
204                  verbose=0):
205         super().__init__(
206             base_estimator=base_estimator,
207             n_estimators=n_estimators)
208 
209         self.max_samples = max_samples
210         self.max_features = max_features
211         self.bootstrap = bootstrap
212         self.bootstrap_features = bootstrap_features
213         self.oob_score = oob_score
214         self.warm_start = warm_start
215         self.n_jobs = n_jobs
216         self.random_state = random_state
217         self.verbose = verbose
218 
219     def fit(self, X, y, sample_weight=None):
220         """Build a Bagging ensemble of estimators from the training
221            set (X, y).
222 
223         Parameters
224         ----------
225         X : {array-like, sparse matrix} of shape (n_samples, n_features)
226             The training input samples. Sparse matrices are accepted only if
227             they are supported by the base estimator.
228 
229         y : array-like of shape (n_samples,)
230             The target values (class labels in classification, real numbers in
231             regression).
232 
233         sample_weight : array-like of shape (n_samples,), default=None
234             Sample weights. If None, then samples are equally weighted.
235             Note that this is supported only if the base estimator supports
236             sample weighting.
237 
238         Returns
239         -------
240         self : object
241         """
242         return self._fit(X, y, self.max_samples, sample_weight=sample_weight)
243 
244     def _parallel_args(self):
245         return {}
246 
247     def _fit(self, X, y, max_samples=None, max_depth=None, sample_weight=None):
248         """Build a Bagging ensemble of estimators from the training
249            set (X, y).
250 
251         Parameters
252         ----------
253         X : {array-like, sparse matrix} of shape (n_samples, n_features)
254             The training input samples. Sparse matrices are accepted only if
255             they are supported by the base estimator.
256 
257         y : array-like of shape (n_samples,)
258             The target values (class labels in classification, real numbers in
259             regression).
260 
261         max_samples : int or float, optional (default=None)
262             Argument to use instead of self.max_samples.
263 
264         max_depth : int, optional (default=None)
265             Override value used when constructing base estimator. Only
266             supported if the base estimator has a max_depth parameter.
267 
268         sample_weight : array-like of shape (n_samples,), default=None
269             Sample weights. If None, then samples are equally weighted.
270             Note that this is supported only if the base estimator supports
271             sample weighting.
272 
273         Returns
274         -------
275         self : object
276         """
277         random_state = check_random_state(self.random_state)
278 
279         # Convert data (X is required to be 2d and indexable)
280         X, y = check_X_y(
281             X, y, ['csr', 'csc'], dtype=None, force_all_finite=False,
282             multi_output=True
283         )
284         if sample_weight is not None:
285             sample_weight = check_array(sample_weight, ensure_2d=False)
286             check_consistent_length(y, sample_weight)
287 
288         # Remap output
289         n_samples, self.n_features_ = X.shape
290         self._n_samples = n_samples
291         y = self._validate_y(y)
292 
293         # Check parameters
294         self._validate_estimator()
295 
296         if max_depth is not None:
297             self.base_estimator_.max_depth = max_depth
298 
299         # Validate max_samples
300         if max_samples is None:
301             max_samples = self.max_samples
302         elif not isinstance(max_samples, numbers.Integral):
303             max_samples = int(max_samples * X.shape[0])
304 
305         if not (0 < max_samples <= X.shape[0]):
306             raise ValueError("max_samples must be in (0, n_samples]")
307 
308         # Store validated integer row sampling value
309         self._max_samples = max_samples
310 
311         # Validate max_features
312         if isinstance(self.max_features, numbers.Integral):
313             max_features = self.max_features
314         elif isinstance(self.max_features, np.float):
315             max_features = self.max_features * self.n_features_
316         else:
317             raise ValueError("max_features must be int or float")
318 
319         if not (0 < max_features <= self.n_features_):
320             raise ValueError("max_features must be in (0, n_features]")
321 
322         max_features = max(1, int(max_features))
323 
324         # Store validated integer feature sampling value
325         self._max_features = max_features
326 
327         # Other checks
328         if not self.bootstrap and self.oob_score:
329             raise ValueError("Out of bag estimation only available"
330                              " if bootstrap=True")
331 
332         if self.warm_start and self.oob_score:
333             raise ValueError("Out of bag estimate only available"
334                              " if warm_start=False")
335 
336         if hasattr(self, "oob_score_") and self.warm_start:
337             del self.oob_score_
338 
339         if not self.warm_start or not hasattr(self, 'estimators_'):
340             # Free allocated memory, if any
341             self.estimators_ = []
342             self.estimators_features_ = []
343 
344         n_more_estimators = self.n_estimators - len(self.estimators_)
345 
346         if n_more_estimators < 0:
347             raise ValueError('n_estimators=%d must be larger or equal to '
348                              'len(estimators_)=%d when warm_start==True'
349                              % (self.n_estimators, len(self.estimators_)))
350 
351         elif n_more_estimators == 0:
352             warn("Warm-start fitting without increasing n_estimators does not "
353                  "fit new trees.")
354             return self
355 
356         # Parallel loop
357         n_jobs, n_estimators, starts = _partition_estimators(n_more_estimators,
358                                                              self.n_jobs)
359         total_n_estimators = sum(n_estimators)
360 
361         # Advance random state to state after training
362         # the first n_estimators
363         if self.warm_start and len(self.estimators_) > 0:
364             random_state.randint(MAX_INT, size=len(self.estimators_))
365 
366         seeds = random_state.randint(MAX_INT, size=n_more_estimators)
367         self._seeds = seeds
368 
369         all_results = Parallel(n_jobs=n_jobs, verbose=self.verbose,
370                                **self._parallel_args())(
371             delayed(_parallel_build_estimators)(
372                 n_estimators[i],
373                 self,
374                 X,
375                 y,
376                 sample_weight,
377                 seeds[starts[i]:starts[i + 1]],
378                 total_n_estimators,
379                 verbose=self.verbose)
380             for i in range(n_jobs))
381 
382         # Reduce
383         self.estimators_ += list(itertools.chain.from_iterable(
384             t[0] for t in all_results))
385         self.estimators_features_ += list(itertools.chain.from_iterable(
386             t[1] for t in all_results))
387 
388         if self.oob_score:
389             self._set_oob_score(X, y)
390 
391         return self
392 
393     @abstractmethod
394     def _set_oob_score(self, X, y):
395         """Calculate out of bag predictions and score."""
396 
397     def _validate_y(self, y):
398         if len(y.shape) == 1 or y.shape[1] == 1:
399             return column_or_1d(y, warn=True)
400         else:
401             return y
402 
403     def _get_estimators_indices(self):
404         # Get drawn indices along both sample and feature axes
405         for seed in self._seeds:
406             # Operations accessing random_state must be performed identically
407             # to those in `_parallel_build_estimators()`
408             random_state = np.random.RandomState(seed)
409             feature_indices, sample_indices = _generate_bagging_indices(
410                 random_state, self.bootstrap_features, self.bootstrap,
411                 self.n_features_, self._n_samples, self._max_features,
412                 self._max_samples)
413 
414             yield feature_indices, sample_indices
415 
416     @property
417     def estimators_samples_(self):
418         """
419         The subset of drawn samples for each base estimator.
420 
421         Returns a dynamically generated list of indices identifying
422         the samples used for fitting each member of the ensemble, i.e.,
423         the in-bag samples.
424 
425         Note: the list is re-created at each call to the property in order
426         to reduce the object memory footprint by not storing the sampling
427         data. Thus fetching the property may be slower than expected.
428         """
429         return [sample_indices
430                 for _, sample_indices in self._get_estimators_indices()]

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 11:10:56.507 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 11:10:56.507 | INFO     | Search-only: skipping patch selection
2025-10-15 11:10:56.597 | INFO     | Task scikit-learn__scikit-learn-15495 completed successfully.
2025-10-15 11:10:56.598 | INFO     | No patch generated. You can try running ACR again.
