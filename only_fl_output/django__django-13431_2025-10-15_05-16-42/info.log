2025-10-15 05:16:42.751 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-15 05:16:47.504 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=268, output_tokens=12, cost=0.003040
2025-10-15 05:17:01.925 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=448, output_tokens=625, cost=0.023230
2025-10-15 05:17:03.084 | INFO     | registered non reproducing test 0
2025-10-15 05:17:23.054 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1343, output_tokens=891, cost=0.040160
2025-10-15 05:17:24.201 | INFO     | registered non reproducing test 1
2025-10-15 05:17:42.514 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2366, output_tokens=786, cost=0.047240
2025-10-15 05:17:43.630 | INFO     | registered non reproducing test 2
2025-10-15 05:17:43.630 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-15 05:17:43.633 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 05:17:51.067 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=577, output_tokens=288, cost=0.014410
2025-10-15 05:17:51.074 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 05:17:53.695 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=642, output_tokens=49, cost=0.007890
2025-10-15 05:17:53.697 | DEBUG    | Extracted a valid json.
2025-10-15 05:17:53.697 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"QuerySet\")",
        "search_method(\"annotate\")",
        "search_method(\"aggregate\")",
        "search_class(\"F\")"
    ],
    "bug_locations": []
}
2025-10-15 05:17:53.767 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-15 05:17:53.768 | DEBUG    | <Agent search> Analyzing search results.
2025-10-15 05:18:13.840 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3076, output_tokens=686, cost=0.051340
2025-10-15 05:18:13.854 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-15 05:18:23.243 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3978, output_tokens=387, cost=0.051390
2025-10-15 05:18:23.251 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-15 05:18:31.474 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=741, output_tokens=327, cost=0.017220
2025-10-15 05:18:31.475 | DEBUG    | Extracted a valid json.
2025-10-15 05:18:31.475 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "aggregate",
            "intended_behavior": "When constructing the SQL query for aggregation, the method should correctly handle and reference annotated fields created in a preceding annotate call. This means ensuring that any aliases used for annotated fields are consistently referenced throughout the query construction process, especially when those annotated fields are utilized in aggregates. The intersection point where the aggregate queries include annotated fields must reference these annotations correctly, avoiding SQL errors like the unknown column issue because of misreferenced aliases. This adjustment ensures that when Django constructs subqueries involving annotations and aggregates, all references to annotated fields are accurate and coherent, preserving the aliasing without leading to resolution errors."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "Although not directly implicated as faulty, ensuring the annotate method's output is correctly utilized by subsequent aggregate calls is essential. The annotate function should clearly define the scope and aliasing of annotated fields so that these can be understood and handled without ambiguity in subsequent operations, specifically aggregate. The method itself may not require changes, but its interaction with aggregate and the overall SQL construction logic should be verified to ensure annotations are appropriately factored into the aggregate's SQL generation logic. This includes validating the chaining of queryset methods where annotations serve as inputs to aggregates, leading to proper SQL command generation that considers the established aliases."
        }
    ]
}
2025-10-15 05:18:31.490 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>aggregate</method>
<code>
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             if not query.annotations[alias].contains_aggregate:
390                 raise TypeError("%s is not an aggregate expression" % alias)
391         return query.get_aggregation(self.db, kwargs)

</code><intended_behavior>When constructing the SQL query for aggregation, the method should correctly handle and reference annotated fields created in a preceding annotate call. This means ensuring that any aliases used for annotated fields are consistently referenced throughout the query construction process, especially when those annotated fields are utilized in aggregates. The intersection point where the aggregate queries include annotated fields must reference these annotations correctly, avoiding SQL errors like the unknown column issue because of misreferenced aliases. This adjustment ensures that when Django constructs subqueries involving annotations and aggregates, all references to annotated fields are accurate and coherent, preserving the aliasing without leading to resolution errors.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             if not query.annotations[alias].contains_aggregate:
390                 raise TypeError("%s is not an aggregate expression" % alias)
391         return query.get_aggregation(self.db, kwargs)
392 
393     def count(self):
394         """
395         Perform a SELECT COUNT() and return the number of records as an
396         integer.
397 
398         If the QuerySet is already fully cached, return the length of the
399         cached results set to avoid multiple SELECT COUNT(*) calls.
400         """
401         if self._result_cache is not None:
402             return len(self._result_cache)
403 
404         return self.query.get_count(using=self.db)
405 
406     def get(self, *args, **kwargs):
407         """
408         Perform the query and return a single object matching the given
409         keyword arguments.
410         """
411         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
412         if self.query.can_filter() and not self.query.distinct_fields:
413             clone = clone.order_by()
414         limit = None
415         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
416             limit = MAX_GET_RESULTS
417             clone.query.set_limits(high=limit)
418         num = len(clone)
419         if num == 1:
420             return clone._result_cache[0]
421         if not num:
422             raise self.model.DoesNotExist(
423                 "%s matching query does not exist." %
424                 self.model._meta.object_name
425             )
426         raise self.model.MultipleObjectsReturned(
427             'get() returned more than one %s -- it returned %s!' % (
428                 self.model._meta.object_name,
429                 num if not limit or num < limit else 'more than %s' % (limit - 1),
430             )
431         )
432 
433     def create(self, **kwargs):
434         """
435         Create a new object with the given kwargs, saving it to the database
436         and returning the created object.
437         """
438         obj = self.model(**kwargs)
439         self._for_write = True
440         obj.save(force_insert=True, using=self.db)
441         return obj
442 
443     def _populate_pk_values(self, objs):
444         for obj in objs:
445             if obj.pk is None:
446                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
447 
448     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
449         """
450         Insert each of the instances into the database. Do *not* call
451         save() on each of the instances, do not send any pre/post_save
452         signals, and do not set the primary key attribute if it is an
453         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
454         Multi-table models are not supported.
455         """
456         # When you bulk insert you don't get the primary keys back (if it's an
457         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
458         # you can't insert into the child tables which references this. There
459         # are two workarounds:
460         # 1) This could be implemented if you didn't have an autoincrement pk
461         # 2) You could do it by doing O(n) normal inserts into the parent
462         #    tables to get the primary keys back and then doing a single bulk
463         #    insert into the childmost table.
464         # We currently set the primary keys on the objects when using
465         # PostgreSQL via the RETURNING ID clause. It should be possible for
466         # Oracle as well, but the semantics for extracting the primary keys is
467         # trickier so it's not done yet.
468         assert batch_size is None or batch_size > 0
469         # Check that the parents share the same concrete model with the our
470         # model to detect the inheritance pattern ConcreteGrandParent ->
471         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
472         # would not identify that case as involving multiple tables.
473         for parent in self.model._meta.get_parent_list():
474             if parent._meta.concrete_model is not self.model._meta.concrete_model:
475                 raise ValueError("Can't bulk create a multi-table inherited model")
476         if not objs:
477             return objs
478         self._for_write = True
479         connection = connections[self.db]
480         opts = self.model._meta
481         fields = opts.concrete_fields
482         objs = list(objs)
483         self._populate_pk_values(objs)
484         with transaction.atomic(using=self.db, savepoint=False):
485             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
486             if objs_with_pk:
487                 returned_columns = self._batched_insert(
488                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
489                 )
490                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
491                     for result, field in zip(results, opts.db_returning_fields):
492                         if field != opts.pk:
493                             setattr(obj_with_pk, field.attname, result)
494                 for obj_with_pk in objs_with_pk:
495                     obj_with_pk._state.adding = False
496                     obj_with_pk._state.db = self.db
497             if objs_without_pk:
498                 fields = [f for f in fields if not isinstance(f, AutoField)]
499                 returned_columns = self._batched_insert(
500                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
501                 )
502                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
503                     assert len(returned_columns) == len(objs_without_pk)
504                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
505                     for result, field in zip(results, opts.db_returning_fields):
506                         setattr(obj_without_pk, field.attname, result)
507                     obj_without_pk._state.adding = False
508                     obj_without_pk._state.db = self.db
509 
510         return objs
511 
512     def bulk_update(self, objs, fields, batch_size=None):
513         """
514         Update the given fields in each of the given objects in the database.
515         """
516         if batch_size is not None and batch_size < 0:
517             raise ValueError('Batch size must be a positive integer.')
518         if not fields:
519             raise ValueError('Field names must be given to bulk_update().')
520         objs = tuple(objs)
521         if any(obj.pk is None for obj in objs):
522             raise ValueError('All bulk_update() objects must have a primary key set.')
523         fields = [self.model._meta.get_field(name) for name in fields]
524         if any(not f.concrete or f.many_to_many for f in fields):
525             raise ValueError('bulk_update() can only be used with concrete fields.')
526         if any(f.primary_key for f in fields):
527             raise ValueError('bulk_update() cannot be used with primary key fields.')
528         if not objs:
529             return
530         # PK is used twice in the resulting update query, once in the filter
531         # and once in the WHEN. Each field will also have one CAST.
532         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
533         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
534         requires_casting = connections[self.db].features.requires_casted_case_in_updates
535         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
536         updates = []
537         for batch_objs in batches:
538             update_kwargs = {}
539             for field in fields:
540                 when_statements = []
541                 for obj in batch_objs:
542                     attr = getattr(obj, field.attname)
543                     if not isinstance(attr, Expression):
544                         attr = Value(attr, output_field=field)
545                     when_statements.append(When(pk=obj.pk, then=attr))
546                 case_statement = Case(*when_statements, output_field=field)
547                 if requires_casting:
548                     case_statement = Cast(case_statement, output_field=field)
549                 update_kwargs[field.attname] = case_statement
550             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
551         with transaction.atomic(using=self.db, savepoint=False):
552             for pks, update_kwargs in updates:
553                 self.filter(pk__in=pks).update(**update_kwargs)
554     bulk_update.alters_data = True
555 
556     def get_or_create(self, defaults=None, **kwargs):
557         """
558         Look up an object with the given kwargs, creating one if necessary.
559         Return a tuple of (object, created), where created is a boolean
560         specifying whether an object was created.
561         """
562         # The get() needs to be targeted at the write database in order
563         # to avoid potential transaction consistency problems.
564         self._for_write = True
565         try:
566             return self.get(**kwargs), False
567         except self.model.DoesNotExist:
568             params = self._extract_model_params(defaults, **kwargs)
569             # Try to create an object using passed params.
570             try:
571                 with transaction.atomic(using=self.db):
572                     params = dict(resolve_callables(params))
573                     return self.create(**params), True
574             except IntegrityError:
575                 try:
576                     return self.get(**kwargs), False
577                 except self.model.DoesNotExist:
578                     pass
579                 raise
580 
581     def update_or_create(self, defaults=None, **kwargs):
582         """
583         Look up an object with the given kwargs, updating one with defaults
584         if it exists, otherwise create a new one.
585         Return a tuple (object, created), where created is a boolean
586         specifying whether an object was created.
587         """
588         defaults = defaults or {}
589         self._for_write = True
590         with transaction.atomic(using=self.db):
591             # Lock the row so that a concurrent update is blocked until
592             # update_or_create() has performed its save.
593             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
594             if created:
595                 return obj, created
596             for k, v in resolve_callables(defaults):
597                 setattr(obj, k, v)
598             obj.save(using=self.db)
599         return obj, False
600 
601     def _extract_model_params(self, defaults, **kwargs):
602         """
603         Prepare `params` for creating a model instance based on the given
604         kwargs; for use by get_or_create().
605         """
606         defaults = defaults or {}
607         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
608         params.update(defaults)
609         property_names = self.model._meta._property_names
610         invalid_params = []
611         for param in params:
612             try:
613                 self.model._meta.get_field(param)
614             except exceptions.FieldDoesNotExist:
615                 # It's okay to use a model's property if it has a setter.
616                 if not (param in property_names and getattr(self.model, param).fset):
617                     invalid_params.append(param)
618         if invalid_params:
619             raise exceptions.FieldError(
620                 "Invalid field name(s) for model %s: '%s'." % (
621                     self.model._meta.object_name,
622                     "', '".join(sorted(invalid_params)),
623                 ))
624         return params
625 
626     def _earliest(self, *fields):
627         """
628         Return the earliest object according to fields (if given) or by the
629         model's Meta.get_latest_by.
630         """
631         if fields:
632             order_by = fields
633         else:
634             order_by = getattr(self.model._meta, 'get_latest_by')
635             if order_by and not isinstance(order_by, (tuple, list)):
636                 order_by = (order_by,)
637         if order_by is None:
638             raise ValueError(
639                 "earliest() and latest() require either fields as positional "
640                 "arguments or 'get_latest_by' in the model's Meta."
641             )
642 
643         assert not self.query.is_sliced, \
644             "Cannot change a query once a slice has been taken."
645         obj = self._chain()
646         obj.query.set_limits(high=1)
647         obj.query.clear_ordering(force_empty=True)
648         obj.query.add_ordering(*order_by)
649         return obj.get()
650 
651     def earliest(self, *fields):
652         return self._earliest(*fields)
653 
654     def latest(self, *fields):
655         return self.reverse()._earliest(*fields)
656 
657     def first(self):
658         """Return the first object of a query or None if no match is found."""
659         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
660             return obj
661 
662     def last(self):
663         """Return the last object of a query or None if no match is found."""
664         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
665             return obj
666 
667     def in_bulk(self, id_list=None, *, field_name='pk'):
668         """
669         Return a dictionary mapping each of the given IDs to the object with
670         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
671         """
672         assert not self.query.is_sliced, \
673             "Cannot use 'limit' or 'offset' with in_bulk"
674         opts = self.model._meta
675         unique_fields = [
676             constraint.fields[0]
677             for constraint in opts.total_unique_constraints
678             if len(constraint.fields) == 1
679         ]
680         if (
681             field_name != 'pk' and
682             not opts.get_field(field_name).unique and
683             field_name not in unique_fields and
684             not self.query.distinct_fields == (field_name,)
685         ):
686             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
687         if id_list is not None:
688             if not id_list:
689                 return {}
690             filter_key = '{}__in'.format(field_name)
691             batch_size = connections[self.db].features.max_query_params
692             id_list = tuple(id_list)
693             # If the database has a limit on the number of query parameters
694             # (e.g. SQLite), retrieve objects in batches if necessary.
695             if batch_size and batch_size < len(id_list):
696                 qs = ()
697                 for offset in range(0, len(id_list), batch_size):
698                     batch = id_list[offset:offset + batch_size]
699                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
700             else:
701                 qs = self.filter(**{filter_key: id_list}).order_by()
702         else:
703             qs = self._chain()
704         return {getattr(obj, field_name): obj for obj in qs}
705 
706     def delete(self):
707         """Delete the records in the current QuerySet."""
708         self._not_support_combined_queries('delete')
709         assert not self.query.is_sliced, \
710             "Cannot use 'limit' or 'offset' with delete."
711 
712         if self._fields is not None:
713             raise TypeError("Cannot call delete() after .values() or .values_list()")
714 
715         del_query = self._chain()
716 
717         # The delete is actually 2 queries - one to find related objects,
718         # and one to delete. Make sure that the discovery of related
719         # objects is performed on the same database as the deletion.
720         del_query._for_write = True
721 
722         # Disable non-supported fields.
723         del_query.query.select_for_update = False
724         del_query.query.select_related = False
725         del_query.query.clear_ordering(force_empty=True)
726 
727         collector = Collector(using=del_query.db)
728         collector.collect(del_query)
729         deleted, _rows_count = collector.delete()
730 
731         # Clear the result cache, in case this QuerySet gets reused.
732         self._result_cache = None
733         return deleted, _rows_count
734 
735     delete.alters_data = True
736     delete.queryset_only = True
737 
738     def _raw_delete(self, using):
739         """
740         Delete objects found from the given queryset in single direct SQL
741         query. No signals are sent and there is no protection for cascades.
742         """
743         query = self.query.clone()
744         query.__class__ = sql.DeleteQuery
745         cursor = query.get_compiler(using).execute_sql(CURSOR)
746         if cursor:
747             with cursor:
748                 return cursor.rowcount
749         return 0
750     _raw_delete.alters_data = True
751 
752     def update(self, **kwargs):
753         """
754         Update all elements in the current QuerySet, setting all the given
755         fields to the appropriate values.
756         """
757         self._not_support_combined_queries('update')
758         assert not self.query.is_sliced, \
759             "Cannot update a query once a slice has been taken."
760         self._for_write = True
761         query = self.query.chain(sql.UpdateQuery)
762         query.add_update_values(kwargs)
763         # Clear any annotations so that they won't be present in subqueries.
764         query.annotations = {}
765         with transaction.mark_for_rollback_on_error(using=self.db):
766             rows = query.get_compiler(self.db).execute_sql(CURSOR)
767         self._result_cache = None
768         return rows
769     update.alters_data = True
770 
771     def _update(self, values):
772         """
773         A version of update() that accepts field objects instead of field names.
774         Used primarily for model saving and not intended for use by general
775         code (it requires too much poking around at model internals to be
776         useful at that level).
777         """
778         assert not self.query.is_sliced, \
779             "Cannot update a query once a slice has been taken."
780         query = self.query.chain(sql.UpdateQuery)
781         query.add_update_fields(values)
782         # Clear any annotations so that they won't be present in subqueries.
783         query.annotations = {}
784         self._result_cache = None
785         return query.get_compiler(self.db).execute_sql(CURSOR)
786     _update.alters_data = True
787     _update.queryset_only = False
788 
789     def exists(self):
790         if self._result_cache is None:
791             return self.query.has_results(using=self.db)
792         return bool(self._result_cache)
793 
794     def _prefetch_related_objects(self):
795         # This method can only be called once the result cache has been filled.
796         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
797         self._prefetch_done = True
798 
799     def explain(self, *, format=None, **options):
800         return self.query.explain(using=self.db, format=format, **options)
801 
802     ##################################################
803     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
804     ##################################################
805 
806     def raw(self, raw_query, params=None, translations=None, using=None):
807         if using is None:
808             using = self.db
809         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
810         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
811         return qs
812 
813     def _values(self, *fields, **expressions):
814         clone = self._chain()
815         if expressions:
816             clone = clone.annotate(**expressions)
817         clone._fields = fields
818         clone.query.set_values(fields)
819         return clone
820 
821     def values(self, *fields, **expressions):
822         fields += tuple(expressions)
823         clone = self._values(*fields, **expressions)
824         clone._iterable_class = ValuesIterable
825         return clone
826 
827     def values_list(self, *fields, flat=False, named=False):
828         if flat and named:
829             raise TypeError("'flat' and 'named' can't be used together.")
830         if flat and len(fields) > 1:
831             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
832 
833         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
834         _fields = []
835         expressions = {}
836         counter = 1
837         for field in fields:
838             if hasattr(field, 'resolve_expression'):
839                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
840                 while True:
841                     field_id = field_id_prefix + str(counter)
842                     counter += 1
843                     if field_id not in field_names:
844                         break
845                 expressions[field_id] = field
846                 _fields.append(field_id)
847             else:
848                 _fields.append(field)
849 
850         clone = self._values(*_fields, **expressions)
851         clone._iterable_class = (
852             NamedValuesListIterable if named
853             else FlatValuesListIterable if flat
854             else ValuesListIterable
855         )
856         return clone
857 
858     def dates(self, field_name, kind, order='ASC'):
859         """
860         Return a list of date objects representing all available dates for
861         the given field_name, scoped to 'kind'.
862         """
863         assert kind in ('year', 'month', 'week', 'day'), \
864             "'kind' must be one of 'year', 'month', 'week', or 'day'."
865         assert order in ('ASC', 'DESC'), \
866             "'order' must be either 'ASC' or 'DESC'."
867         return self.annotate(
868             datefield=Trunc(field_name, kind, output_field=DateField()),
869             plain_field=F(field_name)
870         ).values_list(
871             'datefield', flat=True
872         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
873 
874     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
875         """
876         Return a list of datetime objects representing all available
877         datetimes for the given field_name, scoped to 'kind'.
878         """
879         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
880             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
881         assert order in ('ASC', 'DESC'), \
882             "'order' must be either 'ASC' or 'DESC'."
883         if settings.USE_TZ:
884             if tzinfo is None:
885                 tzinfo = timezone.get_current_timezone()
886         else:
887             tzinfo = None
888         return self.annotate(
889             datetimefield=Trunc(
890                 field_name,
891                 kind,
892                 output_field=DateTimeField(),
893                 tzinfo=tzinfo,
894                 is_dst=is_dst,
895             ),
896             plain_field=F(field_name)
897         ).values_list(
898             'datetimefield', flat=True
899         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
900 
901     def none(self):
902         """Return an empty QuerySet."""
903         clone = self._chain()
904         clone.query.set_empty()
905         return clone
906 
907     ##################################################################
908     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
909     ##################################################################
910 
911     def all(self):
912         """
913         Return a new QuerySet that is a copy of the current one. This allows a
914         QuerySet to proxy for a model manager in some cases.
915         """
916         return self._chain()
917 
918     def filter(self, *args, **kwargs):
919         """
920         Return a new QuerySet instance with the args ANDed to the existing
921         set.
922         """
923         self._not_support_combined_queries('filter')
924         return self._filter_or_exclude(False, args, kwargs)
925 
926     def exclude(self, *args, **kwargs):
927         """
928         Return a new QuerySet instance with NOT (args) ANDed to the existing
929         set.
930         """
931         self._not_support_combined_queries('exclude')
932         return self._filter_or_exclude(True, args, kwargs)
933 
934     def _filter_or_exclude(self, negate, args, kwargs):
935         if args or kwargs:
936             assert not self.query.is_sliced, \
937                 "Cannot filter a query once a slice has been taken."
938 
939         clone = self._chain()
940         if self._defer_next_filter:
941             self._defer_next_filter = False
942             clone._deferred_filter = negate, args, kwargs
943         else:
944             clone._filter_or_exclude_inplace(negate, args, kwargs)
945         return clone
946 
947     def _filter_or_exclude_inplace(self, negate, args, kwargs):
948         if negate:
949             self._query.add_q(~Q(*args, **kwargs))
950         else:
951             self._query.add_q(Q(*args, **kwargs))
952 
953     def complex_filter(self, filter_obj):
954         """
955         Return a new QuerySet instance with filter_obj added to the filters.
956 
957         filter_obj can be a Q object or a dictionary of keyword lookup
958         arguments.
959 
960         This exists to support framework features such as 'limit_choices_to',
961         and usually it will be more natural to use other methods.
962         """
963         if isinstance(filter_obj, Q):
964             clone = self._chain()
965             clone.query.add_q(filter_obj)
966             return clone
967         else:
968             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
969 
970     def _combinator_query(self, combinator, *other_qs, all=False):
971         # Clone the query to inherit the select list and everything
972         clone = self._chain()
973         # Clear limits and ordering so they can be reapplied
974         clone.query.clear_ordering(True)
975         clone.query.clear_limits()
976         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
977         clone.query.combinator = combinator
978         clone.query.combinator_all = all
979         return clone
980 
981     def union(self, *other_qs, all=False):
982         # If the query is an EmptyQuerySet, combine all nonempty querysets.
983         if isinstance(self, EmptyQuerySet):
984             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
985             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
986         return self._combinator_query('union', *other_qs, all=all)
987 
988     def intersection(self, *other_qs):
989         # If any query is an EmptyQuerySet, return it.
990         if isinstance(self, EmptyQuerySet):
991             return self
992         for other in other_qs:
993             if isinstance(other, EmptyQuerySet):
994                 return other
995         return self._combinator_query('intersection', *other_qs)
996 
997     def difference(self, *other_qs):
998         # If the query is an EmptyQuerySet, return it.
999         if isinstance(self, EmptyQuerySet):
1000             return self
1001         return self._combinator_query('difference', *other_qs)
1002 
1003     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1004         """
1005         Return a new QuerySet instance that will select objects with a
1006         FOR UPDATE lock.
1007         """
1008         if nowait and skip_locked:
1009             raise ValueError('The nowait option cannot be used with skip_locked.')
1010         obj = self._chain()
1011         obj._for_write = True
1012         obj.query.select_for_update = True
1013         obj.query.select_for_update_nowait = nowait
1014         obj.query.select_for_update_skip_locked = skip_locked
1015         obj.query.select_for_update_of = of
1016         obj.query.select_for_no_key_update = no_key
1017         return obj
1018 
1019     def select_related(self, *fields):
1020         """
1021         Return a new QuerySet instance that will select related objects.
1022 
1023         If fields are specified, they must be ForeignKey fields and only those
1024         related objects are included in the selection.
1025 
1026         If select_related(None) is called, clear the list.
1027         """
1028         self._not_support_combined_queries('select_related')
1029         if self._fields is not None:
1030             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1031 
1032         obj = self._chain()
1033         if fields == (None,):
1034             obj.query.select_related = False
1035         elif fields:
1036             obj.query.add_select_related(fields)
1037         else:
1038             obj.query.select_related = True
1039         return obj
1040 
1041     def prefetch_related(self, *lookups):
1042         """
1043         Return a new QuerySet instance that will prefetch the specified
1044         Many-To-One and Many-To-Many related objects when the QuerySet is
1045         evaluated.
1046 
1047         When prefetch_related() is called more than once, append to the list of
1048         prefetch lookups. If prefetch_related(None) is called, clear the list.
1049         """
1050         self._not_support_combined_queries('prefetch_related')
1051         clone = self._chain()
1052         if lookups == (None,):
1053             clone._prefetch_related_lookups = ()
1054         else:
1055             for lookup in lookups:
1056                 if isinstance(lookup, Prefetch):
1057                     lookup = lookup.prefetch_to
1058                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1059                 if lookup in self.query._filtered_relations:
1060                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1061             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1062         return clone
1063 
1064     def annotate(self, *args, **kwargs):
1065         """
1066         Return a query set in which the returned objects have been annotated
1067         with extra data or aggregations.
1068         """
1069         self._not_support_combined_queries('annotate')
1070         return self._annotate(args, kwargs, select=True)
1071 
1072     def alias(self, *args, **kwargs):
1073         """
1074         Return a query set with added aliases for extra data or aggregations.
1075         """
1076         self._not_support_combined_queries('alias')
1077         return self._annotate(args, kwargs, select=False)
1078 
1079     def _annotate(self, args, kwargs, select=True):
1080         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1081         annotations = {}
1082         for arg in args:
1083             # The default_alias property may raise a TypeError.
1084             try:
1085                 if arg.default_alias in kwargs:
1086                     raise ValueError("The named annotation '%s' conflicts with the "
1087                                      "default name for another annotation."
1088                                      % arg.default_alias)
1089             except TypeError:
1090                 raise TypeError("Complex annotations require an alias")
1091             annotations[arg.default_alias] = arg
1092         annotations.update(kwargs)
1093 
1094         clone = self._chain()
1095         names = self._fields
1096         if names is None:
1097             names = set(chain.from_iterable(
1098                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1099                 for field in self.model._meta.get_fields()
1100             ))
1101 
1102         for alias, annotation in annotations.items():
1103             if alias in names:
1104                 raise ValueError("The annotation '%s' conflicts with a field on "
1105                                  "the model." % alias)
1106             if isinstance(annotation, FilteredRelation):
1107                 clone.query.add_filtered_relation(annotation, alias)
1108             else:
1109                 clone.query.add_annotation(
1110                     annotation, alias, is_summary=False, select=select,
1111                 )
1112         for alias, annotation in clone.query.annotations.items():
1113             if alias in annotations and annotation.contains_aggregate:
1114                 if clone._fields is None:
1115                     clone.query.group_by = True
1116                 else:
1117                     clone.query.set_group_by()
1118                 break
1119 
1120         return clone
1121 
1122     def order_by(self, *field_names):
1123         """Return a new QuerySet instance with the ordering changed."""
1124         assert not self.query.is_sliced, \
1125             "Cannot reorder a query once a slice has been taken."
1126         obj = self._chain()
1127         obj.query.clear_ordering(force_empty=False)
1128         obj.query.add_ordering(*field_names)
1129         return obj
1130 
1131     def distinct(self, *field_names):
1132         """
1133         Return a new QuerySet instance that will select only distinct results.
1134         """
1135         self._not_support_combined_queries('distinct')
1136         assert not self.query.is_sliced, \
1137             "Cannot create distinct fields once a slice has been taken."
1138         obj = self._chain()
1139         obj.query.add_distinct_fields(*field_names)
1140         return obj
1141 
1142     def extra(self, select=None, where=None, params=None, tables=None,
1143               order_by=None, select_params=None):
1144         """Add extra SQL fragments to the query."""
1145         self._not_support_combined_queries('extra')
1146         assert not self.query.is_sliced, \
1147             "Cannot change a query once a slice has been taken"
1148         clone = self._chain()
1149         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1150         return clone
1151 
1152     def reverse(self):
1153         """Reverse the ordering of the QuerySet."""
1154         if self.query.is_sliced:
1155             raise TypeError('Cannot reverse a query once a slice has been taken.')
1156         clone = self._chain()
1157         clone.query.standard_ordering = not clone.query.standard_ordering
1158         return clone
1159 
1160     def defer(self, *fields):
1161         """
1162         Defer the loading of data for certain fields until they are accessed.
1163         Add the set of deferred fields to any existing set of deferred fields.
1164         The only exception to this is if None is passed in as the only
1165         parameter, in which case removal all deferrals.
1166         """
1167         self._not_support_combined_queries('defer')
1168         if self._fields is not None:
1169             raise TypeError("Cannot call defer() after .values() or .values_list()")
1170         clone = self._chain()
1171         if fields == (None,):
1172             clone.query.clear_deferred_loading()
1173         else:
1174             clone.query.add_deferred_loading(fields)
1175         return clone
1176 
1177     def only(self, *fields):
1178         """
1179         Essentially, the opposite of defer(). Only the fields passed into this
1180         method and that are not already specified as deferred are loaded
1181         immediately when the queryset is evaluated.
1182         """
1183         self._not_support_combined_queries('only')
1184         if self._fields is not None:
1185             raise TypeError("Cannot call only() after .values() or .values_list()")
1186         if fields == (None,):
1187             # Can only pass None to defer(), not only(), as the rest option.
1188             # That won't stop people trying to do this, so let's be explicit.
1189             raise TypeError("Cannot pass None as an argument to only().")
1190         for field in fields:
1191             field = field.split(LOOKUP_SEP, 1)[0]
1192             if field in self.query._filtered_relations:
1193                 raise ValueError('only() is not supported with FilteredRelation.')
1194         clone = self._chain()
1195         clone.query.add_immediate_loading(fields)
1196         return clone
1197 
1198     def using(self, alias):
1199         """Select which database this QuerySet should execute against."""
1200         clone = self._chain()
1201         clone._db = alias
1202         return clone
1203 
1204     ###################################
1205     # PUBLIC INTROSPECTION ATTRIBUTES #
1206     ###################################
1207 
1208     @property
1209     def ordered(self):
1210         """
1211         Return True if the QuerySet is ordered -- i.e. has an order_by()
1212         clause or a default ordering on the model (or is empty).
1213         """
1214         if isinstance(self, EmptyQuerySet):
1215             return True
1216         if self.query.extra_order_by or self.query.order_by:
1217             return True
1218         elif (
1219             self.query.default_ordering and
1220             self.query.get_meta().ordering and
1221             # A default ordering doesn't affect GROUP BY queries.
1222             not self.query.group_by
1223         ):
1224             return True
1225         else:
1226             return False
1227 
1228     @property
1229     def db(self):
1230         """Return the database used if this query is executed now."""
1231         if self._for_write:
1232             return self._db or router.db_for_write(self.model, **self._hints)
1233         return self._db or router.db_for_read(self.model, **self._hints)
1234 
1235     ###################
1236     # PRIVATE METHODS #
1237     ###################
1238 
1239     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1240         """
1241         Insert a new record for the given model. This provides an interface to
1242         the InsertQuery class and is how Model.save() is implemented.
1243         """
1244         self._for_write = True
1245         if using is None:
1246             using = self.db
1247         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1248         query.insert_values(fields, objs, raw=raw)
1249         return query.get_compiler(using=using).execute_sql(returning_fields)
1250     _insert.alters_data = True
1251     _insert.queryset_only = False
1252 
1253     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1254         """
1255         Helper method for bulk_create() to insert objs one batch at a time.
1256         """
1257         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1258             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1259         ops = connections[self.db].ops
1260         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1261         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1262         inserted_rows = []
1263         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1264         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1265             if bulk_return and not ignore_conflicts:
1266                 inserted_rows.extend(self._insert(
1267                     item, fields=fields, using=self.db,
1268                     returning_fields=self.model._meta.db_returning_fields,
1269                     ignore_conflicts=ignore_conflicts,
1270                 ))
1271             else:
1272                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1273         return inserted_rows
1274 
1275     def _chain(self, **kwargs):
1276         """
1277         Return a copy of the current QuerySet that's ready for another
1278         operation.
1279         """
1280         obj = self._clone()
1281         if obj._sticky_filter:
1282             obj.query.filter_is_sticky = True
1283             obj._sticky_filter = False
1284         obj.__dict__.update(kwargs)
1285         return obj
1286 
1287     def _clone(self):
1288         """
1289         Return a copy of the current QuerySet. A lightweight alternative
1290         to deepcopy().
1291         """
1292         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1293         c._sticky_filter = self._sticky_filter
1294         c._for_write = self._for_write
1295         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1296         c._known_related_objects = self._known_related_objects
1297         c._iterable_class = self._iterable_class
1298         c._fields = self._fields
1299         return c
1300 
1301     def _fetch_all(self):
1302         if self._result_cache is None:
1303             self._result_cache = list(self._iterable_class(self))
1304         if self._prefetch_related_lookups and not self._prefetch_done:
1305             self._prefetch_related_objects()
1306 
1307     def _next_is_sticky(self):
1308         """
1309         Indicate that the next filter call and the one following that should
1310         be treated as a single filter. This is only important when it comes to
1311         determining when to reuse tables for many-to-many filters. Required so
1312         that we can filter naturally on the results of related managers.
1313 
1314         This doesn't return a clone of the current QuerySet (it returns
1315         "self"). The method is only used internally and should be immediately
1316         followed by a filter() that does create a clone.
1317         """
1318         self._sticky_filter = True
1319         return self
1320 
1321     def _merge_sanity_check(self, other):
1322         """Check that two QuerySet classes may be merged."""
1323         if self._fields is not None and (
1324                 set(self.query.values_select) != set(other.query.values_select) or
1325                 set(self.query.extra_select) != set(other.query.extra_select) or
1326                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1327             raise TypeError(
1328                 "Merging '%s' classes must involve the same values in each case."
1329                 % self.__class__.__name__
1330             )
1331 
1332     def _merge_known_related_objects(self, other):
1333         """
1334         Keep track of all known related objects from either QuerySet instance.
1335         """
1336         for field, objects in other._known_related_objects.items():
1337             self._known_related_objects.setdefault(field, {}).update(objects)
1338 
1339     def resolve_expression(self, *args, **kwargs):
1340         if self._fields and len(self._fields) > 1:
1341             # values() queryset can only be used as nested queries
1342             # if they are set up to select only a single field.
1343             raise TypeError('Cannot use multi-field values as a filter value.')
1344         query = self.query.resolve_expression(*args, **kwargs)
1345         query._db = self._db
1346         return query
1347     resolve_expression.queryset_only = True
1348 
1349     def _add_hints(self, **hints):
1350         """
1351         Update hinting information for use by routers. Add new key/values or
1352         overwrite existing key/values.
1353         """
1354         self._hints.update(hints)
1355 
1356     def _has_filters(self):
1357         """
1358         Check if this QuerySet has any filtering going on. This isn't
1359         equivalent with checking if all objects are present in results, for
1360         example, qs[1:]._has_filters() -> False.
1361         """
1362         return self.query.has_filters()
1363 
1364     @staticmethod
1365     def _validate_values_are_expressions(values, method_name):
1366         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1367         if invalid_args:
1368             raise TypeError(
1369                 'QuerySet.%s() received non-expression(s): %s.' % (
1370                     method_name,
1371                     ', '.join(invalid_args),
1372                 )
1373             )
1374 
1375     def _not_support_combined_queries(self, operation_name):
1376         if self.query.combinator:
1377             raise NotSupportedError(
1378                 'Calling QuerySet.%s() after %s() is not supported.'
1379                 % (operation_name, self.query.combinator)
1380             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1064     def annotate(self, *args, **kwargs):
1065         """
1066         Return a query set in which the returned objects have been annotated
1067         with extra data or aggregations.
1068         """
1069         self._not_support_combined_queries('annotate')
1070         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>Although not directly implicated as faulty, ensuring the annotate method's output is correctly utilized by subsequent aggregate calls is essential. The annotate function should clearly define the scope and aliasing of annotated fields so that these can be understood and handled without ambiguity in subsequent operations, specifically aggregate. The method itself may not require changes, but its interaction with aggregate and the overall SQL construction logic should be verified to ensure annotations are appropriately factored into the aggregate's SQL generation logic. This includes validating the chaining of queryset methods where annotations serve as inputs to aggregates, leading to proper SQL command generation that considers the established aliases.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             if not query.annotations[alias].contains_aggregate:
390                 raise TypeError("%s is not an aggregate expression" % alias)
391         return query.get_aggregation(self.db, kwargs)
392 
393     def count(self):
394         """
395         Perform a SELECT COUNT() and return the number of records as an
396         integer.
397 
398         If the QuerySet is already fully cached, return the length of the
399         cached results set to avoid multiple SELECT COUNT(*) calls.
400         """
401         if self._result_cache is not None:
402             return len(self._result_cache)
403 
404         return self.query.get_count(using=self.db)
405 
406     def get(self, *args, **kwargs):
407         """
408         Perform the query and return a single object matching the given
409         keyword arguments.
410         """
411         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
412         if self.query.can_filter() and not self.query.distinct_fields:
413             clone = clone.order_by()
414         limit = None
415         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
416             limit = MAX_GET_RESULTS
417             clone.query.set_limits(high=limit)
418         num = len(clone)
419         if num == 1:
420             return clone._result_cache[0]
421         if not num:
422             raise self.model.DoesNotExist(
423                 "%s matching query does not exist." %
424                 self.model._meta.object_name
425             )
426         raise self.model.MultipleObjectsReturned(
427             'get() returned more than one %s -- it returned %s!' % (
428                 self.model._meta.object_name,
429                 num if not limit or num < limit else 'more than %s' % (limit - 1),
430             )
431         )
432 
433     def create(self, **kwargs):
434         """
435         Create a new object with the given kwargs, saving it to the database
436         and returning the created object.
437         """
438         obj = self.model(**kwargs)
439         self._for_write = True
440         obj.save(force_insert=True, using=self.db)
441         return obj
442 
443     def _populate_pk_values(self, objs):
444         for obj in objs:
445             if obj.pk is None:
446                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
447 
448     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
449         """
450         Insert each of the instances into the database. Do *not* call
451         save() on each of the instances, do not send any pre/post_save
452         signals, and do not set the primary key attribute if it is an
453         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
454         Multi-table models are not supported.
455         """
456         # When you bulk insert you don't get the primary keys back (if it's an
457         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
458         # you can't insert into the child tables which references this. There
459         # are two workarounds:
460         # 1) This could be implemented if you didn't have an autoincrement pk
461         # 2) You could do it by doing O(n) normal inserts into the parent
462         #    tables to get the primary keys back and then doing a single bulk
463         #    insert into the childmost table.
464         # We currently set the primary keys on the objects when using
465         # PostgreSQL via the RETURNING ID clause. It should be possible for
466         # Oracle as well, but the semantics for extracting the primary keys is
467         # trickier so it's not done yet.
468         assert batch_size is None or batch_size > 0
469         # Check that the parents share the same concrete model with the our
470         # model to detect the inheritance pattern ConcreteGrandParent ->
471         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
472         # would not identify that case as involving multiple tables.
473         for parent in self.model._meta.get_parent_list():
474             if parent._meta.concrete_model is not self.model._meta.concrete_model:
475                 raise ValueError("Can't bulk create a multi-table inherited model")
476         if not objs:
477             return objs
478         self._for_write = True
479         connection = connections[self.db]
480         opts = self.model._meta
481         fields = opts.concrete_fields
482         objs = list(objs)
483         self._populate_pk_values(objs)
484         with transaction.atomic(using=self.db, savepoint=False):
485             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
486             if objs_with_pk:
487                 returned_columns = self._batched_insert(
488                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
489                 )
490                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
491                     for result, field in zip(results, opts.db_returning_fields):
492                         if field != opts.pk:
493                             setattr(obj_with_pk, field.attname, result)
494                 for obj_with_pk in objs_with_pk:
495                     obj_with_pk._state.adding = False
496                     obj_with_pk._state.db = self.db
497             if objs_without_pk:
498                 fields = [f for f in fields if not isinstance(f, AutoField)]
499                 returned_columns = self._batched_insert(
500                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
501                 )
502                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
503                     assert len(returned_columns) == len(objs_without_pk)
504                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
505                     for result, field in zip(results, opts.db_returning_fields):
506                         setattr(obj_without_pk, field.attname, result)
507                     obj_without_pk._state.adding = False
508                     obj_without_pk._state.db = self.db
509 
510         return objs
511 
512     def bulk_update(self, objs, fields, batch_size=None):
513         """
514         Update the given fields in each of the given objects in the database.
515         """
516         if batch_size is not None and batch_size < 0:
517             raise ValueError('Batch size must be a positive integer.')
518         if not fields:
519             raise ValueError('Field names must be given to bulk_update().')
520         objs = tuple(objs)
521         if any(obj.pk is None for obj in objs):
522             raise ValueError('All bulk_update() objects must have a primary key set.')
523         fields = [self.model._meta.get_field(name) for name in fields]
524         if any(not f.concrete or f.many_to_many for f in fields):
525             raise ValueError('bulk_update() can only be used with concrete fields.')
526         if any(f.primary_key for f in fields):
527             raise ValueError('bulk_update() cannot be used with primary key fields.')
528         if not objs:
529             return
530         # PK is used twice in the resulting update query, once in the filter
531         # and once in the WHEN. Each field will also have one CAST.
532         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
533         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
534         requires_casting = connections[self.db].features.requires_casted_case_in_updates
535         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
536         updates = []
537         for batch_objs in batches:
538             update_kwargs = {}
539             for field in fields:
540                 when_statements = []
541                 for obj in batch_objs:
542                     attr = getattr(obj, field.attname)
543                     if not isinstance(attr, Expression):
544                         attr = Value(attr, output_field=field)
545                     when_statements.append(When(pk=obj.pk, then=attr))
546                 case_statement = Case(*when_statements, output_field=field)
547                 if requires_casting:
548                     case_statement = Cast(case_statement, output_field=field)
549                 update_kwargs[field.attname] = case_statement
550             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
551         with transaction.atomic(using=self.db, savepoint=False):
552             for pks, update_kwargs in updates:
553                 self.filter(pk__in=pks).update(**update_kwargs)
554     bulk_update.alters_data = True
555 
556     def get_or_create(self, defaults=None, **kwargs):
557         """
558         Look up an object with the given kwargs, creating one if necessary.
559         Return a tuple of (object, created), where created is a boolean
560         specifying whether an object was created.
561         """
562         # The get() needs to be targeted at the write database in order
563         # to avoid potential transaction consistency problems.
564         self._for_write = True
565         try:
566             return self.get(**kwargs), False
567         except self.model.DoesNotExist:
568             params = self._extract_model_params(defaults, **kwargs)
569             # Try to create an object using passed params.
570             try:
571                 with transaction.atomic(using=self.db):
572                     params = dict(resolve_callables(params))
573                     return self.create(**params), True
574             except IntegrityError:
575                 try:
576                     return self.get(**kwargs), False
577                 except self.model.DoesNotExist:
578                     pass
579                 raise
580 
581     def update_or_create(self, defaults=None, **kwargs):
582         """
583         Look up an object with the given kwargs, updating one with defaults
584         if it exists, otherwise create a new one.
585         Return a tuple (object, created), where created is a boolean
586         specifying whether an object was created.
587         """
588         defaults = defaults or {}
589         self._for_write = True
590         with transaction.atomic(using=self.db):
591             # Lock the row so that a concurrent update is blocked until
592             # update_or_create() has performed its save.
593             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
594             if created:
595                 return obj, created
596             for k, v in resolve_callables(defaults):
597                 setattr(obj, k, v)
598             obj.save(using=self.db)
599         return obj, False
600 
601     def _extract_model_params(self, defaults, **kwargs):
602         """
603         Prepare `params` for creating a model instance based on the given
604         kwargs; for use by get_or_create().
605         """
606         defaults = defaults or {}
607         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
608         params.update(defaults)
609         property_names = self.model._meta._property_names
610         invalid_params = []
611         for param in params:
612             try:
613                 self.model._meta.get_field(param)
614             except exceptions.FieldDoesNotExist:
615                 # It's okay to use a model's property if it has a setter.
616                 if not (param in property_names and getattr(self.model, param).fset):
617                     invalid_params.append(param)
618         if invalid_params:
619             raise exceptions.FieldError(
620                 "Invalid field name(s) for model %s: '%s'." % (
621                     self.model._meta.object_name,
622                     "', '".join(sorted(invalid_params)),
623                 ))
624         return params
625 
626     def _earliest(self, *fields):
627         """
628         Return the earliest object according to fields (if given) or by the
629         model's Meta.get_latest_by.
630         """
631         if fields:
632             order_by = fields
633         else:
634             order_by = getattr(self.model._meta, 'get_latest_by')
635             if order_by and not isinstance(order_by, (tuple, list)):
636                 order_by = (order_by,)
637         if order_by is None:
638             raise ValueError(
639                 "earliest() and latest() require either fields as positional "
640                 "arguments or 'get_latest_by' in the model's Meta."
641             )
642 
643         assert not self.query.is_sliced, \
644             "Cannot change a query once a slice has been taken."
645         obj = self._chain()
646         obj.query.set_limits(high=1)
647         obj.query.clear_ordering(force_empty=True)
648         obj.query.add_ordering(*order_by)
649         return obj.get()
650 
651     def earliest(self, *fields):
652         return self._earliest(*fields)
653 
654     def latest(self, *fields):
655         return self.reverse()._earliest(*fields)
656 
657     def first(self):
658         """Return the first object of a query or None if no match is found."""
659         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
660             return obj
661 
662     def last(self):
663         """Return the last object of a query or None if no match is found."""
664         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
665             return obj
666 
667     def in_bulk(self, id_list=None, *, field_name='pk'):
668         """
669         Return a dictionary mapping each of the given IDs to the object with
670         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
671         """
672         assert not self.query.is_sliced, \
673             "Cannot use 'limit' or 'offset' with in_bulk"
674         opts = self.model._meta
675         unique_fields = [
676             constraint.fields[0]
677             for constraint in opts.total_unique_constraints
678             if len(constraint.fields) == 1
679         ]
680         if (
681             field_name != 'pk' and
682             not opts.get_field(field_name).unique and
683             field_name not in unique_fields and
684             not self.query.distinct_fields == (field_name,)
685         ):
686             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
687         if id_list is not None:
688             if not id_list:
689                 return {}
690             filter_key = '{}__in'.format(field_name)
691             batch_size = connections[self.db].features.max_query_params
692             id_list = tuple(id_list)
693             # If the database has a limit on the number of query parameters
694             # (e.g. SQLite), retrieve objects in batches if necessary.
695             if batch_size and batch_size < len(id_list):
696                 qs = ()
697                 for offset in range(0, len(id_list), batch_size):
698                     batch = id_list[offset:offset + batch_size]
699                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
700             else:
701                 qs = self.filter(**{filter_key: id_list}).order_by()
702         else:
703             qs = self._chain()
704         return {getattr(obj, field_name): obj for obj in qs}
705 
706     def delete(self):
707         """Delete the records in the current QuerySet."""
708         self._not_support_combined_queries('delete')
709         assert not self.query.is_sliced, \
710             "Cannot use 'limit' or 'offset' with delete."
711 
712         if self._fields is not None:
713             raise TypeError("Cannot call delete() after .values() or .values_list()")
714 
715         del_query = self._chain()
716 
717         # The delete is actually 2 queries - one to find related objects,
718         # and one to delete. Make sure that the discovery of related
719         # objects is performed on the same database as the deletion.
720         del_query._for_write = True
721 
722         # Disable non-supported fields.
723         del_query.query.select_for_update = False
724         del_query.query.select_related = False
725         del_query.query.clear_ordering(force_empty=True)
726 
727         collector = Collector(using=del_query.db)
728         collector.collect(del_query)
729         deleted, _rows_count = collector.delete()
730 
731         # Clear the result cache, in case this QuerySet gets reused.
732         self._result_cache = None
733         return deleted, _rows_count
734 
735     delete.alters_data = True
736     delete.queryset_only = True
737 
738     def _raw_delete(self, using):
739         """
740         Delete objects found from the given queryset in single direct SQL
741         query. No signals are sent and there is no protection for cascades.
742         """
743         query = self.query.clone()
744         query.__class__ = sql.DeleteQuery
745         cursor = query.get_compiler(using).execute_sql(CURSOR)
746         if cursor:
747             with cursor:
748                 return cursor.rowcount
749         return 0
750     _raw_delete.alters_data = True
751 
752     def update(self, **kwargs):
753         """
754         Update all elements in the current QuerySet, setting all the given
755         fields to the appropriate values.
756         """
757         self._not_support_combined_queries('update')
758         assert not self.query.is_sliced, \
759             "Cannot update a query once a slice has been taken."
760         self._for_write = True
761         query = self.query.chain(sql.UpdateQuery)
762         query.add_update_values(kwargs)
763         # Clear any annotations so that they won't be present in subqueries.
764         query.annotations = {}
765         with transaction.mark_for_rollback_on_error(using=self.db):
766             rows = query.get_compiler(self.db).execute_sql(CURSOR)
767         self._result_cache = None
768         return rows
769     update.alters_data = True
770 
771     def _update(self, values):
772         """
773         A version of update() that accepts field objects instead of field names.
774         Used primarily for model saving and not intended for use by general
775         code (it requires too much poking around at model internals to be
776         useful at that level).
777         """
778         assert not self.query.is_sliced, \
779             "Cannot update a query once a slice has been taken."
780         query = self.query.chain(sql.UpdateQuery)
781         query.add_update_fields(values)
782         # Clear any annotations so that they won't be present in subqueries.
783         query.annotations = {}
784         self._result_cache = None
785         return query.get_compiler(self.db).execute_sql(CURSOR)
786     _update.alters_data = True
787     _update.queryset_only = False
788 
789     def exists(self):
790         if self._result_cache is None:
791             return self.query.has_results(using=self.db)
792         return bool(self._result_cache)
793 
794     def _prefetch_related_objects(self):
795         # This method can only be called once the result cache has been filled.
796         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
797         self._prefetch_done = True
798 
799     def explain(self, *, format=None, **options):
800         return self.query.explain(using=self.db, format=format, **options)
801 
802     ##################################################
803     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
804     ##################################################
805 
806     def raw(self, raw_query, params=None, translations=None, using=None):
807         if using is None:
808             using = self.db
809         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
810         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
811         return qs
812 
813     def _values(self, *fields, **expressions):
814         clone = self._chain()
815         if expressions:
816             clone = clone.annotate(**expressions)
817         clone._fields = fields
818         clone.query.set_values(fields)
819         return clone
820 
821     def values(self, *fields, **expressions):
822         fields += tuple(expressions)
823         clone = self._values(*fields, **expressions)
824         clone._iterable_class = ValuesIterable
825         return clone
826 
827     def values_list(self, *fields, flat=False, named=False):
828         if flat and named:
829             raise TypeError("'flat' and 'named' can't be used together.")
830         if flat and len(fields) > 1:
831             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
832 
833         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
834         _fields = []
835         expressions = {}
836         counter = 1
837         for field in fields:
838             if hasattr(field, 'resolve_expression'):
839                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
840                 while True:
841                     field_id = field_id_prefix + str(counter)
842                     counter += 1
843                     if field_id not in field_names:
844                         break
845                 expressions[field_id] = field
846                 _fields.append(field_id)
847             else:
848                 _fields.append(field)
849 
850         clone = self._values(*_fields, **expressions)
851         clone._iterable_class = (
852             NamedValuesListIterable if named
853             else FlatValuesListIterable if flat
854             else ValuesListIterable
855         )
856         return clone
857 
858     def dates(self, field_name, kind, order='ASC'):
859         """
860         Return a list of date objects representing all available dates for
861         the given field_name, scoped to 'kind'.
862         """
863         assert kind in ('year', 'month', 'week', 'day'), \
864             "'kind' must be one of 'year', 'month', 'week', or 'day'."
865         assert order in ('ASC', 'DESC'), \
866             "'order' must be either 'ASC' or 'DESC'."
867         return self.annotate(
868             datefield=Trunc(field_name, kind, output_field=DateField()),
869             plain_field=F(field_name)
870         ).values_list(
871             'datefield', flat=True
872         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
873 
874     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
875         """
876         Return a list of datetime objects representing all available
877         datetimes for the given field_name, scoped to 'kind'.
878         """
879         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
880             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
881         assert order in ('ASC', 'DESC'), \
882             "'order' must be either 'ASC' or 'DESC'."
883         if settings.USE_TZ:
884             if tzinfo is None:
885                 tzinfo = timezone.get_current_timezone()
886         else:
887             tzinfo = None
888         return self.annotate(
889             datetimefield=Trunc(
890                 field_name,
891                 kind,
892                 output_field=DateTimeField(),
893                 tzinfo=tzinfo,
894                 is_dst=is_dst,
895             ),
896             plain_field=F(field_name)
897         ).values_list(
898             'datetimefield', flat=True
899         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
900 
901     def none(self):
902         """Return an empty QuerySet."""
903         clone = self._chain()
904         clone.query.set_empty()
905         return clone
906 
907     ##################################################################
908     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
909     ##################################################################
910 
911     def all(self):
912         """
913         Return a new QuerySet that is a copy of the current one. This allows a
914         QuerySet to proxy for a model manager in some cases.
915         """
916         return self._chain()
917 
918     def filter(self, *args, **kwargs):
919         """
920         Return a new QuerySet instance with the args ANDed to the existing
921         set.
922         """
923         self._not_support_combined_queries('filter')
924         return self._filter_or_exclude(False, args, kwargs)
925 
926     def exclude(self, *args, **kwargs):
927         """
928         Return a new QuerySet instance with NOT (args) ANDed to the existing
929         set.
930         """
931         self._not_support_combined_queries('exclude')
932         return self._filter_or_exclude(True, args, kwargs)
933 
934     def _filter_or_exclude(self, negate, args, kwargs):
935         if args or kwargs:
936             assert not self.query.is_sliced, \
937                 "Cannot filter a query once a slice has been taken."
938 
939         clone = self._chain()
940         if self._defer_next_filter:
941             self._defer_next_filter = False
942             clone._deferred_filter = negate, args, kwargs
943         else:
944             clone._filter_or_exclude_inplace(negate, args, kwargs)
945         return clone
946 
947     def _filter_or_exclude_inplace(self, negate, args, kwargs):
948         if negate:
949             self._query.add_q(~Q(*args, **kwargs))
950         else:
951             self._query.add_q(Q(*args, **kwargs))
952 
953     def complex_filter(self, filter_obj):
954         """
955         Return a new QuerySet instance with filter_obj added to the filters.
956 
957         filter_obj can be a Q object or a dictionary of keyword lookup
958         arguments.
959 
960         This exists to support framework features such as 'limit_choices_to',
961         and usually it will be more natural to use other methods.
962         """
963         if isinstance(filter_obj, Q):
964             clone = self._chain()
965             clone.query.add_q(filter_obj)
966             return clone
967         else:
968             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
969 
970     def _combinator_query(self, combinator, *other_qs, all=False):
971         # Clone the query to inherit the select list and everything
972         clone = self._chain()
973         # Clear limits and ordering so they can be reapplied
974         clone.query.clear_ordering(True)
975         clone.query.clear_limits()
976         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
977         clone.query.combinator = combinator
978         clone.query.combinator_all = all
979         return clone
980 
981     def union(self, *other_qs, all=False):
982         # If the query is an EmptyQuerySet, combine all nonempty querysets.
983         if isinstance(self, EmptyQuerySet):
984             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
985             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
986         return self._combinator_query('union', *other_qs, all=all)
987 
988     def intersection(self, *other_qs):
989         # If any query is an EmptyQuerySet, return it.
990         if isinstance(self, EmptyQuerySet):
991             return self
992         for other in other_qs:
993             if isinstance(other, EmptyQuerySet):
994                 return other
995         return self._combinator_query('intersection', *other_qs)
996 
997     def difference(self, *other_qs):
998         # If the query is an EmptyQuerySet, return it.
999         if isinstance(self, EmptyQuerySet):
1000             return self
1001         return self._combinator_query('difference', *other_qs)
1002 
1003     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1004         """
1005         Return a new QuerySet instance that will select objects with a
1006         FOR UPDATE lock.
1007         """
1008         if nowait and skip_locked:
1009             raise ValueError('The nowait option cannot be used with skip_locked.')
1010         obj = self._chain()
1011         obj._for_write = True
1012         obj.query.select_for_update = True
1013         obj.query.select_for_update_nowait = nowait
1014         obj.query.select_for_update_skip_locked = skip_locked
1015         obj.query.select_for_update_of = of
1016         obj.query.select_for_no_key_update = no_key
1017         return obj
1018 
1019     def select_related(self, *fields):
1020         """
1021         Return a new QuerySet instance that will select related objects.
1022 
1023         If fields are specified, they must be ForeignKey fields and only those
1024         related objects are included in the selection.
1025 
1026         If select_related(None) is called, clear the list.
1027         """
1028         self._not_support_combined_queries('select_related')
1029         if self._fields is not None:
1030             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1031 
1032         obj = self._chain()
1033         if fields == (None,):
1034             obj.query.select_related = False
1035         elif fields:
1036             obj.query.add_select_related(fields)
1037         else:
1038             obj.query.select_related = True
1039         return obj
1040 
1041     def prefetch_related(self, *lookups):
1042         """
1043         Return a new QuerySet instance that will prefetch the specified
1044         Many-To-One and Many-To-Many related objects when the QuerySet is
1045         evaluated.
1046 
1047         When prefetch_related() is called more than once, append to the list of
1048         prefetch lookups. If prefetch_related(None) is called, clear the list.
1049         """
1050         self._not_support_combined_queries('prefetch_related')
1051         clone = self._chain()
1052         if lookups == (None,):
1053             clone._prefetch_related_lookups = ()
1054         else:
1055             for lookup in lookups:
1056                 if isinstance(lookup, Prefetch):
1057                     lookup = lookup.prefetch_to
1058                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1059                 if lookup in self.query._filtered_relations:
1060                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1061             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1062         return clone
1063 
1064     def annotate(self, *args, **kwargs):
1065         """
1066         Return a query set in which the returned objects have been annotated
1067         with extra data or aggregations.
1068         """
1069         self._not_support_combined_queries('annotate')
1070         return self._annotate(args, kwargs, select=True)
1071 
1072     def alias(self, *args, **kwargs):
1073         """
1074         Return a query set with added aliases for extra data or aggregations.
1075         """
1076         self._not_support_combined_queries('alias')
1077         return self._annotate(args, kwargs, select=False)
1078 
1079     def _annotate(self, args, kwargs, select=True):
1080         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1081         annotations = {}
1082         for arg in args:
1083             # The default_alias property may raise a TypeError.
1084             try:
1085                 if arg.default_alias in kwargs:
1086                     raise ValueError("The named annotation '%s' conflicts with the "
1087                                      "default name for another annotation."
1088                                      % arg.default_alias)
1089             except TypeError:
1090                 raise TypeError("Complex annotations require an alias")
1091             annotations[arg.default_alias] = arg
1092         annotations.update(kwargs)
1093 
1094         clone = self._chain()
1095         names = self._fields
1096         if names is None:
1097             names = set(chain.from_iterable(
1098                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1099                 for field in self.model._meta.get_fields()
1100             ))
1101 
1102         for alias, annotation in annotations.items():
1103             if alias in names:
1104                 raise ValueError("The annotation '%s' conflicts with a field on "
1105                                  "the model." % alias)
1106             if isinstance(annotation, FilteredRelation):
1107                 clone.query.add_filtered_relation(annotation, alias)
1108             else:
1109                 clone.query.add_annotation(
1110                     annotation, alias, is_summary=False, select=select,
1111                 )
1112         for alias, annotation in clone.query.annotations.items():
1113             if alias in annotations and annotation.contains_aggregate:
1114                 if clone._fields is None:
1115                     clone.query.group_by = True
1116                 else:
1117                     clone.query.set_group_by()
1118                 break
1119 
1120         return clone
1121 
1122     def order_by(self, *field_names):
1123         """Return a new QuerySet instance with the ordering changed."""
1124         assert not self.query.is_sliced, \
1125             "Cannot reorder a query once a slice has been taken."
1126         obj = self._chain()
1127         obj.query.clear_ordering(force_empty=False)
1128         obj.query.add_ordering(*field_names)
1129         return obj
1130 
1131     def distinct(self, *field_names):
1132         """
1133         Return a new QuerySet instance that will select only distinct results.
1134         """
1135         self._not_support_combined_queries('distinct')
1136         assert not self.query.is_sliced, \
1137             "Cannot create distinct fields once a slice has been taken."
1138         obj = self._chain()
1139         obj.query.add_distinct_fields(*field_names)
1140         return obj
1141 
1142     def extra(self, select=None, where=None, params=None, tables=None,
1143               order_by=None, select_params=None):
1144         """Add extra SQL fragments to the query."""
1145         self._not_support_combined_queries('extra')
1146         assert not self.query.is_sliced, \
1147             "Cannot change a query once a slice has been taken"
1148         clone = self._chain()
1149         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1150         return clone
1151 
1152     def reverse(self):
1153         """Reverse the ordering of the QuerySet."""
1154         if self.query.is_sliced:
1155             raise TypeError('Cannot reverse a query once a slice has been taken.')
1156         clone = self._chain()
1157         clone.query.standard_ordering = not clone.query.standard_ordering
1158         return clone
1159 
1160     def defer(self, *fields):
1161         """
1162         Defer the loading of data for certain fields until they are accessed.
1163         Add the set of deferred fields to any existing set of deferred fields.
1164         The only exception to this is if None is passed in as the only
1165         parameter, in which case removal all deferrals.
1166         """
1167         self._not_support_combined_queries('defer')
1168         if self._fields is not None:
1169             raise TypeError("Cannot call defer() after .values() or .values_list()")
1170         clone = self._chain()
1171         if fields == (None,):
1172             clone.query.clear_deferred_loading()
1173         else:
1174             clone.query.add_deferred_loading(fields)
1175         return clone
1176 
1177     def only(self, *fields):
1178         """
1179         Essentially, the opposite of defer(). Only the fields passed into this
1180         method and that are not already specified as deferred are loaded
1181         immediately when the queryset is evaluated.
1182         """
1183         self._not_support_combined_queries('only')
1184         if self._fields is not None:
1185             raise TypeError("Cannot call only() after .values() or .values_list()")
1186         if fields == (None,):
1187             # Can only pass None to defer(), not only(), as the rest option.
1188             # That won't stop people trying to do this, so let's be explicit.
1189             raise TypeError("Cannot pass None as an argument to only().")
1190         for field in fields:
1191             field = field.split(LOOKUP_SEP, 1)[0]
1192             if field in self.query._filtered_relations:
1193                 raise ValueError('only() is not supported with FilteredRelation.')
1194         clone = self._chain()
1195         clone.query.add_immediate_loading(fields)
1196         return clone
1197 
1198     def using(self, alias):
1199         """Select which database this QuerySet should execute against."""
1200         clone = self._chain()
1201         clone._db = alias
1202         return clone
1203 
1204     ###################################
1205     # PUBLIC INTROSPECTION ATTRIBUTES #
1206     ###################################
1207 
1208     @property
1209     def ordered(self):
1210         """
1211         Return True if the QuerySet is ordered -- i.e. has an order_by()
1212         clause or a default ordering on the model (or is empty).
1213         """
1214         if isinstance(self, EmptyQuerySet):
1215             return True
1216         if self.query.extra_order_by or self.query.order_by:
1217             return True
1218         elif (
1219             self.query.default_ordering and
1220             self.query.get_meta().ordering and
1221             # A default ordering doesn't affect GROUP BY queries.
1222             not self.query.group_by
1223         ):
1224             return True
1225         else:
1226             return False
1227 
1228     @property
1229     def db(self):
1230         """Return the database used if this query is executed now."""
1231         if self._for_write:
1232             return self._db or router.db_for_write(self.model, **self._hints)
1233         return self._db or router.db_for_read(self.model, **self._hints)
1234 
1235     ###################
1236     # PRIVATE METHODS #
1237     ###################
1238 
1239     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1240         """
1241         Insert a new record for the given model. This provides an interface to
1242         the InsertQuery class and is how Model.save() is implemented.
1243         """
1244         self._for_write = True
1245         if using is None:
1246             using = self.db
1247         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1248         query.insert_values(fields, objs, raw=raw)
1249         return query.get_compiler(using=using).execute_sql(returning_fields)
1250     _insert.alters_data = True
1251     _insert.queryset_only = False
1252 
1253     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1254         """
1255         Helper method for bulk_create() to insert objs one batch at a time.
1256         """
1257         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1258             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1259         ops = connections[self.db].ops
1260         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1261         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1262         inserted_rows = []
1263         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1264         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1265             if bulk_return and not ignore_conflicts:
1266                 inserted_rows.extend(self._insert(
1267                     item, fields=fields, using=self.db,
1268                     returning_fields=self.model._meta.db_returning_fields,
1269                     ignore_conflicts=ignore_conflicts,
1270                 ))
1271             else:
1272                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1273         return inserted_rows
1274 
1275     def _chain(self, **kwargs):
1276         """
1277         Return a copy of the current QuerySet that's ready for another
1278         operation.
1279         """
1280         obj = self._clone()
1281         if obj._sticky_filter:
1282             obj.query.filter_is_sticky = True
1283             obj._sticky_filter = False
1284         obj.__dict__.update(kwargs)
1285         return obj
1286 
1287     def _clone(self):
1288         """
1289         Return a copy of the current QuerySet. A lightweight alternative
1290         to deepcopy().
1291         """
1292         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1293         c._sticky_filter = self._sticky_filter
1294         c._for_write = self._for_write
1295         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1296         c._known_related_objects = self._known_related_objects
1297         c._iterable_class = self._iterable_class
1298         c._fields = self._fields
1299         return c
1300 
1301     def _fetch_all(self):
1302         if self._result_cache is None:
1303             self._result_cache = list(self._iterable_class(self))
1304         if self._prefetch_related_lookups and not self._prefetch_done:
1305             self._prefetch_related_objects()
1306 
1307     def _next_is_sticky(self):
1308         """
1309         Indicate that the next filter call and the one following that should
1310         be treated as a single filter. This is only important when it comes to
1311         determining when to reuse tables for many-to-many filters. Required so
1312         that we can filter naturally on the results of related managers.
1313 
1314         This doesn't return a clone of the current QuerySet (it returns
1315         "self"). The method is only used internally and should be immediately
1316         followed by a filter() that does create a clone.
1317         """
1318         self._sticky_filter = True
1319         return self
1320 
1321     def _merge_sanity_check(self, other):
1322         """Check that two QuerySet classes may be merged."""
1323         if self._fields is not None and (
1324                 set(self.query.values_select) != set(other.query.values_select) or
1325                 set(self.query.extra_select) != set(other.query.extra_select) or
1326                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1327             raise TypeError(
1328                 "Merging '%s' classes must involve the same values in each case."
1329                 % self.__class__.__name__
1330             )
1331 
1332     def _merge_known_related_objects(self, other):
1333         """
1334         Keep track of all known related objects from either QuerySet instance.
1335         """
1336         for field, objects in other._known_related_objects.items():
1337             self._known_related_objects.setdefault(field, {}).update(objects)
1338 
1339     def resolve_expression(self, *args, **kwargs):
1340         if self._fields and len(self._fields) > 1:
1341             # values() queryset can only be used as nested queries
1342             # if they are set up to select only a single field.
1343             raise TypeError('Cannot use multi-field values as a filter value.')
1344         query = self.query.resolve_expression(*args, **kwargs)
1345         query._db = self._db
1346         return query
1347     resolve_expression.queryset_only = True
1348 
1349     def _add_hints(self, **hints):
1350         """
1351         Update hinting information for use by routers. Add new key/values or
1352         overwrite existing key/values.
1353         """
1354         self._hints.update(hints)
1355 
1356     def _has_filters(self):
1357         """
1358         Check if this QuerySet has any filtering going on. This isn't
1359         equivalent with checking if all objects are present in results, for
1360         example, qs[1:]._has_filters() -> False.
1361         """
1362         return self.query.has_filters()
1363 
1364     @staticmethod
1365     def _validate_values_are_expressions(values, method_name):
1366         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1367         if invalid_args:
1368             raise TypeError(
1369                 'QuerySet.%s() received non-expression(s): %s.' % (
1370                     method_name,
1371                     ', '.join(invalid_args),
1372                 )
1373             )
1374 
1375     def _not_support_combined_queries(self, operation_name):
1376         if self.query.combinator:
1377             raise NotSupportedError(
1378                 'Calling QuerySet.%s() after %s() is not supported.'
1379                 % (operation_name, self.query.combinator)
1380             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 05:18:31.491 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>aggregate</method>
<code>
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             if not query.annotations[alias].contains_aggregate:
390                 raise TypeError("%s is not an aggregate expression" % alias)
391         return query.get_aggregation(self.db, kwargs)

</code><intended_behavior>When constructing the SQL query for aggregation, the method should correctly handle and reference annotated fields created in a preceding annotate call. This means ensuring that any aliases used for annotated fields are consistently referenced throughout the query construction process, especially when those annotated fields are utilized in aggregates. The intersection point where the aggregate queries include annotated fields must reference these annotations correctly, avoiding SQL errors like the unknown column issue because of misreferenced aliases. This adjustment ensures that when Django constructs subqueries involving annotations and aggregates, all references to annotated fields are accurate and coherent, preserving the aliasing without leading to resolution errors.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             if not query.annotations[alias].contains_aggregate:
390                 raise TypeError("%s is not an aggregate expression" % alias)
391         return query.get_aggregation(self.db, kwargs)
392 
393     def count(self):
394         """
395         Perform a SELECT COUNT() and return the number of records as an
396         integer.
397 
398         If the QuerySet is already fully cached, return the length of the
399         cached results set to avoid multiple SELECT COUNT(*) calls.
400         """
401         if self._result_cache is not None:
402             return len(self._result_cache)
403 
404         return self.query.get_count(using=self.db)
405 
406     def get(self, *args, **kwargs):
407         """
408         Perform the query and return a single object matching the given
409         keyword arguments.
410         """
411         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
412         if self.query.can_filter() and not self.query.distinct_fields:
413             clone = clone.order_by()
414         limit = None
415         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
416             limit = MAX_GET_RESULTS
417             clone.query.set_limits(high=limit)
418         num = len(clone)
419         if num == 1:
420             return clone._result_cache[0]
421         if not num:
422             raise self.model.DoesNotExist(
423                 "%s matching query does not exist." %
424                 self.model._meta.object_name
425             )
426         raise self.model.MultipleObjectsReturned(
427             'get() returned more than one %s -- it returned %s!' % (
428                 self.model._meta.object_name,
429                 num if not limit or num < limit else 'more than %s' % (limit - 1),
430             )
431         )
432 
433     def create(self, **kwargs):
434         """
435         Create a new object with the given kwargs, saving it to the database
436         and returning the created object.
437         """
438         obj = self.model(**kwargs)
439         self._for_write = True
440         obj.save(force_insert=True, using=self.db)
441         return obj
442 
443     def _populate_pk_values(self, objs):
444         for obj in objs:
445             if obj.pk is None:
446                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
447 
448     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
449         """
450         Insert each of the instances into the database. Do *not* call
451         save() on each of the instances, do not send any pre/post_save
452         signals, and do not set the primary key attribute if it is an
453         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
454         Multi-table models are not supported.
455         """
456         # When you bulk insert you don't get the primary keys back (if it's an
457         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
458         # you can't insert into the child tables which references this. There
459         # are two workarounds:
460         # 1) This could be implemented if you didn't have an autoincrement pk
461         # 2) You could do it by doing O(n) normal inserts into the parent
462         #    tables to get the primary keys back and then doing a single bulk
463         #    insert into the childmost table.
464         # We currently set the primary keys on the objects when using
465         # PostgreSQL via the RETURNING ID clause. It should be possible for
466         # Oracle as well, but the semantics for extracting the primary keys is
467         # trickier so it's not done yet.
468         assert batch_size is None or batch_size > 0
469         # Check that the parents share the same concrete model with the our
470         # model to detect the inheritance pattern ConcreteGrandParent ->
471         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
472         # would not identify that case as involving multiple tables.
473         for parent in self.model._meta.get_parent_list():
474             if parent._meta.concrete_model is not self.model._meta.concrete_model:
475                 raise ValueError("Can't bulk create a multi-table inherited model")
476         if not objs:
477             return objs
478         self._for_write = True
479         connection = connections[self.db]
480         opts = self.model._meta
481         fields = opts.concrete_fields
482         objs = list(objs)
483         self._populate_pk_values(objs)
484         with transaction.atomic(using=self.db, savepoint=False):
485             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
486             if objs_with_pk:
487                 returned_columns = self._batched_insert(
488                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
489                 )
490                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
491                     for result, field in zip(results, opts.db_returning_fields):
492                         if field != opts.pk:
493                             setattr(obj_with_pk, field.attname, result)
494                 for obj_with_pk in objs_with_pk:
495                     obj_with_pk._state.adding = False
496                     obj_with_pk._state.db = self.db
497             if objs_without_pk:
498                 fields = [f for f in fields if not isinstance(f, AutoField)]
499                 returned_columns = self._batched_insert(
500                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
501                 )
502                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
503                     assert len(returned_columns) == len(objs_without_pk)
504                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
505                     for result, field in zip(results, opts.db_returning_fields):
506                         setattr(obj_without_pk, field.attname, result)
507                     obj_without_pk._state.adding = False
508                     obj_without_pk._state.db = self.db
509 
510         return objs
511 
512     def bulk_update(self, objs, fields, batch_size=None):
513         """
514         Update the given fields in each of the given objects in the database.
515         """
516         if batch_size is not None and batch_size < 0:
517             raise ValueError('Batch size must be a positive integer.')
518         if not fields:
519             raise ValueError('Field names must be given to bulk_update().')
520         objs = tuple(objs)
521         if any(obj.pk is None for obj in objs):
522             raise ValueError('All bulk_update() objects must have a primary key set.')
523         fields = [self.model._meta.get_field(name) for name in fields]
524         if any(not f.concrete or f.many_to_many for f in fields):
525             raise ValueError('bulk_update() can only be used with concrete fields.')
526         if any(f.primary_key for f in fields):
527             raise ValueError('bulk_update() cannot be used with primary key fields.')
528         if not objs:
529             return
530         # PK is used twice in the resulting update query, once in the filter
531         # and once in the WHEN. Each field will also have one CAST.
532         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
533         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
534         requires_casting = connections[self.db].features.requires_casted_case_in_updates
535         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
536         updates = []
537         for batch_objs in batches:
538             update_kwargs = {}
539             for field in fields:
540                 when_statements = []
541                 for obj in batch_objs:
542                     attr = getattr(obj, field.attname)
543                     if not isinstance(attr, Expression):
544                         attr = Value(attr, output_field=field)
545                     when_statements.append(When(pk=obj.pk, then=attr))
546                 case_statement = Case(*when_statements, output_field=field)
547                 if requires_casting:
548                     case_statement = Cast(case_statement, output_field=field)
549                 update_kwargs[field.attname] = case_statement
550             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
551         with transaction.atomic(using=self.db, savepoint=False):
552             for pks, update_kwargs in updates:
553                 self.filter(pk__in=pks).update(**update_kwargs)
554     bulk_update.alters_data = True
555 
556     def get_or_create(self, defaults=None, **kwargs):
557         """
558         Look up an object with the given kwargs, creating one if necessary.
559         Return a tuple of (object, created), where created is a boolean
560         specifying whether an object was created.
561         """
562         # The get() needs to be targeted at the write database in order
563         # to avoid potential transaction consistency problems.
564         self._for_write = True
565         try:
566             return self.get(**kwargs), False
567         except self.model.DoesNotExist:
568             params = self._extract_model_params(defaults, **kwargs)
569             # Try to create an object using passed params.
570             try:
571                 with transaction.atomic(using=self.db):
572                     params = dict(resolve_callables(params))
573                     return self.create(**params), True
574             except IntegrityError:
575                 try:
576                     return self.get(**kwargs), False
577                 except self.model.DoesNotExist:
578                     pass
579                 raise
580 
581     def update_or_create(self, defaults=None, **kwargs):
582         """
583         Look up an object with the given kwargs, updating one with defaults
584         if it exists, otherwise create a new one.
585         Return a tuple (object, created), where created is a boolean
586         specifying whether an object was created.
587         """
588         defaults = defaults or {}
589         self._for_write = True
590         with transaction.atomic(using=self.db):
591             # Lock the row so that a concurrent update is blocked until
592             # update_or_create() has performed its save.
593             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
594             if created:
595                 return obj, created
596             for k, v in resolve_callables(defaults):
597                 setattr(obj, k, v)
598             obj.save(using=self.db)
599         return obj, False
600 
601     def _extract_model_params(self, defaults, **kwargs):
602         """
603         Prepare `params` for creating a model instance based on the given
604         kwargs; for use by get_or_create().
605         """
606         defaults = defaults or {}
607         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
608         params.update(defaults)
609         property_names = self.model._meta._property_names
610         invalid_params = []
611         for param in params:
612             try:
613                 self.model._meta.get_field(param)
614             except exceptions.FieldDoesNotExist:
615                 # It's okay to use a model's property if it has a setter.
616                 if not (param in property_names and getattr(self.model, param).fset):
617                     invalid_params.append(param)
618         if invalid_params:
619             raise exceptions.FieldError(
620                 "Invalid field name(s) for model %s: '%s'." % (
621                     self.model._meta.object_name,
622                     "', '".join(sorted(invalid_params)),
623                 ))
624         return params
625 
626     def _earliest(self, *fields):
627         """
628         Return the earliest object according to fields (if given) or by the
629         model's Meta.get_latest_by.
630         """
631         if fields:
632             order_by = fields
633         else:
634             order_by = getattr(self.model._meta, 'get_latest_by')
635             if order_by and not isinstance(order_by, (tuple, list)):
636                 order_by = (order_by,)
637         if order_by is None:
638             raise ValueError(
639                 "earliest() and latest() require either fields as positional "
640                 "arguments or 'get_latest_by' in the model's Meta."
641             )
642 
643         assert not self.query.is_sliced, \
644             "Cannot change a query once a slice has been taken."
645         obj = self._chain()
646         obj.query.set_limits(high=1)
647         obj.query.clear_ordering(force_empty=True)
648         obj.query.add_ordering(*order_by)
649         return obj.get()
650 
651     def earliest(self, *fields):
652         return self._earliest(*fields)
653 
654     def latest(self, *fields):
655         return self.reverse()._earliest(*fields)
656 
657     def first(self):
658         """Return the first object of a query or None if no match is found."""
659         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
660             return obj
661 
662     def last(self):
663         """Return the last object of a query or None if no match is found."""
664         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
665             return obj
666 
667     def in_bulk(self, id_list=None, *, field_name='pk'):
668         """
669         Return a dictionary mapping each of the given IDs to the object with
670         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
671         """
672         assert not self.query.is_sliced, \
673             "Cannot use 'limit' or 'offset' with in_bulk"
674         opts = self.model._meta
675         unique_fields = [
676             constraint.fields[0]
677             for constraint in opts.total_unique_constraints
678             if len(constraint.fields) == 1
679         ]
680         if (
681             field_name != 'pk' and
682             not opts.get_field(field_name).unique and
683             field_name not in unique_fields and
684             not self.query.distinct_fields == (field_name,)
685         ):
686             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
687         if id_list is not None:
688             if not id_list:
689                 return {}
690             filter_key = '{}__in'.format(field_name)
691             batch_size = connections[self.db].features.max_query_params
692             id_list = tuple(id_list)
693             # If the database has a limit on the number of query parameters
694             # (e.g. SQLite), retrieve objects in batches if necessary.
695             if batch_size and batch_size < len(id_list):
696                 qs = ()
697                 for offset in range(0, len(id_list), batch_size):
698                     batch = id_list[offset:offset + batch_size]
699                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
700             else:
701                 qs = self.filter(**{filter_key: id_list}).order_by()
702         else:
703             qs = self._chain()
704         return {getattr(obj, field_name): obj for obj in qs}
705 
706     def delete(self):
707         """Delete the records in the current QuerySet."""
708         self._not_support_combined_queries('delete')
709         assert not self.query.is_sliced, \
710             "Cannot use 'limit' or 'offset' with delete."
711 
712         if self._fields is not None:
713             raise TypeError("Cannot call delete() after .values() or .values_list()")
714 
715         del_query = self._chain()
716 
717         # The delete is actually 2 queries - one to find related objects,
718         # and one to delete. Make sure that the discovery of related
719         # objects is performed on the same database as the deletion.
720         del_query._for_write = True
721 
722         # Disable non-supported fields.
723         del_query.query.select_for_update = False
724         del_query.query.select_related = False
725         del_query.query.clear_ordering(force_empty=True)
726 
727         collector = Collector(using=del_query.db)
728         collector.collect(del_query)
729         deleted, _rows_count = collector.delete()
730 
731         # Clear the result cache, in case this QuerySet gets reused.
732         self._result_cache = None
733         return deleted, _rows_count
734 
735     delete.alters_data = True
736     delete.queryset_only = True
737 
738     def _raw_delete(self, using):
739         """
740         Delete objects found from the given queryset in single direct SQL
741         query. No signals are sent and there is no protection for cascades.
742         """
743         query = self.query.clone()
744         query.__class__ = sql.DeleteQuery
745         cursor = query.get_compiler(using).execute_sql(CURSOR)
746         if cursor:
747             with cursor:
748                 return cursor.rowcount
749         return 0
750     _raw_delete.alters_data = True
751 
752     def update(self, **kwargs):
753         """
754         Update all elements in the current QuerySet, setting all the given
755         fields to the appropriate values.
756         """
757         self._not_support_combined_queries('update')
758         assert not self.query.is_sliced, \
759             "Cannot update a query once a slice has been taken."
760         self._for_write = True
761         query = self.query.chain(sql.UpdateQuery)
762         query.add_update_values(kwargs)
763         # Clear any annotations so that they won't be present in subqueries.
764         query.annotations = {}
765         with transaction.mark_for_rollback_on_error(using=self.db):
766             rows = query.get_compiler(self.db).execute_sql(CURSOR)
767         self._result_cache = None
768         return rows
769     update.alters_data = True
770 
771     def _update(self, values):
772         """
773         A version of update() that accepts field objects instead of field names.
774         Used primarily for model saving and not intended for use by general
775         code (it requires too much poking around at model internals to be
776         useful at that level).
777         """
778         assert not self.query.is_sliced, \
779             "Cannot update a query once a slice has been taken."
780         query = self.query.chain(sql.UpdateQuery)
781         query.add_update_fields(values)
782         # Clear any annotations so that they won't be present in subqueries.
783         query.annotations = {}
784         self._result_cache = None
785         return query.get_compiler(self.db).execute_sql(CURSOR)
786     _update.alters_data = True
787     _update.queryset_only = False
788 
789     def exists(self):
790         if self._result_cache is None:
791             return self.query.has_results(using=self.db)
792         return bool(self._result_cache)
793 
794     def _prefetch_related_objects(self):
795         # This method can only be called once the result cache has been filled.
796         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
797         self._prefetch_done = True
798 
799     def explain(self, *, format=None, **options):
800         return self.query.explain(using=self.db, format=format, **options)
801 
802     ##################################################
803     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
804     ##################################################
805 
806     def raw(self, raw_query, params=None, translations=None, using=None):
807         if using is None:
808             using = self.db
809         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
810         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
811         return qs
812 
813     def _values(self, *fields, **expressions):
814         clone = self._chain()
815         if expressions:
816             clone = clone.annotate(**expressions)
817         clone._fields = fields
818         clone.query.set_values(fields)
819         return clone
820 
821     def values(self, *fields, **expressions):
822         fields += tuple(expressions)
823         clone = self._values(*fields, **expressions)
824         clone._iterable_class = ValuesIterable
825         return clone
826 
827     def values_list(self, *fields, flat=False, named=False):
828         if flat and named:
829             raise TypeError("'flat' and 'named' can't be used together.")
830         if flat and len(fields) > 1:
831             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
832 
833         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
834         _fields = []
835         expressions = {}
836         counter = 1
837         for field in fields:
838             if hasattr(field, 'resolve_expression'):
839                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
840                 while True:
841                     field_id = field_id_prefix + str(counter)
842                     counter += 1
843                     if field_id not in field_names:
844                         break
845                 expressions[field_id] = field
846                 _fields.append(field_id)
847             else:
848                 _fields.append(field)
849 
850         clone = self._values(*_fields, **expressions)
851         clone._iterable_class = (
852             NamedValuesListIterable if named
853             else FlatValuesListIterable if flat
854             else ValuesListIterable
855         )
856         return clone
857 
858     def dates(self, field_name, kind, order='ASC'):
859         """
860         Return a list of date objects representing all available dates for
861         the given field_name, scoped to 'kind'.
862         """
863         assert kind in ('year', 'month', 'week', 'day'), \
864             "'kind' must be one of 'year', 'month', 'week', or 'day'."
865         assert order in ('ASC', 'DESC'), \
866             "'order' must be either 'ASC' or 'DESC'."
867         return self.annotate(
868             datefield=Trunc(field_name, kind, output_field=DateField()),
869             plain_field=F(field_name)
870         ).values_list(
871             'datefield', flat=True
872         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
873 
874     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
875         """
876         Return a list of datetime objects representing all available
877         datetimes for the given field_name, scoped to 'kind'.
878         """
879         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
880             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
881         assert order in ('ASC', 'DESC'), \
882             "'order' must be either 'ASC' or 'DESC'."
883         if settings.USE_TZ:
884             if tzinfo is None:
885                 tzinfo = timezone.get_current_timezone()
886         else:
887             tzinfo = None
888         return self.annotate(
889             datetimefield=Trunc(
890                 field_name,
891                 kind,
892                 output_field=DateTimeField(),
893                 tzinfo=tzinfo,
894                 is_dst=is_dst,
895             ),
896             plain_field=F(field_name)
897         ).values_list(
898             'datetimefield', flat=True
899         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
900 
901     def none(self):
902         """Return an empty QuerySet."""
903         clone = self._chain()
904         clone.query.set_empty()
905         return clone
906 
907     ##################################################################
908     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
909     ##################################################################
910 
911     def all(self):
912         """
913         Return a new QuerySet that is a copy of the current one. This allows a
914         QuerySet to proxy for a model manager in some cases.
915         """
916         return self._chain()
917 
918     def filter(self, *args, **kwargs):
919         """
920         Return a new QuerySet instance with the args ANDed to the existing
921         set.
922         """
923         self._not_support_combined_queries('filter')
924         return self._filter_or_exclude(False, args, kwargs)
925 
926     def exclude(self, *args, **kwargs):
927         """
928         Return a new QuerySet instance with NOT (args) ANDed to the existing
929         set.
930         """
931         self._not_support_combined_queries('exclude')
932         return self._filter_or_exclude(True, args, kwargs)
933 
934     def _filter_or_exclude(self, negate, args, kwargs):
935         if args or kwargs:
936             assert not self.query.is_sliced, \
937                 "Cannot filter a query once a slice has been taken."
938 
939         clone = self._chain()
940         if self._defer_next_filter:
941             self._defer_next_filter = False
942             clone._deferred_filter = negate, args, kwargs
943         else:
944             clone._filter_or_exclude_inplace(negate, args, kwargs)
945         return clone
946 
947     def _filter_or_exclude_inplace(self, negate, args, kwargs):
948         if negate:
949             self._query.add_q(~Q(*args, **kwargs))
950         else:
951             self._query.add_q(Q(*args, **kwargs))
952 
953     def complex_filter(self, filter_obj):
954         """
955         Return a new QuerySet instance with filter_obj added to the filters.
956 
957         filter_obj can be a Q object or a dictionary of keyword lookup
958         arguments.
959 
960         This exists to support framework features such as 'limit_choices_to',
961         and usually it will be more natural to use other methods.
962         """
963         if isinstance(filter_obj, Q):
964             clone = self._chain()
965             clone.query.add_q(filter_obj)
966             return clone
967         else:
968             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
969 
970     def _combinator_query(self, combinator, *other_qs, all=False):
971         # Clone the query to inherit the select list and everything
972         clone = self._chain()
973         # Clear limits and ordering so they can be reapplied
974         clone.query.clear_ordering(True)
975         clone.query.clear_limits()
976         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
977         clone.query.combinator = combinator
978         clone.query.combinator_all = all
979         return clone
980 
981     def union(self, *other_qs, all=False):
982         # If the query is an EmptyQuerySet, combine all nonempty querysets.
983         if isinstance(self, EmptyQuerySet):
984             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
985             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
986         return self._combinator_query('union', *other_qs, all=all)
987 
988     def intersection(self, *other_qs):
989         # If any query is an EmptyQuerySet, return it.
990         if isinstance(self, EmptyQuerySet):
991             return self
992         for other in other_qs:
993             if isinstance(other, EmptyQuerySet):
994                 return other
995         return self._combinator_query('intersection', *other_qs)
996 
997     def difference(self, *other_qs):
998         # If the query is an EmptyQuerySet, return it.
999         if isinstance(self, EmptyQuerySet):
1000             return self
1001         return self._combinator_query('difference', *other_qs)
1002 
1003     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1004         """
1005         Return a new QuerySet instance that will select objects with a
1006         FOR UPDATE lock.
1007         """
1008         if nowait and skip_locked:
1009             raise ValueError('The nowait option cannot be used with skip_locked.')
1010         obj = self._chain()
1011         obj._for_write = True
1012         obj.query.select_for_update = True
1013         obj.query.select_for_update_nowait = nowait
1014         obj.query.select_for_update_skip_locked = skip_locked
1015         obj.query.select_for_update_of = of
1016         obj.query.select_for_no_key_update = no_key
1017         return obj
1018 
1019     def select_related(self, *fields):
1020         """
1021         Return a new QuerySet instance that will select related objects.
1022 
1023         If fields are specified, they must be ForeignKey fields and only those
1024         related objects are included in the selection.
1025 
1026         If select_related(None) is called, clear the list.
1027         """
1028         self._not_support_combined_queries('select_related')
1029         if self._fields is not None:
1030             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1031 
1032         obj = self._chain()
1033         if fields == (None,):
1034             obj.query.select_related = False
1035         elif fields:
1036             obj.query.add_select_related(fields)
1037         else:
1038             obj.query.select_related = True
1039         return obj
1040 
1041     def prefetch_related(self, *lookups):
1042         """
1043         Return a new QuerySet instance that will prefetch the specified
1044         Many-To-One and Many-To-Many related objects when the QuerySet is
1045         evaluated.
1046 
1047         When prefetch_related() is called more than once, append to the list of
1048         prefetch lookups. If prefetch_related(None) is called, clear the list.
1049         """
1050         self._not_support_combined_queries('prefetch_related')
1051         clone = self._chain()
1052         if lookups == (None,):
1053             clone._prefetch_related_lookups = ()
1054         else:
1055             for lookup in lookups:
1056                 if isinstance(lookup, Prefetch):
1057                     lookup = lookup.prefetch_to
1058                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1059                 if lookup in self.query._filtered_relations:
1060                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1061             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1062         return clone
1063 
1064     def annotate(self, *args, **kwargs):
1065         """
1066         Return a query set in which the returned objects have been annotated
1067         with extra data or aggregations.
1068         """
1069         self._not_support_combined_queries('annotate')
1070         return self._annotate(args, kwargs, select=True)
1071 
1072     def alias(self, *args, **kwargs):
1073         """
1074         Return a query set with added aliases for extra data or aggregations.
1075         """
1076         self._not_support_combined_queries('alias')
1077         return self._annotate(args, kwargs, select=False)
1078 
1079     def _annotate(self, args, kwargs, select=True):
1080         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1081         annotations = {}
1082         for arg in args:
1083             # The default_alias property may raise a TypeError.
1084             try:
1085                 if arg.default_alias in kwargs:
1086                     raise ValueError("The named annotation '%s' conflicts with the "
1087                                      "default name for another annotation."
1088                                      % arg.default_alias)
1089             except TypeError:
1090                 raise TypeError("Complex annotations require an alias")
1091             annotations[arg.default_alias] = arg
1092         annotations.update(kwargs)
1093 
1094         clone = self._chain()
1095         names = self._fields
1096         if names is None:
1097             names = set(chain.from_iterable(
1098                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1099                 for field in self.model._meta.get_fields()
1100             ))
1101 
1102         for alias, annotation in annotations.items():
1103             if alias in names:
1104                 raise ValueError("The annotation '%s' conflicts with a field on "
1105                                  "the model." % alias)
1106             if isinstance(annotation, FilteredRelation):
1107                 clone.query.add_filtered_relation(annotation, alias)
1108             else:
1109                 clone.query.add_annotation(
1110                     annotation, alias, is_summary=False, select=select,
1111                 )
1112         for alias, annotation in clone.query.annotations.items():
1113             if alias in annotations and annotation.contains_aggregate:
1114                 if clone._fields is None:
1115                     clone.query.group_by = True
1116                 else:
1117                     clone.query.set_group_by()
1118                 break
1119 
1120         return clone
1121 
1122     def order_by(self, *field_names):
1123         """Return a new QuerySet instance with the ordering changed."""
1124         assert not self.query.is_sliced, \
1125             "Cannot reorder a query once a slice has been taken."
1126         obj = self._chain()
1127         obj.query.clear_ordering(force_empty=False)
1128         obj.query.add_ordering(*field_names)
1129         return obj
1130 
1131     def distinct(self, *field_names):
1132         """
1133         Return a new QuerySet instance that will select only distinct results.
1134         """
1135         self._not_support_combined_queries('distinct')
1136         assert not self.query.is_sliced, \
1137             "Cannot create distinct fields once a slice has been taken."
1138         obj = self._chain()
1139         obj.query.add_distinct_fields(*field_names)
1140         return obj
1141 
1142     def extra(self, select=None, where=None, params=None, tables=None,
1143               order_by=None, select_params=None):
1144         """Add extra SQL fragments to the query."""
1145         self._not_support_combined_queries('extra')
1146         assert not self.query.is_sliced, \
1147             "Cannot change a query once a slice has been taken"
1148         clone = self._chain()
1149         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1150         return clone
1151 
1152     def reverse(self):
1153         """Reverse the ordering of the QuerySet."""
1154         if self.query.is_sliced:
1155             raise TypeError('Cannot reverse a query once a slice has been taken.')
1156         clone = self._chain()
1157         clone.query.standard_ordering = not clone.query.standard_ordering
1158         return clone
1159 
1160     def defer(self, *fields):
1161         """
1162         Defer the loading of data for certain fields until they are accessed.
1163         Add the set of deferred fields to any existing set of deferred fields.
1164         The only exception to this is if None is passed in as the only
1165         parameter, in which case removal all deferrals.
1166         """
1167         self._not_support_combined_queries('defer')
1168         if self._fields is not None:
1169             raise TypeError("Cannot call defer() after .values() or .values_list()")
1170         clone = self._chain()
1171         if fields == (None,):
1172             clone.query.clear_deferred_loading()
1173         else:
1174             clone.query.add_deferred_loading(fields)
1175         return clone
1176 
1177     def only(self, *fields):
1178         """
1179         Essentially, the opposite of defer(). Only the fields passed into this
1180         method and that are not already specified as deferred are loaded
1181         immediately when the queryset is evaluated.
1182         """
1183         self._not_support_combined_queries('only')
1184         if self._fields is not None:
1185             raise TypeError("Cannot call only() after .values() or .values_list()")
1186         if fields == (None,):
1187             # Can only pass None to defer(), not only(), as the rest option.
1188             # That won't stop people trying to do this, so let's be explicit.
1189             raise TypeError("Cannot pass None as an argument to only().")
1190         for field in fields:
1191             field = field.split(LOOKUP_SEP, 1)[0]
1192             if field in self.query._filtered_relations:
1193                 raise ValueError('only() is not supported with FilteredRelation.')
1194         clone = self._chain()
1195         clone.query.add_immediate_loading(fields)
1196         return clone
1197 
1198     def using(self, alias):
1199         """Select which database this QuerySet should execute against."""
1200         clone = self._chain()
1201         clone._db = alias
1202         return clone
1203 
1204     ###################################
1205     # PUBLIC INTROSPECTION ATTRIBUTES #
1206     ###################################
1207 
1208     @property
1209     def ordered(self):
1210         """
1211         Return True if the QuerySet is ordered -- i.e. has an order_by()
1212         clause or a default ordering on the model (or is empty).
1213         """
1214         if isinstance(self, EmptyQuerySet):
1215             return True
1216         if self.query.extra_order_by or self.query.order_by:
1217             return True
1218         elif (
1219             self.query.default_ordering and
1220             self.query.get_meta().ordering and
1221             # A default ordering doesn't affect GROUP BY queries.
1222             not self.query.group_by
1223         ):
1224             return True
1225         else:
1226             return False
1227 
1228     @property
1229     def db(self):
1230         """Return the database used if this query is executed now."""
1231         if self._for_write:
1232             return self._db or router.db_for_write(self.model, **self._hints)
1233         return self._db or router.db_for_read(self.model, **self._hints)
1234 
1235     ###################
1236     # PRIVATE METHODS #
1237     ###################
1238 
1239     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1240         """
1241         Insert a new record for the given model. This provides an interface to
1242         the InsertQuery class and is how Model.save() is implemented.
1243         """
1244         self._for_write = True
1245         if using is None:
1246             using = self.db
1247         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1248         query.insert_values(fields, objs, raw=raw)
1249         return query.get_compiler(using=using).execute_sql(returning_fields)
1250     _insert.alters_data = True
1251     _insert.queryset_only = False
1252 
1253     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1254         """
1255         Helper method for bulk_create() to insert objs one batch at a time.
1256         """
1257         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1258             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1259         ops = connections[self.db].ops
1260         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1261         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1262         inserted_rows = []
1263         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1264         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1265             if bulk_return and not ignore_conflicts:
1266                 inserted_rows.extend(self._insert(
1267                     item, fields=fields, using=self.db,
1268                     returning_fields=self.model._meta.db_returning_fields,
1269                     ignore_conflicts=ignore_conflicts,
1270                 ))
1271             else:
1272                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1273         return inserted_rows
1274 
1275     def _chain(self, **kwargs):
1276         """
1277         Return a copy of the current QuerySet that's ready for another
1278         operation.
1279         """
1280         obj = self._clone()
1281         if obj._sticky_filter:
1282             obj.query.filter_is_sticky = True
1283             obj._sticky_filter = False
1284         obj.__dict__.update(kwargs)
1285         return obj
1286 
1287     def _clone(self):
1288         """
1289         Return a copy of the current QuerySet. A lightweight alternative
1290         to deepcopy().
1291         """
1292         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1293         c._sticky_filter = self._sticky_filter
1294         c._for_write = self._for_write
1295         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1296         c._known_related_objects = self._known_related_objects
1297         c._iterable_class = self._iterable_class
1298         c._fields = self._fields
1299         return c
1300 
1301     def _fetch_all(self):
1302         if self._result_cache is None:
1303             self._result_cache = list(self._iterable_class(self))
1304         if self._prefetch_related_lookups and not self._prefetch_done:
1305             self._prefetch_related_objects()
1306 
1307     def _next_is_sticky(self):
1308         """
1309         Indicate that the next filter call and the one following that should
1310         be treated as a single filter. This is only important when it comes to
1311         determining when to reuse tables for many-to-many filters. Required so
1312         that we can filter naturally on the results of related managers.
1313 
1314         This doesn't return a clone of the current QuerySet (it returns
1315         "self"). The method is only used internally and should be immediately
1316         followed by a filter() that does create a clone.
1317         """
1318         self._sticky_filter = True
1319         return self
1320 
1321     def _merge_sanity_check(self, other):
1322         """Check that two QuerySet classes may be merged."""
1323         if self._fields is not None and (
1324                 set(self.query.values_select) != set(other.query.values_select) or
1325                 set(self.query.extra_select) != set(other.query.extra_select) or
1326                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1327             raise TypeError(
1328                 "Merging '%s' classes must involve the same values in each case."
1329                 % self.__class__.__name__
1330             )
1331 
1332     def _merge_known_related_objects(self, other):
1333         """
1334         Keep track of all known related objects from either QuerySet instance.
1335         """
1336         for field, objects in other._known_related_objects.items():
1337             self._known_related_objects.setdefault(field, {}).update(objects)
1338 
1339     def resolve_expression(self, *args, **kwargs):
1340         if self._fields and len(self._fields) > 1:
1341             # values() queryset can only be used as nested queries
1342             # if they are set up to select only a single field.
1343             raise TypeError('Cannot use multi-field values as a filter value.')
1344         query = self.query.resolve_expression(*args, **kwargs)
1345         query._db = self._db
1346         return query
1347     resolve_expression.queryset_only = True
1348 
1349     def _add_hints(self, **hints):
1350         """
1351         Update hinting information for use by routers. Add new key/values or
1352         overwrite existing key/values.
1353         """
1354         self._hints.update(hints)
1355 
1356     def _has_filters(self):
1357         """
1358         Check if this QuerySet has any filtering going on. This isn't
1359         equivalent with checking if all objects are present in results, for
1360         example, qs[1:]._has_filters() -> False.
1361         """
1362         return self.query.has_filters()
1363 
1364     @staticmethod
1365     def _validate_values_are_expressions(values, method_name):
1366         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1367         if invalid_args:
1368             raise TypeError(
1369                 'QuerySet.%s() received non-expression(s): %s.' % (
1370                     method_name,
1371                     ', '.join(invalid_args),
1372                 )
1373             )
1374 
1375     def _not_support_combined_queries(self, operation_name):
1376         if self.query.combinator:
1377             raise NotSupportedError(
1378                 'Calling QuerySet.%s() after %s() is not supported.'
1379                 % (operation_name, self.query.combinator)
1380             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1064     def annotate(self, *args, **kwargs):
1065         """
1066         Return a query set in which the returned objects have been annotated
1067         with extra data or aggregations.
1068         """
1069         self._not_support_combined_queries('annotate')
1070         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>Although not directly implicated as faulty, ensuring the annotate method's output is correctly utilized by subsequent aggregate calls is essential. The annotate function should clearly define the scope and aliasing of annotated fields so that these can be understood and handled without ambiguity in subsequent operations, specifically aggregate. The method itself may not require changes, but its interaction with aggregate and the overall SQL construction logic should be verified to ensure annotations are appropriately factored into the aggregate's SQL generation logic. This includes validating the chaining of queryset methods where annotations serve as inputs to aggregates, leading to proper SQL command generation that considers the established aliases.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         assert ((not isinstance(k, slice) and (k >= 0)) or
295                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
296                  (k.stop is None or k.stop >= 0))), \
297             "Negative indexing is not supported."
298 
299         if self._result_cache is not None:
300             return self._result_cache[k]
301 
302         if isinstance(k, slice):
303             qs = self._chain()
304             if k.start is not None:
305                 start = int(k.start)
306             else:
307                 start = None
308             if k.stop is not None:
309                 stop = int(k.stop)
310             else:
311                 stop = None
312             qs.query.set_limits(start, stop)
313             return list(qs)[::k.step] if k.step else qs
314 
315         qs = self._chain()
316         qs.query.set_limits(k, k + 1)
317         qs._fetch_all()
318         return qs._result_cache[0]
319 
320     def __class_getitem__(cls, *args, **kwargs):
321         return cls
322 
323     def __and__(self, other):
324         self._merge_sanity_check(other)
325         if isinstance(other, EmptyQuerySet):
326             return other
327         if isinstance(self, EmptyQuerySet):
328             return self
329         combined = self._chain()
330         combined._merge_known_related_objects(other)
331         combined.query.combine(other.query, sql.AND)
332         return combined
333 
334     def __or__(self, other):
335         self._merge_sanity_check(other)
336         if isinstance(self, EmptyQuerySet):
337             return other
338         if isinstance(other, EmptyQuerySet):
339             return self
340         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
341         combined = query._chain()
342         combined._merge_known_related_objects(other)
343         if not other.query.can_filter():
344             other = other.model._base_manager.filter(pk__in=other.values('pk'))
345         combined.query.combine(other.query, sql.OR)
346         return combined
347 
348     ####################################
349     # METHODS THAT DO DATABASE QUERIES #
350     ####################################
351 
352     def _iterator(self, use_chunked_fetch, chunk_size):
353         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
354 
355     def iterator(self, chunk_size=2000):
356         """
357         An iterator over the results from applying this QuerySet to the
358         database.
359         """
360         if chunk_size <= 0:
361             raise ValueError('Chunk size must be strictly positive.')
362         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
363         return self._iterator(use_chunked_fetch, chunk_size)
364 
365     def aggregate(self, *args, **kwargs):
366         """
367         Return a dictionary containing the calculations (aggregation)
368         over the current queryset.
369 
370         If args is present the expression is passed as a kwarg using
371         the Aggregate object's default alias.
372         """
373         if self.query.distinct_fields:
374             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
375         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
376         for arg in args:
377             # The default_alias property raises TypeError if default_alias
378             # can't be set automatically or AttributeError if it isn't an
379             # attribute.
380             try:
381                 arg.default_alias
382             except (AttributeError, TypeError):
383                 raise TypeError("Complex aggregates require an alias")
384             kwargs[arg.default_alias] = arg
385 
386         query = self.query.chain()
387         for (alias, aggregate_expr) in kwargs.items():
388             query.add_annotation(aggregate_expr, alias, is_summary=True)
389             if not query.annotations[alias].contains_aggregate:
390                 raise TypeError("%s is not an aggregate expression" % alias)
391         return query.get_aggregation(self.db, kwargs)
392 
393     def count(self):
394         """
395         Perform a SELECT COUNT() and return the number of records as an
396         integer.
397 
398         If the QuerySet is already fully cached, return the length of the
399         cached results set to avoid multiple SELECT COUNT(*) calls.
400         """
401         if self._result_cache is not None:
402             return len(self._result_cache)
403 
404         return self.query.get_count(using=self.db)
405 
406     def get(self, *args, **kwargs):
407         """
408         Perform the query and return a single object matching the given
409         keyword arguments.
410         """
411         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
412         if self.query.can_filter() and not self.query.distinct_fields:
413             clone = clone.order_by()
414         limit = None
415         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
416             limit = MAX_GET_RESULTS
417             clone.query.set_limits(high=limit)
418         num = len(clone)
419         if num == 1:
420             return clone._result_cache[0]
421         if not num:
422             raise self.model.DoesNotExist(
423                 "%s matching query does not exist." %
424                 self.model._meta.object_name
425             )
426         raise self.model.MultipleObjectsReturned(
427             'get() returned more than one %s -- it returned %s!' % (
428                 self.model._meta.object_name,
429                 num if not limit or num < limit else 'more than %s' % (limit - 1),
430             )
431         )
432 
433     def create(self, **kwargs):
434         """
435         Create a new object with the given kwargs, saving it to the database
436         and returning the created object.
437         """
438         obj = self.model(**kwargs)
439         self._for_write = True
440         obj.save(force_insert=True, using=self.db)
441         return obj
442 
443     def _populate_pk_values(self, objs):
444         for obj in objs:
445             if obj.pk is None:
446                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
447 
448     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
449         """
450         Insert each of the instances into the database. Do *not* call
451         save() on each of the instances, do not send any pre/post_save
452         signals, and do not set the primary key attribute if it is an
453         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
454         Multi-table models are not supported.
455         """
456         # When you bulk insert you don't get the primary keys back (if it's an
457         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
458         # you can't insert into the child tables which references this. There
459         # are two workarounds:
460         # 1) This could be implemented if you didn't have an autoincrement pk
461         # 2) You could do it by doing O(n) normal inserts into the parent
462         #    tables to get the primary keys back and then doing a single bulk
463         #    insert into the childmost table.
464         # We currently set the primary keys on the objects when using
465         # PostgreSQL via the RETURNING ID clause. It should be possible for
466         # Oracle as well, but the semantics for extracting the primary keys is
467         # trickier so it's not done yet.
468         assert batch_size is None or batch_size > 0
469         # Check that the parents share the same concrete model with the our
470         # model to detect the inheritance pattern ConcreteGrandParent ->
471         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
472         # would not identify that case as involving multiple tables.
473         for parent in self.model._meta.get_parent_list():
474             if parent._meta.concrete_model is not self.model._meta.concrete_model:
475                 raise ValueError("Can't bulk create a multi-table inherited model")
476         if not objs:
477             return objs
478         self._for_write = True
479         connection = connections[self.db]
480         opts = self.model._meta
481         fields = opts.concrete_fields
482         objs = list(objs)
483         self._populate_pk_values(objs)
484         with transaction.atomic(using=self.db, savepoint=False):
485             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
486             if objs_with_pk:
487                 returned_columns = self._batched_insert(
488                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
489                 )
490                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
491                     for result, field in zip(results, opts.db_returning_fields):
492                         if field != opts.pk:
493                             setattr(obj_with_pk, field.attname, result)
494                 for obj_with_pk in objs_with_pk:
495                     obj_with_pk._state.adding = False
496                     obj_with_pk._state.db = self.db
497             if objs_without_pk:
498                 fields = [f for f in fields if not isinstance(f, AutoField)]
499                 returned_columns = self._batched_insert(
500                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
501                 )
502                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
503                     assert len(returned_columns) == len(objs_without_pk)
504                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
505                     for result, field in zip(results, opts.db_returning_fields):
506                         setattr(obj_without_pk, field.attname, result)
507                     obj_without_pk._state.adding = False
508                     obj_without_pk._state.db = self.db
509 
510         return objs
511 
512     def bulk_update(self, objs, fields, batch_size=None):
513         """
514         Update the given fields in each of the given objects in the database.
515         """
516         if batch_size is not None and batch_size < 0:
517             raise ValueError('Batch size must be a positive integer.')
518         if not fields:
519             raise ValueError('Field names must be given to bulk_update().')
520         objs = tuple(objs)
521         if any(obj.pk is None for obj in objs):
522             raise ValueError('All bulk_update() objects must have a primary key set.')
523         fields = [self.model._meta.get_field(name) for name in fields]
524         if any(not f.concrete or f.many_to_many for f in fields):
525             raise ValueError('bulk_update() can only be used with concrete fields.')
526         if any(f.primary_key for f in fields):
527             raise ValueError('bulk_update() cannot be used with primary key fields.')
528         if not objs:
529             return
530         # PK is used twice in the resulting update query, once in the filter
531         # and once in the WHEN. Each field will also have one CAST.
532         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
533         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
534         requires_casting = connections[self.db].features.requires_casted_case_in_updates
535         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
536         updates = []
537         for batch_objs in batches:
538             update_kwargs = {}
539             for field in fields:
540                 when_statements = []
541                 for obj in batch_objs:
542                     attr = getattr(obj, field.attname)
543                     if not isinstance(attr, Expression):
544                         attr = Value(attr, output_field=field)
545                     when_statements.append(When(pk=obj.pk, then=attr))
546                 case_statement = Case(*when_statements, output_field=field)
547                 if requires_casting:
548                     case_statement = Cast(case_statement, output_field=field)
549                 update_kwargs[field.attname] = case_statement
550             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
551         with transaction.atomic(using=self.db, savepoint=False):
552             for pks, update_kwargs in updates:
553                 self.filter(pk__in=pks).update(**update_kwargs)
554     bulk_update.alters_data = True
555 
556     def get_or_create(self, defaults=None, **kwargs):
557         """
558         Look up an object with the given kwargs, creating one if necessary.
559         Return a tuple of (object, created), where created is a boolean
560         specifying whether an object was created.
561         """
562         # The get() needs to be targeted at the write database in order
563         # to avoid potential transaction consistency problems.
564         self._for_write = True
565         try:
566             return self.get(**kwargs), False
567         except self.model.DoesNotExist:
568             params = self._extract_model_params(defaults, **kwargs)
569             # Try to create an object using passed params.
570             try:
571                 with transaction.atomic(using=self.db):
572                     params = dict(resolve_callables(params))
573                     return self.create(**params), True
574             except IntegrityError:
575                 try:
576                     return self.get(**kwargs), False
577                 except self.model.DoesNotExist:
578                     pass
579                 raise
580 
581     def update_or_create(self, defaults=None, **kwargs):
582         """
583         Look up an object with the given kwargs, updating one with defaults
584         if it exists, otherwise create a new one.
585         Return a tuple (object, created), where created is a boolean
586         specifying whether an object was created.
587         """
588         defaults = defaults or {}
589         self._for_write = True
590         with transaction.atomic(using=self.db):
591             # Lock the row so that a concurrent update is blocked until
592             # update_or_create() has performed its save.
593             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
594             if created:
595                 return obj, created
596             for k, v in resolve_callables(defaults):
597                 setattr(obj, k, v)
598             obj.save(using=self.db)
599         return obj, False
600 
601     def _extract_model_params(self, defaults, **kwargs):
602         """
603         Prepare `params` for creating a model instance based on the given
604         kwargs; for use by get_or_create().
605         """
606         defaults = defaults or {}
607         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
608         params.update(defaults)
609         property_names = self.model._meta._property_names
610         invalid_params = []
611         for param in params:
612             try:
613                 self.model._meta.get_field(param)
614             except exceptions.FieldDoesNotExist:
615                 # It's okay to use a model's property if it has a setter.
616                 if not (param in property_names and getattr(self.model, param).fset):
617                     invalid_params.append(param)
618         if invalid_params:
619             raise exceptions.FieldError(
620                 "Invalid field name(s) for model %s: '%s'." % (
621                     self.model._meta.object_name,
622                     "', '".join(sorted(invalid_params)),
623                 ))
624         return params
625 
626     def _earliest(self, *fields):
627         """
628         Return the earliest object according to fields (if given) or by the
629         model's Meta.get_latest_by.
630         """
631         if fields:
632             order_by = fields
633         else:
634             order_by = getattr(self.model._meta, 'get_latest_by')
635             if order_by and not isinstance(order_by, (tuple, list)):
636                 order_by = (order_by,)
637         if order_by is None:
638             raise ValueError(
639                 "earliest() and latest() require either fields as positional "
640                 "arguments or 'get_latest_by' in the model's Meta."
641             )
642 
643         assert not self.query.is_sliced, \
644             "Cannot change a query once a slice has been taken."
645         obj = self._chain()
646         obj.query.set_limits(high=1)
647         obj.query.clear_ordering(force_empty=True)
648         obj.query.add_ordering(*order_by)
649         return obj.get()
650 
651     def earliest(self, *fields):
652         return self._earliest(*fields)
653 
654     def latest(self, *fields):
655         return self.reverse()._earliest(*fields)
656 
657     def first(self):
658         """Return the first object of a query or None if no match is found."""
659         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
660             return obj
661 
662     def last(self):
663         """Return the last object of a query or None if no match is found."""
664         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
665             return obj
666 
667     def in_bulk(self, id_list=None, *, field_name='pk'):
668         """
669         Return a dictionary mapping each of the given IDs to the object with
670         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
671         """
672         assert not self.query.is_sliced, \
673             "Cannot use 'limit' or 'offset' with in_bulk"
674         opts = self.model._meta
675         unique_fields = [
676             constraint.fields[0]
677             for constraint in opts.total_unique_constraints
678             if len(constraint.fields) == 1
679         ]
680         if (
681             field_name != 'pk' and
682             not opts.get_field(field_name).unique and
683             field_name not in unique_fields and
684             not self.query.distinct_fields == (field_name,)
685         ):
686             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
687         if id_list is not None:
688             if not id_list:
689                 return {}
690             filter_key = '{}__in'.format(field_name)
691             batch_size = connections[self.db].features.max_query_params
692             id_list = tuple(id_list)
693             # If the database has a limit on the number of query parameters
694             # (e.g. SQLite), retrieve objects in batches if necessary.
695             if batch_size and batch_size < len(id_list):
696                 qs = ()
697                 for offset in range(0, len(id_list), batch_size):
698                     batch = id_list[offset:offset + batch_size]
699                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
700             else:
701                 qs = self.filter(**{filter_key: id_list}).order_by()
702         else:
703             qs = self._chain()
704         return {getattr(obj, field_name): obj for obj in qs}
705 
706     def delete(self):
707         """Delete the records in the current QuerySet."""
708         self._not_support_combined_queries('delete')
709         assert not self.query.is_sliced, \
710             "Cannot use 'limit' or 'offset' with delete."
711 
712         if self._fields is not None:
713             raise TypeError("Cannot call delete() after .values() or .values_list()")
714 
715         del_query = self._chain()
716 
717         # The delete is actually 2 queries - one to find related objects,
718         # and one to delete. Make sure that the discovery of related
719         # objects is performed on the same database as the deletion.
720         del_query._for_write = True
721 
722         # Disable non-supported fields.
723         del_query.query.select_for_update = False
724         del_query.query.select_related = False
725         del_query.query.clear_ordering(force_empty=True)
726 
727         collector = Collector(using=del_query.db)
728         collector.collect(del_query)
729         deleted, _rows_count = collector.delete()
730 
731         # Clear the result cache, in case this QuerySet gets reused.
732         self._result_cache = None
733         return deleted, _rows_count
734 
735     delete.alters_data = True
736     delete.queryset_only = True
737 
738     def _raw_delete(self, using):
739         """
740         Delete objects found from the given queryset in single direct SQL
741         query. No signals are sent and there is no protection for cascades.
742         """
743         query = self.query.clone()
744         query.__class__ = sql.DeleteQuery
745         cursor = query.get_compiler(using).execute_sql(CURSOR)
746         if cursor:
747             with cursor:
748                 return cursor.rowcount
749         return 0
750     _raw_delete.alters_data = True
751 
752     def update(self, **kwargs):
753         """
754         Update all elements in the current QuerySet, setting all the given
755         fields to the appropriate values.
756         """
757         self._not_support_combined_queries('update')
758         assert not self.query.is_sliced, \
759             "Cannot update a query once a slice has been taken."
760         self._for_write = True
761         query = self.query.chain(sql.UpdateQuery)
762         query.add_update_values(kwargs)
763         # Clear any annotations so that they won't be present in subqueries.
764         query.annotations = {}
765         with transaction.mark_for_rollback_on_error(using=self.db):
766             rows = query.get_compiler(self.db).execute_sql(CURSOR)
767         self._result_cache = None
768         return rows
769     update.alters_data = True
770 
771     def _update(self, values):
772         """
773         A version of update() that accepts field objects instead of field names.
774         Used primarily for model saving and not intended for use by general
775         code (it requires too much poking around at model internals to be
776         useful at that level).
777         """
778         assert not self.query.is_sliced, \
779             "Cannot update a query once a slice has been taken."
780         query = self.query.chain(sql.UpdateQuery)
781         query.add_update_fields(values)
782         # Clear any annotations so that they won't be present in subqueries.
783         query.annotations = {}
784         self._result_cache = None
785         return query.get_compiler(self.db).execute_sql(CURSOR)
786     _update.alters_data = True
787     _update.queryset_only = False
788 
789     def exists(self):
790         if self._result_cache is None:
791             return self.query.has_results(using=self.db)
792         return bool(self._result_cache)
793 
794     def _prefetch_related_objects(self):
795         # This method can only be called once the result cache has been filled.
796         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
797         self._prefetch_done = True
798 
799     def explain(self, *, format=None, **options):
800         return self.query.explain(using=self.db, format=format, **options)
801 
802     ##################################################
803     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
804     ##################################################
805 
806     def raw(self, raw_query, params=None, translations=None, using=None):
807         if using is None:
808             using = self.db
809         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
810         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
811         return qs
812 
813     def _values(self, *fields, **expressions):
814         clone = self._chain()
815         if expressions:
816             clone = clone.annotate(**expressions)
817         clone._fields = fields
818         clone.query.set_values(fields)
819         return clone
820 
821     def values(self, *fields, **expressions):
822         fields += tuple(expressions)
823         clone = self._values(*fields, **expressions)
824         clone._iterable_class = ValuesIterable
825         return clone
826 
827     def values_list(self, *fields, flat=False, named=False):
828         if flat and named:
829             raise TypeError("'flat' and 'named' can't be used together.")
830         if flat and len(fields) > 1:
831             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
832 
833         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
834         _fields = []
835         expressions = {}
836         counter = 1
837         for field in fields:
838             if hasattr(field, 'resolve_expression'):
839                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
840                 while True:
841                     field_id = field_id_prefix + str(counter)
842                     counter += 1
843                     if field_id not in field_names:
844                         break
845                 expressions[field_id] = field
846                 _fields.append(field_id)
847             else:
848                 _fields.append(field)
849 
850         clone = self._values(*_fields, **expressions)
851         clone._iterable_class = (
852             NamedValuesListIterable if named
853             else FlatValuesListIterable if flat
854             else ValuesListIterable
855         )
856         return clone
857 
858     def dates(self, field_name, kind, order='ASC'):
859         """
860         Return a list of date objects representing all available dates for
861         the given field_name, scoped to 'kind'.
862         """
863         assert kind in ('year', 'month', 'week', 'day'), \
864             "'kind' must be one of 'year', 'month', 'week', or 'day'."
865         assert order in ('ASC', 'DESC'), \
866             "'order' must be either 'ASC' or 'DESC'."
867         return self.annotate(
868             datefield=Trunc(field_name, kind, output_field=DateField()),
869             plain_field=F(field_name)
870         ).values_list(
871             'datefield', flat=True
872         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
873 
874     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
875         """
876         Return a list of datetime objects representing all available
877         datetimes for the given field_name, scoped to 'kind'.
878         """
879         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
880             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
881         assert order in ('ASC', 'DESC'), \
882             "'order' must be either 'ASC' or 'DESC'."
883         if settings.USE_TZ:
884             if tzinfo is None:
885                 tzinfo = timezone.get_current_timezone()
886         else:
887             tzinfo = None
888         return self.annotate(
889             datetimefield=Trunc(
890                 field_name,
891                 kind,
892                 output_field=DateTimeField(),
893                 tzinfo=tzinfo,
894                 is_dst=is_dst,
895             ),
896             plain_field=F(field_name)
897         ).values_list(
898             'datetimefield', flat=True
899         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
900 
901     def none(self):
902         """Return an empty QuerySet."""
903         clone = self._chain()
904         clone.query.set_empty()
905         return clone
906 
907     ##################################################################
908     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
909     ##################################################################
910 
911     def all(self):
912         """
913         Return a new QuerySet that is a copy of the current one. This allows a
914         QuerySet to proxy for a model manager in some cases.
915         """
916         return self._chain()
917 
918     def filter(self, *args, **kwargs):
919         """
920         Return a new QuerySet instance with the args ANDed to the existing
921         set.
922         """
923         self._not_support_combined_queries('filter')
924         return self._filter_or_exclude(False, args, kwargs)
925 
926     def exclude(self, *args, **kwargs):
927         """
928         Return a new QuerySet instance with NOT (args) ANDed to the existing
929         set.
930         """
931         self._not_support_combined_queries('exclude')
932         return self._filter_or_exclude(True, args, kwargs)
933 
934     def _filter_or_exclude(self, negate, args, kwargs):
935         if args or kwargs:
936             assert not self.query.is_sliced, \
937                 "Cannot filter a query once a slice has been taken."
938 
939         clone = self._chain()
940         if self._defer_next_filter:
941             self._defer_next_filter = False
942             clone._deferred_filter = negate, args, kwargs
943         else:
944             clone._filter_or_exclude_inplace(negate, args, kwargs)
945         return clone
946 
947     def _filter_or_exclude_inplace(self, negate, args, kwargs):
948         if negate:
949             self._query.add_q(~Q(*args, **kwargs))
950         else:
951             self._query.add_q(Q(*args, **kwargs))
952 
953     def complex_filter(self, filter_obj):
954         """
955         Return a new QuerySet instance with filter_obj added to the filters.
956 
957         filter_obj can be a Q object or a dictionary of keyword lookup
958         arguments.
959 
960         This exists to support framework features such as 'limit_choices_to',
961         and usually it will be more natural to use other methods.
962         """
963         if isinstance(filter_obj, Q):
964             clone = self._chain()
965             clone.query.add_q(filter_obj)
966             return clone
967         else:
968             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
969 
970     def _combinator_query(self, combinator, *other_qs, all=False):
971         # Clone the query to inherit the select list and everything
972         clone = self._chain()
973         # Clear limits and ordering so they can be reapplied
974         clone.query.clear_ordering(True)
975         clone.query.clear_limits()
976         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
977         clone.query.combinator = combinator
978         clone.query.combinator_all = all
979         return clone
980 
981     def union(self, *other_qs, all=False):
982         # If the query is an EmptyQuerySet, combine all nonempty querysets.
983         if isinstance(self, EmptyQuerySet):
984             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
985             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
986         return self._combinator_query('union', *other_qs, all=all)
987 
988     def intersection(self, *other_qs):
989         # If any query is an EmptyQuerySet, return it.
990         if isinstance(self, EmptyQuerySet):
991             return self
992         for other in other_qs:
993             if isinstance(other, EmptyQuerySet):
994                 return other
995         return self._combinator_query('intersection', *other_qs)
996 
997     def difference(self, *other_qs):
998         # If the query is an EmptyQuerySet, return it.
999         if isinstance(self, EmptyQuerySet):
1000             return self
1001         return self._combinator_query('difference', *other_qs)
1002 
1003     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1004         """
1005         Return a new QuerySet instance that will select objects with a
1006         FOR UPDATE lock.
1007         """
1008         if nowait and skip_locked:
1009             raise ValueError('The nowait option cannot be used with skip_locked.')
1010         obj = self._chain()
1011         obj._for_write = True
1012         obj.query.select_for_update = True
1013         obj.query.select_for_update_nowait = nowait
1014         obj.query.select_for_update_skip_locked = skip_locked
1015         obj.query.select_for_update_of = of
1016         obj.query.select_for_no_key_update = no_key
1017         return obj
1018 
1019     def select_related(self, *fields):
1020         """
1021         Return a new QuerySet instance that will select related objects.
1022 
1023         If fields are specified, they must be ForeignKey fields and only those
1024         related objects are included in the selection.
1025 
1026         If select_related(None) is called, clear the list.
1027         """
1028         self._not_support_combined_queries('select_related')
1029         if self._fields is not None:
1030             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1031 
1032         obj = self._chain()
1033         if fields == (None,):
1034             obj.query.select_related = False
1035         elif fields:
1036             obj.query.add_select_related(fields)
1037         else:
1038             obj.query.select_related = True
1039         return obj
1040 
1041     def prefetch_related(self, *lookups):
1042         """
1043         Return a new QuerySet instance that will prefetch the specified
1044         Many-To-One and Many-To-Many related objects when the QuerySet is
1045         evaluated.
1046 
1047         When prefetch_related() is called more than once, append to the list of
1048         prefetch lookups. If prefetch_related(None) is called, clear the list.
1049         """
1050         self._not_support_combined_queries('prefetch_related')
1051         clone = self._chain()
1052         if lookups == (None,):
1053             clone._prefetch_related_lookups = ()
1054         else:
1055             for lookup in lookups:
1056                 if isinstance(lookup, Prefetch):
1057                     lookup = lookup.prefetch_to
1058                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1059                 if lookup in self.query._filtered_relations:
1060                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1061             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1062         return clone
1063 
1064     def annotate(self, *args, **kwargs):
1065         """
1066         Return a query set in which the returned objects have been annotated
1067         with extra data or aggregations.
1068         """
1069         self._not_support_combined_queries('annotate')
1070         return self._annotate(args, kwargs, select=True)
1071 
1072     def alias(self, *args, **kwargs):
1073         """
1074         Return a query set with added aliases for extra data or aggregations.
1075         """
1076         self._not_support_combined_queries('alias')
1077         return self._annotate(args, kwargs, select=False)
1078 
1079     def _annotate(self, args, kwargs, select=True):
1080         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1081         annotations = {}
1082         for arg in args:
1083             # The default_alias property may raise a TypeError.
1084             try:
1085                 if arg.default_alias in kwargs:
1086                     raise ValueError("The named annotation '%s' conflicts with the "
1087                                      "default name for another annotation."
1088                                      % arg.default_alias)
1089             except TypeError:
1090                 raise TypeError("Complex annotations require an alias")
1091             annotations[arg.default_alias] = arg
1092         annotations.update(kwargs)
1093 
1094         clone = self._chain()
1095         names = self._fields
1096         if names is None:
1097             names = set(chain.from_iterable(
1098                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1099                 for field in self.model._meta.get_fields()
1100             ))
1101 
1102         for alias, annotation in annotations.items():
1103             if alias in names:
1104                 raise ValueError("The annotation '%s' conflicts with a field on "
1105                                  "the model." % alias)
1106             if isinstance(annotation, FilteredRelation):
1107                 clone.query.add_filtered_relation(annotation, alias)
1108             else:
1109                 clone.query.add_annotation(
1110                     annotation, alias, is_summary=False, select=select,
1111                 )
1112         for alias, annotation in clone.query.annotations.items():
1113             if alias in annotations and annotation.contains_aggregate:
1114                 if clone._fields is None:
1115                     clone.query.group_by = True
1116                 else:
1117                     clone.query.set_group_by()
1118                 break
1119 
1120         return clone
1121 
1122     def order_by(self, *field_names):
1123         """Return a new QuerySet instance with the ordering changed."""
1124         assert not self.query.is_sliced, \
1125             "Cannot reorder a query once a slice has been taken."
1126         obj = self._chain()
1127         obj.query.clear_ordering(force_empty=False)
1128         obj.query.add_ordering(*field_names)
1129         return obj
1130 
1131     def distinct(self, *field_names):
1132         """
1133         Return a new QuerySet instance that will select only distinct results.
1134         """
1135         self._not_support_combined_queries('distinct')
1136         assert not self.query.is_sliced, \
1137             "Cannot create distinct fields once a slice has been taken."
1138         obj = self._chain()
1139         obj.query.add_distinct_fields(*field_names)
1140         return obj
1141 
1142     def extra(self, select=None, where=None, params=None, tables=None,
1143               order_by=None, select_params=None):
1144         """Add extra SQL fragments to the query."""
1145         self._not_support_combined_queries('extra')
1146         assert not self.query.is_sliced, \
1147             "Cannot change a query once a slice has been taken"
1148         clone = self._chain()
1149         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1150         return clone
1151 
1152     def reverse(self):
1153         """Reverse the ordering of the QuerySet."""
1154         if self.query.is_sliced:
1155             raise TypeError('Cannot reverse a query once a slice has been taken.')
1156         clone = self._chain()
1157         clone.query.standard_ordering = not clone.query.standard_ordering
1158         return clone
1159 
1160     def defer(self, *fields):
1161         """
1162         Defer the loading of data for certain fields until they are accessed.
1163         Add the set of deferred fields to any existing set of deferred fields.
1164         The only exception to this is if None is passed in as the only
1165         parameter, in which case removal all deferrals.
1166         """
1167         self._not_support_combined_queries('defer')
1168         if self._fields is not None:
1169             raise TypeError("Cannot call defer() after .values() or .values_list()")
1170         clone = self._chain()
1171         if fields == (None,):
1172             clone.query.clear_deferred_loading()
1173         else:
1174             clone.query.add_deferred_loading(fields)
1175         return clone
1176 
1177     def only(self, *fields):
1178         """
1179         Essentially, the opposite of defer(). Only the fields passed into this
1180         method and that are not already specified as deferred are loaded
1181         immediately when the queryset is evaluated.
1182         """
1183         self._not_support_combined_queries('only')
1184         if self._fields is not None:
1185             raise TypeError("Cannot call only() after .values() or .values_list()")
1186         if fields == (None,):
1187             # Can only pass None to defer(), not only(), as the rest option.
1188             # That won't stop people trying to do this, so let's be explicit.
1189             raise TypeError("Cannot pass None as an argument to only().")
1190         for field in fields:
1191             field = field.split(LOOKUP_SEP, 1)[0]
1192             if field in self.query._filtered_relations:
1193                 raise ValueError('only() is not supported with FilteredRelation.')
1194         clone = self._chain()
1195         clone.query.add_immediate_loading(fields)
1196         return clone
1197 
1198     def using(self, alias):
1199         """Select which database this QuerySet should execute against."""
1200         clone = self._chain()
1201         clone._db = alias
1202         return clone
1203 
1204     ###################################
1205     # PUBLIC INTROSPECTION ATTRIBUTES #
1206     ###################################
1207 
1208     @property
1209     def ordered(self):
1210         """
1211         Return True if the QuerySet is ordered -- i.e. has an order_by()
1212         clause or a default ordering on the model (or is empty).
1213         """
1214         if isinstance(self, EmptyQuerySet):
1215             return True
1216         if self.query.extra_order_by or self.query.order_by:
1217             return True
1218         elif (
1219             self.query.default_ordering and
1220             self.query.get_meta().ordering and
1221             # A default ordering doesn't affect GROUP BY queries.
1222             not self.query.group_by
1223         ):
1224             return True
1225         else:
1226             return False
1227 
1228     @property
1229     def db(self):
1230         """Return the database used if this query is executed now."""
1231         if self._for_write:
1232             return self._db or router.db_for_write(self.model, **self._hints)
1233         return self._db or router.db_for_read(self.model, **self._hints)
1234 
1235     ###################
1236     # PRIVATE METHODS #
1237     ###################
1238 
1239     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1240         """
1241         Insert a new record for the given model. This provides an interface to
1242         the InsertQuery class and is how Model.save() is implemented.
1243         """
1244         self._for_write = True
1245         if using is None:
1246             using = self.db
1247         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1248         query.insert_values(fields, objs, raw=raw)
1249         return query.get_compiler(using=using).execute_sql(returning_fields)
1250     _insert.alters_data = True
1251     _insert.queryset_only = False
1252 
1253     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1254         """
1255         Helper method for bulk_create() to insert objs one batch at a time.
1256         """
1257         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1258             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1259         ops = connections[self.db].ops
1260         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1261         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1262         inserted_rows = []
1263         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1264         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1265             if bulk_return and not ignore_conflicts:
1266                 inserted_rows.extend(self._insert(
1267                     item, fields=fields, using=self.db,
1268                     returning_fields=self.model._meta.db_returning_fields,
1269                     ignore_conflicts=ignore_conflicts,
1270                 ))
1271             else:
1272                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1273         return inserted_rows
1274 
1275     def _chain(self, **kwargs):
1276         """
1277         Return a copy of the current QuerySet that's ready for another
1278         operation.
1279         """
1280         obj = self._clone()
1281         if obj._sticky_filter:
1282             obj.query.filter_is_sticky = True
1283             obj._sticky_filter = False
1284         obj.__dict__.update(kwargs)
1285         return obj
1286 
1287     def _clone(self):
1288         """
1289         Return a copy of the current QuerySet. A lightweight alternative
1290         to deepcopy().
1291         """
1292         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1293         c._sticky_filter = self._sticky_filter
1294         c._for_write = self._for_write
1295         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1296         c._known_related_objects = self._known_related_objects
1297         c._iterable_class = self._iterable_class
1298         c._fields = self._fields
1299         return c
1300 
1301     def _fetch_all(self):
1302         if self._result_cache is None:
1303             self._result_cache = list(self._iterable_class(self))
1304         if self._prefetch_related_lookups and not self._prefetch_done:
1305             self._prefetch_related_objects()
1306 
1307     def _next_is_sticky(self):
1308         """
1309         Indicate that the next filter call and the one following that should
1310         be treated as a single filter. This is only important when it comes to
1311         determining when to reuse tables for many-to-many filters. Required so
1312         that we can filter naturally on the results of related managers.
1313 
1314         This doesn't return a clone of the current QuerySet (it returns
1315         "self"). The method is only used internally and should be immediately
1316         followed by a filter() that does create a clone.
1317         """
1318         self._sticky_filter = True
1319         return self
1320 
1321     def _merge_sanity_check(self, other):
1322         """Check that two QuerySet classes may be merged."""
1323         if self._fields is not None and (
1324                 set(self.query.values_select) != set(other.query.values_select) or
1325                 set(self.query.extra_select) != set(other.query.extra_select) or
1326                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1327             raise TypeError(
1328                 "Merging '%s' classes must involve the same values in each case."
1329                 % self.__class__.__name__
1330             )
1331 
1332     def _merge_known_related_objects(self, other):
1333         """
1334         Keep track of all known related objects from either QuerySet instance.
1335         """
1336         for field, objects in other._known_related_objects.items():
1337             self._known_related_objects.setdefault(field, {}).update(objects)
1338 
1339     def resolve_expression(self, *args, **kwargs):
1340         if self._fields and len(self._fields) > 1:
1341             # values() queryset can only be used as nested queries
1342             # if they are set up to select only a single field.
1343             raise TypeError('Cannot use multi-field values as a filter value.')
1344         query = self.query.resolve_expression(*args, **kwargs)
1345         query._db = self._db
1346         return query
1347     resolve_expression.queryset_only = True
1348 
1349     def _add_hints(self, **hints):
1350         """
1351         Update hinting information for use by routers. Add new key/values or
1352         overwrite existing key/values.
1353         """
1354         self._hints.update(hints)
1355 
1356     def _has_filters(self):
1357         """
1358         Check if this QuerySet has any filtering going on. This isn't
1359         equivalent with checking if all objects are present in results, for
1360         example, qs[1:]._has_filters() -> False.
1361         """
1362         return self.query.has_filters()
1363 
1364     @staticmethod
1365     def _validate_values_are_expressions(values, method_name):
1366         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1367         if invalid_args:
1368             raise TypeError(
1369                 'QuerySet.%s() received non-expression(s): %s.' % (
1370                     method_name,
1371                     ', '.join(invalid_args),
1372                 )
1373             )
1374 
1375     def _not_support_combined_queries(self, operation_name):
1376         if self.query.combinator:
1377             raise NotSupportedError(
1378                 'Calling QuerySet.%s() after %s() is not supported.'
1379                 % (operation_name, self.query.combinator)
1380             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-15 05:18:31.491 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-15 05:18:31.491 | INFO     | Search-only: skipping patch selection
2025-10-15 05:18:31.646 | INFO     | Task django__django-13431 completed successfully.
2025-10-15 05:18:31.647 | INFO     | No patch generated. You can try running ACR again.
