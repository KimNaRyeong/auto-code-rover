2025-10-18 05:32:04.235 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 05:32:07.980 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=627, output_tokens=12, cost=0.006630
2025-10-18 05:32:15.806 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=807, output_tokens=367, cost=0.019080
2025-10-18 05:32:16.864 | INFO     | registered non reproducing test 0
2025-10-18 05:32:30.219 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2002, output_tokens=558, cost=0.036760
2025-10-18 05:32:31.266 | INFO     | registered non reproducing test 1
2025-10-18 05:32:45.854 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2196, output_tokens=636, cost=0.041040
2025-10-18 05:32:46.895 | INFO     | registered non reproducing test 2
2025-10-18 05:32:46.895 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 05:32:46.901 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:32:50.454 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=925, output_tokens=123, cost=0.012940
2025-10-18 05:32:50.459 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:32:53.431 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=477, output_tokens=64, cost=0.006690
2025-10-18 05:32:53.432 | DEBUG    | Extracted a valid json.
2025-10-18 05:32:53.433 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"GradientBoostingRegressor\")",
        "search_method_in_class(\"fit\", \"GradientBoostingRegressor\")",
        "search_class(\"Pipeline\")",
        "search_method_in_class(\"fit\", \"Pipeline\")"
    ],
    "bug_locations": []
}
2025-10-18 05:32:53.478 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 05:32:53.479 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 05:33:07.781 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2156, output_tokens=631, cost=0.040490
2025-10-18 05:33:07.794 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:33:14.248 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3003, output_tokens=277, cost=0.038340
2025-10-18 05:33:14.254 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:33:20.910 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=631, output_tokens=201, cost=0.012340
2025-10-18 05:33:20.911 | DEBUG    | Extracted a valid json.
2025-10-18 05:33:20.911 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method_in_file(\"fit\", \"sklearn/ensemble/gradient_boosting.py\")"],
    "bug_locations": [
        {
            "file": "sklearn/ensemble/gradient_boosting.py",
            "class": "GradientBoostingRegressor",
            "method": "fit",
            "intended_behavior": "Adjust the call within either the `GradientBoostingRegressor` or a base class's `fit` method that involves the `init` estimator, to ensure the `sample_weight` is passed in a manner compatible with the `Pipeline.fit` signature. This likely means adjusting the mechanism to include `sample_weight` within `**fit_params` when `init` is a pipeline to preserve compatibility. The goal is to maintain the functionality of using pipelines as initial estimators in gradient boosting regressors while also preserving the existing functionality for other types of estimators or direct usage scenarios where `sample_weight` is applied appropriately."
        }
    ]
}
2025-10-18 05:33:21.037 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 05:33:21.038 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 05:33:35.277 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=8923, output_tokens=605, cost=0.107380
2025-10-18 05:33:35.289 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:33:46.469 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=9744, output_tokens=435, cost=0.110490
2025-10-18 05:33:46.478 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:33:54.978 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=789, output_tokens=351, cost=0.018420
2025-10-18 05:33:54.980 | DEBUG    | Extracted a valid json.
2025-10-18 05:33:54.980 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/ensemble/gradient_boosting.py",
            "class": "BaseGradientBoosting",
            "method": "fit",
            "intended_behavior": "A modification in the fit method of the BaseGradientBoosting class is required to ensure compatibility when using a Pipeline as the init_ estimator. Specifically, the handling of sample_weight needs to adapt based on whether the init_ estimator can directly accept sample_weight or if it should be passed within **fit_params. In lines 1479-1484, where the initial estimator is fitted with sample_weight, add a conditional check to determine if self.init_ is an instance of a Pipeline. If self.init_ is a Pipeline, refactor the call to fit to ensure sample_weight is passed within **fit_params. This might look something like: self.init_.fit(X, y, **{\"sample_weight\": sample_weight}) when sample_weight is not None. If self.init_ does not support sample_weight directly and is not a Pipeline (or does not support sample_weight in any form), proceed with existing fallback logic that attempts to call fit(X, y) without sample_weight. Ensure this behavior preserves existing functionality for estimators that do support sample_weight directly, and only modify the behavior for cases where sample_weight cannot be directly passed. This maintains backward compatibility. This enhancement would correct the issue where the initial fitting process fails due to a TypeError when sample_weight is passed as a positional argument to a Pipeline's fit method, aligning with the flexible nature of fit method signatures across different estimators and ensuring the GradientBoostingRegressor can effectively use pipelines as initial estimators."
        }
    ]
}
2025-10-18 05:33:54.987 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1436         n_samples, self.n_features_ = X.shape
1437 
1438         sample_weight_is_none = sample_weight is None
1439         if sample_weight_is_none:
1440             sample_weight = np.ones(n_samples, dtype=np.float32)
1441         else:
1442             sample_weight = column_or_1d(sample_weight, warn=True)
1443             sample_weight_is_none = False
1444 
1445         check_consistent_length(X, y, sample_weight)
1446 
1447         y = self._validate_y(y, sample_weight)
1448 
1449         if self.n_iter_no_change is not None:
1450             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1451                 train_test_split(X, y, sample_weight,
1452                                  random_state=self.random_state,
1453                                  test_size=self.validation_fraction))
1454             if is_classifier(self):
1455                 if self.n_classes_ != np.unique(y).shape[0]:
1456                     # We choose to error here. The problem is that the init
1457                     # estimator would be trained on y, which has some missing
1458                     # classes now, so its predictions would not have the
1459                     # correct shape.
1460                     raise ValueError(
1461                         'The training data after the early stopping split '
1462                         'is missing some classes. Try using another random '
1463                         'seed.'
1464                     )
1465         else:
1466             X_val = y_val = sample_weight_val = None
1467 
1468         self._check_params()
1469 
1470         if not self._is_initialized():
1471             # init state
1472             self._init_state()
1473 
1474             # fit initial model and initialize raw predictions
1475             if self.init_ == 'zero':
1476                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1477                                            dtype=np.float64)
1478             else:
1479                 try:
1480                     self.init_.fit(X, y, sample_weight=sample_weight)
1481                 except TypeError:
1482                     if sample_weight_is_none:
1483                         self.init_.fit(X, y)
1484                     else:
1485                         raise ValueError(
1486                             "The initial estimator {} does not support sample "
1487                             "weights.".format(self.init_.__class__.__name__))
1488 
1489                 raw_predictions = \
1490                     self.loss_.get_init_raw_predictions(X, self.init_)
1491 
1492 
1493             begin_at_stage = 0
1494 
1495             # The rng state must be preserved if warm_start is True
1496             self._rng = check_random_state(self.random_state)
1497 
1498         else:
1499             # add more estimators to fitted model
1500             # invariant: warm_start = True
1501             if self.n_estimators < self.estimators_.shape[0]:
1502                 raise ValueError('n_estimators=%d must be larger or equal to '
1503                                  'estimators_.shape[0]=%d when '
1504                                  'warm_start==True'
1505                                  % (self.n_estimators,
1506                                     self.estimators_.shape[0]))
1507             begin_at_stage = self.estimators_.shape[0]
1508             # The requirements of _decision_function (called in two lines
1509             # below) are more constrained than fit. It accepts only CSR
1510             # matrices.
1511             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1512             raw_predictions = self._raw_predict(X)
1513             self._resize_state()
1514 
1515         if self.presort is True and issparse(X):
1516             raise ValueError(
1517                 "Presorting is not supported for sparse matrices.")
1518 
1519         presort = self.presort
1520         # Allow presort to be 'auto', which means True if the dataset is dense,
1521         # otherwise it will be False.
1522         if presort == 'auto':
1523             presort = not issparse(X)
1524 
1525         X_idx_sorted = None
1526         if presort:
1527             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1528                                              dtype=np.int32)
1529 
1530         # fit the boosting stages
1531         n_stages = self._fit_stages(
1532             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1533             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1534 
1535         # change shape of arrays after fit (early-stopping or additional ests)
1536         if n_stages != self.estimators_.shape[0]:
1537             self.estimators_ = self.estimators_[:n_stages]
1538             self.train_score_ = self.train_score_[:n_stages]
1539             if hasattr(self, 'oob_improvement_'):
1540                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1541 
1542         self.n_estimators_ = n_stages
1543         return self

</code><intended_behavior>A modification in the fit method of the BaseGradientBoosting class is required to ensure compatibility when using a Pipeline as the init_ estimator. Specifically, the handling of sample_weight needs to adapt based on whether the init_ estimator can directly accept sample_weight or if it should be passed within **fit_params. In lines 1479-1484, where the initial estimator is fitted with sample_weight, add a conditional check to determine if self.init_ is an instance of a Pipeline. If self.init_ is a Pipeline, refactor the call to fit to ensure sample_weight is passed within **fit_params. This might look something like: self.init_.fit(X, y, **{"sample_weight": sample_weight}) when sample_weight is not None. If self.init_ does not support sample_weight directly and is not a Pipeline (or does not support sample_weight in any form), proceed with existing fallback logic that attempts to call fit(X, y) without sample_weight. Ensure this behavior preserves existing functionality for estimators that do support sample_weight directly, and only modify the behavior for cases where sample_weight cannot be directly passed. This maintains backward compatibility. This enhancement would correct the issue where the initial fitting process fails due to a TypeError when sample_weight is passed as a positional argument to a Pipeline's fit method, aligning with the flexible nature of fit method signatures across different estimators and ensuring the GradientBoostingRegressor can effectively use pipelines as initial estimators.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='auto',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.init = init
1192         self.random_state = random_state
1193         self.alpha = alpha
1194         self.verbose = verbose
1195         self.max_leaf_nodes = max_leaf_nodes
1196         self.warm_start = warm_start
1197         self.presort = presort
1198         self.validation_fraction = validation_fraction
1199         self.n_iter_no_change = n_iter_no_change
1200         self.tol = tol
1201 
1202     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1203                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1204         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1205 
1206         assert sample_mask.dtype == np.bool
1207         loss = self.loss_
1208         original_y = y
1209 
1210         # Need to pass a copy of raw_predictions to negative_gradient()
1211         # because raw_predictions is partially updated at the end of the loop
1212         # in update_terminal_regions(), and gradients need to be evaluated at
1213         # iteration i - 1.
1214         raw_predictions_copy = raw_predictions.copy()
1215 
1216         for k in range(loss.K):
1217             if loss.is_multi_class:
1218                 y = np.array(original_y == k, dtype=np.float64)
1219 
1220             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1221                                               sample_weight=sample_weight)
1222 
1223             # induce regression tree on residuals
1224             tree = DecisionTreeRegressor(
1225                 criterion=self.criterion,
1226                 splitter='best',
1227                 max_depth=self.max_depth,
1228                 min_samples_split=self.min_samples_split,
1229                 min_samples_leaf=self.min_samples_leaf,
1230                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1231                 min_impurity_decrease=self.min_impurity_decrease,
1232                 min_impurity_split=self.min_impurity_split,
1233                 max_features=self.max_features,
1234                 max_leaf_nodes=self.max_leaf_nodes,
1235                 random_state=random_state,
1236                 presort=self.presort)
1237 
1238             if self.subsample < 1.0:
1239                 # no inplace multiplication!
1240                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1241 
1242             X = X_csr if X_csr is not None else X
1243             tree.fit(X, residual, sample_weight=sample_weight,
1244                      check_input=False, X_idx_sorted=X_idx_sorted)
1245 
1246             # update tree leaves
1247             loss.update_terminal_regions(
1248                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1249                 sample_mask, learning_rate=self.learning_rate, k=k)
1250 
1251             # add tree to ensemble
1252             self.estimators_[i, k] = tree
1253 
1254         return raw_predictions
1255 
1256     def _check_params(self):
1257         """Check validity of parameters and raise ValueError if not valid. """
1258         if self.n_estimators <= 0:
1259             raise ValueError("n_estimators must be greater than 0 but "
1260                              "was %r" % self.n_estimators)
1261 
1262         if self.learning_rate <= 0.0:
1263             raise ValueError("learning_rate must be greater than 0 but "
1264                              "was %r" % self.learning_rate)
1265 
1266         if (self.loss not in self._SUPPORTED_LOSS
1267                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1268             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1269 
1270         if self.loss == 'deviance':
1271             loss_class = (_gb_losses.MultinomialDeviance
1272                           if len(self.classes_) > 2
1273                           else _gb_losses.BinomialDeviance)
1274         else:
1275             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1276 
1277         if self.loss in ('huber', 'quantile'):
1278             self.loss_ = loss_class(self.n_classes_, self.alpha)
1279         else:
1280             self.loss_ = loss_class(self.n_classes_)
1281 
1282         if not (0.0 < self.subsample <= 1.0):
1283             raise ValueError("subsample must be in (0,1] but "
1284                              "was %r" % self.subsample)
1285 
1286         if self.init is not None:
1287             # init must be an estimator or 'zero'
1288             if isinstance(self.init, BaseEstimator):
1289                 self.loss_.check_init_estimator(self.init)
1290             elif not (isinstance(self.init, str) and self.init == 'zero'):
1291                 raise ValueError(
1292                     "The init parameter must be an estimator or 'zero'. "
1293                     "Got init={}".format(self.init)
1294                 )
1295 
1296         if not (0.0 < self.alpha < 1.0):
1297             raise ValueError("alpha must be in (0.0, 1.0) but "
1298                              "was %r" % self.alpha)
1299 
1300         if isinstance(self.max_features, str):
1301             if self.max_features == "auto":
1302                 # if is_classification
1303                 if self.n_classes_ > 1:
1304                     max_features = max(1, int(np.sqrt(self.n_features_)))
1305                 else:
1306                     # is regression
1307                     max_features = self.n_features_
1308             elif self.max_features == "sqrt":
1309                 max_features = max(1, int(np.sqrt(self.n_features_)))
1310             elif self.max_features == "log2":
1311                 max_features = max(1, int(np.log2(self.n_features_)))
1312             else:
1313                 raise ValueError("Invalid value for max_features: %r. "
1314                                  "Allowed string values are 'auto', 'sqrt' "
1315                                  "or 'log2'." % self.max_features)
1316         elif self.max_features is None:
1317             max_features = self.n_features_
1318         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1319             max_features = self.max_features
1320         else:  # float
1321             if 0. < self.max_features <= 1.:
1322                 max_features = max(int(self.max_features *
1323                                        self.n_features_), 1)
1324             else:
1325                 raise ValueError("max_features must be in (0, n_features]")
1326 
1327         self.max_features_ = max_features
1328 
1329         if not isinstance(self.n_iter_no_change,
1330                           (numbers.Integral, np.integer, type(None))):
1331             raise ValueError("n_iter_no_change should either be None or an "
1332                              "integer. %r was passed"
1333                              % self.n_iter_no_change)
1334 
1335         allowed_presort = ('auto', True, False)
1336         if self.presort not in allowed_presort:
1337             raise ValueError("'presort' should be in {}. Got {!r} instead."
1338                              .format(allowed_presort, self.presort))
1339 
1340     def _init_state(self):
1341         """Initialize model state and allocate model state data structures. """
1342 
1343         self.init_ = self.init
1344         if self.init_ is None:
1345             self.init_ = self.loss_.init_estimator()
1346 
1347         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1348                                     dtype=np.object)
1349         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1350         # do oob?
1351         if self.subsample < 1.0:
1352             self.oob_improvement_ = np.zeros((self.n_estimators),
1353                                              dtype=np.float64)
1354 
1355     def _clear_state(self):
1356         """Clear the state of the gradient boosting model. """
1357         if hasattr(self, 'estimators_'):
1358             self.estimators_ = np.empty((0, 0), dtype=np.object)
1359         if hasattr(self, 'train_score_'):
1360             del self.train_score_
1361         if hasattr(self, 'oob_improvement_'):
1362             del self.oob_improvement_
1363         if hasattr(self, 'init_'):
1364             del self.init_
1365         if hasattr(self, '_rng'):
1366             del self._rng
1367 
1368     def _resize_state(self):
1369         """Add additional ``n_estimators`` entries to all attributes. """
1370         # self.n_estimators is the number of additional est to fit
1371         total_n_estimators = self.n_estimators
1372         if total_n_estimators < self.estimators_.shape[0]:
1373             raise ValueError('resize with smaller n_estimators %d < %d' %
1374                              (total_n_estimators, self.estimators_[0]))
1375 
1376         self.estimators_ = np.resize(self.estimators_,
1377                                      (total_n_estimators, self.loss_.K))
1378         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1379         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1380             # if do oob resize arrays or create new if not available
1381             if hasattr(self, 'oob_improvement_'):
1382                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1383                                                   total_n_estimators)
1384             else:
1385                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1386                                                  dtype=np.float64)
1387 
1388     def _is_initialized(self):
1389         return len(getattr(self, 'estimators_', [])) > 0
1390 
1391     def _check_initialized(self):
1392         """Check that the estimator is initialized, raising an error if not."""
1393         check_is_fitted(self, 'estimators_')
1394 
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1436         n_samples, self.n_features_ = X.shape
1437 
1438         sample_weight_is_none = sample_weight is None
1439         if sample_weight_is_none:
1440             sample_weight = np.ones(n_samples, dtype=np.float32)
1441         else:
1442             sample_weight = column_or_1d(sample_weight, warn=True)
1443             sample_weight_is_none = False
1444 
1445         check_consistent_length(X, y, sample_weight)
1446 
1447         y = self._validate_y(y, sample_weight)
1448 
1449         if self.n_iter_no_change is not None:
1450             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1451                 train_test_split(X, y, sample_weight,
1452                                  random_state=self.random_state,
1453                                  test_size=self.validation_fraction))
1454             if is_classifier(self):
1455                 if self.n_classes_ != np.unique(y).shape[0]:
1456                     # We choose to error here. The problem is that the init
1457                     # estimator would be trained on y, which has some missing
1458                     # classes now, so its predictions would not have the
1459                     # correct shape.
1460                     raise ValueError(
1461                         'The training data after the early stopping split '
1462                         'is missing some classes. Try using another random '
1463                         'seed.'
1464                     )
1465         else:
1466             X_val = y_val = sample_weight_val = None
1467 
1468         self._check_params()
1469 
1470         if not self._is_initialized():
1471             # init state
1472             self._init_state()
1473 
1474             # fit initial model and initialize raw predictions
1475             if self.init_ == 'zero':
1476                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1477                                            dtype=np.float64)
1478             else:
1479                 try:
1480                     self.init_.fit(X, y, sample_weight=sample_weight)
1481                 except TypeError:
1482                     if sample_weight_is_none:
1483                         self.init_.fit(X, y)
1484                     else:
1485                         raise ValueError(
1486                             "The initial estimator {} does not support sample "
1487                             "weights.".format(self.init_.__class__.__name__))
1488 
1489                 raw_predictions = \
1490                     self.loss_.get_init_raw_predictions(X, self.init_)
1491 
1492 
1493             begin_at_stage = 0
1494 
1495             # The rng state must be preserved if warm_start is True
1496             self._rng = check_random_state(self.random_state)
1497 
1498         else:
1499             # add more estimators to fitted model
1500             # invariant: warm_start = True
1501             if self.n_estimators < self.estimators_.shape[0]:
1502                 raise ValueError('n_estimators=%d must be larger or equal to '
1503                                  'estimators_.shape[0]=%d when '
1504                                  'warm_start==True'
1505                                  % (self.n_estimators,
1506                                     self.estimators_.shape[0]))
1507             begin_at_stage = self.estimators_.shape[0]
1508             # The requirements of _decision_function (called in two lines
1509             # below) are more constrained than fit. It accepts only CSR
1510             # matrices.
1511             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1512             raw_predictions = self._raw_predict(X)
1513             self._resize_state()
1514 
1515         if self.presort is True and issparse(X):
1516             raise ValueError(
1517                 "Presorting is not supported for sparse matrices.")
1518 
1519         presort = self.presort
1520         # Allow presort to be 'auto', which means True if the dataset is dense,
1521         # otherwise it will be False.
1522         if presort == 'auto':
1523             presort = not issparse(X)
1524 
1525         X_idx_sorted = None
1526         if presort:
1527             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1528                                              dtype=np.int32)
1529 
1530         # fit the boosting stages
1531         n_stages = self._fit_stages(
1532             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1533             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1534 
1535         # change shape of arrays after fit (early-stopping or additional ests)
1536         if n_stages != self.estimators_.shape[0]:
1537             self.estimators_ = self.estimators_[:n_stages]
1538             self.train_score_ = self.train_score_[:n_stages]
1539             if hasattr(self, 'oob_improvement_'):
1540                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1541 
1542         self.n_estimators_ = n_stages
1543         return self
1544 
1545     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1546                     X_val, y_val, sample_weight_val,
1547                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1548         """Iteratively fits the stages.
1549 
1550         For each stage it computes the progress (OOB, train score)
1551         and delegates to ``_fit_stage``.
1552         Returns the number of stages fit; might differ from ``n_estimators``
1553         due to early stopping.
1554         """
1555         n_samples = X.shape[0]
1556         do_oob = self.subsample < 1.0
1557         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1558         n_inbag = max(1, int(self.subsample * n_samples))
1559         loss_ = self.loss_
1560 
1561         # Set min_weight_leaf from min_weight_fraction_leaf
1562         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1563             min_weight_leaf = (self.min_weight_fraction_leaf *
1564                                np.sum(sample_weight))
1565         else:
1566             min_weight_leaf = 0.
1567 
1568         if self.verbose:
1569             verbose_reporter = VerboseReporter(self.verbose)
1570             verbose_reporter.init(self, begin_at_stage)
1571 
1572         X_csc = csc_matrix(X) if issparse(X) else None
1573         X_csr = csr_matrix(X) if issparse(X) else None
1574 
1575         if self.n_iter_no_change is not None:
1576             loss_history = np.full(self.n_iter_no_change, np.inf)
1577             # We create a generator to get the predictions for X_val after
1578             # the addition of each successive stage
1579             y_val_pred_iter = self._staged_raw_predict(X_val)
1580 
1581         # perform boosting iterations
1582         i = begin_at_stage
1583         for i in range(begin_at_stage, self.n_estimators):
1584 
1585             # subsampling
1586             if do_oob:
1587                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1588                                                   random_state)
1589                 # OOB score before adding this stage
1590                 old_oob_score = loss_(y[~sample_mask],
1591                                       raw_predictions[~sample_mask],
1592                                       sample_weight[~sample_mask])
1593 
1594             # fit next stage of trees
1595             raw_predictions = self._fit_stage(
1596                 i, X, y, raw_predictions, sample_weight, sample_mask,
1597                 random_state, X_idx_sorted, X_csc, X_csr)
1598 
1599             # track deviance (= loss)
1600             if do_oob:
1601                 self.train_score_[i] = loss_(y[sample_mask],
1602                                              raw_predictions[sample_mask],
1603                                              sample_weight[sample_mask])
1604                 self.oob_improvement_[i] = (
1605                     old_oob_score - loss_(y[~sample_mask],
1606                                           raw_predictions[~sample_mask],
1607                                           sample_weight[~sample_mask]))
1608             else:
1609                 # no need to fancy index w/ no subsampling
1610                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1611 
1612             if self.verbose > 0:
1613                 verbose_reporter.update(i, self)
1614 
1615             if monitor is not None:
1616                 early_stopping = monitor(i, self, locals())
1617                 if early_stopping:
1618                     break
1619 
1620             # We also provide an early stopping based on the score from
1621             # validation set (X_val, y_val), if n_iter_no_change is set
1622             if self.n_iter_no_change is not None:
1623                 # By calling next(y_val_pred_iter), we get the predictions
1624                 # for X_val after the addition of the current stage
1625                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1626                                         sample_weight_val)
1627 
1628                 # Require validation_score to be better (less) than at least
1629                 # one of the last n_iter_no_change evaluations
1630                 if np.any(validation_loss + self.tol < loss_history):
1631                     loss_history[i % len(loss_history)] = validation_loss
1632                 else:
1633                     break
1634 
1635         return i + 1
1636 
1637     def _make_estimator(self, append=True):
1638         # we don't need _make_estimator
1639         raise NotImplementedError()
1640 
1641     def _raw_predict_init(self, X):
1642         """Check input and compute raw predictions of the init estimtor."""
1643         self._check_initialized()
1644         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1645         if X.shape[1] != self.n_features_:
1646             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1647                 self.n_features_, X.shape[1]))
1648         if self.init_ == 'zero':
1649             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1650                                        dtype=np.float64)
1651         else:
1652             raw_predictions = self.loss_.get_init_raw_predictions(
1653                 X, self.init_).astype(np.float64)
1654         return raw_predictions
1655 
1656     def _raw_predict(self, X):
1657         """Return the sum of the trees raw predictions (+ init estimator)."""
1658         raw_predictions = self._raw_predict_init(X)
1659         predict_stages(self.estimators_, X, self.learning_rate,
1660                        raw_predictions)
1661         return raw_predictions
1662 
1663     def _staged_raw_predict(self, X):
1664         """Compute raw predictions of ``X`` for each iteration.
1665 
1666         This method allows monitoring (i.e. determine error on testing set)
1667         after each stage.
1668 
1669         Parameters
1670         ----------
1671         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1672             The input samples. Internally, it will be converted to
1673             ``dtype=np.float32`` and if a sparse matrix is provided
1674             to a sparse ``csr_matrix``.
1675 
1676         Returns
1677         -------
1678         raw_predictions : generator of array, shape (n_samples, k)
1679             The raw predictions of the input samples. The order of the
1680             classes corresponds to that in the attribute `classes_`.
1681             Regression and binary classification are special cases with
1682             ``k == 1``, otherwise ``k==n_classes``.
1683         """
1684         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1685         raw_predictions = self._raw_predict_init(X)
1686         for i in range(self.estimators_.shape[0]):
1687             predict_stage(self.estimators_, i, X, self.learning_rate,
1688                           raw_predictions)
1689             yield raw_predictions.copy()
1690 
1691     @property
1692     def feature_importances_(self):
1693         """Return the feature importances (the higher, the more important the
1694            feature).
1695 
1696         Returns
1697         -------
1698         feature_importances_ : array, shape (n_features,)
1699         """
1700         self._check_initialized()
1701 
1702         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1703         for stage in self.estimators_:
1704             stage_sum = sum(tree.tree_.compute_feature_importances(
1705                 normalize=False) for tree in stage) / len(stage)
1706             total_sum += stage_sum
1707 
1708         importances = total_sum / total_sum.sum()
1709         return importances
1710 
1711     def _validate_y(self, y, sample_weight):
1712         # 'sample_weight' is not utilised but is used for
1713         # consistency with similar method _validate_y of GBC
1714         self.n_classes_ = 1
1715         if y.dtype.kind == 'O':
1716             y = y.astype(np.float64)
1717         # Default implementation
1718         return y
1719 
1720     def apply(self, X):
1721         """Apply trees in the ensemble to X, return leaf indices.
1722 
1723         .. versionadded:: 0.17
1724 
1725         Parameters
1726         ----------
1727         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1728             The input samples. Internally, its dtype will be converted to
1729             ``dtype=np.float32``. If a sparse matrix is provided, it will
1730             be converted to a sparse ``csr_matrix``.
1731 
1732         Returns
1733         -------
1734         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1735             For each datapoint x in X and for each tree in the ensemble,
1736             return the index of the leaf x ends up in each estimator.
1737             In the case of binary classification n_classes is 1.
1738         """
1739 
1740         self._check_initialized()
1741         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1742 
1743         # n_classes will be equal to 1 in the binary classification or the
1744         # regression case.
1745         n_estimators, n_classes = self.estimators_.shape
1746         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1747 
1748         for i in range(n_estimators):
1749             for j in range(n_classes):
1750                 estimator = self.estimators_[i, j]
1751                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1752 
1753         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 05:33:54.987 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>fit</method>
<code>
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1436         n_samples, self.n_features_ = X.shape
1437 
1438         sample_weight_is_none = sample_weight is None
1439         if sample_weight_is_none:
1440             sample_weight = np.ones(n_samples, dtype=np.float32)
1441         else:
1442             sample_weight = column_or_1d(sample_weight, warn=True)
1443             sample_weight_is_none = False
1444 
1445         check_consistent_length(X, y, sample_weight)
1446 
1447         y = self._validate_y(y, sample_weight)
1448 
1449         if self.n_iter_no_change is not None:
1450             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1451                 train_test_split(X, y, sample_weight,
1452                                  random_state=self.random_state,
1453                                  test_size=self.validation_fraction))
1454             if is_classifier(self):
1455                 if self.n_classes_ != np.unique(y).shape[0]:
1456                     # We choose to error here. The problem is that the init
1457                     # estimator would be trained on y, which has some missing
1458                     # classes now, so its predictions would not have the
1459                     # correct shape.
1460                     raise ValueError(
1461                         'The training data after the early stopping split '
1462                         'is missing some classes. Try using another random '
1463                         'seed.'
1464                     )
1465         else:
1466             X_val = y_val = sample_weight_val = None
1467 
1468         self._check_params()
1469 
1470         if not self._is_initialized():
1471             # init state
1472             self._init_state()
1473 
1474             # fit initial model and initialize raw predictions
1475             if self.init_ == 'zero':
1476                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1477                                            dtype=np.float64)
1478             else:
1479                 try:
1480                     self.init_.fit(X, y, sample_weight=sample_weight)
1481                 except TypeError:
1482                     if sample_weight_is_none:
1483                         self.init_.fit(X, y)
1484                     else:
1485                         raise ValueError(
1486                             "The initial estimator {} does not support sample "
1487                             "weights.".format(self.init_.__class__.__name__))
1488 
1489                 raw_predictions = \
1490                     self.loss_.get_init_raw_predictions(X, self.init_)
1491 
1492 
1493             begin_at_stage = 0
1494 
1495             # The rng state must be preserved if warm_start is True
1496             self._rng = check_random_state(self.random_state)
1497 
1498         else:
1499             # add more estimators to fitted model
1500             # invariant: warm_start = True
1501             if self.n_estimators < self.estimators_.shape[0]:
1502                 raise ValueError('n_estimators=%d must be larger or equal to '
1503                                  'estimators_.shape[0]=%d when '
1504                                  'warm_start==True'
1505                                  % (self.n_estimators,
1506                                     self.estimators_.shape[0]))
1507             begin_at_stage = self.estimators_.shape[0]
1508             # The requirements of _decision_function (called in two lines
1509             # below) are more constrained than fit. It accepts only CSR
1510             # matrices.
1511             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1512             raw_predictions = self._raw_predict(X)
1513             self._resize_state()
1514 
1515         if self.presort is True and issparse(X):
1516             raise ValueError(
1517                 "Presorting is not supported for sparse matrices.")
1518 
1519         presort = self.presort
1520         # Allow presort to be 'auto', which means True if the dataset is dense,
1521         # otherwise it will be False.
1522         if presort == 'auto':
1523             presort = not issparse(X)
1524 
1525         X_idx_sorted = None
1526         if presort:
1527             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1528                                              dtype=np.int32)
1529 
1530         # fit the boosting stages
1531         n_stages = self._fit_stages(
1532             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1533             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1534 
1535         # change shape of arrays after fit (early-stopping or additional ests)
1536         if n_stages != self.estimators_.shape[0]:
1537             self.estimators_ = self.estimators_[:n_stages]
1538             self.train_score_ = self.train_score_[:n_stages]
1539             if hasattr(self, 'oob_improvement_'):
1540                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1541 
1542         self.n_estimators_ = n_stages
1543         return self

</code><intended_behavior>A modification in the fit method of the BaseGradientBoosting class is required to ensure compatibility when using a Pipeline as the init_ estimator. Specifically, the handling of sample_weight needs to adapt based on whether the init_ estimator can directly accept sample_weight or if it should be passed within **fit_params. In lines 1479-1484, where the initial estimator is fitted with sample_weight, add a conditional check to determine if self.init_ is an instance of a Pipeline. If self.init_ is a Pipeline, refactor the call to fit to ensure sample_weight is passed within **fit_params. This might look something like: self.init_.fit(X, y, **{"sample_weight": sample_weight}) when sample_weight is not None. If self.init_ does not support sample_weight directly and is not a Pipeline (or does not support sample_weight in any form), proceed with existing fallback logic that attempts to call fit(X, y) without sample_weight. Ensure this behavior preserves existing functionality for estimators that do support sample_weight directly, and only modify the behavior for cases where sample_weight cannot be directly passed. This maintains backward compatibility. This enhancement would correct the issue where the initial fitting process fails due to a TypeError when sample_weight is passed as a positional argument to a Pipeline's fit method, aligning with the flexible nature of fit method signatures across different estimators and ensuring the GradientBoostingRegressor can effectively use pipelines as initial estimators.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>BaseGradientBoosting</class>
<method>None</method>
<code>
1166 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1167     """Abstract base class for Gradient Boosting. """
1168 
1169     @abstractmethod
1170     def __init__(self, loss, learning_rate, n_estimators, criterion,
1171                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1172                  max_depth, min_impurity_decrease, min_impurity_split,
1173                  init, subsample, max_features,
1174                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1175                  warm_start=False, presort='auto',
1176                  validation_fraction=0.1, n_iter_no_change=None,
1177                  tol=1e-4):
1178 
1179         self.n_estimators = n_estimators
1180         self.learning_rate = learning_rate
1181         self.loss = loss
1182         self.criterion = criterion
1183         self.min_samples_split = min_samples_split
1184         self.min_samples_leaf = min_samples_leaf
1185         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1186         self.subsample = subsample
1187         self.max_features = max_features
1188         self.max_depth = max_depth
1189         self.min_impurity_decrease = min_impurity_decrease
1190         self.min_impurity_split = min_impurity_split
1191         self.init = init
1192         self.random_state = random_state
1193         self.alpha = alpha
1194         self.verbose = verbose
1195         self.max_leaf_nodes = max_leaf_nodes
1196         self.warm_start = warm_start
1197         self.presort = presort
1198         self.validation_fraction = validation_fraction
1199         self.n_iter_no_change = n_iter_no_change
1200         self.tol = tol
1201 
1202     def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
1203                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1204         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1205 
1206         assert sample_mask.dtype == np.bool
1207         loss = self.loss_
1208         original_y = y
1209 
1210         # Need to pass a copy of raw_predictions to negative_gradient()
1211         # because raw_predictions is partially updated at the end of the loop
1212         # in update_terminal_regions(), and gradients need to be evaluated at
1213         # iteration i - 1.
1214         raw_predictions_copy = raw_predictions.copy()
1215 
1216         for k in range(loss.K):
1217             if loss.is_multi_class:
1218                 y = np.array(original_y == k, dtype=np.float64)
1219 
1220             residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
1221                                               sample_weight=sample_weight)
1222 
1223             # induce regression tree on residuals
1224             tree = DecisionTreeRegressor(
1225                 criterion=self.criterion,
1226                 splitter='best',
1227                 max_depth=self.max_depth,
1228                 min_samples_split=self.min_samples_split,
1229                 min_samples_leaf=self.min_samples_leaf,
1230                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1231                 min_impurity_decrease=self.min_impurity_decrease,
1232                 min_impurity_split=self.min_impurity_split,
1233                 max_features=self.max_features,
1234                 max_leaf_nodes=self.max_leaf_nodes,
1235                 random_state=random_state,
1236                 presort=self.presort)
1237 
1238             if self.subsample < 1.0:
1239                 # no inplace multiplication!
1240                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1241 
1242             X = X_csr if X_csr is not None else X
1243             tree.fit(X, residual, sample_weight=sample_weight,
1244                      check_input=False, X_idx_sorted=X_idx_sorted)
1245 
1246             # update tree leaves
1247             loss.update_terminal_regions(
1248                 tree.tree_, X, y, residual, raw_predictions, sample_weight,
1249                 sample_mask, learning_rate=self.learning_rate, k=k)
1250 
1251             # add tree to ensemble
1252             self.estimators_[i, k] = tree
1253 
1254         return raw_predictions
1255 
1256     def _check_params(self):
1257         """Check validity of parameters and raise ValueError if not valid. """
1258         if self.n_estimators <= 0:
1259             raise ValueError("n_estimators must be greater than 0 but "
1260                              "was %r" % self.n_estimators)
1261 
1262         if self.learning_rate <= 0.0:
1263             raise ValueError("learning_rate must be greater than 0 but "
1264                              "was %r" % self.learning_rate)
1265 
1266         if (self.loss not in self._SUPPORTED_LOSS
1267                 or self.loss not in _gb_losses.LOSS_FUNCTIONS):
1268             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1269 
1270         if self.loss == 'deviance':
1271             loss_class = (_gb_losses.MultinomialDeviance
1272                           if len(self.classes_) > 2
1273                           else _gb_losses.BinomialDeviance)
1274         else:
1275             loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
1276 
1277         if self.loss in ('huber', 'quantile'):
1278             self.loss_ = loss_class(self.n_classes_, self.alpha)
1279         else:
1280             self.loss_ = loss_class(self.n_classes_)
1281 
1282         if not (0.0 < self.subsample <= 1.0):
1283             raise ValueError("subsample must be in (0,1] but "
1284                              "was %r" % self.subsample)
1285 
1286         if self.init is not None:
1287             # init must be an estimator or 'zero'
1288             if isinstance(self.init, BaseEstimator):
1289                 self.loss_.check_init_estimator(self.init)
1290             elif not (isinstance(self.init, str) and self.init == 'zero'):
1291                 raise ValueError(
1292                     "The init parameter must be an estimator or 'zero'. "
1293                     "Got init={}".format(self.init)
1294                 )
1295 
1296         if not (0.0 < self.alpha < 1.0):
1297             raise ValueError("alpha must be in (0.0, 1.0) but "
1298                              "was %r" % self.alpha)
1299 
1300         if isinstance(self.max_features, str):
1301             if self.max_features == "auto":
1302                 # if is_classification
1303                 if self.n_classes_ > 1:
1304                     max_features = max(1, int(np.sqrt(self.n_features_)))
1305                 else:
1306                     # is regression
1307                     max_features = self.n_features_
1308             elif self.max_features == "sqrt":
1309                 max_features = max(1, int(np.sqrt(self.n_features_)))
1310             elif self.max_features == "log2":
1311                 max_features = max(1, int(np.log2(self.n_features_)))
1312             else:
1313                 raise ValueError("Invalid value for max_features: %r. "
1314                                  "Allowed string values are 'auto', 'sqrt' "
1315                                  "or 'log2'." % self.max_features)
1316         elif self.max_features is None:
1317             max_features = self.n_features_
1318         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1319             max_features = self.max_features
1320         else:  # float
1321             if 0. < self.max_features <= 1.:
1322                 max_features = max(int(self.max_features *
1323                                        self.n_features_), 1)
1324             else:
1325                 raise ValueError("max_features must be in (0, n_features]")
1326 
1327         self.max_features_ = max_features
1328 
1329         if not isinstance(self.n_iter_no_change,
1330                           (numbers.Integral, np.integer, type(None))):
1331             raise ValueError("n_iter_no_change should either be None or an "
1332                              "integer. %r was passed"
1333                              % self.n_iter_no_change)
1334 
1335         allowed_presort = ('auto', True, False)
1336         if self.presort not in allowed_presort:
1337             raise ValueError("'presort' should be in {}. Got {!r} instead."
1338                              .format(allowed_presort, self.presort))
1339 
1340     def _init_state(self):
1341         """Initialize model state and allocate model state data structures. """
1342 
1343         self.init_ = self.init
1344         if self.init_ is None:
1345             self.init_ = self.loss_.init_estimator()
1346 
1347         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1348                                     dtype=np.object)
1349         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1350         # do oob?
1351         if self.subsample < 1.0:
1352             self.oob_improvement_ = np.zeros((self.n_estimators),
1353                                              dtype=np.float64)
1354 
1355     def _clear_state(self):
1356         """Clear the state of the gradient boosting model. """
1357         if hasattr(self, 'estimators_'):
1358             self.estimators_ = np.empty((0, 0), dtype=np.object)
1359         if hasattr(self, 'train_score_'):
1360             del self.train_score_
1361         if hasattr(self, 'oob_improvement_'):
1362             del self.oob_improvement_
1363         if hasattr(self, 'init_'):
1364             del self.init_
1365         if hasattr(self, '_rng'):
1366             del self._rng
1367 
1368     def _resize_state(self):
1369         """Add additional ``n_estimators`` entries to all attributes. """
1370         # self.n_estimators is the number of additional est to fit
1371         total_n_estimators = self.n_estimators
1372         if total_n_estimators < self.estimators_.shape[0]:
1373             raise ValueError('resize with smaller n_estimators %d < %d' %
1374                              (total_n_estimators, self.estimators_[0]))
1375 
1376         self.estimators_ = np.resize(self.estimators_,
1377                                      (total_n_estimators, self.loss_.K))
1378         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1379         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1380             # if do oob resize arrays or create new if not available
1381             if hasattr(self, 'oob_improvement_'):
1382                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1383                                                   total_n_estimators)
1384             else:
1385                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1386                                                  dtype=np.float64)
1387 
1388     def _is_initialized(self):
1389         return len(getattr(self, 'estimators_', [])) > 0
1390 
1391     def _check_initialized(self):
1392         """Check that the estimator is initialized, raising an error if not."""
1393         check_is_fitted(self, 'estimators_')
1394 
1395     def fit(self, X, y, sample_weight=None, monitor=None):
1396         """Fit the gradient boosting model.
1397 
1398         Parameters
1399         ----------
1400         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1401             The input samples. Internally, it will be converted to
1402             ``dtype=np.float32`` and if a sparse matrix is provided
1403             to a sparse ``csr_matrix``.
1404 
1405         y : array-like, shape (n_samples,)
1406             Target values (strings or integers in classification, real numbers
1407             in regression)
1408             For classification, labels must correspond to classes.
1409 
1410         sample_weight : array-like, shape (n_samples,) or None
1411             Sample weights. If None, then samples are equally weighted. Splits
1412             that would create child nodes with net zero or negative weight are
1413             ignored while searching for a split in each node. In the case of
1414             classification, splits are also ignored if they would result in any
1415             single class carrying a negative weight in either child node.
1416 
1417         monitor : callable, optional
1418             The monitor is called after each iteration with the current
1419             iteration, a reference to the estimator and the local variables of
1420             ``_fit_stages`` as keyword arguments ``callable(i, self,
1421             locals())``. If the callable returns ``True`` the fitting procedure
1422             is stopped. The monitor can be used for various things such as
1423             computing held-out estimates, early stopping, model introspect, and
1424             snapshoting.
1425 
1426         Returns
1427         -------
1428         self : object
1429         """
1430         # if not warmstart - clear the estimator state
1431         if not self.warm_start:
1432             self._clear_state()
1433 
1434         # Check input
1435         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1436         n_samples, self.n_features_ = X.shape
1437 
1438         sample_weight_is_none = sample_weight is None
1439         if sample_weight_is_none:
1440             sample_weight = np.ones(n_samples, dtype=np.float32)
1441         else:
1442             sample_weight = column_or_1d(sample_weight, warn=True)
1443             sample_weight_is_none = False
1444 
1445         check_consistent_length(X, y, sample_weight)
1446 
1447         y = self._validate_y(y, sample_weight)
1448 
1449         if self.n_iter_no_change is not None:
1450             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1451                 train_test_split(X, y, sample_weight,
1452                                  random_state=self.random_state,
1453                                  test_size=self.validation_fraction))
1454             if is_classifier(self):
1455                 if self.n_classes_ != np.unique(y).shape[0]:
1456                     # We choose to error here. The problem is that the init
1457                     # estimator would be trained on y, which has some missing
1458                     # classes now, so its predictions would not have the
1459                     # correct shape.
1460                     raise ValueError(
1461                         'The training data after the early stopping split '
1462                         'is missing some classes. Try using another random '
1463                         'seed.'
1464                     )
1465         else:
1466             X_val = y_val = sample_weight_val = None
1467 
1468         self._check_params()
1469 
1470         if not self._is_initialized():
1471             # init state
1472             self._init_state()
1473 
1474             # fit initial model and initialize raw predictions
1475             if self.init_ == 'zero':
1476                 raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1477                                            dtype=np.float64)
1478             else:
1479                 try:
1480                     self.init_.fit(X, y, sample_weight=sample_weight)
1481                 except TypeError:
1482                     if sample_weight_is_none:
1483                         self.init_.fit(X, y)
1484                     else:
1485                         raise ValueError(
1486                             "The initial estimator {} does not support sample "
1487                             "weights.".format(self.init_.__class__.__name__))
1488 
1489                 raw_predictions = \
1490                     self.loss_.get_init_raw_predictions(X, self.init_)
1491 
1492 
1493             begin_at_stage = 0
1494 
1495             # The rng state must be preserved if warm_start is True
1496             self._rng = check_random_state(self.random_state)
1497 
1498         else:
1499             # add more estimators to fitted model
1500             # invariant: warm_start = True
1501             if self.n_estimators < self.estimators_.shape[0]:
1502                 raise ValueError('n_estimators=%d must be larger or equal to '
1503                                  'estimators_.shape[0]=%d when '
1504                                  'warm_start==True'
1505                                  % (self.n_estimators,
1506                                     self.estimators_.shape[0]))
1507             begin_at_stage = self.estimators_.shape[0]
1508             # The requirements of _decision_function (called in two lines
1509             # below) are more constrained than fit. It accepts only CSR
1510             # matrices.
1511             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1512             raw_predictions = self._raw_predict(X)
1513             self._resize_state()
1514 
1515         if self.presort is True and issparse(X):
1516             raise ValueError(
1517                 "Presorting is not supported for sparse matrices.")
1518 
1519         presort = self.presort
1520         # Allow presort to be 'auto', which means True if the dataset is dense,
1521         # otherwise it will be False.
1522         if presort == 'auto':
1523             presort = not issparse(X)
1524 
1525         X_idx_sorted = None
1526         if presort:
1527             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1528                                              dtype=np.int32)
1529 
1530         # fit the boosting stages
1531         n_stages = self._fit_stages(
1532             X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
1533             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
1534 
1535         # change shape of arrays after fit (early-stopping or additional ests)
1536         if n_stages != self.estimators_.shape[0]:
1537             self.estimators_ = self.estimators_[:n_stages]
1538             self.train_score_ = self.train_score_[:n_stages]
1539             if hasattr(self, 'oob_improvement_'):
1540                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1541 
1542         self.n_estimators_ = n_stages
1543         return self
1544 
1545     def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
1546                     X_val, y_val, sample_weight_val,
1547                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1548         """Iteratively fits the stages.
1549 
1550         For each stage it computes the progress (OOB, train score)
1551         and delegates to ``_fit_stage``.
1552         Returns the number of stages fit; might differ from ``n_estimators``
1553         due to early stopping.
1554         """
1555         n_samples = X.shape[0]
1556         do_oob = self.subsample < 1.0
1557         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1558         n_inbag = max(1, int(self.subsample * n_samples))
1559         loss_ = self.loss_
1560 
1561         # Set min_weight_leaf from min_weight_fraction_leaf
1562         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1563             min_weight_leaf = (self.min_weight_fraction_leaf *
1564                                np.sum(sample_weight))
1565         else:
1566             min_weight_leaf = 0.
1567 
1568         if self.verbose:
1569             verbose_reporter = VerboseReporter(self.verbose)
1570             verbose_reporter.init(self, begin_at_stage)
1571 
1572         X_csc = csc_matrix(X) if issparse(X) else None
1573         X_csr = csr_matrix(X) if issparse(X) else None
1574 
1575         if self.n_iter_no_change is not None:
1576             loss_history = np.full(self.n_iter_no_change, np.inf)
1577             # We create a generator to get the predictions for X_val after
1578             # the addition of each successive stage
1579             y_val_pred_iter = self._staged_raw_predict(X_val)
1580 
1581         # perform boosting iterations
1582         i = begin_at_stage
1583         for i in range(begin_at_stage, self.n_estimators):
1584 
1585             # subsampling
1586             if do_oob:
1587                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1588                                                   random_state)
1589                 # OOB score before adding this stage
1590                 old_oob_score = loss_(y[~sample_mask],
1591                                       raw_predictions[~sample_mask],
1592                                       sample_weight[~sample_mask])
1593 
1594             # fit next stage of trees
1595             raw_predictions = self._fit_stage(
1596                 i, X, y, raw_predictions, sample_weight, sample_mask,
1597                 random_state, X_idx_sorted, X_csc, X_csr)
1598 
1599             # track deviance (= loss)
1600             if do_oob:
1601                 self.train_score_[i] = loss_(y[sample_mask],
1602                                              raw_predictions[sample_mask],
1603                                              sample_weight[sample_mask])
1604                 self.oob_improvement_[i] = (
1605                     old_oob_score - loss_(y[~sample_mask],
1606                                           raw_predictions[~sample_mask],
1607                                           sample_weight[~sample_mask]))
1608             else:
1609                 # no need to fancy index w/ no subsampling
1610                 self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
1611 
1612             if self.verbose > 0:
1613                 verbose_reporter.update(i, self)
1614 
1615             if monitor is not None:
1616                 early_stopping = monitor(i, self, locals())
1617                 if early_stopping:
1618                     break
1619 
1620             # We also provide an early stopping based on the score from
1621             # validation set (X_val, y_val), if n_iter_no_change is set
1622             if self.n_iter_no_change is not None:
1623                 # By calling next(y_val_pred_iter), we get the predictions
1624                 # for X_val after the addition of the current stage
1625                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1626                                         sample_weight_val)
1627 
1628                 # Require validation_score to be better (less) than at least
1629                 # one of the last n_iter_no_change evaluations
1630                 if np.any(validation_loss + self.tol < loss_history):
1631                     loss_history[i % len(loss_history)] = validation_loss
1632                 else:
1633                     break
1634 
1635         return i + 1
1636 
1637     def _make_estimator(self, append=True):
1638         # we don't need _make_estimator
1639         raise NotImplementedError()
1640 
1641     def _raw_predict_init(self, X):
1642         """Check input and compute raw predictions of the init estimtor."""
1643         self._check_initialized()
1644         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1645         if X.shape[1] != self.n_features_:
1646             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1647                 self.n_features_, X.shape[1]))
1648         if self.init_ == 'zero':
1649             raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
1650                                        dtype=np.float64)
1651         else:
1652             raw_predictions = self.loss_.get_init_raw_predictions(
1653                 X, self.init_).astype(np.float64)
1654         return raw_predictions
1655 
1656     def _raw_predict(self, X):
1657         """Return the sum of the trees raw predictions (+ init estimator)."""
1658         raw_predictions = self._raw_predict_init(X)
1659         predict_stages(self.estimators_, X, self.learning_rate,
1660                        raw_predictions)
1661         return raw_predictions
1662 
1663     def _staged_raw_predict(self, X):
1664         """Compute raw predictions of ``X`` for each iteration.
1665 
1666         This method allows monitoring (i.e. determine error on testing set)
1667         after each stage.
1668 
1669         Parameters
1670         ----------
1671         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1672             The input samples. Internally, it will be converted to
1673             ``dtype=np.float32`` and if a sparse matrix is provided
1674             to a sparse ``csr_matrix``.
1675 
1676         Returns
1677         -------
1678         raw_predictions : generator of array, shape (n_samples, k)
1679             The raw predictions of the input samples. The order of the
1680             classes corresponds to that in the attribute `classes_`.
1681             Regression and binary classification are special cases with
1682             ``k == 1``, otherwise ``k==n_classes``.
1683         """
1684         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1685         raw_predictions = self._raw_predict_init(X)
1686         for i in range(self.estimators_.shape[0]):
1687             predict_stage(self.estimators_, i, X, self.learning_rate,
1688                           raw_predictions)
1689             yield raw_predictions.copy()
1690 
1691     @property
1692     def feature_importances_(self):
1693         """Return the feature importances (the higher, the more important the
1694            feature).
1695 
1696         Returns
1697         -------
1698         feature_importances_ : array, shape (n_features,)
1699         """
1700         self._check_initialized()
1701 
1702         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1703         for stage in self.estimators_:
1704             stage_sum = sum(tree.tree_.compute_feature_importances(
1705                 normalize=False) for tree in stage) / len(stage)
1706             total_sum += stage_sum
1707 
1708         importances = total_sum / total_sum.sum()
1709         return importances
1710 
1711     def _validate_y(self, y, sample_weight):
1712         # 'sample_weight' is not utilised but is used for
1713         # consistency with similar method _validate_y of GBC
1714         self.n_classes_ = 1
1715         if y.dtype.kind == 'O':
1716             y = y.astype(np.float64)
1717         # Default implementation
1718         return y
1719 
1720     def apply(self, X):
1721         """Apply trees in the ensemble to X, return leaf indices.
1722 
1723         .. versionadded:: 0.17
1724 
1725         Parameters
1726         ----------
1727         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1728             The input samples. Internally, its dtype will be converted to
1729             ``dtype=np.float32``. If a sparse matrix is provided, it will
1730             be converted to a sparse ``csr_matrix``.
1731 
1732         Returns
1733         -------
1734         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1735             For each datapoint x in X and for each tree in the ensemble,
1736             return the index of the leaf x ends up in each estimator.
1737             In the case of binary classification n_classes is 1.
1738         """
1739 
1740         self._check_initialized()
1741         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1742 
1743         # n_classes will be equal to 1 in the binary classification or the
1744         # regression case.
1745         n_estimators, n_classes = self.estimators_.shape
1746         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1747 
1748         for i in range(n_estimators):
1749             for j in range(n_classes):
1750                 estimator = self.estimators_[i, j]
1751                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1752 
1753         return leaves

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 05:33:54.988 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 05:33:54.988 | INFO     | Search-only: skipping patch selection
2025-10-18 05:33:55.066 | INFO     | Task scikit-learn__scikit-learn-13472 completed successfully.
2025-10-18 05:33:55.067 | INFO     | No patch generated. You can try running ACR again.
