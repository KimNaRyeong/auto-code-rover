[
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 373,
        "end": 417,
        "class_name": "Variable",
        "method_name": "astype",
        "code": "373     def astype(self, dtype, casting=\"unsafe\", copy=True, keep_attrs=True):\n374         \"\"\"\n375         Copy of the Variable object, with data cast to a specified type.\n376 \n377         Parameters\n378         ----------\n379         dtype : str or dtype\n380              Typecode or data-type to which the array is cast.\n381         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n382              Controls what kind of data casting may occur. Defaults to 'unsafe'\n383              for backwards compatibility.\n384 \n385              * 'no' means the data types should not be cast at all.\n386              * 'equiv' means only byte-order changes are allowed.\n387              * 'safe' means only casts which can preserve values are allowed.\n388              * 'same_kind' means only safe casts or casts within a kind,\n389                  like float64 to float32, are allowed.\n390              * 'unsafe' means any data conversions may be done.\n391         copy : bool, optional\n392              By default, astype always returns a newly allocated array. If this\n393              is set to False and the `dtype` requirement is satisfied, the input\n394              array is returned instead of a copy.\n395         keep_attrs : bool, optional\n396             By default, astype keeps attributes. Set to False to remove\n397             attributes in the returned object.\n398 \n399         Returns\n400         -------\n401         out : same as object\n402             New object with data cast to the specified type.\n403 \n404         See also\n405         --------\n406         np.ndarray.astype\n407         dask.array.Array.astype\n408         \"\"\"\n409         from .computation import apply_ufunc\n410 \n411         return apply_ufunc(\n412             duck_array_ops.astype,\n413             self,\n414             kwargs=dict(dtype=dtype, casting=casting, copy=copy),\n415             keep_attrs=keep_attrs,\n416             dask=\"allowed\",\n417         )\n",
        "intended_behavior": "This method should be enhanced to include an 'order' parameter, allowing users to control the memory layout order of the result ('C', 'F', 'A', or 'K'). Implementation should ensure that this parameter is correctly passed to the underlying numpy array's astype method without disrupting the existing functionality for 'dtype', 'casting', 'copy', and 'keep_attrs'. Care should be taken to preserve backward compatibility and the behavior of the method when the 'order' parameter is not specified."
    },
    {
        "rel_file_path": "xarray/core/variable.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/variable.py",
        "start": 283,
        "end": 2391,
        "class_name": "Variable",
        "method_name": null,
        "code": "283 class Variable(\n284     common.AbstractArray, arithmetic.SupportsArithmetic, utils.NdimSizeLenMixin\n285 ):\n286     \"\"\"A netcdf-like variable consisting of dimensions, data and attributes\n287     which describe a single Array. A single Variable object is not fully\n288     described outside the context of its parent Dataset (if you want such a\n289     fully described object, use a DataArray instead).\n290 \n291     The main functional difference between Variables and numpy arrays is that\n292     numerical operations on Variables implement array broadcasting by dimension\n293     name. For example, adding an Variable with dimensions `('time',)` to\n294     another Variable with dimensions `('space',)` results in a new Variable\n295     with dimensions `('time', 'space')`. Furthermore, numpy reduce operations\n296     like ``mean`` or ``sum`` are overwritten to take a \"dimension\" argument\n297     instead of an \"axis\".\n298 \n299     Variables are light-weight objects used as the building block for datasets.\n300     They are more primitive objects, so operations with them provide marginally\n301     higher performance than using DataArrays. However, manipulating data in the\n302     form of a Dataset or DataArray should almost always be preferred, because\n303     they can use more complete metadata in context of coordinate labels.\n304     \"\"\"\n305 \n306     __slots__ = (\"_dims\", \"_data\", \"_attrs\", \"_encoding\")\n307 \n308     def __init__(self, dims, data, attrs=None, encoding=None, fastpath=False):\n309         \"\"\"\n310         Parameters\n311         ----------\n312         dims : str or sequence of str\n313             Name(s) of the the data dimension(s). Must be either a string (only\n314             for 1D data) or a sequence of strings with length equal to the\n315             number of dimensions.\n316         data : array_like\n317             Data array which supports numpy-like data access.\n318         attrs : dict_like or None, optional\n319             Attributes to assign to the new variable. If None (default), an\n320             empty attribute dictionary is initialized.\n321         encoding : dict_like or None, optional\n322             Dictionary specifying how to encode this array's data into a\n323             serialized format like netCDF4. Currently used keys (for netCDF)\n324             include '_FillValue', 'scale_factor', 'add_offset' and 'dtype'.\n325             Well-behaved code to serialize a Variable should ignore\n326             unrecognized encoding items.\n327         \"\"\"\n328         self._data = as_compatible_data(data, fastpath=fastpath)\n329         self._dims = self._parse_dimensions(dims)\n330         self._attrs = None\n331         self._encoding = None\n332         if attrs is not None:\n333             self.attrs = attrs\n334         if encoding is not None:\n335             self.encoding = encoding\n336 \n337     @property\n338     def dtype(self):\n339         return self._data.dtype\n340 \n341     @property\n342     def shape(self):\n343         return self._data.shape\n344 \n345     @property\n346     def nbytes(self):\n347         return self.size * self.dtype.itemsize\n348 \n349     @property\n350     def _in_memory(self):\n351         return isinstance(self._data, (np.ndarray, np.number, PandasIndexAdapter)) or (\n352             isinstance(self._data, indexing.MemoryCachedArray)\n353             and isinstance(self._data.array, indexing.NumpyIndexingAdapter)\n354         )\n355 \n356     @property\n357     def data(self):\n358         if is_duck_array(self._data):\n359             return self._data\n360         else:\n361             return self.values\n362 \n363     @data.setter\n364     def data(self, data):\n365         data = as_compatible_data(data)\n366         if data.shape != self.shape:\n367             raise ValueError(\n368                 f\"replacement data must match the Variable's shape. \"\n369                 f\"replacement data has shape {data.shape}; Variable has shape {self.shape}\"\n370             )\n371         self._data = data\n372 \n373     def astype(self, dtype, casting=\"unsafe\", copy=True, keep_attrs=True):\n374         \"\"\"\n375         Copy of the Variable object, with data cast to a specified type.\n376 \n377         Parameters\n378         ----------\n379         dtype : str or dtype\n380              Typecode or data-type to which the array is cast.\n381         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n382              Controls what kind of data casting may occur. Defaults to 'unsafe'\n383              for backwards compatibility.\n384 \n385              * 'no' means the data types should not be cast at all.\n386              * 'equiv' means only byte-order changes are allowed.\n387              * 'safe' means only casts which can preserve values are allowed.\n388              * 'same_kind' means only safe casts or casts within a kind,\n389                  like float64 to float32, are allowed.\n390              * 'unsafe' means any data conversions may be done.\n391         copy : bool, optional\n392              By default, astype always returns a newly allocated array. If this\n393              is set to False and the `dtype` requirement is satisfied, the input\n394              array is returned instead of a copy.\n395         keep_attrs : bool, optional\n396             By default, astype keeps attributes. Set to False to remove\n397             attributes in the returned object.\n398 \n399         Returns\n400         -------\n401         out : same as object\n402             New object with data cast to the specified type.\n403 \n404         See also\n405         --------\n406         np.ndarray.astype\n407         dask.array.Array.astype\n408         \"\"\"\n409         from .computation import apply_ufunc\n410 \n411         return apply_ufunc(\n412             duck_array_ops.astype,\n413             self,\n414             kwargs=dict(dtype=dtype, casting=casting, copy=copy),\n415             keep_attrs=keep_attrs,\n416             dask=\"allowed\",\n417         )\n418 \n419     def load(self, **kwargs):\n420         \"\"\"Manually trigger loading of this variable's data from disk or a\n421         remote source into memory and return this variable.\n422 \n423         Normally, it should not be necessary to call this method in user code,\n424         because all xarray functions should either work on deferred data or\n425         load data automatically.\n426 \n427         Parameters\n428         ----------\n429         **kwargs : dict\n430             Additional keyword arguments passed on to ``dask.array.compute``.\n431 \n432         See Also\n433         --------\n434         dask.array.compute\n435         \"\"\"\n436         if is_duck_dask_array(self._data):\n437             self._data = as_compatible_data(self._data.compute(**kwargs))\n438         elif not is_duck_array(self._data):\n439             self._data = np.asarray(self._data)\n440         return self\n441 \n442     def compute(self, **kwargs):\n443         \"\"\"Manually trigger loading of this variable's data from disk or a\n444         remote source into memory and return a new variable. The original is\n445         left unaltered.\n446 \n447         Normally, it should not be necessary to call this method in user code,\n448         because all xarray functions should either work on deferred data or\n449         load data automatically.\n450 \n451         Parameters\n452         ----------\n453         **kwargs : dict\n454             Additional keyword arguments passed on to ``dask.array.compute``.\n455 \n456         See Also\n457         --------\n458         dask.array.compute\n459         \"\"\"\n460         new = self.copy(deep=False)\n461         return new.load(**kwargs)\n462 \n463     def __dask_tokenize__(self):\n464         # Use v.data, instead of v._data, in order to cope with the wrappers\n465         # around NetCDF and the like\n466         from dask.base import normalize_token\n467 \n468         return normalize_token((type(self), self._dims, self.data, self._attrs))\n469 \n470     def __dask_graph__(self):\n471         if is_duck_dask_array(self._data):\n472             return self._data.__dask_graph__()\n473         else:\n474             return None\n475 \n476     def __dask_keys__(self):\n477         return self._data.__dask_keys__()\n478 \n479     def __dask_layers__(self):\n480         return self._data.__dask_layers__()\n481 \n482     @property\n483     def __dask_optimize__(self):\n484         return self._data.__dask_optimize__\n485 \n486     @property\n487     def __dask_scheduler__(self):\n488         return self._data.__dask_scheduler__\n489 \n490     def __dask_postcompute__(self):\n491         array_func, array_args = self._data.__dask_postcompute__()\n492         return (\n493             self._dask_finalize,\n494             (array_func, array_args, self._dims, self._attrs, self._encoding),\n495         )\n496 \n497     def __dask_postpersist__(self):\n498         array_func, array_args = self._data.__dask_postpersist__()\n499         return (\n500             self._dask_finalize,\n501             (array_func, array_args, self._dims, self._attrs, self._encoding),\n502         )\n503 \n504     @staticmethod\n505     def _dask_finalize(results, array_func, array_args, dims, attrs, encoding):\n506         data = array_func(results, *array_args)\n507         return Variable(dims, data, attrs=attrs, encoding=encoding)\n508 \n509     @property\n510     def values(self):\n511         \"\"\"The variable's data as a numpy.ndarray\"\"\"\n512         return _as_array_or_item(self._data)\n513 \n514     @values.setter\n515     def values(self, values):\n516         self.data = values\n517 \n518     def to_base_variable(self):\n519         \"\"\"Return this variable as a base xarray.Variable\"\"\"\n520         return Variable(\n521             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n522         )\n523 \n524     to_variable = utils.alias(to_base_variable, \"to_variable\")\n525 \n526     def to_index_variable(self):\n527         \"\"\"Return this variable as an xarray.IndexVariable\"\"\"\n528         return IndexVariable(\n529             self.dims, self._data, self._attrs, encoding=self._encoding, fastpath=True\n530         )\n531 \n532     to_coord = utils.alias(to_index_variable, \"to_coord\")\n533 \n534     def to_index(self):\n535         \"\"\"Convert this variable to a pandas.Index\"\"\"\n536         return self.to_index_variable().to_index()\n537 \n538     def to_dict(self, data=True):\n539         \"\"\"Dictionary representation of variable.\"\"\"\n540         item = {\"dims\": self.dims, \"attrs\": decode_numpy_dict_values(self.attrs)}\n541         if data:\n542             item[\"data\"] = ensure_us_time_resolution(self.values).tolist()\n543         else:\n544             item.update({\"dtype\": str(self.dtype), \"shape\": self.shape})\n545         return item\n546 \n547     @property\n548     def dims(self):\n549         \"\"\"Tuple of dimension names with which this variable is associated.\"\"\"\n550         return self._dims\n551 \n552     @dims.setter\n553     def dims(self, value):\n554         self._dims = self._parse_dimensions(value)\n555 \n556     def _parse_dimensions(self, dims):\n557         if isinstance(dims, str):\n558             dims = (dims,)\n559         dims = tuple(dims)\n560         if len(dims) != self.ndim:\n561             raise ValueError(\n562                 \"dimensions %s must have the same length as the \"\n563                 \"number of data dimensions, ndim=%s\" % (dims, self.ndim)\n564             )\n565         return dims\n566 \n567     def _item_key_to_tuple(self, key):\n568         if utils.is_dict_like(key):\n569             return tuple(key.get(dim, slice(None)) for dim in self.dims)\n570         else:\n571             return key\n572 \n573     def _broadcast_indexes(self, key):\n574         \"\"\"Prepare an indexing key for an indexing operation.\n575 \n576         Parameters\n577         -----------\n578         key: int, slice, array-like, dict or tuple of integer, slice and array-like\n579             Any valid input for indexing.\n580 \n581         Returns\n582         -------\n583         dims : tuple\n584             Dimension of the resultant variable.\n585         indexers : IndexingTuple subclass\n586             Tuple of integer, array-like, or slices to use when indexing\n587             self._data. The type of this argument indicates the type of\n588             indexing to perform, either basic, outer or vectorized.\n589         new_order : Optional[Sequence[int]]\n590             Optional reordering to do on the result of indexing. If not None,\n591             the first len(new_order) indexing should be moved to these\n592             positions.\n593         \"\"\"\n594         key = self._item_key_to_tuple(key)  # key is a tuple\n595         # key is a tuple of full size\n596         key = indexing.expanded_indexer(key, self.ndim)\n597         # Convert a scalar Variable to an integer\n598         key = tuple(\n599             k.data.item() if isinstance(k, Variable) and k.ndim == 0 else k for k in key\n600         )\n601         # Convert a 0d-array to an integer\n602         key = tuple(\n603             k.item() if isinstance(k, np.ndarray) and k.ndim == 0 else k for k in key\n604         )\n605 \n606         if all(isinstance(k, BASIC_INDEXING_TYPES) for k in key):\n607             return self._broadcast_indexes_basic(key)\n608 \n609         self._validate_indexers(key)\n610         # Detect it can be mapped as an outer indexer\n611         # If all key is unlabeled, or\n612         # key can be mapped as an OuterIndexer.\n613         if all(not isinstance(k, Variable) for k in key):\n614             return self._broadcast_indexes_outer(key)\n615 \n616         # If all key is 1-dimensional and there are no duplicate labels,\n617         # key can be mapped as an OuterIndexer.\n618         dims = []\n619         for k, d in zip(key, self.dims):\n620             if isinstance(k, Variable):\n621                 if len(k.dims) > 1:\n622                     return self._broadcast_indexes_vectorized(key)\n623                 dims.append(k.dims[0])\n624             elif not isinstance(k, integer_types):\n625                 dims.append(d)\n626         if len(set(dims)) == len(dims):\n627             return self._broadcast_indexes_outer(key)\n628 \n629         return self._broadcast_indexes_vectorized(key)\n630 \n631     def _broadcast_indexes_basic(self, key):\n632         dims = tuple(\n633             dim for k, dim in zip(key, self.dims) if not isinstance(k, integer_types)\n634         )\n635         return dims, BasicIndexer(key), None\n636 \n637     def _validate_indexers(self, key):\n638         \"\"\" Make sanity checks \"\"\"\n639         for dim, k in zip(self.dims, key):\n640             if isinstance(k, BASIC_INDEXING_TYPES):\n641                 pass\n642             else:\n643                 if not isinstance(k, Variable):\n644                     k = np.asarray(k)\n645                     if k.ndim > 1:\n646                         raise IndexError(\n647                             \"Unlabeled multi-dimensional array cannot be \"\n648                             \"used for indexing: {}\".format(k)\n649                         )\n650                 if k.dtype.kind == \"b\":\n651                     if self.shape[self.get_axis_num(dim)] != len(k):\n652                         raise IndexError(\n653                             \"Boolean array size {:d} is used to index array \"\n654                             \"with shape {:s}.\".format(len(k), str(self.shape))\n655                         )\n656                     if k.ndim > 1:\n657                         raise IndexError(\n658                             \"{}-dimensional boolean indexing is \"\n659                             \"not supported. \".format(k.ndim)\n660                         )\n661                     if getattr(k, \"dims\", (dim,)) != (dim,):\n662                         raise IndexError(\n663                             \"Boolean indexer should be unlabeled or on the \"\n664                             \"same dimension to the indexed array. Indexer is \"\n665                             \"on {:s} but the target dimension is {:s}.\".format(\n666                                 str(k.dims), dim\n667                             )\n668                         )\n669 \n670     def _broadcast_indexes_outer(self, key):\n671         dims = tuple(\n672             k.dims[0] if isinstance(k, Variable) else dim\n673             for k, dim in zip(key, self.dims)\n674             if not isinstance(k, integer_types)\n675         )\n676 \n677         new_key = []\n678         for k in key:\n679             if isinstance(k, Variable):\n680                 k = k.data\n681             if not isinstance(k, BASIC_INDEXING_TYPES):\n682                 k = np.asarray(k)\n683                 if k.size == 0:\n684                     # Slice by empty list; numpy could not infer the dtype\n685                     k = k.astype(int)\n686                 elif k.dtype.kind == \"b\":\n687                     (k,) = np.nonzero(k)\n688             new_key.append(k)\n689 \n690         return dims, OuterIndexer(tuple(new_key)), None\n691 \n692     def _nonzero(self):\n693         \"\"\" Equivalent numpy's nonzero but returns a tuple of Varibles. \"\"\"\n694         # TODO we should replace dask's native nonzero\n695         # after https://github.com/dask/dask/issues/1076 is implemented.\n696         nonzeros = np.nonzero(self.data)\n697         return tuple(Variable((dim), nz) for nz, dim in zip(nonzeros, self.dims))\n698 \n699     def _broadcast_indexes_vectorized(self, key):\n700         variables = []\n701         out_dims_set = OrderedSet()\n702         for dim, value in zip(self.dims, key):\n703             if isinstance(value, slice):\n704                 out_dims_set.add(dim)\n705             else:\n706                 variable = (\n707                     value\n708                     if isinstance(value, Variable)\n709                     else as_variable(value, name=dim)\n710                 )\n711                 if variable.dtype.kind == \"b\":  # boolean indexing case\n712                     (variable,) = variable._nonzero()\n713 \n714                 variables.append(variable)\n715                 out_dims_set.update(variable.dims)\n716 \n717         variable_dims = set()\n718         for variable in variables:\n719             variable_dims.update(variable.dims)\n720 \n721         slices = []\n722         for i, (dim, value) in enumerate(zip(self.dims, key)):\n723             if isinstance(value, slice):\n724                 if dim in variable_dims:\n725                     # We only convert slice objects to variables if they share\n726                     # a dimension with at least one other variable. Otherwise,\n727                     # we can equivalently leave them as slices aknd transpose\n728                     # the result. This is significantly faster/more efficient\n729                     # for most array backends.\n730                     values = np.arange(*value.indices(self.sizes[dim]))\n731                     variables.insert(i - len(slices), Variable((dim,), values))\n732                 else:\n733                     slices.append((i, value))\n734 \n735         try:\n736             variables = _broadcast_compat_variables(*variables)\n737         except ValueError:\n738             raise IndexError(f\"Dimensions of indexers mismatch: {key}\")\n739 \n740         out_key = [variable.data for variable in variables]\n741         out_dims = tuple(out_dims_set)\n742         slice_positions = set()\n743         for i, value in slices:\n744             out_key.insert(i, value)\n745             new_position = out_dims.index(self.dims[i])\n746             slice_positions.add(new_position)\n747 \n748         if slice_positions:\n749             new_order = [i for i in range(len(out_dims)) if i not in slice_positions]\n750         else:\n751             new_order = None\n752 \n753         return out_dims, VectorizedIndexer(tuple(out_key)), new_order\n754 \n755     def __getitem__(self: VariableType, key) -> VariableType:\n756         \"\"\"Return a new Variable object whose contents are consistent with\n757         getting the provided key from the underlying data.\n758 \n759         NB. __getitem__ and __setitem__ implement xarray-style indexing,\n760         where if keys are unlabeled arrays, we index the array orthogonally\n761         with them. If keys are labeled array (such as Variables), they are\n762         broadcasted with our usual scheme and then the array is indexed with\n763         the broadcasted key, like numpy's fancy indexing.\n764 \n765         If you really want to do indexing like `x[x > 0]`, manipulate the numpy\n766         array `x.values` directly.\n767         \"\"\"\n768         dims, indexer, new_order = self._broadcast_indexes(key)\n769         data = as_indexable(self._data)[indexer]\n770         if new_order:\n771             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n772         return self._finalize_indexing_result(dims, data)\n773 \n774     def _finalize_indexing_result(self: VariableType, dims, data) -> VariableType:\n775         \"\"\"Used by IndexVariable to return IndexVariable objects when possible.\"\"\"\n776         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n777 \n778     def _getitem_with_mask(self, key, fill_value=dtypes.NA):\n779         \"\"\"Index this Variable with -1 remapped to fill_value.\"\"\"\n780         # TODO(shoyer): expose this method in public API somewhere (isel?) and\n781         # use it for reindex.\n782         # TODO(shoyer): add a sanity check that all other integers are\n783         # non-negative\n784         # TODO(shoyer): add an optimization, remapping -1 to an adjacent value\n785         # that is actually indexed rather than mapping it to the last value\n786         # along each axis.\n787 \n788         if fill_value is dtypes.NA:\n789             fill_value = dtypes.get_fill_value(self.dtype)\n790 \n791         dims, indexer, new_order = self._broadcast_indexes(key)\n792 \n793         if self.size:\n794             if is_duck_dask_array(self._data):\n795                 # dask's indexing is faster this way; also vindex does not\n796                 # support negative indices yet:\n797                 # https://github.com/dask/dask/pull/2967\n798                 actual_indexer = indexing.posify_mask_indexer(indexer)\n799             else:\n800                 actual_indexer = indexer\n801 \n802             data = as_indexable(self._data)[actual_indexer]\n803             mask = indexing.create_mask(indexer, self.shape, data)\n804             # we need to invert the mask in order to pass data first. This helps\n805             # pint to choose the correct unit\n806             # TODO: revert after https://github.com/hgrecco/pint/issues/1019 is fixed\n807             data = duck_array_ops.where(np.logical_not(mask), data, fill_value)\n808         else:\n809             # array cannot be indexed along dimensions of size 0, so just\n810             # build the mask directly instead.\n811             mask = indexing.create_mask(indexer, self.shape)\n812             data = np.broadcast_to(fill_value, getattr(mask, \"shape\", ()))\n813 \n814         if new_order:\n815             data = duck_array_ops.moveaxis(data, range(len(new_order)), new_order)\n816         return self._finalize_indexing_result(dims, data)\n817 \n818     def __setitem__(self, key, value):\n819         \"\"\"__setitem__ is overloaded to access the underlying numpy values with\n820         orthogonal indexing.\n821 \n822         See __getitem__ for more details.\n823         \"\"\"\n824         dims, index_tuple, new_order = self._broadcast_indexes(key)\n825 \n826         if not isinstance(value, Variable):\n827             value = as_compatible_data(value)\n828             if value.ndim > len(dims):\n829                 raise ValueError(\n830                     \"shape mismatch: value array of shape %s could not be \"\n831                     \"broadcast to indexing result with %s dimensions\"\n832                     % (value.shape, len(dims))\n833                 )\n834             if value.ndim == 0:\n835                 value = Variable((), value)\n836             else:\n837                 value = Variable(dims[-value.ndim :], value)\n838         # broadcast to become assignable\n839         value = value.set_dims(dims).data\n840 \n841         if new_order:\n842             value = duck_array_ops.asarray(value)\n843             value = value[(len(dims) - value.ndim) * (np.newaxis,) + (Ellipsis,)]\n844             value = duck_array_ops.moveaxis(value, new_order, range(len(new_order)))\n845 \n846         indexable = as_indexable(self._data)\n847         indexable[index_tuple] = value\n848 \n849     @property\n850     def attrs(self) -> Dict[Hashable, Any]:\n851         \"\"\"Dictionary of local attributes on this variable.\"\"\"\n852         if self._attrs is None:\n853             self._attrs = {}\n854         return self._attrs\n855 \n856     @attrs.setter\n857     def attrs(self, value: Mapping[Hashable, Any]) -> None:\n858         self._attrs = dict(value)\n859 \n860     @property\n861     def encoding(self):\n862         \"\"\"Dictionary of encodings on this variable.\"\"\"\n863         if self._encoding is None:\n864             self._encoding = {}\n865         return self._encoding\n866 \n867     @encoding.setter\n868     def encoding(self, value):\n869         try:\n870             self._encoding = dict(value)\n871         except ValueError:\n872             raise ValueError(\"encoding must be castable to a dictionary\")\n873 \n874     def copy(self, deep=True, data=None):\n875         \"\"\"Returns a copy of this object.\n876 \n877         If `deep=True`, the data array is loaded into memory and copied onto\n878         the new object. Dimensions, attributes and encodings are always copied.\n879 \n880         Use `data` to create a new object with the same structure as\n881         original but entirely new data.\n882 \n883         Parameters\n884         ----------\n885         deep : bool, optional\n886             Whether the data array is loaded into memory and copied onto\n887             the new object. Default is True.\n888         data : array_like, optional\n889             Data to use in the new object. Must have same shape as original.\n890             When `data` is used, `deep` is ignored.\n891 \n892         Returns\n893         -------\n894         object : Variable\n895             New object with dimensions, attributes, encodings, and optionally\n896             data copied from original.\n897 \n898         Examples\n899         --------\n900 \n901         Shallow copy versus deep copy\n902 \n903         >>> var = xr.Variable(data=[1, 2, 3], dims=\"x\")\n904         >>> var.copy()\n905         <xarray.Variable (x: 3)>\n906         array([1, 2, 3])\n907         >>> var_0 = var.copy(deep=False)\n908         >>> var_0[0] = 7\n909         >>> var_0\n910         <xarray.Variable (x: 3)>\n911         array([7, 2, 3])\n912         >>> var\n913         <xarray.Variable (x: 3)>\n914         array([7, 2, 3])\n915 \n916         Changing the data using the ``data`` argument maintains the\n917         structure of the original object, but with the new data. Original\n918         object is unaffected.\n919 \n920         >>> var.copy(data=[0.1, 0.2, 0.3])\n921         <xarray.Variable (x: 3)>\n922         array([0.1, 0.2, 0.3])\n923         >>> var\n924         <xarray.Variable (x: 3)>\n925         array([7, 2, 3])\n926 \n927         See Also\n928         --------\n929         pandas.DataFrame.copy\n930         \"\"\"\n931         if data is None:\n932             data = self._data\n933 \n934             if isinstance(data, indexing.MemoryCachedArray):\n935                 # don't share caching between copies\n936                 data = indexing.MemoryCachedArray(data.array)\n937 \n938             if deep:\n939                 data = copy.deepcopy(data)\n940 \n941         else:\n942             data = as_compatible_data(data)\n943             if self.shape != data.shape:\n944                 raise ValueError(\n945                     \"Data shape {} must match shape of object {}\".format(\n946                         data.shape, self.shape\n947                     )\n948                 )\n949 \n950         # note:\n951         # dims is already an immutable tuple\n952         # attributes and encoding will be copied when the new Array is created\n953         return self._replace(data=data)\n954 \n955     def _replace(\n956         self, dims=_default, data=_default, attrs=_default, encoding=_default\n957     ) -> \"Variable\":\n958         if dims is _default:\n959             dims = copy.copy(self._dims)\n960         if data is _default:\n961             data = copy.copy(self.data)\n962         if attrs is _default:\n963             attrs = copy.copy(self._attrs)\n964         if encoding is _default:\n965             encoding = copy.copy(self._encoding)\n966         return type(self)(dims, data, attrs, encoding, fastpath=True)\n967 \n968     def __copy__(self):\n969         return self.copy(deep=False)\n970 \n971     def __deepcopy__(self, memo=None):\n972         # memo does nothing but is required for compatibility with\n973         # copy.deepcopy\n974         return self.copy(deep=True)\n975 \n976     # mutable objects should not be hashable\n977     # https://github.com/python/mypy/issues/4266\n978     __hash__ = None  # type: ignore\n979 \n980     @property\n981     def chunks(self):\n982         \"\"\"Block dimensions for this array's data or None if it's not a dask\n983         array.\n984         \"\"\"\n985         return getattr(self._data, \"chunks\", None)\n986 \n987     _array_counter = itertools.count()\n988 \n989     def chunk(self, chunks={}, name=None, lock=False):\n990         \"\"\"Coerce this array's data into a dask arrays with the given chunks.\n991 \n992         If this variable is a non-dask array, it will be converted to dask\n993         array. If it's a dask array, it will be rechunked to the given chunk\n994         sizes.\n995 \n996         If neither chunks is not provided for one or more dimensions, chunk\n997         sizes along that dimension will not be updated; non-dask arrays will be\n998         converted into dask arrays with a single block.\n999 \n1000         Parameters\n1001         ----------\n1002         chunks : int, tuple or dict, optional\n1003             Chunk sizes along each dimension, e.g., ``5``, ``(5, 5)`` or\n1004             ``{'x': 5, 'y': 5}``.\n1005         name : str, optional\n1006             Used to generate the name for this array in the internal dask\n1007             graph. Does not need not be unique.\n1008         lock : optional\n1009             Passed on to :py:func:`dask.array.from_array`, if the array is not\n1010             already as dask array.\n1011 \n1012         Returns\n1013         -------\n1014         chunked : xarray.Variable\n1015         \"\"\"\n1016         import dask\n1017         import dask.array as da\n1018 \n1019         if chunks is None:\n1020             warnings.warn(\n1021                 \"None value for 'chunks' is deprecated. \"\n1022                 \"It will raise an error in the future. Use instead '{}'\",\n1023                 category=FutureWarning,\n1024             )\n1025             chunks = {}\n1026 \n1027         if utils.is_dict_like(chunks):\n1028             chunks = {self.get_axis_num(dim): chunk for dim, chunk in chunks.items()}\n1029 \n1030         data = self._data\n1031         if is_duck_dask_array(data):\n1032             data = data.rechunk(chunks)\n1033         else:\n1034             if isinstance(data, indexing.ExplicitlyIndexed):\n1035                 # Unambiguously handle array storage backends (like NetCDF4 and h5py)\n1036                 # that can't handle general array indexing. For example, in netCDF4 you\n1037                 # can do \"outer\" indexing along two dimensions independent, which works\n1038                 # differently from how NumPy handles it.\n1039                 # da.from_array works by using lazy indexing with a tuple of slices.\n1040                 # Using OuterIndexer is a pragmatic choice: dask does not yet handle\n1041                 # different indexing types in an explicit way:\n1042                 # https://github.com/dask/dask/issues/2883\n1043                 data = indexing.ImplicitToExplicitIndexingAdapter(\n1044                     data, indexing.OuterIndexer\n1045                 )\n1046                 if LooseVersion(dask.__version__) < \"2.0.0\":\n1047                     kwargs = {}\n1048                 else:\n1049                     # All of our lazily loaded backend array classes should use NumPy\n1050                     # array operations.\n1051                     kwargs = {\"meta\": np.ndarray}\n1052             else:\n1053                 kwargs = {}\n1054 \n1055             if utils.is_dict_like(chunks):\n1056                 chunks = tuple(chunks.get(n, s) for n, s in enumerate(self.shape))\n1057 \n1058             data = da.from_array(data, chunks, name=name, lock=lock, **kwargs)\n1059 \n1060         return type(self)(self.dims, data, self._attrs, self._encoding, fastpath=True)\n1061 \n1062     def _as_sparse(self, sparse_format=_default, fill_value=dtypes.NA):\n1063         \"\"\"\n1064         use sparse-array as backend.\n1065         \"\"\"\n1066         import sparse\n1067 \n1068         # TODO: what to do if dask-backended?\n1069         if fill_value is dtypes.NA:\n1070             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1071         else:\n1072             dtype = dtypes.result_type(self.dtype, fill_value)\n1073 \n1074         if sparse_format is _default:\n1075             sparse_format = \"coo\"\n1076         try:\n1077             as_sparse = getattr(sparse, f\"as_{sparse_format.lower()}\")\n1078         except AttributeError:\n1079             raise ValueError(f\"{sparse_format} is not a valid sparse format\")\n1080 \n1081         data = as_sparse(self.data.astype(dtype), fill_value=fill_value)\n1082         return self._replace(data=data)\n1083 \n1084     def _to_dense(self):\n1085         \"\"\"\n1086         Change backend from sparse to np.array\n1087         \"\"\"\n1088         if hasattr(self._data, \"todense\"):\n1089             return self._replace(data=self._data.todense())\n1090         return self.copy(deep=False)\n1091 \n1092     def isel(\n1093         self: VariableType,\n1094         indexers: Mapping[Hashable, Any] = None,\n1095         missing_dims: str = \"raise\",\n1096         **indexers_kwargs: Any,\n1097     ) -> VariableType:\n1098         \"\"\"Return a new array indexed along the specified dimension(s).\n1099 \n1100         Parameters\n1101         ----------\n1102         **indexers : {dim: indexer, ...}\n1103             Keyword arguments with names matching dimensions and values given\n1104             by integers, slice objects or arrays.\n1105         missing_dims : {\"raise\", \"warn\", \"ignore\"}, default: \"raise\"\n1106             What to do if dimensions that should be selected from are not present in the\n1107             DataArray:\n1108             - \"raise\": raise an exception\n1109             - \"warning\": raise a warning, and ignore the missing dimensions\n1110             - \"ignore\": ignore the missing dimensions\n1111 \n1112         Returns\n1113         -------\n1114         obj : Array object\n1115             A new Array with the selected data and dimensions. In general,\n1116             the new variable's data will be a view of this variable's data,\n1117             unless numpy fancy indexing was triggered by using an array\n1118             indexer, in which case the data will be a copy.\n1119         \"\"\"\n1120         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, \"isel\")\n1121 \n1122         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)\n1123 \n1124         key = tuple(indexers.get(dim, slice(None)) for dim in self.dims)\n1125         return self[key]\n1126 \n1127     def squeeze(self, dim=None):\n1128         \"\"\"Return a new object with squeezed data.\n1129 \n1130         Parameters\n1131         ----------\n1132         dim : None or str or tuple of str, optional\n1133             Selects a subset of the length one dimensions. If a dimension is\n1134             selected with length greater than one, an error is raised. If\n1135             None, all length one dimensions are squeezed.\n1136 \n1137         Returns\n1138         -------\n1139         squeezed : same type as caller\n1140             This object, but with with all or a subset of the dimensions of\n1141             length 1 removed.\n1142 \n1143         See Also\n1144         --------\n1145         numpy.squeeze\n1146         \"\"\"\n1147         dims = common.get_squeeze_dims(self, dim)\n1148         return self.isel({d: 0 for d in dims})\n1149 \n1150     def _shift_one_dim(self, dim, count, fill_value=dtypes.NA):\n1151         axis = self.get_axis_num(dim)\n1152 \n1153         if count > 0:\n1154             keep = slice(None, -count)\n1155         elif count < 0:\n1156             keep = slice(-count, None)\n1157         else:\n1158             keep = slice(None)\n1159 \n1160         trimmed_data = self[(slice(None),) * axis + (keep,)].data\n1161 \n1162         if fill_value is dtypes.NA:\n1163             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1164         else:\n1165             dtype = self.dtype\n1166 \n1167         width = min(abs(count), self.shape[axis])\n1168         dim_pad = (width, 0) if count >= 0 else (0, width)\n1169         pads = [(0, 0) if d != dim else dim_pad for d in self.dims]\n1170 \n1171         data = duck_array_ops.pad(\n1172             trimmed_data.astype(dtype),\n1173             pads,\n1174             mode=\"constant\",\n1175             constant_values=fill_value,\n1176         )\n1177 \n1178         if is_duck_dask_array(data):\n1179             # chunked data should come out with the same chunks; this makes\n1180             # it feasible to combine shifted and unshifted data\n1181             # TODO: remove this once dask.array automatically aligns chunks\n1182             data = data.rechunk(self.data.chunks)\n1183 \n1184         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1185 \n1186     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):\n1187         \"\"\"\n1188         Return a new Variable with shifted data.\n1189 \n1190         Parameters\n1191         ----------\n1192         shifts : mapping of the form {dim: offset}\n1193             Integer offset to shift along each of the given dimensions.\n1194             Positive offsets shift to the right; negative offsets shift to the\n1195             left.\n1196         fill_value: scalar, optional\n1197             Value to use for newly missing values\n1198         **shifts_kwargs\n1199             The keyword arguments form of ``shifts``.\n1200             One of shifts or shifts_kwargs must be provided.\n1201 \n1202         Returns\n1203         -------\n1204         shifted : Variable\n1205             Variable with the same dimensions and attributes but shifted data.\n1206         \"\"\"\n1207         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"shift\")\n1208         result = self\n1209         for dim, count in shifts.items():\n1210             result = result._shift_one_dim(dim, count, fill_value=fill_value)\n1211         return result\n1212 \n1213     def _pad_options_dim_to_index(\n1214         self,\n1215         pad_option: Mapping[Hashable, Union[int, Tuple[int, int]]],\n1216         fill_with_shape=False,\n1217     ):\n1218         if fill_with_shape:\n1219             return [\n1220                 (n, n) if d not in pad_option else pad_option[d]\n1221                 for d, n in zip(self.dims, self.data.shape)\n1222             ]\n1223         return [(0, 0) if d not in pad_option else pad_option[d] for d in self.dims]\n1224 \n1225     def pad(\n1226         self,\n1227         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,\n1228         mode: str = \"constant\",\n1229         stat_length: Union[\n1230             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1231         ] = None,\n1232         constant_values: Union[\n1233             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1234         ] = None,\n1235         end_values: Union[\n1236             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]\n1237         ] = None,\n1238         reflect_type: str = None,\n1239         **pad_width_kwargs: Any,\n1240     ):\n1241         \"\"\"\n1242         Return a new Variable with padded data.\n1243 \n1244         Parameters\n1245         ----------\n1246         pad_width : mapping of hashable to tuple of int\n1247             Mapping with the form of {dim: (pad_before, pad_after)}\n1248             describing the number of values padded along each dimension.\n1249             {dim: pad} is a shortcut for pad_before = pad_after = pad\n1250         mode : str, default: \"constant\"\n1251             See numpy / Dask docs\n1252         stat_length : int, tuple or mapping of hashable to tuple\n1253             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n1254             values at edge of each axis used to calculate the statistic value.\n1255         constant_values : scalar, tuple or mapping of hashable to tuple\n1256             Used in 'constant'.  The values to set the padded values for each\n1257             axis.\n1258         end_values : scalar, tuple or mapping of hashable to tuple\n1259             Used in 'linear_ramp'.  The values used for the ending value of the\n1260             linear_ramp and that will form the edge of the padded array.\n1261         reflect_type : {\"even\", \"odd\"}, optional\n1262             Used in \"reflect\", and \"symmetric\".  The \"even\" style is the\n1263             default with an unaltered reflection around the edge value.  For\n1264             the \"odd\" style, the extended part of the array is created by\n1265             subtracting the reflected values from two times the edge value.\n1266         **pad_width_kwargs\n1267             One of pad_width or pad_width_kwargs must be provided.\n1268 \n1269         Returns\n1270         -------\n1271         padded : Variable\n1272             Variable with the same dimensions and attributes but padded data.\n1273         \"\"\"\n1274         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, \"pad\")\n1275 \n1276         # change default behaviour of pad with mode constant\n1277         if mode == \"constant\" and (\n1278             constant_values is None or constant_values is dtypes.NA\n1279         ):\n1280             dtype, constant_values = dtypes.maybe_promote(self.dtype)\n1281         else:\n1282             dtype = self.dtype\n1283 \n1284         # create pad_options_kwargs, numpy requires only relevant kwargs to be nonempty\n1285         if isinstance(stat_length, dict):\n1286             stat_length = self._pad_options_dim_to_index(\n1287                 stat_length, fill_with_shape=True\n1288             )\n1289         if isinstance(constant_values, dict):\n1290             constant_values = self._pad_options_dim_to_index(constant_values)\n1291         if isinstance(end_values, dict):\n1292             end_values = self._pad_options_dim_to_index(end_values)\n1293 \n1294         # workaround for bug in Dask's default value of stat_length https://github.com/dask/dask/issues/5303\n1295         if stat_length is None and mode in [\"maximum\", \"mean\", \"median\", \"minimum\"]:\n1296             stat_length = [(n, n) for n in self.data.shape]  # type: ignore\n1297 \n1298         # change integer values to a tuple of two of those values and change pad_width to index\n1299         for k, v in pad_width.items():\n1300             if isinstance(v, numbers.Number):\n1301                 pad_width[k] = (v, v)\n1302         pad_width_by_index = self._pad_options_dim_to_index(pad_width)\n1303 \n1304         # create pad_options_kwargs, numpy/dask requires only relevant kwargs to be nonempty\n1305         pad_option_kwargs = {}\n1306         if stat_length is not None:\n1307             pad_option_kwargs[\"stat_length\"] = stat_length\n1308         if constant_values is not None:\n1309             pad_option_kwargs[\"constant_values\"] = constant_values\n1310         if end_values is not None:\n1311             pad_option_kwargs[\"end_values\"] = end_values\n1312         if reflect_type is not None:\n1313             pad_option_kwargs[\"reflect_type\"] = reflect_type  # type: ignore\n1314 \n1315         array = duck_array_ops.pad(\n1316             self.data.astype(dtype, copy=False),\n1317             pad_width_by_index,\n1318             mode=mode,\n1319             **pad_option_kwargs,\n1320         )\n1321 \n1322         return type(self)(self.dims, array)\n1323 \n1324     def _roll_one_dim(self, dim, count):\n1325         axis = self.get_axis_num(dim)\n1326 \n1327         count %= self.shape[axis]\n1328         if count != 0:\n1329             indices = [slice(-count, None), slice(None, -count)]\n1330         else:\n1331             indices = [slice(None)]\n1332 \n1333         arrays = [self[(slice(None),) * axis + (idx,)].data for idx in indices]\n1334 \n1335         data = duck_array_ops.concatenate(arrays, axis)\n1336 \n1337         if is_duck_dask_array(data):\n1338             # chunked data should come out with the same chunks; this makes\n1339             # it feasible to combine shifted and unshifted data\n1340             # TODO: remove this once dask.array automatically aligns chunks\n1341             data = data.rechunk(self.data.chunks)\n1342 \n1343         return type(self)(self.dims, data, self._attrs, fastpath=True)\n1344 \n1345     def roll(self, shifts=None, **shifts_kwargs):\n1346         \"\"\"\n1347         Return a new Variable with rolld data.\n1348 \n1349         Parameters\n1350         ----------\n1351         shifts : mapping of hashable to int\n1352             Integer offset to roll along each of the given dimensions.\n1353             Positive offsets roll to the right; negative offsets roll to the\n1354             left.\n1355         **shifts_kwargs\n1356             The keyword arguments form of ``shifts``.\n1357             One of shifts or shifts_kwargs must be provided.\n1358 \n1359         Returns\n1360         -------\n1361         shifted : Variable\n1362             Variable with the same dimensions and attributes but rolled data.\n1363         \"\"\"\n1364         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, \"roll\")\n1365 \n1366         result = self\n1367         for dim, count in shifts.items():\n1368             result = result._roll_one_dim(dim, count)\n1369         return result\n1370 \n1371     def transpose(self, *dims) -> \"Variable\":\n1372         \"\"\"Return a new Variable object with transposed dimensions.\n1373 \n1374         Parameters\n1375         ----------\n1376         *dims : str, optional\n1377             By default, reverse the dimensions. Otherwise, reorder the\n1378             dimensions to this order.\n1379 \n1380         Returns\n1381         -------\n1382         transposed : Variable\n1383             The returned object has transposed data and dimensions with the\n1384             same attributes as the original.\n1385 \n1386         Notes\n1387         -----\n1388         This operation returns a view of this variable's data. It is\n1389         lazy for dask-backed Variables but not for numpy-backed Variables.\n1390 \n1391         See Also\n1392         --------\n1393         numpy.transpose\n1394         \"\"\"\n1395         if len(dims) == 0:\n1396             dims = self.dims[::-1]\n1397         dims = tuple(infix_dims(dims, self.dims))\n1398         axes = self.get_axis_num(dims)\n1399         if len(dims) < 2 or dims == self.dims:\n1400             # no need to transpose if only one dimension\n1401             # or dims are in same order\n1402             return self.copy(deep=False)\n1403 \n1404         data = as_indexable(self._data).transpose(axes)\n1405         return type(self)(dims, data, self._attrs, self._encoding, fastpath=True)\n1406 \n1407     @property\n1408     def T(self) -> \"Variable\":\n1409         return self.transpose()\n1410 \n1411     def set_dims(self, dims, shape=None):\n1412         \"\"\"Return a new variable with given set of dimensions.\n1413         This method might be used to attach new dimension(s) to variable.\n1414 \n1415         When possible, this operation does not copy this variable's data.\n1416 \n1417         Parameters\n1418         ----------\n1419         dims : str or sequence of str or dict\n1420             Dimensions to include on the new variable. If a dict, values are\n1421             used to provide the sizes of new dimensions; otherwise, new\n1422             dimensions are inserted with length 1.\n1423 \n1424         Returns\n1425         -------\n1426         Variable\n1427         \"\"\"\n1428         if isinstance(dims, str):\n1429             dims = [dims]\n1430 \n1431         if shape is None and utils.is_dict_like(dims):\n1432             shape = dims.values()\n1433 \n1434         missing_dims = set(self.dims) - set(dims)\n1435         if missing_dims:\n1436             raise ValueError(\n1437                 \"new dimensions %r must be a superset of \"\n1438                 \"existing dimensions %r\" % (dims, self.dims)\n1439             )\n1440 \n1441         self_dims = set(self.dims)\n1442         expanded_dims = tuple(d for d in dims if d not in self_dims) + self.dims\n1443 \n1444         if self.dims == expanded_dims:\n1445             # don't use broadcast_to unless necessary so the result remains\n1446             # writeable if possible\n1447             expanded_data = self.data\n1448         elif shape is not None:\n1449             dims_map = dict(zip(dims, shape))\n1450             tmp_shape = tuple(dims_map[d] for d in expanded_dims)\n1451             expanded_data = duck_array_ops.broadcast_to(self.data, tmp_shape)\n1452         else:\n1453             expanded_data = self.data[(None,) * (len(expanded_dims) - self.ndim)]\n1454 \n1455         expanded_var = Variable(\n1456             expanded_dims, expanded_data, self._attrs, self._encoding, fastpath=True\n1457         )\n1458         return expanded_var.transpose(*dims)\n1459 \n1460     def _stack_once(self, dims, new_dim):\n1461         if not set(dims) <= set(self.dims):\n1462             raise ValueError(\"invalid existing dimensions: %s\" % dims)\n1463 \n1464         if new_dim in self.dims:\n1465             raise ValueError(\n1466                 \"cannot create a new dimension with the same \"\n1467                 \"name as an existing dimension\"\n1468             )\n1469 \n1470         if len(dims) == 0:\n1471             # don't stack\n1472             return self.copy(deep=False)\n1473 \n1474         other_dims = [d for d in self.dims if d not in dims]\n1475         dim_order = other_dims + list(dims)\n1476         reordered = self.transpose(*dim_order)\n1477 \n1478         new_shape = reordered.shape[: len(other_dims)] + (-1,)\n1479         new_data = reordered.data.reshape(new_shape)\n1480         new_dims = reordered.dims[: len(other_dims)] + (new_dim,)\n1481 \n1482         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1483 \n1484     def stack(self, dimensions=None, **dimensions_kwargs):\n1485         \"\"\"\n1486         Stack any number of existing dimensions into a single new dimension.\n1487 \n1488         New dimensions will be added at the end, and the order of the data\n1489         along each new dimension will be in contiguous (C) order.\n1490 \n1491         Parameters\n1492         ----------\n1493         dimensions : mapping of hashable to tuple of hashable\n1494             Mapping of form new_name=(dim1, dim2, ...) describing the\n1495             names of new dimensions, and the existing dimensions that\n1496             they replace.\n1497         **dimensions_kwargs\n1498             The keyword arguments form of ``dimensions``.\n1499             One of dimensions or dimensions_kwargs must be provided.\n1500 \n1501         Returns\n1502         -------\n1503         stacked : Variable\n1504             Variable with the same attributes but stacked data.\n1505 \n1506         See also\n1507         --------\n1508         Variable.unstack\n1509         \"\"\"\n1510         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"stack\")\n1511         result = self\n1512         for new_dim, dims in dimensions.items():\n1513             result = result._stack_once(dims, new_dim)\n1514         return result\n1515 \n1516     def _unstack_once(self, dims, old_dim):\n1517         new_dim_names = tuple(dims.keys())\n1518         new_dim_sizes = tuple(dims.values())\n1519 \n1520         if old_dim not in self.dims:\n1521             raise ValueError(\"invalid existing dimension: %s\" % old_dim)\n1522 \n1523         if set(new_dim_names).intersection(self.dims):\n1524             raise ValueError(\n1525                 \"cannot create a new dimension with the same \"\n1526                 \"name as an existing dimension\"\n1527             )\n1528 \n1529         if np.prod(new_dim_sizes) != self.sizes[old_dim]:\n1530             raise ValueError(\n1531                 \"the product of the new dimension sizes must \"\n1532                 \"equal the size of the old dimension\"\n1533             )\n1534 \n1535         other_dims = [d for d in self.dims if d != old_dim]\n1536         dim_order = other_dims + [old_dim]\n1537         reordered = self.transpose(*dim_order)\n1538 \n1539         new_shape = reordered.shape[: len(other_dims)] + new_dim_sizes\n1540         new_data = reordered.data.reshape(new_shape)\n1541         new_dims = reordered.dims[: len(other_dims)] + new_dim_names\n1542 \n1543         return Variable(new_dims, new_data, self._attrs, self._encoding, fastpath=True)\n1544 \n1545     def unstack(self, dimensions=None, **dimensions_kwargs):\n1546         \"\"\"\n1547         Unstack an existing dimension into multiple new dimensions.\n1548 \n1549         New dimensions will be added at the end, and the order of the data\n1550         along each new dimension will be in contiguous (C) order.\n1551 \n1552         Parameters\n1553         ----------\n1554         dimensions : mapping of hashable to mapping of hashable to int\n1555             Mapping of the form old_dim={dim1: size1, ...} describing the\n1556             names of existing dimensions, and the new dimensions and sizes\n1557             that they map to.\n1558         **dimensions_kwargs\n1559             The keyword arguments form of ``dimensions``.\n1560             One of dimensions or dimensions_kwargs must be provided.\n1561 \n1562         Returns\n1563         -------\n1564         unstacked : Variable\n1565             Variable with the same attributes but unstacked data.\n1566 \n1567         See also\n1568         --------\n1569         Variable.stack\n1570         \"\"\"\n1571         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, \"unstack\")\n1572         result = self\n1573         for old_dim, dims in dimensions.items():\n1574             result = result._unstack_once(dims, old_dim)\n1575         return result\n1576 \n1577     def fillna(self, value):\n1578         return ops.fillna(self, value)\n1579 \n1580     def where(self, cond, other=dtypes.NA):\n1581         return ops.where_method(self, cond, other)\n1582 \n1583     def reduce(\n1584         self,\n1585         func,\n1586         dim=None,\n1587         axis=None,\n1588         keep_attrs=None,\n1589         keepdims=False,\n1590         **kwargs,\n1591     ):\n1592         \"\"\"Reduce this array by applying `func` along some dimension(s).\n1593 \n1594         Parameters\n1595         ----------\n1596         func : callable\n1597             Function which can be called in the form\n1598             `func(x, axis=axis, **kwargs)` to return the result of reducing an\n1599             np.ndarray over an integer valued axis.\n1600         dim : str or sequence of str, optional\n1601             Dimension(s) over which to apply `func`.\n1602         axis : int or sequence of int, optional\n1603             Axis(es) over which to apply `func`. Only one of the 'dim'\n1604             and 'axis' arguments can be supplied. If neither are supplied, then\n1605             the reduction is calculated over the flattened array (by calling\n1606             `func(x)` without an axis argument).\n1607         keep_attrs : bool, optional\n1608             If True, the variable's attributes (`attrs`) will be copied from\n1609             the original object to the new one.  If False (default), the new\n1610             object will be returned without attributes.\n1611         keepdims : bool, default: False\n1612             If True, the dimensions which are reduced are left in the result\n1613             as dimensions of size one\n1614         **kwargs : dict\n1615             Additional keyword arguments passed on to `func`.\n1616 \n1617         Returns\n1618         -------\n1619         reduced : Array\n1620             Array with summarized data and the indicated dimension(s)\n1621             removed.\n1622         \"\"\"\n1623         if dim == ...:\n1624             dim = None\n1625         if dim is not None and axis is not None:\n1626             raise ValueError(\"cannot supply both 'axis' and 'dim' arguments\")\n1627 \n1628         if dim is not None:\n1629             axis = self.get_axis_num(dim)\n1630 \n1631         with warnings.catch_warnings():\n1632             warnings.filterwarnings(\n1633                 \"ignore\", r\"Mean of empty slice\", category=RuntimeWarning\n1634             )\n1635             if axis is not None:\n1636                 data = func(self.data, axis=axis, **kwargs)\n1637             else:\n1638                 data = func(self.data, **kwargs)\n1639 \n1640         if getattr(data, \"shape\", ()) == self.shape:\n1641             dims = self.dims\n1642         else:\n1643             removed_axes = (\n1644                 range(self.ndim) if axis is None else np.atleast_1d(axis) % self.ndim\n1645             )\n1646             if keepdims:\n1647                 # Insert np.newaxis for removed dims\n1648                 slices = tuple(\n1649                     np.newaxis if i in removed_axes else slice(None, None)\n1650                     for i in range(self.ndim)\n1651                 )\n1652                 if getattr(data, \"shape\", None) is None:\n1653                     # Reduce has produced a scalar value, not an array-like\n1654                     data = np.asanyarray(data)[slices]\n1655                 else:\n1656                     data = data[slices]\n1657                 dims = self.dims\n1658             else:\n1659                 dims = [\n1660                     adim for n, adim in enumerate(self.dims) if n not in removed_axes\n1661                 ]\n1662 \n1663         if keep_attrs is None:\n1664             keep_attrs = _get_keep_attrs(default=False)\n1665         attrs = self._attrs if keep_attrs else None\n1666 \n1667         return Variable(dims, data, attrs=attrs)\n1668 \n1669     @classmethod\n1670     def concat(cls, variables, dim=\"concat_dim\", positions=None, shortcut=False):\n1671         \"\"\"Concatenate variables along a new or existing dimension.\n1672 \n1673         Parameters\n1674         ----------\n1675         variables : iterable of Variable\n1676             Arrays to stack together. Each variable is expected to have\n1677             matching dimensions and shape except for along the stacked\n1678             dimension.\n1679         dim : str or DataArray, optional\n1680             Name of the dimension to stack along. This can either be a new\n1681             dimension name, in which case it is added along axis=0, or an\n1682             existing dimension name, in which case the location of the\n1683             dimension is unchanged. Where to insert the new dimension is\n1684             determined by the first variable.\n1685         positions : None or list of array-like, optional\n1686             List of integer arrays which specifies the integer positions to\n1687             which to assign each dataset along the concatenated dimension.\n1688             If not supplied, objects are concatenated in the provided order.\n1689         shortcut : bool, optional\n1690             This option is used internally to speed-up groupby operations.\n1691             If `shortcut` is True, some checks of internal consistency between\n1692             arrays to concatenate are skipped.\n1693 \n1694         Returns\n1695         -------\n1696         stacked : Variable\n1697             Concatenated Variable formed by stacking all the supplied variables\n1698             along the given dimension.\n1699         \"\"\"\n1700         if not isinstance(dim, str):\n1701             (dim,) = dim.dims\n1702 \n1703         # can't do this lazily: we need to loop through variables at least\n1704         # twice\n1705         variables = list(variables)\n1706         first_var = variables[0]\n1707 \n1708         arrays = [v.data for v in variables]\n1709 \n1710         if dim in first_var.dims:\n1711             axis = first_var.get_axis_num(dim)\n1712             dims = first_var.dims\n1713             data = duck_array_ops.concatenate(arrays, axis=axis)\n1714             if positions is not None:\n1715                 # TODO: deprecate this option -- we don't need it for groupby\n1716                 # any more.\n1717                 indices = nputils.inverse_permutation(np.concatenate(positions))\n1718                 data = duck_array_ops.take(data, indices, axis=axis)\n1719         else:\n1720             axis = 0\n1721             dims = (dim,) + first_var.dims\n1722             data = duck_array_ops.stack(arrays, axis=axis)\n1723 \n1724         attrs = dict(first_var.attrs)\n1725         encoding = dict(first_var.encoding)\n1726         if not shortcut:\n1727             for var in variables:\n1728                 if var.dims != first_var.dims:\n1729                     raise ValueError(\n1730                         f\"Variable has dimensions {list(var.dims)} but first Variable has dimensions {list(first_var.dims)}\"\n1731                     )\n1732 \n1733         return cls(dims, data, attrs, encoding)\n1734 \n1735     def equals(self, other, equiv=duck_array_ops.array_equiv):\n1736         \"\"\"True if two Variables have the same dimensions and values;\n1737         otherwise False.\n1738 \n1739         Variables can still be equal (like pandas objects) if they have NaN\n1740         values in the same locations.\n1741 \n1742         This method is necessary because `v1 == v2` for Variables\n1743         does element-wise comparisons (like numpy.ndarrays).\n1744         \"\"\"\n1745         other = getattr(other, \"variable\", other)\n1746         try:\n1747             return self.dims == other.dims and (\n1748                 self._data is other._data or equiv(self.data, other.data)\n1749             )\n1750         except (TypeError, AttributeError):\n1751             return False\n1752 \n1753     def broadcast_equals(self, other, equiv=duck_array_ops.array_equiv):\n1754         \"\"\"True if two Variables have the values after being broadcast against\n1755         each other; otherwise False.\n1756 \n1757         Variables can still be equal (like pandas objects) if they have NaN\n1758         values in the same locations.\n1759         \"\"\"\n1760         try:\n1761             self, other = broadcast_variables(self, other)\n1762         except (ValueError, AttributeError):\n1763             return False\n1764         return self.equals(other, equiv=equiv)\n1765 \n1766     def identical(self, other, equiv=duck_array_ops.array_equiv):\n1767         \"\"\"Like equals, but also checks attributes.\"\"\"\n1768         try:\n1769             return utils.dict_equiv(self.attrs, other.attrs) and self.equals(\n1770                 other, equiv=equiv\n1771             )\n1772         except (TypeError, AttributeError):\n1773             return False\n1774 \n1775     def no_conflicts(self, other, equiv=duck_array_ops.array_notnull_equiv):\n1776         \"\"\"True if the intersection of two Variable's non-null data is\n1777         equal; otherwise false.\n1778 \n1779         Variables can thus still be equal if there are locations where either,\n1780         or both, contain NaN values.\n1781         \"\"\"\n1782         return self.broadcast_equals(other, equiv=equiv)\n1783 \n1784     def quantile(\n1785         self, q, dim=None, interpolation=\"linear\", keep_attrs=None, skipna=True\n1786     ):\n1787         \"\"\"Compute the qth quantile of the data along the specified dimension.\n1788 \n1789         Returns the qth quantiles(s) of the array elements.\n1790 \n1791         Parameters\n1792         ----------\n1793         q : float or sequence of float\n1794             Quantile to compute, which must be between 0 and 1\n1795             inclusive.\n1796         dim : str or sequence of str, optional\n1797             Dimension(s) over which to apply quantile.\n1798         interpolation : {\"linear\", \"lower\", \"higher\", \"midpoint\", \"nearest\"}, default: \"linear\"\n1799             This optional parameter specifies the interpolation method to\n1800             use when the desired quantile lies between two data points\n1801             ``i < j``:\n1802 \n1803                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is\n1804                   the fractional part of the index surrounded by ``i`` and\n1805                   ``j``.\n1806                 * lower: ``i``.\n1807                 * higher: ``j``.\n1808                 * nearest: ``i`` or ``j``, whichever is nearest.\n1809                 * midpoint: ``(i + j) / 2``.\n1810 \n1811         keep_attrs : bool, optional\n1812             If True, the variable's attributes (`attrs`) will be copied from\n1813             the original object to the new one.  If False (default), the new\n1814             object will be returned without attributes.\n1815 \n1816         Returns\n1817         -------\n1818         quantiles : Variable\n1819             If `q` is a single quantile, then the result\n1820             is a scalar. If multiple percentiles are given, first axis of\n1821             the result corresponds to the quantile and a quantile dimension\n1822             is added to the return array. The other dimensions are the\n1823             dimensions that remain after the reduction of the array.\n1824 \n1825         See Also\n1826         --------\n1827         numpy.nanquantile, pandas.Series.quantile, Dataset.quantile,\n1828         DataArray.quantile\n1829         \"\"\"\n1830 \n1831         from .computation import apply_ufunc\n1832 \n1833         _quantile_func = np.nanquantile if skipna else np.quantile\n1834 \n1835         if keep_attrs is None:\n1836             keep_attrs = _get_keep_attrs(default=False)\n1837 \n1838         scalar = utils.is_scalar(q)\n1839         q = np.atleast_1d(np.asarray(q, dtype=np.float64))\n1840 \n1841         if dim is None:\n1842             dim = self.dims\n1843 \n1844         if utils.is_scalar(dim):\n1845             dim = [dim]\n1846 \n1847         def _wrapper(npa, **kwargs):\n1848             # move quantile axis to end. required for apply_ufunc\n1849             return np.moveaxis(_quantile_func(npa, **kwargs), 0, -1)\n1850 \n1851         axis = np.arange(-1, -1 * len(dim) - 1, -1)\n1852         result = apply_ufunc(\n1853             _wrapper,\n1854             self,\n1855             input_core_dims=[dim],\n1856             exclude_dims=set(dim),\n1857             output_core_dims=[[\"quantile\"]],\n1858             output_dtypes=[np.float64],\n1859             dask_gufunc_kwargs=dict(output_sizes={\"quantile\": len(q)}),\n1860             dask=\"parallelized\",\n1861             kwargs={\"q\": q, \"axis\": axis, \"interpolation\": interpolation},\n1862         )\n1863 \n1864         # for backward compatibility\n1865         result = result.transpose(\"quantile\", ...)\n1866         if scalar:\n1867             result = result.squeeze(\"quantile\")\n1868         if keep_attrs:\n1869             result.attrs = self._attrs\n1870         return result\n1871 \n1872     def rank(self, dim, pct=False):\n1873         \"\"\"Ranks the data.\n1874 \n1875         Equal values are assigned a rank that is the average of the ranks that\n1876         would have been otherwise assigned to all of the values within that\n1877         set.  Ranks begin at 1, not 0. If `pct`, computes percentage ranks.\n1878 \n1879         NaNs in the input array are returned as NaNs.\n1880 \n1881         The `bottleneck` library is required.\n1882 \n1883         Parameters\n1884         ----------\n1885         dim : str\n1886             Dimension over which to compute rank.\n1887         pct : bool, optional\n1888             If True, compute percentage ranks, otherwise compute integer ranks.\n1889 \n1890         Returns\n1891         -------\n1892         ranked : Variable\n1893 \n1894         See Also\n1895         --------\n1896         Dataset.rank, DataArray.rank\n1897         \"\"\"\n1898         import bottleneck as bn\n1899 \n1900         data = self.data\n1901 \n1902         if is_duck_dask_array(data):\n1903             raise TypeError(\n1904                 \"rank does not work for arrays stored as dask \"\n1905                 \"arrays. Load the data via .compute() or .load() \"\n1906                 \"prior to calling this method.\"\n1907             )\n1908         elif not isinstance(data, np.ndarray):\n1909             raise TypeError(\n1910                 \"rank is not implemented for {} objects.\".format(type(data))\n1911             )\n1912 \n1913         axis = self.get_axis_num(dim)\n1914         func = bn.nanrankdata if self.dtype.kind == \"f\" else bn.rankdata\n1915         ranked = func(data, axis=axis)\n1916         if pct:\n1917             count = np.sum(~np.isnan(data), axis=axis, keepdims=True)\n1918             ranked /= count\n1919         return Variable(self.dims, ranked)\n1920 \n1921     def rolling_window(\n1922         self, dim, window, window_dim, center=False, fill_value=dtypes.NA\n1923     ):\n1924         \"\"\"\n1925         Make a rolling_window along dim and add a new_dim to the last place.\n1926 \n1927         Parameters\n1928         ----------\n1929         dim : str\n1930             Dimension over which to compute rolling_window.\n1931             For nd-rolling, should be list of dimensions.\n1932         window : int\n1933             Window size of the rolling\n1934             For nd-rolling, should be list of integers.\n1935         window_dim : str\n1936             New name of the window dimension.\n1937             For nd-rolling, should be list of integers.\n1938         center : bool, default: False\n1939             If True, pad fill_value for both ends. Otherwise, pad in the head\n1940             of the axis.\n1941         fill_value\n1942             value to be filled.\n1943 \n1944         Returns\n1945         -------\n1946         Variable that is a view of the original array with a added dimension of\n1947         size w.\n1948         The return dim: self.dims + (window_dim, )\n1949         The return shape: self.shape + (window, )\n1950 \n1951         Examples\n1952         --------\n1953         >>> v = Variable((\"a\", \"b\"), np.arange(8).reshape((2, 4)))\n1954         >>> v.rolling_window(\"b\", 3, \"window_dim\")\n1955         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1956         array([[[nan, nan,  0.],\n1957                 [nan,  0.,  1.],\n1958                 [ 0.,  1.,  2.],\n1959                 [ 1.,  2.,  3.]],\n1960         <BLANKLINE>\n1961                [[nan, nan,  4.],\n1962                 [nan,  4.,  5.],\n1963                 [ 4.,  5.,  6.],\n1964                 [ 5.,  6.,  7.]]])\n1965 \n1966         >>> v.rolling_window(\"b\", 3, \"window_dim\", center=True)\n1967         <xarray.Variable (a: 2, b: 4, window_dim: 3)>\n1968         array([[[nan,  0.,  1.],\n1969                 [ 0.,  1.,  2.],\n1970                 [ 1.,  2.,  3.],\n1971                 [ 2.,  3., nan]],\n1972         <BLANKLINE>\n1973                [[nan,  4.,  5.],\n1974                 [ 4.,  5.,  6.],\n1975                 [ 5.,  6.,  7.],\n1976                 [ 6.,  7., nan]]])\n1977         \"\"\"\n1978         if fill_value is dtypes.NA:  # np.nan is passed\n1979             dtype, fill_value = dtypes.maybe_promote(self.dtype)\n1980             array = self.astype(dtype, copy=False).data\n1981         else:\n1982             dtype = self.dtype\n1983             array = self.data\n1984 \n1985         if isinstance(dim, list):\n1986             assert len(dim) == len(window)\n1987             assert len(dim) == len(window_dim)\n1988             assert len(dim) == len(center)\n1989         else:\n1990             dim = [dim]\n1991             window = [window]\n1992             window_dim = [window_dim]\n1993             center = [center]\n1994         axis = [self.get_axis_num(d) for d in dim]\n1995         new_dims = self.dims + tuple(window_dim)\n1996         return Variable(\n1997             new_dims,\n1998             duck_array_ops.rolling_window(\n1999                 array, axis=axis, window=window, center=center, fill_value=fill_value\n2000             ),\n2001         )\n2002 \n2003     def coarsen(\n2004         self, windows, func, boundary=\"exact\", side=\"left\", keep_attrs=None, **kwargs\n2005     ):\n2006         \"\"\"\n2007         Apply reduction function.\n2008         \"\"\"\n2009         windows = {k: v for k, v in windows.items() if k in self.dims}\n2010         if not windows:\n2011             return self.copy()\n2012 \n2013         if keep_attrs is None:\n2014             keep_attrs = _get_keep_attrs(default=False)\n2015 \n2016         if keep_attrs:\n2017             _attrs = self.attrs\n2018         else:\n2019             _attrs = None\n2020 \n2021         reshaped, axes = self._coarsen_reshape(windows, boundary, side)\n2022         if isinstance(func, str):\n2023             name = func\n2024             func = getattr(duck_array_ops, name, None)\n2025             if func is None:\n2026                 raise NameError(f\"{name} is not a valid method.\")\n2027 \n2028         return self._replace(data=func(reshaped, axis=axes, **kwargs), attrs=_attrs)\n2029 \n2030     def _coarsen_reshape(self, windows, boundary, side):\n2031         \"\"\"\n2032         Construct a reshaped-array for coarsen\n2033         \"\"\"\n2034         if not utils.is_dict_like(boundary):\n2035             boundary = {d: boundary for d in windows.keys()}\n2036 \n2037         if not utils.is_dict_like(side):\n2038             side = {d: side for d in windows.keys()}\n2039 \n2040         # remove unrelated dimensions\n2041         boundary = {k: v for k, v in boundary.items() if k in windows}\n2042         side = {k: v for k, v in side.items() if k in windows}\n2043 \n2044         for d, window in windows.items():\n2045             if window <= 0:\n2046                 raise ValueError(f\"window must be > 0. Given {window}\")\n2047 \n2048         variable = self\n2049         for d, window in windows.items():\n2050             # trim or pad the object\n2051             size = variable.shape[self._get_axis_num(d)]\n2052             n = int(size / window)\n2053             if boundary[d] == \"exact\":\n2054                 if n * window != size:\n2055                     raise ValueError(\n2056                         \"Could not coarsen a dimension of size {} with \"\n2057                         \"window {}\".format(size, window)\n2058                     )\n2059             elif boundary[d] == \"trim\":\n2060                 if side[d] == \"left\":\n2061                     variable = variable.isel({d: slice(0, window * n)})\n2062                 else:\n2063                     excess = size - window * n\n2064                     variable = variable.isel({d: slice(excess, None)})\n2065             elif boundary[d] == \"pad\":  # pad\n2066                 pad = window * n - size\n2067                 if pad < 0:\n2068                     pad += window\n2069                 if side[d] == \"left\":\n2070                     pad_width = {d: (0, pad)}\n2071                 else:\n2072                     pad_width = {d: (pad, 0)}\n2073                 variable = variable.pad(pad_width, mode=\"constant\")\n2074             else:\n2075                 raise TypeError(\n2076                     \"{} is invalid for boundary. Valid option is 'exact', \"\n2077                     \"'trim' and 'pad'\".format(boundary[d])\n2078                 )\n2079 \n2080         shape = []\n2081         axes = []\n2082         axis_count = 0\n2083         for i, d in enumerate(variable.dims):\n2084             if d in windows:\n2085                 size = variable.shape[i]\n2086                 shape.append(int(size / windows[d]))\n2087                 shape.append(windows[d])\n2088                 axis_count += 1\n2089                 axes.append(i + axis_count)\n2090             else:\n2091                 shape.append(variable.shape[i])\n2092 \n2093         return variable.data.reshape(shape), tuple(axes)\n2094 \n2095     def isnull(self, keep_attrs: bool = None):\n2096         \"\"\"Test each value in the array for whether it is a missing value.\n2097 \n2098         Returns\n2099         -------\n2100         isnull : Variable\n2101             Same type and shape as object, but the dtype of the data is bool.\n2102 \n2103         See Also\n2104         --------\n2105         pandas.isnull\n2106 \n2107         Examples\n2108         --------\n2109         >>> var = xr.Variable(\"x\", [1, np.nan, 3])\n2110         >>> var\n2111         <xarray.Variable (x: 3)>\n2112         array([ 1., nan,  3.])\n2113         >>> var.isnull()\n2114         <xarray.Variable (x: 3)>\n2115         array([False,  True, False])\n2116         \"\"\"\n2117         from .computation import apply_ufunc\n2118 \n2119         if keep_attrs is None:\n2120             keep_attrs = _get_keep_attrs(default=False)\n2121 \n2122         return apply_ufunc(\n2123             duck_array_ops.isnull,\n2124             self,\n2125             dask=\"allowed\",\n2126             keep_attrs=keep_attrs,\n2127         )\n2128 \n2129     def notnull(self, keep_attrs: bool = None):\n2130         \"\"\"Test each value in the array for whether it is not a missing value.\n2131 \n2132         Returns\n2133         -------\n2134         notnull : Variable\n2135             Same type and shape as object, but the dtype of the data is bool.\n2136 \n2137         See Also\n2138         --------\n2139         pandas.notnull\n2140 \n2141         Examples\n2142         --------\n2143         >>> var = xr.Variable(\"x\", [1, np.nan, 3])\n2144         >>> var\n2145         <xarray.Variable (x: 3)>\n2146         array([ 1., nan,  3.])\n2147         >>> var.notnull()\n2148         <xarray.Variable (x: 3)>\n2149         array([ True, False,  True])\n2150         \"\"\"\n2151         from .computation import apply_ufunc\n2152 \n2153         if keep_attrs is None:\n2154             keep_attrs = _get_keep_attrs(default=False)\n2155 \n2156         return apply_ufunc(\n2157             duck_array_ops.notnull,\n2158             self,\n2159             dask=\"allowed\",\n2160             keep_attrs=keep_attrs,\n2161         )\n2162 \n2163     @property\n2164     def real(self):\n2165         return type(self)(self.dims, self.data.real, self._attrs)\n2166 \n2167     @property\n2168     def imag(self):\n2169         return type(self)(self.dims, self.data.imag, self._attrs)\n2170 \n2171     def __array_wrap__(self, obj, context=None):\n2172         return Variable(self.dims, obj)\n2173 \n2174     @staticmethod\n2175     def _unary_op(f):\n2176         @functools.wraps(f)\n2177         def func(self, *args, **kwargs):\n2178             keep_attrs = kwargs.pop(\"keep_attrs\", None)\n2179             if keep_attrs is None:\n2180                 keep_attrs = _get_keep_attrs(default=True)\n2181             with np.errstate(all=\"ignore\"):\n2182                 result = self.__array_wrap__(f(self.data, *args, **kwargs))\n2183                 if keep_attrs:\n2184                     result.attrs = self.attrs\n2185                 return result\n2186 \n2187         return func\n2188 \n2189     @staticmethod\n2190     def _binary_op(f, reflexive=False, **ignored_kwargs):\n2191         @functools.wraps(f)\n2192         def func(self, other):\n2193             if isinstance(other, (xr.DataArray, xr.Dataset)):\n2194                 return NotImplemented\n2195             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2196             keep_attrs = _get_keep_attrs(default=False)\n2197             attrs = self._attrs if keep_attrs else None\n2198             with np.errstate(all=\"ignore\"):\n2199                 new_data = (\n2200                     f(self_data, other_data)\n2201                     if not reflexive\n2202                     else f(other_data, self_data)\n2203                 )\n2204             result = Variable(dims, new_data, attrs=attrs)\n2205             return result\n2206 \n2207         return func\n2208 \n2209     @staticmethod\n2210     def _inplace_binary_op(f):\n2211         @functools.wraps(f)\n2212         def func(self, other):\n2213             if isinstance(other, xr.Dataset):\n2214                 raise TypeError(\"cannot add a Dataset to a Variable in-place\")\n2215             self_data, other_data, dims = _broadcast_compat_data(self, other)\n2216             if dims != self.dims:\n2217                 raise ValueError(\"dimensions cannot change for in-place operations\")\n2218             with np.errstate(all=\"ignore\"):\n2219                 self.values = f(self_data, other_data)\n2220             return self\n2221 \n2222         return func\n2223 \n2224     def _to_numeric(self, offset=None, datetime_unit=None, dtype=float):\n2225         \"\"\"A (private) method to convert datetime array to numeric dtype\n2226         See duck_array_ops.datetime_to_numeric\n2227         \"\"\"\n2228         numeric_array = duck_array_ops.datetime_to_numeric(\n2229             self.data, offset, datetime_unit, dtype\n2230         )\n2231         return type(self)(self.dims, numeric_array, self._attrs)\n2232 \n2233     def _unravel_argminmax(\n2234         self,\n2235         argminmax: str,\n2236         dim: Union[Hashable, Sequence[Hashable], None],\n2237         axis: Union[int, None],\n2238         keep_attrs: Optional[bool],\n2239         skipna: Optional[bool],\n2240     ) -> Union[\"Variable\", Dict[Hashable, \"Variable\"]]:\n2241         \"\"\"Apply argmin or argmax over one or more dimensions, returning the result as a\n2242         dict of DataArray that can be passed directly to isel.\n2243         \"\"\"\n2244         if dim is None and axis is None:\n2245             warnings.warn(\n2246                 \"Behaviour of argmin/argmax with neither dim nor axis argument will \"\n2247                 \"change to return a dict of indices of each dimension. To get a \"\n2248                 \"single, flat index, please use np.argmin(da.data) or \"\n2249                 \"np.argmax(da.data) instead of da.argmin() or da.argmax().\",\n2250                 DeprecationWarning,\n2251                 stacklevel=3,\n2252             )\n2253 \n2254         argminmax_func = getattr(duck_array_ops, argminmax)\n2255 \n2256         if dim is ...:\n2257             # In future, should do this also when (dim is None and axis is None)\n2258             dim = self.dims\n2259         if (\n2260             dim is None\n2261             or axis is not None\n2262             or not isinstance(dim, Sequence)\n2263             or isinstance(dim, str)\n2264         ):\n2265             # Return int index if single dimension is passed, and is not part of a\n2266             # sequence\n2267             return self.reduce(\n2268                 argminmax_func, dim=dim, axis=axis, keep_attrs=keep_attrs, skipna=skipna\n2269             )\n2270 \n2271         # Get a name for the new dimension that does not conflict with any existing\n2272         # dimension\n2273         newdimname = \"_unravel_argminmax_dim_0\"\n2274         count = 1\n2275         while newdimname in self.dims:\n2276             newdimname = f\"_unravel_argminmax_dim_{count}\"\n2277             count += 1\n2278 \n2279         stacked = self.stack({newdimname: dim})\n2280 \n2281         result_dims = stacked.dims[:-1]\n2282         reduce_shape = tuple(self.sizes[d] for d in dim)\n2283 \n2284         result_flat_indices = stacked.reduce(argminmax_func, axis=-1, skipna=skipna)\n2285 \n2286         result_unravelled_indices = duck_array_ops.unravel_index(\n2287             result_flat_indices.data, reduce_shape\n2288         )\n2289 \n2290         result = {\n2291             d: Variable(dims=result_dims, data=i)\n2292             for d, i in zip(dim, result_unravelled_indices)\n2293         }\n2294 \n2295         if keep_attrs is None:\n2296             keep_attrs = _get_keep_attrs(default=False)\n2297         if keep_attrs:\n2298             for v in result.values():\n2299                 v.attrs = self.attrs\n2300 \n2301         return result\n2302 \n2303     def argmin(\n2304         self,\n2305         dim: Union[Hashable, Sequence[Hashable]] = None,\n2306         axis: int = None,\n2307         keep_attrs: bool = None,\n2308         skipna: bool = None,\n2309     ) -> Union[\"Variable\", Dict[Hashable, \"Variable\"]]:\n2310         \"\"\"Index or indices of the minimum of the Variable over one or more dimensions.\n2311         If a sequence is passed to 'dim', then result returned as dict of Variables,\n2312         which can be passed directly to isel(). If a single str is passed to 'dim' then\n2313         returns a Variable with dtype int.\n2314 \n2315         If there are multiple minima, the indices of the first one found will be\n2316         returned.\n2317 \n2318         Parameters\n2319         ----------\n2320         dim : hashable, sequence of hashable or ..., optional\n2321             The dimensions over which to find the minimum. By default, finds minimum over\n2322             all dimensions - for now returning an int for backward compatibility, but\n2323             this is deprecated, in future will return a dict with indices for all\n2324             dimensions; to return a dict with all dimensions now, pass '...'.\n2325         axis : int, optional\n2326             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n2327             can be supplied.\n2328         keep_attrs : bool, optional\n2329             If True, the attributes (`attrs`) will be copied from the original\n2330             object to the new one.  If False (default), the new object will be\n2331             returned without attributes.\n2332         skipna : bool, optional\n2333             If True, skip missing values (as marked by NaN). By default, only\n2334             skips missing values for float dtypes; other dtypes either do not\n2335             have a sentinel missing value (int) or skipna=True has not been\n2336             implemented (object, datetime64 or timedelta64).\n2337 \n2338         Returns\n2339         -------\n2340         result : Variable or dict of Variable\n2341 \n2342         See also\n2343         --------\n2344         DataArray.argmin, DataArray.idxmin\n2345         \"\"\"\n2346         return self._unravel_argminmax(\"argmin\", dim, axis, keep_attrs, skipna)\n2347 \n2348     def argmax(\n2349         self,\n2350         dim: Union[Hashable, Sequence[Hashable]] = None,\n2351         axis: int = None,\n2352         keep_attrs: bool = None,\n2353         skipna: bool = None,\n2354     ) -> Union[\"Variable\", Dict[Hashable, \"Variable\"]]:\n2355         \"\"\"Index or indices of the maximum of the Variable over one or more dimensions.\n2356         If a sequence is passed to 'dim', then result returned as dict of Variables,\n2357         which can be passed directly to isel(). If a single str is passed to 'dim' then\n2358         returns a Variable with dtype int.\n2359 \n2360         If there are multiple maxima, the indices of the first one found will be\n2361         returned.\n2362 \n2363         Parameters\n2364         ----------\n2365         dim : hashable, sequence of hashable or ..., optional\n2366             The dimensions over which to find the maximum. By default, finds maximum over\n2367             all dimensions - for now returning an int for backward compatibility, but\n2368             this is deprecated, in future will return a dict with indices for all\n2369             dimensions; to return a dict with all dimensions now, pass '...'.\n2370         axis : int, optional\n2371             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments\n2372             can be supplied.\n2373         keep_attrs : bool, optional\n2374             If True, the attributes (`attrs`) will be copied from the original\n2375             object to the new one.  If False (default), the new object will be\n2376             returned without attributes.\n2377         skipna : bool, optional\n2378             If True, skip missing values (as marked by NaN). By default, only\n2379             skips missing values for float dtypes; other dtypes either do not\n2380             have a sentinel missing value (int) or skipna=True has not been\n2381             implemented (object, datetime64 or timedelta64).\n2382 \n2383         Returns\n2384         -------\n2385         result : Variable or dict of Variable\n2386 \n2387         See also\n2388         --------\n2389         DataArray.argmax, DataArray.idxmax\n2390         \"\"\"\n2391         return self._unravel_argminmax(\"argmax\", dim, axis, keep_attrs, skipna)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "xarray/core/common.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/common.py",
        "start": 1394,
        "end": 1439,
        "class_name": null,
        "method_name": "astype",
        "code": "1394     def astype(self, dtype, casting=\"unsafe\", copy=True, keep_attrs=True):\n1395         \"\"\"\n1396         Copy of the xarray object, with data cast to a specified type.\n1397         Leaves coordinate dtype unchanged.\n1398 \n1399         Parameters\n1400         ----------\n1401         dtype : str or dtype\n1402             Typecode or data-type to which the array is cast.\n1403         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n1404             Controls what kind of data casting may occur. Defaults to 'unsafe'\n1405             for backwards compatibility.\n1406 \n1407             * 'no' means the data types should not be cast at all.\n1408             * 'equiv' means only byte-order changes are allowed.\n1409             * 'safe' means only casts which can preserve values are allowed.\n1410             * 'same_kind' means only safe casts or casts within a kind,\n1411                 like float64 to float32, are allowed.\n1412             * 'unsafe' means any data conversions may be done.\n1413         copy : bool, optional\n1414             By default, astype always returns a newly allocated array. If this\n1415             is set to False and the `dtype` requirement is satisfied, the input\n1416             array is returned instead of a copy.\n1417         keep_attrs : bool, optional\n1418             By default, astype keeps attributes. Set to False to remove\n1419             attributes in the returned object.\n1420 \n1421         Returns\n1422         -------\n1423         out : same as object\n1424             New object with data cast to the specified type.\n1425 \n1426         See also\n1427         --------\n1428         np.ndarray.astype\n1429         dask.array.Array.astype\n1430         \"\"\"\n1431         from .computation import apply_ufunc\n1432 \n1433         return apply_ufunc(\n1434             duck_array_ops.astype,\n1435             self,\n1436             kwargs=dict(dtype=dtype, casting=casting, copy=copy),\n1437             keep_attrs=keep_attrs,\n1438             dask=\"allowed\",\n1439         )\n",
        "intended_behavior": "Similarly, this generic 'astype' method applicable to 'DataArray' and 'Dataset' objects should also be updated to include the 'order' parameter. The method must allow the user to specify the memory layout order of operations while maintaining current functionality. As with the 'Variable' class, the addition should pass the 'order' parameter to the underlying handling (likely through 'duck_array_ops.astype' or similar), ensuring compatibility with numpy's array handling and maintaining existing features such as 'keep_attrs'."
    },
    {
        "rel_file_path": "xarray/core/common.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pydata__xarray/setup_pydata__xarray__0.12/xarray/core/common.py",
        "start": 1394,
        "end": 1439,
        "class_name": "DataWithCoords",
        "method_name": "astype",
        "code": "1394     def astype(self, dtype, casting=\"unsafe\", copy=True, keep_attrs=True):\n1395         \"\"\"\n1396         Copy of the xarray object, with data cast to a specified type.\n1397         Leaves coordinate dtype unchanged.\n1398 \n1399         Parameters\n1400         ----------\n1401         dtype : str or dtype\n1402             Typecode or data-type to which the array is cast.\n1403         casting : {'no', 'equiv', 'safe', 'same_kind', 'unsafe'}, optional\n1404             Controls what kind of data casting may occur. Defaults to 'unsafe'\n1405             for backwards compatibility.\n1406 \n1407             * 'no' means the data types should not be cast at all.\n1408             * 'equiv' means only byte-order changes are allowed.\n1409             * 'safe' means only casts which can preserve values are allowed.\n1410             * 'same_kind' means only safe casts or casts within a kind,\n1411                 like float64 to float32, are allowed.\n1412             * 'unsafe' means any data conversions may be done.\n1413         copy : bool, optional\n1414             By default, astype always returns a newly allocated array. If this\n1415             is set to False and the `dtype` requirement is satisfied, the input\n1416             array is returned instead of a copy.\n1417         keep_attrs : bool, optional\n1418             By default, astype keeps attributes. Set to False to remove\n1419             attributes in the returned object.\n1420 \n1421         Returns\n1422         -------\n1423         out : same as object\n1424             New object with data cast to the specified type.\n1425 \n1426         See also\n1427         --------\n1428         np.ndarray.astype\n1429         dask.array.Array.astype\n1430         \"\"\"\n1431         from .computation import apply_ufunc\n1432 \n1433         return apply_ufunc(\n1434             duck_array_ops.astype,\n1435             self,\n1436             kwargs=dict(dtype=dtype, casting=casting, copy=copy),\n1437             keep_attrs=keep_attrs,\n1438             dask=\"allowed\",\n1439         )\n",
        "intended_behavior": "Similarly, this generic 'astype' method applicable to 'DataArray' and 'Dataset' objects should also be updated to include the 'order' parameter. The method must allow the user to specify the memory layout order of operations while maintaining current functionality. As with the 'Variable' class, the addition should pass the 'order' parameter to the underlying handling (likely through 'duck_array_ops.astype' or similar), ensuring compatibility with numpy's array handling and maintaining existing features such as 'keep_attrs'."
    }
]