2025-10-17 23:25:43.433 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 23:25:47.274 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=178, output_tokens=12, cost=0.002140
2025-10-17 23:25:59.103 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=358, output_tokens=528, cost=0.019420
2025-10-17 23:26:00.221 | INFO     | registered non reproducing test 0
2025-10-17 23:26:16.078 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1416, output_tokens=661, cost=0.033990
2025-10-17 23:26:17.168 | INFO     | registered non reproducing test 1
2025-10-17 23:26:31.676 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1368, output_tokens=644, cost=0.033000
2025-10-17 23:26:32.799 | INFO     | registered non reproducing test 2
2025-10-17 23:26:32.799 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 23:26:32.806 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 23:26:40.373 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=483, output_tokens=305, cost=0.013980
2025-10-17 23:26:40.380 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 23:26:42.655 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=659, output_tokens=70, cost=0.008690
2025-10-17 23:26:42.656 | DEBUG    | Extracted a valid json.
2025-10-17 23:26:42.656 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"MyModel\")",
        "search_method_in_class(\"filter\", \"QuerySet\")",
        "search_class(\"Exists\")",
        "search_method_in_class(\"none\", \"QuerySet\")",
        "search_code(\"EmptyResultSet\")"
    ],
    "bug_locations": []
}
2025-10-17 23:26:42.727 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 23:26:42.727 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 23:26:59.652 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1412, output_tokens=837, cost=0.039230
2025-10-17 23:26:59.669 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 23:27:09.744 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2465, output_tokens=491, cost=0.039380
2025-10-17 23:27:09.754 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 23:27:21.213 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=845, output_tokens=431, cost=0.021380
2025-10-17 23:27:21.214 | DEBUG    | Extracted a valid json.
2025-10-17 23:27:21.214 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "django/db/models/expressions.py",
            "class": "Exists",
            "method": "__init__, __invert__, as_sql",
            "intended_behavior": "Ensure that when `Exists` is constructed with an empty queryset, especially when negated, the resulting SQL maintains the correct WHERE clause structure. The `as_sql` method, which generates the SQL for the EXISTS expression, should be reviewed to confirm it handles empty querysets in a way that does not lead to the entire WHERE block being omitted. This means likely adding logic to preserve a placeholder or no-op condition in the SQL when faced with an empty queryset, ensuring the WHERE clause is still present even if it's logically empty."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "none",
            "intended_behavior": "The method should ensure that when `none` is called, and the resulting QuerySet is used within an `Exists` expression, the `EmptyResultSet` is handled in such a way that it doesn't lead to the unintended removal of WHERE clause in the composed SQL query. This might involve marking the QuerySet in a specific way that `Exists` can detect and handle appropriately, ensuring that the intended logic of 'no results possible' is encoded into the SQL without dropping the WHERE clause entirely."
        },
        {
            "file": "django/db/models/sql/compiler.py",
            "class": "",
            "method": "",
            "intended_behavior": "This is a speculative location based on the usage of `EmptyResultSet` across the codebase, particularly in SQL compiler logic. A revision might be necessary to ensure that when compiling queries involving an `Exists` subquery that has been negated and is based on an `none()` queryset, the compiler preserves the WHERE block appropriately, potentially as a no-op or always-false condition, thereby conveying the intended semantics of the original queryset filtering criteria without erroneously excluding necessary SQL syntax."
        }
    ]
}
2025-10-17 23:27:21.235 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1199 class Exists(Subquery):
1200     template = 'EXISTS(%(subquery)s)'
1201     output_field = fields.BooleanField()
1202 
1203     def __init__(self, queryset, negated=False, **kwargs):
1204         self.negated = negated
1205         super().__init__(queryset, **kwargs)
1206 
1207     def __invert__(self):
1208         clone = self.copy()
1209         clone.negated = not self.negated
1210         return clone
1211 
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params
1224 
1225     def select_format(self, compiler, sql, params):
1226         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1227         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1228         # BY list.
1229         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1230             sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
1231         return sql, params

</code><intended_behavior>Ensure that when `Exists` is constructed with an empty queryset, especially when negated, the resulting SQL maintains the correct WHERE clause structure. The `as_sql` method, which generates the SQL for the EXISTS expression, should be reviewed to confirm it handles empty querysets in a way that does not lead to the entire WHERE block being omitted. This means likely adding logic to preserve a placeholder or no-op condition in the SQL when faced with an empty queryset, ensuring the WHERE clause is still present even if it's logically empty.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>none</method>
<code>
1061     def none(self):
1062         """Return an empty QuerySet."""
1063         clone = self._chain()
1064         clone.query.set_empty()
1065         return clone

</code><intended_behavior>The method should ensure that when `none` is called, and the resulting QuerySet is used within an `Exists` expression, the `EmptyResultSet` is handled in such a way that it doesn't lead to the unintended removal of WHERE clause in the composed SQL query. This might involve marking the QuerySet in a specific way that `Exists` can detect and handle appropriately, ensuring that the intended logic of 'no results possible' is encoded into the SQL without dropping the WHERE clause entirely.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
176 class QuerySet:
177     """Represent a lazy database lookup for a set of objects."""
178 
179     def __init__(self, model=None, query=None, using=None, hints=None):
180         self.model = model
181         self._db = using
182         self._hints = hints or {}
183         self._query = query or sql.Query(self.model)
184         self._result_cache = None
185         self._sticky_filter = False
186         self._for_write = False
187         self._prefetch_related_lookups = ()
188         self._prefetch_done = False
189         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
190         self._iterable_class = ModelIterable
191         self._fields = None
192         self._defer_next_filter = False
193         self._deferred_filter = None
194 
195     @property
196     def query(self):
197         if self._deferred_filter:
198             negate, args, kwargs = self._deferred_filter
199             self._filter_or_exclude_inplace(negate, args, kwargs)
200             self._deferred_filter = None
201         return self._query
202 
203     @query.setter
204     def query(self, value):
205         if value.values_select:
206             self._iterable_class = ValuesIterable
207         self._query = value
208 
209     def as_manager(cls):
210         # Address the circular dependency between `Queryset` and `Manager`.
211         from django.db.models.manager import Manager
212         manager = Manager.from_queryset(cls)()
213         manager._built_with_as_manager = True
214         return manager
215     as_manager.queryset_only = True
216     as_manager = classmethod(as_manager)
217 
218     ########################
219     # PYTHON MAGIC METHODS #
220     ########################
221 
222     def __deepcopy__(self, memo):
223         """Don't populate the QuerySet's cache."""
224         obj = self.__class__()
225         for k, v in self.__dict__.items():
226             if k == '_result_cache':
227                 obj.__dict__[k] = None
228             else:
229                 obj.__dict__[k] = copy.deepcopy(v, memo)
230         return obj
231 
232     def __getstate__(self):
233         # Force the cache to be fully populated.
234         self._fetch_all()
235         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
236 
237     def __setstate__(self, state):
238         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
239         if pickled_version:
240             if pickled_version != django.__version__:
241                 warnings.warn(
242                     "Pickled queryset instance's Django version %s does not "
243                     "match the current version %s."
244                     % (pickled_version, django.__version__),
245                     RuntimeWarning,
246                     stacklevel=2,
247                 )
248         else:
249             warnings.warn(
250                 "Pickled queryset instance's Django version is not specified.",
251                 RuntimeWarning,
252                 stacklevel=2,
253             )
254         self.__dict__.update(state)
255 
256     def __repr__(self):
257         data = list(self[:REPR_OUTPUT_SIZE + 1])
258         if len(data) > REPR_OUTPUT_SIZE:
259             data[-1] = "...(remaining elements truncated)..."
260         return '<%s %r>' % (self.__class__.__name__, data)
261 
262     def __len__(self):
263         self._fetch_all()
264         return len(self._result_cache)
265 
266     def __iter__(self):
267         """
268         The queryset iterator protocol uses three nested iterators in the
269         default case:
270             1. sql.compiler.execute_sql()
271                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
272                  using cursor.fetchmany(). This part is responsible for
273                  doing some column masking, and returning the rows in chunks.
274             2. sql.compiler.results_iter()
275                - Returns one row at time. At this point the rows are still just
276                  tuples. In some cases the return values are converted to
277                  Python values at this location.
278             3. self.iterator()
279                - Responsible for turning the rows into model objects.
280         """
281         self._fetch_all()
282         return iter(self._result_cache)
283 
284     def __bool__(self):
285         self._fetch_all()
286         return bool(self._result_cache)
287 
288     def __getitem__(self, k):
289         """Retrieve an item or slice from the set of results."""
290         if not isinstance(k, (int, slice)):
291             raise TypeError(
292                 'QuerySet indices must be integers or slices, not %s.'
293                 % type(k).__name__
294             )
295         if (
296             (isinstance(k, int) and k < 0) or
297             (isinstance(k, slice) and (
298                 (k.start is not None and k.start < 0) or
299                 (k.stop is not None and k.stop < 0)
300             ))
301         ):
302             raise ValueError('Negative indexing is not supported.')
303 
304         if self._result_cache is not None:
305             return self._result_cache[k]
306 
307         if isinstance(k, slice):
308             qs = self._chain()
309             if k.start is not None:
310                 start = int(k.start)
311             else:
312                 start = None
313             if k.stop is not None:
314                 stop = int(k.stop)
315             else:
316                 stop = None
317             qs.query.set_limits(start, stop)
318             return list(qs)[::k.step] if k.step else qs
319 
320         qs = self._chain()
321         qs.query.set_limits(k, k + 1)
322         qs._fetch_all()
323         return qs._result_cache[0]
324 
325     def __class_getitem__(cls, *args, **kwargs):
326         return cls
327 
328     def __and__(self, other):
329         self._check_operator_queryset(other, '&')
330         self._merge_sanity_check(other)
331         if isinstance(other, EmptyQuerySet):
332             return other
333         if isinstance(self, EmptyQuerySet):
334             return self
335         combined = self._chain()
336         combined._merge_known_related_objects(other)
337         combined.query.combine(other.query, sql.AND)
338         return combined
339 
340     def __or__(self, other):
341         self._check_operator_queryset(other, '|')
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         iterable = self._iterable_class(
361             self,
362             chunked_fetch=use_chunked_fetch,
363             chunk_size=chunk_size or 2000,
364         )
365         if not self._prefetch_related_lookups or chunk_size is None:
366             yield from iterable
367             return
368 
369         iterator = iter(iterable)
370         while results := list(islice(iterator, chunk_size)):
371             prefetch_related_objects(results, *self._prefetch_related_lookups)
372             yield from results
373 
374     def iterator(self, chunk_size=None):
375         """
376         An iterator over the results from applying this QuerySet to the
377         database. chunk_size must be provided for QuerySets that prefetch
378         related objects. Otherwise, a default chunk_size of 2000 is supplied.
379         """
380         if chunk_size is None:
381             if self._prefetch_related_lookups:
382                 # When the deprecation ends, replace with:
383                 # raise ValueError(
384                 #     'chunk_size must be provided when using '
385                 #     'QuerySet.iterator() after prefetch_related().'
386                 # )
387                 warnings.warn(
388                     'Using QuerySet.iterator() after prefetch_related() '
389                     'without specifying chunk_size is deprecated.',
390                     category=RemovedInDjango50Warning,
391                     stacklevel=2,
392                 )
393         elif chunk_size <= 0:
394             raise ValueError('Chunk size must be strictly positive.')
395         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
396         return self._iterator(use_chunked_fetch, chunk_size)
397 
398     def aggregate(self, *args, **kwargs):
399         """
400         Return a dictionary containing the calculations (aggregation)
401         over the current queryset.
402 
403         If args is present the expression is passed as a kwarg using
404         the Aggregate object's default alias.
405         """
406         if self.query.distinct_fields:
407             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
408         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
409         for arg in args:
410             # The default_alias property raises TypeError if default_alias
411             # can't be set automatically or AttributeError if it isn't an
412             # attribute.
413             try:
414                 arg.default_alias
415             except (AttributeError, TypeError):
416                 raise TypeError("Complex aggregates require an alias")
417             kwargs[arg.default_alias] = arg
418 
419         query = self.query.chain()
420         for (alias, aggregate_expr) in kwargs.items():
421             query.add_annotation(aggregate_expr, alias, is_summary=True)
422             annotation = query.annotations[alias]
423             if not annotation.contains_aggregate:
424                 raise TypeError("%s is not an aggregate expression" % alias)
425             for expr in annotation.get_source_expressions():
426                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
427                     name = expr.refs
428                     raise exceptions.FieldError(
429                         "Cannot compute %s('%s'): '%s' is an aggregate"
430                         % (annotation.name, name, name)
431                     )
432         return query.get_aggregation(self.db, kwargs)
433 
434     def count(self):
435         """
436         Perform a SELECT COUNT() and return the number of records as an
437         integer.
438 
439         If the QuerySet is already fully cached, return the length of the
440         cached results set to avoid multiple SELECT COUNT(*) calls.
441         """
442         if self._result_cache is not None:
443             return len(self._result_cache)
444 
445         return self.query.get_count(using=self.db)
446 
447     def get(self, *args, **kwargs):
448         """
449         Perform the query and return a single object matching the given
450         keyword arguments.
451         """
452         if self.query.combinator and (args or kwargs):
453             raise NotSupportedError(
454                 'Calling QuerySet.get(...) with filters after %s() is not '
455                 'supported.' % self.query.combinator
456             )
457         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
458         if self.query.can_filter() and not self.query.distinct_fields:
459             clone = clone.order_by()
460         limit = None
461         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
462             limit = MAX_GET_RESULTS
463             clone.query.set_limits(high=limit)
464         num = len(clone)
465         if num == 1:
466             return clone._result_cache[0]
467         if not num:
468             raise self.model.DoesNotExist(
469                 "%s matching query does not exist." %
470                 self.model._meta.object_name
471             )
472         raise self.model.MultipleObjectsReturned(
473             'get() returned more than one %s -- it returned %s!' % (
474                 self.model._meta.object_name,
475                 num if not limit or num < limit else 'more than %s' % (limit - 1),
476             )
477         )
478 
479     def create(self, **kwargs):
480         """
481         Create a new object with the given kwargs, saving it to the database
482         and returning the created object.
483         """
484         obj = self.model(**kwargs)
485         self._for_write = True
486         obj.save(force_insert=True, using=self.db)
487         return obj
488 
489     def _prepare_for_bulk_create(self, objs):
490         for obj in objs:
491             if obj.pk is None:
492                 # Populate new PK values.
493                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
494             obj._prepare_related_fields_for_save(operation_name='bulk_create')
495 
496     def _check_bulk_create_options(self, ignore_conflicts, update_conflicts, update_fields, unique_fields):
497         if ignore_conflicts and update_conflicts:
498             raise ValueError(
499                 'ignore_conflicts and update_conflicts are mutually exclusive.'
500             )
501         db_features = connections[self.db].features
502         if ignore_conflicts:
503             if not db_features.supports_ignore_conflicts:
504                 raise NotSupportedError(
505                     'This database backend does not support ignoring conflicts.'
506                 )
507             return OnConflict.IGNORE
508         elif update_conflicts:
509             if not db_features.supports_update_conflicts:
510                 raise NotSupportedError(
511                     'This database backend does not support updating conflicts.'
512                 )
513             if not update_fields:
514                 raise ValueError(
515                     'Fields that will be updated when a row insertion fails '
516                     'on conflicts must be provided.'
517                 )
518             if unique_fields and not db_features.supports_update_conflicts_with_target:
519                 raise NotSupportedError(
520                     'This database backend does not support updating '
521                     'conflicts with specifying unique fields that can trigger '
522                     'the upsert.'
523                 )
524             if not unique_fields and db_features.supports_update_conflicts_with_target:
525                 raise ValueError(
526                     'Unique fields that can trigger the upsert must be '
527                     'provided.'
528                 )
529             # Updating primary keys and non-concrete fields is forbidden.
530             update_fields = [self.model._meta.get_field(name) for name in update_fields]
531             if any(not f.concrete or f.many_to_many for f in update_fields):
532                 raise ValueError(
533                     'bulk_create() can only be used with concrete fields in '
534                     'update_fields.'
535                 )
536             if any(f.primary_key for f in update_fields):
537                 raise ValueError(
538                     'bulk_create() cannot be used with primary keys in '
539                     'update_fields.'
540                 )
541             if unique_fields:
542                 # Primary key is allowed in unique_fields.
543                 unique_fields = [
544                     self.model._meta.get_field(name)
545                     for name in unique_fields if name != 'pk'
546                 ]
547                 if any(not f.concrete or f.many_to_many for f in unique_fields):
548                     raise ValueError(
549                         'bulk_create() can only be used with concrete fields '
550                         'in unique_fields.'
551                     )
552             return OnConflict.UPDATE
553         return None
554 
555     def bulk_create(
556         self, objs, batch_size=None, ignore_conflicts=False,
557         update_conflicts=False, update_fields=None, unique_fields=None,
558     ):
559         """
560         Insert each of the instances into the database. Do *not* call
561         save() on each of the instances, do not send any pre/post_save
562         signals, and do not set the primary key attribute if it is an
563         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
564         Multi-table models are not supported.
565         """
566         # When you bulk insert you don't get the primary keys back (if it's an
567         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
568         # you can't insert into the child tables which references this. There
569         # are two workarounds:
570         # 1) This could be implemented if you didn't have an autoincrement pk
571         # 2) You could do it by doing O(n) normal inserts into the parent
572         #    tables to get the primary keys back and then doing a single bulk
573         #    insert into the childmost table.
574         # We currently set the primary keys on the objects when using
575         # PostgreSQL via the RETURNING ID clause. It should be possible for
576         # Oracle as well, but the semantics for extracting the primary keys is
577         # trickier so it's not done yet.
578         if batch_size is not None and batch_size <= 0:
579             raise ValueError('Batch size must be a positive integer.')
580         # Check that the parents share the same concrete model with the our
581         # model to detect the inheritance pattern ConcreteGrandParent ->
582         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
583         # would not identify that case as involving multiple tables.
584         for parent in self.model._meta.get_parent_list():
585             if parent._meta.concrete_model is not self.model._meta.concrete_model:
586                 raise ValueError("Can't bulk create a multi-table inherited model")
587         if not objs:
588             return objs
589         on_conflict = self._check_bulk_create_options(
590             ignore_conflicts,
591             update_conflicts,
592             update_fields,
593             unique_fields,
594         )
595         self._for_write = True
596         opts = self.model._meta
597         fields = opts.concrete_fields
598         objs = list(objs)
599         self._prepare_for_bulk_create(objs)
600         with transaction.atomic(using=self.db, savepoint=False):
601             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
602             if objs_with_pk:
603                 returned_columns = self._batched_insert(
604                     objs_with_pk,
605                     fields,
606                     batch_size,
607                     on_conflict=on_conflict,
608                     update_fields=update_fields,
609                     unique_fields=unique_fields,
610                 )
611                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
612                     for result, field in zip(results, opts.db_returning_fields):
613                         if field != opts.pk:
614                             setattr(obj_with_pk, field.attname, result)
615                 for obj_with_pk in objs_with_pk:
616                     obj_with_pk._state.adding = False
617                     obj_with_pk._state.db = self.db
618             if objs_without_pk:
619                 fields = [f for f in fields if not isinstance(f, AutoField)]
620                 returned_columns = self._batched_insert(
621                     objs_without_pk,
622                     fields,
623                     batch_size,
624                     on_conflict=on_conflict,
625                     update_fields=update_fields,
626                     unique_fields=unique_fields,
627                 )
628                 connection = connections[self.db]
629                 if connection.features.can_return_rows_from_bulk_insert and on_conflict is None:
630                     assert len(returned_columns) == len(objs_without_pk)
631                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
632                     for result, field in zip(results, opts.db_returning_fields):
633                         setattr(obj_without_pk, field.attname, result)
634                     obj_without_pk._state.adding = False
635                     obj_without_pk._state.db = self.db
636 
637         return objs
638 
639     def bulk_update(self, objs, fields, batch_size=None):
640         """
641         Update the given fields in each of the given objects in the database.
642         """
643         if batch_size is not None and batch_size < 0:
644             raise ValueError('Batch size must be a positive integer.')
645         if not fields:
646             raise ValueError('Field names must be given to bulk_update().')
647         objs = tuple(objs)
648         if any(obj.pk is None for obj in objs):
649             raise ValueError('All bulk_update() objects must have a primary key set.')
650         fields = [self.model._meta.get_field(name) for name in fields]
651         if any(not f.concrete or f.many_to_many for f in fields):
652             raise ValueError('bulk_update() can only be used with concrete fields.')
653         if any(f.primary_key for f in fields):
654             raise ValueError('bulk_update() cannot be used with primary key fields.')
655         if not objs:
656             return 0
657         for obj in objs:
658             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)
659         # PK is used twice in the resulting update query, once in the filter
660         # and once in the WHEN. Each field will also have one CAST.
661         connection = connections[self.db]
662         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
663         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
664         requires_casting = connection.features.requires_casted_case_in_updates
665         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
666         updates = []
667         for batch_objs in batches:
668             update_kwargs = {}
669             for field in fields:
670                 when_statements = []
671                 for obj in batch_objs:
672                     attr = getattr(obj, field.attname)
673                     if not hasattr(attr, 'resolve_expression'):
674                         attr = Value(attr, output_field=field)
675                     when_statements.append(When(pk=obj.pk, then=attr))
676                 case_statement = Case(*when_statements, output_field=field)
677                 if requires_casting:
678                     case_statement = Cast(case_statement, output_field=field)
679                 update_kwargs[field.attname] = case_statement
680             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
681         rows_updated = 0
682         with transaction.atomic(using=self.db, savepoint=False):
683             for pks, update_kwargs in updates:
684                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
685         return rows_updated
686     bulk_update.alters_data = True
687 
688     def get_or_create(self, defaults=None, **kwargs):
689         """
690         Look up an object with the given kwargs, creating one if necessary.
691         Return a tuple of (object, created), where created is a boolean
692         specifying whether an object was created.
693         """
694         # The get() needs to be targeted at the write database in order
695         # to avoid potential transaction consistency problems.
696         self._for_write = True
697         try:
698             return self.get(**kwargs), False
699         except self.model.DoesNotExist:
700             params = self._extract_model_params(defaults, **kwargs)
701             # Try to create an object using passed params.
702             try:
703                 with transaction.atomic(using=self.db):
704                     params = dict(resolve_callables(params))
705                     return self.create(**params), True
706             except IntegrityError:
707                 try:
708                     return self.get(**kwargs), False
709                 except self.model.DoesNotExist:
710                     pass
711                 raise
712 
713     def update_or_create(self, defaults=None, **kwargs):
714         """
715         Look up an object with the given kwargs, updating one with defaults
716         if it exists, otherwise create a new one.
717         Return a tuple (object, created), where created is a boolean
718         specifying whether an object was created.
719         """
720         defaults = defaults or {}
721         self._for_write = True
722         with transaction.atomic(using=self.db):
723             # Lock the row so that a concurrent update is blocked until
724             # update_or_create() has performed its save.
725             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
726             if created:
727                 return obj, created
728             for k, v in resolve_callables(defaults):
729                 setattr(obj, k, v)
730             obj.save(using=self.db)
731         return obj, False
732 
733     def _extract_model_params(self, defaults, **kwargs):
734         """
735         Prepare `params` for creating a model instance based on the given
736         kwargs; for use by get_or_create().
737         """
738         defaults = defaults or {}
739         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
740         params.update(defaults)
741         property_names = self.model._meta._property_names
742         invalid_params = []
743         for param in params:
744             try:
745                 self.model._meta.get_field(param)
746             except exceptions.FieldDoesNotExist:
747                 # It's okay to use a model's property if it has a setter.
748                 if not (param in property_names and getattr(self.model, param).fset):
749                     invalid_params.append(param)
750         if invalid_params:
751             raise exceptions.FieldError(
752                 "Invalid field name(s) for model %s: '%s'." % (
753                     self.model._meta.object_name,
754                     "', '".join(sorted(invalid_params)),
755                 ))
756         return params
757 
758     def _earliest(self, *fields):
759         """
760         Return the earliest object according to fields (if given) or by the
761         model's Meta.get_latest_by.
762         """
763         if fields:
764             order_by = fields
765         else:
766             order_by = getattr(self.model._meta, 'get_latest_by')
767             if order_by and not isinstance(order_by, (tuple, list)):
768                 order_by = (order_by,)
769         if order_by is None:
770             raise ValueError(
771                 "earliest() and latest() require either fields as positional "
772                 "arguments or 'get_latest_by' in the model's Meta."
773             )
774         obj = self._chain()
775         obj.query.set_limits(high=1)
776         obj.query.clear_ordering(force=True)
777         obj.query.add_ordering(*order_by)
778         return obj.get()
779 
780     def earliest(self, *fields):
781         if self.query.is_sliced:
782             raise TypeError('Cannot change a query once a slice has been taken.')
783         return self._earliest(*fields)
784 
785     def latest(self, *fields):
786         if self.query.is_sliced:
787             raise TypeError('Cannot change a query once a slice has been taken.')
788         return self.reverse()._earliest(*fields)
789 
790     def first(self):
791         """Return the first object of a query or None if no match is found."""
792         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
793             return obj
794 
795     def last(self):
796         """Return the last object of a query or None if no match is found."""
797         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
798             return obj
799 
800     def in_bulk(self, id_list=None, *, field_name='pk'):
801         """
802         Return a dictionary mapping each of the given IDs to the object with
803         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
804         """
805         if self.query.is_sliced:
806             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
807         opts = self.model._meta
808         unique_fields = [
809             constraint.fields[0]
810             for constraint in opts.total_unique_constraints
811             if len(constraint.fields) == 1
812         ]
813         if (
814             field_name != 'pk' and
815             not opts.get_field(field_name).unique and
816             field_name not in unique_fields and
817             self.query.distinct_fields != (field_name,)
818         ):
819             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
820         if id_list is not None:
821             if not id_list:
822                 return {}
823             filter_key = '{}__in'.format(field_name)
824             batch_size = connections[self.db].features.max_query_params
825             id_list = tuple(id_list)
826             # If the database has a limit on the number of query parameters
827             # (e.g. SQLite), retrieve objects in batches if necessary.
828             if batch_size and batch_size < len(id_list):
829                 qs = ()
830                 for offset in range(0, len(id_list), batch_size):
831                     batch = id_list[offset:offset + batch_size]
832                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
833             else:
834                 qs = self.filter(**{filter_key: id_list}).order_by()
835         else:
836             qs = self._chain()
837         return {getattr(obj, field_name): obj for obj in qs}
838 
839     def delete(self):
840         """Delete the records in the current QuerySet."""
841         self._not_support_combined_queries('delete')
842         if self.query.is_sliced:
843             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
844         if self.query.distinct or self.query.distinct_fields:
845             raise TypeError('Cannot call delete() after .distinct().')
846         if self._fields is not None:
847             raise TypeError("Cannot call delete() after .values() or .values_list()")
848 
849         del_query = self._chain()
850 
851         # The delete is actually 2 queries - one to find related objects,
852         # and one to delete. Make sure that the discovery of related
853         # objects is performed on the same database as the deletion.
854         del_query._for_write = True
855 
856         # Disable non-supported fields.
857         del_query.query.select_for_update = False
858         del_query.query.select_related = False
859         del_query.query.clear_ordering(force=True)
860 
861         collector = Collector(using=del_query.db, origin=self)
862         collector.collect(del_query)
863         deleted, _rows_count = collector.delete()
864 
865         # Clear the result cache, in case this QuerySet gets reused.
866         self._result_cache = None
867         return deleted, _rows_count
868 
869     delete.alters_data = True
870     delete.queryset_only = True
871 
872     def _raw_delete(self, using):
873         """
874         Delete objects found from the given queryset in single direct SQL
875         query. No signals are sent and there is no protection for cascades.
876         """
877         query = self.query.clone()
878         query.__class__ = sql.DeleteQuery
879         cursor = query.get_compiler(using).execute_sql(CURSOR)
880         if cursor:
881             with cursor:
882                 return cursor.rowcount
883         return 0
884     _raw_delete.alters_data = True
885 
886     def update(self, **kwargs):
887         """
888         Update all elements in the current QuerySet, setting all the given
889         fields to the appropriate values.
890         """
891         self._not_support_combined_queries('update')
892         if self.query.is_sliced:
893             raise TypeError('Cannot update a query once a slice has been taken.')
894         self._for_write = True
895         query = self.query.chain(sql.UpdateQuery)
896         query.add_update_values(kwargs)
897         # Clear any annotations so that they won't be present in subqueries.
898         query.annotations = {}
899         with transaction.mark_for_rollback_on_error(using=self.db):
900             rows = query.get_compiler(self.db).execute_sql(CURSOR)
901         self._result_cache = None
902         return rows
903     update.alters_data = True
904 
905     def _update(self, values):
906         """
907         A version of update() that accepts field objects instead of field names.
908         Used primarily for model saving and not intended for use by general
909         code (it requires too much poking around at model internals to be
910         useful at that level).
911         """
912         if self.query.is_sliced:
913             raise TypeError('Cannot update a query once a slice has been taken.')
914         query = self.query.chain(sql.UpdateQuery)
915         query.add_update_fields(values)
916         # Clear any annotations so that they won't be present in subqueries.
917         query.annotations = {}
918         self._result_cache = None
919         return query.get_compiler(self.db).execute_sql(CURSOR)
920     _update.alters_data = True
921     _update.queryset_only = False
922 
923     def exists(self):
924         if self._result_cache is None:
925             return self.query.has_results(using=self.db)
926         return bool(self._result_cache)
927 
928     def contains(self, obj):
929         """Return True if the queryset contains an object."""
930         self._not_support_combined_queries('contains')
931         if self._fields is not None:
932             raise TypeError(
933                 'Cannot call QuerySet.contains() after .values() or '
934                 '.values_list().'
935             )
936         try:
937             if obj._meta.concrete_model != self.model._meta.concrete_model:
938                 return False
939         except AttributeError:
940             raise TypeError("'obj' must be a model instance.")
941         if obj.pk is None:
942             raise ValueError(
943                 'QuerySet.contains() cannot be used on unsaved objects.'
944             )
945         if self._result_cache is not None:
946             return obj in self._result_cache
947         return self.filter(pk=obj.pk).exists()
948 
949     def _prefetch_related_objects(self):
950         # This method can only be called once the result cache has been filled.
951         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
952         self._prefetch_done = True
953 
954     def explain(self, *, format=None, **options):
955         return self.query.explain(using=self.db, format=format, **options)
956 
957     ##################################################
958     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
959     ##################################################
960 
961     def raw(self, raw_query, params=(), translations=None, using=None):
962         if using is None:
963             using = self.db
964         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
965         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
966         return qs
967 
968     def _values(self, *fields, **expressions):
969         clone = self._chain()
970         if expressions:
971             clone = clone.annotate(**expressions)
972         clone._fields = fields
973         clone.query.set_values(fields)
974         return clone
975 
976     def values(self, *fields, **expressions):
977         fields += tuple(expressions)
978         clone = self._values(*fields, **expressions)
979         clone._iterable_class = ValuesIterable
980         return clone
981 
982     def values_list(self, *fields, flat=False, named=False):
983         if flat and named:
984             raise TypeError("'flat' and 'named' can't be used together.")
985         if flat and len(fields) > 1:
986             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
987 
988         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
989         _fields = []
990         expressions = {}
991         counter = 1
992         for field in fields:
993             if hasattr(field, 'resolve_expression'):
994                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
995                 while True:
996                     field_id = field_id_prefix + str(counter)
997                     counter += 1
998                     if field_id not in field_names:
999                         break
1000                 expressions[field_id] = field
1001                 _fields.append(field_id)
1002             else:
1003                 _fields.append(field)
1004 
1005         clone = self._values(*_fields, **expressions)
1006         clone._iterable_class = (
1007             NamedValuesListIterable if named
1008             else FlatValuesListIterable if flat
1009             else ValuesListIterable
1010         )
1011         return clone
1012 
1013     def dates(self, field_name, kind, order='ASC'):
1014         """
1015         Return a list of date objects representing all available dates for
1016         the given field_name, scoped to 'kind'.
1017         """
1018         if kind not in ('year', 'month', 'week', 'day'):
1019             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1020         if order not in ('ASC', 'DESC'):
1021             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1022         return self.annotate(
1023             datefield=Trunc(field_name, kind, output_field=DateField()),
1024             plain_field=F(field_name)
1025         ).values_list(
1026             'datefield', flat=True
1027         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
1028 
1029     # RemovedInDjango50Warning: when the deprecation ends, remove is_dst
1030     # argument.
1031     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=timezone.NOT_PASSED):
1032         """
1033         Return a list of datetime objects representing all available
1034         datetimes for the given field_name, scoped to 'kind'.
1035         """
1036         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
1037             raise ValueError(
1038                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1039                 "'hour', 'minute', or 'second'."
1040             )
1041         if order not in ('ASC', 'DESC'):
1042             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1043         if settings.USE_TZ:
1044             if tzinfo is None:
1045                 tzinfo = timezone.get_current_timezone()
1046         else:
1047             tzinfo = None
1048         return self.annotate(
1049             datetimefield=Trunc(
1050                 field_name,
1051                 kind,
1052                 output_field=DateTimeField(),
1053                 tzinfo=tzinfo,
1054                 is_dst=is_dst,
1055             ),
1056             plain_field=F(field_name)
1057         ).values_list(
1058             'datetimefield', flat=True
1059         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
1060 
1061     def none(self):
1062         """Return an empty QuerySet."""
1063         clone = self._chain()
1064         clone.query.set_empty()
1065         return clone
1066 
1067     ##################################################################
1068     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1069     ##################################################################
1070 
1071     def all(self):
1072         """
1073         Return a new QuerySet that is a copy of the current one. This allows a
1074         QuerySet to proxy for a model manager in some cases.
1075         """
1076         return self._chain()
1077 
1078     def filter(self, *args, **kwargs):
1079         """
1080         Return a new QuerySet instance with the args ANDed to the existing
1081         set.
1082         """
1083         self._not_support_combined_queries('filter')
1084         return self._filter_or_exclude(False, args, kwargs)
1085 
1086     def exclude(self, *args, **kwargs):
1087         """
1088         Return a new QuerySet instance with NOT (args) ANDed to the existing
1089         set.
1090         """
1091         self._not_support_combined_queries('exclude')
1092         return self._filter_or_exclude(True, args, kwargs)
1093 
1094     def _filter_or_exclude(self, negate, args, kwargs):
1095         if (args or kwargs) and self.query.is_sliced:
1096             raise TypeError('Cannot filter a query once a slice has been taken.')
1097         clone = self._chain()
1098         if self._defer_next_filter:
1099             self._defer_next_filter = False
1100             clone._deferred_filter = negate, args, kwargs
1101         else:
1102             clone._filter_or_exclude_inplace(negate, args, kwargs)
1103         return clone
1104 
1105     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1106         if negate:
1107             self._query.add_q(~Q(*args, **kwargs))
1108         else:
1109             self._query.add_q(Q(*args, **kwargs))
1110 
1111     def complex_filter(self, filter_obj):
1112         """
1113         Return a new QuerySet instance with filter_obj added to the filters.
1114 
1115         filter_obj can be a Q object or a dictionary of keyword lookup
1116         arguments.
1117 
1118         This exists to support framework features such as 'limit_choices_to',
1119         and usually it will be more natural to use other methods.
1120         """
1121         if isinstance(filter_obj, Q):
1122             clone = self._chain()
1123             clone.query.add_q(filter_obj)
1124             return clone
1125         else:
1126             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1127 
1128     def _combinator_query(self, combinator, *other_qs, all=False):
1129         # Clone the query to inherit the select list and everything
1130         clone = self._chain()
1131         # Clear limits and ordering so they can be reapplied
1132         clone.query.clear_ordering(force=True)
1133         clone.query.clear_limits()
1134         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1135         clone.query.combinator = combinator
1136         clone.query.combinator_all = all
1137         return clone
1138 
1139     def union(self, *other_qs, all=False):
1140         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1141         if isinstance(self, EmptyQuerySet):
1142             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1143             if not qs:
1144                 return self
1145             if len(qs) == 1:
1146                 return qs[0]
1147             return qs[0]._combinator_query('union', *qs[1:], all=all)
1148         return self._combinator_query('union', *other_qs, all=all)
1149 
1150     def intersection(self, *other_qs):
1151         # If any query is an EmptyQuerySet, return it.
1152         if isinstance(self, EmptyQuerySet):
1153             return self
1154         for other in other_qs:
1155             if isinstance(other, EmptyQuerySet):
1156                 return other
1157         return self._combinator_query('intersection', *other_qs)
1158 
1159     def difference(self, *other_qs):
1160         # If the query is an EmptyQuerySet, return it.
1161         if isinstance(self, EmptyQuerySet):
1162             return self
1163         return self._combinator_query('difference', *other_qs)
1164 
1165     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1166         """
1167         Return a new QuerySet instance that will select objects with a
1168         FOR UPDATE lock.
1169         """
1170         if nowait and skip_locked:
1171             raise ValueError('The nowait option cannot be used with skip_locked.')
1172         obj = self._chain()
1173         obj._for_write = True
1174         obj.query.select_for_update = True
1175         obj.query.select_for_update_nowait = nowait
1176         obj.query.select_for_update_skip_locked = skip_locked
1177         obj.query.select_for_update_of = of
1178         obj.query.select_for_no_key_update = no_key
1179         return obj
1180 
1181     def select_related(self, *fields):
1182         """
1183         Return a new QuerySet instance that will select related objects.
1184 
1185         If fields are specified, they must be ForeignKey fields and only those
1186         related objects are included in the selection.
1187 
1188         If select_related(None) is called, clear the list.
1189         """
1190         self._not_support_combined_queries('select_related')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1193 
1194         obj = self._chain()
1195         if fields == (None,):
1196             obj.query.select_related = False
1197         elif fields:
1198             obj.query.add_select_related(fields)
1199         else:
1200             obj.query.select_related = True
1201         return obj
1202 
1203     def prefetch_related(self, *lookups):
1204         """
1205         Return a new QuerySet instance that will prefetch the specified
1206         Many-To-One and Many-To-Many related objects when the QuerySet is
1207         evaluated.
1208 
1209         When prefetch_related() is called more than once, append to the list of
1210         prefetch lookups. If prefetch_related(None) is called, clear the list.
1211         """
1212         self._not_support_combined_queries('prefetch_related')
1213         clone = self._chain()
1214         if lookups == (None,):
1215             clone._prefetch_related_lookups = ()
1216         else:
1217             for lookup in lookups:
1218                 if isinstance(lookup, Prefetch):
1219                     lookup = lookup.prefetch_to
1220                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1221                 if lookup in self.query._filtered_relations:
1222                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1223             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1224         return clone
1225 
1226     def annotate(self, *args, **kwargs):
1227         """
1228         Return a query set in which the returned objects have been annotated
1229         with extra data or aggregations.
1230         """
1231         self._not_support_combined_queries('annotate')
1232         return self._annotate(args, kwargs, select=True)
1233 
1234     def alias(self, *args, **kwargs):
1235         """
1236         Return a query set with added aliases for extra data or aggregations.
1237         """
1238         self._not_support_combined_queries('alias')
1239         return self._annotate(args, kwargs, select=False)
1240 
1241     def _annotate(self, args, kwargs, select=True):
1242         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1243         annotations = {}
1244         for arg in args:
1245             # The default_alias property may raise a TypeError.
1246             try:
1247                 if arg.default_alias in kwargs:
1248                     raise ValueError("The named annotation '%s' conflicts with the "
1249                                      "default name for another annotation."
1250                                      % arg.default_alias)
1251             except TypeError:
1252                 raise TypeError("Complex annotations require an alias")
1253             annotations[arg.default_alias] = arg
1254         annotations.update(kwargs)
1255 
1256         clone = self._chain()
1257         names = self._fields
1258         if names is None:
1259             names = set(chain.from_iterable(
1260                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1261                 for field in self.model._meta.get_fields()
1262             ))
1263 
1264         for alias, annotation in annotations.items():
1265             if alias in names:
1266                 raise ValueError("The annotation '%s' conflicts with a field on "
1267                                  "the model." % alias)
1268             if isinstance(annotation, FilteredRelation):
1269                 clone.query.add_filtered_relation(annotation, alias)
1270             else:
1271                 clone.query.add_annotation(
1272                     annotation, alias, is_summary=False, select=select,
1273                 )
1274         for alias, annotation in clone.query.annotations.items():
1275             if alias in annotations and annotation.contains_aggregate:
1276                 if clone._fields is None:
1277                     clone.query.group_by = True
1278                 else:
1279                     clone.query.set_group_by()
1280                 break
1281 
1282         return clone
1283 
1284     def order_by(self, *field_names):
1285         """Return a new QuerySet instance with the ordering changed."""
1286         if self.query.is_sliced:
1287             raise TypeError('Cannot reorder a query once a slice has been taken.')
1288         obj = self._chain()
1289         obj.query.clear_ordering(force=True, clear_default=False)
1290         obj.query.add_ordering(*field_names)
1291         return obj
1292 
1293     def distinct(self, *field_names):
1294         """
1295         Return a new QuerySet instance that will select only distinct results.
1296         """
1297         self._not_support_combined_queries('distinct')
1298         if self.query.is_sliced:
1299             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1300         obj = self._chain()
1301         obj.query.add_distinct_fields(*field_names)
1302         return obj
1303 
1304     def extra(self, select=None, where=None, params=None, tables=None,
1305               order_by=None, select_params=None):
1306         """Add extra SQL fragments to the query."""
1307         self._not_support_combined_queries('extra')
1308         if self.query.is_sliced:
1309             raise TypeError('Cannot change a query once a slice has been taken.')
1310         clone = self._chain()
1311         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1312         return clone
1313 
1314     def reverse(self):
1315         """Reverse the ordering of the QuerySet."""
1316         if self.query.is_sliced:
1317             raise TypeError('Cannot reverse a query once a slice has been taken.')
1318         clone = self._chain()
1319         clone.query.standard_ordering = not clone.query.standard_ordering
1320         return clone
1321 
1322     def defer(self, *fields):
1323         """
1324         Defer the loading of data for certain fields until they are accessed.
1325         Add the set of deferred fields to any existing set of deferred fields.
1326         The only exception to this is if None is passed in as the only
1327         parameter, in which case removal all deferrals.
1328         """
1329         self._not_support_combined_queries('defer')
1330         if self._fields is not None:
1331             raise TypeError("Cannot call defer() after .values() or .values_list()")
1332         clone = self._chain()
1333         if fields == (None,):
1334             clone.query.clear_deferred_loading()
1335         else:
1336             clone.query.add_deferred_loading(fields)
1337         return clone
1338 
1339     def only(self, *fields):
1340         """
1341         Essentially, the opposite of defer(). Only the fields passed into this
1342         method and that are not already specified as deferred are loaded
1343         immediately when the queryset is evaluated.
1344         """
1345         self._not_support_combined_queries('only')
1346         if self._fields is not None:
1347             raise TypeError("Cannot call only() after .values() or .values_list()")
1348         if fields == (None,):
1349             # Can only pass None to defer(), not only(), as the rest option.
1350             # That won't stop people trying to do this, so let's be explicit.
1351             raise TypeError("Cannot pass None as an argument to only().")
1352         for field in fields:
1353             field = field.split(LOOKUP_SEP, 1)[0]
1354             if field in self.query._filtered_relations:
1355                 raise ValueError('only() is not supported with FilteredRelation.')
1356         clone = self._chain()
1357         clone.query.add_immediate_loading(fields)
1358         return clone
1359 
1360     def using(self, alias):
1361         """Select which database this QuerySet should execute against."""
1362         clone = self._chain()
1363         clone._db = alias
1364         return clone
1365 
1366     ###################################
1367     # PUBLIC INTROSPECTION ATTRIBUTES #
1368     ###################################
1369 
1370     @property
1371     def ordered(self):
1372         """
1373         Return True if the QuerySet is ordered -- i.e. has an order_by()
1374         clause or a default ordering on the model (or is empty).
1375         """
1376         if isinstance(self, EmptyQuerySet):
1377             return True
1378         if self.query.extra_order_by or self.query.order_by:
1379             return True
1380         elif (
1381             self.query.default_ordering and
1382             self.query.get_meta().ordering and
1383             # A default ordering doesn't affect GROUP BY queries.
1384             not self.query.group_by
1385         ):
1386             return True
1387         else:
1388             return False
1389 
1390     @property
1391     def db(self):
1392         """Return the database used if this query is executed now."""
1393         if self._for_write:
1394             return self._db or router.db_for_write(self.model, **self._hints)
1395         return self._db or router.db_for_read(self.model, **self._hints)
1396 
1397     ###################
1398     # PRIVATE METHODS #
1399     ###################
1400 
1401     def _insert(
1402         self, objs, fields, returning_fields=None, raw=False, using=None,
1403         on_conflict=None, update_fields=None, unique_fields=None,
1404     ):
1405         """
1406         Insert a new record for the given model. This provides an interface to
1407         the InsertQuery class and is how Model.save() is implemented.
1408         """
1409         self._for_write = True
1410         if using is None:
1411             using = self.db
1412         query = sql.InsertQuery(
1413             self.model,
1414             on_conflict=on_conflict,
1415             update_fields=update_fields,
1416             unique_fields=unique_fields,
1417         )
1418         query.insert_values(fields, objs, raw=raw)
1419         return query.get_compiler(using=using).execute_sql(returning_fields)
1420     _insert.alters_data = True
1421     _insert.queryset_only = False
1422 
1423     def _batched_insert(
1424         self, objs, fields, batch_size, on_conflict=None, update_fields=None,
1425         unique_fields=None,
1426     ):
1427         """
1428         Helper method for bulk_create() to insert objs one batch at a time.
1429         """
1430         connection = connections[self.db]
1431         ops = connection.ops
1432         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1433         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1434         inserted_rows = []
1435         bulk_return = connection.features.can_return_rows_from_bulk_insert
1436         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1437             if bulk_return and on_conflict is None:
1438                 inserted_rows.extend(self._insert(
1439                     item, fields=fields, using=self.db,
1440                     returning_fields=self.model._meta.db_returning_fields,
1441                 ))
1442             else:
1443                 self._insert(
1444                     item,
1445                     fields=fields,
1446                     using=self.db,
1447                     on_conflict=on_conflict,
1448                     update_fields=update_fields,
1449                     unique_fields=unique_fields,
1450                 )
1451         return inserted_rows
1452 
1453     def _chain(self):
1454         """
1455         Return a copy of the current QuerySet that's ready for another
1456         operation.
1457         """
1458         obj = self._clone()
1459         if obj._sticky_filter:
1460             obj.query.filter_is_sticky = True
1461             obj._sticky_filter = False
1462         return obj
1463 
1464     def _clone(self):
1465         """
1466         Return a copy of the current QuerySet. A lightweight alternative
1467         to deepcopy().
1468         """
1469         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1470         c._sticky_filter = self._sticky_filter
1471         c._for_write = self._for_write
1472         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1473         c._known_related_objects = self._known_related_objects
1474         c._iterable_class = self._iterable_class
1475         c._fields = self._fields
1476         return c
1477 
1478     def _fetch_all(self):
1479         if self._result_cache is None:
1480             self._result_cache = list(self._iterable_class(self))
1481         if self._prefetch_related_lookups and not self._prefetch_done:
1482             self._prefetch_related_objects()
1483 
1484     def _next_is_sticky(self):
1485         """
1486         Indicate that the next filter call and the one following that should
1487         be treated as a single filter. This is only important when it comes to
1488         determining when to reuse tables for many-to-many filters. Required so
1489         that we can filter naturally on the results of related managers.
1490 
1491         This doesn't return a clone of the current QuerySet (it returns
1492         "self"). The method is only used internally and should be immediately
1493         followed by a filter() that does create a clone.
1494         """
1495         self._sticky_filter = True
1496         return self
1497 
1498     def _merge_sanity_check(self, other):
1499         """Check that two QuerySet classes may be merged."""
1500         if self._fields is not None and (
1501                 set(self.query.values_select) != set(other.query.values_select) or
1502                 set(self.query.extra_select) != set(other.query.extra_select) or
1503                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1504             raise TypeError(
1505                 "Merging '%s' classes must involve the same values in each case."
1506                 % self.__class__.__name__
1507             )
1508 
1509     def _merge_known_related_objects(self, other):
1510         """
1511         Keep track of all known related objects from either QuerySet instance.
1512         """
1513         for field, objects in other._known_related_objects.items():
1514             self._known_related_objects.setdefault(field, {}).update(objects)
1515 
1516     def resolve_expression(self, *args, **kwargs):
1517         if self._fields and len(self._fields) > 1:
1518             # values() queryset can only be used as nested queries
1519             # if they are set up to select only a single field.
1520             raise TypeError('Cannot use multi-field values as a filter value.')
1521         query = self.query.resolve_expression(*args, **kwargs)
1522         query._db = self._db
1523         return query
1524     resolve_expression.queryset_only = True
1525 
1526     def _add_hints(self, **hints):
1527         """
1528         Update hinting information for use by routers. Add new key/values or
1529         overwrite existing key/values.
1530         """
1531         self._hints.update(hints)
1532 
1533     def _has_filters(self):
1534         """
1535         Check if this QuerySet has any filtering going on. This isn't
1536         equivalent with checking if all objects are present in results, for
1537         example, qs[1:]._has_filters() -> False.
1538         """
1539         return self.query.has_filters()
1540 
1541     @staticmethod
1542     def _validate_values_are_expressions(values, method_name):
1543         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1544         if invalid_args:
1545             raise TypeError(
1546                 'QuerySet.%s() received non-expression(s): %s.' % (
1547                     method_name,
1548                     ', '.join(invalid_args),
1549                 )
1550             )
1551 
1552     def _not_support_combined_queries(self, operation_name):
1553         if self.query.combinator:
1554             raise NotSupportedError(
1555                 'Calling QuerySet.%s() after %s() is not supported.'
1556                 % (operation_name, self.query.combinator)
1557             )
1558 
1559     def _check_operator_queryset(self, other, operator_):
1560         if self.query.combinator or other.query.combinator:
1561             raise TypeError(f'Cannot use {operator_} operator with combined queryset.')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/compiler.py</file>
<class>None</class>
<method>None</method>
<code>
1 import collections
2 import json
3 import re
4 from functools import partial
5 from itertools import chain
6 
7 from django.core.exceptions import EmptyResultSet, FieldError
8 from django.db import DatabaseError, NotSupportedError
9 from django.db.models.constants import LOOKUP_SEP
10 from django.db.models.expressions import F, OrderBy, RawSQL, Ref, Value
11 from django.db.models.functions import Cast, Random
12 from django.db.models.query_utils import select_related_descend
13 from django.db.models.sql.constants import (
14     CURSOR, GET_ITERATOR_CHUNK_SIZE, MULTI, NO_RESULTS, ORDER_DIR, SINGLE,
15 )
16 from django.db.models.sql.query import Query, get_order_dir
17 from django.db.transaction import TransactionManagementError
18 from django.utils.functional import cached_property
19 from django.utils.hashable import make_hashable
20 from django.utils.regex_helper import _lazy_re_compile
21 
22 
23 class SQLCompiler:
24     # Multiline ordering SQL clause may appear from RawSQL.
25     ordering_parts = _lazy_re_compile(
26         r'^(.*)\s(?:ASC|DESC).*',
27         re.MULTILINE | re.DOTALL,
28     )
29 
30     def __init__(self, query, connection, using, elide_empty=True):
31         self.query = query
32         self.connection = connection
33         self.using = using
34         # Some queries, e.g. coalesced aggregation, need to be executed even if
35         # they would return an empty result set.
36         self.elide_empty = elide_empty
37         self.quote_cache = {'*': '*'}
38         # The select, klass_info, and annotations are needed by QuerySet.iterator()
39         # these are set as a side-effect of executing the query. Note that we calculate
40         # separately a list of extra select columns needed for grammatical correctness
41         # of the query, but these columns are not included in self.select.
42         self.select = None
43         self.annotation_col_map = None
44         self.klass_info = None
45         self._meta_ordering = None
46 
47     def __repr__(self):
48         return (
49             f'<{self.__class__.__qualname__} '
50             f'model={self.query.model.__qualname__} '
51             f'connection={self.connection!r} using={self.using!r}>'
52         )
53 
54     def setup_query(self):
55         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):
56             self.query.get_initial_alias()
57         self.select, self.klass_info, self.annotation_col_map = self.get_select()
58         self.col_count = len(self.select)
59 
60     def pre_sql_setup(self):
61         """
62         Do any necessary class setup immediately prior to producing SQL. This
63         is for things that can't necessarily be done in __init__ because we
64         might not have all the pieces in place at that time.
65         """
66         self.setup_query()
67         order_by = self.get_order_by()
68         self.where, self.having = self.query.where.split_having()
69         extra_select = self.get_extra_select(order_by, self.select)
70         self.has_extra_select = bool(extra_select)
71         group_by = self.get_group_by(self.select + extra_select, order_by)
72         return extra_select, order_by, group_by
73 
74     def get_group_by(self, select, order_by):
75         """
76         Return a list of 2-tuples of form (sql, params).
77 
78         The logic of what exactly the GROUP BY clause contains is hard
79         to describe in other words than "if it passes the test suite,
80         then it is correct".
81         """
82         # Some examples:
83         #     SomeModel.objects.annotate(Count('somecol'))
84         #     GROUP BY: all fields of the model
85         #
86         #    SomeModel.objects.values('name').annotate(Count('somecol'))
87         #    GROUP BY: name
88         #
89         #    SomeModel.objects.annotate(Count('somecol')).values('name')
90         #    GROUP BY: all cols of the model
91         #
92         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')
93         #    GROUP BY: name, pk
94         #
95         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')
96         #    GROUP BY: name, pk
97         #
98         # In fact, the self.query.group_by is the minimal set to GROUP BY. It
99         # can't be ever restricted to a smaller set, but additional columns in
100         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately
101         # the end result is that it is impossible to force the query to have
102         # a chosen GROUP BY clause - you can almost do this by using the form:
103         #     .values(*wanted_cols).annotate(AnAggregate())
104         # but any later annotations, extra selects, values calls that
105         # refer some column outside of the wanted_cols, order_by, or even
106         # filter calls can alter the GROUP BY clause.
107 
108         # The query.group_by is either None (no GROUP BY at all), True
109         # (group by select fields), or a list of expressions to be added
110         # to the group by.
111         if self.query.group_by is None:
112             return []
113         expressions = []
114         if self.query.group_by is not True:
115             # If the group by is set to a list (by .values() call most likely),
116             # then we need to add everything in it to the GROUP BY clause.
117             # Backwards compatibility hack for setting query.group_by. Remove
118             # when  we have public API way of forcing the GROUP BY clause.
119             # Converts string references to expressions.
120             for expr in self.query.group_by:
121                 if not hasattr(expr, 'as_sql'):
122                     expressions.append(self.query.resolve_ref(expr))
123                 else:
124                     expressions.append(expr)
125         # Note that even if the group_by is set, it is only the minimal
126         # set to group by. So, we need to add cols in select, order_by, and
127         # having into the select in any case.
128         ref_sources = {
129             expr.source for expr in expressions if isinstance(expr, Ref)
130         }
131         for expr, _, _ in select:
132             # Skip members of the select clause that are already included
133             # by reference.
134             if expr in ref_sources:
135                 continue
136             cols = expr.get_group_by_cols()
137             for col in cols:
138                 expressions.append(col)
139         if not self._meta_ordering:
140             for expr, (sql, params, is_ref) in order_by:
141                 # Skip references to the SELECT clause, as all expressions in
142                 # the SELECT clause are already part of the GROUP BY.
143                 if not is_ref:
144                     expressions.extend(expr.get_group_by_cols())
145         having_group_by = self.having.get_group_by_cols() if self.having else ()
146         for expr in having_group_by:
147             expressions.append(expr)
148         result = []
149         seen = set()
150         expressions = self.collapse_group_by(expressions, having_group_by)
151 
152         for expr in expressions:
153             sql, params = self.compile(expr)
154             sql, params = expr.select_format(self, sql, params)
155             params_hash = make_hashable(params)
156             if (sql, params_hash) not in seen:
157                 result.append((sql, params))
158                 seen.add((sql, params_hash))
159         return result
160 
161     def collapse_group_by(self, expressions, having):
162         # If the DB can group by primary key, then group by the primary key of
163         # query's main model. Note that for PostgreSQL the GROUP BY clause must
164         # include the primary key of every table, but for MySQL it is enough to
165         # have the main table's primary key.
166         if self.connection.features.allows_group_by_pk:
167             # Determine if the main model's primary key is in the query.
168             pk = None
169             for expr in expressions:
170                 # Is this a reference to query's base table primary key? If the
171                 # expression isn't a Col-like, then skip the expression.
172                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and
173                         getattr(expr, 'alias', None) == self.query.base_table):
174                     pk = expr
175                     break
176             # If the main model's primary key is in the query, group by that
177             # field, HAVING expressions, and expressions associated with tables
178             # that don't have a primary key included in the grouped columns.
179             if pk:
180                 pk_aliases = {
181                     expr.alias for expr in expressions
182                     if hasattr(expr, 'target') and expr.target.primary_key
183                 }
184                 expressions = [pk] + [
185                     expr for expr in expressions
186                     if expr in having or (
187                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases
188                     )
189                 ]
190         elif self.connection.features.allows_group_by_selected_pks:
191             # Filter out all expressions associated with a table's primary key
192             # present in the grouped columns. This is done by identifying all
193             # tables that have their primary key included in the grouped
194             # columns and removing non-primary key columns referring to them.
195             # Unmanaged models are excluded because they could be representing
196             # database views on which the optimization might not be allowed.
197             pks = {
198                 expr for expr in expressions
199                 if (
200                     hasattr(expr, 'target') and
201                     expr.target.primary_key and
202                     self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
203                 )
204             }
205             aliases = {expr.alias for expr in pks}
206             expressions = [
207                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases
208             ]
209         return expressions
210 
211     def get_select(self):
212         """
213         Return three values:
214         - a list of 3-tuples of (expression, (sql, params), alias)
215         - a klass_info structure,
216         - a dictionary of annotations
217 
218         The (sql, params) is what the expression will produce, and alias is the
219         "AS alias" for the column (possibly None).
220 
221         The klass_info structure contains the following information:
222         - The base model of the query.
223         - Which columns for that model are present in the query (by
224           position of the select clause).
225         - related_klass_infos: [f, klass_info] to descent into
226 
227         The annotations is a dictionary of {'attname': column position} values.
228         """
229         select = []
230         klass_info = None
231         annotations = {}
232         select_idx = 0
233         for alias, (sql, params) in self.query.extra_select.items():
234             annotations[alias] = select_idx
235             select.append((RawSQL(sql, params), alias))
236             select_idx += 1
237         assert not (self.query.select and self.query.default_cols)
238         if self.query.default_cols:
239             cols = self.get_default_columns()
240         else:
241             # self.query.select is a special case. These columns never go to
242             # any model.
243             cols = self.query.select
244         if cols:
245             select_list = []
246             for col in cols:
247                 select_list.append(select_idx)
248                 select.append((col, None))
249                 select_idx += 1
250             klass_info = {
251                 'model': self.query.model,
252                 'select_fields': select_list,
253             }
254         for alias, annotation in self.query.annotation_select.items():
255             annotations[alias] = select_idx
256             select.append((annotation, alias))
257             select_idx += 1
258 
259         if self.query.select_related:
260             related_klass_infos = self.get_related_selections(select)
261             klass_info['related_klass_infos'] = related_klass_infos
262 
263             def get_select_from_parent(klass_info):
264                 for ki in klass_info['related_klass_infos']:
265                     if ki['from_parent']:
266                         ki['select_fields'] = (klass_info['select_fields'] +
267                                                ki['select_fields'])
268                     get_select_from_parent(ki)
269             get_select_from_parent(klass_info)
270 
271         ret = []
272         for col, alias in select:
273             try:
274                 sql, params = self.compile(col)
275             except EmptyResultSet:
276                 empty_result_set_value = getattr(col, 'empty_result_set_value', NotImplemented)
277                 if empty_result_set_value is NotImplemented:
278                     # Select a predicate that's always False.
279                     sql, params = '0', ()
280                 else:
281                     sql, params = self.compile(Value(empty_result_set_value))
282             else:
283                 sql, params = col.select_format(self, sql, params)
284             ret.append((col, (sql, params), alias))
285         return ret, klass_info, annotations
286 
287     def _order_by_pairs(self):
288         if self.query.extra_order_by:
289             ordering = self.query.extra_order_by
290         elif not self.query.default_ordering:
291             ordering = self.query.order_by
292         elif self.query.order_by:
293             ordering = self.query.order_by
294         elif self.query.get_meta().ordering:
295             ordering = self.query.get_meta().ordering
296             self._meta_ordering = ordering
297         else:
298             ordering = []
299         if self.query.standard_ordering:
300             default_order, _ = ORDER_DIR['ASC']
301         else:
302             default_order, _ = ORDER_DIR['DESC']
303 
304         for field in ordering:
305             if hasattr(field, 'resolve_expression'):
306                 if isinstance(field, Value):
307                     # output_field must be resolved for constants.
308                     field = Cast(field, field.output_field)
309                 if not isinstance(field, OrderBy):
310                     field = field.asc()
311                 if not self.query.standard_ordering:
312                     field = field.copy()
313                     field.reverse_ordering()
314                 yield field, False
315                 continue
316             if field == '?':  # random
317                 yield OrderBy(Random()), False
318                 continue
319 
320             col, order = get_order_dir(field, default_order)
321             descending = order == 'DESC'
322 
323             if col in self.query.annotation_select:
324                 # Reference to expression in SELECT clause
325                 yield (
326                     OrderBy(
327                         Ref(col, self.query.annotation_select[col]),
328                         descending=descending,
329                     ),
330                     True,
331                 )
332                 continue
333             if col in self.query.annotations:
334                 # References to an expression which is masked out of the SELECT
335                 # clause.
336                 if self.query.combinator and self.select:
337                     # Don't use the resolved annotation because other
338                     # combinated queries might define it differently.
339                     expr = F(col)
340                 else:
341                     expr = self.query.annotations[col]
342                     if isinstance(expr, Value):
343                         # output_field must be resolved for constants.
344                         expr = Cast(expr, expr.output_field)
345                 yield OrderBy(expr, descending=descending), False
346                 continue
347 
348             if '.' in field:
349                 # This came in through an extra(order_by=...) addition. Pass it
350                 # on verbatim.
351                 table, col = col.split('.', 1)
352                 yield (
353                     OrderBy(
354                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),
355                         descending=descending,
356                     ),
357                     False,
358                 )
359                 continue
360 
361             if self.query.extra and col in self.query.extra:
362                 if col in self.query.extra_select:
363                     yield (
364                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),
365                         True,
366                     )
367                 else:
368                     yield (
369                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),
370                         False,
371                     )
372             else:
373                 if self.query.combinator and self.select:
374                     # Don't use the first model's field because other
375                     # combinated queries might define it differently.
376                     yield OrderBy(F(col), descending=descending), False
377                 else:
378                     # 'col' is of the form 'field' or 'field1__field2' or
379                     # '-field1__field2__field', etc.
380                     yield from self.find_ordering_name(
381                         field, self.query.get_meta(), default_order=default_order,
382                     )
383 
384     def get_order_by(self):
385         """
386         Return a list of 2-tuples of the form (expr, (sql, params, is_ref)) for
387         the ORDER BY clause.
388 
389         The order_by clause can alter the select clause (for example it can add
390         aliases to clauses that do not yet have one, or it can add totally new
391         select clauses).
392         """
393         result = []
394         seen = set()
395 
396         for expr, is_ref in self._order_by_pairs():
397             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)
398             if self.query.combinator and self.select:
399                 src = resolved.get_source_expressions()[0]
400                 expr_src = expr.get_source_expressions()[0]
401                 # Relabel order by columns to raw numbers if this is a combined
402                 # query; necessary since the columns can't be referenced by the
403                 # fully qualified name and the simple column names may collide.
404                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):
405                     if is_ref and col_alias == src.refs:
406                         src = src.source
407                     elif col_alias and not (
408                         isinstance(expr_src, F) and col_alias == expr_src.name
409                     ):
410                         continue
411                     if src == sel_expr:
412                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])
413                         break
414                 else:
415                     if col_alias:
416                         raise DatabaseError('ORDER BY term does not match any column in the result set.')
417                     # Add column used in ORDER BY clause to the selected
418                     # columns and to each combined query.
419                     order_by_idx = len(self.query.select) + 1
420                     col_name = f'__orderbycol{order_by_idx}'
421                     for q in self.query.combined_queries:
422                         q.add_annotation(expr_src, col_name)
423                     self.query.add_select_col(resolved, col_name)
424                     resolved.set_source_expressions([RawSQL(f'{order_by_idx}', ())])
425             sql, params = self.compile(resolved)
426             # Don't add the same column twice, but the order direction is
427             # not taken into account so we strip it. When this entire method
428             # is refactored into expressions, then we can check each part as we
429             # generate it.
430             without_ordering = self.ordering_parts.search(sql)[1]
431             params_hash = make_hashable(params)
432             if (without_ordering, params_hash) in seen:
433                 continue
434             seen.add((without_ordering, params_hash))
435             result.append((resolved, (sql, params, is_ref)))
436         return result
437 
438     def get_extra_select(self, order_by, select):
439         extra_select = []
440         if self.query.distinct and not self.query.distinct_fields:
441             select_sql = [t[1] for t in select]
442             for expr, (sql, params, is_ref) in order_by:
443                 without_ordering = self.ordering_parts.search(sql)[1]
444                 if not is_ref and (without_ordering, params) not in select_sql:
445                     extra_select.append((expr, (without_ordering, params), None))
446         return extra_select
447 
448     def quote_name_unless_alias(self, name):
449         """
450         A wrapper around connection.ops.quote_name that doesn't quote aliases
451         for table names. This avoids problems with some SQL dialects that treat
452         quoted strings specially (e.g. PostgreSQL).
453         """
454         if name in self.quote_cache:
455             return self.quote_cache[name]
456         if ((name in self.query.alias_map and name not in self.query.table_map) or
457                 name in self.query.extra_select or (
458                     self.query.external_aliases.get(name) and name not in self.query.table_map)):
459             self.quote_cache[name] = name
460             return name
461         r = self.connection.ops.quote_name(name)
462         self.quote_cache[name] = r
463         return r
464 
465     def compile(self, node):
466         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)
467         if vendor_impl:
468             sql, params = vendor_impl(self, self.connection)
469         else:
470             sql, params = node.as_sql(self, self.connection)
471         return sql, params
472 
473     def get_combinator_sql(self, combinator, all):
474         features = self.connection.features
475         compilers = [
476             query.get_compiler(self.using, self.connection, self.elide_empty)
477             for query in self.query.combined_queries if not query.is_empty()
478         ]
479         if not features.supports_slicing_ordering_in_compound:
480             for query, compiler in zip(self.query.combined_queries, compilers):
481                 if query.low_mark or query.high_mark:
482                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')
483                 if compiler.get_order_by():
484                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')
485         parts = ()
486         for compiler in compilers:
487             try:
488                 # If the columns list is limited, then all combined queries
489                 # must have the same columns list. Set the selects defined on
490                 # the query on all combined queries, if not already set.
491                 if not compiler.query.values_select and self.query.values_select:
492                     compiler.query = compiler.query.clone()
493                     compiler.query.set_values((
494                         *self.query.extra_select,
495                         *self.query.values_select,
496                         *self.query.annotation_select,
497                     ))
498                 part_sql, part_args = compiler.as_sql()
499                 if compiler.query.combinator:
500                     # Wrap in a subquery if wrapping in parentheses isn't
501                     # supported.
502                     if not features.supports_parentheses_in_compound:
503                         part_sql = 'SELECT * FROM ({})'.format(part_sql)
504                     # Add parentheses when combining with compound query if not
505                     # already added for all compound queries.
506                     elif (
507                         self.query.subquery or
508                         not features.supports_slicing_ordering_in_compound
509                     ):
510                         part_sql = '({})'.format(part_sql)
511                 parts += ((part_sql, part_args),)
512             except EmptyResultSet:
513                 # Omit the empty queryset with UNION and with DIFFERENCE if the
514                 # first queryset is nonempty.
515                 if combinator == 'union' or (combinator == 'difference' and parts):
516                     continue
517                 raise
518         if not parts:
519             raise EmptyResultSet
520         combinator_sql = self.connection.ops.set_operators[combinator]
521         if all and combinator == 'union':
522             combinator_sql += ' ALL'
523         braces = '{}'
524         if not self.query.subquery and features.supports_slicing_ordering_in_compound:
525             braces = '({})'
526         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))
527         result = [' {} '.format(combinator_sql).join(sql_parts)]
528         params = []
529         for part in args_parts:
530             params.extend(part)
531         return result, params
532 
533     def as_sql(self, with_limits=True, with_col_aliases=False):
534         """
535         Create the SQL for this query. Return the SQL string and list of
536         parameters.
537 
538         If 'with_limits' is False, any limit/offset information is not included
539         in the query.
540         """
541         refcounts_before = self.query.alias_refcount.copy()
542         try:
543             extra_select, order_by, group_by = self.pre_sql_setup()
544             for_update_part = None
545             # Is a LIMIT/OFFSET clause needed?
546             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)
547             combinator = self.query.combinator
548             features = self.connection.features
549             if combinator:
550                 if not getattr(features, 'supports_select_{}'.format(combinator)):
551                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))
552                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)
553             else:
554                 distinct_fields, distinct_params = self.get_distinct()
555                 # This must come after 'select', 'ordering', and 'distinct'
556                 # (see docstring of get_from_clause() for details).
557                 from_, f_params = self.get_from_clause()
558                 try:
559                     where, w_params = self.compile(self.where) if self.where is not None else ('', [])
560                 except EmptyResultSet:
561                     if self.elide_empty:
562                         raise
563                     # Use a predicate that's always False.
564                     where, w_params = '0 = 1', []
565                 having, h_params = self.compile(self.having) if self.having is not None else ("", [])
566                 result = ['SELECT']
567                 params = []
568 
569                 if self.query.distinct:
570                     distinct_result, distinct_params = self.connection.ops.distinct_sql(
571                         distinct_fields,
572                         distinct_params,
573                     )
574                     result += distinct_result
575                     params += distinct_params
576 
577                 out_cols = []
578                 col_idx = 1
579                 for _, (s_sql, s_params), alias in self.select + extra_select:
580                     if alias:
581                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))
582                     elif with_col_aliases:
583                         s_sql = '%s AS %s' % (
584                             s_sql,
585                             self.connection.ops.quote_name('col%d' % col_idx),
586                         )
587                         col_idx += 1
588                     params.extend(s_params)
589                     out_cols.append(s_sql)
590 
591                 result += [', '.join(out_cols), 'FROM', *from_]
592                 params.extend(f_params)
593 
594                 if self.query.select_for_update and self.connection.features.has_select_for_update:
595                     if self.connection.get_autocommit():
596                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')
597 
598                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:
599                         raise NotSupportedError(
600                             'LIMIT/OFFSET is not supported with '
601                             'select_for_update on this database backend.'
602                         )
603                     nowait = self.query.select_for_update_nowait
604                     skip_locked = self.query.select_for_update_skip_locked
605                     of = self.query.select_for_update_of
606                     no_key = self.query.select_for_no_key_update
607                     # If it's a NOWAIT/SKIP LOCKED/OF/NO KEY query but the
608                     # backend doesn't support it, raise NotSupportedError to
609                     # prevent a possible deadlock.
610                     if nowait and not self.connection.features.has_select_for_update_nowait:
611                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
612                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
613                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
614                     elif of and not self.connection.features.has_select_for_update_of:
615                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
616                     elif no_key and not self.connection.features.has_select_for_no_key_update:
617                         raise NotSupportedError(
618                             'FOR NO KEY UPDATE is not supported on this '
619                             'database backend.'
620                         )
621                     for_update_part = self.connection.ops.for_update_sql(
622                         nowait=nowait,
623                         skip_locked=skip_locked,
624                         of=self.get_select_for_update_of_arguments(),
625                         no_key=no_key,
626                     )
627 
628                 if for_update_part and self.connection.features.for_update_after_from:
629                     result.append(for_update_part)
630 
631                 if where:
632                     result.append('WHERE %s' % where)
633                     params.extend(w_params)
634 
635                 grouping = []
636                 for g_sql, g_params in group_by:
637                     grouping.append(g_sql)
638                     params.extend(g_params)
639                 if grouping:
640                     if distinct_fields:
641                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')
642                     order_by = order_by or self.connection.ops.force_no_ordering()
643                     result.append('GROUP BY %s' % ', '.join(grouping))
644                     if self._meta_ordering:
645                         order_by = None
646                 if having:
647                     result.append('HAVING %s' % having)
648                     params.extend(h_params)
649 
650             if self.query.explain_info:
651                 result.insert(0, self.connection.ops.explain_query_prefix(
652                     self.query.explain_info.format,
653                     **self.query.explain_info.options
654                 ))
655 
656             if order_by:
657                 ordering = []
658                 for _, (o_sql, o_params, _) in order_by:
659                     ordering.append(o_sql)
660                     params.extend(o_params)
661                 result.append('ORDER BY %s' % ', '.join(ordering))
662 
663             if with_limit_offset:
664                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))
665 
666             if for_update_part and not self.connection.features.for_update_after_from:
667                 result.append(for_update_part)
668 
669             if self.query.subquery and extra_select:
670                 # If the query is used as a subquery, the extra selects would
671                 # result in more columns than the left-hand side expression is
672                 # expecting. This can happen when a subquery uses a combination
673                 # of order_by() and distinct(), forcing the ordering expressions
674                 # to be selected as well. Wrap the query in another subquery
675                 # to exclude extraneous selects.
676                 sub_selects = []
677                 sub_params = []
678                 for index, (select, _, alias) in enumerate(self.select, start=1):
679                     if not alias and with_col_aliases:
680                         alias = 'col%d' % index
681                     if alias:
682                         sub_selects.append("%s.%s" % (
683                             self.connection.ops.quote_name('subquery'),
684                             self.connection.ops.quote_name(alias),
685                         ))
686                     else:
687                         select_clone = select.relabeled_clone({select.alias: 'subquery'})
688                         subselect, subparams = select_clone.as_sql(self, self.connection)
689                         sub_selects.append(subselect)
690                         sub_params.extend(subparams)
691                 return 'SELECT %s FROM (%s) subquery' % (
692                     ', '.join(sub_selects),
693                     ' '.join(result),
694                 ), tuple(sub_params + params)
695 
696             return ' '.join(result), tuple(params)
697         finally:
698             # Finally do cleanup - get rid of the joins we created above.
699             self.query.reset_refcounts(refcounts_before)
700 
701     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):
702         """
703         Compute the default columns for selecting every field in the base
704         model. Will sometimes be called to pull in related models (e.g. via
705         select_related), in which case "opts" and "start_alias" will be given
706         to provide a starting point for the traversal.
707 
708         Return a list of strings, quoted appropriately for use in SQL
709         directly, as well as a set of aliases used in the select statement (if
710         'as_pairs' is True, return a list of (alias, col_name) pairs instead
711         of strings as the first component and None as the second component).
712         """
713         result = []
714         if opts is None:
715             opts = self.query.get_meta()
716         only_load = self.deferred_to_columns()
717         start_alias = start_alias or self.query.get_initial_alias()
718         # The 'seen_models' is used to optimize checking the needed parent
719         # alias for a given field. This also includes None -> start_alias to
720         # be used by local fields.
721         seen_models = {None: start_alias}
722 
723         for field in opts.concrete_fields:
724             model = field.model._meta.concrete_model
725             # A proxy model will have a different model and concrete_model. We
726             # will assign None if the field belongs to this model.
727             if model == opts.model:
728                 model = None
729             if from_parent and model is not None and issubclass(
730                     from_parent._meta.concrete_model, model._meta.concrete_model):
731                 # Avoid loading data for already loaded parents.
732                 # We end up here in the case select_related() resolution
733                 # proceeds from parent model to child model. In that case the
734                 # parent model data is already present in the SELECT clause,
735                 # and we want to avoid reloading the same data again.
736                 continue
737             if field.model in only_load and field.attname not in only_load[field.model]:
738                 continue
739             alias = self.query.join_parent_model(opts, model, start_alias,
740                                                  seen_models)
741             column = field.get_col(alias)
742             result.append(column)
743         return result
744 
745     def get_distinct(self):
746         """
747         Return a quoted list of fields to use in DISTINCT ON part of the query.
748 
749         This method can alter the tables in the query, and thus it must be
750         called before get_from_clause().
751         """
752         result = []
753         params = []
754         opts = self.query.get_meta()
755 
756         for name in self.query.distinct_fields:
757             parts = name.split(LOOKUP_SEP)
758             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)
759             targets, alias, _ = self.query.trim_joins(targets, joins, path)
760             for target in targets:
761                 if name in self.query.annotation_select:
762                     result.append(self.connection.ops.quote_name(name))
763                 else:
764                     r, p = self.compile(transform_function(target, alias))
765                     result.append(r)
766                     params.append(p)
767         return result, params
768 
769     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',
770                            already_seen=None):
771         """
772         Return the table alias (the name might be ambiguous, the alias will
773         not be) and column name for ordering by the given 'name' parameter.
774         The 'name' is of the form 'field1__field2__...__fieldN'.
775         """
776         name, order = get_order_dir(name, default_order)
777         descending = order == 'DESC'
778         pieces = name.split(LOOKUP_SEP)
779         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)
780 
781         # If we get to this point and the field is a relation to another model,
782         # append the default ordering for that model unless it is the pk
783         # shortcut or the attribute name of the field that is specified.
784         if (
785             field.is_relation and
786             opts.ordering and
787             getattr(field, 'attname', None) != pieces[-1] and
788             name != 'pk'
789         ):
790             # Firstly, avoid infinite loops.
791             already_seen = already_seen or set()
792             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)
793             if join_tuple in already_seen:
794                 raise FieldError('Infinite loop caused by ordering.')
795             already_seen.add(join_tuple)
796 
797             results = []
798             for item in opts.ordering:
799                 if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):
800                     item = item.desc() if descending else item.asc()
801                 if isinstance(item, OrderBy):
802                     results.append((item, False))
803                     continue
804                 results.extend(self.find_ordering_name(item, opts, alias,
805                                                        order, already_seen))
806             return results
807         targets, alias, _ = self.query.trim_joins(targets, joins, path)
808         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]
809 
810     def _setup_joins(self, pieces, opts, alias):
811         """
812         Helper method for get_order_by() and get_distinct().
813 
814         get_ordering() and get_distinct() must produce same target columns on
815         same input, as the prefixes of get_ordering() and get_distinct() must
816         match. Executing SQL where this is not true is an error.
817         """
818         alias = alias or self.query.get_initial_alias()
819         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)
820         alias = joins[-1]
821         return field, targets, alias, joins, path, opts, transform_function
822 
823     def get_from_clause(self):
824         """
825         Return a list of strings that are joined together to go after the
826         "FROM" part of the query, as well as a list any extra parameters that
827         need to be included. Subclasses, can override this to create a
828         from-clause via a "select".
829 
830         This should only be called after any SQL construction methods that
831         might change the tables that are needed. This means the select columns,
832         ordering, and distinct must be done first.
833         """
834         result = []
835         params = []
836         for alias in tuple(self.query.alias_map):
837             if not self.query.alias_refcount[alias]:
838                 continue
839             try:
840                 from_clause = self.query.alias_map[alias]
841             except KeyError:
842                 # Extra tables can end up in self.tables, but not in the
843                 # alias_map if they aren't in a join. That's OK. We skip them.
844                 continue
845             clause_sql, clause_params = self.compile(from_clause)
846             result.append(clause_sql)
847             params.extend(clause_params)
848         for t in self.query.extra_tables:
849             alias, _ = self.query.table_alias(t)
850             # Only add the alias if it's not already present (the table_alias()
851             # call increments the refcount, so an alias refcount of one means
852             # this is the only reference).
853             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:
854                 result.append(', %s' % self.quote_name_unless_alias(alias))
855         return result, params
856 
857     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,
858                                requested=None, restricted=None):
859         """
860         Fill in the information needed for a select_related query. The current
861         depth is measured as the number of connections away from the root model
862         (for example, cur_depth=1 means we are looking at models with direct
863         connections to the root model).
864         """
865         def _get_field_choices():
866             direct_choices = (f.name for f in opts.fields if f.is_relation)
867             reverse_choices = (
868                 f.field.related_query_name()
869                 for f in opts.related_objects if f.field.unique
870             )
871             return chain(direct_choices, reverse_choices, self.query._filtered_relations)
872 
873         related_klass_infos = []
874         if not restricted and cur_depth > self.query.max_depth:
875             # We've recursed far enough; bail out.
876             return related_klass_infos
877 
878         if not opts:
879             opts = self.query.get_meta()
880             root_alias = self.query.get_initial_alias()
881         only_load = self.query.get_loaded_field_names()
882 
883         # Setup for the case when only particular related fields should be
884         # included in the related selection.
885         fields_found = set()
886         if requested is None:
887             restricted = isinstance(self.query.select_related, dict)
888             if restricted:
889                 requested = self.query.select_related
890 
891         def get_related_klass_infos(klass_info, related_klass_infos):
892             klass_info['related_klass_infos'] = related_klass_infos
893 
894         for f in opts.fields:
895             field_model = f.model._meta.concrete_model
896             fields_found.add(f.name)
897 
898             if restricted:
899                 next = requested.get(f.name, {})
900                 if not f.is_relation:
901                     # If a non-related field is used like a relation,
902                     # or if a single non-relational field is given.
903                     if next or f.name in requested:
904                         raise FieldError(
905                             "Non-relational field given in select_related: '%s'. "
906                             "Choices are: %s" % (
907                                 f.name,
908                                 ", ".join(_get_field_choices()) or '(none)',
909                             )
910                         )
911             else:
912                 next = False
913 
914             if not select_related_descend(f, restricted, requested,
915                                           only_load.get(field_model)):
916                 continue
917             klass_info = {
918                 'model': f.remote_field.model,
919                 'field': f,
920                 'reverse': False,
921                 'local_setter': f.set_cached_value,
922                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,
923                 'from_parent': False,
924             }
925             related_klass_infos.append(klass_info)
926             select_fields = []
927             _, _, _, joins, _, _ = self.query.setup_joins(
928                 [f.name], opts, root_alias)
929             alias = joins[-1]
930             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)
931             for col in columns:
932                 select_fields.append(len(select))
933                 select.append((col, None))
934             klass_info['select_fields'] = select_fields
935             next_klass_infos = self.get_related_selections(
936                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)
937             get_related_klass_infos(klass_info, next_klass_infos)
938 
939         if restricted:
940             related_fields = [
941                 (o.field, o.related_model)
942                 for o in opts.related_objects
943                 if o.field.unique and not o.many_to_many
944             ]
945             for f, model in related_fields:
946                 if not select_related_descend(f, restricted, requested,
947                                               only_load.get(model), reverse=True):
948                     continue
949 
950                 related_field_name = f.related_query_name()
951                 fields_found.add(related_field_name)
952 
953                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)
954                 alias = join_info.joins[-1]
955                 from_parent = issubclass(model, opts.model) and model is not opts.model
956                 klass_info = {
957                     'model': model,
958                     'field': f,
959                     'reverse': True,
960                     'local_setter': f.remote_field.set_cached_value,
961                     'remote_setter': f.set_cached_value,
962                     'from_parent': from_parent,
963                 }
964                 related_klass_infos.append(klass_info)
965                 select_fields = []
966                 columns = self.get_default_columns(
967                     start_alias=alias, opts=model._meta, from_parent=opts.model)
968                 for col in columns:
969                     select_fields.append(len(select))
970                     select.append((col, None))
971                 klass_info['select_fields'] = select_fields
972                 next = requested.get(f.related_query_name(), {})
973                 next_klass_infos = self.get_related_selections(
974                     select, model._meta, alias, cur_depth + 1,
975                     next, restricted)
976                 get_related_klass_infos(klass_info, next_klass_infos)
977 
978             def local_setter(obj, from_obj):
979                 # Set a reverse fk object when relation is non-empty.
980                 if from_obj:
981                     f.remote_field.set_cached_value(from_obj, obj)
982 
983             def remote_setter(name, obj, from_obj):
984                 setattr(from_obj, name, obj)
985 
986             for name in list(requested):
987                 # Filtered relations work only on the topmost level.
988                 if cur_depth > 1:
989                     break
990                 if name in self.query._filtered_relations:
991                     fields_found.add(name)
992                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)
993                     model = join_opts.model
994                     alias = joins[-1]
995                     from_parent = issubclass(model, opts.model) and model is not opts.model
996                     klass_info = {
997                         'model': model,
998                         'field': f,
999                         'reverse': True,
1000                         'local_setter': local_setter,
1001                         'remote_setter': partial(remote_setter, name),
1002                         'from_parent': from_parent,
1003                     }
1004                     related_klass_infos.append(klass_info)
1005                     select_fields = []
1006                     columns = self.get_default_columns(
1007                         start_alias=alias, opts=model._meta,
1008                         from_parent=opts.model,
1009                     )
1010                     for col in columns:
1011                         select_fields.append(len(select))
1012                         select.append((col, None))
1013                     klass_info['select_fields'] = select_fields
1014                     next_requested = requested.get(name, {})
1015                     next_klass_infos = self.get_related_selections(
1016                         select, opts=model._meta, root_alias=alias,
1017                         cur_depth=cur_depth + 1, requested=next_requested,
1018                         restricted=restricted,
1019                     )
1020                     get_related_klass_infos(klass_info, next_klass_infos)
1021             fields_not_found = set(requested).difference(fields_found)
1022             if fields_not_found:
1023                 invalid_fields = ("'%s'" % s for s in fields_not_found)
1024                 raise FieldError(
1025                     'Invalid field name(s) given in select_related: %s. '
1026                     'Choices are: %s' % (
1027                         ', '.join(invalid_fields),
1028                         ', '.join(_get_field_choices()) or '(none)',
1029                     )
1030                 )
1031         return related_klass_infos
1032 
1033     def get_select_for_update_of_arguments(self):
1034         """
1035         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of
1036         the query.
1037         """
1038         def _get_parent_klass_info(klass_info):
1039             concrete_model = klass_info['model']._meta.concrete_model
1040             for parent_model, parent_link in concrete_model._meta.parents.items():
1041                 parent_list = parent_model._meta.get_parent_list()
1042                 yield {
1043                     'model': parent_model,
1044                     'field': parent_link,
1045                     'reverse': False,
1046                     'select_fields': [
1047                         select_index
1048                         for select_index in klass_info['select_fields']
1049                         # Selected columns from a model or its parents.
1050                         if (
1051                             self.select[select_index][0].target.model == parent_model or
1052                             self.select[select_index][0].target.model in parent_list
1053                         )
1054                     ],
1055                 }
1056 
1057         def _get_first_selected_col_from_model(klass_info):
1058             """
1059             Find the first selected column from a model. If it doesn't exist,
1060             don't lock a model.
1061 
1062             select_fields is filled recursively, so it also contains fields
1063             from the parent models.
1064             """
1065             concrete_model = klass_info['model']._meta.concrete_model
1066             for select_index in klass_info['select_fields']:
1067                 if self.select[select_index][0].target.model == concrete_model:
1068                     return self.select[select_index][0]
1069 
1070         def _get_field_choices():
1071             """Yield all allowed field paths in breadth-first search order."""
1072             queue = collections.deque([(None, self.klass_info)])
1073             while queue:
1074                 parent_path, klass_info = queue.popleft()
1075                 if parent_path is None:
1076                     path = []
1077                     yield 'self'
1078                 else:
1079                     field = klass_info['field']
1080                     if klass_info['reverse']:
1081                         field = field.remote_field
1082                     path = parent_path + [field.name]
1083                     yield LOOKUP_SEP.join(path)
1084                 queue.extend(
1085                     (path, klass_info)
1086                     for klass_info in _get_parent_klass_info(klass_info)
1087                 )
1088                 queue.extend(
1089                     (path, klass_info)
1090                     for klass_info in klass_info.get('related_klass_infos', [])
1091                 )
1092         if not self.klass_info:
1093             return []
1094         result = []
1095         invalid_names = []
1096         for name in self.query.select_for_update_of:
1097             klass_info = self.klass_info
1098             if name == 'self':
1099                 col = _get_first_selected_col_from_model(klass_info)
1100             else:
1101                 for part in name.split(LOOKUP_SEP):
1102                     klass_infos = (
1103                         *klass_info.get('related_klass_infos', []),
1104                         *_get_parent_klass_info(klass_info),
1105                     )
1106                     for related_klass_info in klass_infos:
1107                         field = related_klass_info['field']
1108                         if related_klass_info['reverse']:
1109                             field = field.remote_field
1110                         if field.name == part:
1111                             klass_info = related_klass_info
1112                             break
1113                     else:
1114                         klass_info = None
1115                         break
1116                 if klass_info is None:
1117                     invalid_names.append(name)
1118                     continue
1119                 col = _get_first_selected_col_from_model(klass_info)
1120             if col is not None:
1121                 if self.connection.features.select_for_update_of_column:
1122                     result.append(self.compile(col)[0])
1123                 else:
1124                     result.append(self.quote_name_unless_alias(col.alias))
1125         if invalid_names:
1126             raise FieldError(
1127                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '
1128                 'Only relational fields followed in the query are allowed. '
1129                 'Choices are: %s.' % (
1130                     ', '.join(invalid_names),
1131                     ', '.join(_get_field_choices()),
1132                 )
1133             )
1134         return result
1135 
1136     def deferred_to_columns(self):
1137         """
1138         Convert the self.deferred_loading data structure to mapping of table
1139         names to sets of column names which are to be loaded. Return the
1140         dictionary.
1141         """
1142         columns = {}
1143         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)
1144         return columns
1145 
1146     def get_converters(self, expressions):
1147         converters = {}
1148         for i, expression in enumerate(expressions):
1149             if expression:
1150                 backend_converters = self.connection.ops.get_db_converters(expression)
1151                 field_converters = expression.get_db_converters(self.connection)
1152                 if backend_converters or field_converters:
1153                     converters[i] = (backend_converters + field_converters, expression)
1154         return converters
1155 
1156     def apply_converters(self, rows, converters):
1157         connection = self.connection
1158         converters = list(converters.items())
1159         for row in map(list, rows):
1160             for pos, (convs, expression) in converters:
1161                 value = row[pos]
1162                 for converter in convs:
1163                     value = converter(value, expression, connection)
1164                 row[pos] = value
1165             yield row
1166 
1167     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,
1168                      chunk_size=GET_ITERATOR_CHUNK_SIZE):
1169         """Return an iterator over the results from executing this query."""
1170         if results is None:
1171             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)
1172         fields = [s[0] for s in self.select[0:self.col_count]]
1173         converters = self.get_converters(fields)
1174         rows = chain.from_iterable(results)
1175         if converters:
1176             rows = self.apply_converters(rows, converters)
1177             if tuple_expected:
1178                 rows = map(tuple, rows)
1179         return rows
1180 
1181     def has_results(self):
1182         """
1183         Backends (e.g. NoSQL) can override this in order to use optimized
1184         versions of "query has any results."
1185         """
1186         return bool(self.execute_sql(SINGLE))
1187 
1188     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
1189         """
1190         Run the query against the database and return the result(s). The
1191         return value is a single data item if result_type is SINGLE, or an
1192         iterator over the results if the result_type is MULTI.
1193 
1194         result_type is either MULTI (use fetchmany() to retrieve all rows),
1195         SINGLE (only retrieve a single row), or None. In this last case, the
1196         cursor is returned if any query is executed, since it's used by
1197         subclasses such as InsertQuery). It's possible, however, that no query
1198         is needed, as the filters describe an empty set. In that case, None is
1199         returned, to avoid any unnecessary database interaction.
1200         """
1201         result_type = result_type or NO_RESULTS
1202         try:
1203             sql, params = self.as_sql()
1204             if not sql:
1205                 raise EmptyResultSet
1206         except EmptyResultSet:
1207             if result_type == MULTI:
1208                 return iter([])
1209             else:
1210                 return
1211         if chunked_fetch:
1212             cursor = self.connection.chunked_cursor()
1213         else:
1214             cursor = self.connection.cursor()
1215         try:
1216             cursor.execute(sql, params)
1217         except Exception:
1218             # Might fail for server-side cursors (e.g. connection closed)
1219             cursor.close()
1220             raise
1221 
1222         if result_type == CURSOR:
1223             # Give the caller the cursor to process and close.
1224             return cursor
1225         if result_type == SINGLE:
1226             try:
1227                 val = cursor.fetchone()
1228                 if val:
1229                     return val[0:self.col_count]
1230                 return val
1231             finally:
1232                 # done with the cursor
1233                 cursor.close()
1234         if result_type == NO_RESULTS:
1235             cursor.close()
1236             return
1237 
1238         result = cursor_iter(
1239             cursor, self.connection.features.empty_fetchmany_value,
1240             self.col_count if self.has_extra_select else None,
1241             chunk_size,
1242         )
1243         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:
1244             # If we are using non-chunked reads, we return the same data
1245             # structure as normally, but ensure it is all read into memory
1246             # before going any further. Use chunked_fetch if requested,
1247             # unless the database doesn't support it.
1248             return list(result)
1249         return result
1250 
1251     def as_subquery_condition(self, alias, columns, compiler):
1252         qn = compiler.quote_name_unless_alias
1253         qn2 = self.connection.ops.quote_name
1254 
1255         for index, select_col in enumerate(self.query.select):
1256             lhs_sql, lhs_params = self.compile(select_col)
1257             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))
1258             self.query.where.add(
1259                 RawSQL('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')
1260 
1261         sql, params = self.as_sql()
1262         return 'EXISTS (%s)' % sql, params
1263 
1264     def explain_query(self):
1265         result = list(self.execute_sql())
1266         # Some backends return 1 item tuples with strings, and others return
1267         # tuples with integers and strings. Flatten them out into strings.
1268         output_formatter = json.dumps if self.query.explain_info.format == 'json' else str
1269         for row in result[0]:
1270             if not isinstance(row, str):
1271                 yield ' '.join(output_formatter(c) for c in row)
1272             else:
1273                 yield row
1274 
1275 
1276 class SQLInsertCompiler(SQLCompiler):
1277     returning_fields = None
1278     returning_params = tuple()
1279 
1280     def field_as_sql(self, field, val):
1281         """
1282         Take a field and a value intended to be saved on that field, and
1283         return placeholder SQL and accompanying params. Check for raw values,
1284         expressions, and fields with get_placeholder() defined in that order.
1285 
1286         When field is None, consider the value raw and use it as the
1287         placeholder, with no corresponding parameters returned.
1288         """
1289         if field is None:
1290             # A field value of None means the value is raw.
1291             sql, params = val, []
1292         elif hasattr(val, 'as_sql'):
1293             # This is an expression, let's compile it.
1294             sql, params = self.compile(val)
1295         elif hasattr(field, 'get_placeholder'):
1296             # Some fields (e.g. geo fields) need special munging before
1297             # they can be inserted.
1298             sql, params = field.get_placeholder(val, self, self.connection), [val]
1299         else:
1300             # Return the common case for the placeholder
1301             sql, params = '%s', [val]
1302 
1303         # The following hook is only used by Oracle Spatial, which sometimes
1304         # needs to yield 'NULL' and [] as its placeholder and params instead
1305         # of '%s' and [None]. The 'NULL' placeholder is produced earlier by
1306         # OracleOperations.get_geom_placeholder(). The following line removes
1307         # the corresponding None parameter. See ticket #10888.
1308         params = self.connection.ops.modify_insert_params(sql, params)
1309 
1310         return sql, params
1311 
1312     def prepare_value(self, field, value):
1313         """
1314         Prepare a value to be used in a query by resolving it if it is an
1315         expression and otherwise calling the field's get_db_prep_save().
1316         """
1317         if hasattr(value, 'resolve_expression'):
1318             value = value.resolve_expression(self.query, allow_joins=False, for_save=True)
1319             # Don't allow values containing Col expressions. They refer to
1320             # existing columns on a row, but in the case of insert the row
1321             # doesn't exist yet.
1322             if value.contains_column_references:
1323                 raise ValueError(
1324                     'Failed to insert expression "%s" on %s. F() expressions '
1325                     'can only be used to update, not to insert.' % (value, field)
1326                 )
1327             if value.contains_aggregate:
1328                 raise FieldError(
1329                     'Aggregate functions are not allowed in this query '
1330                     '(%s=%r).' % (field.name, value)
1331                 )
1332             if value.contains_over_clause:
1333                 raise FieldError(
1334                     'Window expressions are not allowed in this query (%s=%r).'
1335                     % (field.name, value)
1336                 )
1337         else:
1338             value = field.get_db_prep_save(value, connection=self.connection)
1339         return value
1340 
1341     def pre_save_val(self, field, obj):
1342         """
1343         Get the given field's value off the given obj. pre_save() is used for
1344         things like auto_now on DateTimeField. Skip it if this is a raw query.
1345         """
1346         if self.query.raw:
1347             return getattr(obj, field.attname)
1348         return field.pre_save(obj, add=True)
1349 
1350     def assemble_as_sql(self, fields, value_rows):
1351         """
1352         Take a sequence of N fields and a sequence of M rows of values, and
1353         generate placeholder SQL and parameters for each field and value.
1354         Return a pair containing:
1355          * a sequence of M rows of N SQL placeholder strings, and
1356          * a sequence of M rows of corresponding parameter values.
1357 
1358         Each placeholder string may contain any number of '%s' interpolation
1359         strings, and each parameter row will contain exactly as many params
1360         as the total number of '%s's in the corresponding placeholder row.
1361         """
1362         if not value_rows:
1363             return [], []
1364 
1365         # list of (sql, [params]) tuples for each object to be saved
1366         # Shape: [n_objs][n_fields][2]
1367         rows_of_fields_as_sql = (
1368             (self.field_as_sql(field, v) for field, v in zip(fields, row))
1369             for row in value_rows
1370         )
1371 
1372         # tuple like ([sqls], [[params]s]) for each object to be saved
1373         # Shape: [n_objs][2][n_fields]
1374         sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)
1375 
1376         # Extract separate lists for placeholders and params.
1377         # Each of these has shape [n_objs][n_fields]
1378         placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)
1379 
1380         # Params for each field are still lists, and need to be flattened.
1381         param_rows = [[p for ps in row for p in ps] for row in param_rows]
1382 
1383         return placeholder_rows, param_rows
1384 
1385     def as_sql(self):
1386         # We don't need quote_name_unless_alias() here, since these are all
1387         # going to be column names (so we can avoid the extra overhead).
1388         qn = self.connection.ops.quote_name
1389         opts = self.query.get_meta()
1390         insert_statement = self.connection.ops.insert_statement(
1391             on_conflict=self.query.on_conflict,
1392         )
1393         result = ['%s %s' % (insert_statement, qn(opts.db_table))]
1394         fields = self.query.fields or [opts.pk]
1395         result.append('(%s)' % ', '.join(qn(f.column) for f in fields))
1396 
1397         if self.query.fields:
1398             value_rows = [
1399                 [self.prepare_value(field, self.pre_save_val(field, obj)) for field in fields]
1400                 for obj in self.query.objs
1401             ]
1402         else:
1403             # An empty object.
1404             value_rows = [[self.connection.ops.pk_default_value()] for _ in self.query.objs]
1405             fields = [None]
1406 
1407         # Currently the backends just accept values when generating bulk
1408         # queries and generate their own placeholders. Doing that isn't
1409         # necessary and it should be possible to use placeholders and
1410         # expressions in bulk inserts too.
1411         can_bulk = (not self.returning_fields and self.connection.features.has_bulk_insert)
1412 
1413         placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)
1414 
1415         on_conflict_suffix_sql = self.connection.ops.on_conflict_suffix_sql(
1416             fields,
1417             self.query.on_conflict,
1418             self.query.update_fields,
1419             self.query.unique_fields,
1420         )
1421         if self.returning_fields and self.connection.features.can_return_columns_from_insert:
1422             if self.connection.features.can_return_rows_from_bulk_insert:
1423                 result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))
1424                 params = param_rows
1425             else:
1426                 result.append("VALUES (%s)" % ", ".join(placeholder_rows[0]))
1427                 params = [param_rows[0]]
1428             if on_conflict_suffix_sql:
1429                 result.append(on_conflict_suffix_sql)
1430             # Skip empty r_sql to allow subclasses to customize behavior for
1431             # 3rd party backends. Refs #19096.
1432             r_sql, self.returning_params = self.connection.ops.return_insert_columns(self.returning_fields)
1433             if r_sql:
1434                 result.append(r_sql)
1435                 params += [self.returning_params]
1436             return [(" ".join(result), tuple(chain.from_iterable(params)))]
1437 
1438         if can_bulk:
1439             result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))
1440             if on_conflict_suffix_sql:
1441                 result.append(on_conflict_suffix_sql)
1442             return [(" ".join(result), tuple(p for ps in param_rows for p in ps))]
1443         else:
1444             if on_conflict_suffix_sql:
1445                 result.append(on_conflict_suffix_sql)
1446             return [
1447                 (" ".join(result + ["VALUES (%s)" % ", ".join(p)]), vals)
1448                 for p, vals in zip(placeholder_rows, param_rows)
1449             ]
1450 
1451     def execute_sql(self, returning_fields=None):
1452         assert not (
1453             returning_fields and len(self.query.objs) != 1 and
1454             not self.connection.features.can_return_rows_from_bulk_insert
1455         )
1456         opts = self.query.get_meta()
1457         self.returning_fields = returning_fields
1458         with self.connection.cursor() as cursor:
1459             for sql, params in self.as_sql():
1460                 cursor.execute(sql, params)
1461             if not self.returning_fields:
1462                 return []
1463             if self.connection.features.can_return_rows_from_bulk_insert and len(self.query.objs) > 1:
1464                 rows = self.connection.ops.fetch_returned_insert_rows(cursor)
1465             elif self.connection.features.can_return_columns_from_insert:
1466                 assert len(self.query.objs) == 1
1467                 rows = [self.connection.ops.fetch_returned_insert_columns(
1468                     cursor, self.returning_params,
1469                 )]
1470             else:
1471                 rows = [(self.connection.ops.last_insert_id(
1472                     cursor, opts.db_table, opts.pk.column,
1473                 ),)]
1474         cols = [field.get_col(opts.db_table) for field in self.returning_fields]
1475         converters = self.get_converters(cols)
1476         if converters:
1477             rows = list(self.apply_converters(rows, converters))
1478         return rows
1479 
1480 
1481 class SQLDeleteCompiler(SQLCompiler):
1482     @cached_property
1483     def single_alias(self):
1484         # Ensure base table is in aliases.
1485         self.query.get_initial_alias()
1486         return sum(self.query.alias_refcount[t] > 0 for t in self.query.alias_map) == 1
1487 
1488     @classmethod
1489     def _expr_refs_base_model(cls, expr, base_model):
1490         if isinstance(expr, Query):
1491             return expr.model == base_model
1492         if not hasattr(expr, 'get_source_expressions'):
1493             return False
1494         return any(
1495             cls._expr_refs_base_model(source_expr, base_model)
1496             for source_expr in expr.get_source_expressions()
1497         )
1498 
1499     @cached_property
1500     def contains_self_reference_subquery(self):
1501         return any(
1502             self._expr_refs_base_model(expr, self.query.model)
1503             for expr in chain(self.query.annotations.values(), self.query.where.children)
1504         )
1505 
1506     def _as_sql(self, query):
1507         result = [
1508             'DELETE FROM %s' % self.quote_name_unless_alias(query.base_table)
1509         ]
1510         where, params = self.compile(query.where)
1511         if where:
1512             result.append('WHERE %s' % where)
1513         return ' '.join(result), tuple(params)
1514 
1515     def as_sql(self):
1516         """
1517         Create the SQL for this query. Return the SQL string and list of
1518         parameters.
1519         """
1520         if self.single_alias and not self.contains_self_reference_subquery:
1521             return self._as_sql(self.query)
1522         innerq = self.query.clone()
1523         innerq.__class__ = Query
1524         innerq.clear_select_clause()
1525         pk = self.query.model._meta.pk
1526         innerq.select = [
1527             pk.get_col(self.query.get_initial_alias())
1528         ]
1529         outerq = Query(self.query.model)
1530         if not self.connection.features.update_can_self_select:
1531             # Force the materialization of the inner query to allow reference
1532             # to the target table on MySQL.
1533             sql, params = innerq.get_compiler(connection=self.connection).as_sql()
1534             innerq = RawSQL('SELECT * FROM (%s) subquery' % sql, params)
1535         outerq.add_filter('pk__in', innerq)
1536         return self._as_sql(outerq)
1537 
1538 
1539 class SQLUpdateCompiler(SQLCompiler):
1540     def as_sql(self):
1541         """
1542         Create the SQL for this query. Return the SQL string and list of
1543         parameters.
1544         """
1545         self.pre_sql_setup()
1546         if not self.query.values:
1547             return '', ()
1548         qn = self.quote_name_unless_alias
1549         values, update_params = [], []
1550         for field, model, val in self.query.values:
1551             if hasattr(val, 'resolve_expression'):
1552                 val = val.resolve_expression(self.query, allow_joins=False, for_save=True)
1553                 if val.contains_aggregate:
1554                     raise FieldError(
1555                         'Aggregate functions are not allowed in this query '
1556                         '(%s=%r).' % (field.name, val)
1557                     )
1558                 if val.contains_over_clause:
1559                     raise FieldError(
1560                         'Window expressions are not allowed in this query '
1561                         '(%s=%r).' % (field.name, val)
1562                     )
1563             elif hasattr(val, 'prepare_database_save'):
1564                 if field.remote_field:
1565                     val = field.get_db_prep_save(
1566                         val.prepare_database_save(field),
1567                         connection=self.connection,
1568                     )
1569                 else:
1570                     raise TypeError(
1571                         "Tried to update field %s with a model instance, %r. "
1572                         "Use a value compatible with %s."
1573                         % (field, val, field.__class__.__name__)
1574                     )
1575             else:
1576                 val = field.get_db_prep_save(val, connection=self.connection)
1577 
1578             # Getting the placeholder for the field.
1579             if hasattr(field, 'get_placeholder'):
1580                 placeholder = field.get_placeholder(val, self, self.connection)
1581             else:
1582                 placeholder = '%s'
1583             name = field.column
1584             if hasattr(val, 'as_sql'):
1585                 sql, params = self.compile(val)
1586                 values.append('%s = %s' % (qn(name), placeholder % sql))
1587                 update_params.extend(params)
1588             elif val is not None:
1589                 values.append('%s = %s' % (qn(name), placeholder))
1590                 update_params.append(val)
1591             else:
1592                 values.append('%s = NULL' % qn(name))
1593         table = self.query.base_table
1594         result = [
1595             'UPDATE %s SET' % qn(table),
1596             ', '.join(values),
1597         ]
1598         where, params = self.compile(self.query.where)
1599         if where:
1600             result.append('WHERE %s' % where)
1601         return ' '.join(result), tuple(update_params + params)
1602 
1603     def execute_sql(self, result_type):
1604         """
1605         Execute the specified update. Return the number of rows affected by
1606         the primary update query. The "primary update query" is the first
1607         non-empty query that is executed. Row counts for any subsequent,
1608         related queries are not available.
1609         """
1610         cursor = super().execute_sql(result_type)
1611         try:
1612             rows = cursor.rowcount if cursor else 0
1613             is_empty = cursor is None
1614         finally:
1615             if cursor:
1616                 cursor.close()
1617         for query in self.query.get_related_updates():
1618             aux_rows = query.get_compiler(self.using).execute_sql(result_type)
1619             if is_empty and aux_rows:
1620                 rows = aux_rows
1621                 is_empty = False
1622         return rows
1623 
1624     def pre_sql_setup(self):
1625         """
1626         If the update depends on results from other tables, munge the "where"
1627         conditions to match the format required for (portable) SQL updates.
1628 
1629         If multiple updates are required, pull out the id values to update at
1630         this point so that they don't change as a result of the progressive
1631         updates.
1632         """
1633         refcounts_before = self.query.alias_refcount.copy()
1634         # Ensure base table is in the query
1635         self.query.get_initial_alias()
1636         count = self.query.count_active_tables()
1637         if not self.query.related_updates and count == 1:
1638             return
1639         query = self.query.chain(klass=Query)
1640         query.select_related = False
1641         query.clear_ordering(force=True)
1642         query.extra = {}
1643         query.select = []
1644         query.add_fields([query.get_meta().pk.name])
1645         super().pre_sql_setup()
1646 
1647         must_pre_select = count > 1 and not self.connection.features.update_can_self_select
1648 
1649         # Now we adjust the current query: reset the where clause and get rid
1650         # of all the tables we don't need (since they're in the sub-select).
1651         self.query.clear_where()
1652         if self.query.related_updates or must_pre_select:
1653             # Either we're using the idents in multiple update queries (so
1654             # don't want them to change), or the db backend doesn't support
1655             # selecting from the updating table (e.g. MySQL).
1656             idents = []
1657             for rows in query.get_compiler(self.using).execute_sql(MULTI):
1658                 idents.extend(r[0] for r in rows)
1659             self.query.add_filter('pk__in', idents)
1660             self.query.related_ids = idents
1661         else:
1662             # The fast path. Filters and updates in one query.
1663             self.query.add_filter('pk__in', query)
1664         self.query.reset_refcounts(refcounts_before)
1665 
1666 
1667 class SQLAggregateCompiler(SQLCompiler):
1668     def as_sql(self):
1669         """
1670         Create the SQL for this query. Return the SQL string and list of
1671         parameters.
1672         """
1673         sql, params = [], []
1674         for annotation in self.query.annotation_select.values():
1675             ann_sql, ann_params = self.compile(annotation)
1676             ann_sql, ann_params = annotation.select_format(self, ann_sql, ann_params)
1677             sql.append(ann_sql)
1678             params.extend(ann_params)
1679         self.col_count = len(self.query.annotation_select)
1680         sql = ', '.join(sql)
1681         params = tuple(params)
1682 
1683         inner_query_sql, inner_query_params = self.query.inner_query.get_compiler(
1684             self.using, elide_empty=self.elide_empty,
1685         ).as_sql(with_col_aliases=True)
1686         sql = 'SELECT %s FROM (%s) subquery' % (sql, inner_query_sql)
1687         params = params + inner_query_params
1688         return sql, params
1689 
1690 
1691 def cursor_iter(cursor, sentinel, col_count, itersize):
1692     """
1693     Yield blocks of rows from a cursor and ensure the cursor is closed when
1694     done.
1695     """
1696     try:
1697         for rows in iter((lambda: cursor.fetchmany(itersize)), sentinel):
1698             yield rows if col_count is None else [r[:col_count] for r in rows]
1699     finally:
1700         cursor.close()

</code><intended_behavior>This is a speculative location based on the usage of `EmptyResultSet` across the codebase, particularly in SQL compiler logic. A revision might be necessary to ensure that when compiling queries involving an `Exists` subquery that has been negated and is based on an `none()` queryset, the compiler preserves the WHERE block appropriately, potentially as a no-op or always-false condition, thereby conveying the intended semantics of the original queryset filtering criteria without erroneously excluding necessary SQL syntax.</intended_behavior>]
2025-10-17 23:27:21.235 | INFO     | Search completed. Bug locations: [<file>django/db/models/expressions.py</file>
<class>Exists</class>
<method>None</method>
<code>
1199 class Exists(Subquery):
1200     template = 'EXISTS(%(subquery)s)'
1201     output_field = fields.BooleanField()
1202 
1203     def __init__(self, queryset, negated=False, **kwargs):
1204         self.negated = negated
1205         super().__init__(queryset, **kwargs)
1206 
1207     def __invert__(self):
1208         clone = self.copy()
1209         clone.negated = not self.negated
1210         return clone
1211 
1212     def as_sql(self, compiler, connection, template=None, **extra_context):
1213         query = self.query.exists(using=connection.alias)
1214         sql, params = super().as_sql(
1215             compiler,
1216             connection,
1217             template=template,
1218             query=query,
1219             **extra_context,
1220         )
1221         if self.negated:
1222             sql = 'NOT {}'.format(sql)
1223         return sql, params
1224 
1225     def select_format(self, compiler, sql, params):
1226         # Wrap EXISTS() with a CASE WHEN expression if a database backend
1227         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP
1228         # BY list.
1229         if not compiler.connection.features.supports_boolean_expr_in_select_clause:
1230             sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)
1231         return sql, params

</code><intended_behavior>Ensure that when `Exists` is constructed with an empty queryset, especially when negated, the resulting SQL maintains the correct WHERE clause structure. The `as_sql` method, which generates the SQL for the EXISTS expression, should be reviewed to confirm it handles empty querysets in a way that does not lead to the entire WHERE block being omitted. This means likely adding logic to preserve a placeholder or no-op condition in the SQL when faced with an empty queryset, ensuring the WHERE clause is still present even if it's logically empty.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>none</method>
<code>
1061     def none(self):
1062         """Return an empty QuerySet."""
1063         clone = self._chain()
1064         clone.query.set_empty()
1065         return clone

</code><intended_behavior>The method should ensure that when `none` is called, and the resulting QuerySet is used within an `Exists` expression, the `EmptyResultSet` is handled in such a way that it doesn't lead to the unintended removal of WHERE clause in the composed SQL query. This might involve marking the QuerySet in a specific way that `Exists` can detect and handle appropriately, ensuring that the intended logic of 'no results possible' is encoded into the SQL without dropping the WHERE clause entirely.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
176 class QuerySet:
177     """Represent a lazy database lookup for a set of objects."""
178 
179     def __init__(self, model=None, query=None, using=None, hints=None):
180         self.model = model
181         self._db = using
182         self._hints = hints or {}
183         self._query = query or sql.Query(self.model)
184         self._result_cache = None
185         self._sticky_filter = False
186         self._for_write = False
187         self._prefetch_related_lookups = ()
188         self._prefetch_done = False
189         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
190         self._iterable_class = ModelIterable
191         self._fields = None
192         self._defer_next_filter = False
193         self._deferred_filter = None
194 
195     @property
196     def query(self):
197         if self._deferred_filter:
198             negate, args, kwargs = self._deferred_filter
199             self._filter_or_exclude_inplace(negate, args, kwargs)
200             self._deferred_filter = None
201         return self._query
202 
203     @query.setter
204     def query(self, value):
205         if value.values_select:
206             self._iterable_class = ValuesIterable
207         self._query = value
208 
209     def as_manager(cls):
210         # Address the circular dependency between `Queryset` and `Manager`.
211         from django.db.models.manager import Manager
212         manager = Manager.from_queryset(cls)()
213         manager._built_with_as_manager = True
214         return manager
215     as_manager.queryset_only = True
216     as_manager = classmethod(as_manager)
217 
218     ########################
219     # PYTHON MAGIC METHODS #
220     ########################
221 
222     def __deepcopy__(self, memo):
223         """Don't populate the QuerySet's cache."""
224         obj = self.__class__()
225         for k, v in self.__dict__.items():
226             if k == '_result_cache':
227                 obj.__dict__[k] = None
228             else:
229                 obj.__dict__[k] = copy.deepcopy(v, memo)
230         return obj
231 
232     def __getstate__(self):
233         # Force the cache to be fully populated.
234         self._fetch_all()
235         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
236 
237     def __setstate__(self, state):
238         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
239         if pickled_version:
240             if pickled_version != django.__version__:
241                 warnings.warn(
242                     "Pickled queryset instance's Django version %s does not "
243                     "match the current version %s."
244                     % (pickled_version, django.__version__),
245                     RuntimeWarning,
246                     stacklevel=2,
247                 )
248         else:
249             warnings.warn(
250                 "Pickled queryset instance's Django version is not specified.",
251                 RuntimeWarning,
252                 stacklevel=2,
253             )
254         self.__dict__.update(state)
255 
256     def __repr__(self):
257         data = list(self[:REPR_OUTPUT_SIZE + 1])
258         if len(data) > REPR_OUTPUT_SIZE:
259             data[-1] = "...(remaining elements truncated)..."
260         return '<%s %r>' % (self.__class__.__name__, data)
261 
262     def __len__(self):
263         self._fetch_all()
264         return len(self._result_cache)
265 
266     def __iter__(self):
267         """
268         The queryset iterator protocol uses three nested iterators in the
269         default case:
270             1. sql.compiler.execute_sql()
271                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
272                  using cursor.fetchmany(). This part is responsible for
273                  doing some column masking, and returning the rows in chunks.
274             2. sql.compiler.results_iter()
275                - Returns one row at time. At this point the rows are still just
276                  tuples. In some cases the return values are converted to
277                  Python values at this location.
278             3. self.iterator()
279                - Responsible for turning the rows into model objects.
280         """
281         self._fetch_all()
282         return iter(self._result_cache)
283 
284     def __bool__(self):
285         self._fetch_all()
286         return bool(self._result_cache)
287 
288     def __getitem__(self, k):
289         """Retrieve an item or slice from the set of results."""
290         if not isinstance(k, (int, slice)):
291             raise TypeError(
292                 'QuerySet indices must be integers or slices, not %s.'
293                 % type(k).__name__
294             )
295         if (
296             (isinstance(k, int) and k < 0) or
297             (isinstance(k, slice) and (
298                 (k.start is not None and k.start < 0) or
299                 (k.stop is not None and k.stop < 0)
300             ))
301         ):
302             raise ValueError('Negative indexing is not supported.')
303 
304         if self._result_cache is not None:
305             return self._result_cache[k]
306 
307         if isinstance(k, slice):
308             qs = self._chain()
309             if k.start is not None:
310                 start = int(k.start)
311             else:
312                 start = None
313             if k.stop is not None:
314                 stop = int(k.stop)
315             else:
316                 stop = None
317             qs.query.set_limits(start, stop)
318             return list(qs)[::k.step] if k.step else qs
319 
320         qs = self._chain()
321         qs.query.set_limits(k, k + 1)
322         qs._fetch_all()
323         return qs._result_cache[0]
324 
325     def __class_getitem__(cls, *args, **kwargs):
326         return cls
327 
328     def __and__(self, other):
329         self._check_operator_queryset(other, '&')
330         self._merge_sanity_check(other)
331         if isinstance(other, EmptyQuerySet):
332             return other
333         if isinstance(self, EmptyQuerySet):
334             return self
335         combined = self._chain()
336         combined._merge_known_related_objects(other)
337         combined.query.combine(other.query, sql.AND)
338         return combined
339 
340     def __or__(self, other):
341         self._check_operator_queryset(other, '|')
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         iterable = self._iterable_class(
361             self,
362             chunked_fetch=use_chunked_fetch,
363             chunk_size=chunk_size or 2000,
364         )
365         if not self._prefetch_related_lookups or chunk_size is None:
366             yield from iterable
367             return
368 
369         iterator = iter(iterable)
370         while results := list(islice(iterator, chunk_size)):
371             prefetch_related_objects(results, *self._prefetch_related_lookups)
372             yield from results
373 
374     def iterator(self, chunk_size=None):
375         """
376         An iterator over the results from applying this QuerySet to the
377         database. chunk_size must be provided for QuerySets that prefetch
378         related objects. Otherwise, a default chunk_size of 2000 is supplied.
379         """
380         if chunk_size is None:
381             if self._prefetch_related_lookups:
382                 # When the deprecation ends, replace with:
383                 # raise ValueError(
384                 #     'chunk_size must be provided when using '
385                 #     'QuerySet.iterator() after prefetch_related().'
386                 # )
387                 warnings.warn(
388                     'Using QuerySet.iterator() after prefetch_related() '
389                     'without specifying chunk_size is deprecated.',
390                     category=RemovedInDjango50Warning,
391                     stacklevel=2,
392                 )
393         elif chunk_size <= 0:
394             raise ValueError('Chunk size must be strictly positive.')
395         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
396         return self._iterator(use_chunked_fetch, chunk_size)
397 
398     def aggregate(self, *args, **kwargs):
399         """
400         Return a dictionary containing the calculations (aggregation)
401         over the current queryset.
402 
403         If args is present the expression is passed as a kwarg using
404         the Aggregate object's default alias.
405         """
406         if self.query.distinct_fields:
407             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
408         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
409         for arg in args:
410             # The default_alias property raises TypeError if default_alias
411             # can't be set automatically or AttributeError if it isn't an
412             # attribute.
413             try:
414                 arg.default_alias
415             except (AttributeError, TypeError):
416                 raise TypeError("Complex aggregates require an alias")
417             kwargs[arg.default_alias] = arg
418 
419         query = self.query.chain()
420         for (alias, aggregate_expr) in kwargs.items():
421             query.add_annotation(aggregate_expr, alias, is_summary=True)
422             annotation = query.annotations[alias]
423             if not annotation.contains_aggregate:
424                 raise TypeError("%s is not an aggregate expression" % alias)
425             for expr in annotation.get_source_expressions():
426                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
427                     name = expr.refs
428                     raise exceptions.FieldError(
429                         "Cannot compute %s('%s'): '%s' is an aggregate"
430                         % (annotation.name, name, name)
431                     )
432         return query.get_aggregation(self.db, kwargs)
433 
434     def count(self):
435         """
436         Perform a SELECT COUNT() and return the number of records as an
437         integer.
438 
439         If the QuerySet is already fully cached, return the length of the
440         cached results set to avoid multiple SELECT COUNT(*) calls.
441         """
442         if self._result_cache is not None:
443             return len(self._result_cache)
444 
445         return self.query.get_count(using=self.db)
446 
447     def get(self, *args, **kwargs):
448         """
449         Perform the query and return a single object matching the given
450         keyword arguments.
451         """
452         if self.query.combinator and (args or kwargs):
453             raise NotSupportedError(
454                 'Calling QuerySet.get(...) with filters after %s() is not '
455                 'supported.' % self.query.combinator
456             )
457         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
458         if self.query.can_filter() and not self.query.distinct_fields:
459             clone = clone.order_by()
460         limit = None
461         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
462             limit = MAX_GET_RESULTS
463             clone.query.set_limits(high=limit)
464         num = len(clone)
465         if num == 1:
466             return clone._result_cache[0]
467         if not num:
468             raise self.model.DoesNotExist(
469                 "%s matching query does not exist." %
470                 self.model._meta.object_name
471             )
472         raise self.model.MultipleObjectsReturned(
473             'get() returned more than one %s -- it returned %s!' % (
474                 self.model._meta.object_name,
475                 num if not limit or num < limit else 'more than %s' % (limit - 1),
476             )
477         )
478 
479     def create(self, **kwargs):
480         """
481         Create a new object with the given kwargs, saving it to the database
482         and returning the created object.
483         """
484         obj = self.model(**kwargs)
485         self._for_write = True
486         obj.save(force_insert=True, using=self.db)
487         return obj
488 
489     def _prepare_for_bulk_create(self, objs):
490         for obj in objs:
491             if obj.pk is None:
492                 # Populate new PK values.
493                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
494             obj._prepare_related_fields_for_save(operation_name='bulk_create')
495 
496     def _check_bulk_create_options(self, ignore_conflicts, update_conflicts, update_fields, unique_fields):
497         if ignore_conflicts and update_conflicts:
498             raise ValueError(
499                 'ignore_conflicts and update_conflicts are mutually exclusive.'
500             )
501         db_features = connections[self.db].features
502         if ignore_conflicts:
503             if not db_features.supports_ignore_conflicts:
504                 raise NotSupportedError(
505                     'This database backend does not support ignoring conflicts.'
506                 )
507             return OnConflict.IGNORE
508         elif update_conflicts:
509             if not db_features.supports_update_conflicts:
510                 raise NotSupportedError(
511                     'This database backend does not support updating conflicts.'
512                 )
513             if not update_fields:
514                 raise ValueError(
515                     'Fields that will be updated when a row insertion fails '
516                     'on conflicts must be provided.'
517                 )
518             if unique_fields and not db_features.supports_update_conflicts_with_target:
519                 raise NotSupportedError(
520                     'This database backend does not support updating '
521                     'conflicts with specifying unique fields that can trigger '
522                     'the upsert.'
523                 )
524             if not unique_fields and db_features.supports_update_conflicts_with_target:
525                 raise ValueError(
526                     'Unique fields that can trigger the upsert must be '
527                     'provided.'
528                 )
529             # Updating primary keys and non-concrete fields is forbidden.
530             update_fields = [self.model._meta.get_field(name) for name in update_fields]
531             if any(not f.concrete or f.many_to_many for f in update_fields):
532                 raise ValueError(
533                     'bulk_create() can only be used with concrete fields in '
534                     'update_fields.'
535                 )
536             if any(f.primary_key for f in update_fields):
537                 raise ValueError(
538                     'bulk_create() cannot be used with primary keys in '
539                     'update_fields.'
540                 )
541             if unique_fields:
542                 # Primary key is allowed in unique_fields.
543                 unique_fields = [
544                     self.model._meta.get_field(name)
545                     for name in unique_fields if name != 'pk'
546                 ]
547                 if any(not f.concrete or f.many_to_many for f in unique_fields):
548                     raise ValueError(
549                         'bulk_create() can only be used with concrete fields '
550                         'in unique_fields.'
551                     )
552             return OnConflict.UPDATE
553         return None
554 
555     def bulk_create(
556         self, objs, batch_size=None, ignore_conflicts=False,
557         update_conflicts=False, update_fields=None, unique_fields=None,
558     ):
559         """
560         Insert each of the instances into the database. Do *not* call
561         save() on each of the instances, do not send any pre/post_save
562         signals, and do not set the primary key attribute if it is an
563         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
564         Multi-table models are not supported.
565         """
566         # When you bulk insert you don't get the primary keys back (if it's an
567         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
568         # you can't insert into the child tables which references this. There
569         # are two workarounds:
570         # 1) This could be implemented if you didn't have an autoincrement pk
571         # 2) You could do it by doing O(n) normal inserts into the parent
572         #    tables to get the primary keys back and then doing a single bulk
573         #    insert into the childmost table.
574         # We currently set the primary keys on the objects when using
575         # PostgreSQL via the RETURNING ID clause. It should be possible for
576         # Oracle as well, but the semantics for extracting the primary keys is
577         # trickier so it's not done yet.
578         if batch_size is not None and batch_size <= 0:
579             raise ValueError('Batch size must be a positive integer.')
580         # Check that the parents share the same concrete model with the our
581         # model to detect the inheritance pattern ConcreteGrandParent ->
582         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
583         # would not identify that case as involving multiple tables.
584         for parent in self.model._meta.get_parent_list():
585             if parent._meta.concrete_model is not self.model._meta.concrete_model:
586                 raise ValueError("Can't bulk create a multi-table inherited model")
587         if not objs:
588             return objs
589         on_conflict = self._check_bulk_create_options(
590             ignore_conflicts,
591             update_conflicts,
592             update_fields,
593             unique_fields,
594         )
595         self._for_write = True
596         opts = self.model._meta
597         fields = opts.concrete_fields
598         objs = list(objs)
599         self._prepare_for_bulk_create(objs)
600         with transaction.atomic(using=self.db, savepoint=False):
601             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
602             if objs_with_pk:
603                 returned_columns = self._batched_insert(
604                     objs_with_pk,
605                     fields,
606                     batch_size,
607                     on_conflict=on_conflict,
608                     update_fields=update_fields,
609                     unique_fields=unique_fields,
610                 )
611                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
612                     for result, field in zip(results, opts.db_returning_fields):
613                         if field != opts.pk:
614                             setattr(obj_with_pk, field.attname, result)
615                 for obj_with_pk in objs_with_pk:
616                     obj_with_pk._state.adding = False
617                     obj_with_pk._state.db = self.db
618             if objs_without_pk:
619                 fields = [f for f in fields if not isinstance(f, AutoField)]
620                 returned_columns = self._batched_insert(
621                     objs_without_pk,
622                     fields,
623                     batch_size,
624                     on_conflict=on_conflict,
625                     update_fields=update_fields,
626                     unique_fields=unique_fields,
627                 )
628                 connection = connections[self.db]
629                 if connection.features.can_return_rows_from_bulk_insert and on_conflict is None:
630                     assert len(returned_columns) == len(objs_without_pk)
631                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
632                     for result, field in zip(results, opts.db_returning_fields):
633                         setattr(obj_without_pk, field.attname, result)
634                     obj_without_pk._state.adding = False
635                     obj_without_pk._state.db = self.db
636 
637         return objs
638 
639     def bulk_update(self, objs, fields, batch_size=None):
640         """
641         Update the given fields in each of the given objects in the database.
642         """
643         if batch_size is not None and batch_size < 0:
644             raise ValueError('Batch size must be a positive integer.')
645         if not fields:
646             raise ValueError('Field names must be given to bulk_update().')
647         objs = tuple(objs)
648         if any(obj.pk is None for obj in objs):
649             raise ValueError('All bulk_update() objects must have a primary key set.')
650         fields = [self.model._meta.get_field(name) for name in fields]
651         if any(not f.concrete or f.many_to_many for f in fields):
652             raise ValueError('bulk_update() can only be used with concrete fields.')
653         if any(f.primary_key for f in fields):
654             raise ValueError('bulk_update() cannot be used with primary key fields.')
655         if not objs:
656             return 0
657         for obj in objs:
658             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)
659         # PK is used twice in the resulting update query, once in the filter
660         # and once in the WHEN. Each field will also have one CAST.
661         connection = connections[self.db]
662         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
663         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
664         requires_casting = connection.features.requires_casted_case_in_updates
665         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
666         updates = []
667         for batch_objs in batches:
668             update_kwargs = {}
669             for field in fields:
670                 when_statements = []
671                 for obj in batch_objs:
672                     attr = getattr(obj, field.attname)
673                     if not hasattr(attr, 'resolve_expression'):
674                         attr = Value(attr, output_field=field)
675                     when_statements.append(When(pk=obj.pk, then=attr))
676                 case_statement = Case(*when_statements, output_field=field)
677                 if requires_casting:
678                     case_statement = Cast(case_statement, output_field=field)
679                 update_kwargs[field.attname] = case_statement
680             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
681         rows_updated = 0
682         with transaction.atomic(using=self.db, savepoint=False):
683             for pks, update_kwargs in updates:
684                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
685         return rows_updated
686     bulk_update.alters_data = True
687 
688     def get_or_create(self, defaults=None, **kwargs):
689         """
690         Look up an object with the given kwargs, creating one if necessary.
691         Return a tuple of (object, created), where created is a boolean
692         specifying whether an object was created.
693         """
694         # The get() needs to be targeted at the write database in order
695         # to avoid potential transaction consistency problems.
696         self._for_write = True
697         try:
698             return self.get(**kwargs), False
699         except self.model.DoesNotExist:
700             params = self._extract_model_params(defaults, **kwargs)
701             # Try to create an object using passed params.
702             try:
703                 with transaction.atomic(using=self.db):
704                     params = dict(resolve_callables(params))
705                     return self.create(**params), True
706             except IntegrityError:
707                 try:
708                     return self.get(**kwargs), False
709                 except self.model.DoesNotExist:
710                     pass
711                 raise
712 
713     def update_or_create(self, defaults=None, **kwargs):
714         """
715         Look up an object with the given kwargs, updating one with defaults
716         if it exists, otherwise create a new one.
717         Return a tuple (object, created), where created is a boolean
718         specifying whether an object was created.
719         """
720         defaults = defaults or {}
721         self._for_write = True
722         with transaction.atomic(using=self.db):
723             # Lock the row so that a concurrent update is blocked until
724             # update_or_create() has performed its save.
725             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
726             if created:
727                 return obj, created
728             for k, v in resolve_callables(defaults):
729                 setattr(obj, k, v)
730             obj.save(using=self.db)
731         return obj, False
732 
733     def _extract_model_params(self, defaults, **kwargs):
734         """
735         Prepare `params` for creating a model instance based on the given
736         kwargs; for use by get_or_create().
737         """
738         defaults = defaults or {}
739         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
740         params.update(defaults)
741         property_names = self.model._meta._property_names
742         invalid_params = []
743         for param in params:
744             try:
745                 self.model._meta.get_field(param)
746             except exceptions.FieldDoesNotExist:
747                 # It's okay to use a model's property if it has a setter.
748                 if not (param in property_names and getattr(self.model, param).fset):
749                     invalid_params.append(param)
750         if invalid_params:
751             raise exceptions.FieldError(
752                 "Invalid field name(s) for model %s: '%s'." % (
753                     self.model._meta.object_name,
754                     "', '".join(sorted(invalid_params)),
755                 ))
756         return params
757 
758     def _earliest(self, *fields):
759         """
760         Return the earliest object according to fields (if given) or by the
761         model's Meta.get_latest_by.
762         """
763         if fields:
764             order_by = fields
765         else:
766             order_by = getattr(self.model._meta, 'get_latest_by')
767             if order_by and not isinstance(order_by, (tuple, list)):
768                 order_by = (order_by,)
769         if order_by is None:
770             raise ValueError(
771                 "earliest() and latest() require either fields as positional "
772                 "arguments or 'get_latest_by' in the model's Meta."
773             )
774         obj = self._chain()
775         obj.query.set_limits(high=1)
776         obj.query.clear_ordering(force=True)
777         obj.query.add_ordering(*order_by)
778         return obj.get()
779 
780     def earliest(self, *fields):
781         if self.query.is_sliced:
782             raise TypeError('Cannot change a query once a slice has been taken.')
783         return self._earliest(*fields)
784 
785     def latest(self, *fields):
786         if self.query.is_sliced:
787             raise TypeError('Cannot change a query once a slice has been taken.')
788         return self.reverse()._earliest(*fields)
789 
790     def first(self):
791         """Return the first object of a query or None if no match is found."""
792         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
793             return obj
794 
795     def last(self):
796         """Return the last object of a query or None if no match is found."""
797         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
798             return obj
799 
800     def in_bulk(self, id_list=None, *, field_name='pk'):
801         """
802         Return a dictionary mapping each of the given IDs to the object with
803         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
804         """
805         if self.query.is_sliced:
806             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
807         opts = self.model._meta
808         unique_fields = [
809             constraint.fields[0]
810             for constraint in opts.total_unique_constraints
811             if len(constraint.fields) == 1
812         ]
813         if (
814             field_name != 'pk' and
815             not opts.get_field(field_name).unique and
816             field_name not in unique_fields and
817             self.query.distinct_fields != (field_name,)
818         ):
819             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
820         if id_list is not None:
821             if not id_list:
822                 return {}
823             filter_key = '{}__in'.format(field_name)
824             batch_size = connections[self.db].features.max_query_params
825             id_list = tuple(id_list)
826             # If the database has a limit on the number of query parameters
827             # (e.g. SQLite), retrieve objects in batches if necessary.
828             if batch_size and batch_size < len(id_list):
829                 qs = ()
830                 for offset in range(0, len(id_list), batch_size):
831                     batch = id_list[offset:offset + batch_size]
832                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
833             else:
834                 qs = self.filter(**{filter_key: id_list}).order_by()
835         else:
836             qs = self._chain()
837         return {getattr(obj, field_name): obj for obj in qs}
838 
839     def delete(self):
840         """Delete the records in the current QuerySet."""
841         self._not_support_combined_queries('delete')
842         if self.query.is_sliced:
843             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
844         if self.query.distinct or self.query.distinct_fields:
845             raise TypeError('Cannot call delete() after .distinct().')
846         if self._fields is not None:
847             raise TypeError("Cannot call delete() after .values() or .values_list()")
848 
849         del_query = self._chain()
850 
851         # The delete is actually 2 queries - one to find related objects,
852         # and one to delete. Make sure that the discovery of related
853         # objects is performed on the same database as the deletion.
854         del_query._for_write = True
855 
856         # Disable non-supported fields.
857         del_query.query.select_for_update = False
858         del_query.query.select_related = False
859         del_query.query.clear_ordering(force=True)
860 
861         collector = Collector(using=del_query.db, origin=self)
862         collector.collect(del_query)
863         deleted, _rows_count = collector.delete()
864 
865         # Clear the result cache, in case this QuerySet gets reused.
866         self._result_cache = None
867         return deleted, _rows_count
868 
869     delete.alters_data = True
870     delete.queryset_only = True
871 
872     def _raw_delete(self, using):
873         """
874         Delete objects found from the given queryset in single direct SQL
875         query. No signals are sent and there is no protection for cascades.
876         """
877         query = self.query.clone()
878         query.__class__ = sql.DeleteQuery
879         cursor = query.get_compiler(using).execute_sql(CURSOR)
880         if cursor:
881             with cursor:
882                 return cursor.rowcount
883         return 0
884     _raw_delete.alters_data = True
885 
886     def update(self, **kwargs):
887         """
888         Update all elements in the current QuerySet, setting all the given
889         fields to the appropriate values.
890         """
891         self._not_support_combined_queries('update')
892         if self.query.is_sliced:
893             raise TypeError('Cannot update a query once a slice has been taken.')
894         self._for_write = True
895         query = self.query.chain(sql.UpdateQuery)
896         query.add_update_values(kwargs)
897         # Clear any annotations so that they won't be present in subqueries.
898         query.annotations = {}
899         with transaction.mark_for_rollback_on_error(using=self.db):
900             rows = query.get_compiler(self.db).execute_sql(CURSOR)
901         self._result_cache = None
902         return rows
903     update.alters_data = True
904 
905     def _update(self, values):
906         """
907         A version of update() that accepts field objects instead of field names.
908         Used primarily for model saving and not intended for use by general
909         code (it requires too much poking around at model internals to be
910         useful at that level).
911         """
912         if self.query.is_sliced:
913             raise TypeError('Cannot update a query once a slice has been taken.')
914         query = self.query.chain(sql.UpdateQuery)
915         query.add_update_fields(values)
916         # Clear any annotations so that they won't be present in subqueries.
917         query.annotations = {}
918         self._result_cache = None
919         return query.get_compiler(self.db).execute_sql(CURSOR)
920     _update.alters_data = True
921     _update.queryset_only = False
922 
923     def exists(self):
924         if self._result_cache is None:
925             return self.query.has_results(using=self.db)
926         return bool(self._result_cache)
927 
928     def contains(self, obj):
929         """Return True if the queryset contains an object."""
930         self._not_support_combined_queries('contains')
931         if self._fields is not None:
932             raise TypeError(
933                 'Cannot call QuerySet.contains() after .values() or '
934                 '.values_list().'
935             )
936         try:
937             if obj._meta.concrete_model != self.model._meta.concrete_model:
938                 return False
939         except AttributeError:
940             raise TypeError("'obj' must be a model instance.")
941         if obj.pk is None:
942             raise ValueError(
943                 'QuerySet.contains() cannot be used on unsaved objects.'
944             )
945         if self._result_cache is not None:
946             return obj in self._result_cache
947         return self.filter(pk=obj.pk).exists()
948 
949     def _prefetch_related_objects(self):
950         # This method can only be called once the result cache has been filled.
951         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
952         self._prefetch_done = True
953 
954     def explain(self, *, format=None, **options):
955         return self.query.explain(using=self.db, format=format, **options)
956 
957     ##################################################
958     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
959     ##################################################
960 
961     def raw(self, raw_query, params=(), translations=None, using=None):
962         if using is None:
963             using = self.db
964         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
965         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
966         return qs
967 
968     def _values(self, *fields, **expressions):
969         clone = self._chain()
970         if expressions:
971             clone = clone.annotate(**expressions)
972         clone._fields = fields
973         clone.query.set_values(fields)
974         return clone
975 
976     def values(self, *fields, **expressions):
977         fields += tuple(expressions)
978         clone = self._values(*fields, **expressions)
979         clone._iterable_class = ValuesIterable
980         return clone
981 
982     def values_list(self, *fields, flat=False, named=False):
983         if flat and named:
984             raise TypeError("'flat' and 'named' can't be used together.")
985         if flat and len(fields) > 1:
986             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
987 
988         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
989         _fields = []
990         expressions = {}
991         counter = 1
992         for field in fields:
993             if hasattr(field, 'resolve_expression'):
994                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
995                 while True:
996                     field_id = field_id_prefix + str(counter)
997                     counter += 1
998                     if field_id not in field_names:
999                         break
1000                 expressions[field_id] = field
1001                 _fields.append(field_id)
1002             else:
1003                 _fields.append(field)
1004 
1005         clone = self._values(*_fields, **expressions)
1006         clone._iterable_class = (
1007             NamedValuesListIterable if named
1008             else FlatValuesListIterable if flat
1009             else ValuesListIterable
1010         )
1011         return clone
1012 
1013     def dates(self, field_name, kind, order='ASC'):
1014         """
1015         Return a list of date objects representing all available dates for
1016         the given field_name, scoped to 'kind'.
1017         """
1018         if kind not in ('year', 'month', 'week', 'day'):
1019             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
1020         if order not in ('ASC', 'DESC'):
1021             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1022         return self.annotate(
1023             datefield=Trunc(field_name, kind, output_field=DateField()),
1024             plain_field=F(field_name)
1025         ).values_list(
1026             'datefield', flat=True
1027         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
1028 
1029     # RemovedInDjango50Warning: when the deprecation ends, remove is_dst
1030     # argument.
1031     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=timezone.NOT_PASSED):
1032         """
1033         Return a list of datetime objects representing all available
1034         datetimes for the given field_name, scoped to 'kind'.
1035         """
1036         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
1037             raise ValueError(
1038                 "'kind' must be one of 'year', 'month', 'week', 'day', "
1039                 "'hour', 'minute', or 'second'."
1040             )
1041         if order not in ('ASC', 'DESC'):
1042             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
1043         if settings.USE_TZ:
1044             if tzinfo is None:
1045                 tzinfo = timezone.get_current_timezone()
1046         else:
1047             tzinfo = None
1048         return self.annotate(
1049             datetimefield=Trunc(
1050                 field_name,
1051                 kind,
1052                 output_field=DateTimeField(),
1053                 tzinfo=tzinfo,
1054                 is_dst=is_dst,
1055             ),
1056             plain_field=F(field_name)
1057         ).values_list(
1058             'datetimefield', flat=True
1059         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
1060 
1061     def none(self):
1062         """Return an empty QuerySet."""
1063         clone = self._chain()
1064         clone.query.set_empty()
1065         return clone
1066 
1067     ##################################################################
1068     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
1069     ##################################################################
1070 
1071     def all(self):
1072         """
1073         Return a new QuerySet that is a copy of the current one. This allows a
1074         QuerySet to proxy for a model manager in some cases.
1075         """
1076         return self._chain()
1077 
1078     def filter(self, *args, **kwargs):
1079         """
1080         Return a new QuerySet instance with the args ANDed to the existing
1081         set.
1082         """
1083         self._not_support_combined_queries('filter')
1084         return self._filter_or_exclude(False, args, kwargs)
1085 
1086     def exclude(self, *args, **kwargs):
1087         """
1088         Return a new QuerySet instance with NOT (args) ANDed to the existing
1089         set.
1090         """
1091         self._not_support_combined_queries('exclude')
1092         return self._filter_or_exclude(True, args, kwargs)
1093 
1094     def _filter_or_exclude(self, negate, args, kwargs):
1095         if (args or kwargs) and self.query.is_sliced:
1096             raise TypeError('Cannot filter a query once a slice has been taken.')
1097         clone = self._chain()
1098         if self._defer_next_filter:
1099             self._defer_next_filter = False
1100             clone._deferred_filter = negate, args, kwargs
1101         else:
1102             clone._filter_or_exclude_inplace(negate, args, kwargs)
1103         return clone
1104 
1105     def _filter_or_exclude_inplace(self, negate, args, kwargs):
1106         if negate:
1107             self._query.add_q(~Q(*args, **kwargs))
1108         else:
1109             self._query.add_q(Q(*args, **kwargs))
1110 
1111     def complex_filter(self, filter_obj):
1112         """
1113         Return a new QuerySet instance with filter_obj added to the filters.
1114 
1115         filter_obj can be a Q object or a dictionary of keyword lookup
1116         arguments.
1117 
1118         This exists to support framework features such as 'limit_choices_to',
1119         and usually it will be more natural to use other methods.
1120         """
1121         if isinstance(filter_obj, Q):
1122             clone = self._chain()
1123             clone.query.add_q(filter_obj)
1124             return clone
1125         else:
1126             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1127 
1128     def _combinator_query(self, combinator, *other_qs, all=False):
1129         # Clone the query to inherit the select list and everything
1130         clone = self._chain()
1131         # Clear limits and ordering so they can be reapplied
1132         clone.query.clear_ordering(force=True)
1133         clone.query.clear_limits()
1134         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1135         clone.query.combinator = combinator
1136         clone.query.combinator_all = all
1137         return clone
1138 
1139     def union(self, *other_qs, all=False):
1140         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1141         if isinstance(self, EmptyQuerySet):
1142             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1143             if not qs:
1144                 return self
1145             if len(qs) == 1:
1146                 return qs[0]
1147             return qs[0]._combinator_query('union', *qs[1:], all=all)
1148         return self._combinator_query('union', *other_qs, all=all)
1149 
1150     def intersection(self, *other_qs):
1151         # If any query is an EmptyQuerySet, return it.
1152         if isinstance(self, EmptyQuerySet):
1153             return self
1154         for other in other_qs:
1155             if isinstance(other, EmptyQuerySet):
1156                 return other
1157         return self._combinator_query('intersection', *other_qs)
1158 
1159     def difference(self, *other_qs):
1160         # If the query is an EmptyQuerySet, return it.
1161         if isinstance(self, EmptyQuerySet):
1162             return self
1163         return self._combinator_query('difference', *other_qs)
1164 
1165     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1166         """
1167         Return a new QuerySet instance that will select objects with a
1168         FOR UPDATE lock.
1169         """
1170         if nowait and skip_locked:
1171             raise ValueError('The nowait option cannot be used with skip_locked.')
1172         obj = self._chain()
1173         obj._for_write = True
1174         obj.query.select_for_update = True
1175         obj.query.select_for_update_nowait = nowait
1176         obj.query.select_for_update_skip_locked = skip_locked
1177         obj.query.select_for_update_of = of
1178         obj.query.select_for_no_key_update = no_key
1179         return obj
1180 
1181     def select_related(self, *fields):
1182         """
1183         Return a new QuerySet instance that will select related objects.
1184 
1185         If fields are specified, they must be ForeignKey fields and only those
1186         related objects are included in the selection.
1187 
1188         If select_related(None) is called, clear the list.
1189         """
1190         self._not_support_combined_queries('select_related')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1193 
1194         obj = self._chain()
1195         if fields == (None,):
1196             obj.query.select_related = False
1197         elif fields:
1198             obj.query.add_select_related(fields)
1199         else:
1200             obj.query.select_related = True
1201         return obj
1202 
1203     def prefetch_related(self, *lookups):
1204         """
1205         Return a new QuerySet instance that will prefetch the specified
1206         Many-To-One and Many-To-Many related objects when the QuerySet is
1207         evaluated.
1208 
1209         When prefetch_related() is called more than once, append to the list of
1210         prefetch lookups. If prefetch_related(None) is called, clear the list.
1211         """
1212         self._not_support_combined_queries('prefetch_related')
1213         clone = self._chain()
1214         if lookups == (None,):
1215             clone._prefetch_related_lookups = ()
1216         else:
1217             for lookup in lookups:
1218                 if isinstance(lookup, Prefetch):
1219                     lookup = lookup.prefetch_to
1220                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1221                 if lookup in self.query._filtered_relations:
1222                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1223             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1224         return clone
1225 
1226     def annotate(self, *args, **kwargs):
1227         """
1228         Return a query set in which the returned objects have been annotated
1229         with extra data or aggregations.
1230         """
1231         self._not_support_combined_queries('annotate')
1232         return self._annotate(args, kwargs, select=True)
1233 
1234     def alias(self, *args, **kwargs):
1235         """
1236         Return a query set with added aliases for extra data or aggregations.
1237         """
1238         self._not_support_combined_queries('alias')
1239         return self._annotate(args, kwargs, select=False)
1240 
1241     def _annotate(self, args, kwargs, select=True):
1242         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1243         annotations = {}
1244         for arg in args:
1245             # The default_alias property may raise a TypeError.
1246             try:
1247                 if arg.default_alias in kwargs:
1248                     raise ValueError("The named annotation '%s' conflicts with the "
1249                                      "default name for another annotation."
1250                                      % arg.default_alias)
1251             except TypeError:
1252                 raise TypeError("Complex annotations require an alias")
1253             annotations[arg.default_alias] = arg
1254         annotations.update(kwargs)
1255 
1256         clone = self._chain()
1257         names = self._fields
1258         if names is None:
1259             names = set(chain.from_iterable(
1260                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1261                 for field in self.model._meta.get_fields()
1262             ))
1263 
1264         for alias, annotation in annotations.items():
1265             if alias in names:
1266                 raise ValueError("The annotation '%s' conflicts with a field on "
1267                                  "the model." % alias)
1268             if isinstance(annotation, FilteredRelation):
1269                 clone.query.add_filtered_relation(annotation, alias)
1270             else:
1271                 clone.query.add_annotation(
1272                     annotation, alias, is_summary=False, select=select,
1273                 )
1274         for alias, annotation in clone.query.annotations.items():
1275             if alias in annotations and annotation.contains_aggregate:
1276                 if clone._fields is None:
1277                     clone.query.group_by = True
1278                 else:
1279                     clone.query.set_group_by()
1280                 break
1281 
1282         return clone
1283 
1284     def order_by(self, *field_names):
1285         """Return a new QuerySet instance with the ordering changed."""
1286         if self.query.is_sliced:
1287             raise TypeError('Cannot reorder a query once a slice has been taken.')
1288         obj = self._chain()
1289         obj.query.clear_ordering(force=True, clear_default=False)
1290         obj.query.add_ordering(*field_names)
1291         return obj
1292 
1293     def distinct(self, *field_names):
1294         """
1295         Return a new QuerySet instance that will select only distinct results.
1296         """
1297         self._not_support_combined_queries('distinct')
1298         if self.query.is_sliced:
1299             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1300         obj = self._chain()
1301         obj.query.add_distinct_fields(*field_names)
1302         return obj
1303 
1304     def extra(self, select=None, where=None, params=None, tables=None,
1305               order_by=None, select_params=None):
1306         """Add extra SQL fragments to the query."""
1307         self._not_support_combined_queries('extra')
1308         if self.query.is_sliced:
1309             raise TypeError('Cannot change a query once a slice has been taken.')
1310         clone = self._chain()
1311         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1312         return clone
1313 
1314     def reverse(self):
1315         """Reverse the ordering of the QuerySet."""
1316         if self.query.is_sliced:
1317             raise TypeError('Cannot reverse a query once a slice has been taken.')
1318         clone = self._chain()
1319         clone.query.standard_ordering = not clone.query.standard_ordering
1320         return clone
1321 
1322     def defer(self, *fields):
1323         """
1324         Defer the loading of data for certain fields until they are accessed.
1325         Add the set of deferred fields to any existing set of deferred fields.
1326         The only exception to this is if None is passed in as the only
1327         parameter, in which case removal all deferrals.
1328         """
1329         self._not_support_combined_queries('defer')
1330         if self._fields is not None:
1331             raise TypeError("Cannot call defer() after .values() or .values_list()")
1332         clone = self._chain()
1333         if fields == (None,):
1334             clone.query.clear_deferred_loading()
1335         else:
1336             clone.query.add_deferred_loading(fields)
1337         return clone
1338 
1339     def only(self, *fields):
1340         """
1341         Essentially, the opposite of defer(). Only the fields passed into this
1342         method and that are not already specified as deferred are loaded
1343         immediately when the queryset is evaluated.
1344         """
1345         self._not_support_combined_queries('only')
1346         if self._fields is not None:
1347             raise TypeError("Cannot call only() after .values() or .values_list()")
1348         if fields == (None,):
1349             # Can only pass None to defer(), not only(), as the rest option.
1350             # That won't stop people trying to do this, so let's be explicit.
1351             raise TypeError("Cannot pass None as an argument to only().")
1352         for field in fields:
1353             field = field.split(LOOKUP_SEP, 1)[0]
1354             if field in self.query._filtered_relations:
1355                 raise ValueError('only() is not supported with FilteredRelation.')
1356         clone = self._chain()
1357         clone.query.add_immediate_loading(fields)
1358         return clone
1359 
1360     def using(self, alias):
1361         """Select which database this QuerySet should execute against."""
1362         clone = self._chain()
1363         clone._db = alias
1364         return clone
1365 
1366     ###################################
1367     # PUBLIC INTROSPECTION ATTRIBUTES #
1368     ###################################
1369 
1370     @property
1371     def ordered(self):
1372         """
1373         Return True if the QuerySet is ordered -- i.e. has an order_by()
1374         clause or a default ordering on the model (or is empty).
1375         """
1376         if isinstance(self, EmptyQuerySet):
1377             return True
1378         if self.query.extra_order_by or self.query.order_by:
1379             return True
1380         elif (
1381             self.query.default_ordering and
1382             self.query.get_meta().ordering and
1383             # A default ordering doesn't affect GROUP BY queries.
1384             not self.query.group_by
1385         ):
1386             return True
1387         else:
1388             return False
1389 
1390     @property
1391     def db(self):
1392         """Return the database used if this query is executed now."""
1393         if self._for_write:
1394             return self._db or router.db_for_write(self.model, **self._hints)
1395         return self._db or router.db_for_read(self.model, **self._hints)
1396 
1397     ###################
1398     # PRIVATE METHODS #
1399     ###################
1400 
1401     def _insert(
1402         self, objs, fields, returning_fields=None, raw=False, using=None,
1403         on_conflict=None, update_fields=None, unique_fields=None,
1404     ):
1405         """
1406         Insert a new record for the given model. This provides an interface to
1407         the InsertQuery class and is how Model.save() is implemented.
1408         """
1409         self._for_write = True
1410         if using is None:
1411             using = self.db
1412         query = sql.InsertQuery(
1413             self.model,
1414             on_conflict=on_conflict,
1415             update_fields=update_fields,
1416             unique_fields=unique_fields,
1417         )
1418         query.insert_values(fields, objs, raw=raw)
1419         return query.get_compiler(using=using).execute_sql(returning_fields)
1420     _insert.alters_data = True
1421     _insert.queryset_only = False
1422 
1423     def _batched_insert(
1424         self, objs, fields, batch_size, on_conflict=None, update_fields=None,
1425         unique_fields=None,
1426     ):
1427         """
1428         Helper method for bulk_create() to insert objs one batch at a time.
1429         """
1430         connection = connections[self.db]
1431         ops = connection.ops
1432         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1433         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1434         inserted_rows = []
1435         bulk_return = connection.features.can_return_rows_from_bulk_insert
1436         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1437             if bulk_return and on_conflict is None:
1438                 inserted_rows.extend(self._insert(
1439                     item, fields=fields, using=self.db,
1440                     returning_fields=self.model._meta.db_returning_fields,
1441                 ))
1442             else:
1443                 self._insert(
1444                     item,
1445                     fields=fields,
1446                     using=self.db,
1447                     on_conflict=on_conflict,
1448                     update_fields=update_fields,
1449                     unique_fields=unique_fields,
1450                 )
1451         return inserted_rows
1452 
1453     def _chain(self):
1454         """
1455         Return a copy of the current QuerySet that's ready for another
1456         operation.
1457         """
1458         obj = self._clone()
1459         if obj._sticky_filter:
1460             obj.query.filter_is_sticky = True
1461             obj._sticky_filter = False
1462         return obj
1463 
1464     def _clone(self):
1465         """
1466         Return a copy of the current QuerySet. A lightweight alternative
1467         to deepcopy().
1468         """
1469         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1470         c._sticky_filter = self._sticky_filter
1471         c._for_write = self._for_write
1472         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1473         c._known_related_objects = self._known_related_objects
1474         c._iterable_class = self._iterable_class
1475         c._fields = self._fields
1476         return c
1477 
1478     def _fetch_all(self):
1479         if self._result_cache is None:
1480             self._result_cache = list(self._iterable_class(self))
1481         if self._prefetch_related_lookups and not self._prefetch_done:
1482             self._prefetch_related_objects()
1483 
1484     def _next_is_sticky(self):
1485         """
1486         Indicate that the next filter call and the one following that should
1487         be treated as a single filter. This is only important when it comes to
1488         determining when to reuse tables for many-to-many filters. Required so
1489         that we can filter naturally on the results of related managers.
1490 
1491         This doesn't return a clone of the current QuerySet (it returns
1492         "self"). The method is only used internally and should be immediately
1493         followed by a filter() that does create a clone.
1494         """
1495         self._sticky_filter = True
1496         return self
1497 
1498     def _merge_sanity_check(self, other):
1499         """Check that two QuerySet classes may be merged."""
1500         if self._fields is not None and (
1501                 set(self.query.values_select) != set(other.query.values_select) or
1502                 set(self.query.extra_select) != set(other.query.extra_select) or
1503                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1504             raise TypeError(
1505                 "Merging '%s' classes must involve the same values in each case."
1506                 % self.__class__.__name__
1507             )
1508 
1509     def _merge_known_related_objects(self, other):
1510         """
1511         Keep track of all known related objects from either QuerySet instance.
1512         """
1513         for field, objects in other._known_related_objects.items():
1514             self._known_related_objects.setdefault(field, {}).update(objects)
1515 
1516     def resolve_expression(self, *args, **kwargs):
1517         if self._fields and len(self._fields) > 1:
1518             # values() queryset can only be used as nested queries
1519             # if they are set up to select only a single field.
1520             raise TypeError('Cannot use multi-field values as a filter value.')
1521         query = self.query.resolve_expression(*args, **kwargs)
1522         query._db = self._db
1523         return query
1524     resolve_expression.queryset_only = True
1525 
1526     def _add_hints(self, **hints):
1527         """
1528         Update hinting information for use by routers. Add new key/values or
1529         overwrite existing key/values.
1530         """
1531         self._hints.update(hints)
1532 
1533     def _has_filters(self):
1534         """
1535         Check if this QuerySet has any filtering going on. This isn't
1536         equivalent with checking if all objects are present in results, for
1537         example, qs[1:]._has_filters() -> False.
1538         """
1539         return self.query.has_filters()
1540 
1541     @staticmethod
1542     def _validate_values_are_expressions(values, method_name):
1543         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1544         if invalid_args:
1545             raise TypeError(
1546                 'QuerySet.%s() received non-expression(s): %s.' % (
1547                     method_name,
1548                     ', '.join(invalid_args),
1549                 )
1550             )
1551 
1552     def _not_support_combined_queries(self, operation_name):
1553         if self.query.combinator:
1554             raise NotSupportedError(
1555                 'Calling QuerySet.%s() after %s() is not supported.'
1556                 % (operation_name, self.query.combinator)
1557             )
1558 
1559     def _check_operator_queryset(self, other, operator_):
1560         if self.query.combinator or other.query.combinator:
1561             raise TypeError(f'Cannot use {operator_} operator with combined queryset.')

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>django/db/models/sql/compiler.py</file>
<class>None</class>
<method>None</method>
<code>
1 import collections
2 import json
3 import re
4 from functools import partial
5 from itertools import chain
6 
7 from django.core.exceptions import EmptyResultSet, FieldError
8 from django.db import DatabaseError, NotSupportedError
9 from django.db.models.constants import LOOKUP_SEP
10 from django.db.models.expressions import F, OrderBy, RawSQL, Ref, Value
11 from django.db.models.functions import Cast, Random
12 from django.db.models.query_utils import select_related_descend
13 from django.db.models.sql.constants import (
14     CURSOR, GET_ITERATOR_CHUNK_SIZE, MULTI, NO_RESULTS, ORDER_DIR, SINGLE,
15 )
16 from django.db.models.sql.query import Query, get_order_dir
17 from django.db.transaction import TransactionManagementError
18 from django.utils.functional import cached_property
19 from django.utils.hashable import make_hashable
20 from django.utils.regex_helper import _lazy_re_compile
21 
22 
23 class SQLCompiler:
24     # Multiline ordering SQL clause may appear from RawSQL.
25     ordering_parts = _lazy_re_compile(
26         r'^(.*)\s(?:ASC|DESC).*',
27         re.MULTILINE | re.DOTALL,
28     )
29 
30     def __init__(self, query, connection, using, elide_empty=True):
31         self.query = query
32         self.connection = connection
33         self.using = using
34         # Some queries, e.g. coalesced aggregation, need to be executed even if
35         # they would return an empty result set.
36         self.elide_empty = elide_empty
37         self.quote_cache = {'*': '*'}
38         # The select, klass_info, and annotations are needed by QuerySet.iterator()
39         # these are set as a side-effect of executing the query. Note that we calculate
40         # separately a list of extra select columns needed for grammatical correctness
41         # of the query, but these columns are not included in self.select.
42         self.select = None
43         self.annotation_col_map = None
44         self.klass_info = None
45         self._meta_ordering = None
46 
47     def __repr__(self):
48         return (
49             f'<{self.__class__.__qualname__} '
50             f'model={self.query.model.__qualname__} '
51             f'connection={self.connection!r} using={self.using!r}>'
52         )
53 
54     def setup_query(self):
55         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):
56             self.query.get_initial_alias()
57         self.select, self.klass_info, self.annotation_col_map = self.get_select()
58         self.col_count = len(self.select)
59 
60     def pre_sql_setup(self):
61         """
62         Do any necessary class setup immediately prior to producing SQL. This
63         is for things that can't necessarily be done in __init__ because we
64         might not have all the pieces in place at that time.
65         """
66         self.setup_query()
67         order_by = self.get_order_by()
68         self.where, self.having = self.query.where.split_having()
69         extra_select = self.get_extra_select(order_by, self.select)
70         self.has_extra_select = bool(extra_select)
71         group_by = self.get_group_by(self.select + extra_select, order_by)
72         return extra_select, order_by, group_by
73 
74     def get_group_by(self, select, order_by):
75         """
76         Return a list of 2-tuples of form (sql, params).
77 
78         The logic of what exactly the GROUP BY clause contains is hard
79         to describe in other words than "if it passes the test suite,
80         then it is correct".
81         """
82         # Some examples:
83         #     SomeModel.objects.annotate(Count('somecol'))
84         #     GROUP BY: all fields of the model
85         #
86         #    SomeModel.objects.values('name').annotate(Count('somecol'))
87         #    GROUP BY: name
88         #
89         #    SomeModel.objects.annotate(Count('somecol')).values('name')
90         #    GROUP BY: all cols of the model
91         #
92         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')
93         #    GROUP BY: name, pk
94         #
95         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')
96         #    GROUP BY: name, pk
97         #
98         # In fact, the self.query.group_by is the minimal set to GROUP BY. It
99         # can't be ever restricted to a smaller set, but additional columns in
100         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately
101         # the end result is that it is impossible to force the query to have
102         # a chosen GROUP BY clause - you can almost do this by using the form:
103         #     .values(*wanted_cols).annotate(AnAggregate())
104         # but any later annotations, extra selects, values calls that
105         # refer some column outside of the wanted_cols, order_by, or even
106         # filter calls can alter the GROUP BY clause.
107 
108         # The query.group_by is either None (no GROUP BY at all), True
109         # (group by select fields), or a list of expressions to be added
110         # to the group by.
111         if self.query.group_by is None:
112             return []
113         expressions = []
114         if self.query.group_by is not True:
115             # If the group by is set to a list (by .values() call most likely),
116             # then we need to add everything in it to the GROUP BY clause.
117             # Backwards compatibility hack for setting query.group_by. Remove
118             # when  we have public API way of forcing the GROUP BY clause.
119             # Converts string references to expressions.
120             for expr in self.query.group_by:
121                 if not hasattr(expr, 'as_sql'):
122                     expressions.append(self.query.resolve_ref(expr))
123                 else:
124                     expressions.append(expr)
125         # Note that even if the group_by is set, it is only the minimal
126         # set to group by. So, we need to add cols in select, order_by, and
127         # having into the select in any case.
128         ref_sources = {
129             expr.source for expr in expressions if isinstance(expr, Ref)
130         }
131         for expr, _, _ in select:
132             # Skip members of the select clause that are already included
133             # by reference.
134             if expr in ref_sources:
135                 continue
136             cols = expr.get_group_by_cols()
137             for col in cols:
138                 expressions.append(col)
139         if not self._meta_ordering:
140             for expr, (sql, params, is_ref) in order_by:
141                 # Skip references to the SELECT clause, as all expressions in
142                 # the SELECT clause are already part of the GROUP BY.
143                 if not is_ref:
144                     expressions.extend(expr.get_group_by_cols())
145         having_group_by = self.having.get_group_by_cols() if self.having else ()
146         for expr in having_group_by:
147             expressions.append(expr)
148         result = []
149         seen = set()
150         expressions = self.collapse_group_by(expressions, having_group_by)
151 
152         for expr in expressions:
153             sql, params = self.compile(expr)
154             sql, params = expr.select_format(self, sql, params)
155             params_hash = make_hashable(params)
156             if (sql, params_hash) not in seen:
157                 result.append((sql, params))
158                 seen.add((sql, params_hash))
159         return result
160 
161     def collapse_group_by(self, expressions, having):
162         # If the DB can group by primary key, then group by the primary key of
163         # query's main model. Note that for PostgreSQL the GROUP BY clause must
164         # include the primary key of every table, but for MySQL it is enough to
165         # have the main table's primary key.
166         if self.connection.features.allows_group_by_pk:
167             # Determine if the main model's primary key is in the query.
168             pk = None
169             for expr in expressions:
170                 # Is this a reference to query's base table primary key? If the
171                 # expression isn't a Col-like, then skip the expression.
172                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and
173                         getattr(expr, 'alias', None) == self.query.base_table):
174                     pk = expr
175                     break
176             # If the main model's primary key is in the query, group by that
177             # field, HAVING expressions, and expressions associated with tables
178             # that don't have a primary key included in the grouped columns.
179             if pk:
180                 pk_aliases = {
181                     expr.alias for expr in expressions
182                     if hasattr(expr, 'target') and expr.target.primary_key
183                 }
184                 expressions = [pk] + [
185                     expr for expr in expressions
186                     if expr in having or (
187                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases
188                     )
189                 ]
190         elif self.connection.features.allows_group_by_selected_pks:
191             # Filter out all expressions associated with a table's primary key
192             # present in the grouped columns. This is done by identifying all
193             # tables that have their primary key included in the grouped
194             # columns and removing non-primary key columns referring to them.
195             # Unmanaged models are excluded because they could be representing
196             # database views on which the optimization might not be allowed.
197             pks = {
198                 expr for expr in expressions
199                 if (
200                     hasattr(expr, 'target') and
201                     expr.target.primary_key and
202                     self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
203                 )
204             }
205             aliases = {expr.alias for expr in pks}
206             expressions = [
207                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases
208             ]
209         return expressions
210 
211     def get_select(self):
212         """
213         Return three values:
214         - a list of 3-tuples of (expression, (sql, params), alias)
215         - a klass_info structure,
216         - a dictionary of annotations
217 
218         The (sql, params) is what the expression will produce, and alias is the
219         "AS alias" for the column (possibly None).
220 
221         The klass_info structure contains the following information:
222         - The base model of the query.
223         - Which columns for that model are present in the query (by
224           position of the select clause).
225         - related_klass_infos: [f, klass_info] to descent into
226 
227         The annotations is a dictionary of {'attname': column position} values.
228         """
229         select = []
230         klass_info = None
231         annotations = {}
232         select_idx = 0
233         for alias, (sql, params) in self.query.extra_select.items():
234             annotations[alias] = select_idx
235             select.append((RawSQL(sql, params), alias))
236             select_idx += 1
237         assert not (self.query.select and self.query.default_cols)
238         if self.query.default_cols:
239             cols = self.get_default_columns()
240         else:
241             # self.query.select is a special case. These columns never go to
242             # any model.
243             cols = self.query.select
244         if cols:
245             select_list = []
246             for col in cols:
247                 select_list.append(select_idx)
248                 select.append((col, None))
249                 select_idx += 1
250             klass_info = {
251                 'model': self.query.model,
252                 'select_fields': select_list,
253             }
254         for alias, annotation in self.query.annotation_select.items():
255             annotations[alias] = select_idx
256             select.append((annotation, alias))
257             select_idx += 1
258 
259         if self.query.select_related:
260             related_klass_infos = self.get_related_selections(select)
261             klass_info['related_klass_infos'] = related_klass_infos
262 
263             def get_select_from_parent(klass_info):
264                 for ki in klass_info['related_klass_infos']:
265                     if ki['from_parent']:
266                         ki['select_fields'] = (klass_info['select_fields'] +
267                                                ki['select_fields'])
268                     get_select_from_parent(ki)
269             get_select_from_parent(klass_info)
270 
271         ret = []
272         for col, alias in select:
273             try:
274                 sql, params = self.compile(col)
275             except EmptyResultSet:
276                 empty_result_set_value = getattr(col, 'empty_result_set_value', NotImplemented)
277                 if empty_result_set_value is NotImplemented:
278                     # Select a predicate that's always False.
279                     sql, params = '0', ()
280                 else:
281                     sql, params = self.compile(Value(empty_result_set_value))
282             else:
283                 sql, params = col.select_format(self, sql, params)
284             ret.append((col, (sql, params), alias))
285         return ret, klass_info, annotations
286 
287     def _order_by_pairs(self):
288         if self.query.extra_order_by:
289             ordering = self.query.extra_order_by
290         elif not self.query.default_ordering:
291             ordering = self.query.order_by
292         elif self.query.order_by:
293             ordering = self.query.order_by
294         elif self.query.get_meta().ordering:
295             ordering = self.query.get_meta().ordering
296             self._meta_ordering = ordering
297         else:
298             ordering = []
299         if self.query.standard_ordering:
300             default_order, _ = ORDER_DIR['ASC']
301         else:
302             default_order, _ = ORDER_DIR['DESC']
303 
304         for field in ordering:
305             if hasattr(field, 'resolve_expression'):
306                 if isinstance(field, Value):
307                     # output_field must be resolved for constants.
308                     field = Cast(field, field.output_field)
309                 if not isinstance(field, OrderBy):
310                     field = field.asc()
311                 if not self.query.standard_ordering:
312                     field = field.copy()
313                     field.reverse_ordering()
314                 yield field, False
315                 continue
316             if field == '?':  # random
317                 yield OrderBy(Random()), False
318                 continue
319 
320             col, order = get_order_dir(field, default_order)
321             descending = order == 'DESC'
322 
323             if col in self.query.annotation_select:
324                 # Reference to expression in SELECT clause
325                 yield (
326                     OrderBy(
327                         Ref(col, self.query.annotation_select[col]),
328                         descending=descending,
329                     ),
330                     True,
331                 )
332                 continue
333             if col in self.query.annotations:
334                 # References to an expression which is masked out of the SELECT
335                 # clause.
336                 if self.query.combinator and self.select:
337                     # Don't use the resolved annotation because other
338                     # combinated queries might define it differently.
339                     expr = F(col)
340                 else:
341                     expr = self.query.annotations[col]
342                     if isinstance(expr, Value):
343                         # output_field must be resolved for constants.
344                         expr = Cast(expr, expr.output_field)
345                 yield OrderBy(expr, descending=descending), False
346                 continue
347 
348             if '.' in field:
349                 # This came in through an extra(order_by=...) addition. Pass it
350                 # on verbatim.
351                 table, col = col.split('.', 1)
352                 yield (
353                     OrderBy(
354                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),
355                         descending=descending,
356                     ),
357                     False,
358                 )
359                 continue
360 
361             if self.query.extra and col in self.query.extra:
362                 if col in self.query.extra_select:
363                     yield (
364                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),
365                         True,
366                     )
367                 else:
368                     yield (
369                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),
370                         False,
371                     )
372             else:
373                 if self.query.combinator and self.select:
374                     # Don't use the first model's field because other
375                     # combinated queries might define it differently.
376                     yield OrderBy(F(col), descending=descending), False
377                 else:
378                     # 'col' is of the form 'field' or 'field1__field2' or
379                     # '-field1__field2__field', etc.
380                     yield from self.find_ordering_name(
381                         field, self.query.get_meta(), default_order=default_order,
382                     )
383 
384     def get_order_by(self):
385         """
386         Return a list of 2-tuples of the form (expr, (sql, params, is_ref)) for
387         the ORDER BY clause.
388 
389         The order_by clause can alter the select clause (for example it can add
390         aliases to clauses that do not yet have one, or it can add totally new
391         select clauses).
392         """
393         result = []
394         seen = set()
395 
396         for expr, is_ref in self._order_by_pairs():
397             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)
398             if self.query.combinator and self.select:
399                 src = resolved.get_source_expressions()[0]
400                 expr_src = expr.get_source_expressions()[0]
401                 # Relabel order by columns to raw numbers if this is a combined
402                 # query; necessary since the columns can't be referenced by the
403                 # fully qualified name and the simple column names may collide.
404                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):
405                     if is_ref and col_alias == src.refs:
406                         src = src.source
407                     elif col_alias and not (
408                         isinstance(expr_src, F) and col_alias == expr_src.name
409                     ):
410                         continue
411                     if src == sel_expr:
412                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])
413                         break
414                 else:
415                     if col_alias:
416                         raise DatabaseError('ORDER BY term does not match any column in the result set.')
417                     # Add column used in ORDER BY clause to the selected
418                     # columns and to each combined query.
419                     order_by_idx = len(self.query.select) + 1
420                     col_name = f'__orderbycol{order_by_idx}'
421                     for q in self.query.combined_queries:
422                         q.add_annotation(expr_src, col_name)
423                     self.query.add_select_col(resolved, col_name)
424                     resolved.set_source_expressions([RawSQL(f'{order_by_idx}', ())])
425             sql, params = self.compile(resolved)
426             # Don't add the same column twice, but the order direction is
427             # not taken into account so we strip it. When this entire method
428             # is refactored into expressions, then we can check each part as we
429             # generate it.
430             without_ordering = self.ordering_parts.search(sql)[1]
431             params_hash = make_hashable(params)
432             if (without_ordering, params_hash) in seen:
433                 continue
434             seen.add((without_ordering, params_hash))
435             result.append((resolved, (sql, params, is_ref)))
436         return result
437 
438     def get_extra_select(self, order_by, select):
439         extra_select = []
440         if self.query.distinct and not self.query.distinct_fields:
441             select_sql = [t[1] for t in select]
442             for expr, (sql, params, is_ref) in order_by:
443                 without_ordering = self.ordering_parts.search(sql)[1]
444                 if not is_ref and (without_ordering, params) not in select_sql:
445                     extra_select.append((expr, (without_ordering, params), None))
446         return extra_select
447 
448     def quote_name_unless_alias(self, name):
449         """
450         A wrapper around connection.ops.quote_name that doesn't quote aliases
451         for table names. This avoids problems with some SQL dialects that treat
452         quoted strings specially (e.g. PostgreSQL).
453         """
454         if name in self.quote_cache:
455             return self.quote_cache[name]
456         if ((name in self.query.alias_map and name not in self.query.table_map) or
457                 name in self.query.extra_select or (
458                     self.query.external_aliases.get(name) and name not in self.query.table_map)):
459             self.quote_cache[name] = name
460             return name
461         r = self.connection.ops.quote_name(name)
462         self.quote_cache[name] = r
463         return r
464 
465     def compile(self, node):
466         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)
467         if vendor_impl:
468             sql, params = vendor_impl(self, self.connection)
469         else:
470             sql, params = node.as_sql(self, self.connection)
471         return sql, params
472 
473     def get_combinator_sql(self, combinator, all):
474         features = self.connection.features
475         compilers = [
476             query.get_compiler(self.using, self.connection, self.elide_empty)
477             for query in self.query.combined_queries if not query.is_empty()
478         ]
479         if not features.supports_slicing_ordering_in_compound:
480             for query, compiler in zip(self.query.combined_queries, compilers):
481                 if query.low_mark or query.high_mark:
482                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')
483                 if compiler.get_order_by():
484                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')
485         parts = ()
486         for compiler in compilers:
487             try:
488                 # If the columns list is limited, then all combined queries
489                 # must have the same columns list. Set the selects defined on
490                 # the query on all combined queries, if not already set.
491                 if not compiler.query.values_select and self.query.values_select:
492                     compiler.query = compiler.query.clone()
493                     compiler.query.set_values((
494                         *self.query.extra_select,
495                         *self.query.values_select,
496                         *self.query.annotation_select,
497                     ))
498                 part_sql, part_args = compiler.as_sql()
499                 if compiler.query.combinator:
500                     # Wrap in a subquery if wrapping in parentheses isn't
501                     # supported.
502                     if not features.supports_parentheses_in_compound:
503                         part_sql = 'SELECT * FROM ({})'.format(part_sql)
504                     # Add parentheses when combining with compound query if not
505                     # already added for all compound queries.
506                     elif (
507                         self.query.subquery or
508                         not features.supports_slicing_ordering_in_compound
509                     ):
510                         part_sql = '({})'.format(part_sql)
511                 parts += ((part_sql, part_args),)
512             except EmptyResultSet:
513                 # Omit the empty queryset with UNION and with DIFFERENCE if the
514                 # first queryset is nonempty.
515                 if combinator == 'union' or (combinator == 'difference' and parts):
516                     continue
517                 raise
518         if not parts:
519             raise EmptyResultSet
520         combinator_sql = self.connection.ops.set_operators[combinator]
521         if all and combinator == 'union':
522             combinator_sql += ' ALL'
523         braces = '{}'
524         if not self.query.subquery and features.supports_slicing_ordering_in_compound:
525             braces = '({})'
526         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))
527         result = [' {} '.format(combinator_sql).join(sql_parts)]
528         params = []
529         for part in args_parts:
530             params.extend(part)
531         return result, params
532 
533     def as_sql(self, with_limits=True, with_col_aliases=False):
534         """
535         Create the SQL for this query. Return the SQL string and list of
536         parameters.
537 
538         If 'with_limits' is False, any limit/offset information is not included
539         in the query.
540         """
541         refcounts_before = self.query.alias_refcount.copy()
542         try:
543             extra_select, order_by, group_by = self.pre_sql_setup()
544             for_update_part = None
545             # Is a LIMIT/OFFSET clause needed?
546             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)
547             combinator = self.query.combinator
548             features = self.connection.features
549             if combinator:
550                 if not getattr(features, 'supports_select_{}'.format(combinator)):
551                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))
552                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)
553             else:
554                 distinct_fields, distinct_params = self.get_distinct()
555                 # This must come after 'select', 'ordering', and 'distinct'
556                 # (see docstring of get_from_clause() for details).
557                 from_, f_params = self.get_from_clause()
558                 try:
559                     where, w_params = self.compile(self.where) if self.where is not None else ('', [])
560                 except EmptyResultSet:
561                     if self.elide_empty:
562                         raise
563                     # Use a predicate that's always False.
564                     where, w_params = '0 = 1', []
565                 having, h_params = self.compile(self.having) if self.having is not None else ("", [])
566                 result = ['SELECT']
567                 params = []
568 
569                 if self.query.distinct:
570                     distinct_result, distinct_params = self.connection.ops.distinct_sql(
571                         distinct_fields,
572                         distinct_params,
573                     )
574                     result += distinct_result
575                     params += distinct_params
576 
577                 out_cols = []
578                 col_idx = 1
579                 for _, (s_sql, s_params), alias in self.select + extra_select:
580                     if alias:
581                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))
582                     elif with_col_aliases:
583                         s_sql = '%s AS %s' % (
584                             s_sql,
585                             self.connection.ops.quote_name('col%d' % col_idx),
586                         )
587                         col_idx += 1
588                     params.extend(s_params)
589                     out_cols.append(s_sql)
590 
591                 result += [', '.join(out_cols), 'FROM', *from_]
592                 params.extend(f_params)
593 
594                 if self.query.select_for_update and self.connection.features.has_select_for_update:
595                     if self.connection.get_autocommit():
596                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')
597 
598                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:
599                         raise NotSupportedError(
600                             'LIMIT/OFFSET is not supported with '
601                             'select_for_update on this database backend.'
602                         )
603                     nowait = self.query.select_for_update_nowait
604                     skip_locked = self.query.select_for_update_skip_locked
605                     of = self.query.select_for_update_of
606                     no_key = self.query.select_for_no_key_update
607                     # If it's a NOWAIT/SKIP LOCKED/OF/NO KEY query but the
608                     # backend doesn't support it, raise NotSupportedError to
609                     # prevent a possible deadlock.
610                     if nowait and not self.connection.features.has_select_for_update_nowait:
611                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
612                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
613                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
614                     elif of and not self.connection.features.has_select_for_update_of:
615                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
616                     elif no_key and not self.connection.features.has_select_for_no_key_update:
617                         raise NotSupportedError(
618                             'FOR NO KEY UPDATE is not supported on this '
619                             'database backend.'
620                         )
621                     for_update_part = self.connection.ops.for_update_sql(
622                         nowait=nowait,
623                         skip_locked=skip_locked,
624                         of=self.get_select_for_update_of_arguments(),
625                         no_key=no_key,
626                     )
627 
628                 if for_update_part and self.connection.features.for_update_after_from:
629                     result.append(for_update_part)
630 
631                 if where:
632                     result.append('WHERE %s' % where)
633                     params.extend(w_params)
634 
635                 grouping = []
636                 for g_sql, g_params in group_by:
637                     grouping.append(g_sql)
638                     params.extend(g_params)
639                 if grouping:
640                     if distinct_fields:
641                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')
642                     order_by = order_by or self.connection.ops.force_no_ordering()
643                     result.append('GROUP BY %s' % ', '.join(grouping))
644                     if self._meta_ordering:
645                         order_by = None
646                 if having:
647                     result.append('HAVING %s' % having)
648                     params.extend(h_params)
649 
650             if self.query.explain_info:
651                 result.insert(0, self.connection.ops.explain_query_prefix(
652                     self.query.explain_info.format,
653                     **self.query.explain_info.options
654                 ))
655 
656             if order_by:
657                 ordering = []
658                 for _, (o_sql, o_params, _) in order_by:
659                     ordering.append(o_sql)
660                     params.extend(o_params)
661                 result.append('ORDER BY %s' % ', '.join(ordering))
662 
663             if with_limit_offset:
664                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))
665 
666             if for_update_part and not self.connection.features.for_update_after_from:
667                 result.append(for_update_part)
668 
669             if self.query.subquery and extra_select:
670                 # If the query is used as a subquery, the extra selects would
671                 # result in more columns than the left-hand side expression is
672                 # expecting. This can happen when a subquery uses a combination
673                 # of order_by() and distinct(), forcing the ordering expressions
674                 # to be selected as well. Wrap the query in another subquery
675                 # to exclude extraneous selects.
676                 sub_selects = []
677                 sub_params = []
678                 for index, (select, _, alias) in enumerate(self.select, start=1):
679                     if not alias and with_col_aliases:
680                         alias = 'col%d' % index
681                     if alias:
682                         sub_selects.append("%s.%s" % (
683                             self.connection.ops.quote_name('subquery'),
684                             self.connection.ops.quote_name(alias),
685                         ))
686                     else:
687                         select_clone = select.relabeled_clone({select.alias: 'subquery'})
688                         subselect, subparams = select_clone.as_sql(self, self.connection)
689                         sub_selects.append(subselect)
690                         sub_params.extend(subparams)
691                 return 'SELECT %s FROM (%s) subquery' % (
692                     ', '.join(sub_selects),
693                     ' '.join(result),
694                 ), tuple(sub_params + params)
695 
696             return ' '.join(result), tuple(params)
697         finally:
698             # Finally do cleanup - get rid of the joins we created above.
699             self.query.reset_refcounts(refcounts_before)
700 
701     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):
702         """
703         Compute the default columns for selecting every field in the base
704         model. Will sometimes be called to pull in related models (e.g. via
705         select_related), in which case "opts" and "start_alias" will be given
706         to provide a starting point for the traversal.
707 
708         Return a list of strings, quoted appropriately for use in SQL
709         directly, as well as a set of aliases used in the select statement (if
710         'as_pairs' is True, return a list of (alias, col_name) pairs instead
711         of strings as the first component and None as the second component).
712         """
713         result = []
714         if opts is None:
715             opts = self.query.get_meta()
716         only_load = self.deferred_to_columns()
717         start_alias = start_alias or self.query.get_initial_alias()
718         # The 'seen_models' is used to optimize checking the needed parent
719         # alias for a given field. This also includes None -> start_alias to
720         # be used by local fields.
721         seen_models = {None: start_alias}
722 
723         for field in opts.concrete_fields:
724             model = field.model._meta.concrete_model
725             # A proxy model will have a different model and concrete_model. We
726             # will assign None if the field belongs to this model.
727             if model == opts.model:
728                 model = None
729             if from_parent and model is not None and issubclass(
730                     from_parent._meta.concrete_model, model._meta.concrete_model):
731                 # Avoid loading data for already loaded parents.
732                 # We end up here in the case select_related() resolution
733                 # proceeds from parent model to child model. In that case the
734                 # parent model data is already present in the SELECT clause,
735                 # and we want to avoid reloading the same data again.
736                 continue
737             if field.model in only_load and field.attname not in only_load[field.model]:
738                 continue
739             alias = self.query.join_parent_model(opts, model, start_alias,
740                                                  seen_models)
741             column = field.get_col(alias)
742             result.append(column)
743         return result
744 
745     def get_distinct(self):
746         """
747         Return a quoted list of fields to use in DISTINCT ON part of the query.
748 
749         This method can alter the tables in the query, and thus it must be
750         called before get_from_clause().
751         """
752         result = []
753         params = []
754         opts = self.query.get_meta()
755 
756         for name in self.query.distinct_fields:
757             parts = name.split(LOOKUP_SEP)
758             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)
759             targets, alias, _ = self.query.trim_joins(targets, joins, path)
760             for target in targets:
761                 if name in self.query.annotation_select:
762                     result.append(self.connection.ops.quote_name(name))
763                 else:
764                     r, p = self.compile(transform_function(target, alias))
765                     result.append(r)
766                     params.append(p)
767         return result, params
768 
769     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',
770                            already_seen=None):
771         """
772         Return the table alias (the name might be ambiguous, the alias will
773         not be) and column name for ordering by the given 'name' parameter.
774         The 'name' is of the form 'field1__field2__...__fieldN'.
775         """
776         name, order = get_order_dir(name, default_order)
777         descending = order == 'DESC'
778         pieces = name.split(LOOKUP_SEP)
779         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)
780 
781         # If we get to this point and the field is a relation to another model,
782         # append the default ordering for that model unless it is the pk
783         # shortcut or the attribute name of the field that is specified.
784         if (
785             field.is_relation and
786             opts.ordering and
787             getattr(field, 'attname', None) != pieces[-1] and
788             name != 'pk'
789         ):
790             # Firstly, avoid infinite loops.
791             already_seen = already_seen or set()
792             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)
793             if join_tuple in already_seen:
794                 raise FieldError('Infinite loop caused by ordering.')
795             already_seen.add(join_tuple)
796 
797             results = []
798             for item in opts.ordering:
799                 if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):
800                     item = item.desc() if descending else item.asc()
801                 if isinstance(item, OrderBy):
802                     results.append((item, False))
803                     continue
804                 results.extend(self.find_ordering_name(item, opts, alias,
805                                                        order, already_seen))
806             return results
807         targets, alias, _ = self.query.trim_joins(targets, joins, path)
808         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]
809 
810     def _setup_joins(self, pieces, opts, alias):
811         """
812         Helper method for get_order_by() and get_distinct().
813 
814         get_ordering() and get_distinct() must produce same target columns on
815         same input, as the prefixes of get_ordering() and get_distinct() must
816         match. Executing SQL where this is not true is an error.
817         """
818         alias = alias or self.query.get_initial_alias()
819         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)
820         alias = joins[-1]
821         return field, targets, alias, joins, path, opts, transform_function
822 
823     def get_from_clause(self):
824         """
825         Return a list of strings that are joined together to go after the
826         "FROM" part of the query, as well as a list any extra parameters that
827         need to be included. Subclasses, can override this to create a
828         from-clause via a "select".
829 
830         This should only be called after any SQL construction methods that
831         might change the tables that are needed. This means the select columns,
832         ordering, and distinct must be done first.
833         """
834         result = []
835         params = []
836         for alias in tuple(self.query.alias_map):
837             if not self.query.alias_refcount[alias]:
838                 continue
839             try:
840                 from_clause = self.query.alias_map[alias]
841             except KeyError:
842                 # Extra tables can end up in self.tables, but not in the
843                 # alias_map if they aren't in a join. That's OK. We skip them.
844                 continue
845             clause_sql, clause_params = self.compile(from_clause)
846             result.append(clause_sql)
847             params.extend(clause_params)
848         for t in self.query.extra_tables:
849             alias, _ = self.query.table_alias(t)
850             # Only add the alias if it's not already present (the table_alias()
851             # call increments the refcount, so an alias refcount of one means
852             # this is the only reference).
853             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:
854                 result.append(', %s' % self.quote_name_unless_alias(alias))
855         return result, params
856 
857     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,
858                                requested=None, restricted=None):
859         """
860         Fill in the information needed for a select_related query. The current
861         depth is measured as the number of connections away from the root model
862         (for example, cur_depth=1 means we are looking at models with direct
863         connections to the root model).
864         """
865         def _get_field_choices():
866             direct_choices = (f.name for f in opts.fields if f.is_relation)
867             reverse_choices = (
868                 f.field.related_query_name()
869                 for f in opts.related_objects if f.field.unique
870             )
871             return chain(direct_choices, reverse_choices, self.query._filtered_relations)
872 
873         related_klass_infos = []
874         if not restricted and cur_depth > self.query.max_depth:
875             # We've recursed far enough; bail out.
876             return related_klass_infos
877 
878         if not opts:
879             opts = self.query.get_meta()
880             root_alias = self.query.get_initial_alias()
881         only_load = self.query.get_loaded_field_names()
882 
883         # Setup for the case when only particular related fields should be
884         # included in the related selection.
885         fields_found = set()
886         if requested is None:
887             restricted = isinstance(self.query.select_related, dict)
888             if restricted:
889                 requested = self.query.select_related
890 
891         def get_related_klass_infos(klass_info, related_klass_infos):
892             klass_info['related_klass_infos'] = related_klass_infos
893 
894         for f in opts.fields:
895             field_model = f.model._meta.concrete_model
896             fields_found.add(f.name)
897 
898             if restricted:
899                 next = requested.get(f.name, {})
900                 if not f.is_relation:
901                     # If a non-related field is used like a relation,
902                     # or if a single non-relational field is given.
903                     if next or f.name in requested:
904                         raise FieldError(
905                             "Non-relational field given in select_related: '%s'. "
906                             "Choices are: %s" % (
907                                 f.name,
908                                 ", ".join(_get_field_choices()) or '(none)',
909                             )
910                         )
911             else:
912                 next = False
913 
914             if not select_related_descend(f, restricted, requested,
915                                           only_load.get(field_model)):
916                 continue
917             klass_info = {
918                 'model': f.remote_field.model,
919                 'field': f,
920                 'reverse': False,
921                 'local_setter': f.set_cached_value,
922                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,
923                 'from_parent': False,
924             }
925             related_klass_infos.append(klass_info)
926             select_fields = []
927             _, _, _, joins, _, _ = self.query.setup_joins(
928                 [f.name], opts, root_alias)
929             alias = joins[-1]
930             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)
931             for col in columns:
932                 select_fields.append(len(select))
933                 select.append((col, None))
934             klass_info['select_fields'] = select_fields
935             next_klass_infos = self.get_related_selections(
936                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)
937             get_related_klass_infos(klass_info, next_klass_infos)
938 
939         if restricted:
940             related_fields = [
941                 (o.field, o.related_model)
942                 for o in opts.related_objects
943                 if o.field.unique and not o.many_to_many
944             ]
945             for f, model in related_fields:
946                 if not select_related_descend(f, restricted, requested,
947                                               only_load.get(model), reverse=True):
948                     continue
949 
950                 related_field_name = f.related_query_name()
951                 fields_found.add(related_field_name)
952 
953                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)
954                 alias = join_info.joins[-1]
955                 from_parent = issubclass(model, opts.model) and model is not opts.model
956                 klass_info = {
957                     'model': model,
958                     'field': f,
959                     'reverse': True,
960                     'local_setter': f.remote_field.set_cached_value,
961                     'remote_setter': f.set_cached_value,
962                     'from_parent': from_parent,
963                 }
964                 related_klass_infos.append(klass_info)
965                 select_fields = []
966                 columns = self.get_default_columns(
967                     start_alias=alias, opts=model._meta, from_parent=opts.model)
968                 for col in columns:
969                     select_fields.append(len(select))
970                     select.append((col, None))
971                 klass_info['select_fields'] = select_fields
972                 next = requested.get(f.related_query_name(), {})
973                 next_klass_infos = self.get_related_selections(
974                     select, model._meta, alias, cur_depth + 1,
975                     next, restricted)
976                 get_related_klass_infos(klass_info, next_klass_infos)
977 
978             def local_setter(obj, from_obj):
979                 # Set a reverse fk object when relation is non-empty.
980                 if from_obj:
981                     f.remote_field.set_cached_value(from_obj, obj)
982 
983             def remote_setter(name, obj, from_obj):
984                 setattr(from_obj, name, obj)
985 
986             for name in list(requested):
987                 # Filtered relations work only on the topmost level.
988                 if cur_depth > 1:
989                     break
990                 if name in self.query._filtered_relations:
991                     fields_found.add(name)
992                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)
993                     model = join_opts.model
994                     alias = joins[-1]
995                     from_parent = issubclass(model, opts.model) and model is not opts.model
996                     klass_info = {
997                         'model': model,
998                         'field': f,
999                         'reverse': True,
1000                         'local_setter': local_setter,
1001                         'remote_setter': partial(remote_setter, name),
1002                         'from_parent': from_parent,
1003                     }
1004                     related_klass_infos.append(klass_info)
1005                     select_fields = []
1006                     columns = self.get_default_columns(
1007                         start_alias=alias, opts=model._meta,
1008                         from_parent=opts.model,
1009                     )
1010                     for col in columns:
1011                         select_fields.append(len(select))
1012                         select.append((col, None))
1013                     klass_info['select_fields'] = select_fields
1014                     next_requested = requested.get(name, {})
1015                     next_klass_infos = self.get_related_selections(
1016                         select, opts=model._meta, root_alias=alias,
1017                         cur_depth=cur_depth + 1, requested=next_requested,
1018                         restricted=restricted,
1019                     )
1020                     get_related_klass_infos(klass_info, next_klass_infos)
1021             fields_not_found = set(requested).difference(fields_found)
1022             if fields_not_found:
1023                 invalid_fields = ("'%s'" % s for s in fields_not_found)
1024                 raise FieldError(
1025                     'Invalid field name(s) given in select_related: %s. '
1026                     'Choices are: %s' % (
1027                         ', '.join(invalid_fields),
1028                         ', '.join(_get_field_choices()) or '(none)',
1029                     )
1030                 )
1031         return related_klass_infos
1032 
1033     def get_select_for_update_of_arguments(self):
1034         """
1035         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of
1036         the query.
1037         """
1038         def _get_parent_klass_info(klass_info):
1039             concrete_model = klass_info['model']._meta.concrete_model
1040             for parent_model, parent_link in concrete_model._meta.parents.items():
1041                 parent_list = parent_model._meta.get_parent_list()
1042                 yield {
1043                     'model': parent_model,
1044                     'field': parent_link,
1045                     'reverse': False,
1046                     'select_fields': [
1047                         select_index
1048                         for select_index in klass_info['select_fields']
1049                         # Selected columns from a model or its parents.
1050                         if (
1051                             self.select[select_index][0].target.model == parent_model or
1052                             self.select[select_index][0].target.model in parent_list
1053                         )
1054                     ],
1055                 }
1056 
1057         def _get_first_selected_col_from_model(klass_info):
1058             """
1059             Find the first selected column from a model. If it doesn't exist,
1060             don't lock a model.
1061 
1062             select_fields is filled recursively, so it also contains fields
1063             from the parent models.
1064             """
1065             concrete_model = klass_info['model']._meta.concrete_model
1066             for select_index in klass_info['select_fields']:
1067                 if self.select[select_index][0].target.model == concrete_model:
1068                     return self.select[select_index][0]
1069 
1070         def _get_field_choices():
1071             """Yield all allowed field paths in breadth-first search order."""
1072             queue = collections.deque([(None, self.klass_info)])
1073             while queue:
1074                 parent_path, klass_info = queue.popleft()
1075                 if parent_path is None:
1076                     path = []
1077                     yield 'self'
1078                 else:
1079                     field = klass_info['field']
1080                     if klass_info['reverse']:
1081                         field = field.remote_field
1082                     path = parent_path + [field.name]
1083                     yield LOOKUP_SEP.join(path)
1084                 queue.extend(
1085                     (path, klass_info)
1086                     for klass_info in _get_parent_klass_info(klass_info)
1087                 )
1088                 queue.extend(
1089                     (path, klass_info)
1090                     for klass_info in klass_info.get('related_klass_infos', [])
1091                 )
1092         if not self.klass_info:
1093             return []
1094         result = []
1095         invalid_names = []
1096         for name in self.query.select_for_update_of:
1097             klass_info = self.klass_info
1098             if name == 'self':
1099                 col = _get_first_selected_col_from_model(klass_info)
1100             else:
1101                 for part in name.split(LOOKUP_SEP):
1102                     klass_infos = (
1103                         *klass_info.get('related_klass_infos', []),
1104                         *_get_parent_klass_info(klass_info),
1105                     )
1106                     for related_klass_info in klass_infos:
1107                         field = related_klass_info['field']
1108                         if related_klass_info['reverse']:
1109                             field = field.remote_field
1110                         if field.name == part:
1111                             klass_info = related_klass_info
1112                             break
1113                     else:
1114                         klass_info = None
1115                         break
1116                 if klass_info is None:
1117                     invalid_names.append(name)
1118                     continue
1119                 col = _get_first_selected_col_from_model(klass_info)
1120             if col is not None:
1121                 if self.connection.features.select_for_update_of_column:
1122                     result.append(self.compile(col)[0])
1123                 else:
1124                     result.append(self.quote_name_unless_alias(col.alias))
1125         if invalid_names:
1126             raise FieldError(
1127                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '
1128                 'Only relational fields followed in the query are allowed. '
1129                 'Choices are: %s.' % (
1130                     ', '.join(invalid_names),
1131                     ', '.join(_get_field_choices()),
1132                 )
1133             )
1134         return result
1135 
1136     def deferred_to_columns(self):
1137         """
1138         Convert the self.deferred_loading data structure to mapping of table
1139         names to sets of column names which are to be loaded. Return the
1140         dictionary.
1141         """
1142         columns = {}
1143         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)
1144         return columns
1145 
1146     def get_converters(self, expressions):
1147         converters = {}
1148         for i, expression in enumerate(expressions):
1149             if expression:
1150                 backend_converters = self.connection.ops.get_db_converters(expression)
1151                 field_converters = expression.get_db_converters(self.connection)
1152                 if backend_converters or field_converters:
1153                     converters[i] = (backend_converters + field_converters, expression)
1154         return converters
1155 
1156     def apply_converters(self, rows, converters):
1157         connection = self.connection
1158         converters = list(converters.items())
1159         for row in map(list, rows):
1160             for pos, (convs, expression) in converters:
1161                 value = row[pos]
1162                 for converter in convs:
1163                     value = converter(value, expression, connection)
1164                 row[pos] = value
1165             yield row
1166 
1167     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,
1168                      chunk_size=GET_ITERATOR_CHUNK_SIZE):
1169         """Return an iterator over the results from executing this query."""
1170         if results is None:
1171             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)
1172         fields = [s[0] for s in self.select[0:self.col_count]]
1173         converters = self.get_converters(fields)
1174         rows = chain.from_iterable(results)
1175         if converters:
1176             rows = self.apply_converters(rows, converters)
1177             if tuple_expected:
1178                 rows = map(tuple, rows)
1179         return rows
1180 
1181     def has_results(self):
1182         """
1183         Backends (e.g. NoSQL) can override this in order to use optimized
1184         versions of "query has any results."
1185         """
1186         return bool(self.execute_sql(SINGLE))
1187 
1188     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
1189         """
1190         Run the query against the database and return the result(s). The
1191         return value is a single data item if result_type is SINGLE, or an
1192         iterator over the results if the result_type is MULTI.
1193 
1194         result_type is either MULTI (use fetchmany() to retrieve all rows),
1195         SINGLE (only retrieve a single row), or None. In this last case, the
1196         cursor is returned if any query is executed, since it's used by
1197         subclasses such as InsertQuery). It's possible, however, that no query
1198         is needed, as the filters describe an empty set. In that case, None is
1199         returned, to avoid any unnecessary database interaction.
1200         """
1201         result_type = result_type or NO_RESULTS
1202         try:
1203             sql, params = self.as_sql()
1204             if not sql:
1205                 raise EmptyResultSet
1206         except EmptyResultSet:
1207             if result_type == MULTI:
1208                 return iter([])
1209             else:
1210                 return
1211         if chunked_fetch:
1212             cursor = self.connection.chunked_cursor()
1213         else:
1214             cursor = self.connection.cursor()
1215         try:
1216             cursor.execute(sql, params)
1217         except Exception:
1218             # Might fail for server-side cursors (e.g. connection closed)
1219             cursor.close()
1220             raise
1221 
1222         if result_type == CURSOR:
1223             # Give the caller the cursor to process and close.
1224             return cursor
1225         if result_type == SINGLE:
1226             try:
1227                 val = cursor.fetchone()
1228                 if val:
1229                     return val[0:self.col_count]
1230                 return val
1231             finally:
1232                 # done with the cursor
1233                 cursor.close()
1234         if result_type == NO_RESULTS:
1235             cursor.close()
1236             return
1237 
1238         result = cursor_iter(
1239             cursor, self.connection.features.empty_fetchmany_value,
1240             self.col_count if self.has_extra_select else None,
1241             chunk_size,
1242         )
1243         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:
1244             # If we are using non-chunked reads, we return the same data
1245             # structure as normally, but ensure it is all read into memory
1246             # before going any further. Use chunked_fetch if requested,
1247             # unless the database doesn't support it.
1248             return list(result)
1249         return result
1250 
1251     def as_subquery_condition(self, alias, columns, compiler):
1252         qn = compiler.quote_name_unless_alias
1253         qn2 = self.connection.ops.quote_name
1254 
1255         for index, select_col in enumerate(self.query.select):
1256             lhs_sql, lhs_params = self.compile(select_col)
1257             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))
1258             self.query.where.add(
1259                 RawSQL('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')
1260 
1261         sql, params = self.as_sql()
1262         return 'EXISTS (%s)' % sql, params
1263 
1264     def explain_query(self):
1265         result = list(self.execute_sql())
1266         # Some backends return 1 item tuples with strings, and others return
1267         # tuples with integers and strings. Flatten them out into strings.
1268         output_formatter = json.dumps if self.query.explain_info.format == 'json' else str
1269         for row in result[0]:
1270             if not isinstance(row, str):
1271                 yield ' '.join(output_formatter(c) for c in row)
1272             else:
1273                 yield row
1274 
1275 
1276 class SQLInsertCompiler(SQLCompiler):
1277     returning_fields = None
1278     returning_params = tuple()
1279 
1280     def field_as_sql(self, field, val):
1281         """
1282         Take a field and a value intended to be saved on that field, and
1283         return placeholder SQL and accompanying params. Check for raw values,
1284         expressions, and fields with get_placeholder() defined in that order.
1285 
1286         When field is None, consider the value raw and use it as the
1287         placeholder, with no corresponding parameters returned.
1288         """
1289         if field is None:
1290             # A field value of None means the value is raw.
1291             sql, params = val, []
1292         elif hasattr(val, 'as_sql'):
1293             # This is an expression, let's compile it.
1294             sql, params = self.compile(val)
1295         elif hasattr(field, 'get_placeholder'):
1296             # Some fields (e.g. geo fields) need special munging before
1297             # they can be inserted.
1298             sql, params = field.get_placeholder(val, self, self.connection), [val]
1299         else:
1300             # Return the common case for the placeholder
1301             sql, params = '%s', [val]
1302 
1303         # The following hook is only used by Oracle Spatial, which sometimes
1304         # needs to yield 'NULL' and [] as its placeholder and params instead
1305         # of '%s' and [None]. The 'NULL' placeholder is produced earlier by
1306         # OracleOperations.get_geom_placeholder(). The following line removes
1307         # the corresponding None parameter. See ticket #10888.
1308         params = self.connection.ops.modify_insert_params(sql, params)
1309 
1310         return sql, params
1311 
1312     def prepare_value(self, field, value):
1313         """
1314         Prepare a value to be used in a query by resolving it if it is an
1315         expression and otherwise calling the field's get_db_prep_save().
1316         """
1317         if hasattr(value, 'resolve_expression'):
1318             value = value.resolve_expression(self.query, allow_joins=False, for_save=True)
1319             # Don't allow values containing Col expressions. They refer to
1320             # existing columns on a row, but in the case of insert the row
1321             # doesn't exist yet.
1322             if value.contains_column_references:
1323                 raise ValueError(
1324                     'Failed to insert expression "%s" on %s. F() expressions '
1325                     'can only be used to update, not to insert.' % (value, field)
1326                 )
1327             if value.contains_aggregate:
1328                 raise FieldError(
1329                     'Aggregate functions are not allowed in this query '
1330                     '(%s=%r).' % (field.name, value)
1331                 )
1332             if value.contains_over_clause:
1333                 raise FieldError(
1334                     'Window expressions are not allowed in this query (%s=%r).'
1335                     % (field.name, value)
1336                 )
1337         else:
1338             value = field.get_db_prep_save(value, connection=self.connection)
1339         return value
1340 
1341     def pre_save_val(self, field, obj):
1342         """
1343         Get the given field's value off the given obj. pre_save() is used for
1344         things like auto_now on DateTimeField. Skip it if this is a raw query.
1345         """
1346         if self.query.raw:
1347             return getattr(obj, field.attname)
1348         return field.pre_save(obj, add=True)
1349 
1350     def assemble_as_sql(self, fields, value_rows):
1351         """
1352         Take a sequence of N fields and a sequence of M rows of values, and
1353         generate placeholder SQL and parameters for each field and value.
1354         Return a pair containing:
1355          * a sequence of M rows of N SQL placeholder strings, and
1356          * a sequence of M rows of corresponding parameter values.
1357 
1358         Each placeholder string may contain any number of '%s' interpolation
1359         strings, and each parameter row will contain exactly as many params
1360         as the total number of '%s's in the corresponding placeholder row.
1361         """
1362         if not value_rows:
1363             return [], []
1364 
1365         # list of (sql, [params]) tuples for each object to be saved
1366         # Shape: [n_objs][n_fields][2]
1367         rows_of_fields_as_sql = (
1368             (self.field_as_sql(field, v) for field, v in zip(fields, row))
1369             for row in value_rows
1370         )
1371 
1372         # tuple like ([sqls], [[params]s]) for each object to be saved
1373         # Shape: [n_objs][2][n_fields]
1374         sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)
1375 
1376         # Extract separate lists for placeholders and params.
1377         # Each of these has shape [n_objs][n_fields]
1378         placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)
1379 
1380         # Params for each field are still lists, and need to be flattened.
1381         param_rows = [[p for ps in row for p in ps] for row in param_rows]
1382 
1383         return placeholder_rows, param_rows
1384 
1385     def as_sql(self):
1386         # We don't need quote_name_unless_alias() here, since these are all
1387         # going to be column names (so we can avoid the extra overhead).
1388         qn = self.connection.ops.quote_name
1389         opts = self.query.get_meta()
1390         insert_statement = self.connection.ops.insert_statement(
1391             on_conflict=self.query.on_conflict,
1392         )
1393         result = ['%s %s' % (insert_statement, qn(opts.db_table))]
1394         fields = self.query.fields or [opts.pk]
1395         result.append('(%s)' % ', '.join(qn(f.column) for f in fields))
1396 
1397         if self.query.fields:
1398             value_rows = [
1399                 [self.prepare_value(field, self.pre_save_val(field, obj)) for field in fields]
1400                 for obj in self.query.objs
1401             ]
1402         else:
1403             # An empty object.
1404             value_rows = [[self.connection.ops.pk_default_value()] for _ in self.query.objs]
1405             fields = [None]
1406 
1407         # Currently the backends just accept values when generating bulk
1408         # queries and generate their own placeholders. Doing that isn't
1409         # necessary and it should be possible to use placeholders and
1410         # expressions in bulk inserts too.
1411         can_bulk = (not self.returning_fields and self.connection.features.has_bulk_insert)
1412 
1413         placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)
1414 
1415         on_conflict_suffix_sql = self.connection.ops.on_conflict_suffix_sql(
1416             fields,
1417             self.query.on_conflict,
1418             self.query.update_fields,
1419             self.query.unique_fields,
1420         )
1421         if self.returning_fields and self.connection.features.can_return_columns_from_insert:
1422             if self.connection.features.can_return_rows_from_bulk_insert:
1423                 result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))
1424                 params = param_rows
1425             else:
1426                 result.append("VALUES (%s)" % ", ".join(placeholder_rows[0]))
1427                 params = [param_rows[0]]
1428             if on_conflict_suffix_sql:
1429                 result.append(on_conflict_suffix_sql)
1430             # Skip empty r_sql to allow subclasses to customize behavior for
1431             # 3rd party backends. Refs #19096.
1432             r_sql, self.returning_params = self.connection.ops.return_insert_columns(self.returning_fields)
1433             if r_sql:
1434                 result.append(r_sql)
1435                 params += [self.returning_params]
1436             return [(" ".join(result), tuple(chain.from_iterable(params)))]
1437 
1438         if can_bulk:
1439             result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))
1440             if on_conflict_suffix_sql:
1441                 result.append(on_conflict_suffix_sql)
1442             return [(" ".join(result), tuple(p for ps in param_rows for p in ps))]
1443         else:
1444             if on_conflict_suffix_sql:
1445                 result.append(on_conflict_suffix_sql)
1446             return [
1447                 (" ".join(result + ["VALUES (%s)" % ", ".join(p)]), vals)
1448                 for p, vals in zip(placeholder_rows, param_rows)
1449             ]
1450 
1451     def execute_sql(self, returning_fields=None):
1452         assert not (
1453             returning_fields and len(self.query.objs) != 1 and
1454             not self.connection.features.can_return_rows_from_bulk_insert
1455         )
1456         opts = self.query.get_meta()
1457         self.returning_fields = returning_fields
1458         with self.connection.cursor() as cursor:
1459             for sql, params in self.as_sql():
1460                 cursor.execute(sql, params)
1461             if not self.returning_fields:
1462                 return []
1463             if self.connection.features.can_return_rows_from_bulk_insert and len(self.query.objs) > 1:
1464                 rows = self.connection.ops.fetch_returned_insert_rows(cursor)
1465             elif self.connection.features.can_return_columns_from_insert:
1466                 assert len(self.query.objs) == 1
1467                 rows = [self.connection.ops.fetch_returned_insert_columns(
1468                     cursor, self.returning_params,
1469                 )]
1470             else:
1471                 rows = [(self.connection.ops.last_insert_id(
1472                     cursor, opts.db_table, opts.pk.column,
1473                 ),)]
1474         cols = [field.get_col(opts.db_table) for field in self.returning_fields]
1475         converters = self.get_converters(cols)
1476         if converters:
1477             rows = list(self.apply_converters(rows, converters))
1478         return rows
1479 
1480 
1481 class SQLDeleteCompiler(SQLCompiler):
1482     @cached_property
1483     def single_alias(self):
1484         # Ensure base table is in aliases.
1485         self.query.get_initial_alias()
1486         return sum(self.query.alias_refcount[t] > 0 for t in self.query.alias_map) == 1
1487 
1488     @classmethod
1489     def _expr_refs_base_model(cls, expr, base_model):
1490         if isinstance(expr, Query):
1491             return expr.model == base_model
1492         if not hasattr(expr, 'get_source_expressions'):
1493             return False
1494         return any(
1495             cls._expr_refs_base_model(source_expr, base_model)
1496             for source_expr in expr.get_source_expressions()
1497         )
1498 
1499     @cached_property
1500     def contains_self_reference_subquery(self):
1501         return any(
1502             self._expr_refs_base_model(expr, self.query.model)
1503             for expr in chain(self.query.annotations.values(), self.query.where.children)
1504         )
1505 
1506     def _as_sql(self, query):
1507         result = [
1508             'DELETE FROM %s' % self.quote_name_unless_alias(query.base_table)
1509         ]
1510         where, params = self.compile(query.where)
1511         if where:
1512             result.append('WHERE %s' % where)
1513         return ' '.join(result), tuple(params)
1514 
1515     def as_sql(self):
1516         """
1517         Create the SQL for this query. Return the SQL string and list of
1518         parameters.
1519         """
1520         if self.single_alias and not self.contains_self_reference_subquery:
1521             return self._as_sql(self.query)
1522         innerq = self.query.clone()
1523         innerq.__class__ = Query
1524         innerq.clear_select_clause()
1525         pk = self.query.model._meta.pk
1526         innerq.select = [
1527             pk.get_col(self.query.get_initial_alias())
1528         ]
1529         outerq = Query(self.query.model)
1530         if not self.connection.features.update_can_self_select:
1531             # Force the materialization of the inner query to allow reference
1532             # to the target table on MySQL.
1533             sql, params = innerq.get_compiler(connection=self.connection).as_sql()
1534             innerq = RawSQL('SELECT * FROM (%s) subquery' % sql, params)
1535         outerq.add_filter('pk__in', innerq)
1536         return self._as_sql(outerq)
1537 
1538 
1539 class SQLUpdateCompiler(SQLCompiler):
1540     def as_sql(self):
1541         """
1542         Create the SQL for this query. Return the SQL string and list of
1543         parameters.
1544         """
1545         self.pre_sql_setup()
1546         if not self.query.values:
1547             return '', ()
1548         qn = self.quote_name_unless_alias
1549         values, update_params = [], []
1550         for field, model, val in self.query.values:
1551             if hasattr(val, 'resolve_expression'):
1552                 val = val.resolve_expression(self.query, allow_joins=False, for_save=True)
1553                 if val.contains_aggregate:
1554                     raise FieldError(
1555                         'Aggregate functions are not allowed in this query '
1556                         '(%s=%r).' % (field.name, val)
1557                     )
1558                 if val.contains_over_clause:
1559                     raise FieldError(
1560                         'Window expressions are not allowed in this query '
1561                         '(%s=%r).' % (field.name, val)
1562                     )
1563             elif hasattr(val, 'prepare_database_save'):
1564                 if field.remote_field:
1565                     val = field.get_db_prep_save(
1566                         val.prepare_database_save(field),
1567                         connection=self.connection,
1568                     )
1569                 else:
1570                     raise TypeError(
1571                         "Tried to update field %s with a model instance, %r. "
1572                         "Use a value compatible with %s."
1573                         % (field, val, field.__class__.__name__)
1574                     )
1575             else:
1576                 val = field.get_db_prep_save(val, connection=self.connection)
1577 
1578             # Getting the placeholder for the field.
1579             if hasattr(field, 'get_placeholder'):
1580                 placeholder = field.get_placeholder(val, self, self.connection)
1581             else:
1582                 placeholder = '%s'
1583             name = field.column
1584             if hasattr(val, 'as_sql'):
1585                 sql, params = self.compile(val)
1586                 values.append('%s = %s' % (qn(name), placeholder % sql))
1587                 update_params.extend(params)
1588             elif val is not None:
1589                 values.append('%s = %s' % (qn(name), placeholder))
1590                 update_params.append(val)
1591             else:
1592                 values.append('%s = NULL' % qn(name))
1593         table = self.query.base_table
1594         result = [
1595             'UPDATE %s SET' % qn(table),
1596             ', '.join(values),
1597         ]
1598         where, params = self.compile(self.query.where)
1599         if where:
1600             result.append('WHERE %s' % where)
1601         return ' '.join(result), tuple(update_params + params)
1602 
1603     def execute_sql(self, result_type):
1604         """
1605         Execute the specified update. Return the number of rows affected by
1606         the primary update query. The "primary update query" is the first
1607         non-empty query that is executed. Row counts for any subsequent,
1608         related queries are not available.
1609         """
1610         cursor = super().execute_sql(result_type)
1611         try:
1612             rows = cursor.rowcount if cursor else 0
1613             is_empty = cursor is None
1614         finally:
1615             if cursor:
1616                 cursor.close()
1617         for query in self.query.get_related_updates():
1618             aux_rows = query.get_compiler(self.using).execute_sql(result_type)
1619             if is_empty and aux_rows:
1620                 rows = aux_rows
1621                 is_empty = False
1622         return rows
1623 
1624     def pre_sql_setup(self):
1625         """
1626         If the update depends on results from other tables, munge the "where"
1627         conditions to match the format required for (portable) SQL updates.
1628 
1629         If multiple updates are required, pull out the id values to update at
1630         this point so that they don't change as a result of the progressive
1631         updates.
1632         """
1633         refcounts_before = self.query.alias_refcount.copy()
1634         # Ensure base table is in the query
1635         self.query.get_initial_alias()
1636         count = self.query.count_active_tables()
1637         if not self.query.related_updates and count == 1:
1638             return
1639         query = self.query.chain(klass=Query)
1640         query.select_related = False
1641         query.clear_ordering(force=True)
1642         query.extra = {}
1643         query.select = []
1644         query.add_fields([query.get_meta().pk.name])
1645         super().pre_sql_setup()
1646 
1647         must_pre_select = count > 1 and not self.connection.features.update_can_self_select
1648 
1649         # Now we adjust the current query: reset the where clause and get rid
1650         # of all the tables we don't need (since they're in the sub-select).
1651         self.query.clear_where()
1652         if self.query.related_updates or must_pre_select:
1653             # Either we're using the idents in multiple update queries (so
1654             # don't want them to change), or the db backend doesn't support
1655             # selecting from the updating table (e.g. MySQL).
1656             idents = []
1657             for rows in query.get_compiler(self.using).execute_sql(MULTI):
1658                 idents.extend(r[0] for r in rows)
1659             self.query.add_filter('pk__in', idents)
1660             self.query.related_ids = idents
1661         else:
1662             # The fast path. Filters and updates in one query.
1663             self.query.add_filter('pk__in', query)
1664         self.query.reset_refcounts(refcounts_before)
1665 
1666 
1667 class SQLAggregateCompiler(SQLCompiler):
1668     def as_sql(self):
1669         """
1670         Create the SQL for this query. Return the SQL string and list of
1671         parameters.
1672         """
1673         sql, params = [], []
1674         for annotation in self.query.annotation_select.values():
1675             ann_sql, ann_params = self.compile(annotation)
1676             ann_sql, ann_params = annotation.select_format(self, ann_sql, ann_params)
1677             sql.append(ann_sql)
1678             params.extend(ann_params)
1679         self.col_count = len(self.query.annotation_select)
1680         sql = ', '.join(sql)
1681         params = tuple(params)
1682 
1683         inner_query_sql, inner_query_params = self.query.inner_query.get_compiler(
1684             self.using, elide_empty=self.elide_empty,
1685         ).as_sql(with_col_aliases=True)
1686         sql = 'SELECT %s FROM (%s) subquery' % (sql, inner_query_sql)
1687         params = params + inner_query_params
1688         return sql, params
1689 
1690 
1691 def cursor_iter(cursor, sentinel, col_count, itersize):
1692     """
1693     Yield blocks of rows from a cursor and ensure the cursor is closed when
1694     done.
1695     """
1696     try:
1697         for rows in iter((lambda: cursor.fetchmany(itersize)), sentinel):
1698             yield rows if col_count is None else [r[:col_count] for r in rows]
1699     finally:
1700         cursor.close()

</code><intended_behavior>This is a speculative location based on the usage of `EmptyResultSet` across the codebase, particularly in SQL compiler logic. A revision might be necessary to ensure that when compiling queries involving an `Exists` subquery that has been negated and is based on an `none()` queryset, the compiler preserves the WHERE block appropriately, potentially as a no-op or always-false condition, thereby conveying the intended semantics of the original queryset filtering criteria without erroneously excluding necessary SQL syntax.</intended_behavior>]
2025-10-17 23:27:21.236 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 23:27:21.236 | INFO     | Search-only: skipping patch selection
2025-10-17 23:27:21.378 | INFO     | Task django__django-15382 completed successfully.
2025-10-17 23:27:21.379 | INFO     | No patch generated. You can try running ACR again.
