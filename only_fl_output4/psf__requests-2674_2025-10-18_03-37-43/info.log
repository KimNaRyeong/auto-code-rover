2025-10-18 03:37:43.669 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 03:37:46.975 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=196, output_tokens=12, cost=0.002320
2025-10-18 03:37:46.977 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 03:37:46.984 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 03:37:54.647 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=505, output_tokens=297, cost=0.013960
2025-10-18 03:37:54.655 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 03:37:57.378 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=651, output_tokens=62, cost=0.008370
2025-10-18 03:37:57.379 | DEBUG    | Extracted a valid json.
2025-10-18 03:37:57.380 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"RequestException\")",
        "search_code(\"urllib3.exceptions\")",
        "search_code(\"urllib3.exceptions.DecodeError\")",
        "search_code(\"urllib3.exceptions.TimeoutError\")"
    ],
    "bug_locations": []
}
2025-10-18 03:37:57.407 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 03:37:57.408 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 03:38:13.034 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1181, output_tokens=709, cost=0.033080
2025-10-18 03:38:13.049 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 03:38:20.425 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2106, output_tokens=339, cost=0.031230
2025-10-18 03:38:20.432 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 03:38:25.096 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=693, output_tokens=170, cost=0.012030
2025-10-18 03:38:25.097 | DEBUG    | Extracted a valid json.
2025-10-18 03:38:25.097 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_code_in_file(\"urllib3.exceptions\", \"requests/adapters.py\")"],
    "bug_locations": [
        {
            "file": "requests/adapters.py",
            "class": "",
            "method": "send",
            "intended_behavior": "Inside this method (or potentially others involved with the request/response lifecycle), ensure that any urllib3.exceptions.TimeoutError and urllib3.exceptions.DecodeError are caught and then wrapped inside a requests.exceptions.RequestException derivative that most closely matches the semantics of the caught exception. This might mean creating or utilizing existing requests exceptions like requests.exceptions.Timeout or requests.exceptions.ContentDecodingError. The wrapping process should preserve any relevant information from the original urllib3 exception (such as error messages or codes) and re-raise it as a requests exception."
        }
    ]
}
2025-10-18 03:38:25.105 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 03:38:25.105 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 03:38:36.317 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2617, output_tokens=520, cost=0.041770
2025-10-18 03:38:36.327 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 03:38:45.209 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3353, output_tokens=388, cost=0.045170
2025-10-18 03:38:45.217 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 03:38:48.897 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=742, output_tokens=98, cost=0.010360
2025-10-18 03:38:48.898 | DEBUG    | Extracted a valid json.
2025-10-18 03:38:48.898 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "requests/adapters.py",
            "class": "",
            "method": "",
            "intended_behavior": "Catch `urllib3` exceptions directly related to HTTP operations, such as timeout errors, connection errors, and decoding errors, and wrap these in the appropriate `requests` exceptions, ensuring that the original exception message and any other relevant diagnostic information are preserved."
        }
    ]
}
2025-10-18 03:38:48.901 | DEBUG    | Bug location extracted successfully: [<file>requests/adapters.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.adapters
5 ~~~~~~~~~~~~~~~~~
6 
7 This module contains the transport adapters that Requests uses to define
8 and maintain connections.
9 """
10 
11 import socket
12 
13 from .models import Response
14 from .packages.urllib3.poolmanager import PoolManager, proxy_from_url
15 from .packages.urllib3.response import HTTPResponse
16 from .packages.urllib3.util import Timeout as TimeoutSauce
17 from .packages.urllib3.util.retry import Retry
18 from .compat import urlparse, basestring
19 from .utils import (DEFAULT_CA_BUNDLE_PATH, get_encoding_from_headers,
20                     prepend_scheme_if_needed, get_auth_from_url, urldefragauth)
21 from .structures import CaseInsensitiveDict
22 from .packages.urllib3.exceptions import ConnectTimeoutError
23 from .packages.urllib3.exceptions import HTTPError as _HTTPError
24 from .packages.urllib3.exceptions import MaxRetryError
25 from .packages.urllib3.exceptions import ProxyError as _ProxyError
26 from .packages.urllib3.exceptions import ProtocolError
27 from .packages.urllib3.exceptions import ReadTimeoutError
28 from .packages.urllib3.exceptions import SSLError as _SSLError
29 from .packages.urllib3.exceptions import ResponseError
30 from .cookies import extract_cookies_to_jar
31 from .exceptions import (ConnectionError, ConnectTimeout, ReadTimeout, SSLError,
32                          ProxyError, RetryError)
33 from .auth import _basic_auth_str
34 
35 DEFAULT_POOLBLOCK = False
36 DEFAULT_POOLSIZE = 10
37 DEFAULT_RETRIES = 0
38 DEFAULT_POOL_TIMEOUT = None
39 
40 
41 class BaseAdapter(object):
42     """The Base Transport Adapter"""
43 
44     def __init__(self):
45         super(BaseAdapter, self).__init__()
46 
47     def send(self):
48         raise NotImplementedError
49 
50     def close(self):
51         raise NotImplementedError
52 
53 
54 class HTTPAdapter(BaseAdapter):
55     """The built-in HTTP Adapter for urllib3.
56 
57     Provides a general-case interface for Requests sessions to contact HTTP and
58     HTTPS urls by implementing the Transport Adapter interface. This class will
59     usually be created by the :class:`Session <Session>` class under the
60     covers.
61 
62     :param pool_connections: The number of urllib3 connection pools to cache.
63     :param pool_maxsize: The maximum number of connections to save in the pool.
64     :param int max_retries: The maximum number of retries each connection
65         should attempt. Note, this applies only to failed DNS lookups, socket
66         connections and connection timeouts, never to requests where data has
67         made it to the server. By default, Requests does not retry failed
68         connections. If you need granular control over the conditions under
69         which we retry a request, import urllib3's ``Retry`` class and pass
70         that instead.
71     :param pool_block: Whether the connection pool should block for connections.
72 
73     Usage::
74 
75       >>> import requests
76       >>> s = requests.Session()
77       >>> a = requests.adapters.HTTPAdapter(max_retries=3)
78       >>> s.mount('http://', a)
79     """
80     __attrs__ = ['max_retries', 'config', '_pool_connections', '_pool_maxsize',
81                  '_pool_block']
82 
83     def __init__(self, pool_connections=DEFAULT_POOLSIZE,
84                  pool_maxsize=DEFAULT_POOLSIZE, max_retries=DEFAULT_RETRIES,
85                  pool_block=DEFAULT_POOLBLOCK):
86         if max_retries == DEFAULT_RETRIES:
87             self.max_retries = Retry(0, read=False)
88         else:
89             self.max_retries = Retry.from_int(max_retries)
90         self.config = {}
91         self.proxy_manager = {}
92 
93         super(HTTPAdapter, self).__init__()
94 
95         self._pool_connections = pool_connections
96         self._pool_maxsize = pool_maxsize
97         self._pool_block = pool_block
98 
99         self.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)
100 
101     def __getstate__(self):
102         return dict((attr, getattr(self, attr, None)) for attr in
103                     self.__attrs__)
104 
105     def __setstate__(self, state):
106         # Can't handle by adding 'proxy_manager' to self.__attrs__ because
107         # because self.poolmanager uses a lambda function, which isn't pickleable.
108         self.proxy_manager = {}
109         self.config = {}
110 
111         for attr, value in state.items():
112             setattr(self, attr, value)
113 
114         self.init_poolmanager(self._pool_connections, self._pool_maxsize,
115                               block=self._pool_block)
116 
117     def init_poolmanager(self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs):
118         """Initializes a urllib3 PoolManager.
119 
120         This method should not be called from user code, and is only
121         exposed for use when subclassing the
122         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
123 
124         :param connections: The number of urllib3 connection pools to cache.
125         :param maxsize: The maximum number of connections to save in the pool.
126         :param block: Block when no free connections are available.
127         :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.
128         """
129         # save these values for pickling
130         self._pool_connections = connections
131         self._pool_maxsize = maxsize
132         self._pool_block = block
133 
134         self.poolmanager = PoolManager(num_pools=connections, maxsize=maxsize,
135                                        block=block, strict=True, **pool_kwargs)
136 
137     def proxy_manager_for(self, proxy, **proxy_kwargs):
138         """Return urllib3 ProxyManager for the given proxy.
139 
140         This method should not be called from user code, and is only
141         exposed for use when subclassing the
142         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
143 
144         :param proxy: The proxy to return a urllib3 ProxyManager for.
145         :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.
146         :returns: ProxyManager
147         """
148         if not proxy in self.proxy_manager:
149             proxy_headers = self.proxy_headers(proxy)
150             self.proxy_manager[proxy] = proxy_from_url(
151                 proxy,
152                 proxy_headers=proxy_headers,
153                 num_pools=self._pool_connections,
154                 maxsize=self._pool_maxsize,
155                 block=self._pool_block,
156                 **proxy_kwargs)
157 
158         return self.proxy_manager[proxy]
159 
160     def cert_verify(self, conn, url, verify, cert):
161         """Verify a SSL certificate. This method should not be called from user
162         code, and is only exposed for use when subclassing the
163         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
164 
165         :param conn: The urllib3 connection object associated with the cert.
166         :param url: The requested URL.
167         :param verify: Whether we should actually verify the certificate.
168         :param cert: The SSL certificate to verify.
169         """
170         if url.lower().startswith('https') and verify:
171 
172             cert_loc = None
173 
174             # Allow self-specified cert location.
175             if verify is not True:
176                 cert_loc = verify
177 
178             if not cert_loc:
179                 cert_loc = DEFAULT_CA_BUNDLE_PATH
180 
181             if not cert_loc:
182                 raise Exception("Could not find a suitable SSL CA certificate bundle.")
183 
184             conn.cert_reqs = 'CERT_REQUIRED'
185             conn.ca_certs = cert_loc
186         else:
187             conn.cert_reqs = 'CERT_NONE'
188             conn.ca_certs = None
189 
190         if cert:
191             if not isinstance(cert, basestring):
192                 conn.cert_file = cert[0]
193                 conn.key_file = cert[1]
194             else:
195                 conn.cert_file = cert
196 
197     def build_response(self, req, resp):
198         """Builds a :class:`Response <requests.Response>` object from a urllib3
199         response. This should not be called from user code, and is only exposed
200         for use when subclassing the
201         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`
202 
203         :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
204         :param resp: The urllib3 response object.
205         """
206         response = Response()
207 
208         # Fallback to None if there's no status_code, for whatever reason.
209         response.status_code = getattr(resp, 'status', None)
210 
211         # Make headers case-insensitive.
212         response.headers = CaseInsensitiveDict(getattr(resp, 'headers', {}))
213 
214         # Set encoding.
215         response.encoding = get_encoding_from_headers(response.headers)
216         response.raw = resp
217         response.reason = response.raw.reason
218 
219         if isinstance(req.url, bytes):
220             response.url = req.url.decode('utf-8')
221         else:
222             response.url = req.url
223 
224         # Add new cookies from the server.
225         extract_cookies_to_jar(response.cookies, req, resp)
226 
227         # Give the Response some context.
228         response.request = req
229         response.connection = self
230 
231         return response
232 
233     def get_connection(self, url, proxies=None):
234         """Returns a urllib3 connection for the given URL. This should not be
235         called from user code, and is only exposed for use when subclassing the
236         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
237 
238         :param url: The URL to connect to.
239         :param proxies: (optional) A Requests-style dictionary of proxies used on this request.
240         """
241         proxies = proxies or {}
242         proxy = proxies.get(urlparse(url.lower()).scheme)
243 
244         if proxy:
245             proxy = prepend_scheme_if_needed(proxy, 'http')
246             proxy_manager = self.proxy_manager_for(proxy)
247             conn = proxy_manager.connection_from_url(url)
248         else:
249             # Only scheme should be lower case
250             parsed = urlparse(url)
251             url = parsed.geturl()
252             conn = self.poolmanager.connection_from_url(url)
253 
254         return conn
255 
256     def close(self):
257         """Disposes of any internal state.
258 
259         Currently, this just closes the PoolManager, which closes pooled
260         connections.
261         """
262         self.poolmanager.clear()
263 
264     def request_url(self, request, proxies):
265         """Obtain the url to use when making the final request.
266 
267         If the message is being sent through a HTTP proxy, the full URL has to
268         be used. Otherwise, we should only use the path portion of the URL.
269 
270         This should not be called from user code, and is only exposed for use
271         when subclassing the
272         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
273 
274         :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
275         :param proxies: A dictionary of schemes to proxy URLs.
276         """
277         proxies = proxies or {}
278         scheme = urlparse(request.url).scheme
279         proxy = proxies.get(scheme)
280 
281         if proxy and scheme != 'https':
282             url = urldefragauth(request.url)
283         else:
284             url = request.path_url
285 
286         return url
287 
288     def add_headers(self, request, **kwargs):
289         """Add any headers needed by the connection. As of v2.0 this does
290         nothing by default, but is left for overriding by users that subclass
291         the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
292 
293         This should not be called from user code, and is only exposed for use
294         when subclassing the
295         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
296 
297         :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.
298         :param kwargs: The keyword arguments from the call to send().
299         """
300         pass
301 
302     def proxy_headers(self, proxy):
303         """Returns a dictionary of the headers to add to any request sent
304         through a proxy. This works with urllib3 magic to ensure that they are
305         correctly sent to the proxy, rather than in a tunnelled request if
306         CONNECT is being used.
307 
308         This should not be called from user code, and is only exposed for use
309         when subclassing the
310         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
311 
312         :param proxies: The url of the proxy being used for this request.
313         :param kwargs: Optional additional keyword arguments.
314         """
315         headers = {}
316         username, password = get_auth_from_url(proxy)
317 
318         if username and password:
319             headers['Proxy-Authorization'] = _basic_auth_str(username,
320                                                              password)
321 
322         return headers
323 
324     def send(self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None):
325         """Sends PreparedRequest object. Returns Response object.
326 
327         :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
328         :param stream: (optional) Whether to stream the request content.
329         :param timeout: (optional) How long to wait for the server to send
330             data before giving up, as a float, or a (`connect timeout, read
331             timeout <user/advanced.html#timeouts>`_) tuple.
332         :type timeout: float or tuple
333         :param verify: (optional) Whether to verify SSL certificates.
334         :param cert: (optional) Any user-provided SSL certificate to be trusted.
335         :param proxies: (optional) The proxies dictionary to apply to the request.
336         """
337 
338         conn = self.get_connection(request.url, proxies)
339 
340         self.cert_verify(conn, request.url, verify, cert)
341         url = self.request_url(request, proxies)
342         self.add_headers(request)
343 
344         chunked = not (request.body is None or 'Content-Length' in request.headers)
345 
346         if isinstance(timeout, tuple):
347             try:
348                 connect, read = timeout
349                 timeout = TimeoutSauce(connect=connect, read=read)
350             except ValueError as e:
351                 # this may raise a string formatting error.
352                 err = ("Invalid timeout {0}. Pass a (connect, read) "
353                        "timeout tuple, or a single float to set "
354                        "both timeouts to the same value".format(timeout))
355                 raise ValueError(err)
356         else:
357             timeout = TimeoutSauce(connect=timeout, read=timeout)
358 
359         try:
360             if not chunked:
361                 resp = conn.urlopen(
362                     method=request.method,
363                     url=url,
364                     body=request.body,
365                     headers=request.headers,
366                     redirect=False,
367                     assert_same_host=False,
368                     preload_content=False,
369                     decode_content=False,
370                     retries=self.max_retries,
371                     timeout=timeout
372                 )
373 
374             # Send the request.
375             else:
376                 if hasattr(conn, 'proxy_pool'):
377                     conn = conn.proxy_pool
378 
379                 low_conn = conn._get_conn(timeout=DEFAULT_POOL_TIMEOUT)
380 
381                 try:
382                     low_conn.putrequest(request.method,
383                                         url,
384                                         skip_accept_encoding=True)
385 
386                     for header, value in request.headers.items():
387                         low_conn.putheader(header, value)
388 
389                     low_conn.endheaders()
390 
391                     for i in request.body:
392                         low_conn.send(hex(len(i))[2:].encode('utf-8'))
393                         low_conn.send(b'\r\n')
394                         low_conn.send(i)
395                         low_conn.send(b'\r\n')
396                     low_conn.send(b'0\r\n\r\n')
397 
398                     r = low_conn.getresponse()
399                     resp = HTTPResponse.from_httplib(
400                         r,
401                         pool=conn,
402                         connection=low_conn,
403                         preload_content=False,
404                         decode_content=False
405                     )
406                 except:
407                     # If we hit any problems here, clean up the connection.
408                     # Then, reraise so that we can handle the actual exception.
409                     low_conn.close()
410                     raise
411 
412         except (ProtocolError, socket.error) as err:
413             raise ConnectionError(err, request=request)
414 
415         except MaxRetryError as e:
416             if isinstance(e.reason, ConnectTimeoutError):
417                 raise ConnectTimeout(e, request=request)
418 
419             if isinstance(e.reason, ResponseError):
420                 raise RetryError(e, request=request)
421 
422             raise ConnectionError(e, request=request)
423 
424         except _ProxyError as e:
425             raise ProxyError(e)
426 
427         except (_SSLError, _HTTPError) as e:
428             if isinstance(e, _SSLError):
429                 raise SSLError(e, request=request)
430             elif isinstance(e, ReadTimeoutError):
431                 raise ReadTimeout(e, request=request)
432             else:
433                 raise
434 
435         return self.build_response(request, resp)

</code><intended_behavior>Catch `urllib3` exceptions directly related to HTTP operations, such as timeout errors, connection errors, and decoding errors, and wrap these in the appropriate `requests` exceptions, ensuring that the original exception message and any other relevant diagnostic information are preserved.</intended_behavior>]
2025-10-18 03:38:48.902 | INFO     | Search completed. Bug locations: [<file>requests/adapters.py</file>
<class>None</class>
<method>None</method>
<code>
1 # -*- coding: utf-8 -*-
2 
3 """
4 requests.adapters
5 ~~~~~~~~~~~~~~~~~
6 
7 This module contains the transport adapters that Requests uses to define
8 and maintain connections.
9 """
10 
11 import socket
12 
13 from .models import Response
14 from .packages.urllib3.poolmanager import PoolManager, proxy_from_url
15 from .packages.urllib3.response import HTTPResponse
16 from .packages.urllib3.util import Timeout as TimeoutSauce
17 from .packages.urllib3.util.retry import Retry
18 from .compat import urlparse, basestring
19 from .utils import (DEFAULT_CA_BUNDLE_PATH, get_encoding_from_headers,
20                     prepend_scheme_if_needed, get_auth_from_url, urldefragauth)
21 from .structures import CaseInsensitiveDict
22 from .packages.urllib3.exceptions import ConnectTimeoutError
23 from .packages.urllib3.exceptions import HTTPError as _HTTPError
24 from .packages.urllib3.exceptions import MaxRetryError
25 from .packages.urllib3.exceptions import ProxyError as _ProxyError
26 from .packages.urllib3.exceptions import ProtocolError
27 from .packages.urllib3.exceptions import ReadTimeoutError
28 from .packages.urllib3.exceptions import SSLError as _SSLError
29 from .packages.urllib3.exceptions import ResponseError
30 from .cookies import extract_cookies_to_jar
31 from .exceptions import (ConnectionError, ConnectTimeout, ReadTimeout, SSLError,
32                          ProxyError, RetryError)
33 from .auth import _basic_auth_str
34 
35 DEFAULT_POOLBLOCK = False
36 DEFAULT_POOLSIZE = 10
37 DEFAULT_RETRIES = 0
38 DEFAULT_POOL_TIMEOUT = None
39 
40 
41 class BaseAdapter(object):
42     """The Base Transport Adapter"""
43 
44     def __init__(self):
45         super(BaseAdapter, self).__init__()
46 
47     def send(self):
48         raise NotImplementedError
49 
50     def close(self):
51         raise NotImplementedError
52 
53 
54 class HTTPAdapter(BaseAdapter):
55     """The built-in HTTP Adapter for urllib3.
56 
57     Provides a general-case interface for Requests sessions to contact HTTP and
58     HTTPS urls by implementing the Transport Adapter interface. This class will
59     usually be created by the :class:`Session <Session>` class under the
60     covers.
61 
62     :param pool_connections: The number of urllib3 connection pools to cache.
63     :param pool_maxsize: The maximum number of connections to save in the pool.
64     :param int max_retries: The maximum number of retries each connection
65         should attempt. Note, this applies only to failed DNS lookups, socket
66         connections and connection timeouts, never to requests where data has
67         made it to the server. By default, Requests does not retry failed
68         connections. If you need granular control over the conditions under
69         which we retry a request, import urllib3's ``Retry`` class and pass
70         that instead.
71     :param pool_block: Whether the connection pool should block for connections.
72 
73     Usage::
74 
75       >>> import requests
76       >>> s = requests.Session()
77       >>> a = requests.adapters.HTTPAdapter(max_retries=3)
78       >>> s.mount('http://', a)
79     """
80     __attrs__ = ['max_retries', 'config', '_pool_connections', '_pool_maxsize',
81                  '_pool_block']
82 
83     def __init__(self, pool_connections=DEFAULT_POOLSIZE,
84                  pool_maxsize=DEFAULT_POOLSIZE, max_retries=DEFAULT_RETRIES,
85                  pool_block=DEFAULT_POOLBLOCK):
86         if max_retries == DEFAULT_RETRIES:
87             self.max_retries = Retry(0, read=False)
88         else:
89             self.max_retries = Retry.from_int(max_retries)
90         self.config = {}
91         self.proxy_manager = {}
92 
93         super(HTTPAdapter, self).__init__()
94 
95         self._pool_connections = pool_connections
96         self._pool_maxsize = pool_maxsize
97         self._pool_block = pool_block
98 
99         self.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)
100 
101     def __getstate__(self):
102         return dict((attr, getattr(self, attr, None)) for attr in
103                     self.__attrs__)
104 
105     def __setstate__(self, state):
106         # Can't handle by adding 'proxy_manager' to self.__attrs__ because
107         # because self.poolmanager uses a lambda function, which isn't pickleable.
108         self.proxy_manager = {}
109         self.config = {}
110 
111         for attr, value in state.items():
112             setattr(self, attr, value)
113 
114         self.init_poolmanager(self._pool_connections, self._pool_maxsize,
115                               block=self._pool_block)
116 
117     def init_poolmanager(self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs):
118         """Initializes a urllib3 PoolManager.
119 
120         This method should not be called from user code, and is only
121         exposed for use when subclassing the
122         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
123 
124         :param connections: The number of urllib3 connection pools to cache.
125         :param maxsize: The maximum number of connections to save in the pool.
126         :param block: Block when no free connections are available.
127         :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.
128         """
129         # save these values for pickling
130         self._pool_connections = connections
131         self._pool_maxsize = maxsize
132         self._pool_block = block
133 
134         self.poolmanager = PoolManager(num_pools=connections, maxsize=maxsize,
135                                        block=block, strict=True, **pool_kwargs)
136 
137     def proxy_manager_for(self, proxy, **proxy_kwargs):
138         """Return urllib3 ProxyManager for the given proxy.
139 
140         This method should not be called from user code, and is only
141         exposed for use when subclassing the
142         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
143 
144         :param proxy: The proxy to return a urllib3 ProxyManager for.
145         :param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.
146         :returns: ProxyManager
147         """
148         if not proxy in self.proxy_manager:
149             proxy_headers = self.proxy_headers(proxy)
150             self.proxy_manager[proxy] = proxy_from_url(
151                 proxy,
152                 proxy_headers=proxy_headers,
153                 num_pools=self._pool_connections,
154                 maxsize=self._pool_maxsize,
155                 block=self._pool_block,
156                 **proxy_kwargs)
157 
158         return self.proxy_manager[proxy]
159 
160     def cert_verify(self, conn, url, verify, cert):
161         """Verify a SSL certificate. This method should not be called from user
162         code, and is only exposed for use when subclassing the
163         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
164 
165         :param conn: The urllib3 connection object associated with the cert.
166         :param url: The requested URL.
167         :param verify: Whether we should actually verify the certificate.
168         :param cert: The SSL certificate to verify.
169         """
170         if url.lower().startswith('https') and verify:
171 
172             cert_loc = None
173 
174             # Allow self-specified cert location.
175             if verify is not True:
176                 cert_loc = verify
177 
178             if not cert_loc:
179                 cert_loc = DEFAULT_CA_BUNDLE_PATH
180 
181             if not cert_loc:
182                 raise Exception("Could not find a suitable SSL CA certificate bundle.")
183 
184             conn.cert_reqs = 'CERT_REQUIRED'
185             conn.ca_certs = cert_loc
186         else:
187             conn.cert_reqs = 'CERT_NONE'
188             conn.ca_certs = None
189 
190         if cert:
191             if not isinstance(cert, basestring):
192                 conn.cert_file = cert[0]
193                 conn.key_file = cert[1]
194             else:
195                 conn.cert_file = cert
196 
197     def build_response(self, req, resp):
198         """Builds a :class:`Response <requests.Response>` object from a urllib3
199         response. This should not be called from user code, and is only exposed
200         for use when subclassing the
201         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`
202 
203         :param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
204         :param resp: The urllib3 response object.
205         """
206         response = Response()
207 
208         # Fallback to None if there's no status_code, for whatever reason.
209         response.status_code = getattr(resp, 'status', None)
210 
211         # Make headers case-insensitive.
212         response.headers = CaseInsensitiveDict(getattr(resp, 'headers', {}))
213 
214         # Set encoding.
215         response.encoding = get_encoding_from_headers(response.headers)
216         response.raw = resp
217         response.reason = response.raw.reason
218 
219         if isinstance(req.url, bytes):
220             response.url = req.url.decode('utf-8')
221         else:
222             response.url = req.url
223 
224         # Add new cookies from the server.
225         extract_cookies_to_jar(response.cookies, req, resp)
226 
227         # Give the Response some context.
228         response.request = req
229         response.connection = self
230 
231         return response
232 
233     def get_connection(self, url, proxies=None):
234         """Returns a urllib3 connection for the given URL. This should not be
235         called from user code, and is only exposed for use when subclassing the
236         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
237 
238         :param url: The URL to connect to.
239         :param proxies: (optional) A Requests-style dictionary of proxies used on this request.
240         """
241         proxies = proxies or {}
242         proxy = proxies.get(urlparse(url.lower()).scheme)
243 
244         if proxy:
245             proxy = prepend_scheme_if_needed(proxy, 'http')
246             proxy_manager = self.proxy_manager_for(proxy)
247             conn = proxy_manager.connection_from_url(url)
248         else:
249             # Only scheme should be lower case
250             parsed = urlparse(url)
251             url = parsed.geturl()
252             conn = self.poolmanager.connection_from_url(url)
253 
254         return conn
255 
256     def close(self):
257         """Disposes of any internal state.
258 
259         Currently, this just closes the PoolManager, which closes pooled
260         connections.
261         """
262         self.poolmanager.clear()
263 
264     def request_url(self, request, proxies):
265         """Obtain the url to use when making the final request.
266 
267         If the message is being sent through a HTTP proxy, the full URL has to
268         be used. Otherwise, we should only use the path portion of the URL.
269 
270         This should not be called from user code, and is only exposed for use
271         when subclassing the
272         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
273 
274         :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
275         :param proxies: A dictionary of schemes to proxy URLs.
276         """
277         proxies = proxies or {}
278         scheme = urlparse(request.url).scheme
279         proxy = proxies.get(scheme)
280 
281         if proxy and scheme != 'https':
282             url = urldefragauth(request.url)
283         else:
284             url = request.path_url
285 
286         return url
287 
288     def add_headers(self, request, **kwargs):
289         """Add any headers needed by the connection. As of v2.0 this does
290         nothing by default, but is left for overriding by users that subclass
291         the :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
292 
293         This should not be called from user code, and is only exposed for use
294         when subclassing the
295         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
296 
297         :param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.
298         :param kwargs: The keyword arguments from the call to send().
299         """
300         pass
301 
302     def proxy_headers(self, proxy):
303         """Returns a dictionary of the headers to add to any request sent
304         through a proxy. This works with urllib3 magic to ensure that they are
305         correctly sent to the proxy, rather than in a tunnelled request if
306         CONNECT is being used.
307 
308         This should not be called from user code, and is only exposed for use
309         when subclassing the
310         :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.
311 
312         :param proxies: The url of the proxy being used for this request.
313         :param kwargs: Optional additional keyword arguments.
314         """
315         headers = {}
316         username, password = get_auth_from_url(proxy)
317 
318         if username and password:
319             headers['Proxy-Authorization'] = _basic_auth_str(username,
320                                                              password)
321 
322         return headers
323 
324     def send(self, request, stream=False, timeout=None, verify=True, cert=None, proxies=None):
325         """Sends PreparedRequest object. Returns Response object.
326 
327         :param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
328         :param stream: (optional) Whether to stream the request content.
329         :param timeout: (optional) How long to wait for the server to send
330             data before giving up, as a float, or a (`connect timeout, read
331             timeout <user/advanced.html#timeouts>`_) tuple.
332         :type timeout: float or tuple
333         :param verify: (optional) Whether to verify SSL certificates.
334         :param cert: (optional) Any user-provided SSL certificate to be trusted.
335         :param proxies: (optional) The proxies dictionary to apply to the request.
336         """
337 
338         conn = self.get_connection(request.url, proxies)
339 
340         self.cert_verify(conn, request.url, verify, cert)
341         url = self.request_url(request, proxies)
342         self.add_headers(request)
343 
344         chunked = not (request.body is None or 'Content-Length' in request.headers)
345 
346         if isinstance(timeout, tuple):
347             try:
348                 connect, read = timeout
349                 timeout = TimeoutSauce(connect=connect, read=read)
350             except ValueError as e:
351                 # this may raise a string formatting error.
352                 err = ("Invalid timeout {0}. Pass a (connect, read) "
353                        "timeout tuple, or a single float to set "
354                        "both timeouts to the same value".format(timeout))
355                 raise ValueError(err)
356         else:
357             timeout = TimeoutSauce(connect=timeout, read=timeout)
358 
359         try:
360             if not chunked:
361                 resp = conn.urlopen(
362                     method=request.method,
363                     url=url,
364                     body=request.body,
365                     headers=request.headers,
366                     redirect=False,
367                     assert_same_host=False,
368                     preload_content=False,
369                     decode_content=False,
370                     retries=self.max_retries,
371                     timeout=timeout
372                 )
373 
374             # Send the request.
375             else:
376                 if hasattr(conn, 'proxy_pool'):
377                     conn = conn.proxy_pool
378 
379                 low_conn = conn._get_conn(timeout=DEFAULT_POOL_TIMEOUT)
380 
381                 try:
382                     low_conn.putrequest(request.method,
383                                         url,
384                                         skip_accept_encoding=True)
385 
386                     for header, value in request.headers.items():
387                         low_conn.putheader(header, value)
388 
389                     low_conn.endheaders()
390 
391                     for i in request.body:
392                         low_conn.send(hex(len(i))[2:].encode('utf-8'))
393                         low_conn.send(b'\r\n')
394                         low_conn.send(i)
395                         low_conn.send(b'\r\n')
396                     low_conn.send(b'0\r\n\r\n')
397 
398                     r = low_conn.getresponse()
399                     resp = HTTPResponse.from_httplib(
400                         r,
401                         pool=conn,
402                         connection=low_conn,
403                         preload_content=False,
404                         decode_content=False
405                     )
406                 except:
407                     # If we hit any problems here, clean up the connection.
408                     # Then, reraise so that we can handle the actual exception.
409                     low_conn.close()
410                     raise
411 
412         except (ProtocolError, socket.error) as err:
413             raise ConnectionError(err, request=request)
414 
415         except MaxRetryError as e:
416             if isinstance(e.reason, ConnectTimeoutError):
417                 raise ConnectTimeout(e, request=request)
418 
419             if isinstance(e.reason, ResponseError):
420                 raise RetryError(e, request=request)
421 
422             raise ConnectionError(e, request=request)
423 
424         except _ProxyError as e:
425             raise ProxyError(e)
426 
427         except (_SSLError, _HTTPError) as e:
428             if isinstance(e, _SSLError):
429                 raise SSLError(e, request=request)
430             elif isinstance(e, ReadTimeoutError):
431                 raise ReadTimeout(e, request=request)
432             else:
433                 raise
434 
435         return self.build_response(request, resp)

</code><intended_behavior>Catch `urllib3` exceptions directly related to HTTP operations, such as timeout errors, connection errors, and decoding errors, and wrap these in the appropriate `requests` exceptions, ensuring that the original exception message and any other relevant diagnostic information are preserved.</intended_behavior>]
2025-10-18 03:38:48.902 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 03:38:48.902 | INFO     | Search-only: skipping patch selection
2025-10-18 03:38:48.966 | INFO     | Task psf__requests-2674 completed successfully.
2025-10-18 03:38:48.967 | INFO     | No patch generated. You can try running ACR again.
