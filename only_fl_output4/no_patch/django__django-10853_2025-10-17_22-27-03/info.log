2025-10-17 22:27:04.397 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 22:27:07.200 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=500, output_tokens=12, cost=0.005360
2025-10-17 22:27:20.820 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=680, output_tokens=548, cost=0.023240
2025-10-17 22:27:21.798 | INFO     | registered non reproducing test 0
2025-10-17 22:27:39.164 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1511, output_tokens=792, cost=0.038870
2025-10-17 22:27:40.144 | INFO     | registered non reproducing test 1
2025-10-17 22:27:54.369 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1755, output_tokens=640, cost=0.036750
2025-10-17 22:27:55.330 | INFO     | registered non reproducing test 2
2025-10-17 22:27:55.330 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-17 22:27:55.336 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 22:28:05.649 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=809, output_tokens=418, cost=0.020630
2025-10-17 22:28:05.658 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 22:28:08.849 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=772, output_tokens=81, cost=0.010150
2025-10-17 22:28:08.851 | DEBUG    | Extracted a valid json.
2025-10-17 22:28:08.851 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"django.db.utils.OperationalError\")",
        "search_method(\"execute\")",
        "search_class(\"Window\")",
        "search_class(\"RowNumber\")",
        "search_method(\"annotate\")",
        "search_method(\"features\")",
        "search_method(\"check_feature_support\")"
    ],
    "bug_locations": []
}
2025-10-17 22:28:08.923 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 22:28:08.924 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 22:28:24.925 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2844, output_tokens=801, cost=0.052470
2025-10-17 22:28:24.940 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 22:28:33.308 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3861, output_tokens=315, cost=0.048060
2025-10-17 22:28:33.314 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 22:28:41.836 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=669, output_tokens=342, cost=0.016950
2025-10-17 22:28:41.837 | DEBUG    | Extracted a valid json.
2025-10-17 22:28:41.837 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/backends/sqlite3/base.py",
            "class": "",
            "method": "",
            "intended_behavior": "To resolve the issue, a mechanism should be introduced within the SQLite backend, particularly in the initial setup or query execution pathways that checks the SQLite version in use. If the version is older than 3.25.0, the backend should mark window functions as unsupported. This can be done by setting a feature flag specifically for window functions within the `features` property of the backend class. Before attempting to execute a query involving window functions, the Django ORM should check this feature flag. If unsupported, a descriptive error should be raised informing the developer that their current SQLite version does not support window functions."
        },
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "annotate",
            "intended_behavior": "In the `annotate` method, when annotations that involve window functions are being added to a queryset, the ORM should verify if the underlying database engine (in contexts where SQLite is used) supports window functions. This could be achieved by consulting a feature flag (as described previously) indicating the availability of window functions. If the feature is unsupported, this method should raise a Django-specific exception (e.g., `FeatureNotSupportedError`) with a clear message indicating the absence of window function support in the SQLite version being used. This check would help developers understand the limitations of their current environment, rather than facing a low-level `OperationalError` from the SQLite engine."
        }
    ]
}
2025-10-17 22:28:41.855 | DEBUG    | Bug location extracted successfully: [<file>django/db/backends/sqlite3/base.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 SQLite backend for the sqlite3 module in the standard library.
3 """
4 import datetime
5 import decimal
6 import functools
7 import math
8 import operator
9 import re
10 import statistics
11 import warnings
12 from itertools import chain
13 from sqlite3 import dbapi2 as Database
14 
15 import pytz
16 
17 from django.core.exceptions import ImproperlyConfigured
18 from django.db import utils
19 from django.db.backends import utils as backend_utils
20 from django.db.backends.base.base import BaseDatabaseWrapper
21 from django.utils import timezone
22 from django.utils.dateparse import parse_datetime, parse_time
23 from django.utils.duration import duration_microseconds
24 
25 from .client import DatabaseClient                          # isort:skip
26 from .creation import DatabaseCreation                      # isort:skip
27 from .features import DatabaseFeatures                      # isort:skip
28 from .introspection import DatabaseIntrospection            # isort:skip
29 from .operations import DatabaseOperations                  # isort:skip
30 from .schema import DatabaseSchemaEditor                    # isort:skip
31 
32 
33 def decoder(conv_func):
34     """
35     Convert bytestrings from Python's sqlite3 interface to a regular string.
36     """
37     return lambda s: conv_func(s.decode())
38 
39 
40 def none_guard(func):
41     """
42     Decorator that returns None if any of the arguments to the decorated
43     function are None. Many SQL functions return NULL if any of their arguments
44     are NULL. This decorator simplifies the implementation of this for the
45     custom functions registered below.
46     """
47     @functools.wraps(func)
48     def wrapper(*args, **kwargs):
49         return None if None in args else func(*args, **kwargs)
50     return wrapper
51 
52 
53 def list_aggregate(function):
54     """
55     Return an aggregate class that accumulates values in a list and applies
56     the provided function to the data.
57     """
58     return type('ListAggregate', (list,), {'finalize': function, 'step': list.append})
59 
60 
61 def check_sqlite_version():
62     if Database.sqlite_version_info < (3, 8, 3):
63         raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)
64 
65 
66 check_sqlite_version()
67 
68 Database.register_converter("bool", b'1'.__eq__)
69 Database.register_converter("time", decoder(parse_time))
70 Database.register_converter("datetime", decoder(parse_datetime))
71 Database.register_converter("timestamp", decoder(parse_datetime))
72 Database.register_converter("TIMESTAMP", decoder(parse_datetime))
73 
74 Database.register_adapter(decimal.Decimal, str)
75 
76 
77 class DatabaseWrapper(BaseDatabaseWrapper):
78     vendor = 'sqlite'
79     display_name = 'SQLite'
80     # SQLite doesn't actually support most of these types, but it "does the right
81     # thing" given more verbose field definitions, so leave them as is so that
82     # schema inspection is more useful.
83     data_types = {
84         'AutoField': 'integer',
85         'BigAutoField': 'integer',
86         'BinaryField': 'BLOB',
87         'BooleanField': 'bool',
88         'CharField': 'varchar(%(max_length)s)',
89         'DateField': 'date',
90         'DateTimeField': 'datetime',
91         'DecimalField': 'decimal',
92         'DurationField': 'bigint',
93         'FileField': 'varchar(%(max_length)s)',
94         'FilePathField': 'varchar(%(max_length)s)',
95         'FloatField': 'real',
96         'IntegerField': 'integer',
97         'BigIntegerField': 'bigint',
98         'IPAddressField': 'char(15)',
99         'GenericIPAddressField': 'char(39)',
100         'NullBooleanField': 'bool',
101         'OneToOneField': 'integer',
102         'PositiveIntegerField': 'integer unsigned',
103         'PositiveSmallIntegerField': 'smallint unsigned',
104         'SlugField': 'varchar(%(max_length)s)',
105         'SmallIntegerField': 'smallint',
106         'TextField': 'text',
107         'TimeField': 'time',
108         'UUIDField': 'char(32)',
109     }
110     data_type_check_constraints = {
111         'PositiveIntegerField': '"%(column)s" >= 0',
112         'PositiveSmallIntegerField': '"%(column)s" >= 0',
113     }
114     data_types_suffix = {
115         'AutoField': 'AUTOINCREMENT',
116         'BigAutoField': 'AUTOINCREMENT',
117     }
118     # SQLite requires LIKE statements to include an ESCAPE clause if the value
119     # being escaped has a percent or underscore in it.
120     # See https://www.sqlite.org/lang_expr.html for an explanation.
121     operators = {
122         'exact': '= %s',
123         'iexact': "LIKE %s ESCAPE '\\'",
124         'contains': "LIKE %s ESCAPE '\\'",
125         'icontains': "LIKE %s ESCAPE '\\'",
126         'regex': 'REGEXP %s',
127         'iregex': "REGEXP '(?i)' || %s",
128         'gt': '> %s',
129         'gte': '>= %s',
130         'lt': '< %s',
131         'lte': '<= %s',
132         'startswith': "LIKE %s ESCAPE '\\'",
133         'endswith': "LIKE %s ESCAPE '\\'",
134         'istartswith': "LIKE %s ESCAPE '\\'",
135         'iendswith': "LIKE %s ESCAPE '\\'",
136     }
137 
138     # The patterns below are used to generate SQL pattern lookup clauses when
139     # the right-hand side of the lookup isn't a raw string (it might be an expression
140     # or the result of a bilateral transformation).
141     # In those cases, special characters for LIKE operators (e.g. \, *, _) should be
142     # escaped on database side.
143     #
144     # Note: we use str.format() here for readability as '%' is used as a wildcard for
145     # the LIKE operator.
146     pattern_esc = r"REPLACE(REPLACE(REPLACE({}, '\', '\\'), '%%', '\%%'), '_', '\_')"
147     pattern_ops = {
148         'contains': r"LIKE '%%' || {} || '%%' ESCAPE '\'",
149         'icontains': r"LIKE '%%' || UPPER({}) || '%%' ESCAPE '\'",
150         'startswith': r"LIKE {} || '%%' ESCAPE '\'",
151         'istartswith': r"LIKE UPPER({}) || '%%' ESCAPE '\'",
152         'endswith': r"LIKE '%%' || {} ESCAPE '\'",
153         'iendswith': r"LIKE '%%' || UPPER({}) ESCAPE '\'",
154     }
155 
156     Database = Database
157     SchemaEditorClass = DatabaseSchemaEditor
158     # Classes instantiated in __init__().
159     client_class = DatabaseClient
160     creation_class = DatabaseCreation
161     features_class = DatabaseFeatures
162     introspection_class = DatabaseIntrospection
163     ops_class = DatabaseOperations
164 
165     def get_connection_params(self):
166         settings_dict = self.settings_dict
167         if not settings_dict['NAME']:
168             raise ImproperlyConfigured(
169                 "settings.DATABASES is improperly configured. "
170                 "Please supply the NAME value.")
171         kwargs = {
172             'database': settings_dict['NAME'],
173             'detect_types': Database.PARSE_DECLTYPES | Database.PARSE_COLNAMES,
174             **settings_dict['OPTIONS'],
175         }
176         # Always allow the underlying SQLite connection to be shareable
177         # between multiple threads. The safe-guarding will be handled at a
178         # higher level by the `BaseDatabaseWrapper.allow_thread_sharing`
179         # property. This is necessary as the shareability is disabled by
180         # default in pysqlite and it cannot be changed once a connection is
181         # opened.
182         if 'check_same_thread' in kwargs and kwargs['check_same_thread']:
183             warnings.warn(
184                 'The `check_same_thread` option was provided and set to '
185                 'True. It will be overridden with False. Use the '
186                 '`DatabaseWrapper.allow_thread_sharing` property instead '
187                 'for controlling thread shareability.',
188                 RuntimeWarning
189             )
190         kwargs.update({'check_same_thread': False, 'uri': True})
191         return kwargs
192 
193     def get_new_connection(self, conn_params):
194         conn = Database.connect(**conn_params)
195         conn.create_function("django_date_extract", 2, _sqlite_datetime_extract)
196         conn.create_function("django_date_trunc", 2, _sqlite_date_trunc)
197         conn.create_function("django_datetime_cast_date", 2, _sqlite_datetime_cast_date)
198         conn.create_function("django_datetime_cast_time", 2, _sqlite_datetime_cast_time)
199         conn.create_function("django_datetime_extract", 3, _sqlite_datetime_extract)
200         conn.create_function("django_datetime_trunc", 3, _sqlite_datetime_trunc)
201         conn.create_function("django_time_extract", 2, _sqlite_time_extract)
202         conn.create_function("django_time_trunc", 2, _sqlite_time_trunc)
203         conn.create_function("django_time_diff", 2, _sqlite_time_diff)
204         conn.create_function("django_timestamp_diff", 2, _sqlite_timestamp_diff)
205         conn.create_function("django_format_dtdelta", 3, _sqlite_format_dtdelta)
206         conn.create_function('regexp', 2, _sqlite_regexp)
207         conn.create_function('ACOS', 1, none_guard(math.acos))
208         conn.create_function('ASIN', 1, none_guard(math.asin))
209         conn.create_function('ATAN', 1, none_guard(math.atan))
210         conn.create_function('ATAN2', 2, none_guard(math.atan2))
211         conn.create_function('CEILING', 1, none_guard(math.ceil))
212         conn.create_function('COS', 1, none_guard(math.cos))
213         conn.create_function('COT', 1, none_guard(lambda x: 1 / math.tan(x)))
214         conn.create_function('DEGREES', 1, none_guard(math.degrees))
215         conn.create_function('EXP', 1, none_guard(math.exp))
216         conn.create_function('FLOOR', 1, none_guard(math.floor))
217         conn.create_function('LN', 1, none_guard(math.log))
218         conn.create_function('LOG', 2, none_guard(lambda x, y: math.log(y, x)))
219         conn.create_function('LPAD', 3, _sqlite_lpad)
220         conn.create_function('MOD', 2, none_guard(math.fmod))
221         conn.create_function('PI', 0, lambda: math.pi)
222         conn.create_function('POWER', 2, none_guard(operator.pow))
223         conn.create_function('RADIANS', 1, none_guard(math.radians))
224         conn.create_function('REPEAT', 2, none_guard(operator.mul))
225         conn.create_function('REVERSE', 1, none_guard(lambda x: x[::-1]))
226         conn.create_function('RPAD', 3, _sqlite_rpad)
227         conn.create_function('SIN', 1, none_guard(math.sin))
228         conn.create_function('SQRT', 1, none_guard(math.sqrt))
229         conn.create_function('TAN', 1, none_guard(math.tan))
230         conn.create_aggregate('STDDEV_POP', 1, list_aggregate(statistics.pstdev))
231         conn.create_aggregate('STDDEV_SAMP', 1, list_aggregate(statistics.stdev))
232         conn.create_aggregate('VAR_POP', 1, list_aggregate(statistics.pvariance))
233         conn.create_aggregate('VAR_SAMP', 1, list_aggregate(statistics.variance))
234         conn.execute('PRAGMA foreign_keys = ON')
235         return conn
236 
237     def init_connection_state(self):
238         pass
239 
240     def create_cursor(self, name=None):
241         return self.connection.cursor(factory=SQLiteCursorWrapper)
242 
243     def close(self):
244         self.validate_thread_sharing()
245         # If database is in memory, closing the connection destroys the
246         # database. To prevent accidental data loss, ignore close requests on
247         # an in-memory db.
248         if not self.is_in_memory_db():
249             BaseDatabaseWrapper.close(self)
250 
251     def _savepoint_allowed(self):
252         # When 'isolation_level' is not None, sqlite3 commits before each
253         # savepoint; it's a bug. When it is None, savepoints don't make sense
254         # because autocommit is enabled. The only exception is inside 'atomic'
255         # blocks. To work around that bug, on SQLite, 'atomic' starts a
256         # transaction explicitly rather than simply disable autocommit.
257         return self.in_atomic_block
258 
259     def _set_autocommit(self, autocommit):
260         if autocommit:
261             level = None
262         else:
263             # sqlite3's internal default is ''. It's different from None.
264             # See Modules/_sqlite/connection.c.
265             level = ''
266         # 'isolation_level' is a misleading API.
267         # SQLite always runs at the SERIALIZABLE isolation level.
268         with self.wrap_database_errors:
269             self.connection.isolation_level = level
270 
271     def disable_constraint_checking(self):
272         with self.cursor() as cursor:
273             cursor.execute('PRAGMA foreign_keys = OFF')
274             # Foreign key constraints cannot be turned off while in a multi-
275             # statement transaction. Fetch the current state of the pragma
276             # to determine if constraints are effectively disabled.
277             enabled = cursor.execute('PRAGMA foreign_keys').fetchone()[0]
278         return not bool(enabled)
279 
280     def enable_constraint_checking(self):
281         self.cursor().execute('PRAGMA foreign_keys = ON')
282 
283     def check_constraints(self, table_names=None):
284         """
285         Check each table name in `table_names` for rows with invalid foreign
286         key references. This method is intended to be used in conjunction with
287         `disable_constraint_checking()` and `enable_constraint_checking()`, to
288         determine if rows with invalid references were entered while constraint
289         checks were off.
290         """
291         if self.features.supports_pragma_foreign_key_check:
292             with self.cursor() as cursor:
293                 if table_names is None:
294                     violations = self.cursor().execute('PRAGMA foreign_key_check').fetchall()
295                 else:
296                     violations = chain.from_iterable(
297                         cursor.execute('PRAGMA foreign_key_check(%s)' % table_name).fetchall()
298                         for table_name in table_names
299                     )
300                 # See https://www.sqlite.org/pragma.html#pragma_foreign_key_check
301                 for table_name, rowid, referenced_table_name, foreign_key_index in violations:
302                     foreign_key = cursor.execute(
303                         'PRAGMA foreign_key_list(%s)' % table_name
304                     ).fetchall()[foreign_key_index]
305                     column_name, referenced_column_name = foreign_key[3:5]
306                     primary_key_column_name = self.introspection.get_primary_key_column(cursor, table_name)
307                     primary_key_value, bad_value = cursor.execute(
308                         'SELECT %s, %s FROM %s WHERE rowid = %%s' % (
309                             primary_key_column_name, column_name, table_name
310                         ),
311                         (rowid,),
312                     ).fetchone()
313                     raise utils.IntegrityError(
314                         "The row in table '%s' with primary key '%s' has an "
315                         "invalid foreign key: %s.%s contains a value '%s' that "
316                         "does not have a corresponding value in %s.%s." % (
317                             table_name, primary_key_value, table_name, column_name,
318                             bad_value, referenced_table_name, referenced_column_name
319                         )
320                     )
321         else:
322             with self.cursor() as cursor:
323                 if table_names is None:
324                     table_names = self.introspection.table_names(cursor)
325                 for table_name in table_names:
326                     primary_key_column_name = self.introspection.get_primary_key_column(cursor, table_name)
327                     if not primary_key_column_name:
328                         continue
329                     key_columns = self.introspection.get_key_columns(cursor, table_name)
330                     for column_name, referenced_table_name, referenced_column_name in key_columns:
331                         cursor.execute(
332                             """
333                             SELECT REFERRING.`%s`, REFERRING.`%s` FROM `%s` as REFERRING
334                             LEFT JOIN `%s` as REFERRED
335                             ON (REFERRING.`%s` = REFERRED.`%s`)
336                             WHERE REFERRING.`%s` IS NOT NULL AND REFERRED.`%s` IS NULL
337                             """
338                             % (
339                                 primary_key_column_name, column_name, table_name,
340                                 referenced_table_name, column_name, referenced_column_name,
341                                 column_name, referenced_column_name,
342                             )
343                         )
344                         for bad_row in cursor.fetchall():
345                             raise utils.IntegrityError(
346                                 "The row in table '%s' with primary key '%s' has an "
347                                 "invalid foreign key: %s.%s contains a value '%s' that "
348                                 "does not have a corresponding value in %s.%s." % (
349                                     table_name, bad_row[0], table_name, column_name,
350                                     bad_row[1], referenced_table_name, referenced_column_name,
351                                 )
352                             )
353 
354     def is_usable(self):
355         return True
356 
357     def _start_transaction_under_autocommit(self):
358         """
359         Start a transaction explicitly in autocommit mode.
360 
361         Staying in autocommit mode works around a bug of sqlite3 that breaks
362         savepoints when autocommit is disabled.
363         """
364         self.cursor().execute("BEGIN")
365 
366     def is_in_memory_db(self):
367         return self.creation.is_in_memory_db(self.settings_dict['NAME'])
368 
369 
370 FORMAT_QMARK_REGEX = re.compile(r'(?<!%)%s')
371 
372 
373 class SQLiteCursorWrapper(Database.Cursor):
374     """
375     Django uses "format" style placeholders, but pysqlite2 uses "qmark" style.
376     This fixes it -- but note that if you want to use a literal "%s" in a query,
377     you'll need to use "%%s".
378     """
379     def execute(self, query, params=None):
380         if params is None:
381             return Database.Cursor.execute(self, query)
382         query = self.convert_query(query)
383         return Database.Cursor.execute(self, query, params)
384 
385     def executemany(self, query, param_list):
386         query = self.convert_query(query)
387         return Database.Cursor.executemany(self, query, param_list)
388 
389     def convert_query(self, query):
390         return FORMAT_QMARK_REGEX.sub('?', query).replace('%%', '%')
391 
392 
393 def _sqlite_datetime_parse(dt, tzname=None):
394     if dt is None:
395         return None
396     try:
397         dt = backend_utils.typecast_timestamp(dt)
398     except (TypeError, ValueError):
399         return None
400     if tzname is not None:
401         dt = timezone.localtime(dt, pytz.timezone(tzname))
402     return dt
403 
404 
405 def _sqlite_date_trunc(lookup_type, dt):
406     dt = _sqlite_datetime_parse(dt)
407     if dt is None:
408         return None
409     if lookup_type == 'year':
410         return "%i-01-01" % dt.year
411     elif lookup_type == 'quarter':
412         month_in_quarter = dt.month - (dt.month - 1) % 3
413         return '%i-%02i-01' % (dt.year, month_in_quarter)
414     elif lookup_type == 'month':
415         return "%i-%02i-01" % (dt.year, dt.month)
416     elif lookup_type == 'week':
417         dt = dt - datetime.timedelta(days=dt.weekday())
418         return "%i-%02i-%02i" % (dt.year, dt.month, dt.day)
419     elif lookup_type == 'day':
420         return "%i-%02i-%02i" % (dt.year, dt.month, dt.day)
421 
422 
423 def _sqlite_time_trunc(lookup_type, dt):
424     if dt is None:
425         return None
426     try:
427         dt = backend_utils.typecast_time(dt)
428     except (ValueError, TypeError):
429         return None
430     if lookup_type == 'hour':
431         return "%02i:00:00" % dt.hour
432     elif lookup_type == 'minute':
433         return "%02i:%02i:00" % (dt.hour, dt.minute)
434     elif lookup_type == 'second':
435         return "%02i:%02i:%02i" % (dt.hour, dt.minute, dt.second)
436 
437 
438 def _sqlite_datetime_cast_date(dt, tzname):
439     dt = _sqlite_datetime_parse(dt, tzname)
440     if dt is None:
441         return None
442     return dt.date().isoformat()
443 
444 
445 def _sqlite_datetime_cast_time(dt, tzname):
446     dt = _sqlite_datetime_parse(dt, tzname)
447     if dt is None:
448         return None
449     return dt.time().isoformat()
450 
451 
452 def _sqlite_datetime_extract(lookup_type, dt, tzname=None):
453     dt = _sqlite_datetime_parse(dt, tzname)
454     if dt is None:
455         return None
456     if lookup_type == 'week_day':
457         return (dt.isoweekday() % 7) + 1
458     elif lookup_type == 'week':
459         return dt.isocalendar()[1]
460     elif lookup_type == 'quarter':
461         return math.ceil(dt.month / 3)
462     elif lookup_type == 'iso_year':
463         return dt.isocalendar()[0]
464     else:
465         return getattr(dt, lookup_type)
466 
467 
468 def _sqlite_datetime_trunc(lookup_type, dt, tzname):
469     dt = _sqlite_datetime_parse(dt, tzname)
470     if dt is None:
471         return None
472     if lookup_type == 'year':
473         return "%i-01-01 00:00:00" % dt.year
474     elif lookup_type == 'quarter':
475         month_in_quarter = dt.month - (dt.month - 1) % 3
476         return '%i-%02i-01 00:00:00' % (dt.year, month_in_quarter)
477     elif lookup_type == 'month':
478         return "%i-%02i-01 00:00:00" % (dt.year, dt.month)
479     elif lookup_type == 'week':
480         dt = dt - datetime.timedelta(days=dt.weekday())
481         return "%i-%02i-%02i 00:00:00" % (dt.year, dt.month, dt.day)
482     elif lookup_type == 'day':
483         return "%i-%02i-%02i 00:00:00" % (dt.year, dt.month, dt.day)
484     elif lookup_type == 'hour':
485         return "%i-%02i-%02i %02i:00:00" % (dt.year, dt.month, dt.day, dt.hour)
486     elif lookup_type == 'minute':
487         return "%i-%02i-%02i %02i:%02i:00" % (dt.year, dt.month, dt.day, dt.hour, dt.minute)
488     elif lookup_type == 'second':
489         return "%i-%02i-%02i %02i:%02i:%02i" % (dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)
490 
491 
492 def _sqlite_time_extract(lookup_type, dt):
493     if dt is None:
494         return None
495     try:
496         dt = backend_utils.typecast_time(dt)
497     except (ValueError, TypeError):
498         return None
499     return getattr(dt, lookup_type)
500 
501 
502 @none_guard
503 def _sqlite_format_dtdelta(conn, lhs, rhs):
504     """
505     LHS and RHS can be either:
506     - An integer number of microseconds
507     - A string representing a datetime
508     """
509     try:
510         real_lhs = datetime.timedelta(0, 0, lhs) if isinstance(lhs, int) else backend_utils.typecast_timestamp(lhs)
511         real_rhs = datetime.timedelta(0, 0, rhs) if isinstance(rhs, int) else backend_utils.typecast_timestamp(rhs)
512         if conn.strip() == '+':
513             out = real_lhs + real_rhs
514         else:
515             out = real_lhs - real_rhs
516     except (ValueError, TypeError):
517         return None
518     # typecast_timestamp returns a date or a datetime without timezone.
519     # It will be formatted as "%Y-%m-%d" or "%Y-%m-%d %H:%M:%S[.%f]"
520     return str(out)
521 
522 
523 @none_guard
524 def _sqlite_time_diff(lhs, rhs):
525     left = backend_utils.typecast_time(lhs)
526     right = backend_utils.typecast_time(rhs)
527     return (
528         (left.hour * 60 * 60 * 1000000) +
529         (left.minute * 60 * 1000000) +
530         (left.second * 1000000) +
531         (left.microsecond) -
532         (right.hour * 60 * 60 * 1000000) -
533         (right.minute * 60 * 1000000) -
534         (right.second * 1000000) -
535         (right.microsecond)
536     )
537 
538 
539 @none_guard
540 def _sqlite_timestamp_diff(lhs, rhs):
541     left = backend_utils.typecast_timestamp(lhs)
542     right = backend_utils.typecast_timestamp(rhs)
543     return duration_microseconds(left - right)
544 
545 
546 @none_guard
547 def _sqlite_regexp(re_pattern, re_string):
548     return bool(re.search(re_pattern, str(re_string)))
549 
550 
551 @none_guard
552 def _sqlite_lpad(text, length, fill_text):
553     if len(text) >= length:
554         return text[:length]
555     return (fill_text * length)[:length - len(text)] + text
556 
557 
558 @none_guard
559 def _sqlite_rpad(text, length, fill_text):
560     return (text + fill_text * length)[:length]

</code><intended_behavior>To resolve the issue, a mechanism should be introduced within the SQLite backend, particularly in the initial setup or query execution pathways that checks the SQLite version in use. If the version is older than 3.25.0, the backend should mark window functions as unsupported. This can be done by setting a feature flag specifically for window functions within the `features` property of the backend class. Before attempting to execute a query involving window functions, the Django ORM should check this feature flag. If unsupported, a descriptive error should be raised informing the developer that their current SQLite version does not support window functions.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone

</code><intended_behavior>In the `annotate` method, when annotations that involve window functions are being added to a queryset, the ORM should verify if the underlying database engine (in contexts where SQLite is used) supports window functions. This could be achieved by consulting a feature flag (as described previously) indicating the availability of window functions. If the feature is unsupported, this method should raise a Django-specific exception (e.g., `FeatureNotSupportedError`) with a clear message indicating the absence of window function support in the SQLite version being used. This check would help developers understand the limitations of their current environment, rather than facing a low-level `OperationalError` from the SQLite engine.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "…(remaining elements truncated)…"
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError
284         assert ((not isinstance(k, slice) and (k >= 0)) or
285                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
286                  (k.stop is None or k.stop >= 0))), \
287             "Negative indexing is not supported."
288 
289         if self._result_cache is not None:
290             return self._result_cache[k]
291 
292         if isinstance(k, slice):
293             qs = self._chain()
294             if k.start is not None:
295                 start = int(k.start)
296             else:
297                 start = None
298             if k.stop is not None:
299                 stop = int(k.stop)
300             else:
301                 stop = None
302             qs.query.set_limits(start, stop)
303             return list(qs)[::k.step] if k.step else qs
304 
305         qs = self._chain()
306         qs.query.set_limits(k, k + 1)
307         qs._fetch_all()
308         return qs._result_cache[0]
309 
310     def __and__(self, other):
311         self._merge_sanity_check(other)
312         if isinstance(other, EmptyQuerySet):
313             return other
314         if isinstance(self, EmptyQuerySet):
315             return self
316         combined = self._chain()
317         combined._merge_known_related_objects(other)
318         combined.query.combine(other.query, sql.AND)
319         return combined
320 
321     def __or__(self, other):
322         self._merge_sanity_check(other)
323         if isinstance(self, EmptyQuerySet):
324             return other
325         if isinstance(other, EmptyQuerySet):
326             return self
327         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
328         combined = query._chain()
329         combined._merge_known_related_objects(other)
330         if not other.query.can_filter():
331             other = other.model._base_manager.filter(pk__in=other.values('pk'))
332         combined.query.combine(other.query, sql.OR)
333         return combined
334 
335     ####################################
336     # METHODS THAT DO DATABASE QUERIES #
337     ####################################
338 
339     def _iterator(self, use_chunked_fetch, chunk_size):
340         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
341 
342     def iterator(self, chunk_size=2000):
343         """
344         An iterator over the results from applying this QuerySet to the
345         database.
346         """
347         if chunk_size <= 0:
348             raise ValueError('Chunk size must be strictly positive.')
349         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
350         return self._iterator(use_chunked_fetch, chunk_size)
351 
352     def aggregate(self, *args, **kwargs):
353         """
354         Return a dictionary containing the calculations (aggregation)
355         over the current queryset.
356 
357         If args is present the expression is passed as a kwarg using
358         the Aggregate object's default alias.
359         """
360         if self.query.distinct_fields:
361             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
362         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
363         for arg in args:
364             # The default_alias property raises TypeError if default_alias
365             # can't be set automatically or AttributeError if it isn't an
366             # attribute.
367             try:
368                 arg.default_alias
369             except (AttributeError, TypeError):
370                 raise TypeError("Complex aggregates require an alias")
371             kwargs[arg.default_alias] = arg
372 
373         query = self.query.chain()
374         for (alias, aggregate_expr) in kwargs.items():
375             query.add_annotation(aggregate_expr, alias, is_summary=True)
376             if not query.annotations[alias].contains_aggregate:
377                 raise TypeError("%s is not an aggregate expression" % alias)
378         return query.get_aggregation(self.db, kwargs)
379 
380     def count(self):
381         """
382         Perform a SELECT COUNT() and return the number of records as an
383         integer.
384 
385         If the QuerySet is already fully cached, return the length of the
386         cached results set to avoid multiple SELECT COUNT(*) calls.
387         """
388         if self._result_cache is not None:
389             return len(self._result_cache)
390 
391         return self.query.get_count(using=self.db)
392 
393     def get(self, *args, **kwargs):
394         """
395         Perform the query and return a single object matching the given
396         keyword arguments.
397         """
398         clone = self.filter(*args, **kwargs)
399         if self.query.can_filter() and not self.query.distinct_fields:
400             clone = clone.order_by()
401         num = len(clone)
402         if num == 1:
403             return clone._result_cache[0]
404         if not num:
405             raise self.model.DoesNotExist(
406                 "%s matching query does not exist." %
407                 self.model._meta.object_name
408             )
409         raise self.model.MultipleObjectsReturned(
410             "get() returned more than one %s -- it returned %s!" %
411             (self.model._meta.object_name, num)
412         )
413 
414     def create(self, **kwargs):
415         """
416         Create a new object with the given kwargs, saving it to the database
417         and returning the created object.
418         """
419         obj = self.model(**kwargs)
420         self._for_write = True
421         obj.save(force_insert=True, using=self.db)
422         return obj
423 
424     def _populate_pk_values(self, objs):
425         for obj in objs:
426             if obj.pk is None:
427                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
428 
429     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
430         """
431         Insert each of the instances into the database. Do *not* call
432         save() on each of the instances, do not send any pre/post_save
433         signals, and do not set the primary key attribute if it is an
434         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
435         Multi-table models are not supported.
436         """
437         # When you bulk insert you don't get the primary keys back (if it's an
438         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
439         # you can't insert into the child tables which references this. There
440         # are two workarounds:
441         # 1) This could be implemented if you didn't have an autoincrement pk
442         # 2) You could do it by doing O(n) normal inserts into the parent
443         #    tables to get the primary keys back and then doing a single bulk
444         #    insert into the childmost table.
445         # We currently set the primary keys on the objects when using
446         # PostgreSQL via the RETURNING ID clause. It should be possible for
447         # Oracle as well, but the semantics for extracting the primary keys is
448         # trickier so it's not done yet.
449         assert batch_size is None or batch_size > 0
450         # Check that the parents share the same concrete model with the our
451         # model to detect the inheritance pattern ConcreteGrandParent ->
452         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
453         # would not identify that case as involving multiple tables.
454         for parent in self.model._meta.get_parent_list():
455             if parent._meta.concrete_model is not self.model._meta.concrete_model:
456                 raise ValueError("Can't bulk create a multi-table inherited model")
457         if not objs:
458             return objs
459         self._for_write = True
460         connection = connections[self.db]
461         fields = self.model._meta.concrete_fields
462         objs = list(objs)
463         self._populate_pk_values(objs)
464         with transaction.atomic(using=self.db, savepoint=False):
465             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
466             if objs_with_pk:
467                 self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
468                 for obj_with_pk in objs_with_pk:
469                     obj_with_pk._state.adding = False
470                     obj_with_pk._state.db = self.db
471             if objs_without_pk:
472                 fields = [f for f in fields if not isinstance(f, AutoField)]
473                 ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
474                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
475                     assert len(ids) == len(objs_without_pk)
476                 for obj_without_pk, pk in zip(objs_without_pk, ids):
477                     obj_without_pk.pk = pk
478                     obj_without_pk._state.adding = False
479                     obj_without_pk._state.db = self.db
480 
481         return objs
482 
483     def bulk_update(self, objs, fields, batch_size=None):
484         """
485         Update the given fields in each of the given objects in the database.
486         """
487         if batch_size is not None and batch_size < 0:
488             raise ValueError('Batch size must be a positive integer.')
489         if not fields:
490             raise ValueError('Field names must be given to bulk_update().')
491         objs = tuple(objs)
492         if any(obj.pk is None for obj in objs):
493             raise ValueError('All bulk_update() objects must have a primary key set.')
494         fields = [self.model._meta.get_field(name) for name in fields]
495         if any(not f.concrete or f.many_to_many for f in fields):
496             raise ValueError('bulk_update() can only be used with concrete fields.')
497         if any(f.primary_key for f in fields):
498             raise ValueError('bulk_update() cannot be used with primary key fields.')
499         if not objs:
500             return
501         # PK is used twice in the resulting update query, once in the filter
502         # and once in the WHEN. Each field will also have one CAST.
503         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
504         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
505         requires_casting = connections[self.db].features.requires_casted_case_in_updates
506         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
507         updates = []
508         for batch_objs in batches:
509             update_kwargs = {}
510             for field in fields:
511                 when_statements = []
512                 for obj in batch_objs:
513                     attr = getattr(obj, field.attname)
514                     if not isinstance(attr, Expression):
515                         attr = Value(attr, output_field=field)
516                     when_statements.append(When(pk=obj.pk, then=attr))
517                 case_statement = Case(*when_statements, output_field=field)
518                 if requires_casting:
519                     case_statement = Cast(case_statement, output_field=field)
520                 update_kwargs[field.attname] = case_statement
521             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
522         with transaction.atomic(using=self.db, savepoint=False):
523             for pks, update_kwargs in updates:
524                 self.filter(pk__in=pks).update(**update_kwargs)
525     bulk_update.alters_data = True
526 
527     def get_or_create(self, defaults=None, **kwargs):
528         """
529         Look up an object with the given kwargs, creating one if necessary.
530         Return a tuple of (object, created), where created is a boolean
531         specifying whether an object was created.
532         """
533         # The get() needs to be targeted at the write database in order
534         # to avoid potential transaction consistency problems.
535         self._for_write = True
536         try:
537             return self.get(**kwargs), False
538         except self.model.DoesNotExist:
539             params = self._extract_model_params(defaults, **kwargs)
540             return self._create_object_from_params(kwargs, params)
541 
542     def update_or_create(self, defaults=None, **kwargs):
543         """
544         Look up an object with the given kwargs, updating one with defaults
545         if it exists, otherwise create a new one.
546         Return a tuple (object, created), where created is a boolean
547         specifying whether an object was created.
548         """
549         defaults = defaults or {}
550         self._for_write = True
551         with transaction.atomic(using=self.db):
552             try:
553                 obj = self.select_for_update().get(**kwargs)
554             except self.model.DoesNotExist:
555                 params = self._extract_model_params(defaults, **kwargs)
556                 # Lock the row so that a concurrent update is blocked until
557                 # after update_or_create() has performed its save.
558                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
559                 if created:
560                     return obj, created
561             for k, v in defaults.items():
562                 setattr(obj, k, v() if callable(v) else v)
563             obj.save(using=self.db)
564         return obj, False
565 
566     def _create_object_from_params(self, lookup, params, lock=False):
567         """
568         Try to create an object using passed params. Used by get_or_create()
569         and update_or_create().
570         """
571         try:
572             with transaction.atomic(using=self.db):
573                 params = {k: v() if callable(v) else v for k, v in params.items()}
574                 obj = self.create(**params)
575             return obj, True
576         except IntegrityError as e:
577             try:
578                 qs = self.select_for_update() if lock else self
579                 return qs.get(**lookup), False
580             except self.model.DoesNotExist:
581                 pass
582             raise e
583 
584     def _extract_model_params(self, defaults, **kwargs):
585         """
586         Prepare `params` for creating a model instance based on the given
587         kwargs; for use by get_or_create() and update_or_create().
588         """
589         defaults = defaults or {}
590         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
591         params.update(defaults)
592         property_names = self.model._meta._property_names
593         invalid_params = []
594         for param in params:
595             try:
596                 self.model._meta.get_field(param)
597             except exceptions.FieldDoesNotExist:
598                 # It's okay to use a model's property if it has a setter.
599                 if not (param in property_names and getattr(self.model, param).fset):
600                     invalid_params.append(param)
601         if invalid_params:
602             raise exceptions.FieldError(
603                 "Invalid field name(s) for model %s: '%s'." % (
604                     self.model._meta.object_name,
605                     "', '".join(sorted(invalid_params)),
606                 ))
607         return params
608 
609     def _earliest(self, *fields):
610         """
611         Return the earliest object according to fields (if given) or by the
612         model's Meta.get_latest_by.
613         """
614         if fields:
615             order_by = fields
616         else:
617             order_by = getattr(self.model._meta, 'get_latest_by')
618             if order_by and not isinstance(order_by, (tuple, list)):
619                 order_by = (order_by,)
620         if order_by is None:
621             raise ValueError(
622                 "earliest() and latest() require either fields as positional "
623                 "arguments or 'get_latest_by' in the model's Meta."
624             )
625 
626         assert self.query.can_filter(), \
627             "Cannot change a query once a slice has been taken."
628         obj = self._chain()
629         obj.query.set_limits(high=1)
630         obj.query.clear_ordering(force_empty=True)
631         obj.query.add_ordering(*order_by)
632         return obj.get()
633 
634     def earliest(self, *fields):
635         return self._earliest(*fields)
636 
637     def latest(self, *fields):
638         return self.reverse()._earliest(*fields)
639 
640     def first(self):
641         """Return the first object of a query or None if no match is found."""
642         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
643             return obj
644 
645     def last(self):
646         """Return the last object of a query or None if no match is found."""
647         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
648             return obj
649 
650     def in_bulk(self, id_list=None, *, field_name='pk'):
651         """
652         Return a dictionary mapping each of the given IDs to the object with
653         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
654         """
655         assert self.query.can_filter(), \
656             "Cannot use 'limit' or 'offset' with in_bulk"
657         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
658             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
659         if id_list is not None:
660             if not id_list:
661                 return {}
662             filter_key = '{}__in'.format(field_name)
663             batch_size = connections[self.db].features.max_query_params
664             id_list = tuple(id_list)
665             # If the database has a limit on the number of query parameters
666             # (e.g. SQLite), retrieve objects in batches if necessary.
667             if batch_size and batch_size < len(id_list):
668                 qs = ()
669                 for offset in range(0, len(id_list), batch_size):
670                     batch = id_list[offset:offset + batch_size]
671                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
672             else:
673                 qs = self.filter(**{filter_key: id_list}).order_by()
674         else:
675             qs = self._chain()
676         return {getattr(obj, field_name): obj for obj in qs}
677 
678     def delete(self):
679         """Delete the records in the current QuerySet."""
680         assert self.query.can_filter(), \
681             "Cannot use 'limit' or 'offset' with delete."
682 
683         if self._fields is not None:
684             raise TypeError("Cannot call delete() after .values() or .values_list()")
685 
686         del_query = self._chain()
687 
688         # The delete is actually 2 queries - one to find related objects,
689         # and one to delete. Make sure that the discovery of related
690         # objects is performed on the same database as the deletion.
691         del_query._for_write = True
692 
693         # Disable non-supported fields.
694         del_query.query.select_for_update = False
695         del_query.query.select_related = False
696         del_query.query.clear_ordering(force_empty=True)
697 
698         collector = Collector(using=del_query.db)
699         collector.collect(del_query)
700         deleted, _rows_count = collector.delete()
701 
702         # Clear the result cache, in case this QuerySet gets reused.
703         self._result_cache = None
704         return deleted, _rows_count
705 
706     delete.alters_data = True
707     delete.queryset_only = True
708 
709     def _raw_delete(self, using):
710         """
711         Delete objects found from the given queryset in single direct SQL
712         query. No signals are sent and there is no protection for cascades.
713         """
714         return sql.DeleteQuery(self.model).delete_qs(self, using)
715     _raw_delete.alters_data = True
716 
717     def update(self, **kwargs):
718         """
719         Update all elements in the current QuerySet, setting all the given
720         fields to the appropriate values.
721         """
722         assert self.query.can_filter(), \
723             "Cannot update a query once a slice has been taken."
724         self._for_write = True
725         query = self.query.chain(sql.UpdateQuery)
726         query.add_update_values(kwargs)
727         # Clear any annotations so that they won't be present in subqueries.
728         query.annotations = {}
729         with transaction.mark_for_rollback_on_error(using=self.db):
730             rows = query.get_compiler(self.db).execute_sql(CURSOR)
731         self._result_cache = None
732         return rows
733     update.alters_data = True
734 
735     def _update(self, values):
736         """
737         A version of update() that accepts field objects instead of field names.
738         Used primarily for model saving and not intended for use by general
739         code (it requires too much poking around at model internals to be
740         useful at that level).
741         """
742         assert self.query.can_filter(), \
743             "Cannot update a query once a slice has been taken."
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_fields(values)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         self._result_cache = None
749         return query.get_compiler(self.db).execute_sql(CURSOR)
750     _update.alters_data = True
751     _update.queryset_only = False
752 
753     def exists(self):
754         if self._result_cache is None:
755             return self.query.has_results(using=self.db)
756         return bool(self._result_cache)
757 
758     def _prefetch_related_objects(self):
759         # This method can only be called once the result cache has been filled.
760         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
761         self._prefetch_done = True
762 
763     def explain(self, *, format=None, **options):
764         return self.query.explain(using=self.db, format=format, **options)
765 
766     ##################################################
767     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
768     ##################################################
769 
770     def raw(self, raw_query, params=None, translations=None, using=None):
771         if using is None:
772             using = self.db
773         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
774         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
775         return qs
776 
777     def _values(self, *fields, **expressions):
778         clone = self._chain()
779         if expressions:
780             clone = clone.annotate(**expressions)
781         clone._fields = fields
782         clone.query.set_values(fields)
783         return clone
784 
785     def values(self, *fields, **expressions):
786         fields += tuple(expressions)
787         clone = self._values(*fields, **expressions)
788         clone._iterable_class = ValuesIterable
789         return clone
790 
791     def values_list(self, *fields, flat=False, named=False):
792         if flat and named:
793             raise TypeError("'flat' and 'named' can't be used together.")
794         if flat and len(fields) > 1:
795             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
796 
797         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
798         _fields = []
799         expressions = {}
800         counter = 1
801         for field in fields:
802             if hasattr(field, 'resolve_expression'):
803                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
804                 while True:
805                     field_id = field_id_prefix + str(counter)
806                     counter += 1
807                     if field_id not in field_names:
808                         break
809                 expressions[field_id] = field
810                 _fields.append(field_id)
811             else:
812                 _fields.append(field)
813 
814         clone = self._values(*_fields, **expressions)
815         clone._iterable_class = (
816             NamedValuesListIterable if named
817             else FlatValuesListIterable if flat
818             else ValuesListIterable
819         )
820         return clone
821 
822     def dates(self, field_name, kind, order='ASC'):
823         """
824         Return a list of date objects representing all available dates for
825         the given field_name, scoped to 'kind'.
826         """
827         assert kind in ('year', 'month', 'week', 'day'), \
828             "'kind' must be one of 'year', 'month', 'week', or 'day'."
829         assert order in ('ASC', 'DESC'), \
830             "'order' must be either 'ASC' or 'DESC'."
831         return self.annotate(
832             datefield=Trunc(field_name, kind, output_field=DateField()),
833             plain_field=F(field_name)
834         ).values_list(
835             'datefield', flat=True
836         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
837 
838     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
839         """
840         Return a list of datetime objects representing all available
841         datetimes for the given field_name, scoped to 'kind'.
842         """
843         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
844             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
845         assert order in ('ASC', 'DESC'), \
846             "'order' must be either 'ASC' or 'DESC'."
847         if settings.USE_TZ:
848             if tzinfo is None:
849                 tzinfo = timezone.get_current_timezone()
850         else:
851             tzinfo = None
852         return self.annotate(
853             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
854             plain_field=F(field_name)
855         ).values_list(
856             'datetimefield', flat=True
857         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
858 
859     def none(self):
860         """Return an empty QuerySet."""
861         clone = self._chain()
862         clone.query.set_empty()
863         return clone
864 
865     ##################################################################
866     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
867     ##################################################################
868 
869     def all(self):
870         """
871         Return a new QuerySet that is a copy of the current one. This allows a
872         QuerySet to proxy for a model manager in some cases.
873         """
874         return self._chain()
875 
876     def filter(self, *args, **kwargs):
877         """
878         Return a new QuerySet instance with the args ANDed to the existing
879         set.
880         """
881         return self._filter_or_exclude(False, *args, **kwargs)
882 
883     def exclude(self, *args, **kwargs):
884         """
885         Return a new QuerySet instance with NOT (args) ANDed to the existing
886         set.
887         """
888         return self._filter_or_exclude(True, *args, **kwargs)
889 
890     def _filter_or_exclude(self, negate, *args, **kwargs):
891         if args or kwargs:
892             assert self.query.can_filter(), \
893                 "Cannot filter a query once a slice has been taken."
894 
895         clone = self._chain()
896         if negate:
897             clone.query.add_q(~Q(*args, **kwargs))
898         else:
899             clone.query.add_q(Q(*args, **kwargs))
900         return clone
901 
902     def complex_filter(self, filter_obj):
903         """
904         Return a new QuerySet instance with filter_obj added to the filters.
905 
906         filter_obj can be a Q object or a dictionary of keyword lookup
907         arguments.
908 
909         This exists to support framework features such as 'limit_choices_to',
910         and usually it will be more natural to use other methods.
911         """
912         if isinstance(filter_obj, Q):
913             clone = self._chain()
914             clone.query.add_q(filter_obj)
915             return clone
916         else:
917             return self._filter_or_exclude(None, **filter_obj)
918 
919     def _combinator_query(self, combinator, *other_qs, all=False):
920         # Clone the query to inherit the select list and everything
921         clone = self._chain()
922         # Clear limits and ordering so they can be reapplied
923         clone.query.clear_ordering(True)
924         clone.query.clear_limits()
925         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
926         clone.query.combinator = combinator
927         clone.query.combinator_all = all
928         return clone
929 
930     def union(self, *other_qs, all=False):
931         # If the query is an EmptyQuerySet, combine all nonempty querysets.
932         if isinstance(self, EmptyQuerySet):
933             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
934             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
935         return self._combinator_query('union', *other_qs, all=all)
936 
937     def intersection(self, *other_qs):
938         # If any query is an EmptyQuerySet, return it.
939         if isinstance(self, EmptyQuerySet):
940             return self
941         for other in other_qs:
942             if isinstance(other, EmptyQuerySet):
943                 return other
944         return self._combinator_query('intersection', *other_qs)
945 
946     def difference(self, *other_qs):
947         # If the query is an EmptyQuerySet, return it.
948         if isinstance(self, EmptyQuerySet):
949             return self
950         return self._combinator_query('difference', *other_qs)
951 
952     def select_for_update(self, nowait=False, skip_locked=False, of=()):
953         """
954         Return a new QuerySet instance that will select objects with a
955         FOR UPDATE lock.
956         """
957         if nowait and skip_locked:
958             raise ValueError('The nowait option cannot be used with skip_locked.')
959         obj = self._chain()
960         obj._for_write = True
961         obj.query.select_for_update = True
962         obj.query.select_for_update_nowait = nowait
963         obj.query.select_for_update_skip_locked = skip_locked
964         obj.query.select_for_update_of = of
965         return obj
966 
967     def select_related(self, *fields):
968         """
969         Return a new QuerySet instance that will select related objects.
970 
971         If fields are specified, they must be ForeignKey fields and only those
972         related objects are included in the selection.
973 
974         If select_related(None) is called, clear the list.
975         """
976 
977         if self._fields is not None:
978             raise TypeError("Cannot call select_related() after .values() or .values_list()")
979 
980         obj = self._chain()
981         if fields == (None,):
982             obj.query.select_related = False
983         elif fields:
984             obj.query.add_select_related(fields)
985         else:
986             obj.query.select_related = True
987         return obj
988 
989     def prefetch_related(self, *lookups):
990         """
991         Return a new QuerySet instance that will prefetch the specified
992         Many-To-One and Many-To-Many related objects when the QuerySet is
993         evaluated.
994 
995         When prefetch_related() is called more than once, append to the list of
996         prefetch lookups. If prefetch_related(None) is called, clear the list.
997         """
998         clone = self._chain()
999         if lookups == (None,):
1000             clone._prefetch_related_lookups = ()
1001         else:
1002             for lookup in lookups:
1003                 if isinstance(lookup, Prefetch):
1004                     lookup = lookup.prefetch_to
1005                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1006                 if lookup in self.query._filtered_relations:
1007                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1008             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1009         return clone
1010 
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone
1056 
1057     def order_by(self, *field_names):
1058         """Return a new QuerySet instance with the ordering changed."""
1059         assert self.query.can_filter(), \
1060             "Cannot reorder a query once a slice has been taken."
1061         obj = self._chain()
1062         obj.query.clear_ordering(force_empty=False)
1063         obj.query.add_ordering(*field_names)
1064         return obj
1065 
1066     def distinct(self, *field_names):
1067         """
1068         Return a new QuerySet instance that will select only distinct results.
1069         """
1070         assert self.query.can_filter(), \
1071             "Cannot create distinct fields once a slice has been taken."
1072         obj = self._chain()
1073         obj.query.add_distinct_fields(*field_names)
1074         return obj
1075 
1076     def extra(self, select=None, where=None, params=None, tables=None,
1077               order_by=None, select_params=None):
1078         """Add extra SQL fragments to the query."""
1079         assert self.query.can_filter(), \
1080             "Cannot change a query once a slice has been taken"
1081         clone = self._chain()
1082         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1083         return clone
1084 
1085     def reverse(self):
1086         """Reverse the ordering of the QuerySet."""
1087         if not self.query.can_filter():
1088             raise TypeError('Cannot reverse a query once a slice has been taken.')
1089         clone = self._chain()
1090         clone.query.standard_ordering = not clone.query.standard_ordering
1091         return clone
1092 
1093     def defer(self, *fields):
1094         """
1095         Defer the loading of data for certain fields until they are accessed.
1096         Add the set of deferred fields to any existing set of deferred fields.
1097         The only exception to this is if None is passed in as the only
1098         parameter, in which case removal all deferrals.
1099         """
1100         if self._fields is not None:
1101             raise TypeError("Cannot call defer() after .values() or .values_list()")
1102         clone = self._chain()
1103         if fields == (None,):
1104             clone.query.clear_deferred_loading()
1105         else:
1106             clone.query.add_deferred_loading(fields)
1107         return clone
1108 
1109     def only(self, *fields):
1110         """
1111         Essentially, the opposite of defer(). Only the fields passed into this
1112         method and that are not already specified as deferred are loaded
1113         immediately when the queryset is evaluated.
1114         """
1115         if self._fields is not None:
1116             raise TypeError("Cannot call only() after .values() or .values_list()")
1117         if fields == (None,):
1118             # Can only pass None to defer(), not only(), as the rest option.
1119             # That won't stop people trying to do this, so let's be explicit.
1120             raise TypeError("Cannot pass None as an argument to only().")
1121         for field in fields:
1122             field = field.split(LOOKUP_SEP, 1)[0]
1123             if field in self.query._filtered_relations:
1124                 raise ValueError('only() is not supported with FilteredRelation.')
1125         clone = self._chain()
1126         clone.query.add_immediate_loading(fields)
1127         return clone
1128 
1129     def using(self, alias):
1130         """Select which database this QuerySet should execute against."""
1131         clone = self._chain()
1132         clone._db = alias
1133         return clone
1134 
1135     ###################################
1136     # PUBLIC INTROSPECTION ATTRIBUTES #
1137     ###################################
1138 
1139     @property
1140     def ordered(self):
1141         """
1142         Return True if the QuerySet is ordered -- i.e. has an order_by()
1143         clause or a default ordering on the model (or is empty).
1144         """
1145         if isinstance(self, EmptyQuerySet):
1146             return True
1147         if self.query.extra_order_by or self.query.order_by:
1148             return True
1149         elif self.query.default_ordering and self.query.get_meta().ordering:
1150             return True
1151         else:
1152             return False
1153 
1154     @property
1155     def db(self):
1156         """Return the database used if this query is executed now."""
1157         if self._for_write:
1158             return self._db or router.db_for_write(self.model, **self._hints)
1159         return self._db or router.db_for_read(self.model, **self._hints)
1160 
1161     ###################
1162     # PRIVATE METHODS #
1163     ###################
1164 
1165     def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):
1166         """
1167         Insert a new record for the given model. This provides an interface to
1168         the InsertQuery class and is how Model.save() is implemented.
1169         """
1170         self._for_write = True
1171         if using is None:
1172             using = self.db
1173         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1174         query.insert_values(fields, objs, raw=raw)
1175         return query.get_compiler(using=using).execute_sql(return_id)
1176     _insert.alters_data = True
1177     _insert.queryset_only = False
1178 
1179     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1180         """
1181         Helper method for bulk_create() to insert objs one batch at a time.
1182         """
1183         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1184             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1185         ops = connections[self.db].ops
1186         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1187         inserted_ids = []
1188         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1189         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1190             if bulk_return and not ignore_conflicts:
1191                 inserted_id = self._insert(
1192                     item, fields=fields, using=self.db, return_id=True,
1193                     ignore_conflicts=ignore_conflicts,
1194                 )
1195                 if isinstance(inserted_id, list):
1196                     inserted_ids.extend(inserted_id)
1197                 else:
1198                     inserted_ids.append(inserted_id)
1199             else:
1200                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1201         return inserted_ids
1202 
1203     def _chain(self, **kwargs):
1204         """
1205         Return a copy of the current QuerySet that's ready for another
1206         operation.
1207         """
1208         obj = self._clone()
1209         if obj._sticky_filter:
1210             obj.query.filter_is_sticky = True
1211             obj._sticky_filter = False
1212         obj.__dict__.update(kwargs)
1213         return obj
1214 
1215     def _clone(self):
1216         """
1217         Return a copy of the current QuerySet. A lightweight alternative
1218         to deepcopy().
1219         """
1220         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1221         c._sticky_filter = self._sticky_filter
1222         c._for_write = self._for_write
1223         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1224         c._known_related_objects = self._known_related_objects
1225         c._iterable_class = self._iterable_class
1226         c._fields = self._fields
1227         return c
1228 
1229     def _fetch_all(self):
1230         if self._result_cache is None:
1231             self._result_cache = list(self._iterable_class(self))
1232         if self._prefetch_related_lookups and not self._prefetch_done:
1233             self._prefetch_related_objects()
1234 
1235     def _next_is_sticky(self):
1236         """
1237         Indicate that the next filter call and the one following that should
1238         be treated as a single filter. This is only important when it comes to
1239         determining when to reuse tables for many-to-many filters. Required so
1240         that we can filter naturally on the results of related managers.
1241 
1242         This doesn't return a clone of the current QuerySet (it returns
1243         "self"). The method is only used internally and should be immediately
1244         followed by a filter() that does create a clone.
1245         """
1246         self._sticky_filter = True
1247         return self
1248 
1249     def _merge_sanity_check(self, other):
1250         """Check that two QuerySet classes may be merged."""
1251         if self._fields is not None and (
1252                 set(self.query.values_select) != set(other.query.values_select) or
1253                 set(self.query.extra_select) != set(other.query.extra_select) or
1254                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1255             raise TypeError(
1256                 "Merging '%s' classes must involve the same values in each case."
1257                 % self.__class__.__name__
1258             )
1259 
1260     def _merge_known_related_objects(self, other):
1261         """
1262         Keep track of all known related objects from either QuerySet instance.
1263         """
1264         for field, objects in other._known_related_objects.items():
1265             self._known_related_objects.setdefault(field, {}).update(objects)
1266 
1267     def resolve_expression(self, *args, **kwargs):
1268         if self._fields and len(self._fields) > 1:
1269             # values() queryset can only be used as nested queries
1270             # if they are set up to select only a single field.
1271             raise TypeError('Cannot use multi-field values as a filter value.')
1272         query = self.query.resolve_expression(*args, **kwargs)
1273         query._db = self._db
1274         return query
1275     resolve_expression.queryset_only = True
1276 
1277     def _add_hints(self, **hints):
1278         """
1279         Update hinting information for use by routers. Add new key/values or
1280         overwrite existing key/values.
1281         """
1282         self._hints.update(hints)
1283 
1284     def _has_filters(self):
1285         """
1286         Check if this QuerySet has any filtering going on. This isn't
1287         equivalent with checking if all objects are present in results, for
1288         example, qs[1:]._has_filters() -> False.
1289         """
1290         return self.query.has_filters()
1291 
1292     @staticmethod
1293     def _validate_values_are_expressions(values, method_name):
1294         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1295         if invalid_args:
1296             raise TypeError(
1297                 'QuerySet.%s() received non-expression(s): %s.' % (
1298                     method_name,
1299                     ', '.join(invalid_args),
1300                 )
1301             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 22:28:41.856 | INFO     | Search completed. Bug locations: [<file>django/db/backends/sqlite3/base.py</file>
<class>None</class>
<method>None</method>
<code>
1 """
2 SQLite backend for the sqlite3 module in the standard library.
3 """
4 import datetime
5 import decimal
6 import functools
7 import math
8 import operator
9 import re
10 import statistics
11 import warnings
12 from itertools import chain
13 from sqlite3 import dbapi2 as Database
14 
15 import pytz
16 
17 from django.core.exceptions import ImproperlyConfigured
18 from django.db import utils
19 from django.db.backends import utils as backend_utils
20 from django.db.backends.base.base import BaseDatabaseWrapper
21 from django.utils import timezone
22 from django.utils.dateparse import parse_datetime, parse_time
23 from django.utils.duration import duration_microseconds
24 
25 from .client import DatabaseClient                          # isort:skip
26 from .creation import DatabaseCreation                      # isort:skip
27 from .features import DatabaseFeatures                      # isort:skip
28 from .introspection import DatabaseIntrospection            # isort:skip
29 from .operations import DatabaseOperations                  # isort:skip
30 from .schema import DatabaseSchemaEditor                    # isort:skip
31 
32 
33 def decoder(conv_func):
34     """
35     Convert bytestrings from Python's sqlite3 interface to a regular string.
36     """
37     return lambda s: conv_func(s.decode())
38 
39 
40 def none_guard(func):
41     """
42     Decorator that returns None if any of the arguments to the decorated
43     function are None. Many SQL functions return NULL if any of their arguments
44     are NULL. This decorator simplifies the implementation of this for the
45     custom functions registered below.
46     """
47     @functools.wraps(func)
48     def wrapper(*args, **kwargs):
49         return None if None in args else func(*args, **kwargs)
50     return wrapper
51 
52 
53 def list_aggregate(function):
54     """
55     Return an aggregate class that accumulates values in a list and applies
56     the provided function to the data.
57     """
58     return type('ListAggregate', (list,), {'finalize': function, 'step': list.append})
59 
60 
61 def check_sqlite_version():
62     if Database.sqlite_version_info < (3, 8, 3):
63         raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)
64 
65 
66 check_sqlite_version()
67 
68 Database.register_converter("bool", b'1'.__eq__)
69 Database.register_converter("time", decoder(parse_time))
70 Database.register_converter("datetime", decoder(parse_datetime))
71 Database.register_converter("timestamp", decoder(parse_datetime))
72 Database.register_converter("TIMESTAMP", decoder(parse_datetime))
73 
74 Database.register_adapter(decimal.Decimal, str)
75 
76 
77 class DatabaseWrapper(BaseDatabaseWrapper):
78     vendor = 'sqlite'
79     display_name = 'SQLite'
80     # SQLite doesn't actually support most of these types, but it "does the right
81     # thing" given more verbose field definitions, so leave them as is so that
82     # schema inspection is more useful.
83     data_types = {
84         'AutoField': 'integer',
85         'BigAutoField': 'integer',
86         'BinaryField': 'BLOB',
87         'BooleanField': 'bool',
88         'CharField': 'varchar(%(max_length)s)',
89         'DateField': 'date',
90         'DateTimeField': 'datetime',
91         'DecimalField': 'decimal',
92         'DurationField': 'bigint',
93         'FileField': 'varchar(%(max_length)s)',
94         'FilePathField': 'varchar(%(max_length)s)',
95         'FloatField': 'real',
96         'IntegerField': 'integer',
97         'BigIntegerField': 'bigint',
98         'IPAddressField': 'char(15)',
99         'GenericIPAddressField': 'char(39)',
100         'NullBooleanField': 'bool',
101         'OneToOneField': 'integer',
102         'PositiveIntegerField': 'integer unsigned',
103         'PositiveSmallIntegerField': 'smallint unsigned',
104         'SlugField': 'varchar(%(max_length)s)',
105         'SmallIntegerField': 'smallint',
106         'TextField': 'text',
107         'TimeField': 'time',
108         'UUIDField': 'char(32)',
109     }
110     data_type_check_constraints = {
111         'PositiveIntegerField': '"%(column)s" >= 0',
112         'PositiveSmallIntegerField': '"%(column)s" >= 0',
113     }
114     data_types_suffix = {
115         'AutoField': 'AUTOINCREMENT',
116         'BigAutoField': 'AUTOINCREMENT',
117     }
118     # SQLite requires LIKE statements to include an ESCAPE clause if the value
119     # being escaped has a percent or underscore in it.
120     # See https://www.sqlite.org/lang_expr.html for an explanation.
121     operators = {
122         'exact': '= %s',
123         'iexact': "LIKE %s ESCAPE '\\'",
124         'contains': "LIKE %s ESCAPE '\\'",
125         'icontains': "LIKE %s ESCAPE '\\'",
126         'regex': 'REGEXP %s',
127         'iregex': "REGEXP '(?i)' || %s",
128         'gt': '> %s',
129         'gte': '>= %s',
130         'lt': '< %s',
131         'lte': '<= %s',
132         'startswith': "LIKE %s ESCAPE '\\'",
133         'endswith': "LIKE %s ESCAPE '\\'",
134         'istartswith': "LIKE %s ESCAPE '\\'",
135         'iendswith': "LIKE %s ESCAPE '\\'",
136     }
137 
138     # The patterns below are used to generate SQL pattern lookup clauses when
139     # the right-hand side of the lookup isn't a raw string (it might be an expression
140     # or the result of a bilateral transformation).
141     # In those cases, special characters for LIKE operators (e.g. \, *, _) should be
142     # escaped on database side.
143     #
144     # Note: we use str.format() here for readability as '%' is used as a wildcard for
145     # the LIKE operator.
146     pattern_esc = r"REPLACE(REPLACE(REPLACE({}, '\', '\\'), '%%', '\%%'), '_', '\_')"
147     pattern_ops = {
148         'contains': r"LIKE '%%' || {} || '%%' ESCAPE '\'",
149         'icontains': r"LIKE '%%' || UPPER({}) || '%%' ESCAPE '\'",
150         'startswith': r"LIKE {} || '%%' ESCAPE '\'",
151         'istartswith': r"LIKE UPPER({}) || '%%' ESCAPE '\'",
152         'endswith': r"LIKE '%%' || {} ESCAPE '\'",
153         'iendswith': r"LIKE '%%' || UPPER({}) ESCAPE '\'",
154     }
155 
156     Database = Database
157     SchemaEditorClass = DatabaseSchemaEditor
158     # Classes instantiated in __init__().
159     client_class = DatabaseClient
160     creation_class = DatabaseCreation
161     features_class = DatabaseFeatures
162     introspection_class = DatabaseIntrospection
163     ops_class = DatabaseOperations
164 
165     def get_connection_params(self):
166         settings_dict = self.settings_dict
167         if not settings_dict['NAME']:
168             raise ImproperlyConfigured(
169                 "settings.DATABASES is improperly configured. "
170                 "Please supply the NAME value.")
171         kwargs = {
172             'database': settings_dict['NAME'],
173             'detect_types': Database.PARSE_DECLTYPES | Database.PARSE_COLNAMES,
174             **settings_dict['OPTIONS'],
175         }
176         # Always allow the underlying SQLite connection to be shareable
177         # between multiple threads. The safe-guarding will be handled at a
178         # higher level by the `BaseDatabaseWrapper.allow_thread_sharing`
179         # property. This is necessary as the shareability is disabled by
180         # default in pysqlite and it cannot be changed once a connection is
181         # opened.
182         if 'check_same_thread' in kwargs and kwargs['check_same_thread']:
183             warnings.warn(
184                 'The `check_same_thread` option was provided and set to '
185                 'True. It will be overridden with False. Use the '
186                 '`DatabaseWrapper.allow_thread_sharing` property instead '
187                 'for controlling thread shareability.',
188                 RuntimeWarning
189             )
190         kwargs.update({'check_same_thread': False, 'uri': True})
191         return kwargs
192 
193     def get_new_connection(self, conn_params):
194         conn = Database.connect(**conn_params)
195         conn.create_function("django_date_extract", 2, _sqlite_datetime_extract)
196         conn.create_function("django_date_trunc", 2, _sqlite_date_trunc)
197         conn.create_function("django_datetime_cast_date", 2, _sqlite_datetime_cast_date)
198         conn.create_function("django_datetime_cast_time", 2, _sqlite_datetime_cast_time)
199         conn.create_function("django_datetime_extract", 3, _sqlite_datetime_extract)
200         conn.create_function("django_datetime_trunc", 3, _sqlite_datetime_trunc)
201         conn.create_function("django_time_extract", 2, _sqlite_time_extract)
202         conn.create_function("django_time_trunc", 2, _sqlite_time_trunc)
203         conn.create_function("django_time_diff", 2, _sqlite_time_diff)
204         conn.create_function("django_timestamp_diff", 2, _sqlite_timestamp_diff)
205         conn.create_function("django_format_dtdelta", 3, _sqlite_format_dtdelta)
206         conn.create_function('regexp', 2, _sqlite_regexp)
207         conn.create_function('ACOS', 1, none_guard(math.acos))
208         conn.create_function('ASIN', 1, none_guard(math.asin))
209         conn.create_function('ATAN', 1, none_guard(math.atan))
210         conn.create_function('ATAN2', 2, none_guard(math.atan2))
211         conn.create_function('CEILING', 1, none_guard(math.ceil))
212         conn.create_function('COS', 1, none_guard(math.cos))
213         conn.create_function('COT', 1, none_guard(lambda x: 1 / math.tan(x)))
214         conn.create_function('DEGREES', 1, none_guard(math.degrees))
215         conn.create_function('EXP', 1, none_guard(math.exp))
216         conn.create_function('FLOOR', 1, none_guard(math.floor))
217         conn.create_function('LN', 1, none_guard(math.log))
218         conn.create_function('LOG', 2, none_guard(lambda x, y: math.log(y, x)))
219         conn.create_function('LPAD', 3, _sqlite_lpad)
220         conn.create_function('MOD', 2, none_guard(math.fmod))
221         conn.create_function('PI', 0, lambda: math.pi)
222         conn.create_function('POWER', 2, none_guard(operator.pow))
223         conn.create_function('RADIANS', 1, none_guard(math.radians))
224         conn.create_function('REPEAT', 2, none_guard(operator.mul))
225         conn.create_function('REVERSE', 1, none_guard(lambda x: x[::-1]))
226         conn.create_function('RPAD', 3, _sqlite_rpad)
227         conn.create_function('SIN', 1, none_guard(math.sin))
228         conn.create_function('SQRT', 1, none_guard(math.sqrt))
229         conn.create_function('TAN', 1, none_guard(math.tan))
230         conn.create_aggregate('STDDEV_POP', 1, list_aggregate(statistics.pstdev))
231         conn.create_aggregate('STDDEV_SAMP', 1, list_aggregate(statistics.stdev))
232         conn.create_aggregate('VAR_POP', 1, list_aggregate(statistics.pvariance))
233         conn.create_aggregate('VAR_SAMP', 1, list_aggregate(statistics.variance))
234         conn.execute('PRAGMA foreign_keys = ON')
235         return conn
236 
237     def init_connection_state(self):
238         pass
239 
240     def create_cursor(self, name=None):
241         return self.connection.cursor(factory=SQLiteCursorWrapper)
242 
243     def close(self):
244         self.validate_thread_sharing()
245         # If database is in memory, closing the connection destroys the
246         # database. To prevent accidental data loss, ignore close requests on
247         # an in-memory db.
248         if not self.is_in_memory_db():
249             BaseDatabaseWrapper.close(self)
250 
251     def _savepoint_allowed(self):
252         # When 'isolation_level' is not None, sqlite3 commits before each
253         # savepoint; it's a bug. When it is None, savepoints don't make sense
254         # because autocommit is enabled. The only exception is inside 'atomic'
255         # blocks. To work around that bug, on SQLite, 'atomic' starts a
256         # transaction explicitly rather than simply disable autocommit.
257         return self.in_atomic_block
258 
259     def _set_autocommit(self, autocommit):
260         if autocommit:
261             level = None
262         else:
263             # sqlite3's internal default is ''. It's different from None.
264             # See Modules/_sqlite/connection.c.
265             level = ''
266         # 'isolation_level' is a misleading API.
267         # SQLite always runs at the SERIALIZABLE isolation level.
268         with self.wrap_database_errors:
269             self.connection.isolation_level = level
270 
271     def disable_constraint_checking(self):
272         with self.cursor() as cursor:
273             cursor.execute('PRAGMA foreign_keys = OFF')
274             # Foreign key constraints cannot be turned off while in a multi-
275             # statement transaction. Fetch the current state of the pragma
276             # to determine if constraints are effectively disabled.
277             enabled = cursor.execute('PRAGMA foreign_keys').fetchone()[0]
278         return not bool(enabled)
279 
280     def enable_constraint_checking(self):
281         self.cursor().execute('PRAGMA foreign_keys = ON')
282 
283     def check_constraints(self, table_names=None):
284         """
285         Check each table name in `table_names` for rows with invalid foreign
286         key references. This method is intended to be used in conjunction with
287         `disable_constraint_checking()` and `enable_constraint_checking()`, to
288         determine if rows with invalid references were entered while constraint
289         checks were off.
290         """
291         if self.features.supports_pragma_foreign_key_check:
292             with self.cursor() as cursor:
293                 if table_names is None:
294                     violations = self.cursor().execute('PRAGMA foreign_key_check').fetchall()
295                 else:
296                     violations = chain.from_iterable(
297                         cursor.execute('PRAGMA foreign_key_check(%s)' % table_name).fetchall()
298                         for table_name in table_names
299                     )
300                 # See https://www.sqlite.org/pragma.html#pragma_foreign_key_check
301                 for table_name, rowid, referenced_table_name, foreign_key_index in violations:
302                     foreign_key = cursor.execute(
303                         'PRAGMA foreign_key_list(%s)' % table_name
304                     ).fetchall()[foreign_key_index]
305                     column_name, referenced_column_name = foreign_key[3:5]
306                     primary_key_column_name = self.introspection.get_primary_key_column(cursor, table_name)
307                     primary_key_value, bad_value = cursor.execute(
308                         'SELECT %s, %s FROM %s WHERE rowid = %%s' % (
309                             primary_key_column_name, column_name, table_name
310                         ),
311                         (rowid,),
312                     ).fetchone()
313                     raise utils.IntegrityError(
314                         "The row in table '%s' with primary key '%s' has an "
315                         "invalid foreign key: %s.%s contains a value '%s' that "
316                         "does not have a corresponding value in %s.%s." % (
317                             table_name, primary_key_value, table_name, column_name,
318                             bad_value, referenced_table_name, referenced_column_name
319                         )
320                     )
321         else:
322             with self.cursor() as cursor:
323                 if table_names is None:
324                     table_names = self.introspection.table_names(cursor)
325                 for table_name in table_names:
326                     primary_key_column_name = self.introspection.get_primary_key_column(cursor, table_name)
327                     if not primary_key_column_name:
328                         continue
329                     key_columns = self.introspection.get_key_columns(cursor, table_name)
330                     for column_name, referenced_table_name, referenced_column_name in key_columns:
331                         cursor.execute(
332                             """
333                             SELECT REFERRING.`%s`, REFERRING.`%s` FROM `%s` as REFERRING
334                             LEFT JOIN `%s` as REFERRED
335                             ON (REFERRING.`%s` = REFERRED.`%s`)
336                             WHERE REFERRING.`%s` IS NOT NULL AND REFERRED.`%s` IS NULL
337                             """
338                             % (
339                                 primary_key_column_name, column_name, table_name,
340                                 referenced_table_name, column_name, referenced_column_name,
341                                 column_name, referenced_column_name,
342                             )
343                         )
344                         for bad_row in cursor.fetchall():
345                             raise utils.IntegrityError(
346                                 "The row in table '%s' with primary key '%s' has an "
347                                 "invalid foreign key: %s.%s contains a value '%s' that "
348                                 "does not have a corresponding value in %s.%s." % (
349                                     table_name, bad_row[0], table_name, column_name,
350                                     bad_row[1], referenced_table_name, referenced_column_name,
351                                 )
352                             )
353 
354     def is_usable(self):
355         return True
356 
357     def _start_transaction_under_autocommit(self):
358         """
359         Start a transaction explicitly in autocommit mode.
360 
361         Staying in autocommit mode works around a bug of sqlite3 that breaks
362         savepoints when autocommit is disabled.
363         """
364         self.cursor().execute("BEGIN")
365 
366     def is_in_memory_db(self):
367         return self.creation.is_in_memory_db(self.settings_dict['NAME'])
368 
369 
370 FORMAT_QMARK_REGEX = re.compile(r'(?<!%)%s')
371 
372 
373 class SQLiteCursorWrapper(Database.Cursor):
374     """
375     Django uses "format" style placeholders, but pysqlite2 uses "qmark" style.
376     This fixes it -- but note that if you want to use a literal "%s" in a query,
377     you'll need to use "%%s".
378     """
379     def execute(self, query, params=None):
380         if params is None:
381             return Database.Cursor.execute(self, query)
382         query = self.convert_query(query)
383         return Database.Cursor.execute(self, query, params)
384 
385     def executemany(self, query, param_list):
386         query = self.convert_query(query)
387         return Database.Cursor.executemany(self, query, param_list)
388 
389     def convert_query(self, query):
390         return FORMAT_QMARK_REGEX.sub('?', query).replace('%%', '%')
391 
392 
393 def _sqlite_datetime_parse(dt, tzname=None):
394     if dt is None:
395         return None
396     try:
397         dt = backend_utils.typecast_timestamp(dt)
398     except (TypeError, ValueError):
399         return None
400     if tzname is not None:
401         dt = timezone.localtime(dt, pytz.timezone(tzname))
402     return dt
403 
404 
405 def _sqlite_date_trunc(lookup_type, dt):
406     dt = _sqlite_datetime_parse(dt)
407     if dt is None:
408         return None
409     if lookup_type == 'year':
410         return "%i-01-01" % dt.year
411     elif lookup_type == 'quarter':
412         month_in_quarter = dt.month - (dt.month - 1) % 3
413         return '%i-%02i-01' % (dt.year, month_in_quarter)
414     elif lookup_type == 'month':
415         return "%i-%02i-01" % (dt.year, dt.month)
416     elif lookup_type == 'week':
417         dt = dt - datetime.timedelta(days=dt.weekday())
418         return "%i-%02i-%02i" % (dt.year, dt.month, dt.day)
419     elif lookup_type == 'day':
420         return "%i-%02i-%02i" % (dt.year, dt.month, dt.day)
421 
422 
423 def _sqlite_time_trunc(lookup_type, dt):
424     if dt is None:
425         return None
426     try:
427         dt = backend_utils.typecast_time(dt)
428     except (ValueError, TypeError):
429         return None
430     if lookup_type == 'hour':
431         return "%02i:00:00" % dt.hour
432     elif lookup_type == 'minute':
433         return "%02i:%02i:00" % (dt.hour, dt.minute)
434     elif lookup_type == 'second':
435         return "%02i:%02i:%02i" % (dt.hour, dt.minute, dt.second)
436 
437 
438 def _sqlite_datetime_cast_date(dt, tzname):
439     dt = _sqlite_datetime_parse(dt, tzname)
440     if dt is None:
441         return None
442     return dt.date().isoformat()
443 
444 
445 def _sqlite_datetime_cast_time(dt, tzname):
446     dt = _sqlite_datetime_parse(dt, tzname)
447     if dt is None:
448         return None
449     return dt.time().isoformat()
450 
451 
452 def _sqlite_datetime_extract(lookup_type, dt, tzname=None):
453     dt = _sqlite_datetime_parse(dt, tzname)
454     if dt is None:
455         return None
456     if lookup_type == 'week_day':
457         return (dt.isoweekday() % 7) + 1
458     elif lookup_type == 'week':
459         return dt.isocalendar()[1]
460     elif lookup_type == 'quarter':
461         return math.ceil(dt.month / 3)
462     elif lookup_type == 'iso_year':
463         return dt.isocalendar()[0]
464     else:
465         return getattr(dt, lookup_type)
466 
467 
468 def _sqlite_datetime_trunc(lookup_type, dt, tzname):
469     dt = _sqlite_datetime_parse(dt, tzname)
470     if dt is None:
471         return None
472     if lookup_type == 'year':
473         return "%i-01-01 00:00:00" % dt.year
474     elif lookup_type == 'quarter':
475         month_in_quarter = dt.month - (dt.month - 1) % 3
476         return '%i-%02i-01 00:00:00' % (dt.year, month_in_quarter)
477     elif lookup_type == 'month':
478         return "%i-%02i-01 00:00:00" % (dt.year, dt.month)
479     elif lookup_type == 'week':
480         dt = dt - datetime.timedelta(days=dt.weekday())
481         return "%i-%02i-%02i 00:00:00" % (dt.year, dt.month, dt.day)
482     elif lookup_type == 'day':
483         return "%i-%02i-%02i 00:00:00" % (dt.year, dt.month, dt.day)
484     elif lookup_type == 'hour':
485         return "%i-%02i-%02i %02i:00:00" % (dt.year, dt.month, dt.day, dt.hour)
486     elif lookup_type == 'minute':
487         return "%i-%02i-%02i %02i:%02i:00" % (dt.year, dt.month, dt.day, dt.hour, dt.minute)
488     elif lookup_type == 'second':
489         return "%i-%02i-%02i %02i:%02i:%02i" % (dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)
490 
491 
492 def _sqlite_time_extract(lookup_type, dt):
493     if dt is None:
494         return None
495     try:
496         dt = backend_utils.typecast_time(dt)
497     except (ValueError, TypeError):
498         return None
499     return getattr(dt, lookup_type)
500 
501 
502 @none_guard
503 def _sqlite_format_dtdelta(conn, lhs, rhs):
504     """
505     LHS and RHS can be either:
506     - An integer number of microseconds
507     - A string representing a datetime
508     """
509     try:
510         real_lhs = datetime.timedelta(0, 0, lhs) if isinstance(lhs, int) else backend_utils.typecast_timestamp(lhs)
511         real_rhs = datetime.timedelta(0, 0, rhs) if isinstance(rhs, int) else backend_utils.typecast_timestamp(rhs)
512         if conn.strip() == '+':
513             out = real_lhs + real_rhs
514         else:
515             out = real_lhs - real_rhs
516     except (ValueError, TypeError):
517         return None
518     # typecast_timestamp returns a date or a datetime without timezone.
519     # It will be formatted as "%Y-%m-%d" or "%Y-%m-%d %H:%M:%S[.%f]"
520     return str(out)
521 
522 
523 @none_guard
524 def _sqlite_time_diff(lhs, rhs):
525     left = backend_utils.typecast_time(lhs)
526     right = backend_utils.typecast_time(rhs)
527     return (
528         (left.hour * 60 * 60 * 1000000) +
529         (left.minute * 60 * 1000000) +
530         (left.second * 1000000) +
531         (left.microsecond) -
532         (right.hour * 60 * 60 * 1000000) -
533         (right.minute * 60 * 1000000) -
534         (right.second * 1000000) -
535         (right.microsecond)
536     )
537 
538 
539 @none_guard
540 def _sqlite_timestamp_diff(lhs, rhs):
541     left = backend_utils.typecast_timestamp(lhs)
542     right = backend_utils.typecast_timestamp(rhs)
543     return duration_microseconds(left - right)
544 
545 
546 @none_guard
547 def _sqlite_regexp(re_pattern, re_string):
548     return bool(re.search(re_pattern, str(re_string)))
549 
550 
551 @none_guard
552 def _sqlite_lpad(text, length, fill_text):
553     if len(text) >= length:
554         return text[:length]
555     return (fill_text * length)[:length - len(text)] + text
556 
557 
558 @none_guard
559 def _sqlite_rpad(text, length, fill_text):
560     return (text + fill_text * length)[:length]

</code><intended_behavior>To resolve the issue, a mechanism should be introduced within the SQLite backend, particularly in the initial setup or query execution pathways that checks the SQLite version in use. If the version is older than 3.25.0, the backend should mark window functions as unsupported. This can be done by setting a feature flag specifically for window functions within the `features` property of the backend class. Before attempting to execute a query involving window functions, the Django ORM should check this feature flag. If unsupported, a descriptive error should be raised informing the developer that their current SQLite version does not support window functions.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone

</code><intended_behavior>In the `annotate` method, when annotations that involve window functions are being added to a queryset, the ORM should verify if the underlying database engine (in contexts where SQLite is used) supports window functions. This could be achieved by consulting a feature flag (as described previously) indicating the availability of window functions. If the feature is unsupported, this method should raise a Django-specific exception (e.g., `FeatureNotSupportedError`) with a clear message indicating the absence of window function support in the SQLite version being used. This check would help developers understand the limitations of their current environment, rather than facing a low-level `OperationalError` from the SQLite engine.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "…(remaining elements truncated)…"
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError
284         assert ((not isinstance(k, slice) and (k >= 0)) or
285                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
286                  (k.stop is None or k.stop >= 0))), \
287             "Negative indexing is not supported."
288 
289         if self._result_cache is not None:
290             return self._result_cache[k]
291 
292         if isinstance(k, slice):
293             qs = self._chain()
294             if k.start is not None:
295                 start = int(k.start)
296             else:
297                 start = None
298             if k.stop is not None:
299                 stop = int(k.stop)
300             else:
301                 stop = None
302             qs.query.set_limits(start, stop)
303             return list(qs)[::k.step] if k.step else qs
304 
305         qs = self._chain()
306         qs.query.set_limits(k, k + 1)
307         qs._fetch_all()
308         return qs._result_cache[0]
309 
310     def __and__(self, other):
311         self._merge_sanity_check(other)
312         if isinstance(other, EmptyQuerySet):
313             return other
314         if isinstance(self, EmptyQuerySet):
315             return self
316         combined = self._chain()
317         combined._merge_known_related_objects(other)
318         combined.query.combine(other.query, sql.AND)
319         return combined
320 
321     def __or__(self, other):
322         self._merge_sanity_check(other)
323         if isinstance(self, EmptyQuerySet):
324             return other
325         if isinstance(other, EmptyQuerySet):
326             return self
327         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
328         combined = query._chain()
329         combined._merge_known_related_objects(other)
330         if not other.query.can_filter():
331             other = other.model._base_manager.filter(pk__in=other.values('pk'))
332         combined.query.combine(other.query, sql.OR)
333         return combined
334 
335     ####################################
336     # METHODS THAT DO DATABASE QUERIES #
337     ####################################
338 
339     def _iterator(self, use_chunked_fetch, chunk_size):
340         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
341 
342     def iterator(self, chunk_size=2000):
343         """
344         An iterator over the results from applying this QuerySet to the
345         database.
346         """
347         if chunk_size <= 0:
348             raise ValueError('Chunk size must be strictly positive.')
349         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
350         return self._iterator(use_chunked_fetch, chunk_size)
351 
352     def aggregate(self, *args, **kwargs):
353         """
354         Return a dictionary containing the calculations (aggregation)
355         over the current queryset.
356 
357         If args is present the expression is passed as a kwarg using
358         the Aggregate object's default alias.
359         """
360         if self.query.distinct_fields:
361             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
362         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
363         for arg in args:
364             # The default_alias property raises TypeError if default_alias
365             # can't be set automatically or AttributeError if it isn't an
366             # attribute.
367             try:
368                 arg.default_alias
369             except (AttributeError, TypeError):
370                 raise TypeError("Complex aggregates require an alias")
371             kwargs[arg.default_alias] = arg
372 
373         query = self.query.chain()
374         for (alias, aggregate_expr) in kwargs.items():
375             query.add_annotation(aggregate_expr, alias, is_summary=True)
376             if not query.annotations[alias].contains_aggregate:
377                 raise TypeError("%s is not an aggregate expression" % alias)
378         return query.get_aggregation(self.db, kwargs)
379 
380     def count(self):
381         """
382         Perform a SELECT COUNT() and return the number of records as an
383         integer.
384 
385         If the QuerySet is already fully cached, return the length of the
386         cached results set to avoid multiple SELECT COUNT(*) calls.
387         """
388         if self._result_cache is not None:
389             return len(self._result_cache)
390 
391         return self.query.get_count(using=self.db)
392 
393     def get(self, *args, **kwargs):
394         """
395         Perform the query and return a single object matching the given
396         keyword arguments.
397         """
398         clone = self.filter(*args, **kwargs)
399         if self.query.can_filter() and not self.query.distinct_fields:
400             clone = clone.order_by()
401         num = len(clone)
402         if num == 1:
403             return clone._result_cache[0]
404         if not num:
405             raise self.model.DoesNotExist(
406                 "%s matching query does not exist." %
407                 self.model._meta.object_name
408             )
409         raise self.model.MultipleObjectsReturned(
410             "get() returned more than one %s -- it returned %s!" %
411             (self.model._meta.object_name, num)
412         )
413 
414     def create(self, **kwargs):
415         """
416         Create a new object with the given kwargs, saving it to the database
417         and returning the created object.
418         """
419         obj = self.model(**kwargs)
420         self._for_write = True
421         obj.save(force_insert=True, using=self.db)
422         return obj
423 
424     def _populate_pk_values(self, objs):
425         for obj in objs:
426             if obj.pk is None:
427                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
428 
429     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
430         """
431         Insert each of the instances into the database. Do *not* call
432         save() on each of the instances, do not send any pre/post_save
433         signals, and do not set the primary key attribute if it is an
434         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
435         Multi-table models are not supported.
436         """
437         # When you bulk insert you don't get the primary keys back (if it's an
438         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
439         # you can't insert into the child tables which references this. There
440         # are two workarounds:
441         # 1) This could be implemented if you didn't have an autoincrement pk
442         # 2) You could do it by doing O(n) normal inserts into the parent
443         #    tables to get the primary keys back and then doing a single bulk
444         #    insert into the childmost table.
445         # We currently set the primary keys on the objects when using
446         # PostgreSQL via the RETURNING ID clause. It should be possible for
447         # Oracle as well, but the semantics for extracting the primary keys is
448         # trickier so it's not done yet.
449         assert batch_size is None or batch_size > 0
450         # Check that the parents share the same concrete model with the our
451         # model to detect the inheritance pattern ConcreteGrandParent ->
452         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
453         # would not identify that case as involving multiple tables.
454         for parent in self.model._meta.get_parent_list():
455             if parent._meta.concrete_model is not self.model._meta.concrete_model:
456                 raise ValueError("Can't bulk create a multi-table inherited model")
457         if not objs:
458             return objs
459         self._for_write = True
460         connection = connections[self.db]
461         fields = self.model._meta.concrete_fields
462         objs = list(objs)
463         self._populate_pk_values(objs)
464         with transaction.atomic(using=self.db, savepoint=False):
465             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
466             if objs_with_pk:
467                 self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
468                 for obj_with_pk in objs_with_pk:
469                     obj_with_pk._state.adding = False
470                     obj_with_pk._state.db = self.db
471             if objs_without_pk:
472                 fields = [f for f in fields if not isinstance(f, AutoField)]
473                 ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
474                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
475                     assert len(ids) == len(objs_without_pk)
476                 for obj_without_pk, pk in zip(objs_without_pk, ids):
477                     obj_without_pk.pk = pk
478                     obj_without_pk._state.adding = False
479                     obj_without_pk._state.db = self.db
480 
481         return objs
482 
483     def bulk_update(self, objs, fields, batch_size=None):
484         """
485         Update the given fields in each of the given objects in the database.
486         """
487         if batch_size is not None and batch_size < 0:
488             raise ValueError('Batch size must be a positive integer.')
489         if not fields:
490             raise ValueError('Field names must be given to bulk_update().')
491         objs = tuple(objs)
492         if any(obj.pk is None for obj in objs):
493             raise ValueError('All bulk_update() objects must have a primary key set.')
494         fields = [self.model._meta.get_field(name) for name in fields]
495         if any(not f.concrete or f.many_to_many for f in fields):
496             raise ValueError('bulk_update() can only be used with concrete fields.')
497         if any(f.primary_key for f in fields):
498             raise ValueError('bulk_update() cannot be used with primary key fields.')
499         if not objs:
500             return
501         # PK is used twice in the resulting update query, once in the filter
502         # and once in the WHEN. Each field will also have one CAST.
503         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
504         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
505         requires_casting = connections[self.db].features.requires_casted_case_in_updates
506         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
507         updates = []
508         for batch_objs in batches:
509             update_kwargs = {}
510             for field in fields:
511                 when_statements = []
512                 for obj in batch_objs:
513                     attr = getattr(obj, field.attname)
514                     if not isinstance(attr, Expression):
515                         attr = Value(attr, output_field=field)
516                     when_statements.append(When(pk=obj.pk, then=attr))
517                 case_statement = Case(*when_statements, output_field=field)
518                 if requires_casting:
519                     case_statement = Cast(case_statement, output_field=field)
520                 update_kwargs[field.attname] = case_statement
521             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
522         with transaction.atomic(using=self.db, savepoint=False):
523             for pks, update_kwargs in updates:
524                 self.filter(pk__in=pks).update(**update_kwargs)
525     bulk_update.alters_data = True
526 
527     def get_or_create(self, defaults=None, **kwargs):
528         """
529         Look up an object with the given kwargs, creating one if necessary.
530         Return a tuple of (object, created), where created is a boolean
531         specifying whether an object was created.
532         """
533         # The get() needs to be targeted at the write database in order
534         # to avoid potential transaction consistency problems.
535         self._for_write = True
536         try:
537             return self.get(**kwargs), False
538         except self.model.DoesNotExist:
539             params = self._extract_model_params(defaults, **kwargs)
540             return self._create_object_from_params(kwargs, params)
541 
542     def update_or_create(self, defaults=None, **kwargs):
543         """
544         Look up an object with the given kwargs, updating one with defaults
545         if it exists, otherwise create a new one.
546         Return a tuple (object, created), where created is a boolean
547         specifying whether an object was created.
548         """
549         defaults = defaults or {}
550         self._for_write = True
551         with transaction.atomic(using=self.db):
552             try:
553                 obj = self.select_for_update().get(**kwargs)
554             except self.model.DoesNotExist:
555                 params = self._extract_model_params(defaults, **kwargs)
556                 # Lock the row so that a concurrent update is blocked until
557                 # after update_or_create() has performed its save.
558                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
559                 if created:
560                     return obj, created
561             for k, v in defaults.items():
562                 setattr(obj, k, v() if callable(v) else v)
563             obj.save(using=self.db)
564         return obj, False
565 
566     def _create_object_from_params(self, lookup, params, lock=False):
567         """
568         Try to create an object using passed params. Used by get_or_create()
569         and update_or_create().
570         """
571         try:
572             with transaction.atomic(using=self.db):
573                 params = {k: v() if callable(v) else v for k, v in params.items()}
574                 obj = self.create(**params)
575             return obj, True
576         except IntegrityError as e:
577             try:
578                 qs = self.select_for_update() if lock else self
579                 return qs.get(**lookup), False
580             except self.model.DoesNotExist:
581                 pass
582             raise e
583 
584     def _extract_model_params(self, defaults, **kwargs):
585         """
586         Prepare `params` for creating a model instance based on the given
587         kwargs; for use by get_or_create() and update_or_create().
588         """
589         defaults = defaults or {}
590         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
591         params.update(defaults)
592         property_names = self.model._meta._property_names
593         invalid_params = []
594         for param in params:
595             try:
596                 self.model._meta.get_field(param)
597             except exceptions.FieldDoesNotExist:
598                 # It's okay to use a model's property if it has a setter.
599                 if not (param in property_names and getattr(self.model, param).fset):
600                     invalid_params.append(param)
601         if invalid_params:
602             raise exceptions.FieldError(
603                 "Invalid field name(s) for model %s: '%s'." % (
604                     self.model._meta.object_name,
605                     "', '".join(sorted(invalid_params)),
606                 ))
607         return params
608 
609     def _earliest(self, *fields):
610         """
611         Return the earliest object according to fields (if given) or by the
612         model's Meta.get_latest_by.
613         """
614         if fields:
615             order_by = fields
616         else:
617             order_by = getattr(self.model._meta, 'get_latest_by')
618             if order_by and not isinstance(order_by, (tuple, list)):
619                 order_by = (order_by,)
620         if order_by is None:
621             raise ValueError(
622                 "earliest() and latest() require either fields as positional "
623                 "arguments or 'get_latest_by' in the model's Meta."
624             )
625 
626         assert self.query.can_filter(), \
627             "Cannot change a query once a slice has been taken."
628         obj = self._chain()
629         obj.query.set_limits(high=1)
630         obj.query.clear_ordering(force_empty=True)
631         obj.query.add_ordering(*order_by)
632         return obj.get()
633 
634     def earliest(self, *fields):
635         return self._earliest(*fields)
636 
637     def latest(self, *fields):
638         return self.reverse()._earliest(*fields)
639 
640     def first(self):
641         """Return the first object of a query or None if no match is found."""
642         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
643             return obj
644 
645     def last(self):
646         """Return the last object of a query or None if no match is found."""
647         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
648             return obj
649 
650     def in_bulk(self, id_list=None, *, field_name='pk'):
651         """
652         Return a dictionary mapping each of the given IDs to the object with
653         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
654         """
655         assert self.query.can_filter(), \
656             "Cannot use 'limit' or 'offset' with in_bulk"
657         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
658             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
659         if id_list is not None:
660             if not id_list:
661                 return {}
662             filter_key = '{}__in'.format(field_name)
663             batch_size = connections[self.db].features.max_query_params
664             id_list = tuple(id_list)
665             # If the database has a limit on the number of query parameters
666             # (e.g. SQLite), retrieve objects in batches if necessary.
667             if batch_size and batch_size < len(id_list):
668                 qs = ()
669                 for offset in range(0, len(id_list), batch_size):
670                     batch = id_list[offset:offset + batch_size]
671                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
672             else:
673                 qs = self.filter(**{filter_key: id_list}).order_by()
674         else:
675             qs = self._chain()
676         return {getattr(obj, field_name): obj for obj in qs}
677 
678     def delete(self):
679         """Delete the records in the current QuerySet."""
680         assert self.query.can_filter(), \
681             "Cannot use 'limit' or 'offset' with delete."
682 
683         if self._fields is not None:
684             raise TypeError("Cannot call delete() after .values() or .values_list()")
685 
686         del_query = self._chain()
687 
688         # The delete is actually 2 queries - one to find related objects,
689         # and one to delete. Make sure that the discovery of related
690         # objects is performed on the same database as the deletion.
691         del_query._for_write = True
692 
693         # Disable non-supported fields.
694         del_query.query.select_for_update = False
695         del_query.query.select_related = False
696         del_query.query.clear_ordering(force_empty=True)
697 
698         collector = Collector(using=del_query.db)
699         collector.collect(del_query)
700         deleted, _rows_count = collector.delete()
701 
702         # Clear the result cache, in case this QuerySet gets reused.
703         self._result_cache = None
704         return deleted, _rows_count
705 
706     delete.alters_data = True
707     delete.queryset_only = True
708 
709     def _raw_delete(self, using):
710         """
711         Delete objects found from the given queryset in single direct SQL
712         query. No signals are sent and there is no protection for cascades.
713         """
714         return sql.DeleteQuery(self.model).delete_qs(self, using)
715     _raw_delete.alters_data = True
716 
717     def update(self, **kwargs):
718         """
719         Update all elements in the current QuerySet, setting all the given
720         fields to the appropriate values.
721         """
722         assert self.query.can_filter(), \
723             "Cannot update a query once a slice has been taken."
724         self._for_write = True
725         query = self.query.chain(sql.UpdateQuery)
726         query.add_update_values(kwargs)
727         # Clear any annotations so that they won't be present in subqueries.
728         query.annotations = {}
729         with transaction.mark_for_rollback_on_error(using=self.db):
730             rows = query.get_compiler(self.db).execute_sql(CURSOR)
731         self._result_cache = None
732         return rows
733     update.alters_data = True
734 
735     def _update(self, values):
736         """
737         A version of update() that accepts field objects instead of field names.
738         Used primarily for model saving and not intended for use by general
739         code (it requires too much poking around at model internals to be
740         useful at that level).
741         """
742         assert self.query.can_filter(), \
743             "Cannot update a query once a slice has been taken."
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_fields(values)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         self._result_cache = None
749         return query.get_compiler(self.db).execute_sql(CURSOR)
750     _update.alters_data = True
751     _update.queryset_only = False
752 
753     def exists(self):
754         if self._result_cache is None:
755             return self.query.has_results(using=self.db)
756         return bool(self._result_cache)
757 
758     def _prefetch_related_objects(self):
759         # This method can only be called once the result cache has been filled.
760         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
761         self._prefetch_done = True
762 
763     def explain(self, *, format=None, **options):
764         return self.query.explain(using=self.db, format=format, **options)
765 
766     ##################################################
767     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
768     ##################################################
769 
770     def raw(self, raw_query, params=None, translations=None, using=None):
771         if using is None:
772             using = self.db
773         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
774         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
775         return qs
776 
777     def _values(self, *fields, **expressions):
778         clone = self._chain()
779         if expressions:
780             clone = clone.annotate(**expressions)
781         clone._fields = fields
782         clone.query.set_values(fields)
783         return clone
784 
785     def values(self, *fields, **expressions):
786         fields += tuple(expressions)
787         clone = self._values(*fields, **expressions)
788         clone._iterable_class = ValuesIterable
789         return clone
790 
791     def values_list(self, *fields, flat=False, named=False):
792         if flat and named:
793             raise TypeError("'flat' and 'named' can't be used together.")
794         if flat and len(fields) > 1:
795             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
796 
797         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
798         _fields = []
799         expressions = {}
800         counter = 1
801         for field in fields:
802             if hasattr(field, 'resolve_expression'):
803                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
804                 while True:
805                     field_id = field_id_prefix + str(counter)
806                     counter += 1
807                     if field_id not in field_names:
808                         break
809                 expressions[field_id] = field
810                 _fields.append(field_id)
811             else:
812                 _fields.append(field)
813 
814         clone = self._values(*_fields, **expressions)
815         clone._iterable_class = (
816             NamedValuesListIterable if named
817             else FlatValuesListIterable if flat
818             else ValuesListIterable
819         )
820         return clone
821 
822     def dates(self, field_name, kind, order='ASC'):
823         """
824         Return a list of date objects representing all available dates for
825         the given field_name, scoped to 'kind'.
826         """
827         assert kind in ('year', 'month', 'week', 'day'), \
828             "'kind' must be one of 'year', 'month', 'week', or 'day'."
829         assert order in ('ASC', 'DESC'), \
830             "'order' must be either 'ASC' or 'DESC'."
831         return self.annotate(
832             datefield=Trunc(field_name, kind, output_field=DateField()),
833             plain_field=F(field_name)
834         ).values_list(
835             'datefield', flat=True
836         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
837 
838     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
839         """
840         Return a list of datetime objects representing all available
841         datetimes for the given field_name, scoped to 'kind'.
842         """
843         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
844             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
845         assert order in ('ASC', 'DESC'), \
846             "'order' must be either 'ASC' or 'DESC'."
847         if settings.USE_TZ:
848             if tzinfo is None:
849                 tzinfo = timezone.get_current_timezone()
850         else:
851             tzinfo = None
852         return self.annotate(
853             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
854             plain_field=F(field_name)
855         ).values_list(
856             'datetimefield', flat=True
857         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
858 
859     def none(self):
860         """Return an empty QuerySet."""
861         clone = self._chain()
862         clone.query.set_empty()
863         return clone
864 
865     ##################################################################
866     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
867     ##################################################################
868 
869     def all(self):
870         """
871         Return a new QuerySet that is a copy of the current one. This allows a
872         QuerySet to proxy for a model manager in some cases.
873         """
874         return self._chain()
875 
876     def filter(self, *args, **kwargs):
877         """
878         Return a new QuerySet instance with the args ANDed to the existing
879         set.
880         """
881         return self._filter_or_exclude(False, *args, **kwargs)
882 
883     def exclude(self, *args, **kwargs):
884         """
885         Return a new QuerySet instance with NOT (args) ANDed to the existing
886         set.
887         """
888         return self._filter_or_exclude(True, *args, **kwargs)
889 
890     def _filter_or_exclude(self, negate, *args, **kwargs):
891         if args or kwargs:
892             assert self.query.can_filter(), \
893                 "Cannot filter a query once a slice has been taken."
894 
895         clone = self._chain()
896         if negate:
897             clone.query.add_q(~Q(*args, **kwargs))
898         else:
899             clone.query.add_q(Q(*args, **kwargs))
900         return clone
901 
902     def complex_filter(self, filter_obj):
903         """
904         Return a new QuerySet instance with filter_obj added to the filters.
905 
906         filter_obj can be a Q object or a dictionary of keyword lookup
907         arguments.
908 
909         This exists to support framework features such as 'limit_choices_to',
910         and usually it will be more natural to use other methods.
911         """
912         if isinstance(filter_obj, Q):
913             clone = self._chain()
914             clone.query.add_q(filter_obj)
915             return clone
916         else:
917             return self._filter_or_exclude(None, **filter_obj)
918 
919     def _combinator_query(self, combinator, *other_qs, all=False):
920         # Clone the query to inherit the select list and everything
921         clone = self._chain()
922         # Clear limits and ordering so they can be reapplied
923         clone.query.clear_ordering(True)
924         clone.query.clear_limits()
925         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
926         clone.query.combinator = combinator
927         clone.query.combinator_all = all
928         return clone
929 
930     def union(self, *other_qs, all=False):
931         # If the query is an EmptyQuerySet, combine all nonempty querysets.
932         if isinstance(self, EmptyQuerySet):
933             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
934             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
935         return self._combinator_query('union', *other_qs, all=all)
936 
937     def intersection(self, *other_qs):
938         # If any query is an EmptyQuerySet, return it.
939         if isinstance(self, EmptyQuerySet):
940             return self
941         for other in other_qs:
942             if isinstance(other, EmptyQuerySet):
943                 return other
944         return self._combinator_query('intersection', *other_qs)
945 
946     def difference(self, *other_qs):
947         # If the query is an EmptyQuerySet, return it.
948         if isinstance(self, EmptyQuerySet):
949             return self
950         return self._combinator_query('difference', *other_qs)
951 
952     def select_for_update(self, nowait=False, skip_locked=False, of=()):
953         """
954         Return a new QuerySet instance that will select objects with a
955         FOR UPDATE lock.
956         """
957         if nowait and skip_locked:
958             raise ValueError('The nowait option cannot be used with skip_locked.')
959         obj = self._chain()
960         obj._for_write = True
961         obj.query.select_for_update = True
962         obj.query.select_for_update_nowait = nowait
963         obj.query.select_for_update_skip_locked = skip_locked
964         obj.query.select_for_update_of = of
965         return obj
966 
967     def select_related(self, *fields):
968         """
969         Return a new QuerySet instance that will select related objects.
970 
971         If fields are specified, they must be ForeignKey fields and only those
972         related objects are included in the selection.
973 
974         If select_related(None) is called, clear the list.
975         """
976 
977         if self._fields is not None:
978             raise TypeError("Cannot call select_related() after .values() or .values_list()")
979 
980         obj = self._chain()
981         if fields == (None,):
982             obj.query.select_related = False
983         elif fields:
984             obj.query.add_select_related(fields)
985         else:
986             obj.query.select_related = True
987         return obj
988 
989     def prefetch_related(self, *lookups):
990         """
991         Return a new QuerySet instance that will prefetch the specified
992         Many-To-One and Many-To-Many related objects when the QuerySet is
993         evaluated.
994 
995         When prefetch_related() is called more than once, append to the list of
996         prefetch lookups. If prefetch_related(None) is called, clear the list.
997         """
998         clone = self._chain()
999         if lookups == (None,):
1000             clone._prefetch_related_lookups = ()
1001         else:
1002             for lookup in lookups:
1003                 if isinstance(lookup, Prefetch):
1004                     lookup = lookup.prefetch_to
1005                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1006                 if lookup in self.query._filtered_relations:
1007                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1008             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1009         return clone
1010 
1011     def annotate(self, *args, **kwargs):
1012         """
1013         Return a query set in which the returned objects have been annotated
1014         with extra data or aggregations.
1015         """
1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1017         annotations = {}
1018         for arg in args:
1019             # The default_alias property may raise a TypeError.
1020             try:
1021                 if arg.default_alias in kwargs:
1022                     raise ValueError("The named annotation '%s' conflicts with the "
1023                                      "default name for another annotation."
1024                                      % arg.default_alias)
1025             except TypeError:
1026                 raise TypeError("Complex annotations require an alias")
1027             annotations[arg.default_alias] = arg
1028         annotations.update(kwargs)
1029 
1030         clone = self._chain()
1031         names = self._fields
1032         if names is None:
1033             names = set(chain.from_iterable(
1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1035                 for field in self.model._meta.get_fields()
1036             ))
1037 
1038         for alias, annotation in annotations.items():
1039             if alias in names:
1040                 raise ValueError("The annotation '%s' conflicts with a field on "
1041                                  "the model." % alias)
1042             if isinstance(annotation, FilteredRelation):
1043                 clone.query.add_filtered_relation(annotation, alias)
1044             else:
1045                 clone.query.add_annotation(annotation, alias, is_summary=False)
1046 
1047         for alias, annotation in clone.query.annotations.items():
1048             if alias in annotations and annotation.contains_aggregate:
1049                 if clone._fields is None:
1050                     clone.query.group_by = True
1051                 else:
1052                     clone.query.set_group_by()
1053                 break
1054 
1055         return clone
1056 
1057     def order_by(self, *field_names):
1058         """Return a new QuerySet instance with the ordering changed."""
1059         assert self.query.can_filter(), \
1060             "Cannot reorder a query once a slice has been taken."
1061         obj = self._chain()
1062         obj.query.clear_ordering(force_empty=False)
1063         obj.query.add_ordering(*field_names)
1064         return obj
1065 
1066     def distinct(self, *field_names):
1067         """
1068         Return a new QuerySet instance that will select only distinct results.
1069         """
1070         assert self.query.can_filter(), \
1071             "Cannot create distinct fields once a slice has been taken."
1072         obj = self._chain()
1073         obj.query.add_distinct_fields(*field_names)
1074         return obj
1075 
1076     def extra(self, select=None, where=None, params=None, tables=None,
1077               order_by=None, select_params=None):
1078         """Add extra SQL fragments to the query."""
1079         assert self.query.can_filter(), \
1080             "Cannot change a query once a slice has been taken"
1081         clone = self._chain()
1082         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1083         return clone
1084 
1085     def reverse(self):
1086         """Reverse the ordering of the QuerySet."""
1087         if not self.query.can_filter():
1088             raise TypeError('Cannot reverse a query once a slice has been taken.')
1089         clone = self._chain()
1090         clone.query.standard_ordering = not clone.query.standard_ordering
1091         return clone
1092 
1093     def defer(self, *fields):
1094         """
1095         Defer the loading of data for certain fields until they are accessed.
1096         Add the set of deferred fields to any existing set of deferred fields.
1097         The only exception to this is if None is passed in as the only
1098         parameter, in which case removal all deferrals.
1099         """
1100         if self._fields is not None:
1101             raise TypeError("Cannot call defer() after .values() or .values_list()")
1102         clone = self._chain()
1103         if fields == (None,):
1104             clone.query.clear_deferred_loading()
1105         else:
1106             clone.query.add_deferred_loading(fields)
1107         return clone
1108 
1109     def only(self, *fields):
1110         """
1111         Essentially, the opposite of defer(). Only the fields passed into this
1112         method and that are not already specified as deferred are loaded
1113         immediately when the queryset is evaluated.
1114         """
1115         if self._fields is not None:
1116             raise TypeError("Cannot call only() after .values() or .values_list()")
1117         if fields == (None,):
1118             # Can only pass None to defer(), not only(), as the rest option.
1119             # That won't stop people trying to do this, so let's be explicit.
1120             raise TypeError("Cannot pass None as an argument to only().")
1121         for field in fields:
1122             field = field.split(LOOKUP_SEP, 1)[0]
1123             if field in self.query._filtered_relations:
1124                 raise ValueError('only() is not supported with FilteredRelation.')
1125         clone = self._chain()
1126         clone.query.add_immediate_loading(fields)
1127         return clone
1128 
1129     def using(self, alias):
1130         """Select which database this QuerySet should execute against."""
1131         clone = self._chain()
1132         clone._db = alias
1133         return clone
1134 
1135     ###################################
1136     # PUBLIC INTROSPECTION ATTRIBUTES #
1137     ###################################
1138 
1139     @property
1140     def ordered(self):
1141         """
1142         Return True if the QuerySet is ordered -- i.e. has an order_by()
1143         clause or a default ordering on the model (or is empty).
1144         """
1145         if isinstance(self, EmptyQuerySet):
1146             return True
1147         if self.query.extra_order_by or self.query.order_by:
1148             return True
1149         elif self.query.default_ordering and self.query.get_meta().ordering:
1150             return True
1151         else:
1152             return False
1153 
1154     @property
1155     def db(self):
1156         """Return the database used if this query is executed now."""
1157         if self._for_write:
1158             return self._db or router.db_for_write(self.model, **self._hints)
1159         return self._db or router.db_for_read(self.model, **self._hints)
1160 
1161     ###################
1162     # PRIVATE METHODS #
1163     ###################
1164 
1165     def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):
1166         """
1167         Insert a new record for the given model. This provides an interface to
1168         the InsertQuery class and is how Model.save() is implemented.
1169         """
1170         self._for_write = True
1171         if using is None:
1172             using = self.db
1173         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1174         query.insert_values(fields, objs, raw=raw)
1175         return query.get_compiler(using=using).execute_sql(return_id)
1176     _insert.alters_data = True
1177     _insert.queryset_only = False
1178 
1179     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1180         """
1181         Helper method for bulk_create() to insert objs one batch at a time.
1182         """
1183         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1184             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1185         ops = connections[self.db].ops
1186         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1187         inserted_ids = []
1188         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1189         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1190             if bulk_return and not ignore_conflicts:
1191                 inserted_id = self._insert(
1192                     item, fields=fields, using=self.db, return_id=True,
1193                     ignore_conflicts=ignore_conflicts,
1194                 )
1195                 if isinstance(inserted_id, list):
1196                     inserted_ids.extend(inserted_id)
1197                 else:
1198                     inserted_ids.append(inserted_id)
1199             else:
1200                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1201         return inserted_ids
1202 
1203     def _chain(self, **kwargs):
1204         """
1205         Return a copy of the current QuerySet that's ready for another
1206         operation.
1207         """
1208         obj = self._clone()
1209         if obj._sticky_filter:
1210             obj.query.filter_is_sticky = True
1211             obj._sticky_filter = False
1212         obj.__dict__.update(kwargs)
1213         return obj
1214 
1215     def _clone(self):
1216         """
1217         Return a copy of the current QuerySet. A lightweight alternative
1218         to deepcopy().
1219         """
1220         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1221         c._sticky_filter = self._sticky_filter
1222         c._for_write = self._for_write
1223         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1224         c._known_related_objects = self._known_related_objects
1225         c._iterable_class = self._iterable_class
1226         c._fields = self._fields
1227         return c
1228 
1229     def _fetch_all(self):
1230         if self._result_cache is None:
1231             self._result_cache = list(self._iterable_class(self))
1232         if self._prefetch_related_lookups and not self._prefetch_done:
1233             self._prefetch_related_objects()
1234 
1235     def _next_is_sticky(self):
1236         """
1237         Indicate that the next filter call and the one following that should
1238         be treated as a single filter. This is only important when it comes to
1239         determining when to reuse tables for many-to-many filters. Required so
1240         that we can filter naturally on the results of related managers.
1241 
1242         This doesn't return a clone of the current QuerySet (it returns
1243         "self"). The method is only used internally and should be immediately
1244         followed by a filter() that does create a clone.
1245         """
1246         self._sticky_filter = True
1247         return self
1248 
1249     def _merge_sanity_check(self, other):
1250         """Check that two QuerySet classes may be merged."""
1251         if self._fields is not None and (
1252                 set(self.query.values_select) != set(other.query.values_select) or
1253                 set(self.query.extra_select) != set(other.query.extra_select) or
1254                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1255             raise TypeError(
1256                 "Merging '%s' classes must involve the same values in each case."
1257                 % self.__class__.__name__
1258             )
1259 
1260     def _merge_known_related_objects(self, other):
1261         """
1262         Keep track of all known related objects from either QuerySet instance.
1263         """
1264         for field, objects in other._known_related_objects.items():
1265             self._known_related_objects.setdefault(field, {}).update(objects)
1266 
1267     def resolve_expression(self, *args, **kwargs):
1268         if self._fields and len(self._fields) > 1:
1269             # values() queryset can only be used as nested queries
1270             # if they are set up to select only a single field.
1271             raise TypeError('Cannot use multi-field values as a filter value.')
1272         query = self.query.resolve_expression(*args, **kwargs)
1273         query._db = self._db
1274         return query
1275     resolve_expression.queryset_only = True
1276 
1277     def _add_hints(self, **hints):
1278         """
1279         Update hinting information for use by routers. Add new key/values or
1280         overwrite existing key/values.
1281         """
1282         self._hints.update(hints)
1283 
1284     def _has_filters(self):
1285         """
1286         Check if this QuerySet has any filtering going on. This isn't
1287         equivalent with checking if all objects are present in results, for
1288         example, qs[1:]._has_filters() -> False.
1289         """
1290         return self.query.has_filters()
1291 
1292     @staticmethod
1293     def _validate_values_are_expressions(values, method_name):
1294         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1295         if invalid_args:
1296             raise TypeError(
1297                 'QuerySet.%s() received non-expression(s): %s.' % (
1298                     method_name,
1299                     ', '.join(invalid_args),
1300                 )
1301             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 22:28:41.856 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 22:28:41.856 | INFO     | Search-only: skipping patch selection
2025-10-17 22:28:42.057 | INFO     | Task django__django-10853 completed successfully.
2025-10-17 22:28:42.058 | INFO     | No patch generated. You can try running ACR again.
