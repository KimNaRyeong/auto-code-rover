2025-10-18 00:18:02.936 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 00:18:06.577 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=534, output_tokens=12, cost=0.005700
2025-10-18 00:18:19.852 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=714, output_tokens=631, cost=0.026070
2025-10-18 00:18:21.099 | INFO     | registered non reproducing test 0
2025-10-18 00:18:36.297 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1618, output_tokens=745, cost=0.038530
2025-10-18 00:18:37.450 | INFO     | registered non reproducing test 1
2025-10-18 00:18:53.452 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2361, output_tokens=772, cost=0.046770
2025-10-18 00:18:54.620 | INFO     | registered non reproducing test 2
2025-10-18 00:18:54.620 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 00:18:54.626 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 00:19:03.100 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=842, output_tokens=367, cost=0.019430
2025-10-18 00:19:03.110 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 00:19:05.958 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=721, output_tokens=63, cost=0.009100
2025-10-18 00:19:05.960 | DEBUG    | Extracted a valid json.
2025-10-18 00:19:05.960 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"ForeignKey\")",
        "search_method(\"prefetch_related\")",
        "search_method(\"get_prefetch_queryset\")",
        "search_method(\"build_filter\")",
        "search_code(\"__in\")"
    ],
    "bug_locations": []
}
2025-10-18 00:19:07.247 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 00:19:07.248 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 00:19:27.333 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=10340, output_tokens=846, cost=0.128780
2025-10-18 00:19:27.346 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 00:19:40.228 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=11402, output_tokens=436, cost=0.127100
2025-10-18 00:19:40.237 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 00:19:43.986 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=790, output_tokens=153, cost=0.012490
2025-10-18 00:19:43.987 | DEBUG    | Extracted a valid json.
2025-10-18 00:19:43.987 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "Implementation logic of prefetch_related",
            "intended_behavior": "The method responsible for constructing IN queries as part of prefetch_related operations should be adjusted to exclude NULL values from the list of IDs used in the IN clause."
        },
        {
            "file": "django/db/models/sql/query.py",
            "class": "Query",
            "method": "build_filter",
            "intended_behavior": "Review should ensure that modifications to handle the exclusion of NULL values in IN lookups do not inadvertently affect the general functionality of build_filter."
        }
    ]
}
2025-10-18 00:19:44.008 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, *args, **kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, *args, **kwargs)
951 
952     def _filter_or_exclude(self, negate, *args, **kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, **filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone
1128 
1129     def order_by(self, *field_names):
1130         """Return a new QuerySet instance with the ordering changed."""
1131         assert not self.query.is_sliced, \
1132             "Cannot reorder a query once a slice has been taken."
1133         obj = self._chain()
1134         obj.query.clear_ordering(force_empty=False)
1135         obj.query.add_ordering(*field_names)
1136         return obj
1137 
1138     def distinct(self, *field_names):
1139         """
1140         Return a new QuerySet instance that will select only distinct results.
1141         """
1142         self._not_support_combined_queries('distinct')
1143         assert not self.query.is_sliced, \
1144             "Cannot create distinct fields once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.add_distinct_fields(*field_names)
1147         return obj
1148 
1149     def extra(self, select=None, where=None, params=None, tables=None,
1150               order_by=None, select_params=None):
1151         """Add extra SQL fragments to the query."""
1152         self._not_support_combined_queries('extra')
1153         assert not self.query.is_sliced, \
1154             "Cannot change a query once a slice has been taken"
1155         clone = self._chain()
1156         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1157         return clone
1158 
1159     def reverse(self):
1160         """Reverse the ordering of the QuerySet."""
1161         if self.query.is_sliced:
1162             raise TypeError('Cannot reverse a query once a slice has been taken.')
1163         clone = self._chain()
1164         clone.query.standard_ordering = not clone.query.standard_ordering
1165         return clone
1166 
1167     def defer(self, *fields):
1168         """
1169         Defer the loading of data for certain fields until they are accessed.
1170         Add the set of deferred fields to any existing set of deferred fields.
1171         The only exception to this is if None is passed in as the only
1172         parameter, in which case removal all deferrals.
1173         """
1174         self._not_support_combined_queries('defer')
1175         if self._fields is not None:
1176             raise TypeError("Cannot call defer() after .values() or .values_list()")
1177         clone = self._chain()
1178         if fields == (None,):
1179             clone.query.clear_deferred_loading()
1180         else:
1181             clone.query.add_deferred_loading(fields)
1182         return clone
1183 
1184     def only(self, *fields):
1185         """
1186         Essentially, the opposite of defer(). Only the fields passed into this
1187         method and that are not already specified as deferred are loaded
1188         immediately when the queryset is evaluated.
1189         """
1190         self._not_support_combined_queries('only')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call only() after .values() or .values_list()")
1193         if fields == (None,):
1194             # Can only pass None to defer(), not only(), as the rest option.
1195             # That won't stop people trying to do this, so let's be explicit.
1196             raise TypeError("Cannot pass None as an argument to only().")
1197         for field in fields:
1198             field = field.split(LOOKUP_SEP, 1)[0]
1199             if field in self.query._filtered_relations:
1200                 raise ValueError('only() is not supported with FilteredRelation.')
1201         clone = self._chain()
1202         clone.query.add_immediate_loading(fields)
1203         return clone
1204 
1205     def using(self, alias):
1206         """Select which database this QuerySet should execute against."""
1207         clone = self._chain()
1208         clone._db = alias
1209         return clone
1210 
1211     ###################################
1212     # PUBLIC INTROSPECTION ATTRIBUTES #
1213     ###################################
1214 
1215     @property
1216     def ordered(self):
1217         """
1218         Return True if the QuerySet is ordered -- i.e. has an order_by()
1219         clause or a default ordering on the model (or is empty).
1220         """
1221         if isinstance(self, EmptyQuerySet):
1222             return True
1223         if self.query.extra_order_by or self.query.order_by:
1224             return True
1225         elif self.query.default_ordering and self.query.get_meta().ordering:
1226             return True
1227         else:
1228             return False
1229 
1230     @property
1231     def db(self):
1232         """Return the database used if this query is executed now."""
1233         if self._for_write:
1234             return self._db or router.db_for_write(self.model, **self._hints)
1235         return self._db or router.db_for_read(self.model, **self._hints)
1236 
1237     ###################
1238     # PRIVATE METHODS #
1239     ###################
1240 
1241     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1242         """
1243         Insert a new record for the given model. This provides an interface to
1244         the InsertQuery class and is how Model.save() is implemented.
1245         """
1246         self._for_write = True
1247         if using is None:
1248             using = self.db
1249         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1250         query.insert_values(fields, objs, raw=raw)
1251         return query.get_compiler(using=using).execute_sql(returning_fields)
1252     _insert.alters_data = True
1253     _insert.queryset_only = False
1254 
1255     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1256         """
1257         Helper method for bulk_create() to insert objs one batch at a time.
1258         """
1259         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1260             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1261         ops = connections[self.db].ops
1262         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1263         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1264         inserted_rows = []
1265         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1266         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1267             if bulk_return and not ignore_conflicts:
1268                 inserted_rows.extend(self._insert(
1269                     item, fields=fields, using=self.db,
1270                     returning_fields=self.model._meta.db_returning_fields,
1271                     ignore_conflicts=ignore_conflicts,
1272                 ))
1273             else:
1274                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1275         return inserted_rows
1276 
1277     def _chain(self, **kwargs):
1278         """
1279         Return a copy of the current QuerySet that's ready for another
1280         operation.
1281         """
1282         obj = self._clone()
1283         if obj._sticky_filter:
1284             obj.query.filter_is_sticky = True
1285             obj._sticky_filter = False
1286         obj.__dict__.update(kwargs)
1287         return obj
1288 
1289     def _clone(self):
1290         """
1291         Return a copy of the current QuerySet. A lightweight alternative
1292         to deepcopy().
1293         """
1294         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1295         c._sticky_filter = self._sticky_filter
1296         c._for_write = self._for_write
1297         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1298         c._known_related_objects = self._known_related_objects
1299         c._iterable_class = self._iterable_class
1300         c._fields = self._fields
1301         return c
1302 
1303     def _fetch_all(self):
1304         if self._result_cache is None:
1305             self._result_cache = list(self._iterable_class(self))
1306         if self._prefetch_related_lookups and not self._prefetch_done:
1307             self._prefetch_related_objects()
1308 
1309     def _next_is_sticky(self):
1310         """
1311         Indicate that the next filter call and the one following that should
1312         be treated as a single filter. This is only important when it comes to
1313         determining when to reuse tables for many-to-many filters. Required so
1314         that we can filter naturally on the results of related managers.
1315 
1316         This doesn't return a clone of the current QuerySet (it returns
1317         "self"). The method is only used internally and should be immediately
1318         followed by a filter() that does create a clone.
1319         """
1320         self._sticky_filter = True
1321         return self
1322 
1323     def _merge_sanity_check(self, other):
1324         """Check that two QuerySet classes may be merged."""
1325         if self._fields is not None and (
1326                 set(self.query.values_select) != set(other.query.values_select) or
1327                 set(self.query.extra_select) != set(other.query.extra_select) or
1328                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1329             raise TypeError(
1330                 "Merging '%s' classes must involve the same values in each case."
1331                 % self.__class__.__name__
1332             )
1333 
1334     def _merge_known_related_objects(self, other):
1335         """
1336         Keep track of all known related objects from either QuerySet instance.
1337         """
1338         for field, objects in other._known_related_objects.items():
1339             self._known_related_objects.setdefault(field, {}).update(objects)
1340 
1341     def resolve_expression(self, *args, **kwargs):
1342         if self._fields and len(self._fields) > 1:
1343             # values() queryset can only be used as nested queries
1344             # if they are set up to select only a single field.
1345             raise TypeError('Cannot use multi-field values as a filter value.')
1346         query = self.query.resolve_expression(*args, **kwargs)
1347         query._db = self._db
1348         return query
1349     resolve_expression.queryset_only = True
1350 
1351     def _add_hints(self, **hints):
1352         """
1353         Update hinting information for use by routers. Add new key/values or
1354         overwrite existing key/values.
1355         """
1356         self._hints.update(hints)
1357 
1358     def _has_filters(self):
1359         """
1360         Check if this QuerySet has any filtering going on. This isn't
1361         equivalent with checking if all objects are present in results, for
1362         example, qs[1:]._has_filters() -> False.
1363         """
1364         return self.query.has_filters()
1365 
1366     @staticmethod
1367     def _validate_values_are_expressions(values, method_name):
1368         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1369         if invalid_args:
1370             raise TypeError(
1371                 'QuerySet.%s() received non-expression(s): %s.' % (
1372                     method_name,
1373                     ', '.join(invalid_args),
1374                 )
1375             )
1376 
1377     def _not_support_combined_queries(self, operation_name):
1378         if self.query.combinator:
1379             raise NotSupportedError(
1380                 'Calling QuerySet.%s() after %s() is not supported.'
1381                 % (operation_name, self.query.combinator)
1382             )

</code><intended_behavior>The method responsible for constructing IN queries as part of prefetch_related operations should be adjusted to exclude NULL values from the list of IDs used in the IN clause.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>build_filter</method>
<code>
1204     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1205                      can_reuse=None, allow_joins=True, split_subq=True,
1206                      reuse_with_filtered_relation=False, check_filterable=True):
1207         """
1208         Build a WhereNode for a single filter clause but don't add it
1209         to this Query. Query.add_q() will then add this filter to the where
1210         Node.
1211 
1212         The 'branch_negated' tells us if the current branch contains any
1213         negations. This will be used to determine if subqueries are needed.
1214 
1215         The 'current_negated' is used to determine if the current filter is
1216         negated or not and this will be used to determine if IS NULL filtering
1217         is needed.
1218 
1219         The difference between current_negated and branch_negated is that
1220         branch_negated is set on first negation, but current_negated is
1221         flipped for each negation.
1222 
1223         Note that add_filter will not do any negating itself, that is done
1224         upper in the code by add_q().
1225 
1226         The 'can_reuse' is a set of reusable joins for multijoins.
1227 
1228         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse
1229         will be reused.
1230 
1231         The method will create a filter clause that can be added to the current
1232         query. However, if the filter isn't added to the query then the caller
1233         is responsible for unreffing the joins used.
1234         """
1235         if isinstance(filter_expr, dict):
1236             raise FieldError("Cannot parse keyword query as dict")
1237         if isinstance(filter_expr, Q):
1238             return self._add_q(
1239                 filter_expr,
1240                 branch_negated=branch_negated,
1241                 current_negated=current_negated,
1242                 used_aliases=can_reuse,
1243                 allow_joins=allow_joins,
1244                 split_subq=split_subq,
1245                 check_filterable=check_filterable,
1246             )
1247         if hasattr(filter_expr, 'resolve_expression'):
1248             if not getattr(filter_expr, 'conditional', False):
1249                 raise TypeError('Cannot filter against a non-conditional expression.')
1250             condition = self.build_lookup(
1251                 ['exact'], filter_expr.resolve_expression(self, allow_joins=allow_joins), True
1252             )
1253             clause = self.where_class()
1254             clause.add(condition, AND)
1255             return clause, []
1256         arg, value = filter_expr
1257         if not arg:
1258             raise FieldError("Cannot parse keyword query %r" % arg)
1259         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1260 
1261         if check_filterable:
1262             self.check_filterable(reffed_expression)
1263 
1264         if not allow_joins and len(parts) > 1:
1265             raise FieldError("Joined field references are not permitted in this query")
1266 
1267         pre_joins = self.alias_refcount.copy()
1268         value = self.resolve_lookup_value(value, can_reuse, allow_joins)
1269         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1270 
1271         if check_filterable:
1272             self.check_filterable(value)
1273 
1274         clause = self.where_class()
1275         if reffed_expression:
1276             condition = self.build_lookup(lookups, reffed_expression, value)
1277             clause.add(condition, AND)
1278             return clause, []
1279 
1280         opts = self.get_meta()
1281         alias = self.get_initial_alias()
1282         allow_many = not branch_negated or not split_subq
1283 
1284         try:
1285             join_info = self.setup_joins(
1286                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1287                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1288             )
1289 
1290             # Prevent iterator from being consumed by check_related_objects()
1291             if isinstance(value, Iterator):
1292                 value = list(value)
1293             self.check_related_objects(join_info.final_field, value, join_info.opts)
1294 
1295             # split_exclude() needs to know which joins were generated for the
1296             # lookup parts
1297             self._lookup_joins = join_info.joins
1298         except MultiJoin as e:
1299             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1300 
1301         # Update used_joins before trimming since they are reused to determine
1302         # which joins could be later promoted to INNER.
1303         used_joins.update(join_info.joins)
1304         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1305         if can_reuse is not None:
1306             can_reuse.update(join_list)
1307 
1308         if join_info.final_field.is_relation:
1309             # No support for transforms for relational fields
1310             num_lookups = len(lookups)
1311             if num_lookups > 1:
1312                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1313             if len(targets) == 1:
1314                 col = self._get_col(targets[0], join_info.final_field, alias)
1315             else:
1316                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1317         else:
1318             col = self._get_col(targets[0], join_info.final_field, alias)
1319 
1320         condition = self.build_lookup(lookups, col, value)
1321         lookup_type = condition.lookup_name
1322         clause.add(condition, AND)
1323 
1324         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1325         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1326             require_outer = True
1327             if (lookup_type != 'isnull' and (
1328                     self.is_nullable(targets[0]) or
1329                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1330                 # The condition added here will be SQL like this:
1331                 # NOT (col IS NOT NULL), where the first NOT is added in
1332                 # upper layers of code. The reason for addition is that if col
1333                 # is null, then col != someval will result in SQL "unknown"
1334                 # which isn't the same as in Python. The Python None handling
1335                 # is wanted, and it can be gotten by
1336                 # (col IS NULL OR col != someval)
1337                 #   <=>
1338                 # NOT (col IS NOT NULL AND col = someval).
1339                 lookup_class = targets[0].get_lookup('isnull')
1340                 col = self._get_col(targets[0], join_info.targets[0], alias)
1341                 clause.add(lookup_class(col, False), AND)
1342         return clause, used_joins if not require_outer else ()

</code><intended_behavior>Review should ensure that modifications to handle the exclusion of NULL values in IN lookups do not inadvertently affect the general functionality of build_filter.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
136 class Query(BaseExpression):
137     """A single SQL query."""
138 
139     alias_prefix = 'T'
140     subq_aliases = frozenset([alias_prefix])
141 
142     compiler = 'SQLCompiler'
143 
144     def __init__(self, model, where=WhereNode, alias_cols=True):
145         self.model = model
146         self.alias_refcount = {}
147         # alias_map is the most important data structure regarding joins.
148         # It's used for recording which joins exist in the query and what
149         # types they are. The key is the alias of the joined table (possibly
150         # the table name) and the value is a Join-like object (see
151         # sql.datastructures.Join for more information).
152         self.alias_map = {}
153         # Whether to provide alias to columns during reference resolving.
154         self.alias_cols = alias_cols
155         # Sometimes the query contains references to aliases in outer queries (as
156         # a result of split_exclude). Correct alias quoting needs to know these
157         # aliases too.
158         # Map external tables to whether they are aliased.
159         self.external_aliases = {}
160         self.table_map = {}     # Maps table names to list of aliases.
161         self.default_cols = True
162         self.default_ordering = True
163         self.standard_ordering = True
164         self.used_aliases = set()
165         self.filter_is_sticky = False
166         self.subquery = False
167 
168         # SQL-related attributes
169         # Select and related select clauses are expressions to use in the
170         # SELECT clause of the query.
171         # The select is used for cases where we want to set up the select
172         # clause to contain other than default fields (values(), subqueries...)
173         # Note that annotations go to annotations dictionary.
174         self.select = ()
175         self.where = where()
176         self.where_class = where
177         # The group_by attribute can have one of the following forms:
178         #  - None: no group by at all in the query
179         #  - A tuple of expressions: group by (at least) those expressions.
180         #    String refs are also allowed for now.
181         #  - True: group by all select fields of the model
182         # See compiler.get_group_by() for details.
183         self.group_by = None
184         self.order_by = ()
185         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
186         self.distinct = False
187         self.distinct_fields = ()
188         self.select_for_update = False
189         self.select_for_update_nowait = False
190         self.select_for_update_skip_locked = False
191         self.select_for_update_of = ()
192         self.select_for_no_key_update = False
193 
194         self.select_related = False
195         # Arbitrary limit for select_related to prevents infinite recursion.
196         self.max_depth = 5
197 
198         # Holds the selects defined by a call to values() or values_list()
199         # excluding annotation_select and extra_select.
200         self.values_select = ()
201 
202         # SQL annotation-related attributes
203         self.annotations = {}  # Maps alias -> Annotation Expression
204         self.annotation_select_mask = None
205         self._annotation_select_cache = None
206 
207         # Set combination attributes
208         self.combinator = None
209         self.combinator_all = False
210         self.combined_queries = ()
211 
212         # These are for extensions. The contents are more or less appended
213         # verbatim to the appropriate clause.
214         self.extra = {}  # Maps col_alias -> (col_sql, params).
215         self.extra_select_mask = None
216         self._extra_select_cache = None
217 
218         self.extra_tables = ()
219         self.extra_order_by = ()
220 
221         # A tuple that is a set of model field names and either True, if these
222         # are the fields to defer, or False if these are the only fields to
223         # load.
224         self.deferred_loading = (frozenset(), True)
225 
226         self._filtered_relations = {}
227 
228         self.explain_query = False
229         self.explain_format = None
230         self.explain_options = {}
231 
232     @property
233     def output_field(self):
234         if len(self.select) == 1:
235             select = self.select[0]
236             return getattr(select, 'target', None) or select.field
237         elif len(self.annotation_select) == 1:
238             return next(iter(self.annotation_select.values())).output_field
239 
240     @property
241     def has_select_fields(self):
242         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)
243 
244     @cached_property
245     def base_table(self):
246         for alias in self.alias_map:
247             return alias
248 
249     def __str__(self):
250         """
251         Return the query as a string of SQL with the parameter values
252         substituted in (use sql_with_params() to see the unsubstituted string).
253 
254         Parameter values won't necessarily be quoted correctly, since that is
255         done by the database interface at execution time.
256         """
257         sql, params = self.sql_with_params()
258         return sql % params
259 
260     def sql_with_params(self):
261         """
262         Return the query as an SQL string and the parameters that will be
263         substituted into the query.
264         """
265         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
266 
267     def __deepcopy__(self, memo):
268         """Limit the amount of work when a Query is deepcopied."""
269         result = self.clone()
270         memo[id(self)] = result
271         return result
272 
273     def get_compiler(self, using=None, connection=None):
274         if using is None and connection is None:
275             raise ValueError("Need either using or connection")
276         if using:
277             connection = connections[using]
278         return connection.ops.compiler(self.compiler)(self, connection, using)
279 
280     def get_meta(self):
281         """
282         Return the Options instance (the model._meta) from which to start
283         processing. Normally, this is self.model._meta, but it can be changed
284         by subclasses.
285         """
286         return self.model._meta
287 
288     def clone(self):
289         """
290         Return a copy of the current Query. A lightweight alternative to
291         to deepcopy().
292         """
293         obj = Empty()
294         obj.__class__ = self.__class__
295         # Copy references to everything.
296         obj.__dict__ = self.__dict__.copy()
297         # Clone attributes that can't use shallow copy.
298         obj.alias_refcount = self.alias_refcount.copy()
299         obj.alias_map = self.alias_map.copy()
300         obj.external_aliases = self.external_aliases.copy()
301         obj.table_map = self.table_map.copy()
302         obj.where = self.where.clone()
303         obj.annotations = self.annotations.copy()
304         if self.annotation_select_mask is None:
305             obj.annotation_select_mask = None
306         else:
307             obj.annotation_select_mask = self.annotation_select_mask.copy()
308         # _annotation_select_cache cannot be copied, as doing so breaks the
309         # (necessary) state in which both annotations and
310         # _annotation_select_cache point to the same underlying objects.
311         # It will get re-populated in the cloned queryset the next time it's
312         # used.
313         obj._annotation_select_cache = None
314         obj.extra = self.extra.copy()
315         if self.extra_select_mask is None:
316             obj.extra_select_mask = None
317         else:
318             obj.extra_select_mask = self.extra_select_mask.copy()
319         if self._extra_select_cache is None:
320             obj._extra_select_cache = None
321         else:
322             obj._extra_select_cache = self._extra_select_cache.copy()
323         if self.select_related is not False:
324             # Use deepcopy because select_related stores fields in nested
325             # dicts.
326             obj.select_related = copy.deepcopy(obj.select_related)
327         if 'subq_aliases' in self.__dict__:
328             obj.subq_aliases = self.subq_aliases.copy()
329         obj.used_aliases = self.used_aliases.copy()
330         obj._filtered_relations = self._filtered_relations.copy()
331         # Clear the cached_property
332         try:
333             del obj.base_table
334         except AttributeError:
335             pass
336         return obj
337 
338     def chain(self, klass=None):
339         """
340         Return a copy of the current Query that's ready for another operation.
341         The klass argument changes the type of the Query, e.g. UpdateQuery.
342         """
343         obj = self.clone()
344         if klass and obj.__class__ != klass:
345             obj.__class__ = klass
346         if not obj.filter_is_sticky:
347             obj.used_aliases = set()
348         obj.filter_is_sticky = False
349         if hasattr(obj, '_setup_query'):
350             obj._setup_query()
351         return obj
352 
353     def relabeled_clone(self, change_map):
354         clone = self.clone()
355         clone.change_aliases(change_map)
356         return clone
357 
358     def _get_col(self, target, field, alias):
359         if not self.alias_cols:
360             alias = None
361         return target.get_col(alias, field)
362 
363     def rewrite_cols(self, annotation, col_cnt):
364         # We must make sure the inner query has the referred columns in it.
365         # If we are aggregating over an annotation, then Django uses Ref()
366         # instances to note this. However, if we are annotating over a column
367         # of a related model, then it might be that column isn't part of the
368         # SELECT clause of the inner query, and we must manually make sure
369         # the column is selected. An example case is:
370         #    .aggregate(Sum('author__awards'))
371         # Resolving this expression results in a join to author, but there
372         # is no guarantee the awards column of author is in the select clause
373         # of the query. Thus we must manually add the column to the inner
374         # query.
375         orig_exprs = annotation.get_source_expressions()
376         new_exprs = []
377         for expr in orig_exprs:
378             # FIXME: These conditions are fairly arbitrary. Identify a better
379             # method of having expressions decide which code path they should
380             # take.
381             if isinstance(expr, Ref):
382                 # Its already a Ref to subquery (see resolve_ref() for
383                 # details)
384                 new_exprs.append(expr)
385             elif isinstance(expr, (WhereNode, Lookup)):
386                 # Decompose the subexpressions further. The code here is
387                 # copied from the else clause, but this condition must appear
388                 # before the contains_aggregate/is_summary condition below.
389                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
390                 new_exprs.append(new_expr)
391             else:
392                 # Reuse aliases of expressions already selected in subquery.
393                 for col_alias, selected_annotation in self.annotation_select.items():
394                     if selected_annotation is expr:
395                         new_expr = Ref(col_alias, expr)
396                         break
397                 else:
398                     # An expression that is not selected the subquery.
399                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
400                         # Reference column or another aggregate. Select it
401                         # under a non-conflicting alias.
402                         col_cnt += 1
403                         col_alias = '__col%d' % col_cnt
404                         self.annotations[col_alias] = expr
405                         self.append_annotation_mask([col_alias])
406                         new_expr = Ref(col_alias, expr)
407                     else:
408                         # Some other expression not referencing database values
409                         # directly. Its subexpression might contain Cols.
410                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
411                 new_exprs.append(new_expr)
412         annotation.set_source_expressions(new_exprs)
413         return annotation, col_cnt
414 
415     def get_aggregation(self, using, added_aggregate_names):
416         """
417         Return the dictionary with the values of the existing aggregations.
418         """
419         if not self.annotation_select:
420             return {}
421         existing_annotations = [
422             annotation for alias, annotation
423             in self.annotations.items()
424             if alias not in added_aggregate_names
425         ]
426         # Decide if we need to use a subquery.
427         #
428         # Existing annotations would cause incorrect results as get_aggregation()
429         # must produce just one result and thus must not use GROUP BY. But we
430         # aren't smart enough to remove the existing annotations from the
431         # query, so those would force us to use GROUP BY.
432         #
433         # If the query has limit or distinct, or uses set operations, then
434         # those operations must be done in a subquery so that the query
435         # aggregates on the limit and/or distinct results instead of applying
436         # the distinct and limit after the aggregation.
437         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or
438                 self.distinct or self.combinator):
439             from django.db.models.sql.subqueries import AggregateQuery
440             outer_query = AggregateQuery(self.model)
441             inner_query = self.clone()
442             inner_query.select_for_update = False
443             inner_query.select_related = False
444             inner_query.set_annotation_mask(self.annotation_select)
445             if not self.is_sliced and not self.distinct_fields:
446                 # Queries with distinct_fields need ordering and when a limit
447                 # is applied we must take the slice from the ordered query.
448                 # Otherwise no need for ordering.
449                 inner_query.clear_ordering(True)
450             if not inner_query.distinct:
451                 # If the inner query uses default select and it has some
452                 # aggregate annotations, then we must make sure the inner
453                 # query is grouped by the main model's primary key. However,
454                 # clearing the select clause can alter results if distinct is
455                 # used.
456                 has_existing_aggregate_annotations = any(
457                     annotation for annotation in existing_annotations
458                     if getattr(annotation, 'contains_aggregate', True)
459                 )
460                 if inner_query.default_cols and has_existing_aggregate_annotations:
461                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
462                 inner_query.default_cols = False
463 
464             relabels = {t: 'subquery' for t in inner_query.alias_map}
465             relabels[None] = 'subquery'
466             # Remove any aggregates marked for reduction from the subquery
467             # and move them to the outer AggregateQuery.
468             col_cnt = 0
469             for alias, expression in list(inner_query.annotation_select.items()):
470                 annotation_select_mask = inner_query.annotation_select_mask
471                 if expression.is_summary:
472                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
473                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
474                     del inner_query.annotations[alias]
475                     annotation_select_mask.remove(alias)
476                 # Make sure the annotation_select wont use cached results.
477                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
478             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:
479                 # In case of Model.objects[0:3].count(), there would be no
480                 # field selected in the inner query, yet we must use a subquery.
481                 # So, make sure at least one field is selected.
482                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
483             try:
484                 outer_query.add_subquery(inner_query, using)
485             except EmptyResultSet:
486                 return {
487                     alias: None
488                     for alias in outer_query.annotation_select
489                 }
490         else:
491             outer_query = self
492             self.select = ()
493             self.default_cols = False
494             self.extra = {}
495 
496         outer_query.clear_ordering(True)
497         outer_query.clear_limits()
498         outer_query.select_for_update = False
499         outer_query.select_related = False
500         compiler = outer_query.get_compiler(using)
501         result = compiler.execute_sql(SINGLE)
502         if result is None:
503             result = [None] * len(outer_query.annotation_select)
504 
505         converters = compiler.get_converters(outer_query.annotation_select.values())
506         result = next(compiler.apply_converters((result,), converters))
507 
508         return dict(zip(outer_query.annotation_select, result))
509 
510     def get_count(self, using):
511         """
512         Perform a COUNT() query using the current filter constraints.
513         """
514         obj = self.clone()
515         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
516         number = obj.get_aggregation(using, ['__count'])['__count']
517         if number is None:
518             number = 0
519         return number
520 
521     def has_filters(self):
522         return self.where
523 
524     def has_results(self, using):
525         q = self.clone()
526         if not q.distinct:
527             if q.group_by is True:
528                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
529                 # Disable GROUP BY aliases to avoid orphaning references to the
530                 # SELECT clause which is about to be cleared.
531                 q.set_group_by(allow_aliases=False)
532             q.clear_select_clause()
533         q.clear_ordering(True)
534         q.set_limits(high=1)
535         compiler = q.get_compiler(using=using)
536         return compiler.has_results()
537 
538     def explain(self, using, format=None, **options):
539         q = self.clone()
540         q.explain_query = True
541         q.explain_format = format
542         q.explain_options = options
543         compiler = q.get_compiler(using=using)
544         return '\n'.join(compiler.explain_query())
545 
546     def combine(self, rhs, connector):
547         """
548         Merge the 'rhs' query into the current one (with any 'rhs' effects
549         being applied *after* (that is, "to the right of") anything in the
550         current query. 'rhs' is not modified during a call to this function.
551 
552         The 'connector' parameter describes how to connect filters from the
553         'rhs' query.
554         """
555         assert self.model == rhs.model, \
556             "Cannot combine queries on two different base models."
557         assert not self.is_sliced, \
558             "Cannot combine queries once a slice has been taken."
559         assert self.distinct == rhs.distinct, \
560             "Cannot combine a unique query with a non-unique query."
561         assert self.distinct_fields == rhs.distinct_fields, \
562             "Cannot combine queries with different distinct fields."
563 
564         # Work out how to relabel the rhs aliases, if necessary.
565         change_map = {}
566         conjunction = (connector == AND)
567 
568         # Determine which existing joins can be reused. When combining the
569         # query with AND we must recreate all joins for m2m filters. When
570         # combining with OR we can reuse joins. The reason is that in AND
571         # case a single row can't fulfill a condition like:
572         #     revrel__col=1 & revrel__col=2
573         # But, there might be two different related rows matching this
574         # condition. In OR case a single True is enough, so single row is
575         # enough, too.
576         #
577         # Note that we will be creating duplicate joins for non-m2m joins in
578         # the AND case. The results will be correct but this creates too many
579         # joins. This is something that could be fixed later on.
580         reuse = set() if conjunction else set(self.alias_map)
581         # Base table must be present in the query - this is the same
582         # table on both sides.
583         self.get_initial_alias()
584         joinpromoter = JoinPromoter(connector, 2, False)
585         joinpromoter.add_votes(
586             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
587         rhs_votes = set()
588         # Now, add the joins from rhs query into the new query (skipping base
589         # table).
590         rhs_tables = list(rhs.alias_map)[1:]
591         for alias in rhs_tables:
592             join = rhs.alias_map[alias]
593             # If the left side of the join was already relabeled, use the
594             # updated alias.
595             join = join.relabeled_clone(change_map)
596             new_alias = self.join(join, reuse=reuse)
597             if join.join_type == INNER:
598                 rhs_votes.add(new_alias)
599             # We can't reuse the same join again in the query. If we have two
600             # distinct joins for the same connection in rhs query, then the
601             # combined query must have two joins, too.
602             reuse.discard(new_alias)
603             if alias != new_alias:
604                 change_map[alias] = new_alias
605             if not rhs.alias_refcount[alias]:
606                 # The alias was unused in the rhs query. Unref it so that it
607                 # will be unused in the new query, too. We have to add and
608                 # unref the alias so that join promotion has information of
609                 # the join type for the unused alias.
610                 self.unref_alias(new_alias)
611         joinpromoter.add_votes(rhs_votes)
612         joinpromoter.update_join_types(self)
613 
614         # Now relabel a copy of the rhs where-clause and add it to the current
615         # one.
616         w = rhs.where.clone()
617         w.relabel_aliases(change_map)
618         self.where.add(w, connector)
619 
620         # Selection columns and extra extensions are those provided by 'rhs'.
621         if rhs.select:
622             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])
623         else:
624             self.select = ()
625 
626         if connector == OR:
627             # It would be nice to be able to handle this, but the queries don't
628             # really make sense (or return consistent value sets). Not worth
629             # the extra complexity when you can write a real query instead.
630             if self.extra and rhs.extra:
631                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
632         self.extra.update(rhs.extra)
633         extra_select_mask = set()
634         if self.extra_select_mask is not None:
635             extra_select_mask.update(self.extra_select_mask)
636         if rhs.extra_select_mask is not None:
637             extra_select_mask.update(rhs.extra_select_mask)
638         if extra_select_mask:
639             self.set_extra_mask(extra_select_mask)
640         self.extra_tables += rhs.extra_tables
641 
642         # Ordering uses the 'rhs' ordering, unless it has none, in which case
643         # the current ordering is used.
644         self.order_by = rhs.order_by or self.order_by
645         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
646 
647     def deferred_to_data(self, target, callback):
648         """
649         Convert the self.deferred_loading data structure to an alternate data
650         structure, describing the field that *will* be loaded. This is used to
651         compute the columns to select from the database and also by the
652         QuerySet class to work out which fields are being initialized on each
653         model. Models that have all their fields included aren't mentioned in
654         the result, only those that have field restrictions in place.
655 
656         The "target" parameter is the instance that is populated (in place).
657         The "callback" is a function that is called whenever a (model, field)
658         pair need to be added to "target". It accepts three parameters:
659         "target", and the model and list of fields being added for that model.
660         """
661         field_names, defer = self.deferred_loading
662         if not field_names:
663             return
664         orig_opts = self.get_meta()
665         seen = {}
666         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
667         for field_name in field_names:
668             parts = field_name.split(LOOKUP_SEP)
669             cur_model = self.model._meta.concrete_model
670             opts = orig_opts
671             for name in parts[:-1]:
672                 old_model = cur_model
673                 if name in self._filtered_relations:
674                     name = self._filtered_relations[name].relation_name
675                 source = opts.get_field(name)
676                 if is_reverse_o2o(source):
677                     cur_model = source.related_model
678                 else:
679                     cur_model = source.remote_field.model
680                 opts = cur_model._meta
681                 # Even if we're "just passing through" this model, we must add
682                 # both the current model's pk and the related reference field
683                 # (if it's not a reverse relation) to the things we select.
684                 if not is_reverse_o2o(source):
685                     must_include[old_model].add(source)
686                 add_to_dict(must_include, cur_model, opts.pk)
687             field = opts.get_field(parts[-1])
688             is_reverse_object = field.auto_created and not field.concrete
689             model = field.related_model if is_reverse_object else field.model
690             model = model._meta.concrete_model
691             if model == opts.model:
692                 model = cur_model
693             if not is_reverse_o2o(field):
694                 add_to_dict(seen, model, field)
695 
696         if defer:
697             # We need to load all fields for each model, except those that
698             # appear in "seen" (for all models that appear in "seen"). The only
699             # slight complexity here is handling fields that exist on parent
700             # models.
701             workset = {}
702             for model, values in seen.items():
703                 for field in model._meta.local_fields:
704                     if field not in values:
705                         m = field.model._meta.concrete_model
706                         add_to_dict(workset, m, field)
707             for model, values in must_include.items():
708                 # If we haven't included a model in workset, we don't add the
709                 # corresponding must_include fields for that model, since an
710                 # empty set means "include all fields". That's why there's no
711                 # "else" branch here.
712                 if model in workset:
713                     workset[model].update(values)
714             for model, values in workset.items():
715                 callback(target, model, values)
716         else:
717             for model, values in must_include.items():
718                 if model in seen:
719                     seen[model].update(values)
720                 else:
721                     # As we've passed through this model, but not explicitly
722                     # included any fields, we have to make sure it's mentioned
723                     # so that only the "must include" fields are pulled in.
724                     seen[model] = values
725             # Now ensure that every model in the inheritance chain is mentioned
726             # in the parent list. Again, it must be mentioned to ensure that
727             # only "must include" fields are pulled in.
728             for model in orig_opts.get_parent_list():
729                 seen.setdefault(model, set())
730             for model, values in seen.items():
731                 callback(target, model, values)
732 
733     def table_alias(self, table_name, create=False, filtered_relation=None):
734         """
735         Return a table alias for the given table_name and whether this is a
736         new alias or not.
737 
738         If 'create' is true, a new alias is always created. Otherwise, the
739         most recently created alias for the table (if one exists) is reused.
740         """
741         alias_list = self.table_map.get(table_name)
742         if not create and alias_list:
743             alias = alias_list[0]
744             self.alias_refcount[alias] += 1
745             return alias, False
746 
747         # Create a new alias for this table.
748         if alias_list:
749             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
750             alias_list.append(alias)
751         else:
752             # The first occurrence of a table uses the table name directly.
753             alias = filtered_relation.alias if filtered_relation is not None else table_name
754             self.table_map[table_name] = [alias]
755         self.alias_refcount[alias] = 1
756         return alias, True
757 
758     def ref_alias(self, alias):
759         """Increases the reference count for this alias."""
760         self.alias_refcount[alias] += 1
761 
762     def unref_alias(self, alias, amount=1):
763         """Decreases the reference count for this alias."""
764         self.alias_refcount[alias] -= amount
765 
766     def promote_joins(self, aliases):
767         """
768         Promote recursively the join type of given aliases and its children to
769         an outer join. If 'unconditional' is False, only promote the join if
770         it is nullable or the parent join is an outer join.
771 
772         The children promotion is done to avoid join chains that contain a LOUTER
773         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
774         then we must also promote b->c automatically, or otherwise the promotion
775         of a->b doesn't actually change anything in the query results.
776         """
777         aliases = list(aliases)
778         while aliases:
779             alias = aliases.pop(0)
780             if self.alias_map[alias].join_type is None:
781                 # This is the base table (first FROM entry) - this table
782                 # isn't really joined at all in the query, so we should not
783                 # alter its join type.
784                 continue
785             # Only the first alias (skipped above) should have None join_type
786             assert self.alias_map[alias].join_type is not None
787             parent_alias = self.alias_map[alias].parent_alias
788             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
789             already_louter = self.alias_map[alias].join_type == LOUTER
790             if ((self.alias_map[alias].nullable or parent_louter) and
791                     not already_louter):
792                 self.alias_map[alias] = self.alias_map[alias].promote()
793                 # Join type of 'alias' changed, so re-examine all aliases that
794                 # refer to this one.
795                 aliases.extend(
796                     join for join in self.alias_map
797                     if self.alias_map[join].parent_alias == alias and join not in aliases
798                 )
799 
800     def demote_joins(self, aliases):
801         """
802         Change join type from LOUTER to INNER for all joins in aliases.
803 
804         Similarly to promote_joins(), this method must ensure no join chains
805         containing first an outer, then an inner join are generated. If we
806         are demoting b->c join in chain a LOUTER b LOUTER c then we must
807         demote a->b automatically, or otherwise the demotion of b->c doesn't
808         actually change anything in the query results. .
809         """
810         aliases = list(aliases)
811         while aliases:
812             alias = aliases.pop(0)
813             if self.alias_map[alias].join_type == LOUTER:
814                 self.alias_map[alias] = self.alias_map[alias].demote()
815                 parent_alias = self.alias_map[alias].parent_alias
816                 if self.alias_map[parent_alias].join_type == INNER:
817                     aliases.append(parent_alias)
818 
819     def reset_refcounts(self, to_counts):
820         """
821         Reset reference counts for aliases so that they match the value passed
822         in `to_counts`.
823         """
824         for alias, cur_refcount in self.alias_refcount.copy().items():
825             unref_amount = cur_refcount - to_counts.get(alias, 0)
826             self.unref_alias(alias, unref_amount)
827 
828     def change_aliases(self, change_map):
829         """
830         Change the aliases in change_map (which maps old-alias -> new-alias),
831         relabelling any references to them in select columns and the where
832         clause.
833         """
834         assert set(change_map).isdisjoint(change_map.values())
835 
836         # 1. Update references in "select" (normal columns plus aliases),
837         # "group by" and "where".
838         self.where.relabel_aliases(change_map)
839         if isinstance(self.group_by, tuple):
840             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])
841         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])
842         self.annotations = self.annotations and {
843             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()
844         }
845 
846         # 2. Rename the alias in the internal table/alias datastructures.
847         for old_alias, new_alias in change_map.items():
848             if old_alias not in self.alias_map:
849                 continue
850             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
851             self.alias_map[new_alias] = alias_data
852             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
853             del self.alias_refcount[old_alias]
854             del self.alias_map[old_alias]
855 
856             table_aliases = self.table_map[alias_data.table_name]
857             for pos, alias in enumerate(table_aliases):
858                 if alias == old_alias:
859                     table_aliases[pos] = new_alias
860                     break
861         self.external_aliases = {
862             # Table is aliased or it's being changed and thus is aliased.
863             change_map.get(alias, alias): (aliased or alias in change_map)
864             for alias, aliased in self.external_aliases.items()
865         }
866 
867     def bump_prefix(self, outer_query):
868         """
869         Change the alias prefix to the next letter in the alphabet in a way
870         that the outer query's aliases and this query's aliases will not
871         conflict. Even tables that previously had no alias will get an alias
872         after this call.
873         """
874         def prefix_gen():
875             """
876             Generate a sequence of characters in alphabetical order:
877                 -> 'A', 'B', 'C', ...
878 
879             When the alphabet is finished, the sequence will continue with the
880             Cartesian product:
881                 -> 'AA', 'AB', 'AC', ...
882             """
883             alphabet = ascii_uppercase
884             prefix = chr(ord(self.alias_prefix) + 1)
885             yield prefix
886             for n in count(1):
887                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
888                 for s in product(seq, repeat=n):
889                     yield ''.join(s)
890                 prefix = None
891 
892         if self.alias_prefix != outer_query.alias_prefix:
893             # No clashes between self and outer query should be possible.
894             return
895 
896         # Explicitly avoid infinite loop. The constant divider is based on how
897         # much depth recursive subquery references add to the stack. This value
898         # might need to be adjusted when adding or removing function calls from
899         # the code path in charge of performing these operations.
900         local_recursion_limit = sys.getrecursionlimit() // 16
901         for pos, prefix in enumerate(prefix_gen()):
902             if prefix not in self.subq_aliases:
903                 self.alias_prefix = prefix
904                 break
905             if pos > local_recursion_limit:
906                 raise RecursionError(
907                     'Maximum recursion depth exceeded: too many subqueries.'
908                 )
909         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
910         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
911         self.change_aliases({
912             alias: '%s%d' % (self.alias_prefix, pos)
913             for pos, alias in enumerate(self.alias_map)
914         })
915 
916     def get_initial_alias(self):
917         """
918         Return the first alias for this query, after increasing its reference
919         count.
920         """
921         if self.alias_map:
922             alias = self.base_table
923             self.ref_alias(alias)
924         else:
925             alias = self.join(BaseTable(self.get_meta().db_table, None))
926         return alias
927 
928     def count_active_tables(self):
929         """
930         Return the number of tables in this query with a non-zero reference
931         count. After execution, the reference counts are zeroed, so tables
932         added in compiler will not be seen by this method.
933         """
934         return len([1 for count in self.alias_refcount.values() if count])
935 
936     def join(self, join, reuse=None, reuse_with_filtered_relation=False):
937         """
938         Return an alias for the 'join', either reusing an existing alias for
939         that join or creating a new one. 'join' is either a
940         sql.datastructures.BaseTable or Join.
941 
942         The 'reuse' parameter can be either None which means all joins are
943         reusable, or it can be a set containing the aliases that can be reused.
944 
945         The 'reuse_with_filtered_relation' parameter is used when computing
946         FilteredRelation instances.
947 
948         A join is always created as LOUTER if the lhs alias is LOUTER to make
949         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
950         joins are created as LOUTER if the join is nullable.
951         """
952         if reuse_with_filtered_relation and reuse:
953             reuse_aliases = [
954                 a for a, j in self.alias_map.items()
955                 if a in reuse and j.equals(join, with_filtered_relation=False)
956             ]
957         else:
958             reuse_aliases = [
959                 a for a, j in self.alias_map.items()
960                 if (reuse is None or a in reuse) and j == join
961             ]
962         if reuse_aliases:
963             if join.table_alias in reuse_aliases:
964                 reuse_alias = join.table_alias
965             else:
966                 # Reuse the most recent alias of the joined table
967                 # (a many-to-many relation may be joined multiple times).
968                 reuse_alias = reuse_aliases[-1]
969             self.ref_alias(reuse_alias)
970             return reuse_alias
971 
972         # No reuse is possible, so we need a new alias.
973         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)
974         if join.join_type:
975             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
976                 join_type = LOUTER
977             else:
978                 join_type = INNER
979             join.join_type = join_type
980         join.table_alias = alias
981         self.alias_map[alias] = join
982         return alias
983 
984     def join_parent_model(self, opts, model, alias, seen):
985         """
986         Make sure the given 'model' is joined in the query. If 'model' isn't
987         a parent of 'opts' or if it is None this method is a no-op.
988 
989         The 'alias' is the root alias for starting the join, 'seen' is a dict
990         of model -> alias of existing joins. It must also contain a mapping
991         of None -> some alias. This will be returned in the no-op case.
992         """
993         if model in seen:
994             return seen[model]
995         chain = opts.get_base_chain(model)
996         if not chain:
997             return alias
998         curr_opts = opts
999         for int_model in chain:
1000             if int_model in seen:
1001                 curr_opts = int_model._meta
1002                 alias = seen[int_model]
1003                 continue
1004             # Proxy model have elements in base chain
1005             # with no parents, assign the new options
1006             # object and skip to the next base in that
1007             # case
1008             if not curr_opts.parents[int_model]:
1009                 curr_opts = int_model._meta
1010                 continue
1011             link_field = curr_opts.get_ancestor_link(int_model)
1012             join_info = self.setup_joins([link_field.name], curr_opts, alias)
1013             curr_opts = int_model._meta
1014             alias = seen[int_model] = join_info.joins[-1]
1015         return alias or seen[None]
1016 
1017     def add_annotation(self, annotation, alias, is_summary=False):
1018         """Add a single annotation expression to the Query."""
1019         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
1020                                                    summarize=is_summary)
1021         self.append_annotation_mask([alias])
1022         self.annotations[alias] = annotation
1023 
1024     def resolve_expression(self, query, *args, **kwargs):
1025         clone = self.clone()
1026         # Subqueries need to use a different set of aliases than the outer query.
1027         clone.bump_prefix(query)
1028         clone.subquery = True
1029         # It's safe to drop ordering if the queryset isn't using slicing,
1030         # distinct(*fields) or select_for_update().
1031         if (self.low_mark == 0 and self.high_mark is None and
1032                 not self.distinct_fields and
1033                 not self.select_for_update):
1034             clone.clear_ordering(True)
1035         clone.where.resolve_expression(query, *args, **kwargs)
1036         for key, value in clone.annotations.items():
1037             resolved = value.resolve_expression(query, *args, **kwargs)
1038             if hasattr(resolved, 'external_aliases'):
1039                 resolved.external_aliases.update(clone.external_aliases)
1040             clone.annotations[key] = resolved
1041         # Outer query's aliases are considered external.
1042         for alias, table in query.alias_map.items():
1043             clone.external_aliases[alias] = (
1044                 (isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias) or
1045                 (isinstance(table, BaseTable) and table.table_name != table.table_alias)
1046             )
1047         return clone
1048 
1049     def get_external_cols(self):
1050         exprs = chain(self.annotations.values(), self.where.children)
1051         return [
1052             col for col in self._gen_cols(exprs)
1053             if col.alias in self.external_aliases
1054         ]
1055 
1056     def as_sql(self, compiler, connection):
1057         sql, params = self.get_compiler(connection=connection).as_sql()
1058         if self.subquery:
1059             sql = '(%s)' % sql
1060         return sql, params
1061 
1062     def resolve_lookup_value(self, value, can_reuse, allow_joins):
1063         if hasattr(value, 'resolve_expression'):
1064             value = value.resolve_expression(
1065                 self, reuse=can_reuse, allow_joins=allow_joins,
1066             )
1067         elif isinstance(value, (list, tuple)):
1068             # The items of the iterable may be expressions and therefore need
1069             # to be resolved independently.
1070             return type(value)(
1071                 self.resolve_lookup_value(sub_value, can_reuse, allow_joins)
1072                 for sub_value in value
1073             )
1074         return value
1075 
1076     def solve_lookup_type(self, lookup):
1077         """
1078         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').
1079         """
1080         lookup_splitted = lookup.split(LOOKUP_SEP)
1081         if self.annotations:
1082             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1083             if expression:
1084                 return expression_lookups, (), expression
1085         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1086         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1087         if len(lookup_parts) > 1 and not field_parts:
1088             raise FieldError(
1089                 'Invalid lookup "%s" for model %s".' %
1090                 (lookup, self.get_meta().model.__name__)
1091             )
1092         return lookup_parts, field_parts, False
1093 
1094     def check_query_object_type(self, value, opts, field):
1095         """
1096         Check whether the object passed while querying is of the correct type.
1097         If not, raise a ValueError specifying the wrong object.
1098         """
1099         if hasattr(value, '_meta'):
1100             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1101                 raise ValueError(
1102                     'Cannot query "%s": Must be "%s" instance.' %
1103                     (value, opts.object_name))
1104 
1105     def check_related_objects(self, field, value, opts):
1106         """Check the type of object passed to query relations."""
1107         if field.is_relation:
1108             # Check that the field and the queryset use the same model in a
1109             # query like .filter(author=Author.objects.all()). For example, the
1110             # opts would be Author's (from the author field) and value.model
1111             # would be Author.objects.all() queryset's .model (Author also).
1112             # The field is the related field on the lhs side.
1113             if (isinstance(value, Query) and not value.has_select_fields and
1114                     not check_rel_lookup_compatibility(value.model, opts, field)):
1115                 raise ValueError(
1116                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1117                     (value.model._meta.object_name, opts.object_name)
1118                 )
1119             elif hasattr(value, '_meta'):
1120                 self.check_query_object_type(value, opts, field)
1121             elif hasattr(value, '__iter__'):
1122                 for v in value:
1123                     self.check_query_object_type(v, opts, field)
1124 
1125     def check_filterable(self, expression):
1126         """Raise an error if expression cannot be used in a WHERE clause."""
1127         if (
1128             hasattr(expression, 'resolve_expression') and
1129             not getattr(expression, 'filterable', True)
1130         ):
1131             raise NotSupportedError(
1132                 expression.__class__.__name__ + ' is disallowed in the filter '
1133                 'clause.'
1134             )
1135         if hasattr(expression, 'get_source_expressions'):
1136             for expr in expression.get_source_expressions():
1137                 self.check_filterable(expr)
1138 
1139     def build_lookup(self, lookups, lhs, rhs):
1140         """
1141         Try to extract transforms and lookup from given lhs.
1142 
1143         The lhs value is something that works like SQLExpression.
1144         The rhs value is what the lookup is going to compare against.
1145         The lookups is a list of names to extract using get_lookup()
1146         and get_transform().
1147         """
1148         # __exact is the default lookup if one isn't given.
1149         *transforms, lookup_name = lookups or ['exact']
1150         for name in transforms:
1151             lhs = self.try_transform(lhs, name)
1152         # First try get_lookup() so that the lookup takes precedence if the lhs
1153         # supports both transform and lookup for the name.
1154         lookup_class = lhs.get_lookup(lookup_name)
1155         if not lookup_class:
1156             if lhs.field.is_relation:
1157                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))
1158             # A lookup wasn't found. Try to interpret the name as a transform
1159             # and do an Exact lookup against it.
1160             lhs = self.try_transform(lhs, lookup_name)
1161             lookup_name = 'exact'
1162             lookup_class = lhs.get_lookup(lookup_name)
1163             if not lookup_class:
1164                 return
1165 
1166         lookup = lookup_class(lhs, rhs)
1167         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
1168         # uses of None as a query value unless the lookup supports it.
1169         if lookup.rhs is None and not lookup.can_use_none_as_rhs:
1170             if lookup_name not in ('exact', 'iexact'):
1171                 raise ValueError("Cannot use None as a query value")
1172             return lhs.get_lookup('isnull')(lhs, True)
1173 
1174         # For Oracle '' is equivalent to null. The check must be done at this
1175         # stage because join promotion can't be done in the compiler. Using
1176         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.
1177         # A similar thing is done in is_nullable(), too.
1178         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1179                 lookup_name == 'exact' and lookup.rhs == ''):
1180             return lhs.get_lookup('isnull')(lhs, True)
1181 
1182         return lookup
1183 
1184     def try_transform(self, lhs, name):
1185         """
1186         Helper method for build_lookup(). Try to fetch and initialize
1187         a transform for name parameter from lhs.
1188         """
1189         transform_class = lhs.get_transform(name)
1190         if transform_class:
1191             return transform_class(lhs)
1192         else:
1193             output_field = lhs.output_field.__class__
1194             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())
1195             if suggested_lookups:
1196                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)
1197             else:
1198                 suggestion = '.'
1199             raise FieldError(
1200                 "Unsupported lookup '%s' for %s or join on the field not "
1201                 "permitted%s" % (name, output_field.__name__, suggestion)
1202             )
1203 
1204     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1205                      can_reuse=None, allow_joins=True, split_subq=True,
1206                      reuse_with_filtered_relation=False, check_filterable=True):
1207         """
1208         Build a WhereNode for a single filter clause but don't add it
1209         to this Query. Query.add_q() will then add this filter to the where
1210         Node.
1211 
1212         The 'branch_negated' tells us if the current branch contains any
1213         negations. This will be used to determine if subqueries are needed.
1214 
1215         The 'current_negated' is used to determine if the current filter is
1216         negated or not and this will be used to determine if IS NULL filtering
1217         is needed.
1218 
1219         The difference between current_negated and branch_negated is that
1220         branch_negated is set on first negation, but current_negated is
1221         flipped for each negation.
1222 
1223         Note that add_filter will not do any negating itself, that is done
1224         upper in the code by add_q().
1225 
1226         The 'can_reuse' is a set of reusable joins for multijoins.
1227 
1228         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse
1229         will be reused.
1230 
1231         The method will create a filter clause that can be added to the current
1232         query. However, if the filter isn't added to the query then the caller
1233         is responsible for unreffing the joins used.
1234         """
1235         if isinstance(filter_expr, dict):
1236             raise FieldError("Cannot parse keyword query as dict")
1237         if isinstance(filter_expr, Q):
1238             return self._add_q(
1239                 filter_expr,
1240                 branch_negated=branch_negated,
1241                 current_negated=current_negated,
1242                 used_aliases=can_reuse,
1243                 allow_joins=allow_joins,
1244                 split_subq=split_subq,
1245                 check_filterable=check_filterable,
1246             )
1247         if hasattr(filter_expr, 'resolve_expression'):
1248             if not getattr(filter_expr, 'conditional', False):
1249                 raise TypeError('Cannot filter against a non-conditional expression.')
1250             condition = self.build_lookup(
1251                 ['exact'], filter_expr.resolve_expression(self, allow_joins=allow_joins), True
1252             )
1253             clause = self.where_class()
1254             clause.add(condition, AND)
1255             return clause, []
1256         arg, value = filter_expr
1257         if not arg:
1258             raise FieldError("Cannot parse keyword query %r" % arg)
1259         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1260 
1261         if check_filterable:
1262             self.check_filterable(reffed_expression)
1263 
1264         if not allow_joins and len(parts) > 1:
1265             raise FieldError("Joined field references are not permitted in this query")
1266 
1267         pre_joins = self.alias_refcount.copy()
1268         value = self.resolve_lookup_value(value, can_reuse, allow_joins)
1269         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1270 
1271         if check_filterable:
1272             self.check_filterable(value)
1273 
1274         clause = self.where_class()
1275         if reffed_expression:
1276             condition = self.build_lookup(lookups, reffed_expression, value)
1277             clause.add(condition, AND)
1278             return clause, []
1279 
1280         opts = self.get_meta()
1281         alias = self.get_initial_alias()
1282         allow_many = not branch_negated or not split_subq
1283 
1284         try:
1285             join_info = self.setup_joins(
1286                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1287                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1288             )
1289 
1290             # Prevent iterator from being consumed by check_related_objects()
1291             if isinstance(value, Iterator):
1292                 value = list(value)
1293             self.check_related_objects(join_info.final_field, value, join_info.opts)
1294 
1295             # split_exclude() needs to know which joins were generated for the
1296             # lookup parts
1297             self._lookup_joins = join_info.joins
1298         except MultiJoin as e:
1299             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1300 
1301         # Update used_joins before trimming since they are reused to determine
1302         # which joins could be later promoted to INNER.
1303         used_joins.update(join_info.joins)
1304         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1305         if can_reuse is not None:
1306             can_reuse.update(join_list)
1307 
1308         if join_info.final_field.is_relation:
1309             # No support for transforms for relational fields
1310             num_lookups = len(lookups)
1311             if num_lookups > 1:
1312                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1313             if len(targets) == 1:
1314                 col = self._get_col(targets[0], join_info.final_field, alias)
1315             else:
1316                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1317         else:
1318             col = self._get_col(targets[0], join_info.final_field, alias)
1319 
1320         condition = self.build_lookup(lookups, col, value)
1321         lookup_type = condition.lookup_name
1322         clause.add(condition, AND)
1323 
1324         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1325         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1326             require_outer = True
1327             if (lookup_type != 'isnull' and (
1328                     self.is_nullable(targets[0]) or
1329                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1330                 # The condition added here will be SQL like this:
1331                 # NOT (col IS NOT NULL), where the first NOT is added in
1332                 # upper layers of code. The reason for addition is that if col
1333                 # is null, then col != someval will result in SQL "unknown"
1334                 # which isn't the same as in Python. The Python None handling
1335                 # is wanted, and it can be gotten by
1336                 # (col IS NULL OR col != someval)
1337                 #   <=>
1338                 # NOT (col IS NOT NULL AND col = someval).
1339                 lookup_class = targets[0].get_lookup('isnull')
1340                 col = self._get_col(targets[0], join_info.targets[0], alias)
1341                 clause.add(lookup_class(col, False), AND)
1342         return clause, used_joins if not require_outer else ()
1343 
1344     def add_filter(self, filter_clause):
1345         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1346 
1347     def add_q(self, q_object):
1348         """
1349         A preprocessor for the internal _add_q(). Responsible for doing final
1350         join promotion.
1351         """
1352         # For join promotion this case is doing an AND for the added q_object
1353         # and existing conditions. So, any existing inner join forces the join
1354         # type to remain inner. Existing outer joins can however be demoted.
1355         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1356         # rel_a doesn't produce any rows, then the whole condition must fail.
1357         # So, demotion is OK.
1358         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}
1359         clause, _ = self._add_q(q_object, self.used_aliases)
1360         if clause:
1361             self.where.add(clause, AND)
1362         self.demote_joins(existing_inner)
1363 
1364     def build_where(self, filter_expr):
1365         return self.build_filter(filter_expr, allow_joins=False)[0]
1366 
1367     def _add_q(self, q_object, used_aliases, branch_negated=False,
1368                current_negated=False, allow_joins=True, split_subq=True,
1369                check_filterable=True):
1370         """Add a Q-object to the current filter."""
1371         connector = q_object.connector
1372         current_negated = current_negated ^ q_object.negated
1373         branch_negated = branch_negated or q_object.negated
1374         target_clause = self.where_class(connector=connector,
1375                                          negated=q_object.negated)
1376         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1377         for child in q_object.children:
1378             child_clause, needed_inner = self.build_filter(
1379                 child, can_reuse=used_aliases, branch_negated=branch_negated,
1380                 current_negated=current_negated, allow_joins=allow_joins,
1381                 split_subq=split_subq, check_filterable=check_filterable,
1382             )
1383             joinpromoter.add_votes(needed_inner)
1384             if child_clause:
1385                 target_clause.add(child_clause, connector)
1386         needed_inner = joinpromoter.update_join_types(self)
1387         return target_clause, needed_inner
1388 
1389     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):
1390         """Add a FilteredRelation object to the current filter."""
1391         connector = q_object.connector
1392         current_negated ^= q_object.negated
1393         branch_negated = branch_negated or q_object.negated
1394         target_clause = self.where_class(connector=connector, negated=q_object.negated)
1395         for child in q_object.children:
1396             if isinstance(child, Node):
1397                 child_clause = self.build_filtered_relation_q(
1398                     child, reuse=reuse, branch_negated=branch_negated,
1399                     current_negated=current_negated,
1400                 )
1401             else:
1402                 child_clause, _ = self.build_filter(
1403                     child, can_reuse=reuse, branch_negated=branch_negated,
1404                     current_negated=current_negated,
1405                     allow_joins=True, split_subq=False,
1406                     reuse_with_filtered_relation=True,
1407                 )
1408             target_clause.add(child_clause, connector)
1409         return target_clause
1410 
1411     def add_filtered_relation(self, filtered_relation, alias):
1412         filtered_relation.alias = alias
1413         lookups = dict(get_children_from_q(filtered_relation.condition))
1414         for lookup in chain((filtered_relation.relation_name,), lookups):
1415             lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)
1416             shift = 2 if not lookup_parts else 1
1417             if len(field_parts) > (shift + len(lookup_parts)):
1418                 raise ValueError(
1419                     "FilteredRelation's condition doesn't support nested "
1420                     "relations (got %r)." % lookup
1421                 )
1422         self._filtered_relations[filtered_relation.alias] = filtered_relation
1423 
1424     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1425         """
1426         Walk the list of names and turns them into PathInfo tuples. A single
1427         name in 'names' can generate multiple PathInfos (m2m, for example).
1428 
1429         'names' is the path of names to travel, 'opts' is the model Options we
1430         start the name resolving from, 'allow_many' is as for setup_joins().
1431         If fail_on_missing is set to True, then a name that can't be resolved
1432         will generate a FieldError.
1433 
1434         Return a list of PathInfo tuples. In addition return the final field
1435         (the last used join field) and target (which is a field guaranteed to
1436         contain the same value as the final field). Finally, return those names
1437         that weren't found (which are likely transforms and the final lookup).
1438         """
1439         path, names_with_path = [], []
1440         for pos, name in enumerate(names):
1441             cur_names_with_path = (name, [])
1442             if name == 'pk':
1443                 name = opts.pk.name
1444 
1445             field = None
1446             filtered_relation = None
1447             try:
1448                 field = opts.get_field(name)
1449             except FieldDoesNotExist:
1450                 if name in self.annotation_select:
1451                     field = self.annotation_select[name].output_field
1452                 elif name in self._filtered_relations and pos == 0:
1453                     filtered_relation = self._filtered_relations[name]
1454                     field = opts.get_field(filtered_relation.relation_name)
1455             if field is not None:
1456                 # Fields that contain one-to-many relations with a generic
1457                 # model (like a GenericForeignKey) cannot generate reverse
1458                 # relations and therefore cannot be used for reverse querying.
1459                 if field.is_relation and not field.related_model:
1460                     raise FieldError(
1461                         "Field %r does not generate an automatic reverse "
1462                         "relation and therefore cannot be used for reverse "
1463                         "querying. If it is a GenericForeignKey, consider "
1464                         "adding a GenericRelation." % name
1465                     )
1466                 try:
1467                     model = field.model._meta.concrete_model
1468                 except AttributeError:
1469                     # QuerySet.annotate() may introduce fields that aren't
1470                     # attached to a model.
1471                     model = None
1472             else:
1473                 # We didn't find the current field, so move position back
1474                 # one step.
1475                 pos -= 1
1476                 if pos == -1 or fail_on_missing:
1477                     available = sorted([
1478                         *get_field_names_from_opts(opts),
1479                         *self.annotation_select,
1480                         *self._filtered_relations,
1481                     ])
1482                     raise FieldError("Cannot resolve keyword '%s' into field. "
1483                                      "Choices are: %s" % (name, ", ".join(available)))
1484                 break
1485             # Check if we need any joins for concrete inheritance cases (the
1486             # field lives in parent, but we are currently in one of its
1487             # children)
1488             if model is not opts.model:
1489                 path_to_parent = opts.get_path_to_parent(model)
1490                 if path_to_parent:
1491                     path.extend(path_to_parent)
1492                     cur_names_with_path[1].extend(path_to_parent)
1493                     opts = path_to_parent[-1].to_opts
1494             if hasattr(field, 'get_path_info'):
1495                 pathinfos = field.get_path_info(filtered_relation)
1496                 if not allow_many:
1497                     for inner_pos, p in enumerate(pathinfos):
1498                         if p.m2m:
1499                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1500                             names_with_path.append(cur_names_with_path)
1501                             raise MultiJoin(pos + 1, names_with_path)
1502                 last = pathinfos[-1]
1503                 path.extend(pathinfos)
1504                 final_field = last.join_field
1505                 opts = last.to_opts
1506                 targets = last.target_fields
1507                 cur_names_with_path[1].extend(pathinfos)
1508                 names_with_path.append(cur_names_with_path)
1509             else:
1510                 # Local non-relational field.
1511                 final_field = field
1512                 targets = (field,)
1513                 if fail_on_missing and pos + 1 != len(names):
1514                     raise FieldError(
1515                         "Cannot resolve keyword %r into field. Join on '%s'"
1516                         " not permitted." % (names[pos + 1], name))
1517                 break
1518         return path, final_field, targets, names[pos + 1:]
1519 
1520     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,
1521                     reuse_with_filtered_relation=False):
1522         """
1523         Compute the necessary table joins for the passage through the fields
1524         given in 'names'. 'opts' is the Options class for the current model
1525         (which gives the table we are starting from), 'alias' is the alias for
1526         the table to start the joining from.
1527 
1528         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1529         can be None in which case all joins are reusable or a set of aliases
1530         that can be reused. Note that non-reverse foreign keys are always
1531         reusable when using setup_joins().
1532 
1533         The 'reuse_with_filtered_relation' can be used to force 'can_reuse'
1534         parameter and force the relation on the given connections.
1535 
1536         If 'allow_many' is False, then any reverse foreign key seen will
1537         generate a MultiJoin exception.
1538 
1539         Return the final field involved in the joins, the target field (used
1540         for any 'where' constraint), the final 'opts' value, the joins, the
1541         field path traveled to generate the joins, and a transform function
1542         that takes a field and alias and is equivalent to `field.get_col(alias)`
1543         in the simple case but wraps field transforms if they were included in
1544         names.
1545 
1546         The target field is the field containing the concrete value. Final
1547         field can be something different, for example foreign key pointing to
1548         that value. Final field is needed for example in some value
1549         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1550         key field for example).
1551         """
1552         joins = [alias]
1553         # The transform can't be applied yet, as joins must be trimmed later.
1554         # To avoid making every caller of this method look up transforms
1555         # directly, compute transforms here and create a partial that converts
1556         # fields to the appropriate wrapped version.
1557 
1558         def final_transformer(field, alias):
1559             return field.get_col(alias)
1560 
1561         # Try resolving all the names as fields first. If there's an error,
1562         # treat trailing names as lookups until a field can be resolved.
1563         last_field_exception = None
1564         for pivot in range(len(names), 0, -1):
1565             try:
1566                 path, final_field, targets, rest = self.names_to_path(
1567                     names[:pivot], opts, allow_many, fail_on_missing=True,
1568                 )
1569             except FieldError as exc:
1570                 if pivot == 1:
1571                     # The first item cannot be a lookup, so it's safe
1572                     # to raise the field error here.
1573                     raise
1574                 else:
1575                     last_field_exception = exc
1576             else:
1577                 # The transforms are the remaining items that couldn't be
1578                 # resolved into fields.
1579                 transforms = names[pivot:]
1580                 break
1581         for name in transforms:
1582             def transform(field, alias, *, name, previous):
1583                 try:
1584                     wrapped = previous(field, alias)
1585                     return self.try_transform(wrapped, name)
1586                 except FieldError:
1587                     # FieldError is raised if the transform doesn't exist.
1588                     if isinstance(final_field, Field) and last_field_exception:
1589                         raise last_field_exception
1590                     else:
1591                         raise
1592             final_transformer = functools.partial(transform, name=name, previous=final_transformer)
1593         # Then, add the path to the query's joins. Note that we can't trim
1594         # joins at this stage - we will need the information about join type
1595         # of the trimmed joins.
1596         for join in path:
1597             if join.filtered_relation:
1598                 filtered_relation = join.filtered_relation.clone()
1599                 table_alias = filtered_relation.alias
1600             else:
1601                 filtered_relation = None
1602                 table_alias = None
1603             opts = join.to_opts
1604             if join.direct:
1605                 nullable = self.is_nullable(join.join_field)
1606             else:
1607                 nullable = True
1608             connection = Join(
1609                 opts.db_table, alias, table_alias, INNER, join.join_field,
1610                 nullable, filtered_relation=filtered_relation,
1611             )
1612             reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None
1613             alias = self.join(
1614                 connection, reuse=reuse,
1615                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1616             )
1617             joins.append(alias)
1618             if filtered_relation:
1619                 filtered_relation.path = joins[:]
1620         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)
1621 
1622     def trim_joins(self, targets, joins, path):
1623         """
1624         The 'target' parameter is the final field being joined to, 'joins'
1625         is the full list of join aliases. The 'path' contain the PathInfos
1626         used to create the joins.
1627 
1628         Return the final target field and table alias and the new active
1629         joins.
1630 
1631         Always trim any direct join if the target column is already in the
1632         previous table. Can't trim reverse joins as it's unknown if there's
1633         anything on the other side of the join.
1634         """
1635         joins = joins[:]
1636         for pos, info in enumerate(reversed(path)):
1637             if len(joins) == 1 or not info.direct:
1638                 break
1639             if info.filtered_relation:
1640                 break
1641             join_targets = {t.column for t in info.join_field.foreign_related_fields}
1642             cur_targets = {t.column for t in targets}
1643             if not cur_targets.issubset(join_targets):
1644                 break
1645             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1646             targets = tuple(targets_dict[t.column] for t in targets)
1647             self.unref_alias(joins.pop())
1648         return targets, joins[-1], joins
1649 
1650     @classmethod
1651     def _gen_cols(cls, exprs):
1652         for expr in exprs:
1653             if isinstance(expr, Col):
1654                 yield expr
1655             else:
1656                 yield from cls._gen_cols(expr.get_source_expressions())
1657 
1658     @classmethod
1659     def _gen_col_aliases(cls, exprs):
1660         yield from (expr.alias for expr in cls._gen_cols(exprs))
1661 
1662     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1663         if not allow_joins and LOOKUP_SEP in name:
1664             raise FieldError("Joined field references are not permitted in this query")
1665         annotation = self.annotations.get(name)
1666         if annotation is not None:
1667             if not allow_joins:
1668                 for alias in self._gen_col_aliases([annotation]):
1669                     if isinstance(self.alias_map[alias], Join):
1670                         raise FieldError(
1671                             'Joined field references are not permitted in '
1672                             'this query'
1673                         )
1674             if summarize:
1675                 # Summarize currently means we are doing an aggregate() query
1676                 # which is executed as a wrapped subquery if any of the
1677                 # aggregate() elements reference an existing annotation. In
1678                 # that case we need to return a Ref to the subquery's annotation.
1679                 return Ref(name, self.annotation_select[name])
1680             else:
1681                 return annotation
1682         else:
1683             field_list = name.split(LOOKUP_SEP)
1684             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)
1685             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1686             if not allow_joins and len(join_list) > 1:
1687                 raise FieldError('Joined field references are not permitted in this query')
1688             if len(targets) > 1:
1689                 raise FieldError("Referencing multicolumn fields with F() objects "
1690                                  "isn't supported")
1691             # Verify that the last lookup in name is a field or a transform:
1692             # transform_function() raises FieldError if not.
1693             join_info.transform_function(targets[0], final_alias)
1694             if reuse is not None:
1695                 reuse.update(join_list)
1696             return self._get_col(targets[0], join_info.targets[0], join_list[-1])
1697 
1698     def split_exclude(self, filter_expr, can_reuse, names_with_path):
1699         """
1700         When doing an exclude against any kind of N-to-many relation, we need
1701         to use a subquery. This method constructs the nested query, given the
1702         original exclude filter (filter_expr) and the portion up to the first
1703         N-to-many relation field.
1704 
1705         For example, if the origin filter is ~Q(child__name='foo'), filter_expr
1706         is ('child__name', 'foo') and can_reuse is a set of joins usable for
1707         filters in the original query.
1708 
1709         We will turn this into equivalent of:
1710             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1711                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1712 
1713         It might be worth it to consider using WHERE NOT EXISTS as that has
1714         saner null handling, and is easier for the backend's optimizer to
1715         handle.
1716         """
1717         filter_lhs, filter_rhs = filter_expr
1718         if isinstance(filter_rhs, OuterRef):
1719             filter_expr = (filter_lhs, OuterRef(filter_rhs))
1720         elif isinstance(filter_rhs, F):
1721             filter_expr = (filter_lhs, OuterRef(filter_rhs.name))
1722         # Generate the inner query.
1723         query = Query(self.model)
1724         query._filtered_relations = self._filtered_relations
1725         query.add_filter(filter_expr)
1726         query.clear_ordering(True)
1727         # Try to have as simple as possible subquery -> trim leading joins from
1728         # the subquery.
1729         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1730 
1731         # Add extra check to make sure the selected field will not be null
1732         # since we are adding an IN <subquery> clause. This prevents the
1733         # database from tripping over IN (...,NULL,...) selects and returning
1734         # nothing
1735         col = query.select[0]
1736         select_field = col.target
1737         alias = col.alias
1738         if self.is_nullable(select_field):
1739             lookup_class = select_field.get_lookup('isnull')
1740             lookup = lookup_class(select_field.get_col(alias), False)
1741             query.where.add(lookup, AND)
1742         if alias in can_reuse:
1743             pk = select_field.model._meta.pk
1744             # Need to add a restriction so that outer query's filters are in effect for
1745             # the subquery, too.
1746             query.bump_prefix(self)
1747             lookup_class = select_field.get_lookup('exact')
1748             # Note that the query.select[0].alias is different from alias
1749             # due to bump_prefix above.
1750             lookup = lookup_class(pk.get_col(query.select[0].alias),
1751                                   pk.get_col(alias))
1752             query.where.add(lookup, AND)
1753             query.external_aliases[alias] = True
1754 
1755         condition, needed_inner = self.build_filter(
1756             ('%s__in' % trimmed_prefix, query),
1757             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1758         if contains_louter:
1759             or_null_condition, _ = self.build_filter(
1760                 ('%s__isnull' % trimmed_prefix, True),
1761                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1762             condition.add(or_null_condition, OR)
1763             # Note that the end result will be:
1764             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1765             # This might look crazy but due to how IN works, this seems to be
1766             # correct. If the IS NOT NULL check is removed then outercol NOT
1767             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1768             # outercol IS NULL we will not match the row.
1769         return condition, needed_inner
1770 
1771     def set_empty(self):
1772         self.where.add(NothingNode(), AND)
1773 
1774     def is_empty(self):
1775         return any(isinstance(c, NothingNode) for c in self.where.children)
1776 
1777     def set_limits(self, low=None, high=None):
1778         """
1779         Adjust the limits on the rows retrieved. Use low/high to set these,
1780         as it makes it more Pythonic to read and write. When the SQL query is
1781         created, convert them to the appropriate offset and limit values.
1782 
1783         Apply any limits passed in here to the existing constraints. Add low
1784         to the current low value and clamp both to any existing high value.
1785         """
1786         if high is not None:
1787             if self.high_mark is not None:
1788                 self.high_mark = min(self.high_mark, self.low_mark + high)
1789             else:
1790                 self.high_mark = self.low_mark + high
1791         if low is not None:
1792             if self.high_mark is not None:
1793                 self.low_mark = min(self.high_mark, self.low_mark + low)
1794             else:
1795                 self.low_mark = self.low_mark + low
1796 
1797         if self.low_mark == self.high_mark:
1798             self.set_empty()
1799 
1800     def clear_limits(self):
1801         """Clear any existing limits."""
1802         self.low_mark, self.high_mark = 0, None
1803 
1804     @property
1805     def is_sliced(self):
1806         return self.low_mark != 0 or self.high_mark is not None
1807 
1808     def has_limit_one(self):
1809         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1
1810 
1811     def can_filter(self):
1812         """
1813         Return True if adding filters to this instance is still possible.
1814 
1815         Typically, this means no limits or offsets have been put on the results.
1816         """
1817         return not self.is_sliced
1818 
1819     def clear_select_clause(self):
1820         """Remove all fields from SELECT clause."""
1821         self.select = ()
1822         self.default_cols = False
1823         self.select_related = False
1824         self.set_extra_mask(())
1825         self.set_annotation_mask(())
1826 
1827     def clear_select_fields(self):
1828         """
1829         Clear the list of fields to select (but not extra_select columns).
1830         Some queryset types completely replace any existing list of select
1831         columns.
1832         """
1833         self.select = ()
1834         self.values_select = ()
1835 
1836     def add_select_col(self, col):
1837         self.select += col,
1838         self.values_select += col.output_field.name,
1839 
1840     def set_select(self, cols):
1841         self.default_cols = False
1842         self.select = tuple(cols)
1843 
1844     def add_distinct_fields(self, *field_names):
1845         """
1846         Add and resolve the given fields to the query's "distinct on" clause.
1847         """
1848         self.distinct_fields = field_names
1849         self.distinct = True
1850 
1851     def add_fields(self, field_names, allow_m2m=True):
1852         """
1853         Add the given (model) fields to the select set. Add the field names in
1854         the order specified.
1855         """
1856         alias = self.get_initial_alias()
1857         opts = self.get_meta()
1858 
1859         try:
1860             cols = []
1861             for name in field_names:
1862                 # Join promotion note - we must not remove any rows here, so
1863                 # if there is no existing joins, use outer join.
1864                 join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1865                 targets, final_alias, joins = self.trim_joins(
1866                     join_info.targets,
1867                     join_info.joins,
1868                     join_info.path,
1869                 )
1870                 for target in targets:
1871                     cols.append(join_info.transform_function(target, final_alias))
1872             if cols:
1873                 self.set_select(cols)
1874         except MultiJoin:
1875             raise FieldError("Invalid field name: '%s'" % name)
1876         except FieldError:
1877             if LOOKUP_SEP in name:
1878                 # For lookups spanning over relationships, show the error
1879                 # from the model on which the lookup failed.
1880                 raise
1881             else:
1882                 names = sorted([
1883                     *get_field_names_from_opts(opts), *self.extra,
1884                     *self.annotation_select, *self._filtered_relations
1885                 ])
1886                 raise FieldError("Cannot resolve keyword %r into field. "
1887                                  "Choices are: %s" % (name, ", ".join(names)))
1888 
1889     def add_ordering(self, *ordering):
1890         """
1891         Add items from the 'ordering' sequence to the query's "order by"
1892         clause. These items are either field names (not column names) --
1893         possibly with a direction prefix ('-' or '?') -- or OrderBy
1894         expressions.
1895 
1896         If 'ordering' is empty, clear all ordering from the query.
1897         """
1898         errors = []
1899         for item in ordering:
1900             if isinstance(item, str):
1901                 if '.' in item:
1902                     warnings.warn(
1903                         'Passing column raw column aliases to order_by() is '
1904                         'deprecated. Wrap %r in a RawSQL expression before '
1905                         'passing it to order_by().' % item,
1906                         category=RemovedInDjango40Warning,
1907                         stacklevel=3,
1908                     )
1909                     continue
1910                 if item == '?':
1911                     continue
1912                 if item.startswith('-'):
1913                     item = item[1:]
1914                 if item in self.annotations:
1915                     continue
1916                 if self.extra and item in self.extra:
1917                     continue
1918                 # names_to_path() validates the lookup. A descriptive
1919                 # FieldError will be raise if it's not.
1920                 self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)
1921             elif not hasattr(item, 'resolve_expression'):
1922                 errors.append(item)
1923             if getattr(item, 'contains_aggregate', False):
1924                 raise FieldError(
1925                     'Using an aggregate in order_by() without also including '
1926                     'it in annotate() is not allowed: %s' % item
1927                 )
1928         if errors:
1929             raise FieldError('Invalid order_by arguments: %s' % errors)
1930         if ordering:
1931             self.order_by += ordering
1932         else:
1933             self.default_ordering = False
1934 
1935     def clear_ordering(self, force_empty):
1936         """
1937         Remove any ordering settings. If 'force_empty' is True, there will be
1938         no ordering in the resulting query (not even the model's default).
1939         """
1940         self.order_by = ()
1941         self.extra_order_by = ()
1942         if force_empty:
1943             self.default_ordering = False
1944 
1945     def set_group_by(self, allow_aliases=True):
1946         """
1947         Expand the GROUP BY clause required by the query.
1948 
1949         This will usually be the set of all non-aggregate fields in the
1950         return data. If the database backend supports grouping by the
1951         primary key, and the query would be equivalent, the optimization
1952         will be made automatically.
1953         """
1954         # Column names from JOINs to check collisions with aliases.
1955         if allow_aliases:
1956             column_names = set()
1957             seen_models = set()
1958             for join in list(self.alias_map.values())[1:]:  # Skip base table.
1959                 model = join.join_field.related_model
1960                 if model not in seen_models:
1961                     column_names.update({
1962                         field.column
1963                         for field in model._meta.local_concrete_fields
1964                     })
1965                     seen_models.add(model)
1966 
1967         group_by = list(self.select)
1968         if self.annotation_select:
1969             for alias, annotation in self.annotation_select.items():
1970                 signature = inspect.signature(annotation.get_group_by_cols)
1971                 if 'alias' not in signature.parameters:
1972                     annotation_class = annotation.__class__
1973                     msg = (
1974                         '`alias=None` must be added to the signature of '
1975                         '%s.%s.get_group_by_cols().'
1976                     ) % (annotation_class.__module__, annotation_class.__qualname__)
1977                     warnings.warn(msg, category=RemovedInDjango40Warning)
1978                     group_by_cols = annotation.get_group_by_cols()
1979                 else:
1980                     if not allow_aliases or alias in column_names:
1981                         alias = None
1982                     group_by_cols = annotation.get_group_by_cols(alias=alias)
1983                 group_by.extend(group_by_cols)
1984         self.group_by = tuple(group_by)
1985 
1986     def add_select_related(self, fields):
1987         """
1988         Set up the select_related data structure so that we only select
1989         certain related models (as opposed to all models, when
1990         self.select_related=True).
1991         """
1992         if isinstance(self.select_related, bool):
1993             field_dict = {}
1994         else:
1995             field_dict = self.select_related
1996         for field in fields:
1997             d = field_dict
1998             for part in field.split(LOOKUP_SEP):
1999                 d = d.setdefault(part, {})
2000         self.select_related = field_dict
2001 
2002     def add_extra(self, select, select_params, where, params, tables, order_by):
2003         """
2004         Add data to the various extra_* attributes for user-created additions
2005         to the query.
2006         """
2007         if select:
2008             # We need to pair any placeholder markers in the 'select'
2009             # dictionary with their parameters in 'select_params' so that
2010             # subsequent updates to the select dictionary also adjust the
2011             # parameters appropriately.
2012             select_pairs = {}
2013             if select_params:
2014                 param_iter = iter(select_params)
2015             else:
2016                 param_iter = iter([])
2017             for name, entry in select.items():
2018                 entry = str(entry)
2019                 entry_params = []
2020                 pos = entry.find("%s")
2021                 while pos != -1:
2022                     if pos == 0 or entry[pos - 1] != '%':
2023                         entry_params.append(next(param_iter))
2024                     pos = entry.find("%s", pos + 2)
2025                 select_pairs[name] = (entry, entry_params)
2026             self.extra.update(select_pairs)
2027         if where or params:
2028             self.where.add(ExtraWhere(where, params), AND)
2029         if tables:
2030             self.extra_tables += tuple(tables)
2031         if order_by:
2032             self.extra_order_by = order_by
2033 
2034     def clear_deferred_loading(self):
2035         """Remove any fields from the deferred loading set."""
2036         self.deferred_loading = (frozenset(), True)
2037 
2038     def add_deferred_loading(self, field_names):
2039         """
2040         Add the given list of model field names to the set of fields to
2041         exclude from loading from the database when automatic column selection
2042         is done. Add the new field names to any existing field names that
2043         are deferred (or removed from any existing field names that are marked
2044         as the only ones for immediate loading).
2045         """
2046         # Fields on related models are stored in the literal double-underscore
2047         # format, so that we can use a set datastructure. We do the foo__bar
2048         # splitting and handling when computing the SQL column names (as part of
2049         # get_columns()).
2050         existing, defer = self.deferred_loading
2051         if defer:
2052             # Add to existing deferred names.
2053             self.deferred_loading = existing.union(field_names), True
2054         else:
2055             # Remove names from the set of any existing "immediate load" names.
2056             self.deferred_loading = existing.difference(field_names), False
2057 
2058     def add_immediate_loading(self, field_names):
2059         """
2060         Add the given list of model field names to the set of fields to
2061         retrieve when the SQL is executed ("immediate loading" fields). The
2062         field names replace any existing immediate loading field names. If
2063         there are field names already specified for deferred loading, remove
2064         those names from the new field_names before storing the new names
2065         for immediate loading. (That is, immediate loading overrides any
2066         existing immediate values, but respects existing deferrals.)
2067         """
2068         existing, defer = self.deferred_loading
2069         field_names = set(field_names)
2070         if 'pk' in field_names:
2071             field_names.remove('pk')
2072             field_names.add(self.get_meta().pk.name)
2073 
2074         if defer:
2075             # Remove any existing deferred names from the current set before
2076             # setting the new names.
2077             self.deferred_loading = field_names.difference(existing), False
2078         else:
2079             # Replace any existing "immediate load" field names.
2080             self.deferred_loading = frozenset(field_names), False
2081 
2082     def get_loaded_field_names(self):
2083         """
2084         If any fields are marked to be deferred, return a dictionary mapping
2085         models to a set of names in those fields that will be loaded. If a
2086         model is not in the returned dictionary, none of its fields are
2087         deferred.
2088 
2089         If no fields are marked for deferral, return an empty dictionary.
2090         """
2091         # We cache this because we call this function multiple times
2092         # (compiler.fill_related_selections, query.iterator)
2093         try:
2094             return self._loaded_field_names_cache
2095         except AttributeError:
2096             collection = {}
2097             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
2098             self._loaded_field_names_cache = collection
2099             return collection
2100 
2101     def get_loaded_field_names_cb(self, target, model, fields):
2102         """Callback used by get_deferred_field_names()."""
2103         target[model] = {f.attname for f in fields}
2104 
2105     def set_annotation_mask(self, names):
2106         """Set the mask of annotations that will be returned by the SELECT."""
2107         if names is None:
2108             self.annotation_select_mask = None
2109         else:
2110             self.annotation_select_mask = set(names)
2111         self._annotation_select_cache = None
2112 
2113     def append_annotation_mask(self, names):
2114         if self.annotation_select_mask is not None:
2115             self.set_annotation_mask(self.annotation_select_mask.union(names))
2116 
2117     def set_extra_mask(self, names):
2118         """
2119         Set the mask of extra select items that will be returned by SELECT.
2120         Don't remove them from the Query since they might be used later.
2121         """
2122         if names is None:
2123             self.extra_select_mask = None
2124         else:
2125             self.extra_select_mask = set(names)
2126         self._extra_select_cache = None
2127 
2128     def set_values(self, fields):
2129         self.select_related = False
2130         self.clear_deferred_loading()
2131         self.clear_select_fields()
2132 
2133         if fields:
2134             field_names = []
2135             extra_names = []
2136             annotation_names = []
2137             if not self.extra and not self.annotations:
2138                 # Shortcut - if there are no extra or annotations, then
2139                 # the values() clause must be just field names.
2140                 field_names = list(fields)
2141             else:
2142                 self.default_cols = False
2143                 for f in fields:
2144                     if f in self.extra_select:
2145                         extra_names.append(f)
2146                     elif f in self.annotation_select:
2147                         annotation_names.append(f)
2148                     else:
2149                         field_names.append(f)
2150             self.set_extra_mask(extra_names)
2151             self.set_annotation_mask(annotation_names)
2152         else:
2153             field_names = [f.attname for f in self.model._meta.concrete_fields]
2154         # Selected annotations must be known before setting the GROUP BY
2155         # clause.
2156         if self.group_by is True:
2157             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
2158             # Disable GROUP BY aliases to avoid orphaning references to the
2159             # SELECT clause which is about to be cleared.
2160             self.set_group_by(allow_aliases=False)
2161             self.clear_select_fields()
2162         elif self.group_by:
2163             # Resolve GROUP BY annotation references if they are not part of
2164             # the selected fields anymore.
2165             group_by = []
2166             for expr in self.group_by:
2167                 if isinstance(expr, Ref) and expr.refs not in field_names:
2168                     expr = self.annotations[expr.refs]
2169                 group_by.append(expr)
2170             self.group_by = tuple(group_by)
2171 
2172         self.values_select = tuple(field_names)
2173         self.add_fields(field_names, True)
2174 
2175     @property
2176     def annotation_select(self):
2177         """
2178         Return the dictionary of aggregate columns that are not masked and
2179         should be used in the SELECT clause. Cache this result for performance.
2180         """
2181         if self._annotation_select_cache is not None:
2182             return self._annotation_select_cache
2183         elif not self.annotations:
2184             return {}
2185         elif self.annotation_select_mask is not None:
2186             self._annotation_select_cache = {
2187                 k: v for k, v in self.annotations.items()
2188                 if k in self.annotation_select_mask
2189             }
2190             return self._annotation_select_cache
2191         else:
2192             return self.annotations
2193 
2194     @property
2195     def extra_select(self):
2196         if self._extra_select_cache is not None:
2197             return self._extra_select_cache
2198         if not self.extra:
2199             return {}
2200         elif self.extra_select_mask is not None:
2201             self._extra_select_cache = {
2202                 k: v for k, v in self.extra.items()
2203                 if k in self.extra_select_mask
2204             }
2205             return self._extra_select_cache
2206         else:
2207             return self.extra
2208 
2209     def trim_start(self, names_with_path):
2210         """
2211         Trim joins from the start of the join path. The candidates for trim
2212         are the PathInfos in names_with_path structure that are m2m joins.
2213 
2214         Also set the select column so the start matches the join.
2215 
2216         This method is meant to be used for generating the subquery joins &
2217         cols in split_exclude().
2218 
2219         Return a lookup usable for doing outerq.filter(lookup=self) and a
2220         boolean indicating if the joins in the prefix contain a LEFT OUTER join.
2221         _"""
2222         all_paths = []
2223         for _, paths in names_with_path:
2224             all_paths.extend(paths)
2225         contains_louter = False
2226         # Trim and operate only on tables that were generated for
2227         # the lookup part of the query. That is, avoid trimming
2228         # joins generated for F() expressions.
2229         lookup_tables = [
2230             t for t in self.alias_map
2231             if t in self._lookup_joins or t == self.base_table
2232         ]
2233         for trimmed_paths, path in enumerate(all_paths):
2234             if path.m2m:
2235                 break
2236             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
2237                 contains_louter = True
2238             alias = lookup_tables[trimmed_paths]
2239             self.unref_alias(alias)
2240         # The path.join_field is a Rel, lets get the other side's field
2241         join_field = path.join_field.field
2242         # Build the filter prefix.
2243         paths_in_prefix = trimmed_paths
2244         trimmed_prefix = []
2245         for name, path in names_with_path:
2246             if paths_in_prefix - len(path) < 0:
2247                 break
2248             trimmed_prefix.append(name)
2249             paths_in_prefix -= len(path)
2250         trimmed_prefix.append(
2251             join_field.foreign_related_fields[0].name)
2252         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
2253         # Lets still see if we can trim the first join from the inner query
2254         # (that is, self). We can't do this for:
2255         # - LEFT JOINs because we would miss those rows that have nothing on
2256         #   the outer side,
2257         # - INNER JOINs from filtered relations because we would miss their
2258         #   filters.
2259         first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]
2260         if first_join.join_type != LOUTER and not first_join.filtered_relation:
2261             select_fields = [r[0] for r in join_field.related_fields]
2262             select_alias = lookup_tables[trimmed_paths + 1]
2263             self.unref_alias(lookup_tables[trimmed_paths])
2264             extra_restriction = join_field.get_extra_restriction(
2265                 self.where_class, None, lookup_tables[trimmed_paths + 1])
2266             if extra_restriction:
2267                 self.where.add(extra_restriction, AND)
2268         else:
2269             # TODO: It might be possible to trim more joins from the start of the
2270             # inner query if it happens to have a longer join chain containing the
2271             # values in select_fields. Lets punt this one for now.
2272             select_fields = [r[1] for r in join_field.related_fields]
2273             select_alias = lookup_tables[trimmed_paths]
2274         # The found starting point is likely a Join instead of a BaseTable reference.
2275         # But the first entry in the query's FROM clause must not be a JOIN.
2276         for table in self.alias_map:
2277             if self.alias_refcount[table] > 0:
2278                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
2279                 break
2280         self.set_select([f.get_col(select_alias) for f in select_fields])
2281         return trimmed_prefix, contains_louter
2282 
2283     def is_nullable(self, field):
2284         """
2285         Check if the given field should be treated as nullable.
2286 
2287         Some backends treat '' as null and Django treats such fields as
2288         nullable for those backends. In such situations field.null can be
2289         False even if we should treat the field as nullable.
2290         """
2291         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2292         # (nor should it have) knowledge of which connection is going to be
2293         # used. The proper fix would be to defer all decisions where
2294         # is_nullable() is needed to the compiler stage, but that is not easy
2295         # to do currently.
2296         return (
2297             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
2298             field.empty_strings_allowed
2299         ) or field.null

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 00:19:44.008 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, *args, **kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         self._query = value
214 
215     def as_manager(cls):
216         # Address the circular dependency between `Queryset` and `Manager`.
217         from django.db.models.manager import Manager
218         manager = Manager.from_queryset(cls)()
219         manager._built_with_as_manager = True
220         return manager
221     as_manager.queryset_only = True
222     as_manager = classmethod(as_manager)
223 
224     ########################
225     # PYTHON MAGIC METHODS #
226     ########################
227 
228     def __deepcopy__(self, memo):
229         """Don't populate the QuerySet's cache."""
230         obj = self.__class__()
231         for k, v in self.__dict__.items():
232             if k == '_result_cache':
233                 obj.__dict__[k] = None
234             else:
235                 obj.__dict__[k] = copy.deepcopy(v, memo)
236         return obj
237 
238     def __getstate__(self):
239         # Force the cache to be fully populated.
240         self._fetch_all()
241         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
242 
243     def __setstate__(self, state):
244         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
245         if pickled_version:
246             if pickled_version != django.__version__:
247                 warnings.warn(
248                     "Pickled queryset instance's Django version %s does not "
249                     "match the current version %s."
250                     % (pickled_version, django.__version__),
251                     RuntimeWarning,
252                     stacklevel=2,
253                 )
254         else:
255             warnings.warn(
256                 "Pickled queryset instance's Django version is not specified.",
257                 RuntimeWarning,
258                 stacklevel=2,
259             )
260         self.__dict__.update(state)
261 
262     def __repr__(self):
263         data = list(self[:REPR_OUTPUT_SIZE + 1])
264         if len(data) > REPR_OUTPUT_SIZE:
265             data[-1] = "...(remaining elements truncated)..."
266         return '<%s %r>' % (self.__class__.__name__, data)
267 
268     def __len__(self):
269         self._fetch_all()
270         return len(self._result_cache)
271 
272     def __iter__(self):
273         """
274         The queryset iterator protocol uses three nested iterators in the
275         default case:
276             1. sql.compiler.execute_sql()
277                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
278                  using cursor.fetchmany(). This part is responsible for
279                  doing some column masking, and returning the rows in chunks.
280             2. sql.compiler.results_iter()
281                - Returns one row at time. At this point the rows are still just
282                  tuples. In some cases the return values are converted to
283                  Python values at this location.
284             3. self.iterator()
285                - Responsible for turning the rows into model objects.
286         """
287         self._fetch_all()
288         return iter(self._result_cache)
289 
290     def __bool__(self):
291         self._fetch_all()
292         return bool(self._result_cache)
293 
294     def __getitem__(self, k):
295         """Retrieve an item or slice from the set of results."""
296         if not isinstance(k, (int, slice)):
297             raise TypeError(
298                 'QuerySet indices must be integers or slices, not %s.'
299                 % type(k).__name__
300             )
301         assert ((not isinstance(k, slice) and (k >= 0)) or
302                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
303                  (k.stop is None or k.stop >= 0))), \
304             "Negative indexing is not supported."
305 
306         if self._result_cache is not None:
307             return self._result_cache[k]
308 
309         if isinstance(k, slice):
310             qs = self._chain()
311             if k.start is not None:
312                 start = int(k.start)
313             else:
314                 start = None
315             if k.stop is not None:
316                 stop = int(k.stop)
317             else:
318                 stop = None
319             qs.query.set_limits(start, stop)
320             return list(qs)[::k.step] if k.step else qs
321 
322         qs = self._chain()
323         qs.query.set_limits(k, k + 1)
324         qs._fetch_all()
325         return qs._result_cache[0]
326 
327     def __class_getitem__(cls, *args, **kwargs):
328         return cls
329 
330     def __and__(self, other):
331         self._merge_sanity_check(other)
332         if isinstance(other, EmptyQuerySet):
333             return other
334         if isinstance(self, EmptyQuerySet):
335             return self
336         combined = self._chain()
337         combined._merge_known_related_objects(other)
338         combined.query.combine(other.query, sql.AND)
339         return combined
340 
341     def __or__(self, other):
342         self._merge_sanity_check(other)
343         if isinstance(self, EmptyQuerySet):
344             return other
345         if isinstance(other, EmptyQuerySet):
346             return self
347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
348         combined = query._chain()
349         combined._merge_known_related_objects(other)
350         if not other.query.can_filter():
351             other = other.model._base_manager.filter(pk__in=other.values('pk'))
352         combined.query.combine(other.query, sql.OR)
353         return combined
354 
355     ####################################
356     # METHODS THAT DO DATABASE QUERIES #
357     ####################################
358 
359     def _iterator(self, use_chunked_fetch, chunk_size):
360         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
361 
362     def iterator(self, chunk_size=2000):
363         """
364         An iterator over the results from applying this QuerySet to the
365         database.
366         """
367         if chunk_size <= 0:
368             raise ValueError('Chunk size must be strictly positive.')
369         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
370         return self._iterator(use_chunked_fetch, chunk_size)
371 
372     def aggregate(self, *args, **kwargs):
373         """
374         Return a dictionary containing the calculations (aggregation)
375         over the current queryset.
376 
377         If args is present the expression is passed as a kwarg using
378         the Aggregate object's default alias.
379         """
380         if self.query.distinct_fields:
381             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
382         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
383         for arg in args:
384             # The default_alias property raises TypeError if default_alias
385             # can't be set automatically or AttributeError if it isn't an
386             # attribute.
387             try:
388                 arg.default_alias
389             except (AttributeError, TypeError):
390                 raise TypeError("Complex aggregates require an alias")
391             kwargs[arg.default_alias] = arg
392 
393         query = self.query.chain()
394         for (alias, aggregate_expr) in kwargs.items():
395             query.add_annotation(aggregate_expr, alias, is_summary=True)
396             if not query.annotations[alias].contains_aggregate:
397                 raise TypeError("%s is not an aggregate expression" % alias)
398         return query.get_aggregation(self.db, kwargs)
399 
400     def count(self):
401         """
402         Perform a SELECT COUNT() and return the number of records as an
403         integer.
404 
405         If the QuerySet is already fully cached, return the length of the
406         cached results set to avoid multiple SELECT COUNT(*) calls.
407         """
408         if self._result_cache is not None:
409             return len(self._result_cache)
410 
411         return self.query.get_count(using=self.db)
412 
413     def get(self, *args, **kwargs):
414         """
415         Perform the query and return a single object matching the given
416         keyword arguments.
417         """
418         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
419         if self.query.can_filter() and not self.query.distinct_fields:
420             clone = clone.order_by()
421         limit = None
422         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
423             limit = MAX_GET_RESULTS
424             clone.query.set_limits(high=limit)
425         num = len(clone)
426         if num == 1:
427             return clone._result_cache[0]
428         if not num:
429             raise self.model.DoesNotExist(
430                 "%s matching query does not exist." %
431                 self.model._meta.object_name
432             )
433         raise self.model.MultipleObjectsReturned(
434             'get() returned more than one %s -- it returned %s!' % (
435                 self.model._meta.object_name,
436                 num if not limit or num < limit else 'more than %s' % (limit - 1),
437             )
438         )
439 
440     def create(self, **kwargs):
441         """
442         Create a new object with the given kwargs, saving it to the database
443         and returning the created object.
444         """
445         obj = self.model(**kwargs)
446         self._for_write = True
447         obj.save(force_insert=True, using=self.db)
448         return obj
449 
450     def _populate_pk_values(self, objs):
451         for obj in objs:
452             if obj.pk is None:
453                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
454 
455     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
456         """
457         Insert each of the instances into the database. Do *not* call
458         save() on each of the instances, do not send any pre/post_save
459         signals, and do not set the primary key attribute if it is an
460         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
461         Multi-table models are not supported.
462         """
463         # When you bulk insert you don't get the primary keys back (if it's an
464         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
465         # you can't insert into the child tables which references this. There
466         # are two workarounds:
467         # 1) This could be implemented if you didn't have an autoincrement pk
468         # 2) You could do it by doing O(n) normal inserts into the parent
469         #    tables to get the primary keys back and then doing a single bulk
470         #    insert into the childmost table.
471         # We currently set the primary keys on the objects when using
472         # PostgreSQL via the RETURNING ID clause. It should be possible for
473         # Oracle as well, but the semantics for extracting the primary keys is
474         # trickier so it's not done yet.
475         assert batch_size is None or batch_size > 0
476         # Check that the parents share the same concrete model with the our
477         # model to detect the inheritance pattern ConcreteGrandParent ->
478         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
479         # would not identify that case as involving multiple tables.
480         for parent in self.model._meta.get_parent_list():
481             if parent._meta.concrete_model is not self.model._meta.concrete_model:
482                 raise ValueError("Can't bulk create a multi-table inherited model")
483         if not objs:
484             return objs
485         self._for_write = True
486         connection = connections[self.db]
487         opts = self.model._meta
488         fields = opts.concrete_fields
489         objs = list(objs)
490         self._populate_pk_values(objs)
491         with transaction.atomic(using=self.db, savepoint=False):
492             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
493             if objs_with_pk:
494                 returned_columns = self._batched_insert(
495                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
496                 )
497                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
498                     for result, field in zip(results, opts.db_returning_fields):
499                         if field != opts.pk:
500                             setattr(obj_with_pk, field.attname, result)
501                 for obj_with_pk in objs_with_pk:
502                     obj_with_pk._state.adding = False
503                     obj_with_pk._state.db = self.db
504             if objs_without_pk:
505                 fields = [f for f in fields if not isinstance(f, AutoField)]
506                 returned_columns = self._batched_insert(
507                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
508                 )
509                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
510                     assert len(returned_columns) == len(objs_without_pk)
511                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
512                     for result, field in zip(results, opts.db_returning_fields):
513                         setattr(obj_without_pk, field.attname, result)
514                     obj_without_pk._state.adding = False
515                     obj_without_pk._state.db = self.db
516 
517         return objs
518 
519     def bulk_update(self, objs, fields, batch_size=None):
520         """
521         Update the given fields in each of the given objects in the database.
522         """
523         if batch_size is not None and batch_size < 0:
524             raise ValueError('Batch size must be a positive integer.')
525         if not fields:
526             raise ValueError('Field names must be given to bulk_update().')
527         objs = tuple(objs)
528         if any(obj.pk is None for obj in objs):
529             raise ValueError('All bulk_update() objects must have a primary key set.')
530         fields = [self.model._meta.get_field(name) for name in fields]
531         if any(not f.concrete or f.many_to_many for f in fields):
532             raise ValueError('bulk_update() can only be used with concrete fields.')
533         if any(f.primary_key for f in fields):
534             raise ValueError('bulk_update() cannot be used with primary key fields.')
535         if not objs:
536             return
537         # PK is used twice in the resulting update query, once in the filter
538         # and once in the WHEN. Each field will also have one CAST.
539         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
540         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
541         requires_casting = connections[self.db].features.requires_casted_case_in_updates
542         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
543         updates = []
544         for batch_objs in batches:
545             update_kwargs = {}
546             for field in fields:
547                 when_statements = []
548                 for obj in batch_objs:
549                     attr = getattr(obj, field.attname)
550                     if not isinstance(attr, Expression):
551                         attr = Value(attr, output_field=field)
552                     when_statements.append(When(pk=obj.pk, then=attr))
553                 case_statement = Case(*when_statements, output_field=field)
554                 if requires_casting:
555                     case_statement = Cast(case_statement, output_field=field)
556                 update_kwargs[field.attname] = case_statement
557             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
558         with transaction.atomic(using=self.db, savepoint=False):
559             for pks, update_kwargs in updates:
560                 self.filter(pk__in=pks).update(**update_kwargs)
561     bulk_update.alters_data = True
562 
563     def get_or_create(self, defaults=None, **kwargs):
564         """
565         Look up an object with the given kwargs, creating one if necessary.
566         Return a tuple of (object, created), where created is a boolean
567         specifying whether an object was created.
568         """
569         # The get() needs to be targeted at the write database in order
570         # to avoid potential transaction consistency problems.
571         self._for_write = True
572         try:
573             return self.get(**kwargs), False
574         except self.model.DoesNotExist:
575             params = self._extract_model_params(defaults, **kwargs)
576             return self._create_object_from_params(kwargs, params)
577 
578     def update_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, updating one with defaults
581         if it exists, otherwise create a new one.
582         Return a tuple (object, created), where created is a boolean
583         specifying whether an object was created.
584         """
585         defaults = defaults or {}
586         self._for_write = True
587         with transaction.atomic(using=self.db):
588             try:
589                 obj = self.select_for_update().get(**kwargs)
590             except self.model.DoesNotExist:
591                 params = self._extract_model_params(defaults, **kwargs)
592                 # Lock the row so that a concurrent update is blocked until
593                 # after update_or_create() has performed its save.
594                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
595                 if created:
596                     return obj, created
597             for k, v in resolve_callables(defaults):
598                 setattr(obj, k, v)
599             obj.save(using=self.db)
600         return obj, False
601 
602     def _create_object_from_params(self, lookup, params, lock=False):
603         """
604         Try to create an object using passed params. Used by get_or_create()
605         and update_or_create().
606         """
607         try:
608             with transaction.atomic(using=self.db):
609                 params = dict(resolve_callables(params))
610                 obj = self.create(**params)
611             return obj, True
612         except IntegrityError:
613             try:
614                 qs = self.select_for_update() if lock else self
615                 return qs.get(**lookup), False
616             except self.model.DoesNotExist:
617                 pass
618             raise
619 
620     def _extract_model_params(self, defaults, **kwargs):
621         """
622         Prepare `params` for creating a model instance based on the given
623         kwargs; for use by get_or_create() and update_or_create().
624         """
625         defaults = defaults or {}
626         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
627         params.update(defaults)
628         property_names = self.model._meta._property_names
629         invalid_params = []
630         for param in params:
631             try:
632                 self.model._meta.get_field(param)
633             except exceptions.FieldDoesNotExist:
634                 # It's okay to use a model's property if it has a setter.
635                 if not (param in property_names and getattr(self.model, param).fset):
636                     invalid_params.append(param)
637         if invalid_params:
638             raise exceptions.FieldError(
639                 "Invalid field name(s) for model %s: '%s'." % (
640                     self.model._meta.object_name,
641                     "', '".join(sorted(invalid_params)),
642                 ))
643         return params
644 
645     def _earliest(self, *fields):
646         """
647         Return the earliest object according to fields (if given) or by the
648         model's Meta.get_latest_by.
649         """
650         if fields:
651             order_by = fields
652         else:
653             order_by = getattr(self.model._meta, 'get_latest_by')
654             if order_by and not isinstance(order_by, (tuple, list)):
655                 order_by = (order_by,)
656         if order_by is None:
657             raise ValueError(
658                 "earliest() and latest() require either fields as positional "
659                 "arguments or 'get_latest_by' in the model's Meta."
660             )
661 
662         assert not self.query.is_sliced, \
663             "Cannot change a query once a slice has been taken."
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force_empty=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         return self._earliest(*fields)
672 
673     def latest(self, *fields):
674         return self.reverse()._earliest(*fields)
675 
676     def first(self):
677         """Return the first object of a query or None if no match is found."""
678         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
679             return obj
680 
681     def last(self):
682         """Return the last object of a query or None if no match is found."""
683         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
684             return obj
685 
686     def in_bulk(self, id_list=None, *, field_name='pk'):
687         """
688         Return a dictionary mapping each of the given IDs to the object with
689         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
690         """
691         assert not self.query.is_sliced, \
692             "Cannot use 'limit' or 'offset' with in_bulk"
693         opts = self.model._meta
694         unique_fields = [
695             constraint.fields[0]
696             for constraint in opts.total_unique_constraints
697             if len(constraint.fields) == 1
698         ]
699         if (
700             field_name != 'pk' and
701             not opts.get_field(field_name).unique and
702             field_name not in unique_fields
703         ):
704             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
705         if id_list is not None:
706             if not id_list:
707                 return {}
708             filter_key = '{}__in'.format(field_name)
709             batch_size = connections[self.db].features.max_query_params
710             id_list = tuple(id_list)
711             # If the database has a limit on the number of query parameters
712             # (e.g. SQLite), retrieve objects in batches if necessary.
713             if batch_size and batch_size < len(id_list):
714                 qs = ()
715                 for offset in range(0, len(id_list), batch_size):
716                     batch = id_list[offset:offset + batch_size]
717                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
718             else:
719                 qs = self.filter(**{filter_key: id_list}).order_by()
720         else:
721             qs = self._chain()
722         return {getattr(obj, field_name): obj for obj in qs}
723 
724     def delete(self):
725         """Delete the records in the current QuerySet."""
726         self._not_support_combined_queries('delete')
727         assert not self.query.is_sliced, \
728             "Cannot use 'limit' or 'offset' with delete."
729 
730         if self._fields is not None:
731             raise TypeError("Cannot call delete() after .values() or .values_list()")
732 
733         del_query = self._chain()
734 
735         # The delete is actually 2 queries - one to find related objects,
736         # and one to delete. Make sure that the discovery of related
737         # objects is performed on the same database as the deletion.
738         del_query._for_write = True
739 
740         # Disable non-supported fields.
741         del_query.query.select_for_update = False
742         del_query.query.select_related = False
743         del_query.query.clear_ordering(force_empty=True)
744 
745         collector = Collector(using=del_query.db)
746         collector.collect(del_query)
747         deleted, _rows_count = collector.delete()
748 
749         # Clear the result cache, in case this QuerySet gets reused.
750         self._result_cache = None
751         return deleted, _rows_count
752 
753     delete.alters_data = True
754     delete.queryset_only = True
755 
756     def _raw_delete(self, using):
757         """
758         Delete objects found from the given queryset in single direct SQL
759         query. No signals are sent and there is no protection for cascades.
760         """
761         query = self.query.clone()
762         query.__class__ = sql.DeleteQuery
763         cursor = query.get_compiler(using).execute_sql(CURSOR)
764         if cursor:
765             with cursor:
766                 return cursor.rowcount
767         return 0
768     _raw_delete.alters_data = True
769 
770     def update(self, **kwargs):
771         """
772         Update all elements in the current QuerySet, setting all the given
773         fields to the appropriate values.
774         """
775         self._not_support_combined_queries('update')
776         assert not self.query.is_sliced, \
777             "Cannot update a query once a slice has been taken."
778         self._for_write = True
779         query = self.query.chain(sql.UpdateQuery)
780         query.add_update_values(kwargs)
781         # Clear any annotations so that they won't be present in subqueries.
782         query.annotations = {}
783         with transaction.mark_for_rollback_on_error(using=self.db):
784             rows = query.get_compiler(self.db).execute_sql(CURSOR)
785         self._result_cache = None
786         return rows
787     update.alters_data = True
788 
789     def _update(self, values):
790         """
791         A version of update() that accepts field objects instead of field names.
792         Used primarily for model saving and not intended for use by general
793         code (it requires too much poking around at model internals to be
794         useful at that level).
795         """
796         assert not self.query.is_sliced, \
797             "Cannot update a query once a slice has been taken."
798         query = self.query.chain(sql.UpdateQuery)
799         query.add_update_fields(values)
800         # Clear any annotations so that they won't be present in subqueries.
801         query.annotations = {}
802         self._result_cache = None
803         return query.get_compiler(self.db).execute_sql(CURSOR)
804     _update.alters_data = True
805     _update.queryset_only = False
806 
807     def exists(self):
808         if self._result_cache is None:
809             return self.query.has_results(using=self.db)
810         return bool(self._result_cache)
811 
812     def _prefetch_related_objects(self):
813         # This method can only be called once the result cache has been filled.
814         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
815         self._prefetch_done = True
816 
817     def explain(self, *, format=None, **options):
818         return self.query.explain(using=self.db, format=format, **options)
819 
820     ##################################################
821     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
822     ##################################################
823 
824     def raw(self, raw_query, params=None, translations=None, using=None):
825         if using is None:
826             using = self.db
827         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
828         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
829         return qs
830 
831     def _values(self, *fields, **expressions):
832         clone = self._chain()
833         if expressions:
834             clone = clone.annotate(**expressions)
835         clone._fields = fields
836         clone.query.set_values(fields)
837         return clone
838 
839     def values(self, *fields, **expressions):
840         fields += tuple(expressions)
841         clone = self._values(*fields, **expressions)
842         clone._iterable_class = ValuesIterable
843         return clone
844 
845     def values_list(self, *fields, flat=False, named=False):
846         if flat and named:
847             raise TypeError("'flat' and 'named' can't be used together.")
848         if flat and len(fields) > 1:
849             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
850 
851         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
852         _fields = []
853         expressions = {}
854         counter = 1
855         for field in fields:
856             if hasattr(field, 'resolve_expression'):
857                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
858                 while True:
859                     field_id = field_id_prefix + str(counter)
860                     counter += 1
861                     if field_id not in field_names:
862                         break
863                 expressions[field_id] = field
864                 _fields.append(field_id)
865             else:
866                 _fields.append(field)
867 
868         clone = self._values(*_fields, **expressions)
869         clone._iterable_class = (
870             NamedValuesListIterable if named
871             else FlatValuesListIterable if flat
872             else ValuesListIterable
873         )
874         return clone
875 
876     def dates(self, field_name, kind, order='ASC'):
877         """
878         Return a list of date objects representing all available dates for
879         the given field_name, scoped to 'kind'.
880         """
881         assert kind in ('year', 'month', 'week', 'day'), \
882             "'kind' must be one of 'year', 'month', 'week', or 'day'."
883         assert order in ('ASC', 'DESC'), \
884             "'order' must be either 'ASC' or 'DESC'."
885         return self.annotate(
886             datefield=Trunc(field_name, kind, output_field=DateField()),
887             plain_field=F(field_name)
888         ).values_list(
889             'datefield', flat=True
890         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
891 
892     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
893         """
894         Return a list of datetime objects representing all available
895         datetimes for the given field_name, scoped to 'kind'.
896         """
897         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
898             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
899         assert order in ('ASC', 'DESC'), \
900             "'order' must be either 'ASC' or 'DESC'."
901         if settings.USE_TZ:
902             if tzinfo is None:
903                 tzinfo = timezone.get_current_timezone()
904         else:
905             tzinfo = None
906         return self.annotate(
907             datetimefield=Trunc(
908                 field_name,
909                 kind,
910                 output_field=DateTimeField(),
911                 tzinfo=tzinfo,
912                 is_dst=is_dst,
913             ),
914             plain_field=F(field_name)
915         ).values_list(
916             'datetimefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
918 
919     def none(self):
920         """Return an empty QuerySet."""
921         clone = self._chain()
922         clone.query.set_empty()
923         return clone
924 
925     ##################################################################
926     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
927     ##################################################################
928 
929     def all(self):
930         """
931         Return a new QuerySet that is a copy of the current one. This allows a
932         QuerySet to proxy for a model manager in some cases.
933         """
934         return self._chain()
935 
936     def filter(self, *args, **kwargs):
937         """
938         Return a new QuerySet instance with the args ANDed to the existing
939         set.
940         """
941         self._not_support_combined_queries('filter')
942         return self._filter_or_exclude(False, *args, **kwargs)
943 
944     def exclude(self, *args, **kwargs):
945         """
946         Return a new QuerySet instance with NOT (args) ANDed to the existing
947         set.
948         """
949         self._not_support_combined_queries('exclude')
950         return self._filter_or_exclude(True, *args, **kwargs)
951 
952     def _filter_or_exclude(self, negate, *args, **kwargs):
953         if args or kwargs:
954             assert not self.query.is_sliced, \
955                 "Cannot filter a query once a slice has been taken."
956 
957         clone = self._chain()
958         if self._defer_next_filter:
959             self._defer_next_filter = False
960             clone._deferred_filter = negate, args, kwargs
961         else:
962             clone._filter_or_exclude_inplace(negate, *args, **kwargs)
963         return clone
964 
965     def _filter_or_exclude_inplace(self, negate, *args, **kwargs):
966         if negate:
967             self._query.add_q(~Q(*args, **kwargs))
968         else:
969             self._query.add_q(Q(*args, **kwargs))
970 
971     def complex_filter(self, filter_obj):
972         """
973         Return a new QuerySet instance with filter_obj added to the filters.
974 
975         filter_obj can be a Q object or a dictionary of keyword lookup
976         arguments.
977 
978         This exists to support framework features such as 'limit_choices_to',
979         and usually it will be more natural to use other methods.
980         """
981         if isinstance(filter_obj, Q):
982             clone = self._chain()
983             clone.query.add_q(filter_obj)
984             return clone
985         else:
986             return self._filter_or_exclude(False, **filter_obj)
987 
988     def _combinator_query(self, combinator, *other_qs, all=False):
989         # Clone the query to inherit the select list and everything
990         clone = self._chain()
991         # Clear limits and ordering so they can be reapplied
992         clone.query.clear_ordering(True)
993         clone.query.clear_limits()
994         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
995         clone.query.combinator = combinator
996         clone.query.combinator_all = all
997         return clone
998 
999     def union(self, *other_qs, all=False):
1000         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1001         if isinstance(self, EmptyQuerySet):
1002             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1003             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
1004         return self._combinator_query('union', *other_qs, all=all)
1005 
1006     def intersection(self, *other_qs):
1007         # If any query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         for other in other_qs:
1011             if isinstance(other, EmptyQuerySet):
1012                 return other
1013         return self._combinator_query('intersection', *other_qs)
1014 
1015     def difference(self, *other_qs):
1016         # If the query is an EmptyQuerySet, return it.
1017         if isinstance(self, EmptyQuerySet):
1018             return self
1019         return self._combinator_query('difference', *other_qs)
1020 
1021     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1022         """
1023         Return a new QuerySet instance that will select objects with a
1024         FOR UPDATE lock.
1025         """
1026         if nowait and skip_locked:
1027             raise ValueError('The nowait option cannot be used with skip_locked.')
1028         obj = self._chain()
1029         obj._for_write = True
1030         obj.query.select_for_update = True
1031         obj.query.select_for_update_nowait = nowait
1032         obj.query.select_for_update_skip_locked = skip_locked
1033         obj.query.select_for_update_of = of
1034         obj.query.select_for_no_key_update = no_key
1035         return obj
1036 
1037     def select_related(self, *fields):
1038         """
1039         Return a new QuerySet instance that will select related objects.
1040 
1041         If fields are specified, they must be ForeignKey fields and only those
1042         related objects are included in the selection.
1043 
1044         If select_related(None) is called, clear the list.
1045         """
1046         self._not_support_combined_queries('select_related')
1047         if self._fields is not None:
1048             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1049 
1050         obj = self._chain()
1051         if fields == (None,):
1052             obj.query.select_related = False
1053         elif fields:
1054             obj.query.add_select_related(fields)
1055         else:
1056             obj.query.select_related = True
1057         return obj
1058 
1059     def prefetch_related(self, *lookups):
1060         """
1061         Return a new QuerySet instance that will prefetch the specified
1062         Many-To-One and Many-To-Many related objects when the QuerySet is
1063         evaluated.
1064 
1065         When prefetch_related() is called more than once, append to the list of
1066         prefetch lookups. If prefetch_related(None) is called, clear the list.
1067         """
1068         self._not_support_combined_queries('prefetch_related')
1069         clone = self._chain()
1070         if lookups == (None,):
1071             clone._prefetch_related_lookups = ()
1072         else:
1073             for lookup in lookups:
1074                 if isinstance(lookup, Prefetch):
1075                     lookup = lookup.prefetch_to
1076                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1077                 if lookup in self.query._filtered_relations:
1078                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1079             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1080         return clone
1081 
1082     def annotate(self, *args, **kwargs):
1083         """
1084         Return a query set in which the returned objects have been annotated
1085         with extra data or aggregations.
1086         """
1087         self._not_support_combined_queries('annotate')
1088         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1089         annotations = {}
1090         for arg in args:
1091             # The default_alias property may raise a TypeError.
1092             try:
1093                 if arg.default_alias in kwargs:
1094                     raise ValueError("The named annotation '%s' conflicts with the "
1095                                      "default name for another annotation."
1096                                      % arg.default_alias)
1097             except TypeError:
1098                 raise TypeError("Complex annotations require an alias")
1099             annotations[arg.default_alias] = arg
1100         annotations.update(kwargs)
1101 
1102         clone = self._chain()
1103         names = self._fields
1104         if names is None:
1105             names = set(chain.from_iterable(
1106                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1107                 for field in self.model._meta.get_fields()
1108             ))
1109 
1110         for alias, annotation in annotations.items():
1111             if alias in names:
1112                 raise ValueError("The annotation '%s' conflicts with a field on "
1113                                  "the model." % alias)
1114             if isinstance(annotation, FilteredRelation):
1115                 clone.query.add_filtered_relation(annotation, alias)
1116             else:
1117                 clone.query.add_annotation(annotation, alias, is_summary=False)
1118 
1119         for alias, annotation in clone.query.annotations.items():
1120             if alias in annotations and annotation.contains_aggregate:
1121                 if clone._fields is None:
1122                     clone.query.group_by = True
1123                 else:
1124                     clone.query.set_group_by()
1125                 break
1126 
1127         return clone
1128 
1129     def order_by(self, *field_names):
1130         """Return a new QuerySet instance with the ordering changed."""
1131         assert not self.query.is_sliced, \
1132             "Cannot reorder a query once a slice has been taken."
1133         obj = self._chain()
1134         obj.query.clear_ordering(force_empty=False)
1135         obj.query.add_ordering(*field_names)
1136         return obj
1137 
1138     def distinct(self, *field_names):
1139         """
1140         Return a new QuerySet instance that will select only distinct results.
1141         """
1142         self._not_support_combined_queries('distinct')
1143         assert not self.query.is_sliced, \
1144             "Cannot create distinct fields once a slice has been taken."
1145         obj = self._chain()
1146         obj.query.add_distinct_fields(*field_names)
1147         return obj
1148 
1149     def extra(self, select=None, where=None, params=None, tables=None,
1150               order_by=None, select_params=None):
1151         """Add extra SQL fragments to the query."""
1152         self._not_support_combined_queries('extra')
1153         assert not self.query.is_sliced, \
1154             "Cannot change a query once a slice has been taken"
1155         clone = self._chain()
1156         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1157         return clone
1158 
1159     def reverse(self):
1160         """Reverse the ordering of the QuerySet."""
1161         if self.query.is_sliced:
1162             raise TypeError('Cannot reverse a query once a slice has been taken.')
1163         clone = self._chain()
1164         clone.query.standard_ordering = not clone.query.standard_ordering
1165         return clone
1166 
1167     def defer(self, *fields):
1168         """
1169         Defer the loading of data for certain fields until they are accessed.
1170         Add the set of deferred fields to any existing set of deferred fields.
1171         The only exception to this is if None is passed in as the only
1172         parameter, in which case removal all deferrals.
1173         """
1174         self._not_support_combined_queries('defer')
1175         if self._fields is not None:
1176             raise TypeError("Cannot call defer() after .values() or .values_list()")
1177         clone = self._chain()
1178         if fields == (None,):
1179             clone.query.clear_deferred_loading()
1180         else:
1181             clone.query.add_deferred_loading(fields)
1182         return clone
1183 
1184     def only(self, *fields):
1185         """
1186         Essentially, the opposite of defer(). Only the fields passed into this
1187         method and that are not already specified as deferred are loaded
1188         immediately when the queryset is evaluated.
1189         """
1190         self._not_support_combined_queries('only')
1191         if self._fields is not None:
1192             raise TypeError("Cannot call only() after .values() or .values_list()")
1193         if fields == (None,):
1194             # Can only pass None to defer(), not only(), as the rest option.
1195             # That won't stop people trying to do this, so let's be explicit.
1196             raise TypeError("Cannot pass None as an argument to only().")
1197         for field in fields:
1198             field = field.split(LOOKUP_SEP, 1)[0]
1199             if field in self.query._filtered_relations:
1200                 raise ValueError('only() is not supported with FilteredRelation.')
1201         clone = self._chain()
1202         clone.query.add_immediate_loading(fields)
1203         return clone
1204 
1205     def using(self, alias):
1206         """Select which database this QuerySet should execute against."""
1207         clone = self._chain()
1208         clone._db = alias
1209         return clone
1210 
1211     ###################################
1212     # PUBLIC INTROSPECTION ATTRIBUTES #
1213     ###################################
1214 
1215     @property
1216     def ordered(self):
1217         """
1218         Return True if the QuerySet is ordered -- i.e. has an order_by()
1219         clause or a default ordering on the model (or is empty).
1220         """
1221         if isinstance(self, EmptyQuerySet):
1222             return True
1223         if self.query.extra_order_by or self.query.order_by:
1224             return True
1225         elif self.query.default_ordering and self.query.get_meta().ordering:
1226             return True
1227         else:
1228             return False
1229 
1230     @property
1231     def db(self):
1232         """Return the database used if this query is executed now."""
1233         if self._for_write:
1234             return self._db or router.db_for_write(self.model, **self._hints)
1235         return self._db or router.db_for_read(self.model, **self._hints)
1236 
1237     ###################
1238     # PRIVATE METHODS #
1239     ###################
1240 
1241     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1242         """
1243         Insert a new record for the given model. This provides an interface to
1244         the InsertQuery class and is how Model.save() is implemented.
1245         """
1246         self._for_write = True
1247         if using is None:
1248             using = self.db
1249         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1250         query.insert_values(fields, objs, raw=raw)
1251         return query.get_compiler(using=using).execute_sql(returning_fields)
1252     _insert.alters_data = True
1253     _insert.queryset_only = False
1254 
1255     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1256         """
1257         Helper method for bulk_create() to insert objs one batch at a time.
1258         """
1259         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1260             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1261         ops = connections[self.db].ops
1262         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1263         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1264         inserted_rows = []
1265         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1266         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1267             if bulk_return and not ignore_conflicts:
1268                 inserted_rows.extend(self._insert(
1269                     item, fields=fields, using=self.db,
1270                     returning_fields=self.model._meta.db_returning_fields,
1271                     ignore_conflicts=ignore_conflicts,
1272                 ))
1273             else:
1274                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1275         return inserted_rows
1276 
1277     def _chain(self, **kwargs):
1278         """
1279         Return a copy of the current QuerySet that's ready for another
1280         operation.
1281         """
1282         obj = self._clone()
1283         if obj._sticky_filter:
1284             obj.query.filter_is_sticky = True
1285             obj._sticky_filter = False
1286         obj.__dict__.update(kwargs)
1287         return obj
1288 
1289     def _clone(self):
1290         """
1291         Return a copy of the current QuerySet. A lightweight alternative
1292         to deepcopy().
1293         """
1294         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1295         c._sticky_filter = self._sticky_filter
1296         c._for_write = self._for_write
1297         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1298         c._known_related_objects = self._known_related_objects
1299         c._iterable_class = self._iterable_class
1300         c._fields = self._fields
1301         return c
1302 
1303     def _fetch_all(self):
1304         if self._result_cache is None:
1305             self._result_cache = list(self._iterable_class(self))
1306         if self._prefetch_related_lookups and not self._prefetch_done:
1307             self._prefetch_related_objects()
1308 
1309     def _next_is_sticky(self):
1310         """
1311         Indicate that the next filter call and the one following that should
1312         be treated as a single filter. This is only important when it comes to
1313         determining when to reuse tables for many-to-many filters. Required so
1314         that we can filter naturally on the results of related managers.
1315 
1316         This doesn't return a clone of the current QuerySet (it returns
1317         "self"). The method is only used internally and should be immediately
1318         followed by a filter() that does create a clone.
1319         """
1320         self._sticky_filter = True
1321         return self
1322 
1323     def _merge_sanity_check(self, other):
1324         """Check that two QuerySet classes may be merged."""
1325         if self._fields is not None and (
1326                 set(self.query.values_select) != set(other.query.values_select) or
1327                 set(self.query.extra_select) != set(other.query.extra_select) or
1328                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1329             raise TypeError(
1330                 "Merging '%s' classes must involve the same values in each case."
1331                 % self.__class__.__name__
1332             )
1333 
1334     def _merge_known_related_objects(self, other):
1335         """
1336         Keep track of all known related objects from either QuerySet instance.
1337         """
1338         for field, objects in other._known_related_objects.items():
1339             self._known_related_objects.setdefault(field, {}).update(objects)
1340 
1341     def resolve_expression(self, *args, **kwargs):
1342         if self._fields and len(self._fields) > 1:
1343             # values() queryset can only be used as nested queries
1344             # if they are set up to select only a single field.
1345             raise TypeError('Cannot use multi-field values as a filter value.')
1346         query = self.query.resolve_expression(*args, **kwargs)
1347         query._db = self._db
1348         return query
1349     resolve_expression.queryset_only = True
1350 
1351     def _add_hints(self, **hints):
1352         """
1353         Update hinting information for use by routers. Add new key/values or
1354         overwrite existing key/values.
1355         """
1356         self._hints.update(hints)
1357 
1358     def _has_filters(self):
1359         """
1360         Check if this QuerySet has any filtering going on. This isn't
1361         equivalent with checking if all objects are present in results, for
1362         example, qs[1:]._has_filters() -> False.
1363         """
1364         return self.query.has_filters()
1365 
1366     @staticmethod
1367     def _validate_values_are_expressions(values, method_name):
1368         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1369         if invalid_args:
1370             raise TypeError(
1371                 'QuerySet.%s() received non-expression(s): %s.' % (
1372                     method_name,
1373                     ', '.join(invalid_args),
1374                 )
1375             )
1376 
1377     def _not_support_combined_queries(self, operation_name):
1378         if self.query.combinator:
1379             raise NotSupportedError(
1380                 'Calling QuerySet.%s() after %s() is not supported.'
1381                 % (operation_name, self.query.combinator)
1382             )

</code><intended_behavior>The method responsible for constructing IN queries as part of prefetch_related operations should be adjusted to exclude NULL values from the list of IDs used in the IN clause.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>build_filter</method>
<code>
1204     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1205                      can_reuse=None, allow_joins=True, split_subq=True,
1206                      reuse_with_filtered_relation=False, check_filterable=True):
1207         """
1208         Build a WhereNode for a single filter clause but don't add it
1209         to this Query. Query.add_q() will then add this filter to the where
1210         Node.
1211 
1212         The 'branch_negated' tells us if the current branch contains any
1213         negations. This will be used to determine if subqueries are needed.
1214 
1215         The 'current_negated' is used to determine if the current filter is
1216         negated or not and this will be used to determine if IS NULL filtering
1217         is needed.
1218 
1219         The difference between current_negated and branch_negated is that
1220         branch_negated is set on first negation, but current_negated is
1221         flipped for each negation.
1222 
1223         Note that add_filter will not do any negating itself, that is done
1224         upper in the code by add_q().
1225 
1226         The 'can_reuse' is a set of reusable joins for multijoins.
1227 
1228         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse
1229         will be reused.
1230 
1231         The method will create a filter clause that can be added to the current
1232         query. However, if the filter isn't added to the query then the caller
1233         is responsible for unreffing the joins used.
1234         """
1235         if isinstance(filter_expr, dict):
1236             raise FieldError("Cannot parse keyword query as dict")
1237         if isinstance(filter_expr, Q):
1238             return self._add_q(
1239                 filter_expr,
1240                 branch_negated=branch_negated,
1241                 current_negated=current_negated,
1242                 used_aliases=can_reuse,
1243                 allow_joins=allow_joins,
1244                 split_subq=split_subq,
1245                 check_filterable=check_filterable,
1246             )
1247         if hasattr(filter_expr, 'resolve_expression'):
1248             if not getattr(filter_expr, 'conditional', False):
1249                 raise TypeError('Cannot filter against a non-conditional expression.')
1250             condition = self.build_lookup(
1251                 ['exact'], filter_expr.resolve_expression(self, allow_joins=allow_joins), True
1252             )
1253             clause = self.where_class()
1254             clause.add(condition, AND)
1255             return clause, []
1256         arg, value = filter_expr
1257         if not arg:
1258             raise FieldError("Cannot parse keyword query %r" % arg)
1259         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1260 
1261         if check_filterable:
1262             self.check_filterable(reffed_expression)
1263 
1264         if not allow_joins and len(parts) > 1:
1265             raise FieldError("Joined field references are not permitted in this query")
1266 
1267         pre_joins = self.alias_refcount.copy()
1268         value = self.resolve_lookup_value(value, can_reuse, allow_joins)
1269         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1270 
1271         if check_filterable:
1272             self.check_filterable(value)
1273 
1274         clause = self.where_class()
1275         if reffed_expression:
1276             condition = self.build_lookup(lookups, reffed_expression, value)
1277             clause.add(condition, AND)
1278             return clause, []
1279 
1280         opts = self.get_meta()
1281         alias = self.get_initial_alias()
1282         allow_many = not branch_negated or not split_subq
1283 
1284         try:
1285             join_info = self.setup_joins(
1286                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1287                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1288             )
1289 
1290             # Prevent iterator from being consumed by check_related_objects()
1291             if isinstance(value, Iterator):
1292                 value = list(value)
1293             self.check_related_objects(join_info.final_field, value, join_info.opts)
1294 
1295             # split_exclude() needs to know which joins were generated for the
1296             # lookup parts
1297             self._lookup_joins = join_info.joins
1298         except MultiJoin as e:
1299             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1300 
1301         # Update used_joins before trimming since they are reused to determine
1302         # which joins could be later promoted to INNER.
1303         used_joins.update(join_info.joins)
1304         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1305         if can_reuse is not None:
1306             can_reuse.update(join_list)
1307 
1308         if join_info.final_field.is_relation:
1309             # No support for transforms for relational fields
1310             num_lookups = len(lookups)
1311             if num_lookups > 1:
1312                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1313             if len(targets) == 1:
1314                 col = self._get_col(targets[0], join_info.final_field, alias)
1315             else:
1316                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1317         else:
1318             col = self._get_col(targets[0], join_info.final_field, alias)
1319 
1320         condition = self.build_lookup(lookups, col, value)
1321         lookup_type = condition.lookup_name
1322         clause.add(condition, AND)
1323 
1324         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1325         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1326             require_outer = True
1327             if (lookup_type != 'isnull' and (
1328                     self.is_nullable(targets[0]) or
1329                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1330                 # The condition added here will be SQL like this:
1331                 # NOT (col IS NOT NULL), where the first NOT is added in
1332                 # upper layers of code. The reason for addition is that if col
1333                 # is null, then col != someval will result in SQL "unknown"
1334                 # which isn't the same as in Python. The Python None handling
1335                 # is wanted, and it can be gotten by
1336                 # (col IS NULL OR col != someval)
1337                 #   <=>
1338                 # NOT (col IS NOT NULL AND col = someval).
1339                 lookup_class = targets[0].get_lookup('isnull')
1340                 col = self._get_col(targets[0], join_info.targets[0], alias)
1341                 clause.add(lookup_class(col, False), AND)
1342         return clause, used_joins if not require_outer else ()

</code><intended_behavior>Review should ensure that modifications to handle the exclusion of NULL values in IN lookups do not inadvertently affect the general functionality of build_filter.</intended_behavior>, <file>django/db/models/sql/query.py</file>
<class>Query</class>
<method>None</method>
<code>
136 class Query(BaseExpression):
137     """A single SQL query."""
138 
139     alias_prefix = 'T'
140     subq_aliases = frozenset([alias_prefix])
141 
142     compiler = 'SQLCompiler'
143 
144     def __init__(self, model, where=WhereNode, alias_cols=True):
145         self.model = model
146         self.alias_refcount = {}
147         # alias_map is the most important data structure regarding joins.
148         # It's used for recording which joins exist in the query and what
149         # types they are. The key is the alias of the joined table (possibly
150         # the table name) and the value is a Join-like object (see
151         # sql.datastructures.Join for more information).
152         self.alias_map = {}
153         # Whether to provide alias to columns during reference resolving.
154         self.alias_cols = alias_cols
155         # Sometimes the query contains references to aliases in outer queries (as
156         # a result of split_exclude). Correct alias quoting needs to know these
157         # aliases too.
158         # Map external tables to whether they are aliased.
159         self.external_aliases = {}
160         self.table_map = {}     # Maps table names to list of aliases.
161         self.default_cols = True
162         self.default_ordering = True
163         self.standard_ordering = True
164         self.used_aliases = set()
165         self.filter_is_sticky = False
166         self.subquery = False
167 
168         # SQL-related attributes
169         # Select and related select clauses are expressions to use in the
170         # SELECT clause of the query.
171         # The select is used for cases where we want to set up the select
172         # clause to contain other than default fields (values(), subqueries...)
173         # Note that annotations go to annotations dictionary.
174         self.select = ()
175         self.where = where()
176         self.where_class = where
177         # The group_by attribute can have one of the following forms:
178         #  - None: no group by at all in the query
179         #  - A tuple of expressions: group by (at least) those expressions.
180         #    String refs are also allowed for now.
181         #  - True: group by all select fields of the model
182         # See compiler.get_group_by() for details.
183         self.group_by = None
184         self.order_by = ()
185         self.low_mark, self.high_mark = 0, None  # Used for offset/limit
186         self.distinct = False
187         self.distinct_fields = ()
188         self.select_for_update = False
189         self.select_for_update_nowait = False
190         self.select_for_update_skip_locked = False
191         self.select_for_update_of = ()
192         self.select_for_no_key_update = False
193 
194         self.select_related = False
195         # Arbitrary limit for select_related to prevents infinite recursion.
196         self.max_depth = 5
197 
198         # Holds the selects defined by a call to values() or values_list()
199         # excluding annotation_select and extra_select.
200         self.values_select = ()
201 
202         # SQL annotation-related attributes
203         self.annotations = {}  # Maps alias -> Annotation Expression
204         self.annotation_select_mask = None
205         self._annotation_select_cache = None
206 
207         # Set combination attributes
208         self.combinator = None
209         self.combinator_all = False
210         self.combined_queries = ()
211 
212         # These are for extensions. The contents are more or less appended
213         # verbatim to the appropriate clause.
214         self.extra = {}  # Maps col_alias -> (col_sql, params).
215         self.extra_select_mask = None
216         self._extra_select_cache = None
217 
218         self.extra_tables = ()
219         self.extra_order_by = ()
220 
221         # A tuple that is a set of model field names and either True, if these
222         # are the fields to defer, or False if these are the only fields to
223         # load.
224         self.deferred_loading = (frozenset(), True)
225 
226         self._filtered_relations = {}
227 
228         self.explain_query = False
229         self.explain_format = None
230         self.explain_options = {}
231 
232     @property
233     def output_field(self):
234         if len(self.select) == 1:
235             select = self.select[0]
236             return getattr(select, 'target', None) or select.field
237         elif len(self.annotation_select) == 1:
238             return next(iter(self.annotation_select.values())).output_field
239 
240     @property
241     def has_select_fields(self):
242         return bool(self.select or self.annotation_select_mask or self.extra_select_mask)
243 
244     @cached_property
245     def base_table(self):
246         for alias in self.alias_map:
247             return alias
248 
249     def __str__(self):
250         """
251         Return the query as a string of SQL with the parameter values
252         substituted in (use sql_with_params() to see the unsubstituted string).
253 
254         Parameter values won't necessarily be quoted correctly, since that is
255         done by the database interface at execution time.
256         """
257         sql, params = self.sql_with_params()
258         return sql % params
259 
260     def sql_with_params(self):
261         """
262         Return the query as an SQL string and the parameters that will be
263         substituted into the query.
264         """
265         return self.get_compiler(DEFAULT_DB_ALIAS).as_sql()
266 
267     def __deepcopy__(self, memo):
268         """Limit the amount of work when a Query is deepcopied."""
269         result = self.clone()
270         memo[id(self)] = result
271         return result
272 
273     def get_compiler(self, using=None, connection=None):
274         if using is None and connection is None:
275             raise ValueError("Need either using or connection")
276         if using:
277             connection = connections[using]
278         return connection.ops.compiler(self.compiler)(self, connection, using)
279 
280     def get_meta(self):
281         """
282         Return the Options instance (the model._meta) from which to start
283         processing. Normally, this is self.model._meta, but it can be changed
284         by subclasses.
285         """
286         return self.model._meta
287 
288     def clone(self):
289         """
290         Return a copy of the current Query. A lightweight alternative to
291         to deepcopy().
292         """
293         obj = Empty()
294         obj.__class__ = self.__class__
295         # Copy references to everything.
296         obj.__dict__ = self.__dict__.copy()
297         # Clone attributes that can't use shallow copy.
298         obj.alias_refcount = self.alias_refcount.copy()
299         obj.alias_map = self.alias_map.copy()
300         obj.external_aliases = self.external_aliases.copy()
301         obj.table_map = self.table_map.copy()
302         obj.where = self.where.clone()
303         obj.annotations = self.annotations.copy()
304         if self.annotation_select_mask is None:
305             obj.annotation_select_mask = None
306         else:
307             obj.annotation_select_mask = self.annotation_select_mask.copy()
308         # _annotation_select_cache cannot be copied, as doing so breaks the
309         # (necessary) state in which both annotations and
310         # _annotation_select_cache point to the same underlying objects.
311         # It will get re-populated in the cloned queryset the next time it's
312         # used.
313         obj._annotation_select_cache = None
314         obj.extra = self.extra.copy()
315         if self.extra_select_mask is None:
316             obj.extra_select_mask = None
317         else:
318             obj.extra_select_mask = self.extra_select_mask.copy()
319         if self._extra_select_cache is None:
320             obj._extra_select_cache = None
321         else:
322             obj._extra_select_cache = self._extra_select_cache.copy()
323         if self.select_related is not False:
324             # Use deepcopy because select_related stores fields in nested
325             # dicts.
326             obj.select_related = copy.deepcopy(obj.select_related)
327         if 'subq_aliases' in self.__dict__:
328             obj.subq_aliases = self.subq_aliases.copy()
329         obj.used_aliases = self.used_aliases.copy()
330         obj._filtered_relations = self._filtered_relations.copy()
331         # Clear the cached_property
332         try:
333             del obj.base_table
334         except AttributeError:
335             pass
336         return obj
337 
338     def chain(self, klass=None):
339         """
340         Return a copy of the current Query that's ready for another operation.
341         The klass argument changes the type of the Query, e.g. UpdateQuery.
342         """
343         obj = self.clone()
344         if klass and obj.__class__ != klass:
345             obj.__class__ = klass
346         if not obj.filter_is_sticky:
347             obj.used_aliases = set()
348         obj.filter_is_sticky = False
349         if hasattr(obj, '_setup_query'):
350             obj._setup_query()
351         return obj
352 
353     def relabeled_clone(self, change_map):
354         clone = self.clone()
355         clone.change_aliases(change_map)
356         return clone
357 
358     def _get_col(self, target, field, alias):
359         if not self.alias_cols:
360             alias = None
361         return target.get_col(alias, field)
362 
363     def rewrite_cols(self, annotation, col_cnt):
364         # We must make sure the inner query has the referred columns in it.
365         # If we are aggregating over an annotation, then Django uses Ref()
366         # instances to note this. However, if we are annotating over a column
367         # of a related model, then it might be that column isn't part of the
368         # SELECT clause of the inner query, and we must manually make sure
369         # the column is selected. An example case is:
370         #    .aggregate(Sum('author__awards'))
371         # Resolving this expression results in a join to author, but there
372         # is no guarantee the awards column of author is in the select clause
373         # of the query. Thus we must manually add the column to the inner
374         # query.
375         orig_exprs = annotation.get_source_expressions()
376         new_exprs = []
377         for expr in orig_exprs:
378             # FIXME: These conditions are fairly arbitrary. Identify a better
379             # method of having expressions decide which code path they should
380             # take.
381             if isinstance(expr, Ref):
382                 # Its already a Ref to subquery (see resolve_ref() for
383                 # details)
384                 new_exprs.append(expr)
385             elif isinstance(expr, (WhereNode, Lookup)):
386                 # Decompose the subexpressions further. The code here is
387                 # copied from the else clause, but this condition must appear
388                 # before the contains_aggregate/is_summary condition below.
389                 new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
390                 new_exprs.append(new_expr)
391             else:
392                 # Reuse aliases of expressions already selected in subquery.
393                 for col_alias, selected_annotation in self.annotation_select.items():
394                     if selected_annotation is expr:
395                         new_expr = Ref(col_alias, expr)
396                         break
397                 else:
398                     # An expression that is not selected the subquery.
399                     if isinstance(expr, Col) or (expr.contains_aggregate and not expr.is_summary):
400                         # Reference column or another aggregate. Select it
401                         # under a non-conflicting alias.
402                         col_cnt += 1
403                         col_alias = '__col%d' % col_cnt
404                         self.annotations[col_alias] = expr
405                         self.append_annotation_mask([col_alias])
406                         new_expr = Ref(col_alias, expr)
407                     else:
408                         # Some other expression not referencing database values
409                         # directly. Its subexpression might contain Cols.
410                         new_expr, col_cnt = self.rewrite_cols(expr, col_cnt)
411                 new_exprs.append(new_expr)
412         annotation.set_source_expressions(new_exprs)
413         return annotation, col_cnt
414 
415     def get_aggregation(self, using, added_aggregate_names):
416         """
417         Return the dictionary with the values of the existing aggregations.
418         """
419         if not self.annotation_select:
420             return {}
421         existing_annotations = [
422             annotation for alias, annotation
423             in self.annotations.items()
424             if alias not in added_aggregate_names
425         ]
426         # Decide if we need to use a subquery.
427         #
428         # Existing annotations would cause incorrect results as get_aggregation()
429         # must produce just one result and thus must not use GROUP BY. But we
430         # aren't smart enough to remove the existing annotations from the
431         # query, so those would force us to use GROUP BY.
432         #
433         # If the query has limit or distinct, or uses set operations, then
434         # those operations must be done in a subquery so that the query
435         # aggregates on the limit and/or distinct results instead of applying
436         # the distinct and limit after the aggregation.
437         if (isinstance(self.group_by, tuple) or self.is_sliced or existing_annotations or
438                 self.distinct or self.combinator):
439             from django.db.models.sql.subqueries import AggregateQuery
440             outer_query = AggregateQuery(self.model)
441             inner_query = self.clone()
442             inner_query.select_for_update = False
443             inner_query.select_related = False
444             inner_query.set_annotation_mask(self.annotation_select)
445             if not self.is_sliced and not self.distinct_fields:
446                 # Queries with distinct_fields need ordering and when a limit
447                 # is applied we must take the slice from the ordered query.
448                 # Otherwise no need for ordering.
449                 inner_query.clear_ordering(True)
450             if not inner_query.distinct:
451                 # If the inner query uses default select and it has some
452                 # aggregate annotations, then we must make sure the inner
453                 # query is grouped by the main model's primary key. However,
454                 # clearing the select clause can alter results if distinct is
455                 # used.
456                 has_existing_aggregate_annotations = any(
457                     annotation for annotation in existing_annotations
458                     if getattr(annotation, 'contains_aggregate', True)
459                 )
460                 if inner_query.default_cols and has_existing_aggregate_annotations:
461                     inner_query.group_by = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
462                 inner_query.default_cols = False
463 
464             relabels = {t: 'subquery' for t in inner_query.alias_map}
465             relabels[None] = 'subquery'
466             # Remove any aggregates marked for reduction from the subquery
467             # and move them to the outer AggregateQuery.
468             col_cnt = 0
469             for alias, expression in list(inner_query.annotation_select.items()):
470                 annotation_select_mask = inner_query.annotation_select_mask
471                 if expression.is_summary:
472                     expression, col_cnt = inner_query.rewrite_cols(expression, col_cnt)
473                     outer_query.annotations[alias] = expression.relabeled_clone(relabels)
474                     del inner_query.annotations[alias]
475                     annotation_select_mask.remove(alias)
476                 # Make sure the annotation_select wont use cached results.
477                 inner_query.set_annotation_mask(inner_query.annotation_select_mask)
478             if inner_query.select == () and not inner_query.default_cols and not inner_query.annotation_select_mask:
479                 # In case of Model.objects[0:3].count(), there would be no
480                 # field selected in the inner query, yet we must use a subquery.
481                 # So, make sure at least one field is selected.
482                 inner_query.select = (self.model._meta.pk.get_col(inner_query.get_initial_alias()),)
483             try:
484                 outer_query.add_subquery(inner_query, using)
485             except EmptyResultSet:
486                 return {
487                     alias: None
488                     for alias in outer_query.annotation_select
489                 }
490         else:
491             outer_query = self
492             self.select = ()
493             self.default_cols = False
494             self.extra = {}
495 
496         outer_query.clear_ordering(True)
497         outer_query.clear_limits()
498         outer_query.select_for_update = False
499         outer_query.select_related = False
500         compiler = outer_query.get_compiler(using)
501         result = compiler.execute_sql(SINGLE)
502         if result is None:
503             result = [None] * len(outer_query.annotation_select)
504 
505         converters = compiler.get_converters(outer_query.annotation_select.values())
506         result = next(compiler.apply_converters((result,), converters))
507 
508         return dict(zip(outer_query.annotation_select, result))
509 
510     def get_count(self, using):
511         """
512         Perform a COUNT() query using the current filter constraints.
513         """
514         obj = self.clone()
515         obj.add_annotation(Count('*'), alias='__count', is_summary=True)
516         number = obj.get_aggregation(using, ['__count'])['__count']
517         if number is None:
518             number = 0
519         return number
520 
521     def has_filters(self):
522         return self.where
523 
524     def has_results(self, using):
525         q = self.clone()
526         if not q.distinct:
527             if q.group_by is True:
528                 q.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
529                 # Disable GROUP BY aliases to avoid orphaning references to the
530                 # SELECT clause which is about to be cleared.
531                 q.set_group_by(allow_aliases=False)
532             q.clear_select_clause()
533         q.clear_ordering(True)
534         q.set_limits(high=1)
535         compiler = q.get_compiler(using=using)
536         return compiler.has_results()
537 
538     def explain(self, using, format=None, **options):
539         q = self.clone()
540         q.explain_query = True
541         q.explain_format = format
542         q.explain_options = options
543         compiler = q.get_compiler(using=using)
544         return '\n'.join(compiler.explain_query())
545 
546     def combine(self, rhs, connector):
547         """
548         Merge the 'rhs' query into the current one (with any 'rhs' effects
549         being applied *after* (that is, "to the right of") anything in the
550         current query. 'rhs' is not modified during a call to this function.
551 
552         The 'connector' parameter describes how to connect filters from the
553         'rhs' query.
554         """
555         assert self.model == rhs.model, \
556             "Cannot combine queries on two different base models."
557         assert not self.is_sliced, \
558             "Cannot combine queries once a slice has been taken."
559         assert self.distinct == rhs.distinct, \
560             "Cannot combine a unique query with a non-unique query."
561         assert self.distinct_fields == rhs.distinct_fields, \
562             "Cannot combine queries with different distinct fields."
563 
564         # Work out how to relabel the rhs aliases, if necessary.
565         change_map = {}
566         conjunction = (connector == AND)
567 
568         # Determine which existing joins can be reused. When combining the
569         # query with AND we must recreate all joins for m2m filters. When
570         # combining with OR we can reuse joins. The reason is that in AND
571         # case a single row can't fulfill a condition like:
572         #     revrel__col=1 & revrel__col=2
573         # But, there might be two different related rows matching this
574         # condition. In OR case a single True is enough, so single row is
575         # enough, too.
576         #
577         # Note that we will be creating duplicate joins for non-m2m joins in
578         # the AND case. The results will be correct but this creates too many
579         # joins. This is something that could be fixed later on.
580         reuse = set() if conjunction else set(self.alias_map)
581         # Base table must be present in the query - this is the same
582         # table on both sides.
583         self.get_initial_alias()
584         joinpromoter = JoinPromoter(connector, 2, False)
585         joinpromoter.add_votes(
586             j for j in self.alias_map if self.alias_map[j].join_type == INNER)
587         rhs_votes = set()
588         # Now, add the joins from rhs query into the new query (skipping base
589         # table).
590         rhs_tables = list(rhs.alias_map)[1:]
591         for alias in rhs_tables:
592             join = rhs.alias_map[alias]
593             # If the left side of the join was already relabeled, use the
594             # updated alias.
595             join = join.relabeled_clone(change_map)
596             new_alias = self.join(join, reuse=reuse)
597             if join.join_type == INNER:
598                 rhs_votes.add(new_alias)
599             # We can't reuse the same join again in the query. If we have two
600             # distinct joins for the same connection in rhs query, then the
601             # combined query must have two joins, too.
602             reuse.discard(new_alias)
603             if alias != new_alias:
604                 change_map[alias] = new_alias
605             if not rhs.alias_refcount[alias]:
606                 # The alias was unused in the rhs query. Unref it so that it
607                 # will be unused in the new query, too. We have to add and
608                 # unref the alias so that join promotion has information of
609                 # the join type for the unused alias.
610                 self.unref_alias(new_alias)
611         joinpromoter.add_votes(rhs_votes)
612         joinpromoter.update_join_types(self)
613 
614         # Now relabel a copy of the rhs where-clause and add it to the current
615         # one.
616         w = rhs.where.clone()
617         w.relabel_aliases(change_map)
618         self.where.add(w, connector)
619 
620         # Selection columns and extra extensions are those provided by 'rhs'.
621         if rhs.select:
622             self.set_select([col.relabeled_clone(change_map) for col in rhs.select])
623         else:
624             self.select = ()
625 
626         if connector == OR:
627             # It would be nice to be able to handle this, but the queries don't
628             # really make sense (or return consistent value sets). Not worth
629             # the extra complexity when you can write a real query instead.
630             if self.extra and rhs.extra:
631                 raise ValueError("When merging querysets using 'or', you cannot have extra(select=...) on both sides.")
632         self.extra.update(rhs.extra)
633         extra_select_mask = set()
634         if self.extra_select_mask is not None:
635             extra_select_mask.update(self.extra_select_mask)
636         if rhs.extra_select_mask is not None:
637             extra_select_mask.update(rhs.extra_select_mask)
638         if extra_select_mask:
639             self.set_extra_mask(extra_select_mask)
640         self.extra_tables += rhs.extra_tables
641 
642         # Ordering uses the 'rhs' ordering, unless it has none, in which case
643         # the current ordering is used.
644         self.order_by = rhs.order_by or self.order_by
645         self.extra_order_by = rhs.extra_order_by or self.extra_order_by
646 
647     def deferred_to_data(self, target, callback):
648         """
649         Convert the self.deferred_loading data structure to an alternate data
650         structure, describing the field that *will* be loaded. This is used to
651         compute the columns to select from the database and also by the
652         QuerySet class to work out which fields are being initialized on each
653         model. Models that have all their fields included aren't mentioned in
654         the result, only those that have field restrictions in place.
655 
656         The "target" parameter is the instance that is populated (in place).
657         The "callback" is a function that is called whenever a (model, field)
658         pair need to be added to "target". It accepts three parameters:
659         "target", and the model and list of fields being added for that model.
660         """
661         field_names, defer = self.deferred_loading
662         if not field_names:
663             return
664         orig_opts = self.get_meta()
665         seen = {}
666         must_include = {orig_opts.concrete_model: {orig_opts.pk}}
667         for field_name in field_names:
668             parts = field_name.split(LOOKUP_SEP)
669             cur_model = self.model._meta.concrete_model
670             opts = orig_opts
671             for name in parts[:-1]:
672                 old_model = cur_model
673                 if name in self._filtered_relations:
674                     name = self._filtered_relations[name].relation_name
675                 source = opts.get_field(name)
676                 if is_reverse_o2o(source):
677                     cur_model = source.related_model
678                 else:
679                     cur_model = source.remote_field.model
680                 opts = cur_model._meta
681                 # Even if we're "just passing through" this model, we must add
682                 # both the current model's pk and the related reference field
683                 # (if it's not a reverse relation) to the things we select.
684                 if not is_reverse_o2o(source):
685                     must_include[old_model].add(source)
686                 add_to_dict(must_include, cur_model, opts.pk)
687             field = opts.get_field(parts[-1])
688             is_reverse_object = field.auto_created and not field.concrete
689             model = field.related_model if is_reverse_object else field.model
690             model = model._meta.concrete_model
691             if model == opts.model:
692                 model = cur_model
693             if not is_reverse_o2o(field):
694                 add_to_dict(seen, model, field)
695 
696         if defer:
697             # We need to load all fields for each model, except those that
698             # appear in "seen" (for all models that appear in "seen"). The only
699             # slight complexity here is handling fields that exist on parent
700             # models.
701             workset = {}
702             for model, values in seen.items():
703                 for field in model._meta.local_fields:
704                     if field not in values:
705                         m = field.model._meta.concrete_model
706                         add_to_dict(workset, m, field)
707             for model, values in must_include.items():
708                 # If we haven't included a model in workset, we don't add the
709                 # corresponding must_include fields for that model, since an
710                 # empty set means "include all fields". That's why there's no
711                 # "else" branch here.
712                 if model in workset:
713                     workset[model].update(values)
714             for model, values in workset.items():
715                 callback(target, model, values)
716         else:
717             for model, values in must_include.items():
718                 if model in seen:
719                     seen[model].update(values)
720                 else:
721                     # As we've passed through this model, but not explicitly
722                     # included any fields, we have to make sure it's mentioned
723                     # so that only the "must include" fields are pulled in.
724                     seen[model] = values
725             # Now ensure that every model in the inheritance chain is mentioned
726             # in the parent list. Again, it must be mentioned to ensure that
727             # only "must include" fields are pulled in.
728             for model in orig_opts.get_parent_list():
729                 seen.setdefault(model, set())
730             for model, values in seen.items():
731                 callback(target, model, values)
732 
733     def table_alias(self, table_name, create=False, filtered_relation=None):
734         """
735         Return a table alias for the given table_name and whether this is a
736         new alias or not.
737 
738         If 'create' is true, a new alias is always created. Otherwise, the
739         most recently created alias for the table (if one exists) is reused.
740         """
741         alias_list = self.table_map.get(table_name)
742         if not create and alias_list:
743             alias = alias_list[0]
744             self.alias_refcount[alias] += 1
745             return alias, False
746 
747         # Create a new alias for this table.
748         if alias_list:
749             alias = '%s%d' % (self.alias_prefix, len(self.alias_map) + 1)
750             alias_list.append(alias)
751         else:
752             # The first occurrence of a table uses the table name directly.
753             alias = filtered_relation.alias if filtered_relation is not None else table_name
754             self.table_map[table_name] = [alias]
755         self.alias_refcount[alias] = 1
756         return alias, True
757 
758     def ref_alias(self, alias):
759         """Increases the reference count for this alias."""
760         self.alias_refcount[alias] += 1
761 
762     def unref_alias(self, alias, amount=1):
763         """Decreases the reference count for this alias."""
764         self.alias_refcount[alias] -= amount
765 
766     def promote_joins(self, aliases):
767         """
768         Promote recursively the join type of given aliases and its children to
769         an outer join. If 'unconditional' is False, only promote the join if
770         it is nullable or the parent join is an outer join.
771 
772         The children promotion is done to avoid join chains that contain a LOUTER
773         b INNER c. So, if we have currently a INNER b INNER c and a->b is promoted,
774         then we must also promote b->c automatically, or otherwise the promotion
775         of a->b doesn't actually change anything in the query results.
776         """
777         aliases = list(aliases)
778         while aliases:
779             alias = aliases.pop(0)
780             if self.alias_map[alias].join_type is None:
781                 # This is the base table (first FROM entry) - this table
782                 # isn't really joined at all in the query, so we should not
783                 # alter its join type.
784                 continue
785             # Only the first alias (skipped above) should have None join_type
786             assert self.alias_map[alias].join_type is not None
787             parent_alias = self.alias_map[alias].parent_alias
788             parent_louter = parent_alias and self.alias_map[parent_alias].join_type == LOUTER
789             already_louter = self.alias_map[alias].join_type == LOUTER
790             if ((self.alias_map[alias].nullable or parent_louter) and
791                     not already_louter):
792                 self.alias_map[alias] = self.alias_map[alias].promote()
793                 # Join type of 'alias' changed, so re-examine all aliases that
794                 # refer to this one.
795                 aliases.extend(
796                     join for join in self.alias_map
797                     if self.alias_map[join].parent_alias == alias and join not in aliases
798                 )
799 
800     def demote_joins(self, aliases):
801         """
802         Change join type from LOUTER to INNER for all joins in aliases.
803 
804         Similarly to promote_joins(), this method must ensure no join chains
805         containing first an outer, then an inner join are generated. If we
806         are demoting b->c join in chain a LOUTER b LOUTER c then we must
807         demote a->b automatically, or otherwise the demotion of b->c doesn't
808         actually change anything in the query results. .
809         """
810         aliases = list(aliases)
811         while aliases:
812             alias = aliases.pop(0)
813             if self.alias_map[alias].join_type == LOUTER:
814                 self.alias_map[alias] = self.alias_map[alias].demote()
815                 parent_alias = self.alias_map[alias].parent_alias
816                 if self.alias_map[parent_alias].join_type == INNER:
817                     aliases.append(parent_alias)
818 
819     def reset_refcounts(self, to_counts):
820         """
821         Reset reference counts for aliases so that they match the value passed
822         in `to_counts`.
823         """
824         for alias, cur_refcount in self.alias_refcount.copy().items():
825             unref_amount = cur_refcount - to_counts.get(alias, 0)
826             self.unref_alias(alias, unref_amount)
827 
828     def change_aliases(self, change_map):
829         """
830         Change the aliases in change_map (which maps old-alias -> new-alias),
831         relabelling any references to them in select columns and the where
832         clause.
833         """
834         assert set(change_map).isdisjoint(change_map.values())
835 
836         # 1. Update references in "select" (normal columns plus aliases),
837         # "group by" and "where".
838         self.where.relabel_aliases(change_map)
839         if isinstance(self.group_by, tuple):
840             self.group_by = tuple([col.relabeled_clone(change_map) for col in self.group_by])
841         self.select = tuple([col.relabeled_clone(change_map) for col in self.select])
842         self.annotations = self.annotations and {
843             key: col.relabeled_clone(change_map) for key, col in self.annotations.items()
844         }
845 
846         # 2. Rename the alias in the internal table/alias datastructures.
847         for old_alias, new_alias in change_map.items():
848             if old_alias not in self.alias_map:
849                 continue
850             alias_data = self.alias_map[old_alias].relabeled_clone(change_map)
851             self.alias_map[new_alias] = alias_data
852             self.alias_refcount[new_alias] = self.alias_refcount[old_alias]
853             del self.alias_refcount[old_alias]
854             del self.alias_map[old_alias]
855 
856             table_aliases = self.table_map[alias_data.table_name]
857             for pos, alias in enumerate(table_aliases):
858                 if alias == old_alias:
859                     table_aliases[pos] = new_alias
860                     break
861         self.external_aliases = {
862             # Table is aliased or it's being changed and thus is aliased.
863             change_map.get(alias, alias): (aliased or alias in change_map)
864             for alias, aliased in self.external_aliases.items()
865         }
866 
867     def bump_prefix(self, outer_query):
868         """
869         Change the alias prefix to the next letter in the alphabet in a way
870         that the outer query's aliases and this query's aliases will not
871         conflict. Even tables that previously had no alias will get an alias
872         after this call.
873         """
874         def prefix_gen():
875             """
876             Generate a sequence of characters in alphabetical order:
877                 -> 'A', 'B', 'C', ...
878 
879             When the alphabet is finished, the sequence will continue with the
880             Cartesian product:
881                 -> 'AA', 'AB', 'AC', ...
882             """
883             alphabet = ascii_uppercase
884             prefix = chr(ord(self.alias_prefix) + 1)
885             yield prefix
886             for n in count(1):
887                 seq = alphabet[alphabet.index(prefix):] if prefix else alphabet
888                 for s in product(seq, repeat=n):
889                     yield ''.join(s)
890                 prefix = None
891 
892         if self.alias_prefix != outer_query.alias_prefix:
893             # No clashes between self and outer query should be possible.
894             return
895 
896         # Explicitly avoid infinite loop. The constant divider is based on how
897         # much depth recursive subquery references add to the stack. This value
898         # might need to be adjusted when adding or removing function calls from
899         # the code path in charge of performing these operations.
900         local_recursion_limit = sys.getrecursionlimit() // 16
901         for pos, prefix in enumerate(prefix_gen()):
902             if prefix not in self.subq_aliases:
903                 self.alias_prefix = prefix
904                 break
905             if pos > local_recursion_limit:
906                 raise RecursionError(
907                     'Maximum recursion depth exceeded: too many subqueries.'
908                 )
909         self.subq_aliases = self.subq_aliases.union([self.alias_prefix])
910         outer_query.subq_aliases = outer_query.subq_aliases.union(self.subq_aliases)
911         self.change_aliases({
912             alias: '%s%d' % (self.alias_prefix, pos)
913             for pos, alias in enumerate(self.alias_map)
914         })
915 
916     def get_initial_alias(self):
917         """
918         Return the first alias for this query, after increasing its reference
919         count.
920         """
921         if self.alias_map:
922             alias = self.base_table
923             self.ref_alias(alias)
924         else:
925             alias = self.join(BaseTable(self.get_meta().db_table, None))
926         return alias
927 
928     def count_active_tables(self):
929         """
930         Return the number of tables in this query with a non-zero reference
931         count. After execution, the reference counts are zeroed, so tables
932         added in compiler will not be seen by this method.
933         """
934         return len([1 for count in self.alias_refcount.values() if count])
935 
936     def join(self, join, reuse=None, reuse_with_filtered_relation=False):
937         """
938         Return an alias for the 'join', either reusing an existing alias for
939         that join or creating a new one. 'join' is either a
940         sql.datastructures.BaseTable or Join.
941 
942         The 'reuse' parameter can be either None which means all joins are
943         reusable, or it can be a set containing the aliases that can be reused.
944 
945         The 'reuse_with_filtered_relation' parameter is used when computing
946         FilteredRelation instances.
947 
948         A join is always created as LOUTER if the lhs alias is LOUTER to make
949         sure chains like t1 LOUTER t2 INNER t3 aren't generated. All new
950         joins are created as LOUTER if the join is nullable.
951         """
952         if reuse_with_filtered_relation and reuse:
953             reuse_aliases = [
954                 a for a, j in self.alias_map.items()
955                 if a in reuse and j.equals(join, with_filtered_relation=False)
956             ]
957         else:
958             reuse_aliases = [
959                 a for a, j in self.alias_map.items()
960                 if (reuse is None or a in reuse) and j == join
961             ]
962         if reuse_aliases:
963             if join.table_alias in reuse_aliases:
964                 reuse_alias = join.table_alias
965             else:
966                 # Reuse the most recent alias of the joined table
967                 # (a many-to-many relation may be joined multiple times).
968                 reuse_alias = reuse_aliases[-1]
969             self.ref_alias(reuse_alias)
970             return reuse_alias
971 
972         # No reuse is possible, so we need a new alias.
973         alias, _ = self.table_alias(join.table_name, create=True, filtered_relation=join.filtered_relation)
974         if join.join_type:
975             if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
976                 join_type = LOUTER
977             else:
978                 join_type = INNER
979             join.join_type = join_type
980         join.table_alias = alias
981         self.alias_map[alias] = join
982         return alias
983 
984     def join_parent_model(self, opts, model, alias, seen):
985         """
986         Make sure the given 'model' is joined in the query. If 'model' isn't
987         a parent of 'opts' or if it is None this method is a no-op.
988 
989         The 'alias' is the root alias for starting the join, 'seen' is a dict
990         of model -> alias of existing joins. It must also contain a mapping
991         of None -> some alias. This will be returned in the no-op case.
992         """
993         if model in seen:
994             return seen[model]
995         chain = opts.get_base_chain(model)
996         if not chain:
997             return alias
998         curr_opts = opts
999         for int_model in chain:
1000             if int_model in seen:
1001                 curr_opts = int_model._meta
1002                 alias = seen[int_model]
1003                 continue
1004             # Proxy model have elements in base chain
1005             # with no parents, assign the new options
1006             # object and skip to the next base in that
1007             # case
1008             if not curr_opts.parents[int_model]:
1009                 curr_opts = int_model._meta
1010                 continue
1011             link_field = curr_opts.get_ancestor_link(int_model)
1012             join_info = self.setup_joins([link_field.name], curr_opts, alias)
1013             curr_opts = int_model._meta
1014             alias = seen[int_model] = join_info.joins[-1]
1015         return alias or seen[None]
1016 
1017     def add_annotation(self, annotation, alias, is_summary=False):
1018         """Add a single annotation expression to the Query."""
1019         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,
1020                                                    summarize=is_summary)
1021         self.append_annotation_mask([alias])
1022         self.annotations[alias] = annotation
1023 
1024     def resolve_expression(self, query, *args, **kwargs):
1025         clone = self.clone()
1026         # Subqueries need to use a different set of aliases than the outer query.
1027         clone.bump_prefix(query)
1028         clone.subquery = True
1029         # It's safe to drop ordering if the queryset isn't using slicing,
1030         # distinct(*fields) or select_for_update().
1031         if (self.low_mark == 0 and self.high_mark is None and
1032                 not self.distinct_fields and
1033                 not self.select_for_update):
1034             clone.clear_ordering(True)
1035         clone.where.resolve_expression(query, *args, **kwargs)
1036         for key, value in clone.annotations.items():
1037             resolved = value.resolve_expression(query, *args, **kwargs)
1038             if hasattr(resolved, 'external_aliases'):
1039                 resolved.external_aliases.update(clone.external_aliases)
1040             clone.annotations[key] = resolved
1041         # Outer query's aliases are considered external.
1042         for alias, table in query.alias_map.items():
1043             clone.external_aliases[alias] = (
1044                 (isinstance(table, Join) and table.join_field.related_model._meta.db_table != alias) or
1045                 (isinstance(table, BaseTable) and table.table_name != table.table_alias)
1046             )
1047         return clone
1048 
1049     def get_external_cols(self):
1050         exprs = chain(self.annotations.values(), self.where.children)
1051         return [
1052             col for col in self._gen_cols(exprs)
1053             if col.alias in self.external_aliases
1054         ]
1055 
1056     def as_sql(self, compiler, connection):
1057         sql, params = self.get_compiler(connection=connection).as_sql()
1058         if self.subquery:
1059             sql = '(%s)' % sql
1060         return sql, params
1061 
1062     def resolve_lookup_value(self, value, can_reuse, allow_joins):
1063         if hasattr(value, 'resolve_expression'):
1064             value = value.resolve_expression(
1065                 self, reuse=can_reuse, allow_joins=allow_joins,
1066             )
1067         elif isinstance(value, (list, tuple)):
1068             # The items of the iterable may be expressions and therefore need
1069             # to be resolved independently.
1070             return type(value)(
1071                 self.resolve_lookup_value(sub_value, can_reuse, allow_joins)
1072                 for sub_value in value
1073             )
1074         return value
1075 
1076     def solve_lookup_type(self, lookup):
1077         """
1078         Solve the lookup type from the lookup (e.g.: 'foobar__id__icontains').
1079         """
1080         lookup_splitted = lookup.split(LOOKUP_SEP)
1081         if self.annotations:
1082             expression, expression_lookups = refs_expression(lookup_splitted, self.annotations)
1083             if expression:
1084                 return expression_lookups, (), expression
1085         _, field, _, lookup_parts = self.names_to_path(lookup_splitted, self.get_meta())
1086         field_parts = lookup_splitted[0:len(lookup_splitted) - len(lookup_parts)]
1087         if len(lookup_parts) > 1 and not field_parts:
1088             raise FieldError(
1089                 'Invalid lookup "%s" for model %s".' %
1090                 (lookup, self.get_meta().model.__name__)
1091             )
1092         return lookup_parts, field_parts, False
1093 
1094     def check_query_object_type(self, value, opts, field):
1095         """
1096         Check whether the object passed while querying is of the correct type.
1097         If not, raise a ValueError specifying the wrong object.
1098         """
1099         if hasattr(value, '_meta'):
1100             if not check_rel_lookup_compatibility(value._meta.model, opts, field):
1101                 raise ValueError(
1102                     'Cannot query "%s": Must be "%s" instance.' %
1103                     (value, opts.object_name))
1104 
1105     def check_related_objects(self, field, value, opts):
1106         """Check the type of object passed to query relations."""
1107         if field.is_relation:
1108             # Check that the field and the queryset use the same model in a
1109             # query like .filter(author=Author.objects.all()). For example, the
1110             # opts would be Author's (from the author field) and value.model
1111             # would be Author.objects.all() queryset's .model (Author also).
1112             # The field is the related field on the lhs side.
1113             if (isinstance(value, Query) and not value.has_select_fields and
1114                     not check_rel_lookup_compatibility(value.model, opts, field)):
1115                 raise ValueError(
1116                     'Cannot use QuerySet for "%s": Use a QuerySet for "%s".' %
1117                     (value.model._meta.object_name, opts.object_name)
1118                 )
1119             elif hasattr(value, '_meta'):
1120                 self.check_query_object_type(value, opts, field)
1121             elif hasattr(value, '__iter__'):
1122                 for v in value:
1123                     self.check_query_object_type(v, opts, field)
1124 
1125     def check_filterable(self, expression):
1126         """Raise an error if expression cannot be used in a WHERE clause."""
1127         if (
1128             hasattr(expression, 'resolve_expression') and
1129             not getattr(expression, 'filterable', True)
1130         ):
1131             raise NotSupportedError(
1132                 expression.__class__.__name__ + ' is disallowed in the filter '
1133                 'clause.'
1134             )
1135         if hasattr(expression, 'get_source_expressions'):
1136             for expr in expression.get_source_expressions():
1137                 self.check_filterable(expr)
1138 
1139     def build_lookup(self, lookups, lhs, rhs):
1140         """
1141         Try to extract transforms and lookup from given lhs.
1142 
1143         The lhs value is something that works like SQLExpression.
1144         The rhs value is what the lookup is going to compare against.
1145         The lookups is a list of names to extract using get_lookup()
1146         and get_transform().
1147         """
1148         # __exact is the default lookup if one isn't given.
1149         *transforms, lookup_name = lookups or ['exact']
1150         for name in transforms:
1151             lhs = self.try_transform(lhs, name)
1152         # First try get_lookup() so that the lookup takes precedence if the lhs
1153         # supports both transform and lookup for the name.
1154         lookup_class = lhs.get_lookup(lookup_name)
1155         if not lookup_class:
1156             if lhs.field.is_relation:
1157                 raise FieldError('Related Field got invalid lookup: {}'.format(lookup_name))
1158             # A lookup wasn't found. Try to interpret the name as a transform
1159             # and do an Exact lookup against it.
1160             lhs = self.try_transform(lhs, lookup_name)
1161             lookup_name = 'exact'
1162             lookup_class = lhs.get_lookup(lookup_name)
1163             if not lookup_class:
1164                 return
1165 
1166         lookup = lookup_class(lhs, rhs)
1167         # Interpret '__exact=None' as the sql 'is NULL'; otherwise, reject all
1168         # uses of None as a query value unless the lookup supports it.
1169         if lookup.rhs is None and not lookup.can_use_none_as_rhs:
1170             if lookup_name not in ('exact', 'iexact'):
1171                 raise ValueError("Cannot use None as a query value")
1172             return lhs.get_lookup('isnull')(lhs, True)
1173 
1174         # For Oracle '' is equivalent to null. The check must be done at this
1175         # stage because join promotion can't be done in the compiler. Using
1176         # DEFAULT_DB_ALIAS isn't nice but it's the best that can be done here.
1177         # A similar thing is done in is_nullable(), too.
1178         if (connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
1179                 lookup_name == 'exact' and lookup.rhs == ''):
1180             return lhs.get_lookup('isnull')(lhs, True)
1181 
1182         return lookup
1183 
1184     def try_transform(self, lhs, name):
1185         """
1186         Helper method for build_lookup(). Try to fetch and initialize
1187         a transform for name parameter from lhs.
1188         """
1189         transform_class = lhs.get_transform(name)
1190         if transform_class:
1191             return transform_class(lhs)
1192         else:
1193             output_field = lhs.output_field.__class__
1194             suggested_lookups = difflib.get_close_matches(name, output_field.get_lookups())
1195             if suggested_lookups:
1196                 suggestion = ', perhaps you meant %s?' % ' or '.join(suggested_lookups)
1197             else:
1198                 suggestion = '.'
1199             raise FieldError(
1200                 "Unsupported lookup '%s' for %s or join on the field not "
1201                 "permitted%s" % (name, output_field.__name__, suggestion)
1202             )
1203 
1204     def build_filter(self, filter_expr, branch_negated=False, current_negated=False,
1205                      can_reuse=None, allow_joins=True, split_subq=True,
1206                      reuse_with_filtered_relation=False, check_filterable=True):
1207         """
1208         Build a WhereNode for a single filter clause but don't add it
1209         to this Query. Query.add_q() will then add this filter to the where
1210         Node.
1211 
1212         The 'branch_negated' tells us if the current branch contains any
1213         negations. This will be used to determine if subqueries are needed.
1214 
1215         The 'current_negated' is used to determine if the current filter is
1216         negated or not and this will be used to determine if IS NULL filtering
1217         is needed.
1218 
1219         The difference between current_negated and branch_negated is that
1220         branch_negated is set on first negation, but current_negated is
1221         flipped for each negation.
1222 
1223         Note that add_filter will not do any negating itself, that is done
1224         upper in the code by add_q().
1225 
1226         The 'can_reuse' is a set of reusable joins for multijoins.
1227 
1228         If 'reuse_with_filtered_relation' is True, then only joins in can_reuse
1229         will be reused.
1230 
1231         The method will create a filter clause that can be added to the current
1232         query. However, if the filter isn't added to the query then the caller
1233         is responsible for unreffing the joins used.
1234         """
1235         if isinstance(filter_expr, dict):
1236             raise FieldError("Cannot parse keyword query as dict")
1237         if isinstance(filter_expr, Q):
1238             return self._add_q(
1239                 filter_expr,
1240                 branch_negated=branch_negated,
1241                 current_negated=current_negated,
1242                 used_aliases=can_reuse,
1243                 allow_joins=allow_joins,
1244                 split_subq=split_subq,
1245                 check_filterable=check_filterable,
1246             )
1247         if hasattr(filter_expr, 'resolve_expression'):
1248             if not getattr(filter_expr, 'conditional', False):
1249                 raise TypeError('Cannot filter against a non-conditional expression.')
1250             condition = self.build_lookup(
1251                 ['exact'], filter_expr.resolve_expression(self, allow_joins=allow_joins), True
1252             )
1253             clause = self.where_class()
1254             clause.add(condition, AND)
1255             return clause, []
1256         arg, value = filter_expr
1257         if not arg:
1258             raise FieldError("Cannot parse keyword query %r" % arg)
1259         lookups, parts, reffed_expression = self.solve_lookup_type(arg)
1260 
1261         if check_filterable:
1262             self.check_filterable(reffed_expression)
1263 
1264         if not allow_joins and len(parts) > 1:
1265             raise FieldError("Joined field references are not permitted in this query")
1266 
1267         pre_joins = self.alias_refcount.copy()
1268         value = self.resolve_lookup_value(value, can_reuse, allow_joins)
1269         used_joins = {k for k, v in self.alias_refcount.items() if v > pre_joins.get(k, 0)}
1270 
1271         if check_filterable:
1272             self.check_filterable(value)
1273 
1274         clause = self.where_class()
1275         if reffed_expression:
1276             condition = self.build_lookup(lookups, reffed_expression, value)
1277             clause.add(condition, AND)
1278             return clause, []
1279 
1280         opts = self.get_meta()
1281         alias = self.get_initial_alias()
1282         allow_many = not branch_negated or not split_subq
1283 
1284         try:
1285             join_info = self.setup_joins(
1286                 parts, opts, alias, can_reuse=can_reuse, allow_many=allow_many,
1287                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1288             )
1289 
1290             # Prevent iterator from being consumed by check_related_objects()
1291             if isinstance(value, Iterator):
1292                 value = list(value)
1293             self.check_related_objects(join_info.final_field, value, join_info.opts)
1294 
1295             # split_exclude() needs to know which joins were generated for the
1296             # lookup parts
1297             self._lookup_joins = join_info.joins
1298         except MultiJoin as e:
1299             return self.split_exclude(filter_expr, can_reuse, e.names_with_path)
1300 
1301         # Update used_joins before trimming since they are reused to determine
1302         # which joins could be later promoted to INNER.
1303         used_joins.update(join_info.joins)
1304         targets, alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1305         if can_reuse is not None:
1306             can_reuse.update(join_list)
1307 
1308         if join_info.final_field.is_relation:
1309             # No support for transforms for relational fields
1310             num_lookups = len(lookups)
1311             if num_lookups > 1:
1312                 raise FieldError('Related Field got invalid lookup: {}'.format(lookups[0]))
1313             if len(targets) == 1:
1314                 col = self._get_col(targets[0], join_info.final_field, alias)
1315             else:
1316                 col = MultiColSource(alias, targets, join_info.targets, join_info.final_field)
1317         else:
1318             col = self._get_col(targets[0], join_info.final_field, alias)
1319 
1320         condition = self.build_lookup(lookups, col, value)
1321         lookup_type = condition.lookup_name
1322         clause.add(condition, AND)
1323 
1324         require_outer = lookup_type == 'isnull' and condition.rhs is True and not current_negated
1325         if current_negated and (lookup_type != 'isnull' or condition.rhs is False) and condition.rhs is not None:
1326             require_outer = True
1327             if (lookup_type != 'isnull' and (
1328                     self.is_nullable(targets[0]) or
1329                     self.alias_map[join_list[-1]].join_type == LOUTER)):
1330                 # The condition added here will be SQL like this:
1331                 # NOT (col IS NOT NULL), where the first NOT is added in
1332                 # upper layers of code. The reason for addition is that if col
1333                 # is null, then col != someval will result in SQL "unknown"
1334                 # which isn't the same as in Python. The Python None handling
1335                 # is wanted, and it can be gotten by
1336                 # (col IS NULL OR col != someval)
1337                 #   <=>
1338                 # NOT (col IS NOT NULL AND col = someval).
1339                 lookup_class = targets[0].get_lookup('isnull')
1340                 col = self._get_col(targets[0], join_info.targets[0], alias)
1341                 clause.add(lookup_class(col, False), AND)
1342         return clause, used_joins if not require_outer else ()
1343 
1344     def add_filter(self, filter_clause):
1345         self.add_q(Q(**{filter_clause[0]: filter_clause[1]}))
1346 
1347     def add_q(self, q_object):
1348         """
1349         A preprocessor for the internal _add_q(). Responsible for doing final
1350         join promotion.
1351         """
1352         # For join promotion this case is doing an AND for the added q_object
1353         # and existing conditions. So, any existing inner join forces the join
1354         # type to remain inner. Existing outer joins can however be demoted.
1355         # (Consider case where rel_a is LOUTER and rel_a__col=1 is added - if
1356         # rel_a doesn't produce any rows, then the whole condition must fail.
1357         # So, demotion is OK.
1358         existing_inner = {a for a in self.alias_map if self.alias_map[a].join_type == INNER}
1359         clause, _ = self._add_q(q_object, self.used_aliases)
1360         if clause:
1361             self.where.add(clause, AND)
1362         self.demote_joins(existing_inner)
1363 
1364     def build_where(self, filter_expr):
1365         return self.build_filter(filter_expr, allow_joins=False)[0]
1366 
1367     def _add_q(self, q_object, used_aliases, branch_negated=False,
1368                current_negated=False, allow_joins=True, split_subq=True,
1369                check_filterable=True):
1370         """Add a Q-object to the current filter."""
1371         connector = q_object.connector
1372         current_negated = current_negated ^ q_object.negated
1373         branch_negated = branch_negated or q_object.negated
1374         target_clause = self.where_class(connector=connector,
1375                                          negated=q_object.negated)
1376         joinpromoter = JoinPromoter(q_object.connector, len(q_object.children), current_negated)
1377         for child in q_object.children:
1378             child_clause, needed_inner = self.build_filter(
1379                 child, can_reuse=used_aliases, branch_negated=branch_negated,
1380                 current_negated=current_negated, allow_joins=allow_joins,
1381                 split_subq=split_subq, check_filterable=check_filterable,
1382             )
1383             joinpromoter.add_votes(needed_inner)
1384             if child_clause:
1385                 target_clause.add(child_clause, connector)
1386         needed_inner = joinpromoter.update_join_types(self)
1387         return target_clause, needed_inner
1388 
1389     def build_filtered_relation_q(self, q_object, reuse, branch_negated=False, current_negated=False):
1390         """Add a FilteredRelation object to the current filter."""
1391         connector = q_object.connector
1392         current_negated ^= q_object.negated
1393         branch_negated = branch_negated or q_object.negated
1394         target_clause = self.where_class(connector=connector, negated=q_object.negated)
1395         for child in q_object.children:
1396             if isinstance(child, Node):
1397                 child_clause = self.build_filtered_relation_q(
1398                     child, reuse=reuse, branch_negated=branch_negated,
1399                     current_negated=current_negated,
1400                 )
1401             else:
1402                 child_clause, _ = self.build_filter(
1403                     child, can_reuse=reuse, branch_negated=branch_negated,
1404                     current_negated=current_negated,
1405                     allow_joins=True, split_subq=False,
1406                     reuse_with_filtered_relation=True,
1407                 )
1408             target_clause.add(child_clause, connector)
1409         return target_clause
1410 
1411     def add_filtered_relation(self, filtered_relation, alias):
1412         filtered_relation.alias = alias
1413         lookups = dict(get_children_from_q(filtered_relation.condition))
1414         for lookup in chain((filtered_relation.relation_name,), lookups):
1415             lookup_parts, field_parts, _ = self.solve_lookup_type(lookup)
1416             shift = 2 if not lookup_parts else 1
1417             if len(field_parts) > (shift + len(lookup_parts)):
1418                 raise ValueError(
1419                     "FilteredRelation's condition doesn't support nested "
1420                     "relations (got %r)." % lookup
1421                 )
1422         self._filtered_relations[filtered_relation.alias] = filtered_relation
1423 
1424     def names_to_path(self, names, opts, allow_many=True, fail_on_missing=False):
1425         """
1426         Walk the list of names and turns them into PathInfo tuples. A single
1427         name in 'names' can generate multiple PathInfos (m2m, for example).
1428 
1429         'names' is the path of names to travel, 'opts' is the model Options we
1430         start the name resolving from, 'allow_many' is as for setup_joins().
1431         If fail_on_missing is set to True, then a name that can't be resolved
1432         will generate a FieldError.
1433 
1434         Return a list of PathInfo tuples. In addition return the final field
1435         (the last used join field) and target (which is a field guaranteed to
1436         contain the same value as the final field). Finally, return those names
1437         that weren't found (which are likely transforms and the final lookup).
1438         """
1439         path, names_with_path = [], []
1440         for pos, name in enumerate(names):
1441             cur_names_with_path = (name, [])
1442             if name == 'pk':
1443                 name = opts.pk.name
1444 
1445             field = None
1446             filtered_relation = None
1447             try:
1448                 field = opts.get_field(name)
1449             except FieldDoesNotExist:
1450                 if name in self.annotation_select:
1451                     field = self.annotation_select[name].output_field
1452                 elif name in self._filtered_relations and pos == 0:
1453                     filtered_relation = self._filtered_relations[name]
1454                     field = opts.get_field(filtered_relation.relation_name)
1455             if field is not None:
1456                 # Fields that contain one-to-many relations with a generic
1457                 # model (like a GenericForeignKey) cannot generate reverse
1458                 # relations and therefore cannot be used for reverse querying.
1459                 if field.is_relation and not field.related_model:
1460                     raise FieldError(
1461                         "Field %r does not generate an automatic reverse "
1462                         "relation and therefore cannot be used for reverse "
1463                         "querying. If it is a GenericForeignKey, consider "
1464                         "adding a GenericRelation." % name
1465                     )
1466                 try:
1467                     model = field.model._meta.concrete_model
1468                 except AttributeError:
1469                     # QuerySet.annotate() may introduce fields that aren't
1470                     # attached to a model.
1471                     model = None
1472             else:
1473                 # We didn't find the current field, so move position back
1474                 # one step.
1475                 pos -= 1
1476                 if pos == -1 or fail_on_missing:
1477                     available = sorted([
1478                         *get_field_names_from_opts(opts),
1479                         *self.annotation_select,
1480                         *self._filtered_relations,
1481                     ])
1482                     raise FieldError("Cannot resolve keyword '%s' into field. "
1483                                      "Choices are: %s" % (name, ", ".join(available)))
1484                 break
1485             # Check if we need any joins for concrete inheritance cases (the
1486             # field lives in parent, but we are currently in one of its
1487             # children)
1488             if model is not opts.model:
1489                 path_to_parent = opts.get_path_to_parent(model)
1490                 if path_to_parent:
1491                     path.extend(path_to_parent)
1492                     cur_names_with_path[1].extend(path_to_parent)
1493                     opts = path_to_parent[-1].to_opts
1494             if hasattr(field, 'get_path_info'):
1495                 pathinfos = field.get_path_info(filtered_relation)
1496                 if not allow_many:
1497                     for inner_pos, p in enumerate(pathinfos):
1498                         if p.m2m:
1499                             cur_names_with_path[1].extend(pathinfos[0:inner_pos + 1])
1500                             names_with_path.append(cur_names_with_path)
1501                             raise MultiJoin(pos + 1, names_with_path)
1502                 last = pathinfos[-1]
1503                 path.extend(pathinfos)
1504                 final_field = last.join_field
1505                 opts = last.to_opts
1506                 targets = last.target_fields
1507                 cur_names_with_path[1].extend(pathinfos)
1508                 names_with_path.append(cur_names_with_path)
1509             else:
1510                 # Local non-relational field.
1511                 final_field = field
1512                 targets = (field,)
1513                 if fail_on_missing and pos + 1 != len(names):
1514                     raise FieldError(
1515                         "Cannot resolve keyword %r into field. Join on '%s'"
1516                         " not permitted." % (names[pos + 1], name))
1517                 break
1518         return path, final_field, targets, names[pos + 1:]
1519 
1520     def setup_joins(self, names, opts, alias, can_reuse=None, allow_many=True,
1521                     reuse_with_filtered_relation=False):
1522         """
1523         Compute the necessary table joins for the passage through the fields
1524         given in 'names'. 'opts' is the Options class for the current model
1525         (which gives the table we are starting from), 'alias' is the alias for
1526         the table to start the joining from.
1527 
1528         The 'can_reuse' defines the reverse foreign key joins we can reuse. It
1529         can be None in which case all joins are reusable or a set of aliases
1530         that can be reused. Note that non-reverse foreign keys are always
1531         reusable when using setup_joins().
1532 
1533         The 'reuse_with_filtered_relation' can be used to force 'can_reuse'
1534         parameter and force the relation on the given connections.
1535 
1536         If 'allow_many' is False, then any reverse foreign key seen will
1537         generate a MultiJoin exception.
1538 
1539         Return the final field involved in the joins, the target field (used
1540         for any 'where' constraint), the final 'opts' value, the joins, the
1541         field path traveled to generate the joins, and a transform function
1542         that takes a field and alias and is equivalent to `field.get_col(alias)`
1543         in the simple case but wraps field transforms if they were included in
1544         names.
1545 
1546         The target field is the field containing the concrete value. Final
1547         field can be something different, for example foreign key pointing to
1548         that value. Final field is needed for example in some value
1549         conversions (convert 'obj' in fk__id=obj to pk val using the foreign
1550         key field for example).
1551         """
1552         joins = [alias]
1553         # The transform can't be applied yet, as joins must be trimmed later.
1554         # To avoid making every caller of this method look up transforms
1555         # directly, compute transforms here and create a partial that converts
1556         # fields to the appropriate wrapped version.
1557 
1558         def final_transformer(field, alias):
1559             return field.get_col(alias)
1560 
1561         # Try resolving all the names as fields first. If there's an error,
1562         # treat trailing names as lookups until a field can be resolved.
1563         last_field_exception = None
1564         for pivot in range(len(names), 0, -1):
1565             try:
1566                 path, final_field, targets, rest = self.names_to_path(
1567                     names[:pivot], opts, allow_many, fail_on_missing=True,
1568                 )
1569             except FieldError as exc:
1570                 if pivot == 1:
1571                     # The first item cannot be a lookup, so it's safe
1572                     # to raise the field error here.
1573                     raise
1574                 else:
1575                     last_field_exception = exc
1576             else:
1577                 # The transforms are the remaining items that couldn't be
1578                 # resolved into fields.
1579                 transforms = names[pivot:]
1580                 break
1581         for name in transforms:
1582             def transform(field, alias, *, name, previous):
1583                 try:
1584                     wrapped = previous(field, alias)
1585                     return self.try_transform(wrapped, name)
1586                 except FieldError:
1587                     # FieldError is raised if the transform doesn't exist.
1588                     if isinstance(final_field, Field) and last_field_exception:
1589                         raise last_field_exception
1590                     else:
1591                         raise
1592             final_transformer = functools.partial(transform, name=name, previous=final_transformer)
1593         # Then, add the path to the query's joins. Note that we can't trim
1594         # joins at this stage - we will need the information about join type
1595         # of the trimmed joins.
1596         for join in path:
1597             if join.filtered_relation:
1598                 filtered_relation = join.filtered_relation.clone()
1599                 table_alias = filtered_relation.alias
1600             else:
1601                 filtered_relation = None
1602                 table_alias = None
1603             opts = join.to_opts
1604             if join.direct:
1605                 nullable = self.is_nullable(join.join_field)
1606             else:
1607                 nullable = True
1608             connection = Join(
1609                 opts.db_table, alias, table_alias, INNER, join.join_field,
1610                 nullable, filtered_relation=filtered_relation,
1611             )
1612             reuse = can_reuse if join.m2m or reuse_with_filtered_relation else None
1613             alias = self.join(
1614                 connection, reuse=reuse,
1615                 reuse_with_filtered_relation=reuse_with_filtered_relation,
1616             )
1617             joins.append(alias)
1618             if filtered_relation:
1619                 filtered_relation.path = joins[:]
1620         return JoinInfo(final_field, targets, opts, joins, path, final_transformer)
1621 
1622     def trim_joins(self, targets, joins, path):
1623         """
1624         The 'target' parameter is the final field being joined to, 'joins'
1625         is the full list of join aliases. The 'path' contain the PathInfos
1626         used to create the joins.
1627 
1628         Return the final target field and table alias and the new active
1629         joins.
1630 
1631         Always trim any direct join if the target column is already in the
1632         previous table. Can't trim reverse joins as it's unknown if there's
1633         anything on the other side of the join.
1634         """
1635         joins = joins[:]
1636         for pos, info in enumerate(reversed(path)):
1637             if len(joins) == 1 or not info.direct:
1638                 break
1639             if info.filtered_relation:
1640                 break
1641             join_targets = {t.column for t in info.join_field.foreign_related_fields}
1642             cur_targets = {t.column for t in targets}
1643             if not cur_targets.issubset(join_targets):
1644                 break
1645             targets_dict = {r[1].column: r[0] for r in info.join_field.related_fields if r[1].column in cur_targets}
1646             targets = tuple(targets_dict[t.column] for t in targets)
1647             self.unref_alias(joins.pop())
1648         return targets, joins[-1], joins
1649 
1650     @classmethod
1651     def _gen_cols(cls, exprs):
1652         for expr in exprs:
1653             if isinstance(expr, Col):
1654                 yield expr
1655             else:
1656                 yield from cls._gen_cols(expr.get_source_expressions())
1657 
1658     @classmethod
1659     def _gen_col_aliases(cls, exprs):
1660         yield from (expr.alias for expr in cls._gen_cols(exprs))
1661 
1662     def resolve_ref(self, name, allow_joins=True, reuse=None, summarize=False):
1663         if not allow_joins and LOOKUP_SEP in name:
1664             raise FieldError("Joined field references are not permitted in this query")
1665         annotation = self.annotations.get(name)
1666         if annotation is not None:
1667             if not allow_joins:
1668                 for alias in self._gen_col_aliases([annotation]):
1669                     if isinstance(self.alias_map[alias], Join):
1670                         raise FieldError(
1671                             'Joined field references are not permitted in '
1672                             'this query'
1673                         )
1674             if summarize:
1675                 # Summarize currently means we are doing an aggregate() query
1676                 # which is executed as a wrapped subquery if any of the
1677                 # aggregate() elements reference an existing annotation. In
1678                 # that case we need to return a Ref to the subquery's annotation.
1679                 return Ref(name, self.annotation_select[name])
1680             else:
1681                 return annotation
1682         else:
1683             field_list = name.split(LOOKUP_SEP)
1684             join_info = self.setup_joins(field_list, self.get_meta(), self.get_initial_alias(), can_reuse=reuse)
1685             targets, final_alias, join_list = self.trim_joins(join_info.targets, join_info.joins, join_info.path)
1686             if not allow_joins and len(join_list) > 1:
1687                 raise FieldError('Joined field references are not permitted in this query')
1688             if len(targets) > 1:
1689                 raise FieldError("Referencing multicolumn fields with F() objects "
1690                                  "isn't supported")
1691             # Verify that the last lookup in name is a field or a transform:
1692             # transform_function() raises FieldError if not.
1693             join_info.transform_function(targets[0], final_alias)
1694             if reuse is not None:
1695                 reuse.update(join_list)
1696             return self._get_col(targets[0], join_info.targets[0], join_list[-1])
1697 
1698     def split_exclude(self, filter_expr, can_reuse, names_with_path):
1699         """
1700         When doing an exclude against any kind of N-to-many relation, we need
1701         to use a subquery. This method constructs the nested query, given the
1702         original exclude filter (filter_expr) and the portion up to the first
1703         N-to-many relation field.
1704 
1705         For example, if the origin filter is ~Q(child__name='foo'), filter_expr
1706         is ('child__name', 'foo') and can_reuse is a set of joins usable for
1707         filters in the original query.
1708 
1709         We will turn this into equivalent of:
1710             WHERE NOT (pk IN (SELECT parent_id FROM thetable
1711                               WHERE name = 'foo' AND parent_id IS NOT NULL))
1712 
1713         It might be worth it to consider using WHERE NOT EXISTS as that has
1714         saner null handling, and is easier for the backend's optimizer to
1715         handle.
1716         """
1717         filter_lhs, filter_rhs = filter_expr
1718         if isinstance(filter_rhs, OuterRef):
1719             filter_expr = (filter_lhs, OuterRef(filter_rhs))
1720         elif isinstance(filter_rhs, F):
1721             filter_expr = (filter_lhs, OuterRef(filter_rhs.name))
1722         # Generate the inner query.
1723         query = Query(self.model)
1724         query._filtered_relations = self._filtered_relations
1725         query.add_filter(filter_expr)
1726         query.clear_ordering(True)
1727         # Try to have as simple as possible subquery -> trim leading joins from
1728         # the subquery.
1729         trimmed_prefix, contains_louter = query.trim_start(names_with_path)
1730 
1731         # Add extra check to make sure the selected field will not be null
1732         # since we are adding an IN <subquery> clause. This prevents the
1733         # database from tripping over IN (...,NULL,...) selects and returning
1734         # nothing
1735         col = query.select[0]
1736         select_field = col.target
1737         alias = col.alias
1738         if self.is_nullable(select_field):
1739             lookup_class = select_field.get_lookup('isnull')
1740             lookup = lookup_class(select_field.get_col(alias), False)
1741             query.where.add(lookup, AND)
1742         if alias in can_reuse:
1743             pk = select_field.model._meta.pk
1744             # Need to add a restriction so that outer query's filters are in effect for
1745             # the subquery, too.
1746             query.bump_prefix(self)
1747             lookup_class = select_field.get_lookup('exact')
1748             # Note that the query.select[0].alias is different from alias
1749             # due to bump_prefix above.
1750             lookup = lookup_class(pk.get_col(query.select[0].alias),
1751                                   pk.get_col(alias))
1752             query.where.add(lookup, AND)
1753             query.external_aliases[alias] = True
1754 
1755         condition, needed_inner = self.build_filter(
1756             ('%s__in' % trimmed_prefix, query),
1757             current_negated=True, branch_negated=True, can_reuse=can_reuse)
1758         if contains_louter:
1759             or_null_condition, _ = self.build_filter(
1760                 ('%s__isnull' % trimmed_prefix, True),
1761                 current_negated=True, branch_negated=True, can_reuse=can_reuse)
1762             condition.add(or_null_condition, OR)
1763             # Note that the end result will be:
1764             # (outercol NOT IN innerq AND outercol IS NOT NULL) OR outercol IS NULL.
1765             # This might look crazy but due to how IN works, this seems to be
1766             # correct. If the IS NOT NULL check is removed then outercol NOT
1767             # IN will return UNKNOWN. If the IS NULL check is removed, then if
1768             # outercol IS NULL we will not match the row.
1769         return condition, needed_inner
1770 
1771     def set_empty(self):
1772         self.where.add(NothingNode(), AND)
1773 
1774     def is_empty(self):
1775         return any(isinstance(c, NothingNode) for c in self.where.children)
1776 
1777     def set_limits(self, low=None, high=None):
1778         """
1779         Adjust the limits on the rows retrieved. Use low/high to set these,
1780         as it makes it more Pythonic to read and write. When the SQL query is
1781         created, convert them to the appropriate offset and limit values.
1782 
1783         Apply any limits passed in here to the existing constraints. Add low
1784         to the current low value and clamp both to any existing high value.
1785         """
1786         if high is not None:
1787             if self.high_mark is not None:
1788                 self.high_mark = min(self.high_mark, self.low_mark + high)
1789             else:
1790                 self.high_mark = self.low_mark + high
1791         if low is not None:
1792             if self.high_mark is not None:
1793                 self.low_mark = min(self.high_mark, self.low_mark + low)
1794             else:
1795                 self.low_mark = self.low_mark + low
1796 
1797         if self.low_mark == self.high_mark:
1798             self.set_empty()
1799 
1800     def clear_limits(self):
1801         """Clear any existing limits."""
1802         self.low_mark, self.high_mark = 0, None
1803 
1804     @property
1805     def is_sliced(self):
1806         return self.low_mark != 0 or self.high_mark is not None
1807 
1808     def has_limit_one(self):
1809         return self.high_mark is not None and (self.high_mark - self.low_mark) == 1
1810 
1811     def can_filter(self):
1812         """
1813         Return True if adding filters to this instance is still possible.
1814 
1815         Typically, this means no limits or offsets have been put on the results.
1816         """
1817         return not self.is_sliced
1818 
1819     def clear_select_clause(self):
1820         """Remove all fields from SELECT clause."""
1821         self.select = ()
1822         self.default_cols = False
1823         self.select_related = False
1824         self.set_extra_mask(())
1825         self.set_annotation_mask(())
1826 
1827     def clear_select_fields(self):
1828         """
1829         Clear the list of fields to select (but not extra_select columns).
1830         Some queryset types completely replace any existing list of select
1831         columns.
1832         """
1833         self.select = ()
1834         self.values_select = ()
1835 
1836     def add_select_col(self, col):
1837         self.select += col,
1838         self.values_select += col.output_field.name,
1839 
1840     def set_select(self, cols):
1841         self.default_cols = False
1842         self.select = tuple(cols)
1843 
1844     def add_distinct_fields(self, *field_names):
1845         """
1846         Add and resolve the given fields to the query's "distinct on" clause.
1847         """
1848         self.distinct_fields = field_names
1849         self.distinct = True
1850 
1851     def add_fields(self, field_names, allow_m2m=True):
1852         """
1853         Add the given (model) fields to the select set. Add the field names in
1854         the order specified.
1855         """
1856         alias = self.get_initial_alias()
1857         opts = self.get_meta()
1858 
1859         try:
1860             cols = []
1861             for name in field_names:
1862                 # Join promotion note - we must not remove any rows here, so
1863                 # if there is no existing joins, use outer join.
1864                 join_info = self.setup_joins(name.split(LOOKUP_SEP), opts, alias, allow_many=allow_m2m)
1865                 targets, final_alias, joins = self.trim_joins(
1866                     join_info.targets,
1867                     join_info.joins,
1868                     join_info.path,
1869                 )
1870                 for target in targets:
1871                     cols.append(join_info.transform_function(target, final_alias))
1872             if cols:
1873                 self.set_select(cols)
1874         except MultiJoin:
1875             raise FieldError("Invalid field name: '%s'" % name)
1876         except FieldError:
1877             if LOOKUP_SEP in name:
1878                 # For lookups spanning over relationships, show the error
1879                 # from the model on which the lookup failed.
1880                 raise
1881             else:
1882                 names = sorted([
1883                     *get_field_names_from_opts(opts), *self.extra,
1884                     *self.annotation_select, *self._filtered_relations
1885                 ])
1886                 raise FieldError("Cannot resolve keyword %r into field. "
1887                                  "Choices are: %s" % (name, ", ".join(names)))
1888 
1889     def add_ordering(self, *ordering):
1890         """
1891         Add items from the 'ordering' sequence to the query's "order by"
1892         clause. These items are either field names (not column names) --
1893         possibly with a direction prefix ('-' or '?') -- or OrderBy
1894         expressions.
1895 
1896         If 'ordering' is empty, clear all ordering from the query.
1897         """
1898         errors = []
1899         for item in ordering:
1900             if isinstance(item, str):
1901                 if '.' in item:
1902                     warnings.warn(
1903                         'Passing column raw column aliases to order_by() is '
1904                         'deprecated. Wrap %r in a RawSQL expression before '
1905                         'passing it to order_by().' % item,
1906                         category=RemovedInDjango40Warning,
1907                         stacklevel=3,
1908                     )
1909                     continue
1910                 if item == '?':
1911                     continue
1912                 if item.startswith('-'):
1913                     item = item[1:]
1914                 if item in self.annotations:
1915                     continue
1916                 if self.extra and item in self.extra:
1917                     continue
1918                 # names_to_path() validates the lookup. A descriptive
1919                 # FieldError will be raise if it's not.
1920                 self.names_to_path(item.split(LOOKUP_SEP), self.model._meta)
1921             elif not hasattr(item, 'resolve_expression'):
1922                 errors.append(item)
1923             if getattr(item, 'contains_aggregate', False):
1924                 raise FieldError(
1925                     'Using an aggregate in order_by() without also including '
1926                     'it in annotate() is not allowed: %s' % item
1927                 )
1928         if errors:
1929             raise FieldError('Invalid order_by arguments: %s' % errors)
1930         if ordering:
1931             self.order_by += ordering
1932         else:
1933             self.default_ordering = False
1934 
1935     def clear_ordering(self, force_empty):
1936         """
1937         Remove any ordering settings. If 'force_empty' is True, there will be
1938         no ordering in the resulting query (not even the model's default).
1939         """
1940         self.order_by = ()
1941         self.extra_order_by = ()
1942         if force_empty:
1943             self.default_ordering = False
1944 
1945     def set_group_by(self, allow_aliases=True):
1946         """
1947         Expand the GROUP BY clause required by the query.
1948 
1949         This will usually be the set of all non-aggregate fields in the
1950         return data. If the database backend supports grouping by the
1951         primary key, and the query would be equivalent, the optimization
1952         will be made automatically.
1953         """
1954         # Column names from JOINs to check collisions with aliases.
1955         if allow_aliases:
1956             column_names = set()
1957             seen_models = set()
1958             for join in list(self.alias_map.values())[1:]:  # Skip base table.
1959                 model = join.join_field.related_model
1960                 if model not in seen_models:
1961                     column_names.update({
1962                         field.column
1963                         for field in model._meta.local_concrete_fields
1964                     })
1965                     seen_models.add(model)
1966 
1967         group_by = list(self.select)
1968         if self.annotation_select:
1969             for alias, annotation in self.annotation_select.items():
1970                 signature = inspect.signature(annotation.get_group_by_cols)
1971                 if 'alias' not in signature.parameters:
1972                     annotation_class = annotation.__class__
1973                     msg = (
1974                         '`alias=None` must be added to the signature of '
1975                         '%s.%s.get_group_by_cols().'
1976                     ) % (annotation_class.__module__, annotation_class.__qualname__)
1977                     warnings.warn(msg, category=RemovedInDjango40Warning)
1978                     group_by_cols = annotation.get_group_by_cols()
1979                 else:
1980                     if not allow_aliases or alias in column_names:
1981                         alias = None
1982                     group_by_cols = annotation.get_group_by_cols(alias=alias)
1983                 group_by.extend(group_by_cols)
1984         self.group_by = tuple(group_by)
1985 
1986     def add_select_related(self, fields):
1987         """
1988         Set up the select_related data structure so that we only select
1989         certain related models (as opposed to all models, when
1990         self.select_related=True).
1991         """
1992         if isinstance(self.select_related, bool):
1993             field_dict = {}
1994         else:
1995             field_dict = self.select_related
1996         for field in fields:
1997             d = field_dict
1998             for part in field.split(LOOKUP_SEP):
1999                 d = d.setdefault(part, {})
2000         self.select_related = field_dict
2001 
2002     def add_extra(self, select, select_params, where, params, tables, order_by):
2003         """
2004         Add data to the various extra_* attributes for user-created additions
2005         to the query.
2006         """
2007         if select:
2008             # We need to pair any placeholder markers in the 'select'
2009             # dictionary with their parameters in 'select_params' so that
2010             # subsequent updates to the select dictionary also adjust the
2011             # parameters appropriately.
2012             select_pairs = {}
2013             if select_params:
2014                 param_iter = iter(select_params)
2015             else:
2016                 param_iter = iter([])
2017             for name, entry in select.items():
2018                 entry = str(entry)
2019                 entry_params = []
2020                 pos = entry.find("%s")
2021                 while pos != -1:
2022                     if pos == 0 or entry[pos - 1] != '%':
2023                         entry_params.append(next(param_iter))
2024                     pos = entry.find("%s", pos + 2)
2025                 select_pairs[name] = (entry, entry_params)
2026             self.extra.update(select_pairs)
2027         if where or params:
2028             self.where.add(ExtraWhere(where, params), AND)
2029         if tables:
2030             self.extra_tables += tuple(tables)
2031         if order_by:
2032             self.extra_order_by = order_by
2033 
2034     def clear_deferred_loading(self):
2035         """Remove any fields from the deferred loading set."""
2036         self.deferred_loading = (frozenset(), True)
2037 
2038     def add_deferred_loading(self, field_names):
2039         """
2040         Add the given list of model field names to the set of fields to
2041         exclude from loading from the database when automatic column selection
2042         is done. Add the new field names to any existing field names that
2043         are deferred (or removed from any existing field names that are marked
2044         as the only ones for immediate loading).
2045         """
2046         # Fields on related models are stored in the literal double-underscore
2047         # format, so that we can use a set datastructure. We do the foo__bar
2048         # splitting and handling when computing the SQL column names (as part of
2049         # get_columns()).
2050         existing, defer = self.deferred_loading
2051         if defer:
2052             # Add to existing deferred names.
2053             self.deferred_loading = existing.union(field_names), True
2054         else:
2055             # Remove names from the set of any existing "immediate load" names.
2056             self.deferred_loading = existing.difference(field_names), False
2057 
2058     def add_immediate_loading(self, field_names):
2059         """
2060         Add the given list of model field names to the set of fields to
2061         retrieve when the SQL is executed ("immediate loading" fields). The
2062         field names replace any existing immediate loading field names. If
2063         there are field names already specified for deferred loading, remove
2064         those names from the new field_names before storing the new names
2065         for immediate loading. (That is, immediate loading overrides any
2066         existing immediate values, but respects existing deferrals.)
2067         """
2068         existing, defer = self.deferred_loading
2069         field_names = set(field_names)
2070         if 'pk' in field_names:
2071             field_names.remove('pk')
2072             field_names.add(self.get_meta().pk.name)
2073 
2074         if defer:
2075             # Remove any existing deferred names from the current set before
2076             # setting the new names.
2077             self.deferred_loading = field_names.difference(existing), False
2078         else:
2079             # Replace any existing "immediate load" field names.
2080             self.deferred_loading = frozenset(field_names), False
2081 
2082     def get_loaded_field_names(self):
2083         """
2084         If any fields are marked to be deferred, return a dictionary mapping
2085         models to a set of names in those fields that will be loaded. If a
2086         model is not in the returned dictionary, none of its fields are
2087         deferred.
2088 
2089         If no fields are marked for deferral, return an empty dictionary.
2090         """
2091         # We cache this because we call this function multiple times
2092         # (compiler.fill_related_selections, query.iterator)
2093         try:
2094             return self._loaded_field_names_cache
2095         except AttributeError:
2096             collection = {}
2097             self.deferred_to_data(collection, self.get_loaded_field_names_cb)
2098             self._loaded_field_names_cache = collection
2099             return collection
2100 
2101     def get_loaded_field_names_cb(self, target, model, fields):
2102         """Callback used by get_deferred_field_names()."""
2103         target[model] = {f.attname for f in fields}
2104 
2105     def set_annotation_mask(self, names):
2106         """Set the mask of annotations that will be returned by the SELECT."""
2107         if names is None:
2108             self.annotation_select_mask = None
2109         else:
2110             self.annotation_select_mask = set(names)
2111         self._annotation_select_cache = None
2112 
2113     def append_annotation_mask(self, names):
2114         if self.annotation_select_mask is not None:
2115             self.set_annotation_mask(self.annotation_select_mask.union(names))
2116 
2117     def set_extra_mask(self, names):
2118         """
2119         Set the mask of extra select items that will be returned by SELECT.
2120         Don't remove them from the Query since they might be used later.
2121         """
2122         if names is None:
2123             self.extra_select_mask = None
2124         else:
2125             self.extra_select_mask = set(names)
2126         self._extra_select_cache = None
2127 
2128     def set_values(self, fields):
2129         self.select_related = False
2130         self.clear_deferred_loading()
2131         self.clear_select_fields()
2132 
2133         if fields:
2134             field_names = []
2135             extra_names = []
2136             annotation_names = []
2137             if not self.extra and not self.annotations:
2138                 # Shortcut - if there are no extra or annotations, then
2139                 # the values() clause must be just field names.
2140                 field_names = list(fields)
2141             else:
2142                 self.default_cols = False
2143                 for f in fields:
2144                     if f in self.extra_select:
2145                         extra_names.append(f)
2146                     elif f in self.annotation_select:
2147                         annotation_names.append(f)
2148                     else:
2149                         field_names.append(f)
2150             self.set_extra_mask(extra_names)
2151             self.set_annotation_mask(annotation_names)
2152         else:
2153             field_names = [f.attname for f in self.model._meta.concrete_fields]
2154         # Selected annotations must be known before setting the GROUP BY
2155         # clause.
2156         if self.group_by is True:
2157             self.add_fields((f.attname for f in self.model._meta.concrete_fields), False)
2158             # Disable GROUP BY aliases to avoid orphaning references to the
2159             # SELECT clause which is about to be cleared.
2160             self.set_group_by(allow_aliases=False)
2161             self.clear_select_fields()
2162         elif self.group_by:
2163             # Resolve GROUP BY annotation references if they are not part of
2164             # the selected fields anymore.
2165             group_by = []
2166             for expr in self.group_by:
2167                 if isinstance(expr, Ref) and expr.refs not in field_names:
2168                     expr = self.annotations[expr.refs]
2169                 group_by.append(expr)
2170             self.group_by = tuple(group_by)
2171 
2172         self.values_select = tuple(field_names)
2173         self.add_fields(field_names, True)
2174 
2175     @property
2176     def annotation_select(self):
2177         """
2178         Return the dictionary of aggregate columns that are not masked and
2179         should be used in the SELECT clause. Cache this result for performance.
2180         """
2181         if self._annotation_select_cache is not None:
2182             return self._annotation_select_cache
2183         elif not self.annotations:
2184             return {}
2185         elif self.annotation_select_mask is not None:
2186             self._annotation_select_cache = {
2187                 k: v for k, v in self.annotations.items()
2188                 if k in self.annotation_select_mask
2189             }
2190             return self._annotation_select_cache
2191         else:
2192             return self.annotations
2193 
2194     @property
2195     def extra_select(self):
2196         if self._extra_select_cache is not None:
2197             return self._extra_select_cache
2198         if not self.extra:
2199             return {}
2200         elif self.extra_select_mask is not None:
2201             self._extra_select_cache = {
2202                 k: v for k, v in self.extra.items()
2203                 if k in self.extra_select_mask
2204             }
2205             return self._extra_select_cache
2206         else:
2207             return self.extra
2208 
2209     def trim_start(self, names_with_path):
2210         """
2211         Trim joins from the start of the join path. The candidates for trim
2212         are the PathInfos in names_with_path structure that are m2m joins.
2213 
2214         Also set the select column so the start matches the join.
2215 
2216         This method is meant to be used for generating the subquery joins &
2217         cols in split_exclude().
2218 
2219         Return a lookup usable for doing outerq.filter(lookup=self) and a
2220         boolean indicating if the joins in the prefix contain a LEFT OUTER join.
2221         _"""
2222         all_paths = []
2223         for _, paths in names_with_path:
2224             all_paths.extend(paths)
2225         contains_louter = False
2226         # Trim and operate only on tables that were generated for
2227         # the lookup part of the query. That is, avoid trimming
2228         # joins generated for F() expressions.
2229         lookup_tables = [
2230             t for t in self.alias_map
2231             if t in self._lookup_joins or t == self.base_table
2232         ]
2233         for trimmed_paths, path in enumerate(all_paths):
2234             if path.m2m:
2235                 break
2236             if self.alias_map[lookup_tables[trimmed_paths + 1]].join_type == LOUTER:
2237                 contains_louter = True
2238             alias = lookup_tables[trimmed_paths]
2239             self.unref_alias(alias)
2240         # The path.join_field is a Rel, lets get the other side's field
2241         join_field = path.join_field.field
2242         # Build the filter prefix.
2243         paths_in_prefix = trimmed_paths
2244         trimmed_prefix = []
2245         for name, path in names_with_path:
2246             if paths_in_prefix - len(path) < 0:
2247                 break
2248             trimmed_prefix.append(name)
2249             paths_in_prefix -= len(path)
2250         trimmed_prefix.append(
2251             join_field.foreign_related_fields[0].name)
2252         trimmed_prefix = LOOKUP_SEP.join(trimmed_prefix)
2253         # Lets still see if we can trim the first join from the inner query
2254         # (that is, self). We can't do this for:
2255         # - LEFT JOINs because we would miss those rows that have nothing on
2256         #   the outer side,
2257         # - INNER JOINs from filtered relations because we would miss their
2258         #   filters.
2259         first_join = self.alias_map[lookup_tables[trimmed_paths + 1]]
2260         if first_join.join_type != LOUTER and not first_join.filtered_relation:
2261             select_fields = [r[0] for r in join_field.related_fields]
2262             select_alias = lookup_tables[trimmed_paths + 1]
2263             self.unref_alias(lookup_tables[trimmed_paths])
2264             extra_restriction = join_field.get_extra_restriction(
2265                 self.where_class, None, lookup_tables[trimmed_paths + 1])
2266             if extra_restriction:
2267                 self.where.add(extra_restriction, AND)
2268         else:
2269             # TODO: It might be possible to trim more joins from the start of the
2270             # inner query if it happens to have a longer join chain containing the
2271             # values in select_fields. Lets punt this one for now.
2272             select_fields = [r[1] for r in join_field.related_fields]
2273             select_alias = lookup_tables[trimmed_paths]
2274         # The found starting point is likely a Join instead of a BaseTable reference.
2275         # But the first entry in the query's FROM clause must not be a JOIN.
2276         for table in self.alias_map:
2277             if self.alias_refcount[table] > 0:
2278                 self.alias_map[table] = BaseTable(self.alias_map[table].table_name, table)
2279                 break
2280         self.set_select([f.get_col(select_alias) for f in select_fields])
2281         return trimmed_prefix, contains_louter
2282 
2283     def is_nullable(self, field):
2284         """
2285         Check if the given field should be treated as nullable.
2286 
2287         Some backends treat '' as null and Django treats such fields as
2288         nullable for those backends. In such situations field.null can be
2289         False even if we should treat the field as nullable.
2290         """
2291         # We need to use DEFAULT_DB_ALIAS here, as QuerySet does not have
2292         # (nor should it have) knowledge of which connection is going to be
2293         # used. The proper fix would be to defer all decisions where
2294         # is_nullable() is needed to the compiler stage, but that is not easy
2295         # to do currently.
2296         return (
2297             connections[DEFAULT_DB_ALIAS].features.interprets_empty_strings_as_nulls and
2298             field.empty_strings_allowed
2299         ) or field.null

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 00:19:44.009 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 00:19:44.009 | INFO     | Search-only: skipping patch selection
2025-10-18 00:19:44.148 | INFO     | Task django__django-13030 completed successfully.
2025-10-18 00:19:44.148 | INFO     | No patch generated. You can try running ACR again.
