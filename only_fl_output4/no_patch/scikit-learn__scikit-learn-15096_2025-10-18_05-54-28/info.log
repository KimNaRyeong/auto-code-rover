2025-10-18 05:54:28.856 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 05:54:32.496 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=333, output_tokens=12, cost=0.003690
2025-10-18 05:54:32.498 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 05:54:32.506 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:54:36.181 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=638, output_tokens=169, cost=0.011450
2025-10-18 05:54:36.186 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:54:38.743 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=523, output_tokens=54, cost=0.006850
2025-10-18 05:54:38.744 | DEBUG    | Extracted a valid json.
2025-10-18 05:54:38.744 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"GridSearchCV\")",
        "search_method_in_class(\"fit\", \"GridSearchCV\")",
        "search_code(\"cv_results_['params']\")"
    ],
    "bug_locations": []
}
2025-10-18 05:54:38.792 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 05:54:38.792 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 05:54:51.661 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1211, output_tokens=565, cost=0.029060
2025-10-18 05:54:51.676 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:54:53.954 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1992, output_tokens=64, cost=0.021840
2025-10-18 05:54:53.957 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:54:56.605 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=418, output_tokens=82, cost=0.006640
2025-10-18 05:54:56.606 | DEBUG    | Extracted a valid json.
2025-10-18 05:54:56.606 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class_in_file(\"BaseSearchCV\", \"sklearn/model_selection/_search.py\")",
        "search_method_in_file(\"_run_search\", \"sklearn/model_selection/_search.py\")",
        "search_code_in_file(\"cv_results_['params']\", \"sklearn/model_selection/_search.py\")"
    ],
    "bug_locations": []
}
2025-10-18 05:54:56.886 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 05:54:56.887 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 05:55:12.700 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=7683, output_tokens=687, cost=0.097440
2025-10-18 05:55:12.710 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:55:23.800 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=8586, output_tokens=465, cost=0.099810
2025-10-18 05:55:23.810 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:55:28.387 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=819, output_tokens=156, cost=0.012870
2025-10-18 05:55:28.389 | DEBUG    | Extracted a valid json.
2025-10-18 05:55:28.389 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/model_selection/_search.py", 
            "class": "BaseSearchCV", 
            "method": "_format_results", 
            "intended_behavior": "Modify the implementation to ensure that only the parameters of the estimator before fitting are included in `cv_results_['params']`."
        }, 
        {
            "file": "sklearn/model_selection/_search.py", 
            "class": "BaseSearchCV", 
            "method": "fit", 
            "intended_behavior": "Ensure that the `fit` method or associated methods involved in the fitting process accurately clone estimator parameters at the start of the process."
        }
    ]
}
2025-10-18 05:55:28.396 | DEBUG    | Bug location extracted successfully: [<file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>_format_results</method>
<code>
751     def _format_results(self, candidate_params, scorers, n_splits, out):
752         n_candidates = len(candidate_params)
753 
754         # if one choose to see train score, "out" will contain train score info
755         if self.return_train_score:
756             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,
757              score_time) = zip(*out)
758         else:
759             (test_score_dicts, test_sample_counts, fit_time,
760              score_time) = zip(*out)
761 
762         # test_score_dicts and train_score dicts are lists of dictionaries and
763         # we make them into dict of lists
764         test_scores = _aggregate_score_dicts(test_score_dicts)
765         if self.return_train_score:
766             train_scores = _aggregate_score_dicts(train_score_dicts)
767 
768         results = {}
769 
770         def _store(key_name, array, weights=None, splits=False, rank=False):
771             """A small helper to store the scores/times to the cv_results_"""
772             # When iterated first by splits, then by parameters
773             # We want `array` to have `n_candidates` rows and `n_splits` cols.
774             array = np.array(array, dtype=np.float64).reshape(n_candidates,
775                                                               n_splits)
776             if splits:
777                 for split_i in range(n_splits):
778                     # Uses closure to alter the results
779                     results["split%d_%s"
780                             % (split_i, key_name)] = array[:, split_i]
781 
782             array_means = np.average(array, axis=1, weights=weights)
783             results['mean_%s' % key_name] = array_means
784             # Weighted std is not directly available in numpy
785             array_stds = np.sqrt(np.average((array -
786                                              array_means[:, np.newaxis]) ** 2,
787                                             axis=1, weights=weights))
788             results['std_%s' % key_name] = array_stds
789 
790             if rank:
791                 results["rank_%s" % key_name] = np.asarray(
792                     rankdata(-array_means, method='min'), dtype=np.int32)
793 
794         _store('fit_time', fit_time)
795         _store('score_time', score_time)
796         # Use one MaskedArray and mask all the places where the param is not
797         # applicable for that candidate. Use defaultdict as each candidate may
798         # not contain all the params
799         param_results = defaultdict(partial(MaskedArray,
800                                             np.empty(n_candidates,),
801                                             mask=True,
802                                             dtype=object))
803         for cand_i, params in enumerate(candidate_params):
804             for name, value in params.items():
805                 # An all masked empty array gets created for the key
806                 # `"param_%s" % name` at the first occurrence of `name`.
807                 # Setting the value at an index also unmasks that index
808                 param_results["param_%s" % name][cand_i] = value
809 
810         results.update(param_results)
811         # Store a list of param dicts at the key 'params'
812         results['params'] = candidate_params
813 
814         # NOTE test_sample counts (weights) remain the same for all candidates
815         test_sample_counts = np.array(test_sample_counts[:n_splits],
816                                       dtype=np.int)
817 
818         if self.iid != 'deprecated':
819             warnings.warn(
820                 "The parameter 'iid' is deprecated in 0.22 and will be "
821                 "removed in 0.24.", DeprecationWarning
822             )
823             iid = self.iid
824         else:
825             iid = False
826 
827         for scorer_name in scorers.keys():
828             # Computed the (weighted) mean and std for test scores alone
829             _store('test_%s' % scorer_name, test_scores[scorer_name],
830                    splits=True, rank=True,
831                    weights=test_sample_counts if iid else None)
832             if self.return_train_score:
833                 _store('train_%s' % scorer_name, train_scores[scorer_name],
834                        splits=True)
835 
836         return results

</code><intended_behavior>Modify the implementation to ensure that only the parameters of the estimator before fitting are included in `cv_results_['params']`.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>None</method>
<code>
400 class BaseSearchCV(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):
401     """Abstract base class for hyper parameter search with cross-validation.
402     """
403 
404     @abstractmethod
405     def __init__(self, estimator, scoring=None, n_jobs=None, iid='deprecated',
406                  refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
407                  error_score=np.nan, return_train_score=True):
408 
409         self.scoring = scoring
410         self.estimator = estimator
411         self.n_jobs = n_jobs
412         self.iid = iid
413         self.refit = refit
414         self.cv = cv
415         self.verbose = verbose
416         self.pre_dispatch = pre_dispatch
417         self.error_score = error_score
418         self.return_train_score = return_train_score
419 
420     @property
421     def _estimator_type(self):
422         return self.estimator._estimator_type
423 
424     def score(self, X, y=None):
425         """Returns the score on the given data, if the estimator has been refit.
426 
427         This uses the score defined by ``scoring`` where provided, and the
428         ``best_estimator_.score`` method otherwise.
429 
430         Parameters
431         ----------
432         X : array-like, shape = [n_samples, n_features]
433             Input data, where n_samples is the number of samples and
434             n_features is the number of features.
435 
436         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
437             Target relative to X for classification or regression;
438             None for unsupervised learning.
439 
440         Returns
441         -------
442         score : float
443         """
444         self._check_is_fitted('score')
445         if self.scorer_ is None:
446             raise ValueError("No score function explicitly defined, "
447                              "and the estimator doesn't provide one %s"
448                              % self.best_estimator_)
449         score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_
450         return score(self.best_estimator_, X, y)
451 
452     def _check_is_fitted(self, method_name):
453         if not self.refit:
454             raise NotFittedError('This %s instance was initialized '
455                                  'with refit=False. %s is '
456                                  'available only after refitting on the best '
457                                  'parameters. You can refit an estimator '
458                                  'manually using the ``best_params_`` '
459                                  'attribute'
460                                  % (type(self).__name__, method_name))
461         else:
462             check_is_fitted(self)
463 
464     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
465     def predict(self, X):
466         """Call predict on the estimator with the best found parameters.
467 
468         Only available if ``refit=True`` and the underlying estimator supports
469         ``predict``.
470 
471         Parameters
472         ----------
473         X : indexable, length n_samples
474             Must fulfill the input assumptions of the
475             underlying estimator.
476 
477         """
478         self._check_is_fitted('predict')
479         return self.best_estimator_.predict(X)
480 
481     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
482     def predict_proba(self, X):
483         """Call predict_proba on the estimator with the best found parameters.
484 
485         Only available if ``refit=True`` and the underlying estimator supports
486         ``predict_proba``.
487 
488         Parameters
489         ----------
490         X : indexable, length n_samples
491             Must fulfill the input assumptions of the
492             underlying estimator.
493 
494         """
495         self._check_is_fitted('predict_proba')
496         return self.best_estimator_.predict_proba(X)
497 
498     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
499     def predict_log_proba(self, X):
500         """Call predict_log_proba on the estimator with the best found parameters.
501 
502         Only available if ``refit=True`` and the underlying estimator supports
503         ``predict_log_proba``.
504 
505         Parameters
506         ----------
507         X : indexable, length n_samples
508             Must fulfill the input assumptions of the
509             underlying estimator.
510 
511         """
512         self._check_is_fitted('predict_log_proba')
513         return self.best_estimator_.predict_log_proba(X)
514 
515     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
516     def decision_function(self, X):
517         """Call decision_function on the estimator with the best found parameters.
518 
519         Only available if ``refit=True`` and the underlying estimator supports
520         ``decision_function``.
521 
522         Parameters
523         ----------
524         X : indexable, length n_samples
525             Must fulfill the input assumptions of the
526             underlying estimator.
527 
528         """
529         self._check_is_fitted('decision_function')
530         return self.best_estimator_.decision_function(X)
531 
532     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
533     def transform(self, X):
534         """Call transform on the estimator with the best found parameters.
535 
536         Only available if the underlying estimator supports ``transform`` and
537         ``refit=True``.
538 
539         Parameters
540         ----------
541         X : indexable, length n_samples
542             Must fulfill the input assumptions of the
543             underlying estimator.
544 
545         """
546         self._check_is_fitted('transform')
547         return self.best_estimator_.transform(X)
548 
549     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
550     def inverse_transform(self, Xt):
551         """Call inverse_transform on the estimator with the best found params.
552 
553         Only available if the underlying estimator implements
554         ``inverse_transform`` and ``refit=True``.
555 
556         Parameters
557         ----------
558         Xt : indexable, length n_samples
559             Must fulfill the input assumptions of the
560             underlying estimator.
561 
562         """
563         self._check_is_fitted('inverse_transform')
564         return self.best_estimator_.inverse_transform(Xt)
565 
566     @property
567     def classes_(self):
568         self._check_is_fitted("classes_")
569         return self.best_estimator_.classes_
570 
571     def _run_search(self, evaluate_candidates):
572         """Repeatedly calls `evaluate_candidates` to conduct a search.
573 
574         This method, implemented in sub-classes, makes it possible to
575         customize the the scheduling of evaluations: GridSearchCV and
576         RandomizedSearchCV schedule evaluations for their whole parameter
577         search space at once but other more sequential approaches are also
578         possible: for instance is possible to iteratively schedule evaluations
579         for new regions of the parameter search space based on previously
580         collected evaluation results. This makes it possible to implement
581         Bayesian optimization or more generally sequential model-based
582         optimization by deriving from the BaseSearchCV abstract base class.
583 
584         Parameters
585         ----------
586         evaluate_candidates : callable
587             This callback accepts a list of candidates, where each candidate is
588             a dict of parameter settings. It returns a dict of all results so
589             far, formatted like ``cv_results_``.
590 
591         Examples
592         --------
593 
594         ::
595 
596             def _run_search(self, evaluate_candidates):
597                 'Try C=0.1 only if C=1 is better than C=10'
598                 all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
599                 score = all_results['mean_test_score']
600                 if score[0] < score[1]:
601                     evaluate_candidates([{'C': 0.1}])
602         """
603         raise NotImplementedError("_run_search not implemented.")
604 
605     def fit(self, X, y=None, groups=None, **fit_params):
606         """Run fit with all sets of parameters.
607 
608         Parameters
609         ----------
610 
611         X : array-like, shape = [n_samples, n_features]
612             Training vector, where n_samples is the number of samples and
613             n_features is the number of features.
614 
615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
616             Target relative to X for classification or regression;
617             None for unsupervised learning.
618 
619         groups : array-like, with shape (n_samples,), optional
620             Group labels for the samples used while splitting the dataset into
621             train/test set. Only used in conjunction with a "Group" :term:`cv`
622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
623 
624         **fit_params : dict of string -> object
625             Parameters passed to the ``fit`` method of the estimator
626         """
627         estimator = self.estimator
628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
629 
630         scorers, self.multimetric_ = _check_multimetric_scoring(
631             self.estimator, scoring=self.scoring)
632 
633         if self.multimetric_:
634             if self.refit is not False and (
635                     not isinstance(self.refit, str) or
636                     # This will work for both dict / list (tuple)
637                     self.refit not in scorers) and not callable(self.refit):
638                 raise ValueError("For multi-metric scoring, the parameter "
639                                  "refit must be set to a scorer key or a "
640                                  "callable to refit an estimator with the "
641                                  "best parameter setting on the whole "
642                                  "data and make the best_* attributes "
643                                  "available for that metric. If this is "
644                                  "not needed, refit should be set to "
645                                  "False explicitly. %r was passed."
646                                  % self.refit)
647             else:
648                 refit_metric = self.refit
649         else:
650             refit_metric = 'score'
651 
652         X, y, groups = indexable(X, y, groups)
653         n_splits = cv.get_n_splits(X, y, groups)
654 
655         base_estimator = clone(self.estimator)
656 
657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
658                             pre_dispatch=self.pre_dispatch)
659 
660         fit_and_score_kwargs = dict(scorer=scorers,
661                                     fit_params=fit_params,
662                                     return_train_score=self.return_train_score,
663                                     return_n_test_samples=True,
664                                     return_times=True,
665                                     return_parameters=False,
666                                     error_score=self.error_score,
667                                     verbose=self.verbose)
668         results = {}
669         with parallel:
670             all_candidate_params = []
671             all_out = []
672 
673             def evaluate_candidates(candidate_params):
674                 candidate_params = list(candidate_params)
675                 n_candidates = len(candidate_params)
676 
677                 if self.verbose > 0:
678                     print("Fitting {0} folds for each of {1} candidates,"
679                           " totalling {2} fits".format(
680                               n_splits, n_candidates, n_candidates * n_splits))
681 
682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
683                                                        X, y,
684                                                        train=train, test=test,
685                                                        parameters=parameters,
686                                                        **fit_and_score_kwargs)
687                                for parameters, (train, test)
688                                in product(candidate_params,
689                                           cv.split(X, y, groups)))
690 
691                 if len(out) < 1:
692                     raise ValueError('No fits were performed. '
693                                      'Was the CV iterator empty? '
694                                      'Were there no candidates?')
695                 elif len(out) != n_candidates * n_splits:
696                     raise ValueError('cv.split and cv.get_n_splits returned '
697                                      'inconsistent results. Expected {} '
698                                      'splits, got {}'
699                                      .format(n_splits,
700                                              len(out) // n_candidates))
701 
702                 all_candidate_params.extend(candidate_params)
703                 all_out.extend(out)
704 
705                 nonlocal results
706                 results = self._format_results(
707                     all_candidate_params, scorers, n_splits, all_out)
708                 return results
709 
710             self._run_search(evaluate_candidates)
711 
712         # For multi-metric evaluation, store the best_index_, best_params_ and
713         # best_score_ iff refit is one of the scorer names
714         # In single metric evaluation, refit_metric is "score"
715         if self.refit or not self.multimetric_:
716             # If callable, refit is expected to return the index of the best
717             # parameter set.
718             if callable(self.refit):
719                 self.best_index_ = self.refit(results)
720                 if not isinstance(self.best_index_, numbers.Integral):
721                     raise TypeError('best_index_ returned is not an integer')
722                 if (self.best_index_ < 0 or
723                    self.best_index_ >= len(results["params"])):
724                     raise IndexError('best_index_ index out of range')
725             else:
726                 self.best_index_ = results["rank_test_%s"
727                                            % refit_metric].argmin()
728                 self.best_score_ = results["mean_test_%s" % refit_metric][
729                                            self.best_index_]
730             self.best_params_ = results["params"][self.best_index_]
731 
732         if self.refit:
733             self.best_estimator_ = clone(base_estimator).set_params(
734                 **self.best_params_)
735             refit_start_time = time.time()
736             if y is not None:
737                 self.best_estimator_.fit(X, y, **fit_params)
738             else:
739                 self.best_estimator_.fit(X, **fit_params)
740             refit_end_time = time.time()
741             self.refit_time_ = refit_end_time - refit_start_time
742 
743         # Store the only scorer not as a dict for single metric evaluation
744         self.scorer_ = scorers if self.multimetric_ else scorers['score']
745 
746         self.cv_results_ = results
747         self.n_splits_ = n_splits
748 
749         return self
750 
751     def _format_results(self, candidate_params, scorers, n_splits, out):
752         n_candidates = len(candidate_params)
753 
754         # if one choose to see train score, "out" will contain train score info
755         if self.return_train_score:
756             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,
757              score_time) = zip(*out)
758         else:
759             (test_score_dicts, test_sample_counts, fit_time,
760              score_time) = zip(*out)
761 
762         # test_score_dicts and train_score dicts are lists of dictionaries and
763         # we make them into dict of lists
764         test_scores = _aggregate_score_dicts(test_score_dicts)
765         if self.return_train_score:
766             train_scores = _aggregate_score_dicts(train_score_dicts)
767 
768         results = {}
769 
770         def _store(key_name, array, weights=None, splits=False, rank=False):
771             """A small helper to store the scores/times to the cv_results_"""
772             # When iterated first by splits, then by parameters
773             # We want `array` to have `n_candidates` rows and `n_splits` cols.
774             array = np.array(array, dtype=np.float64).reshape(n_candidates,
775                                                               n_splits)
776             if splits:
777                 for split_i in range(n_splits):
778                     # Uses closure to alter the results
779                     results["split%d_%s"
780                             % (split_i, key_name)] = array[:, split_i]
781 
782             array_means = np.average(array, axis=1, weights=weights)
783             results['mean_%s' % key_name] = array_means
784             # Weighted std is not directly available in numpy
785             array_stds = np.sqrt(np.average((array -
786                                              array_means[:, np.newaxis]) ** 2,
787                                             axis=1, weights=weights))
788             results['std_%s' % key_name] = array_stds
789 
790             if rank:
791                 results["rank_%s" % key_name] = np.asarray(
792                     rankdata(-array_means, method='min'), dtype=np.int32)
793 
794         _store('fit_time', fit_time)
795         _store('score_time', score_time)
796         # Use one MaskedArray and mask all the places where the param is not
797         # applicable for that candidate. Use defaultdict as each candidate may
798         # not contain all the params
799         param_results = defaultdict(partial(MaskedArray,
800                                             np.empty(n_candidates,),
801                                             mask=True,
802                                             dtype=object))
803         for cand_i, params in enumerate(candidate_params):
804             for name, value in params.items():
805                 # An all masked empty array gets created for the key
806                 # `"param_%s" % name` at the first occurrence of `name`.
807                 # Setting the value at an index also unmasks that index
808                 param_results["param_%s" % name][cand_i] = value
809 
810         results.update(param_results)
811         # Store a list of param dicts at the key 'params'
812         results['params'] = candidate_params
813 
814         # NOTE test_sample counts (weights) remain the same for all candidates
815         test_sample_counts = np.array(test_sample_counts[:n_splits],
816                                       dtype=np.int)
817 
818         if self.iid != 'deprecated':
819             warnings.warn(
820                 "The parameter 'iid' is deprecated in 0.22 and will be "
821                 "removed in 0.24.", DeprecationWarning
822             )
823             iid = self.iid
824         else:
825             iid = False
826 
827         for scorer_name in scorers.keys():
828             # Computed the (weighted) mean and std for test scores alone
829             _store('test_%s' % scorer_name, test_scores[scorer_name],
830                    splits=True, rank=True,
831                    weights=test_sample_counts if iid else None)
832             if self.return_train_score:
833                 _store('train_%s' % scorer_name, train_scores[scorer_name],
834                        splits=True)
835 
836         return results

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>fit</method>
<code>
605     def fit(self, X, y=None, groups=None, **fit_params):
606         """Run fit with all sets of parameters.
607 
608         Parameters
609         ----------
610 
611         X : array-like, shape = [n_samples, n_features]
612             Training vector, where n_samples is the number of samples and
613             n_features is the number of features.
614 
615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
616             Target relative to X for classification or regression;
617             None for unsupervised learning.
618 
619         groups : array-like, with shape (n_samples,), optional
620             Group labels for the samples used while splitting the dataset into
621             train/test set. Only used in conjunction with a "Group" :term:`cv`
622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
623 
624         **fit_params : dict of string -> object
625             Parameters passed to the ``fit`` method of the estimator
626         """
627         estimator = self.estimator
628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
629 
630         scorers, self.multimetric_ = _check_multimetric_scoring(
631             self.estimator, scoring=self.scoring)
632 
633         if self.multimetric_:
634             if self.refit is not False and (
635                     not isinstance(self.refit, str) or
636                     # This will work for both dict / list (tuple)
637                     self.refit not in scorers) and not callable(self.refit):
638                 raise ValueError("For multi-metric scoring, the parameter "
639                                  "refit must be set to a scorer key or a "
640                                  "callable to refit an estimator with the "
641                                  "best parameter setting on the whole "
642                                  "data and make the best_* attributes "
643                                  "available for that metric. If this is "
644                                  "not needed, refit should be set to "
645                                  "False explicitly. %r was passed."
646                                  % self.refit)
647             else:
648                 refit_metric = self.refit
649         else:
650             refit_metric = 'score'
651 
652         X, y, groups = indexable(X, y, groups)
653         n_splits = cv.get_n_splits(X, y, groups)
654 
655         base_estimator = clone(self.estimator)
656 
657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
658                             pre_dispatch=self.pre_dispatch)
659 
660         fit_and_score_kwargs = dict(scorer=scorers,
661                                     fit_params=fit_params,
662                                     return_train_score=self.return_train_score,
663                                     return_n_test_samples=True,
664                                     return_times=True,
665                                     return_parameters=False,
666                                     error_score=self.error_score,
667                                     verbose=self.verbose)
668         results = {}
669         with parallel:
670             all_candidate_params = []
671             all_out = []
672 
673             def evaluate_candidates(candidate_params):
674                 candidate_params = list(candidate_params)
675                 n_candidates = len(candidate_params)
676 
677                 if self.verbose > 0:
678                     print("Fitting {0} folds for each of {1} candidates,"
679                           " totalling {2} fits".format(
680                               n_splits, n_candidates, n_candidates * n_splits))
681 
682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
683                                                        X, y,
684                                                        train=train, test=test,
685                                                        parameters=parameters,
686                                                        **fit_and_score_kwargs)
687                                for parameters, (train, test)
688                                in product(candidate_params,
689                                           cv.split(X, y, groups)))
690 
691                 if len(out) < 1:
692                     raise ValueError('No fits were performed. '
693                                      'Was the CV iterator empty? '
694                                      'Were there no candidates?')
695                 elif len(out) != n_candidates * n_splits:
696                     raise ValueError('cv.split and cv.get_n_splits returned '
697                                      'inconsistent results. Expected {} '
698                                      'splits, got {}'
699                                      .format(n_splits,
700                                              len(out) // n_candidates))
701 
702                 all_candidate_params.extend(candidate_params)
703                 all_out.extend(out)
704 
705                 nonlocal results
706                 results = self._format_results(
707                     all_candidate_params, scorers, n_splits, all_out)
708                 return results
709 
710             self._run_search(evaluate_candidates)
711 
712         # For multi-metric evaluation, store the best_index_, best_params_ and
713         # best_score_ iff refit is one of the scorer names
714         # In single metric evaluation, refit_metric is "score"
715         if self.refit or not self.multimetric_:
716             # If callable, refit is expected to return the index of the best
717             # parameter set.
718             if callable(self.refit):
719                 self.best_index_ = self.refit(results)
720                 if not isinstance(self.best_index_, numbers.Integral):
721                     raise TypeError('best_index_ returned is not an integer')
722                 if (self.best_index_ < 0 or
723                    self.best_index_ >= len(results["params"])):
724                     raise IndexError('best_index_ index out of range')
725             else:
726                 self.best_index_ = results["rank_test_%s"
727                                            % refit_metric].argmin()
728                 self.best_score_ = results["mean_test_%s" % refit_metric][
729                                            self.best_index_]
730             self.best_params_ = results["params"][self.best_index_]
731 
732         if self.refit:
733             self.best_estimator_ = clone(base_estimator).set_params(
734                 **self.best_params_)
735             refit_start_time = time.time()
736             if y is not None:
737                 self.best_estimator_.fit(X, y, **fit_params)
738             else:
739                 self.best_estimator_.fit(X, **fit_params)
740             refit_end_time = time.time()
741             self.refit_time_ = refit_end_time - refit_start_time
742 
743         # Store the only scorer not as a dict for single metric evaluation
744         self.scorer_ = scorers if self.multimetric_ else scorers['score']
745 
746         self.cv_results_ = results
747         self.n_splits_ = n_splits
748 
749         return self

</code><intended_behavior>Ensure that the `fit` method or associated methods involved in the fitting process accurately clone estimator parameters at the start of the process.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>None</method>
<code>
400 class BaseSearchCV(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):
401     """Abstract base class for hyper parameter search with cross-validation.
402     """
403 
404     @abstractmethod
405     def __init__(self, estimator, scoring=None, n_jobs=None, iid='deprecated',
406                  refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
407                  error_score=np.nan, return_train_score=True):
408 
409         self.scoring = scoring
410         self.estimator = estimator
411         self.n_jobs = n_jobs
412         self.iid = iid
413         self.refit = refit
414         self.cv = cv
415         self.verbose = verbose
416         self.pre_dispatch = pre_dispatch
417         self.error_score = error_score
418         self.return_train_score = return_train_score
419 
420     @property
421     def _estimator_type(self):
422         return self.estimator._estimator_type
423 
424     def score(self, X, y=None):
425         """Returns the score on the given data, if the estimator has been refit.
426 
427         This uses the score defined by ``scoring`` where provided, and the
428         ``best_estimator_.score`` method otherwise.
429 
430         Parameters
431         ----------
432         X : array-like, shape = [n_samples, n_features]
433             Input data, where n_samples is the number of samples and
434             n_features is the number of features.
435 
436         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
437             Target relative to X for classification or regression;
438             None for unsupervised learning.
439 
440         Returns
441         -------
442         score : float
443         """
444         self._check_is_fitted('score')
445         if self.scorer_ is None:
446             raise ValueError("No score function explicitly defined, "
447                              "and the estimator doesn't provide one %s"
448                              % self.best_estimator_)
449         score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_
450         return score(self.best_estimator_, X, y)
451 
452     def _check_is_fitted(self, method_name):
453         if not self.refit:
454             raise NotFittedError('This %s instance was initialized '
455                                  'with refit=False. %s is '
456                                  'available only after refitting on the best '
457                                  'parameters. You can refit an estimator '
458                                  'manually using the ``best_params_`` '
459                                  'attribute'
460                                  % (type(self).__name__, method_name))
461         else:
462             check_is_fitted(self)
463 
464     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
465     def predict(self, X):
466         """Call predict on the estimator with the best found parameters.
467 
468         Only available if ``refit=True`` and the underlying estimator supports
469         ``predict``.
470 
471         Parameters
472         ----------
473         X : indexable, length n_samples
474             Must fulfill the input assumptions of the
475             underlying estimator.
476 
477         """
478         self._check_is_fitted('predict')
479         return self.best_estimator_.predict(X)
480 
481     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
482     def predict_proba(self, X):
483         """Call predict_proba on the estimator with the best found parameters.
484 
485         Only available if ``refit=True`` and the underlying estimator supports
486         ``predict_proba``.
487 
488         Parameters
489         ----------
490         X : indexable, length n_samples
491             Must fulfill the input assumptions of the
492             underlying estimator.
493 
494         """
495         self._check_is_fitted('predict_proba')
496         return self.best_estimator_.predict_proba(X)
497 
498     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
499     def predict_log_proba(self, X):
500         """Call predict_log_proba on the estimator with the best found parameters.
501 
502         Only available if ``refit=True`` and the underlying estimator supports
503         ``predict_log_proba``.
504 
505         Parameters
506         ----------
507         X : indexable, length n_samples
508             Must fulfill the input assumptions of the
509             underlying estimator.
510 
511         """
512         self._check_is_fitted('predict_log_proba')
513         return self.best_estimator_.predict_log_proba(X)
514 
515     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
516     def decision_function(self, X):
517         """Call decision_function on the estimator with the best found parameters.
518 
519         Only available if ``refit=True`` and the underlying estimator supports
520         ``decision_function``.
521 
522         Parameters
523         ----------
524         X : indexable, length n_samples
525             Must fulfill the input assumptions of the
526             underlying estimator.
527 
528         """
529         self._check_is_fitted('decision_function')
530         return self.best_estimator_.decision_function(X)
531 
532     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
533     def transform(self, X):
534         """Call transform on the estimator with the best found parameters.
535 
536         Only available if the underlying estimator supports ``transform`` and
537         ``refit=True``.
538 
539         Parameters
540         ----------
541         X : indexable, length n_samples
542             Must fulfill the input assumptions of the
543             underlying estimator.
544 
545         """
546         self._check_is_fitted('transform')
547         return self.best_estimator_.transform(X)
548 
549     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
550     def inverse_transform(self, Xt):
551         """Call inverse_transform on the estimator with the best found params.
552 
553         Only available if the underlying estimator implements
554         ``inverse_transform`` and ``refit=True``.
555 
556         Parameters
557         ----------
558         Xt : indexable, length n_samples
559             Must fulfill the input assumptions of the
560             underlying estimator.
561 
562         """
563         self._check_is_fitted('inverse_transform')
564         return self.best_estimator_.inverse_transform(Xt)
565 
566     @property
567     def classes_(self):
568         self._check_is_fitted("classes_")
569         return self.best_estimator_.classes_
570 
571     def _run_search(self, evaluate_candidates):
572         """Repeatedly calls `evaluate_candidates` to conduct a search.
573 
574         This method, implemented in sub-classes, makes it possible to
575         customize the the scheduling of evaluations: GridSearchCV and
576         RandomizedSearchCV schedule evaluations for their whole parameter
577         search space at once but other more sequential approaches are also
578         possible: for instance is possible to iteratively schedule evaluations
579         for new regions of the parameter search space based on previously
580         collected evaluation results. This makes it possible to implement
581         Bayesian optimization or more generally sequential model-based
582         optimization by deriving from the BaseSearchCV abstract base class.
583 
584         Parameters
585         ----------
586         evaluate_candidates : callable
587             This callback accepts a list of candidates, where each candidate is
588             a dict of parameter settings. It returns a dict of all results so
589             far, formatted like ``cv_results_``.
590 
591         Examples
592         --------
593 
594         ::
595 
596             def _run_search(self, evaluate_candidates):
597                 'Try C=0.1 only if C=1 is better than C=10'
598                 all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
599                 score = all_results['mean_test_score']
600                 if score[0] < score[1]:
601                     evaluate_candidates([{'C': 0.1}])
602         """
603         raise NotImplementedError("_run_search not implemented.")
604 
605     def fit(self, X, y=None, groups=None, **fit_params):
606         """Run fit with all sets of parameters.
607 
608         Parameters
609         ----------
610 
611         X : array-like, shape = [n_samples, n_features]
612             Training vector, where n_samples is the number of samples and
613             n_features is the number of features.
614 
615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
616             Target relative to X for classification or regression;
617             None for unsupervised learning.
618 
619         groups : array-like, with shape (n_samples,), optional
620             Group labels for the samples used while splitting the dataset into
621             train/test set. Only used in conjunction with a "Group" :term:`cv`
622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
623 
624         **fit_params : dict of string -> object
625             Parameters passed to the ``fit`` method of the estimator
626         """
627         estimator = self.estimator
628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
629 
630         scorers, self.multimetric_ = _check_multimetric_scoring(
631             self.estimator, scoring=self.scoring)
632 
633         if self.multimetric_:
634             if self.refit is not False and (
635                     not isinstance(self.refit, str) or
636                     # This will work for both dict / list (tuple)
637                     self.refit not in scorers) and not callable(self.refit):
638                 raise ValueError("For multi-metric scoring, the parameter "
639                                  "refit must be set to a scorer key or a "
640                                  "callable to refit an estimator with the "
641                                  "best parameter setting on the whole "
642                                  "data and make the best_* attributes "
643                                  "available for that metric. If this is "
644                                  "not needed, refit should be set to "
645                                  "False explicitly. %r was passed."
646                                  % self.refit)
647             else:
648                 refit_metric = self.refit
649         else:
650             refit_metric = 'score'
651 
652         X, y, groups = indexable(X, y, groups)
653         n_splits = cv.get_n_splits(X, y, groups)
654 
655         base_estimator = clone(self.estimator)
656 
657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
658                             pre_dispatch=self.pre_dispatch)
659 
660         fit_and_score_kwargs = dict(scorer=scorers,
661                                     fit_params=fit_params,
662                                     return_train_score=self.return_train_score,
663                                     return_n_test_samples=True,
664                                     return_times=True,
665                                     return_parameters=False,
666                                     error_score=self.error_score,
667                                     verbose=self.verbose)
668         results = {}
669         with parallel:
670             all_candidate_params = []
671             all_out = []
672 
673             def evaluate_candidates(candidate_params):
674                 candidate_params = list(candidate_params)
675                 n_candidates = len(candidate_params)
676 
677                 if self.verbose > 0:
678                     print("Fitting {0} folds for each of {1} candidates,"
679                           " totalling {2} fits".format(
680                               n_splits, n_candidates, n_candidates * n_splits))
681 
682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
683                                                        X, y,
684                                                        train=train, test=test,
685                                                        parameters=parameters,
686                                                        **fit_and_score_kwargs)
687                                for parameters, (train, test)
688                                in product(candidate_params,
689                                           cv.split(X, y, groups)))
690 
691                 if len(out) < 1:
692                     raise ValueError('No fits were performed. '
693                                      'Was the CV iterator empty? '
694                                      'Were there no candidates?')
695                 elif len(out) != n_candidates * n_splits:
696                     raise ValueError('cv.split and cv.get_n_splits returned '
697                                      'inconsistent results. Expected {} '
698                                      'splits, got {}'
699                                      .format(n_splits,
700                                              len(out) // n_candidates))
701 
702                 all_candidate_params.extend(candidate_params)
703                 all_out.extend(out)
704 
705                 nonlocal results
706                 results = self._format_results(
707                     all_candidate_params, scorers, n_splits, all_out)
708                 return results
709 
710             self._run_search(evaluate_candidates)
711 
712         # For multi-metric evaluation, store the best_index_, best_params_ and
713         # best_score_ iff refit is one of the scorer names
714         # In single metric evaluation, refit_metric is "score"
715         if self.refit or not self.multimetric_:
716             # If callable, refit is expected to return the index of the best
717             # parameter set.
718             if callable(self.refit):
719                 self.best_index_ = self.refit(results)
720                 if not isinstance(self.best_index_, numbers.Integral):
721                     raise TypeError('best_index_ returned is not an integer')
722                 if (self.best_index_ < 0 or
723                    self.best_index_ >= len(results["params"])):
724                     raise IndexError('best_index_ index out of range')
725             else:
726                 self.best_index_ = results["rank_test_%s"
727                                            % refit_metric].argmin()
728                 self.best_score_ = results["mean_test_%s" % refit_metric][
729                                            self.best_index_]
730             self.best_params_ = results["params"][self.best_index_]
731 
732         if self.refit:
733             self.best_estimator_ = clone(base_estimator).set_params(
734                 **self.best_params_)
735             refit_start_time = time.time()
736             if y is not None:
737                 self.best_estimator_.fit(X, y, **fit_params)
738             else:
739                 self.best_estimator_.fit(X, **fit_params)
740             refit_end_time = time.time()
741             self.refit_time_ = refit_end_time - refit_start_time
742 
743         # Store the only scorer not as a dict for single metric evaluation
744         self.scorer_ = scorers if self.multimetric_ else scorers['score']
745 
746         self.cv_results_ = results
747         self.n_splits_ = n_splits
748 
749         return self
750 
751     def _format_results(self, candidate_params, scorers, n_splits, out):
752         n_candidates = len(candidate_params)
753 
754         # if one choose to see train score, "out" will contain train score info
755         if self.return_train_score:
756             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,
757              score_time) = zip(*out)
758         else:
759             (test_score_dicts, test_sample_counts, fit_time,
760              score_time) = zip(*out)
761 
762         # test_score_dicts and train_score dicts are lists of dictionaries and
763         # we make them into dict of lists
764         test_scores = _aggregate_score_dicts(test_score_dicts)
765         if self.return_train_score:
766             train_scores = _aggregate_score_dicts(train_score_dicts)
767 
768         results = {}
769 
770         def _store(key_name, array, weights=None, splits=False, rank=False):
771             """A small helper to store the scores/times to the cv_results_"""
772             # When iterated first by splits, then by parameters
773             # We want `array` to have `n_candidates` rows and `n_splits` cols.
774             array = np.array(array, dtype=np.float64).reshape(n_candidates,
775                                                               n_splits)
776             if splits:
777                 for split_i in range(n_splits):
778                     # Uses closure to alter the results
779                     results["split%d_%s"
780                             % (split_i, key_name)] = array[:, split_i]
781 
782             array_means = np.average(array, axis=1, weights=weights)
783             results['mean_%s' % key_name] = array_means
784             # Weighted std is not directly available in numpy
785             array_stds = np.sqrt(np.average((array -
786                                              array_means[:, np.newaxis]) ** 2,
787                                             axis=1, weights=weights))
788             results['std_%s' % key_name] = array_stds
789 
790             if rank:
791                 results["rank_%s" % key_name] = np.asarray(
792                     rankdata(-array_means, method='min'), dtype=np.int32)
793 
794         _store('fit_time', fit_time)
795         _store('score_time', score_time)
796         # Use one MaskedArray and mask all the places where the param is not
797         # applicable for that candidate. Use defaultdict as each candidate may
798         # not contain all the params
799         param_results = defaultdict(partial(MaskedArray,
800                                             np.empty(n_candidates,),
801                                             mask=True,
802                                             dtype=object))
803         for cand_i, params in enumerate(candidate_params):
804             for name, value in params.items():
805                 # An all masked empty array gets created for the key
806                 # `"param_%s" % name` at the first occurrence of `name`.
807                 # Setting the value at an index also unmasks that index
808                 param_results["param_%s" % name][cand_i] = value
809 
810         results.update(param_results)
811         # Store a list of param dicts at the key 'params'
812         results['params'] = candidate_params
813 
814         # NOTE test_sample counts (weights) remain the same for all candidates
815         test_sample_counts = np.array(test_sample_counts[:n_splits],
816                                       dtype=np.int)
817 
818         if self.iid != 'deprecated':
819             warnings.warn(
820                 "The parameter 'iid' is deprecated in 0.22 and will be "
821                 "removed in 0.24.", DeprecationWarning
822             )
823             iid = self.iid
824         else:
825             iid = False
826 
827         for scorer_name in scorers.keys():
828             # Computed the (weighted) mean and std for test scores alone
829             _store('test_%s' % scorer_name, test_scores[scorer_name],
830                    splits=True, rank=True,
831                    weights=test_sample_counts if iid else None)
832             if self.return_train_score:
833                 _store('train_%s' % scorer_name, train_scores[scorer_name],
834                        splits=True)
835 
836         return results

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 05:55:28.397 | INFO     | Search completed. Bug locations: [<file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>_format_results</method>
<code>
751     def _format_results(self, candidate_params, scorers, n_splits, out):
752         n_candidates = len(candidate_params)
753 
754         # if one choose to see train score, "out" will contain train score info
755         if self.return_train_score:
756             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,
757              score_time) = zip(*out)
758         else:
759             (test_score_dicts, test_sample_counts, fit_time,
760              score_time) = zip(*out)
761 
762         # test_score_dicts and train_score dicts are lists of dictionaries and
763         # we make them into dict of lists
764         test_scores = _aggregate_score_dicts(test_score_dicts)
765         if self.return_train_score:
766             train_scores = _aggregate_score_dicts(train_score_dicts)
767 
768         results = {}
769 
770         def _store(key_name, array, weights=None, splits=False, rank=False):
771             """A small helper to store the scores/times to the cv_results_"""
772             # When iterated first by splits, then by parameters
773             # We want `array` to have `n_candidates` rows and `n_splits` cols.
774             array = np.array(array, dtype=np.float64).reshape(n_candidates,
775                                                               n_splits)
776             if splits:
777                 for split_i in range(n_splits):
778                     # Uses closure to alter the results
779                     results["split%d_%s"
780                             % (split_i, key_name)] = array[:, split_i]
781 
782             array_means = np.average(array, axis=1, weights=weights)
783             results['mean_%s' % key_name] = array_means
784             # Weighted std is not directly available in numpy
785             array_stds = np.sqrt(np.average((array -
786                                              array_means[:, np.newaxis]) ** 2,
787                                             axis=1, weights=weights))
788             results['std_%s' % key_name] = array_stds
789 
790             if rank:
791                 results["rank_%s" % key_name] = np.asarray(
792                     rankdata(-array_means, method='min'), dtype=np.int32)
793 
794         _store('fit_time', fit_time)
795         _store('score_time', score_time)
796         # Use one MaskedArray and mask all the places where the param is not
797         # applicable for that candidate. Use defaultdict as each candidate may
798         # not contain all the params
799         param_results = defaultdict(partial(MaskedArray,
800                                             np.empty(n_candidates,),
801                                             mask=True,
802                                             dtype=object))
803         for cand_i, params in enumerate(candidate_params):
804             for name, value in params.items():
805                 # An all masked empty array gets created for the key
806                 # `"param_%s" % name` at the first occurrence of `name`.
807                 # Setting the value at an index also unmasks that index
808                 param_results["param_%s" % name][cand_i] = value
809 
810         results.update(param_results)
811         # Store a list of param dicts at the key 'params'
812         results['params'] = candidate_params
813 
814         # NOTE test_sample counts (weights) remain the same for all candidates
815         test_sample_counts = np.array(test_sample_counts[:n_splits],
816                                       dtype=np.int)
817 
818         if self.iid != 'deprecated':
819             warnings.warn(
820                 "The parameter 'iid' is deprecated in 0.22 and will be "
821                 "removed in 0.24.", DeprecationWarning
822             )
823             iid = self.iid
824         else:
825             iid = False
826 
827         for scorer_name in scorers.keys():
828             # Computed the (weighted) mean and std for test scores alone
829             _store('test_%s' % scorer_name, test_scores[scorer_name],
830                    splits=True, rank=True,
831                    weights=test_sample_counts if iid else None)
832             if self.return_train_score:
833                 _store('train_%s' % scorer_name, train_scores[scorer_name],
834                        splits=True)
835 
836         return results

</code><intended_behavior>Modify the implementation to ensure that only the parameters of the estimator before fitting are included in `cv_results_['params']`.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>None</method>
<code>
400 class BaseSearchCV(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):
401     """Abstract base class for hyper parameter search with cross-validation.
402     """
403 
404     @abstractmethod
405     def __init__(self, estimator, scoring=None, n_jobs=None, iid='deprecated',
406                  refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
407                  error_score=np.nan, return_train_score=True):
408 
409         self.scoring = scoring
410         self.estimator = estimator
411         self.n_jobs = n_jobs
412         self.iid = iid
413         self.refit = refit
414         self.cv = cv
415         self.verbose = verbose
416         self.pre_dispatch = pre_dispatch
417         self.error_score = error_score
418         self.return_train_score = return_train_score
419 
420     @property
421     def _estimator_type(self):
422         return self.estimator._estimator_type
423 
424     def score(self, X, y=None):
425         """Returns the score on the given data, if the estimator has been refit.
426 
427         This uses the score defined by ``scoring`` where provided, and the
428         ``best_estimator_.score`` method otherwise.
429 
430         Parameters
431         ----------
432         X : array-like, shape = [n_samples, n_features]
433             Input data, where n_samples is the number of samples and
434             n_features is the number of features.
435 
436         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
437             Target relative to X for classification or regression;
438             None for unsupervised learning.
439 
440         Returns
441         -------
442         score : float
443         """
444         self._check_is_fitted('score')
445         if self.scorer_ is None:
446             raise ValueError("No score function explicitly defined, "
447                              "and the estimator doesn't provide one %s"
448                              % self.best_estimator_)
449         score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_
450         return score(self.best_estimator_, X, y)
451 
452     def _check_is_fitted(self, method_name):
453         if not self.refit:
454             raise NotFittedError('This %s instance was initialized '
455                                  'with refit=False. %s is '
456                                  'available only after refitting on the best '
457                                  'parameters. You can refit an estimator '
458                                  'manually using the ``best_params_`` '
459                                  'attribute'
460                                  % (type(self).__name__, method_name))
461         else:
462             check_is_fitted(self)
463 
464     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
465     def predict(self, X):
466         """Call predict on the estimator with the best found parameters.
467 
468         Only available if ``refit=True`` and the underlying estimator supports
469         ``predict``.
470 
471         Parameters
472         ----------
473         X : indexable, length n_samples
474             Must fulfill the input assumptions of the
475             underlying estimator.
476 
477         """
478         self._check_is_fitted('predict')
479         return self.best_estimator_.predict(X)
480 
481     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
482     def predict_proba(self, X):
483         """Call predict_proba on the estimator with the best found parameters.
484 
485         Only available if ``refit=True`` and the underlying estimator supports
486         ``predict_proba``.
487 
488         Parameters
489         ----------
490         X : indexable, length n_samples
491             Must fulfill the input assumptions of the
492             underlying estimator.
493 
494         """
495         self._check_is_fitted('predict_proba')
496         return self.best_estimator_.predict_proba(X)
497 
498     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
499     def predict_log_proba(self, X):
500         """Call predict_log_proba on the estimator with the best found parameters.
501 
502         Only available if ``refit=True`` and the underlying estimator supports
503         ``predict_log_proba``.
504 
505         Parameters
506         ----------
507         X : indexable, length n_samples
508             Must fulfill the input assumptions of the
509             underlying estimator.
510 
511         """
512         self._check_is_fitted('predict_log_proba')
513         return self.best_estimator_.predict_log_proba(X)
514 
515     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
516     def decision_function(self, X):
517         """Call decision_function on the estimator with the best found parameters.
518 
519         Only available if ``refit=True`` and the underlying estimator supports
520         ``decision_function``.
521 
522         Parameters
523         ----------
524         X : indexable, length n_samples
525             Must fulfill the input assumptions of the
526             underlying estimator.
527 
528         """
529         self._check_is_fitted('decision_function')
530         return self.best_estimator_.decision_function(X)
531 
532     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
533     def transform(self, X):
534         """Call transform on the estimator with the best found parameters.
535 
536         Only available if the underlying estimator supports ``transform`` and
537         ``refit=True``.
538 
539         Parameters
540         ----------
541         X : indexable, length n_samples
542             Must fulfill the input assumptions of the
543             underlying estimator.
544 
545         """
546         self._check_is_fitted('transform')
547         return self.best_estimator_.transform(X)
548 
549     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
550     def inverse_transform(self, Xt):
551         """Call inverse_transform on the estimator with the best found params.
552 
553         Only available if the underlying estimator implements
554         ``inverse_transform`` and ``refit=True``.
555 
556         Parameters
557         ----------
558         Xt : indexable, length n_samples
559             Must fulfill the input assumptions of the
560             underlying estimator.
561 
562         """
563         self._check_is_fitted('inverse_transform')
564         return self.best_estimator_.inverse_transform(Xt)
565 
566     @property
567     def classes_(self):
568         self._check_is_fitted("classes_")
569         return self.best_estimator_.classes_
570 
571     def _run_search(self, evaluate_candidates):
572         """Repeatedly calls `evaluate_candidates` to conduct a search.
573 
574         This method, implemented in sub-classes, makes it possible to
575         customize the the scheduling of evaluations: GridSearchCV and
576         RandomizedSearchCV schedule evaluations for their whole parameter
577         search space at once but other more sequential approaches are also
578         possible: for instance is possible to iteratively schedule evaluations
579         for new regions of the parameter search space based on previously
580         collected evaluation results. This makes it possible to implement
581         Bayesian optimization or more generally sequential model-based
582         optimization by deriving from the BaseSearchCV abstract base class.
583 
584         Parameters
585         ----------
586         evaluate_candidates : callable
587             This callback accepts a list of candidates, where each candidate is
588             a dict of parameter settings. It returns a dict of all results so
589             far, formatted like ``cv_results_``.
590 
591         Examples
592         --------
593 
594         ::
595 
596             def _run_search(self, evaluate_candidates):
597                 'Try C=0.1 only if C=1 is better than C=10'
598                 all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
599                 score = all_results['mean_test_score']
600                 if score[0] < score[1]:
601                     evaluate_candidates([{'C': 0.1}])
602         """
603         raise NotImplementedError("_run_search not implemented.")
604 
605     def fit(self, X, y=None, groups=None, **fit_params):
606         """Run fit with all sets of parameters.
607 
608         Parameters
609         ----------
610 
611         X : array-like, shape = [n_samples, n_features]
612             Training vector, where n_samples is the number of samples and
613             n_features is the number of features.
614 
615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
616             Target relative to X for classification or regression;
617             None for unsupervised learning.
618 
619         groups : array-like, with shape (n_samples,), optional
620             Group labels for the samples used while splitting the dataset into
621             train/test set. Only used in conjunction with a "Group" :term:`cv`
622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
623 
624         **fit_params : dict of string -> object
625             Parameters passed to the ``fit`` method of the estimator
626         """
627         estimator = self.estimator
628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
629 
630         scorers, self.multimetric_ = _check_multimetric_scoring(
631             self.estimator, scoring=self.scoring)
632 
633         if self.multimetric_:
634             if self.refit is not False and (
635                     not isinstance(self.refit, str) or
636                     # This will work for both dict / list (tuple)
637                     self.refit not in scorers) and not callable(self.refit):
638                 raise ValueError("For multi-metric scoring, the parameter "
639                                  "refit must be set to a scorer key or a "
640                                  "callable to refit an estimator with the "
641                                  "best parameter setting on the whole "
642                                  "data and make the best_* attributes "
643                                  "available for that metric. If this is "
644                                  "not needed, refit should be set to "
645                                  "False explicitly. %r was passed."
646                                  % self.refit)
647             else:
648                 refit_metric = self.refit
649         else:
650             refit_metric = 'score'
651 
652         X, y, groups = indexable(X, y, groups)
653         n_splits = cv.get_n_splits(X, y, groups)
654 
655         base_estimator = clone(self.estimator)
656 
657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
658                             pre_dispatch=self.pre_dispatch)
659 
660         fit_and_score_kwargs = dict(scorer=scorers,
661                                     fit_params=fit_params,
662                                     return_train_score=self.return_train_score,
663                                     return_n_test_samples=True,
664                                     return_times=True,
665                                     return_parameters=False,
666                                     error_score=self.error_score,
667                                     verbose=self.verbose)
668         results = {}
669         with parallel:
670             all_candidate_params = []
671             all_out = []
672 
673             def evaluate_candidates(candidate_params):
674                 candidate_params = list(candidate_params)
675                 n_candidates = len(candidate_params)
676 
677                 if self.verbose > 0:
678                     print("Fitting {0} folds for each of {1} candidates,"
679                           " totalling {2} fits".format(
680                               n_splits, n_candidates, n_candidates * n_splits))
681 
682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
683                                                        X, y,
684                                                        train=train, test=test,
685                                                        parameters=parameters,
686                                                        **fit_and_score_kwargs)
687                                for parameters, (train, test)
688                                in product(candidate_params,
689                                           cv.split(X, y, groups)))
690 
691                 if len(out) < 1:
692                     raise ValueError('No fits were performed. '
693                                      'Was the CV iterator empty? '
694                                      'Were there no candidates?')
695                 elif len(out) != n_candidates * n_splits:
696                     raise ValueError('cv.split and cv.get_n_splits returned '
697                                      'inconsistent results. Expected {} '
698                                      'splits, got {}'
699                                      .format(n_splits,
700                                              len(out) // n_candidates))
701 
702                 all_candidate_params.extend(candidate_params)
703                 all_out.extend(out)
704 
705                 nonlocal results
706                 results = self._format_results(
707                     all_candidate_params, scorers, n_splits, all_out)
708                 return results
709 
710             self._run_search(evaluate_candidates)
711 
712         # For multi-metric evaluation, store the best_index_, best_params_ and
713         # best_score_ iff refit is one of the scorer names
714         # In single metric evaluation, refit_metric is "score"
715         if self.refit or not self.multimetric_:
716             # If callable, refit is expected to return the index of the best
717             # parameter set.
718             if callable(self.refit):
719                 self.best_index_ = self.refit(results)
720                 if not isinstance(self.best_index_, numbers.Integral):
721                     raise TypeError('best_index_ returned is not an integer')
722                 if (self.best_index_ < 0 or
723                    self.best_index_ >= len(results["params"])):
724                     raise IndexError('best_index_ index out of range')
725             else:
726                 self.best_index_ = results["rank_test_%s"
727                                            % refit_metric].argmin()
728                 self.best_score_ = results["mean_test_%s" % refit_metric][
729                                            self.best_index_]
730             self.best_params_ = results["params"][self.best_index_]
731 
732         if self.refit:
733             self.best_estimator_ = clone(base_estimator).set_params(
734                 **self.best_params_)
735             refit_start_time = time.time()
736             if y is not None:
737                 self.best_estimator_.fit(X, y, **fit_params)
738             else:
739                 self.best_estimator_.fit(X, **fit_params)
740             refit_end_time = time.time()
741             self.refit_time_ = refit_end_time - refit_start_time
742 
743         # Store the only scorer not as a dict for single metric evaluation
744         self.scorer_ = scorers if self.multimetric_ else scorers['score']
745 
746         self.cv_results_ = results
747         self.n_splits_ = n_splits
748 
749         return self
750 
751     def _format_results(self, candidate_params, scorers, n_splits, out):
752         n_candidates = len(candidate_params)
753 
754         # if one choose to see train score, "out" will contain train score info
755         if self.return_train_score:
756             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,
757              score_time) = zip(*out)
758         else:
759             (test_score_dicts, test_sample_counts, fit_time,
760              score_time) = zip(*out)
761 
762         # test_score_dicts and train_score dicts are lists of dictionaries and
763         # we make them into dict of lists
764         test_scores = _aggregate_score_dicts(test_score_dicts)
765         if self.return_train_score:
766             train_scores = _aggregate_score_dicts(train_score_dicts)
767 
768         results = {}
769 
770         def _store(key_name, array, weights=None, splits=False, rank=False):
771             """A small helper to store the scores/times to the cv_results_"""
772             # When iterated first by splits, then by parameters
773             # We want `array` to have `n_candidates` rows and `n_splits` cols.
774             array = np.array(array, dtype=np.float64).reshape(n_candidates,
775                                                               n_splits)
776             if splits:
777                 for split_i in range(n_splits):
778                     # Uses closure to alter the results
779                     results["split%d_%s"
780                             % (split_i, key_name)] = array[:, split_i]
781 
782             array_means = np.average(array, axis=1, weights=weights)
783             results['mean_%s' % key_name] = array_means
784             # Weighted std is not directly available in numpy
785             array_stds = np.sqrt(np.average((array -
786                                              array_means[:, np.newaxis]) ** 2,
787                                             axis=1, weights=weights))
788             results['std_%s' % key_name] = array_stds
789 
790             if rank:
791                 results["rank_%s" % key_name] = np.asarray(
792                     rankdata(-array_means, method='min'), dtype=np.int32)
793 
794         _store('fit_time', fit_time)
795         _store('score_time', score_time)
796         # Use one MaskedArray and mask all the places where the param is not
797         # applicable for that candidate. Use defaultdict as each candidate may
798         # not contain all the params
799         param_results = defaultdict(partial(MaskedArray,
800                                             np.empty(n_candidates,),
801                                             mask=True,
802                                             dtype=object))
803         for cand_i, params in enumerate(candidate_params):
804             for name, value in params.items():
805                 # An all masked empty array gets created for the key
806                 # `"param_%s" % name` at the first occurrence of `name`.
807                 # Setting the value at an index also unmasks that index
808                 param_results["param_%s" % name][cand_i] = value
809 
810         results.update(param_results)
811         # Store a list of param dicts at the key 'params'
812         results['params'] = candidate_params
813 
814         # NOTE test_sample counts (weights) remain the same for all candidates
815         test_sample_counts = np.array(test_sample_counts[:n_splits],
816                                       dtype=np.int)
817 
818         if self.iid != 'deprecated':
819             warnings.warn(
820                 "The parameter 'iid' is deprecated in 0.22 and will be "
821                 "removed in 0.24.", DeprecationWarning
822             )
823             iid = self.iid
824         else:
825             iid = False
826 
827         for scorer_name in scorers.keys():
828             # Computed the (weighted) mean and std for test scores alone
829             _store('test_%s' % scorer_name, test_scores[scorer_name],
830                    splits=True, rank=True,
831                    weights=test_sample_counts if iid else None)
832             if self.return_train_score:
833                 _store('train_%s' % scorer_name, train_scores[scorer_name],
834                        splits=True)
835 
836         return results

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>fit</method>
<code>
605     def fit(self, X, y=None, groups=None, **fit_params):
606         """Run fit with all sets of parameters.
607 
608         Parameters
609         ----------
610 
611         X : array-like, shape = [n_samples, n_features]
612             Training vector, where n_samples is the number of samples and
613             n_features is the number of features.
614 
615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
616             Target relative to X for classification or regression;
617             None for unsupervised learning.
618 
619         groups : array-like, with shape (n_samples,), optional
620             Group labels for the samples used while splitting the dataset into
621             train/test set. Only used in conjunction with a "Group" :term:`cv`
622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
623 
624         **fit_params : dict of string -> object
625             Parameters passed to the ``fit`` method of the estimator
626         """
627         estimator = self.estimator
628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
629 
630         scorers, self.multimetric_ = _check_multimetric_scoring(
631             self.estimator, scoring=self.scoring)
632 
633         if self.multimetric_:
634             if self.refit is not False and (
635                     not isinstance(self.refit, str) or
636                     # This will work for both dict / list (tuple)
637                     self.refit not in scorers) and not callable(self.refit):
638                 raise ValueError("For multi-metric scoring, the parameter "
639                                  "refit must be set to a scorer key or a "
640                                  "callable to refit an estimator with the "
641                                  "best parameter setting on the whole "
642                                  "data and make the best_* attributes "
643                                  "available for that metric. If this is "
644                                  "not needed, refit should be set to "
645                                  "False explicitly. %r was passed."
646                                  % self.refit)
647             else:
648                 refit_metric = self.refit
649         else:
650             refit_metric = 'score'
651 
652         X, y, groups = indexable(X, y, groups)
653         n_splits = cv.get_n_splits(X, y, groups)
654 
655         base_estimator = clone(self.estimator)
656 
657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
658                             pre_dispatch=self.pre_dispatch)
659 
660         fit_and_score_kwargs = dict(scorer=scorers,
661                                     fit_params=fit_params,
662                                     return_train_score=self.return_train_score,
663                                     return_n_test_samples=True,
664                                     return_times=True,
665                                     return_parameters=False,
666                                     error_score=self.error_score,
667                                     verbose=self.verbose)
668         results = {}
669         with parallel:
670             all_candidate_params = []
671             all_out = []
672 
673             def evaluate_candidates(candidate_params):
674                 candidate_params = list(candidate_params)
675                 n_candidates = len(candidate_params)
676 
677                 if self.verbose > 0:
678                     print("Fitting {0} folds for each of {1} candidates,"
679                           " totalling {2} fits".format(
680                               n_splits, n_candidates, n_candidates * n_splits))
681 
682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
683                                                        X, y,
684                                                        train=train, test=test,
685                                                        parameters=parameters,
686                                                        **fit_and_score_kwargs)
687                                for parameters, (train, test)
688                                in product(candidate_params,
689                                           cv.split(X, y, groups)))
690 
691                 if len(out) < 1:
692                     raise ValueError('No fits were performed. '
693                                      'Was the CV iterator empty? '
694                                      'Were there no candidates?')
695                 elif len(out) != n_candidates * n_splits:
696                     raise ValueError('cv.split and cv.get_n_splits returned '
697                                      'inconsistent results. Expected {} '
698                                      'splits, got {}'
699                                      .format(n_splits,
700                                              len(out) // n_candidates))
701 
702                 all_candidate_params.extend(candidate_params)
703                 all_out.extend(out)
704 
705                 nonlocal results
706                 results = self._format_results(
707                     all_candidate_params, scorers, n_splits, all_out)
708                 return results
709 
710             self._run_search(evaluate_candidates)
711 
712         # For multi-metric evaluation, store the best_index_, best_params_ and
713         # best_score_ iff refit is one of the scorer names
714         # In single metric evaluation, refit_metric is "score"
715         if self.refit or not self.multimetric_:
716             # If callable, refit is expected to return the index of the best
717             # parameter set.
718             if callable(self.refit):
719                 self.best_index_ = self.refit(results)
720                 if not isinstance(self.best_index_, numbers.Integral):
721                     raise TypeError('best_index_ returned is not an integer')
722                 if (self.best_index_ < 0 or
723                    self.best_index_ >= len(results["params"])):
724                     raise IndexError('best_index_ index out of range')
725             else:
726                 self.best_index_ = results["rank_test_%s"
727                                            % refit_metric].argmin()
728                 self.best_score_ = results["mean_test_%s" % refit_metric][
729                                            self.best_index_]
730             self.best_params_ = results["params"][self.best_index_]
731 
732         if self.refit:
733             self.best_estimator_ = clone(base_estimator).set_params(
734                 **self.best_params_)
735             refit_start_time = time.time()
736             if y is not None:
737                 self.best_estimator_.fit(X, y, **fit_params)
738             else:
739                 self.best_estimator_.fit(X, **fit_params)
740             refit_end_time = time.time()
741             self.refit_time_ = refit_end_time - refit_start_time
742 
743         # Store the only scorer not as a dict for single metric evaluation
744         self.scorer_ = scorers if self.multimetric_ else scorers['score']
745 
746         self.cv_results_ = results
747         self.n_splits_ = n_splits
748 
749         return self

</code><intended_behavior>Ensure that the `fit` method or associated methods involved in the fitting process accurately clone estimator parameters at the start of the process.</intended_behavior>, <file>sklearn/model_selection/_search.py</file>
<class>BaseSearchCV</class>
<method>None</method>
<code>
400 class BaseSearchCV(MetaEstimatorMixin, BaseEstimator, metaclass=ABCMeta):
401     """Abstract base class for hyper parameter search with cross-validation.
402     """
403 
404     @abstractmethod
405     def __init__(self, estimator, scoring=None, n_jobs=None, iid='deprecated',
406                  refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',
407                  error_score=np.nan, return_train_score=True):
408 
409         self.scoring = scoring
410         self.estimator = estimator
411         self.n_jobs = n_jobs
412         self.iid = iid
413         self.refit = refit
414         self.cv = cv
415         self.verbose = verbose
416         self.pre_dispatch = pre_dispatch
417         self.error_score = error_score
418         self.return_train_score = return_train_score
419 
420     @property
421     def _estimator_type(self):
422         return self.estimator._estimator_type
423 
424     def score(self, X, y=None):
425         """Returns the score on the given data, if the estimator has been refit.
426 
427         This uses the score defined by ``scoring`` where provided, and the
428         ``best_estimator_.score`` method otherwise.
429 
430         Parameters
431         ----------
432         X : array-like, shape = [n_samples, n_features]
433             Input data, where n_samples is the number of samples and
434             n_features is the number of features.
435 
436         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
437             Target relative to X for classification or regression;
438             None for unsupervised learning.
439 
440         Returns
441         -------
442         score : float
443         """
444         self._check_is_fitted('score')
445         if self.scorer_ is None:
446             raise ValueError("No score function explicitly defined, "
447                              "and the estimator doesn't provide one %s"
448                              % self.best_estimator_)
449         score = self.scorer_[self.refit] if self.multimetric_ else self.scorer_
450         return score(self.best_estimator_, X, y)
451 
452     def _check_is_fitted(self, method_name):
453         if not self.refit:
454             raise NotFittedError('This %s instance was initialized '
455                                  'with refit=False. %s is '
456                                  'available only after refitting on the best '
457                                  'parameters. You can refit an estimator '
458                                  'manually using the ``best_params_`` '
459                                  'attribute'
460                                  % (type(self).__name__, method_name))
461         else:
462             check_is_fitted(self)
463 
464     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
465     def predict(self, X):
466         """Call predict on the estimator with the best found parameters.
467 
468         Only available if ``refit=True`` and the underlying estimator supports
469         ``predict``.
470 
471         Parameters
472         ----------
473         X : indexable, length n_samples
474             Must fulfill the input assumptions of the
475             underlying estimator.
476 
477         """
478         self._check_is_fitted('predict')
479         return self.best_estimator_.predict(X)
480 
481     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
482     def predict_proba(self, X):
483         """Call predict_proba on the estimator with the best found parameters.
484 
485         Only available if ``refit=True`` and the underlying estimator supports
486         ``predict_proba``.
487 
488         Parameters
489         ----------
490         X : indexable, length n_samples
491             Must fulfill the input assumptions of the
492             underlying estimator.
493 
494         """
495         self._check_is_fitted('predict_proba')
496         return self.best_estimator_.predict_proba(X)
497 
498     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
499     def predict_log_proba(self, X):
500         """Call predict_log_proba on the estimator with the best found parameters.
501 
502         Only available if ``refit=True`` and the underlying estimator supports
503         ``predict_log_proba``.
504 
505         Parameters
506         ----------
507         X : indexable, length n_samples
508             Must fulfill the input assumptions of the
509             underlying estimator.
510 
511         """
512         self._check_is_fitted('predict_log_proba')
513         return self.best_estimator_.predict_log_proba(X)
514 
515     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
516     def decision_function(self, X):
517         """Call decision_function on the estimator with the best found parameters.
518 
519         Only available if ``refit=True`` and the underlying estimator supports
520         ``decision_function``.
521 
522         Parameters
523         ----------
524         X : indexable, length n_samples
525             Must fulfill the input assumptions of the
526             underlying estimator.
527 
528         """
529         self._check_is_fitted('decision_function')
530         return self.best_estimator_.decision_function(X)
531 
532     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
533     def transform(self, X):
534         """Call transform on the estimator with the best found parameters.
535 
536         Only available if the underlying estimator supports ``transform`` and
537         ``refit=True``.
538 
539         Parameters
540         ----------
541         X : indexable, length n_samples
542             Must fulfill the input assumptions of the
543             underlying estimator.
544 
545         """
546         self._check_is_fitted('transform')
547         return self.best_estimator_.transform(X)
548 
549     @if_delegate_has_method(delegate=('best_estimator_', 'estimator'))
550     def inverse_transform(self, Xt):
551         """Call inverse_transform on the estimator with the best found params.
552 
553         Only available if the underlying estimator implements
554         ``inverse_transform`` and ``refit=True``.
555 
556         Parameters
557         ----------
558         Xt : indexable, length n_samples
559             Must fulfill the input assumptions of the
560             underlying estimator.
561 
562         """
563         self._check_is_fitted('inverse_transform')
564         return self.best_estimator_.inverse_transform(Xt)
565 
566     @property
567     def classes_(self):
568         self._check_is_fitted("classes_")
569         return self.best_estimator_.classes_
570 
571     def _run_search(self, evaluate_candidates):
572         """Repeatedly calls `evaluate_candidates` to conduct a search.
573 
574         This method, implemented in sub-classes, makes it possible to
575         customize the the scheduling of evaluations: GridSearchCV and
576         RandomizedSearchCV schedule evaluations for their whole parameter
577         search space at once but other more sequential approaches are also
578         possible: for instance is possible to iteratively schedule evaluations
579         for new regions of the parameter search space based on previously
580         collected evaluation results. This makes it possible to implement
581         Bayesian optimization or more generally sequential model-based
582         optimization by deriving from the BaseSearchCV abstract base class.
583 
584         Parameters
585         ----------
586         evaluate_candidates : callable
587             This callback accepts a list of candidates, where each candidate is
588             a dict of parameter settings. It returns a dict of all results so
589             far, formatted like ``cv_results_``.
590 
591         Examples
592         --------
593 
594         ::
595 
596             def _run_search(self, evaluate_candidates):
597                 'Try C=0.1 only if C=1 is better than C=10'
598                 all_results = evaluate_candidates([{'C': 1}, {'C': 10}])
599                 score = all_results['mean_test_score']
600                 if score[0] < score[1]:
601                     evaluate_candidates([{'C': 0.1}])
602         """
603         raise NotImplementedError("_run_search not implemented.")
604 
605     def fit(self, X, y=None, groups=None, **fit_params):
606         """Run fit with all sets of parameters.
607 
608         Parameters
609         ----------
610 
611         X : array-like, shape = [n_samples, n_features]
612             Training vector, where n_samples is the number of samples and
613             n_features is the number of features.
614 
615         y : array-like, shape = [n_samples] or [n_samples, n_output], optional
616             Target relative to X for classification or regression;
617             None for unsupervised learning.
618 
619         groups : array-like, with shape (n_samples,), optional
620             Group labels for the samples used while splitting the dataset into
621             train/test set. Only used in conjunction with a "Group" :term:`cv`
622             instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).
623 
624         **fit_params : dict of string -> object
625             Parameters passed to the ``fit`` method of the estimator
626         """
627         estimator = self.estimator
628         cv = check_cv(self.cv, y, classifier=is_classifier(estimator))
629 
630         scorers, self.multimetric_ = _check_multimetric_scoring(
631             self.estimator, scoring=self.scoring)
632 
633         if self.multimetric_:
634             if self.refit is not False and (
635                     not isinstance(self.refit, str) or
636                     # This will work for both dict / list (tuple)
637                     self.refit not in scorers) and not callable(self.refit):
638                 raise ValueError("For multi-metric scoring, the parameter "
639                                  "refit must be set to a scorer key or a "
640                                  "callable to refit an estimator with the "
641                                  "best parameter setting on the whole "
642                                  "data and make the best_* attributes "
643                                  "available for that metric. If this is "
644                                  "not needed, refit should be set to "
645                                  "False explicitly. %r was passed."
646                                  % self.refit)
647             else:
648                 refit_metric = self.refit
649         else:
650             refit_metric = 'score'
651 
652         X, y, groups = indexable(X, y, groups)
653         n_splits = cv.get_n_splits(X, y, groups)
654 
655         base_estimator = clone(self.estimator)
656 
657         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
658                             pre_dispatch=self.pre_dispatch)
659 
660         fit_and_score_kwargs = dict(scorer=scorers,
661                                     fit_params=fit_params,
662                                     return_train_score=self.return_train_score,
663                                     return_n_test_samples=True,
664                                     return_times=True,
665                                     return_parameters=False,
666                                     error_score=self.error_score,
667                                     verbose=self.verbose)
668         results = {}
669         with parallel:
670             all_candidate_params = []
671             all_out = []
672 
673             def evaluate_candidates(candidate_params):
674                 candidate_params = list(candidate_params)
675                 n_candidates = len(candidate_params)
676 
677                 if self.verbose > 0:
678                     print("Fitting {0} folds for each of {1} candidates,"
679                           " totalling {2} fits".format(
680                               n_splits, n_candidates, n_candidates * n_splits))
681 
682                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),
683                                                        X, y,
684                                                        train=train, test=test,
685                                                        parameters=parameters,
686                                                        **fit_and_score_kwargs)
687                                for parameters, (train, test)
688                                in product(candidate_params,
689                                           cv.split(X, y, groups)))
690 
691                 if len(out) < 1:
692                     raise ValueError('No fits were performed. '
693                                      'Was the CV iterator empty? '
694                                      'Were there no candidates?')
695                 elif len(out) != n_candidates * n_splits:
696                     raise ValueError('cv.split and cv.get_n_splits returned '
697                                      'inconsistent results. Expected {} '
698                                      'splits, got {}'
699                                      .format(n_splits,
700                                              len(out) // n_candidates))
701 
702                 all_candidate_params.extend(candidate_params)
703                 all_out.extend(out)
704 
705                 nonlocal results
706                 results = self._format_results(
707                     all_candidate_params, scorers, n_splits, all_out)
708                 return results
709 
710             self._run_search(evaluate_candidates)
711 
712         # For multi-metric evaluation, store the best_index_, best_params_ and
713         # best_score_ iff refit is one of the scorer names
714         # In single metric evaluation, refit_metric is "score"
715         if self.refit or not self.multimetric_:
716             # If callable, refit is expected to return the index of the best
717             # parameter set.
718             if callable(self.refit):
719                 self.best_index_ = self.refit(results)
720                 if not isinstance(self.best_index_, numbers.Integral):
721                     raise TypeError('best_index_ returned is not an integer')
722                 if (self.best_index_ < 0 or
723                    self.best_index_ >= len(results["params"])):
724                     raise IndexError('best_index_ index out of range')
725             else:
726                 self.best_index_ = results["rank_test_%s"
727                                            % refit_metric].argmin()
728                 self.best_score_ = results["mean_test_%s" % refit_metric][
729                                            self.best_index_]
730             self.best_params_ = results["params"][self.best_index_]
731 
732         if self.refit:
733             self.best_estimator_ = clone(base_estimator).set_params(
734                 **self.best_params_)
735             refit_start_time = time.time()
736             if y is not None:
737                 self.best_estimator_.fit(X, y, **fit_params)
738             else:
739                 self.best_estimator_.fit(X, **fit_params)
740             refit_end_time = time.time()
741             self.refit_time_ = refit_end_time - refit_start_time
742 
743         # Store the only scorer not as a dict for single metric evaluation
744         self.scorer_ = scorers if self.multimetric_ else scorers['score']
745 
746         self.cv_results_ = results
747         self.n_splits_ = n_splits
748 
749         return self
750 
751     def _format_results(self, candidate_params, scorers, n_splits, out):
752         n_candidates = len(candidate_params)
753 
754         # if one choose to see train score, "out" will contain train score info
755         if self.return_train_score:
756             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,
757              score_time) = zip(*out)
758         else:
759             (test_score_dicts, test_sample_counts, fit_time,
760              score_time) = zip(*out)
761 
762         # test_score_dicts and train_score dicts are lists of dictionaries and
763         # we make them into dict of lists
764         test_scores = _aggregate_score_dicts(test_score_dicts)
765         if self.return_train_score:
766             train_scores = _aggregate_score_dicts(train_score_dicts)
767 
768         results = {}
769 
770         def _store(key_name, array, weights=None, splits=False, rank=False):
771             """A small helper to store the scores/times to the cv_results_"""
772             # When iterated first by splits, then by parameters
773             # We want `array` to have `n_candidates` rows and `n_splits` cols.
774             array = np.array(array, dtype=np.float64).reshape(n_candidates,
775                                                               n_splits)
776             if splits:
777                 for split_i in range(n_splits):
778                     # Uses closure to alter the results
779                     results["split%d_%s"
780                             % (split_i, key_name)] = array[:, split_i]
781 
782             array_means = np.average(array, axis=1, weights=weights)
783             results['mean_%s' % key_name] = array_means
784             # Weighted std is not directly available in numpy
785             array_stds = np.sqrt(np.average((array -
786                                              array_means[:, np.newaxis]) ** 2,
787                                             axis=1, weights=weights))
788             results['std_%s' % key_name] = array_stds
789 
790             if rank:
791                 results["rank_%s" % key_name] = np.asarray(
792                     rankdata(-array_means, method='min'), dtype=np.int32)
793 
794         _store('fit_time', fit_time)
795         _store('score_time', score_time)
796         # Use one MaskedArray and mask all the places where the param is not
797         # applicable for that candidate. Use defaultdict as each candidate may
798         # not contain all the params
799         param_results = defaultdict(partial(MaskedArray,
800                                             np.empty(n_candidates,),
801                                             mask=True,
802                                             dtype=object))
803         for cand_i, params in enumerate(candidate_params):
804             for name, value in params.items():
805                 # An all masked empty array gets created for the key
806                 # `"param_%s" % name` at the first occurrence of `name`.
807                 # Setting the value at an index also unmasks that index
808                 param_results["param_%s" % name][cand_i] = value
809 
810         results.update(param_results)
811         # Store a list of param dicts at the key 'params'
812         results['params'] = candidate_params
813 
814         # NOTE test_sample counts (weights) remain the same for all candidates
815         test_sample_counts = np.array(test_sample_counts[:n_splits],
816                                       dtype=np.int)
817 
818         if self.iid != 'deprecated':
819             warnings.warn(
820                 "The parameter 'iid' is deprecated in 0.22 and will be "
821                 "removed in 0.24.", DeprecationWarning
822             )
823             iid = self.iid
824         else:
825             iid = False
826 
827         for scorer_name in scorers.keys():
828             # Computed the (weighted) mean and std for test scores alone
829             _store('test_%s' % scorer_name, test_scores[scorer_name],
830                    splits=True, rank=True,
831                    weights=test_sample_counts if iid else None)
832             if self.return_train_score:
833                 _store('train_%s' % scorer_name, train_scores[scorer_name],
834                        splits=True)
835 
836         return results

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 05:55:28.397 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 05:55:28.397 | INFO     | Search-only: skipping patch selection
2025-10-18 05:55:28.487 | INFO     | Task scikit-learn__scikit-learn-15096 completed successfully.
2025-10-18 05:55:28.487 | INFO     | No patch generated. You can try running ACR again.
