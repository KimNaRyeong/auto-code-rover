2025-10-18 00:41:35.971 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 00:41:40.608 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=258, output_tokens=12, cost=0.002940
2025-10-18 00:41:55.689 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=438, output_tokens=625, cost=0.023130
2025-10-18 00:41:56.836 | INFO     | registered non reproducing test 0
2025-10-18 00:42:10.207 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1591, output_tokens=603, cost=0.034000
2025-10-18 00:42:11.420 | INFO     | registered non reproducing test 1
2025-10-18 00:42:26.894 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2550, output_tokens=757, cost=0.048210
2025-10-18 00:42:28.049 | INFO     | registered non reproducing test 2
2025-10-18 00:42:28.049 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 00:42:28.056 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 00:42:36.222 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=554, output_tokens=322, cost=0.015200
2025-10-18 00:42:36.229 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 00:42:38.150 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=676, output_tokens=57, cost=0.008470
2025-10-18 00:42:38.151 | DEBUG    | Extracted a valid json.
2025-10-18 00:42:38.151 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Row\")",
        "search_method_in_class(\"__reduce__\", \"Row\")",
        "search_method(\"values_list\")",
        "search_code(\"namedtuple\")"
    ],
    "bug_locations": []
}
2025-10-18 00:42:38.248 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 00:42:38.249 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 00:42:56.713 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1901, output_tokens=777, cost=0.042320
2025-10-18 00:42:56.724 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 00:43:01.343 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2894, output_tokens=179, cost=0.034310
2025-10-18 00:43:01.347 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 00:43:06.751 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=533, output_tokens=198, cost=0.011270
2025-10-18 00:43:06.752 | DEBUG    | Extracted a valid json.
2025-10-18 00:43:06.758 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "values_list",
            "intended_behavior": "The `values_list` method should ensure that the dynamically generated `namedtuple` instances, referred to as `Row` when `named=True`, are pickleable. This would likely involve adding or ensuring that the `__reduce__` method is correctly implemented for these `namedtuple` instances. The implementation must ensure that these instances can be serialized and deserialized without losing functionality, thereby allowing these objects to be used with caching mechanisms like `cacheops`. This should be done without affecting the current functionality of returning a queryset of `namedtuple` instances for improved user accessibility and performance. The fix may involve modifying the way `namedtuple` classes are dynamically created or augmenting them with additional methods to support pickling."
        }
    ]
}
2025-10-18 00:43:06.770 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>values_list</method>
<code>
836     def values_list(self, *fields, flat=False, named=False):
837         if flat and named:
838             raise TypeError("'flat' and 'named' can't be used together.")
839         if flat and len(fields) > 1:
840             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
841 
842         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
843         _fields = []
844         expressions = {}
845         counter = 1
846         for field in fields:
847             if hasattr(field, 'resolve_expression'):
848                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
849                 while True:
850                     field_id = field_id_prefix + str(counter)
851                     counter += 1
852                     if field_id not in field_names:
853                         break
854                 expressions[field_id] = field
855                 _fields.append(field_id)
856             else:
857                 _fields.append(field)
858 
859         clone = self._values(*_fields, **expressions)
860         clone._iterable_class = (
861             NamedValuesListIterable if named
862             else FlatValuesListIterable if flat
863             else ValuesListIterable
864         )
865         return clone

</code><intended_behavior>The `values_list` method should ensure that the dynamically generated `namedtuple` instances, referred to as `Row` when `named=True`, are pickleable. This would likely involve adding or ensuring that the `__reduce__` method is correctly implemented for these `namedtuple` instances. The implementation must ensure that these instances can be serialized and deserialized without losing functionality, thereby allowing these objects to be used with caching mechanisms like `cacheops`. This should be done without affecting the current functionality of returning a queryset of `namedtuple` instances for improved user accessibility and performance. The fix may involve modifying the way `namedtuple` classes are dynamically created or augmenting them with additional methods to support pickling.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, args, kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         if value.values_select:
214             self._iterable_class = ValuesIterable
215         self._query = value
216 
217     def as_manager(cls):
218         # Address the circular dependency between `Queryset` and `Manager`.
219         from django.db.models.manager import Manager
220         manager = Manager.from_queryset(cls)()
221         manager._built_with_as_manager = True
222         return manager
223     as_manager.queryset_only = True
224     as_manager = classmethod(as_manager)
225 
226     ########################
227     # PYTHON MAGIC METHODS #
228     ########################
229 
230     def __deepcopy__(self, memo):
231         """Don't populate the QuerySet's cache."""
232         obj = self.__class__()
233         for k, v in self.__dict__.items():
234             if k == '_result_cache':
235                 obj.__dict__[k] = None
236             else:
237                 obj.__dict__[k] = copy.deepcopy(v, memo)
238         return obj
239 
240     def __getstate__(self):
241         # Force the cache to be fully populated.
242         self._fetch_all()
243         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
244 
245     def __setstate__(self, state):
246         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
247         if pickled_version:
248             if pickled_version != django.__version__:
249                 warnings.warn(
250                     "Pickled queryset instance's Django version %s does not "
251                     "match the current version %s."
252                     % (pickled_version, django.__version__),
253                     RuntimeWarning,
254                     stacklevel=2,
255                 )
256         else:
257             warnings.warn(
258                 "Pickled queryset instance's Django version is not specified.",
259                 RuntimeWarning,
260                 stacklevel=2,
261             )
262         self.__dict__.update(state)
263 
264     def __repr__(self):
265         data = list(self[:REPR_OUTPUT_SIZE + 1])
266         if len(data) > REPR_OUTPUT_SIZE:
267             data[-1] = "...(remaining elements truncated)..."
268         return '<%s %r>' % (self.__class__.__name__, data)
269 
270     def __len__(self):
271         self._fetch_all()
272         return len(self._result_cache)
273 
274     def __iter__(self):
275         """
276         The queryset iterator protocol uses three nested iterators in the
277         default case:
278             1. sql.compiler.execute_sql()
279                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
280                  using cursor.fetchmany(). This part is responsible for
281                  doing some column masking, and returning the rows in chunks.
282             2. sql.compiler.results_iter()
283                - Returns one row at time. At this point the rows are still just
284                  tuples. In some cases the return values are converted to
285                  Python values at this location.
286             3. self.iterator()
287                - Responsible for turning the rows into model objects.
288         """
289         self._fetch_all()
290         return iter(self._result_cache)
291 
292     def __bool__(self):
293         self._fetch_all()
294         return bool(self._result_cache)
295 
296     def __getitem__(self, k):
297         """Retrieve an item or slice from the set of results."""
298         if not isinstance(k, (int, slice)):
299             raise TypeError(
300                 'QuerySet indices must be integers or slices, not %s.'
301                 % type(k).__name__
302             )
303         assert ((not isinstance(k, slice) and (k >= 0)) or
304                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
305                  (k.stop is None or k.stop >= 0))), \
306             "Negative indexing is not supported."
307 
308         if self._result_cache is not None:
309             return self._result_cache[k]
310 
311         if isinstance(k, slice):
312             qs = self._chain()
313             if k.start is not None:
314                 start = int(k.start)
315             else:
316                 start = None
317             if k.stop is not None:
318                 stop = int(k.stop)
319             else:
320                 stop = None
321             qs.query.set_limits(start, stop)
322             return list(qs)[::k.step] if k.step else qs
323 
324         qs = self._chain()
325         qs.query.set_limits(k, k + 1)
326         qs._fetch_all()
327         return qs._result_cache[0]
328 
329     def __class_getitem__(cls, *args, **kwargs):
330         return cls
331 
332     def __and__(self, other):
333         self._merge_sanity_check(other)
334         if isinstance(other, EmptyQuerySet):
335             return other
336         if isinstance(self, EmptyQuerySet):
337             return self
338         combined = self._chain()
339         combined._merge_known_related_objects(other)
340         combined.query.combine(other.query, sql.AND)
341         return combined
342 
343     def __or__(self, other):
344         self._merge_sanity_check(other)
345         if isinstance(self, EmptyQuerySet):
346             return other
347         if isinstance(other, EmptyQuerySet):
348             return self
349         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
350         combined = query._chain()
351         combined._merge_known_related_objects(other)
352         if not other.query.can_filter():
353             other = other.model._base_manager.filter(pk__in=other.values('pk'))
354         combined.query.combine(other.query, sql.OR)
355         return combined
356 
357     ####################################
358     # METHODS THAT DO DATABASE QUERIES #
359     ####################################
360 
361     def _iterator(self, use_chunked_fetch, chunk_size):
362         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
363 
364     def iterator(self, chunk_size=2000):
365         """
366         An iterator over the results from applying this QuerySet to the
367         database.
368         """
369         if chunk_size <= 0:
370             raise ValueError('Chunk size must be strictly positive.')
371         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
372         return self._iterator(use_chunked_fetch, chunk_size)
373 
374     def aggregate(self, *args, **kwargs):
375         """
376         Return a dictionary containing the calculations (aggregation)
377         over the current queryset.
378 
379         If args is present the expression is passed as a kwarg using
380         the Aggregate object's default alias.
381         """
382         if self.query.distinct_fields:
383             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
384         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
385         for arg in args:
386             # The default_alias property raises TypeError if default_alias
387             # can't be set automatically or AttributeError if it isn't an
388             # attribute.
389             try:
390                 arg.default_alias
391             except (AttributeError, TypeError):
392                 raise TypeError("Complex aggregates require an alias")
393             kwargs[arg.default_alias] = arg
394 
395         query = self.query.chain()
396         for (alias, aggregate_expr) in kwargs.items():
397             query.add_annotation(aggregate_expr, alias, is_summary=True)
398             if not query.annotations[alias].contains_aggregate:
399                 raise TypeError("%s is not an aggregate expression" % alias)
400         return query.get_aggregation(self.db, kwargs)
401 
402     def count(self):
403         """
404         Perform a SELECT COUNT() and return the number of records as an
405         integer.
406 
407         If the QuerySet is already fully cached, return the length of the
408         cached results set to avoid multiple SELECT COUNT(*) calls.
409         """
410         if self._result_cache is not None:
411             return len(self._result_cache)
412 
413         return self.query.get_count(using=self.db)
414 
415     def get(self, *args, **kwargs):
416         """
417         Perform the query and return a single object matching the given
418         keyword arguments.
419         """
420         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
421         if self.query.can_filter() and not self.query.distinct_fields:
422             clone = clone.order_by()
423         limit = None
424         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
425             limit = MAX_GET_RESULTS
426             clone.query.set_limits(high=limit)
427         num = len(clone)
428         if num == 1:
429             return clone._result_cache[0]
430         if not num:
431             raise self.model.DoesNotExist(
432                 "%s matching query does not exist." %
433                 self.model._meta.object_name
434             )
435         raise self.model.MultipleObjectsReturned(
436             'get() returned more than one %s -- it returned %s!' % (
437                 self.model._meta.object_name,
438                 num if not limit or num < limit else 'more than %s' % (limit - 1),
439             )
440         )
441 
442     def create(self, **kwargs):
443         """
444         Create a new object with the given kwargs, saving it to the database
445         and returning the created object.
446         """
447         obj = self.model(**kwargs)
448         self._for_write = True
449         obj.save(force_insert=True, using=self.db)
450         return obj
451 
452     def _populate_pk_values(self, objs):
453         for obj in objs:
454             if obj.pk is None:
455                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
456 
457     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
458         """
459         Insert each of the instances into the database. Do *not* call
460         save() on each of the instances, do not send any pre/post_save
461         signals, and do not set the primary key attribute if it is an
462         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
463         Multi-table models are not supported.
464         """
465         # When you bulk insert you don't get the primary keys back (if it's an
466         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
467         # you can't insert into the child tables which references this. There
468         # are two workarounds:
469         # 1) This could be implemented if you didn't have an autoincrement pk
470         # 2) You could do it by doing O(n) normal inserts into the parent
471         #    tables to get the primary keys back and then doing a single bulk
472         #    insert into the childmost table.
473         # We currently set the primary keys on the objects when using
474         # PostgreSQL via the RETURNING ID clause. It should be possible for
475         # Oracle as well, but the semantics for extracting the primary keys is
476         # trickier so it's not done yet.
477         assert batch_size is None or batch_size > 0
478         # Check that the parents share the same concrete model with the our
479         # model to detect the inheritance pattern ConcreteGrandParent ->
480         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
481         # would not identify that case as involving multiple tables.
482         for parent in self.model._meta.get_parent_list():
483             if parent._meta.concrete_model is not self.model._meta.concrete_model:
484                 raise ValueError("Can't bulk create a multi-table inherited model")
485         if not objs:
486             return objs
487         self._for_write = True
488         connection = connections[self.db]
489         opts = self.model._meta
490         fields = opts.concrete_fields
491         objs = list(objs)
492         self._populate_pk_values(objs)
493         with transaction.atomic(using=self.db, savepoint=False):
494             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
495             if objs_with_pk:
496                 returned_columns = self._batched_insert(
497                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
498                 )
499                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
500                     for result, field in zip(results, opts.db_returning_fields):
501                         if field != opts.pk:
502                             setattr(obj_with_pk, field.attname, result)
503                 for obj_with_pk in objs_with_pk:
504                     obj_with_pk._state.adding = False
505                     obj_with_pk._state.db = self.db
506             if objs_without_pk:
507                 fields = [f for f in fields if not isinstance(f, AutoField)]
508                 returned_columns = self._batched_insert(
509                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
510                 )
511                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
512                     assert len(returned_columns) == len(objs_without_pk)
513                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
514                     for result, field in zip(results, opts.db_returning_fields):
515                         setattr(obj_without_pk, field.attname, result)
516                     obj_without_pk._state.adding = False
517                     obj_without_pk._state.db = self.db
518 
519         return objs
520 
521     def bulk_update(self, objs, fields, batch_size=None):
522         """
523         Update the given fields in each of the given objects in the database.
524         """
525         if batch_size is not None and batch_size < 0:
526             raise ValueError('Batch size must be a positive integer.')
527         if not fields:
528             raise ValueError('Field names must be given to bulk_update().')
529         objs = tuple(objs)
530         if any(obj.pk is None for obj in objs):
531             raise ValueError('All bulk_update() objects must have a primary key set.')
532         fields = [self.model._meta.get_field(name) for name in fields]
533         if any(not f.concrete or f.many_to_many for f in fields):
534             raise ValueError('bulk_update() can only be used with concrete fields.')
535         if any(f.primary_key for f in fields):
536             raise ValueError('bulk_update() cannot be used with primary key fields.')
537         if not objs:
538             return
539         # PK is used twice in the resulting update query, once in the filter
540         # and once in the WHEN. Each field will also have one CAST.
541         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
542         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
543         requires_casting = connections[self.db].features.requires_casted_case_in_updates
544         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
545         updates = []
546         for batch_objs in batches:
547             update_kwargs = {}
548             for field in fields:
549                 when_statements = []
550                 for obj in batch_objs:
551                     attr = getattr(obj, field.attname)
552                     if not isinstance(attr, Expression):
553                         attr = Value(attr, output_field=field)
554                     when_statements.append(When(pk=obj.pk, then=attr))
555                 case_statement = Case(*when_statements, output_field=field)
556                 if requires_casting:
557                     case_statement = Cast(case_statement, output_field=field)
558                 update_kwargs[field.attname] = case_statement
559             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
560         with transaction.atomic(using=self.db, savepoint=False):
561             for pks, update_kwargs in updates:
562                 self.filter(pk__in=pks).update(**update_kwargs)
563     bulk_update.alters_data = True
564 
565     def get_or_create(self, defaults=None, **kwargs):
566         """
567         Look up an object with the given kwargs, creating one if necessary.
568         Return a tuple of (object, created), where created is a boolean
569         specifying whether an object was created.
570         """
571         # The get() needs to be targeted at the write database in order
572         # to avoid potential transaction consistency problems.
573         self._for_write = True
574         try:
575             return self.get(**kwargs), False
576         except self.model.DoesNotExist:
577             params = self._extract_model_params(defaults, **kwargs)
578             # Try to create an object using passed params.
579             try:
580                 with transaction.atomic(using=self.db):
581                     params = dict(resolve_callables(params))
582                     return self.create(**params), True
583             except IntegrityError:
584                 try:
585                     return self.get(**kwargs), False
586                 except self.model.DoesNotExist:
587                     pass
588                 raise
589 
590     def update_or_create(self, defaults=None, **kwargs):
591         """
592         Look up an object with the given kwargs, updating one with defaults
593         if it exists, otherwise create a new one.
594         Return a tuple (object, created), where created is a boolean
595         specifying whether an object was created.
596         """
597         defaults = defaults or {}
598         self._for_write = True
599         with transaction.atomic(using=self.db):
600             # Lock the row so that a concurrent update is blocked until
601             # update_or_create() has performed its save.
602             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
603             if created:
604                 return obj, created
605             for k, v in resolve_callables(defaults):
606                 setattr(obj, k, v)
607             obj.save(using=self.db)
608         return obj, False
609 
610     def _extract_model_params(self, defaults, **kwargs):
611         """
612         Prepare `params` for creating a model instance based on the given
613         kwargs; for use by get_or_create().
614         """
615         defaults = defaults or {}
616         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
617         params.update(defaults)
618         property_names = self.model._meta._property_names
619         invalid_params = []
620         for param in params:
621             try:
622                 self.model._meta.get_field(param)
623             except exceptions.FieldDoesNotExist:
624                 # It's okay to use a model's property if it has a setter.
625                 if not (param in property_names and getattr(self.model, param).fset):
626                     invalid_params.append(param)
627         if invalid_params:
628             raise exceptions.FieldError(
629                 "Invalid field name(s) for model %s: '%s'." % (
630                     self.model._meta.object_name,
631                     "', '".join(sorted(invalid_params)),
632                 ))
633         return params
634 
635     def _earliest(self, *fields):
636         """
637         Return the earliest object according to fields (if given) or by the
638         model's Meta.get_latest_by.
639         """
640         if fields:
641             order_by = fields
642         else:
643             order_by = getattr(self.model._meta, 'get_latest_by')
644             if order_by and not isinstance(order_by, (tuple, list)):
645                 order_by = (order_by,)
646         if order_by is None:
647             raise ValueError(
648                 "earliest() and latest() require either fields as positional "
649                 "arguments or 'get_latest_by' in the model's Meta."
650             )
651 
652         assert not self.query.is_sliced, \
653             "Cannot change a query once a slice has been taken."
654         obj = self._chain()
655         obj.query.set_limits(high=1)
656         obj.query.clear_ordering(force_empty=True)
657         obj.query.add_ordering(*order_by)
658         return obj.get()
659 
660     def earliest(self, *fields):
661         return self._earliest(*fields)
662 
663     def latest(self, *fields):
664         return self.reverse()._earliest(*fields)
665 
666     def first(self):
667         """Return the first object of a query or None if no match is found."""
668         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
669             return obj
670 
671     def last(self):
672         """Return the last object of a query or None if no match is found."""
673         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
674             return obj
675 
676     def in_bulk(self, id_list=None, *, field_name='pk'):
677         """
678         Return a dictionary mapping each of the given IDs to the object with
679         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
680         """
681         assert not self.query.is_sliced, \
682             "Cannot use 'limit' or 'offset' with in_bulk"
683         opts = self.model._meta
684         unique_fields = [
685             constraint.fields[0]
686             for constraint in opts.total_unique_constraints
687             if len(constraint.fields) == 1
688         ]
689         if (
690             field_name != 'pk' and
691             not opts.get_field(field_name).unique and
692             field_name not in unique_fields and
693             not self.query.distinct_fields == (field_name,)
694         ):
695             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
696         if id_list is not None:
697             if not id_list:
698                 return {}
699             filter_key = '{}__in'.format(field_name)
700             batch_size = connections[self.db].features.max_query_params
701             id_list = tuple(id_list)
702             # If the database has a limit on the number of query parameters
703             # (e.g. SQLite), retrieve objects in batches if necessary.
704             if batch_size and batch_size < len(id_list):
705                 qs = ()
706                 for offset in range(0, len(id_list), batch_size):
707                     batch = id_list[offset:offset + batch_size]
708                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
709             else:
710                 qs = self.filter(**{filter_key: id_list}).order_by()
711         else:
712             qs = self._chain()
713         return {getattr(obj, field_name): obj for obj in qs}
714 
715     def delete(self):
716         """Delete the records in the current QuerySet."""
717         self._not_support_combined_queries('delete')
718         assert not self.query.is_sliced, \
719             "Cannot use 'limit' or 'offset' with delete."
720 
721         if self._fields is not None:
722             raise TypeError("Cannot call delete() after .values() or .values_list()")
723 
724         del_query = self._chain()
725 
726         # The delete is actually 2 queries - one to find related objects,
727         # and one to delete. Make sure that the discovery of related
728         # objects is performed on the same database as the deletion.
729         del_query._for_write = True
730 
731         # Disable non-supported fields.
732         del_query.query.select_for_update = False
733         del_query.query.select_related = False
734         del_query.query.clear_ordering(force_empty=True)
735 
736         collector = Collector(using=del_query.db)
737         collector.collect(del_query)
738         deleted, _rows_count = collector.delete()
739 
740         # Clear the result cache, in case this QuerySet gets reused.
741         self._result_cache = None
742         return deleted, _rows_count
743 
744     delete.alters_data = True
745     delete.queryset_only = True
746 
747     def _raw_delete(self, using):
748         """
749         Delete objects found from the given queryset in single direct SQL
750         query. No signals are sent and there is no protection for cascades.
751         """
752         query = self.query.clone()
753         query.__class__ = sql.DeleteQuery
754         cursor = query.get_compiler(using).execute_sql(CURSOR)
755         if cursor:
756             with cursor:
757                 return cursor.rowcount
758         return 0
759     _raw_delete.alters_data = True
760 
761     def update(self, **kwargs):
762         """
763         Update all elements in the current QuerySet, setting all the given
764         fields to the appropriate values.
765         """
766         self._not_support_combined_queries('update')
767         assert not self.query.is_sliced, \
768             "Cannot update a query once a slice has been taken."
769         self._for_write = True
770         query = self.query.chain(sql.UpdateQuery)
771         query.add_update_values(kwargs)
772         # Clear any annotations so that they won't be present in subqueries.
773         query.annotations = {}
774         with transaction.mark_for_rollback_on_error(using=self.db):
775             rows = query.get_compiler(self.db).execute_sql(CURSOR)
776         self._result_cache = None
777         return rows
778     update.alters_data = True
779 
780     def _update(self, values):
781         """
782         A version of update() that accepts field objects instead of field names.
783         Used primarily for model saving and not intended for use by general
784         code (it requires too much poking around at model internals to be
785         useful at that level).
786         """
787         assert not self.query.is_sliced, \
788             "Cannot update a query once a slice has been taken."
789         query = self.query.chain(sql.UpdateQuery)
790         query.add_update_fields(values)
791         # Clear any annotations so that they won't be present in subqueries.
792         query.annotations = {}
793         self._result_cache = None
794         return query.get_compiler(self.db).execute_sql(CURSOR)
795     _update.alters_data = True
796     _update.queryset_only = False
797 
798     def exists(self):
799         if self._result_cache is None:
800             return self.query.has_results(using=self.db)
801         return bool(self._result_cache)
802 
803     def _prefetch_related_objects(self):
804         # This method can only be called once the result cache has been filled.
805         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
806         self._prefetch_done = True
807 
808     def explain(self, *, format=None, **options):
809         return self.query.explain(using=self.db, format=format, **options)
810 
811     ##################################################
812     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
813     ##################################################
814 
815     def raw(self, raw_query, params=None, translations=None, using=None):
816         if using is None:
817             using = self.db
818         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
819         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
820         return qs
821 
822     def _values(self, *fields, **expressions):
823         clone = self._chain()
824         if expressions:
825             clone = clone.annotate(**expressions)
826         clone._fields = fields
827         clone.query.set_values(fields)
828         return clone
829 
830     def values(self, *fields, **expressions):
831         fields += tuple(expressions)
832         clone = self._values(*fields, **expressions)
833         clone._iterable_class = ValuesIterable
834         return clone
835 
836     def values_list(self, *fields, flat=False, named=False):
837         if flat and named:
838             raise TypeError("'flat' and 'named' can't be used together.")
839         if flat and len(fields) > 1:
840             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
841 
842         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
843         _fields = []
844         expressions = {}
845         counter = 1
846         for field in fields:
847             if hasattr(field, 'resolve_expression'):
848                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
849                 while True:
850                     field_id = field_id_prefix + str(counter)
851                     counter += 1
852                     if field_id not in field_names:
853                         break
854                 expressions[field_id] = field
855                 _fields.append(field_id)
856             else:
857                 _fields.append(field)
858 
859         clone = self._values(*_fields, **expressions)
860         clone._iterable_class = (
861             NamedValuesListIterable if named
862             else FlatValuesListIterable if flat
863             else ValuesListIterable
864         )
865         return clone
866 
867     def dates(self, field_name, kind, order='ASC'):
868         """
869         Return a list of date objects representing all available dates for
870         the given field_name, scoped to 'kind'.
871         """
872         assert kind in ('year', 'month', 'week', 'day'), \
873             "'kind' must be one of 'year', 'month', 'week', or 'day'."
874         assert order in ('ASC', 'DESC'), \
875             "'order' must be either 'ASC' or 'DESC'."
876         return self.annotate(
877             datefield=Trunc(field_name, kind, output_field=DateField()),
878             plain_field=F(field_name)
879         ).values_list(
880             'datefield', flat=True
881         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
882 
883     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
884         """
885         Return a list of datetime objects representing all available
886         datetimes for the given field_name, scoped to 'kind'.
887         """
888         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
889             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
890         assert order in ('ASC', 'DESC'), \
891             "'order' must be either 'ASC' or 'DESC'."
892         if settings.USE_TZ:
893             if tzinfo is None:
894                 tzinfo = timezone.get_current_timezone()
895         else:
896             tzinfo = None
897         return self.annotate(
898             datetimefield=Trunc(
899                 field_name,
900                 kind,
901                 output_field=DateTimeField(),
902                 tzinfo=tzinfo,
903                 is_dst=is_dst,
904             ),
905             plain_field=F(field_name)
906         ).values_list(
907             'datetimefield', flat=True
908         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
909 
910     def none(self):
911         """Return an empty QuerySet."""
912         clone = self._chain()
913         clone.query.set_empty()
914         return clone
915 
916     ##################################################################
917     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
918     ##################################################################
919 
920     def all(self):
921         """
922         Return a new QuerySet that is a copy of the current one. This allows a
923         QuerySet to proxy for a model manager in some cases.
924         """
925         return self._chain()
926 
927     def filter(self, *args, **kwargs):
928         """
929         Return a new QuerySet instance with the args ANDed to the existing
930         set.
931         """
932         self._not_support_combined_queries('filter')
933         return self._filter_or_exclude(False, args, kwargs)
934 
935     def exclude(self, *args, **kwargs):
936         """
937         Return a new QuerySet instance with NOT (args) ANDed to the existing
938         set.
939         """
940         self._not_support_combined_queries('exclude')
941         return self._filter_or_exclude(True, args, kwargs)
942 
943     def _filter_or_exclude(self, negate, args, kwargs):
944         if args or kwargs:
945             assert not self.query.is_sliced, \
946                 "Cannot filter a query once a slice has been taken."
947 
948         clone = self._chain()
949         if self._defer_next_filter:
950             self._defer_next_filter = False
951             clone._deferred_filter = negate, args, kwargs
952         else:
953             clone._filter_or_exclude_inplace(negate, args, kwargs)
954         return clone
955 
956     def _filter_or_exclude_inplace(self, negate, args, kwargs):
957         if negate:
958             self._query.add_q(~Q(*args, **kwargs))
959         else:
960             self._query.add_q(Q(*args, **kwargs))
961 
962     def complex_filter(self, filter_obj):
963         """
964         Return a new QuerySet instance with filter_obj added to the filters.
965 
966         filter_obj can be a Q object or a dictionary of keyword lookup
967         arguments.
968 
969         This exists to support framework features such as 'limit_choices_to',
970         and usually it will be more natural to use other methods.
971         """
972         if isinstance(filter_obj, Q):
973             clone = self._chain()
974             clone.query.add_q(filter_obj)
975             return clone
976         else:
977             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
978 
979     def _combinator_query(self, combinator, *other_qs, all=False):
980         # Clone the query to inherit the select list and everything
981         clone = self._chain()
982         # Clear limits and ordering so they can be reapplied
983         clone.query.clear_ordering(True)
984         clone.query.clear_limits()
985         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
986         clone.query.combinator = combinator
987         clone.query.combinator_all = all
988         return clone
989 
990     def union(self, *other_qs, all=False):
991         # If the query is an EmptyQuerySet, combine all nonempty querysets.
992         if isinstance(self, EmptyQuerySet):
993             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
994             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
995         return self._combinator_query('union', *other_qs, all=all)
996 
997     def intersection(self, *other_qs):
998         # If any query is an EmptyQuerySet, return it.
999         if isinstance(self, EmptyQuerySet):
1000             return self
1001         for other in other_qs:
1002             if isinstance(other, EmptyQuerySet):
1003                 return other
1004         return self._combinator_query('intersection', *other_qs)
1005 
1006     def difference(self, *other_qs):
1007         # If the query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         return self._combinator_query('difference', *other_qs)
1011 
1012     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1013         """
1014         Return a new QuerySet instance that will select objects with a
1015         FOR UPDATE lock.
1016         """
1017         if nowait and skip_locked:
1018             raise ValueError('The nowait option cannot be used with skip_locked.')
1019         obj = self._chain()
1020         obj._for_write = True
1021         obj.query.select_for_update = True
1022         obj.query.select_for_update_nowait = nowait
1023         obj.query.select_for_update_skip_locked = skip_locked
1024         obj.query.select_for_update_of = of
1025         obj.query.select_for_no_key_update = no_key
1026         return obj
1027 
1028     def select_related(self, *fields):
1029         """
1030         Return a new QuerySet instance that will select related objects.
1031 
1032         If fields are specified, they must be ForeignKey fields and only those
1033         related objects are included in the selection.
1034 
1035         If select_related(None) is called, clear the list.
1036         """
1037         self._not_support_combined_queries('select_related')
1038         if self._fields is not None:
1039             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1040 
1041         obj = self._chain()
1042         if fields == (None,):
1043             obj.query.select_related = False
1044         elif fields:
1045             obj.query.add_select_related(fields)
1046         else:
1047             obj.query.select_related = True
1048         return obj
1049 
1050     def prefetch_related(self, *lookups):
1051         """
1052         Return a new QuerySet instance that will prefetch the specified
1053         Many-To-One and Many-To-Many related objects when the QuerySet is
1054         evaluated.
1055 
1056         When prefetch_related() is called more than once, append to the list of
1057         prefetch lookups. If prefetch_related(None) is called, clear the list.
1058         """
1059         self._not_support_combined_queries('prefetch_related')
1060         clone = self._chain()
1061         if lookups == (None,):
1062             clone._prefetch_related_lookups = ()
1063         else:
1064             for lookup in lookups:
1065                 if isinstance(lookup, Prefetch):
1066                     lookup = lookup.prefetch_to
1067                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1068                 if lookup in self.query._filtered_relations:
1069                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1070             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1071         return clone
1072 
1073     def annotate(self, *args, **kwargs):
1074         """
1075         Return a query set in which the returned objects have been annotated
1076         with extra data or aggregations.
1077         """
1078         self._not_support_combined_queries('annotate')
1079         return self._annotate(args, kwargs, select=True)
1080 
1081     def alias(self, *args, **kwargs):
1082         """
1083         Return a query set with added aliases for extra data or aggregations.
1084         """
1085         self._not_support_combined_queries('alias')
1086         return self._annotate(args, kwargs, select=False)
1087 
1088     def _annotate(self, args, kwargs, select=True):
1089         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1090         annotations = {}
1091         for arg in args:
1092             # The default_alias property may raise a TypeError.
1093             try:
1094                 if arg.default_alias in kwargs:
1095                     raise ValueError("The named annotation '%s' conflicts with the "
1096                                      "default name for another annotation."
1097                                      % arg.default_alias)
1098             except TypeError:
1099                 raise TypeError("Complex annotations require an alias")
1100             annotations[arg.default_alias] = arg
1101         annotations.update(kwargs)
1102 
1103         clone = self._chain()
1104         names = self._fields
1105         if names is None:
1106             names = set(chain.from_iterable(
1107                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1108                 for field in self.model._meta.get_fields()
1109             ))
1110 
1111         for alias, annotation in annotations.items():
1112             if alias in names:
1113                 raise ValueError("The annotation '%s' conflicts with a field on "
1114                                  "the model." % alias)
1115             if isinstance(annotation, FilteredRelation):
1116                 clone.query.add_filtered_relation(annotation, alias)
1117             else:
1118                 clone.query.add_annotation(
1119                     annotation, alias, is_summary=False, select=select,
1120                 )
1121         for alias, annotation in clone.query.annotations.items():
1122             if alias in annotations and annotation.contains_aggregate:
1123                 if clone._fields is None:
1124                     clone.query.group_by = True
1125                 else:
1126                     clone.query.set_group_by()
1127                 break
1128 
1129         return clone
1130 
1131     def order_by(self, *field_names):
1132         """Return a new QuerySet instance with the ordering changed."""
1133         assert not self.query.is_sliced, \
1134             "Cannot reorder a query once a slice has been taken."
1135         obj = self._chain()
1136         obj.query.clear_ordering(force_empty=False)
1137         obj.query.add_ordering(*field_names)
1138         return obj
1139 
1140     def distinct(self, *field_names):
1141         """
1142         Return a new QuerySet instance that will select only distinct results.
1143         """
1144         self._not_support_combined_queries('distinct')
1145         assert not self.query.is_sliced, \
1146             "Cannot create distinct fields once a slice has been taken."
1147         obj = self._chain()
1148         obj.query.add_distinct_fields(*field_names)
1149         return obj
1150 
1151     def extra(self, select=None, where=None, params=None, tables=None,
1152               order_by=None, select_params=None):
1153         """Add extra SQL fragments to the query."""
1154         self._not_support_combined_queries('extra')
1155         assert not self.query.is_sliced, \
1156             "Cannot change a query once a slice has been taken"
1157         clone = self._chain()
1158         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1159         return clone
1160 
1161     def reverse(self):
1162         """Reverse the ordering of the QuerySet."""
1163         if self.query.is_sliced:
1164             raise TypeError('Cannot reverse a query once a slice has been taken.')
1165         clone = self._chain()
1166         clone.query.standard_ordering = not clone.query.standard_ordering
1167         return clone
1168 
1169     def defer(self, *fields):
1170         """
1171         Defer the loading of data for certain fields until they are accessed.
1172         Add the set of deferred fields to any existing set of deferred fields.
1173         The only exception to this is if None is passed in as the only
1174         parameter, in which case removal all deferrals.
1175         """
1176         self._not_support_combined_queries('defer')
1177         if self._fields is not None:
1178             raise TypeError("Cannot call defer() after .values() or .values_list()")
1179         clone = self._chain()
1180         if fields == (None,):
1181             clone.query.clear_deferred_loading()
1182         else:
1183             clone.query.add_deferred_loading(fields)
1184         return clone
1185 
1186     def only(self, *fields):
1187         """
1188         Essentially, the opposite of defer(). Only the fields passed into this
1189         method and that are not already specified as deferred are loaded
1190         immediately when the queryset is evaluated.
1191         """
1192         self._not_support_combined_queries('only')
1193         if self._fields is not None:
1194             raise TypeError("Cannot call only() after .values() or .values_list()")
1195         if fields == (None,):
1196             # Can only pass None to defer(), not only(), as the rest option.
1197             # That won't stop people trying to do this, so let's be explicit.
1198             raise TypeError("Cannot pass None as an argument to only().")
1199         for field in fields:
1200             field = field.split(LOOKUP_SEP, 1)[0]
1201             if field in self.query._filtered_relations:
1202                 raise ValueError('only() is not supported with FilteredRelation.')
1203         clone = self._chain()
1204         clone.query.add_immediate_loading(fields)
1205         return clone
1206 
1207     def using(self, alias):
1208         """Select which database this QuerySet should execute against."""
1209         clone = self._chain()
1210         clone._db = alias
1211         return clone
1212 
1213     ###################################
1214     # PUBLIC INTROSPECTION ATTRIBUTES #
1215     ###################################
1216 
1217     @property
1218     def ordered(self):
1219         """
1220         Return True if the QuerySet is ordered -- i.e. has an order_by()
1221         clause or a default ordering on the model (or is empty).
1222         """
1223         if isinstance(self, EmptyQuerySet):
1224             return True
1225         if self.query.extra_order_by or self.query.order_by:
1226             return True
1227         elif (
1228             self.query.default_ordering and
1229             self.query.get_meta().ordering and
1230             # A default ordering doesn't affect GROUP BY queries.
1231             not self.query.group_by
1232         ):
1233             return True
1234         else:
1235             return False
1236 
1237     @property
1238     def db(self):
1239         """Return the database used if this query is executed now."""
1240         if self._for_write:
1241             return self._db or router.db_for_write(self.model, **self._hints)
1242         return self._db or router.db_for_read(self.model, **self._hints)
1243 
1244     ###################
1245     # PRIVATE METHODS #
1246     ###################
1247 
1248     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1249         """
1250         Insert a new record for the given model. This provides an interface to
1251         the InsertQuery class and is how Model.save() is implemented.
1252         """
1253         self._for_write = True
1254         if using is None:
1255             using = self.db
1256         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1257         query.insert_values(fields, objs, raw=raw)
1258         return query.get_compiler(using=using).execute_sql(returning_fields)
1259     _insert.alters_data = True
1260     _insert.queryset_only = False
1261 
1262     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1263         """
1264         Helper method for bulk_create() to insert objs one batch at a time.
1265         """
1266         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1267             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1268         ops = connections[self.db].ops
1269         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1270         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1271         inserted_rows = []
1272         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1273         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1274             if bulk_return and not ignore_conflicts:
1275                 inserted_rows.extend(self._insert(
1276                     item, fields=fields, using=self.db,
1277                     returning_fields=self.model._meta.db_returning_fields,
1278                     ignore_conflicts=ignore_conflicts,
1279                 ))
1280             else:
1281                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1282         return inserted_rows
1283 
1284     def _chain(self, **kwargs):
1285         """
1286         Return a copy of the current QuerySet that's ready for another
1287         operation.
1288         """
1289         obj = self._clone()
1290         if obj._sticky_filter:
1291             obj.query.filter_is_sticky = True
1292             obj._sticky_filter = False
1293         obj.__dict__.update(kwargs)
1294         return obj
1295 
1296     def _clone(self):
1297         """
1298         Return a copy of the current QuerySet. A lightweight alternative
1299         to deepcopy().
1300         """
1301         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1302         c._sticky_filter = self._sticky_filter
1303         c._for_write = self._for_write
1304         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1305         c._known_related_objects = self._known_related_objects
1306         c._iterable_class = self._iterable_class
1307         c._fields = self._fields
1308         return c
1309 
1310     def _fetch_all(self):
1311         if self._result_cache is None:
1312             self._result_cache = list(self._iterable_class(self))
1313         if self._prefetch_related_lookups and not self._prefetch_done:
1314             self._prefetch_related_objects()
1315 
1316     def _next_is_sticky(self):
1317         """
1318         Indicate that the next filter call and the one following that should
1319         be treated as a single filter. This is only important when it comes to
1320         determining when to reuse tables for many-to-many filters. Required so
1321         that we can filter naturally on the results of related managers.
1322 
1323         This doesn't return a clone of the current QuerySet (it returns
1324         "self"). The method is only used internally and should be immediately
1325         followed by a filter() that does create a clone.
1326         """
1327         self._sticky_filter = True
1328         return self
1329 
1330     def _merge_sanity_check(self, other):
1331         """Check that two QuerySet classes may be merged."""
1332         if self._fields is not None and (
1333                 set(self.query.values_select) != set(other.query.values_select) or
1334                 set(self.query.extra_select) != set(other.query.extra_select) or
1335                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1336             raise TypeError(
1337                 "Merging '%s' classes must involve the same values in each case."
1338                 % self.__class__.__name__
1339             )
1340 
1341     def _merge_known_related_objects(self, other):
1342         """
1343         Keep track of all known related objects from either QuerySet instance.
1344         """
1345         for field, objects in other._known_related_objects.items():
1346             self._known_related_objects.setdefault(field, {}).update(objects)
1347 
1348     def resolve_expression(self, *args, **kwargs):
1349         if self._fields and len(self._fields) > 1:
1350             # values() queryset can only be used as nested queries
1351             # if they are set up to select only a single field.
1352             raise TypeError('Cannot use multi-field values as a filter value.')
1353         query = self.query.resolve_expression(*args, **kwargs)
1354         query._db = self._db
1355         return query
1356     resolve_expression.queryset_only = True
1357 
1358     def _add_hints(self, **hints):
1359         """
1360         Update hinting information for use by routers. Add new key/values or
1361         overwrite existing key/values.
1362         """
1363         self._hints.update(hints)
1364 
1365     def _has_filters(self):
1366         """
1367         Check if this QuerySet has any filtering going on. This isn't
1368         equivalent with checking if all objects are present in results, for
1369         example, qs[1:]._has_filters() -> False.
1370         """
1371         return self.query.has_filters()
1372 
1373     @staticmethod
1374     def _validate_values_are_expressions(values, method_name):
1375         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1376         if invalid_args:
1377             raise TypeError(
1378                 'QuerySet.%s() received non-expression(s): %s.' % (
1379                     method_name,
1380                     ', '.join(invalid_args),
1381                 )
1382             )
1383 
1384     def _not_support_combined_queries(self, operation_name):
1385         if self.query.combinator:
1386             raise NotSupportedError(
1387                 'Calling QuerySet.%s() after %s() is not supported.'
1388                 % (operation_name, self.query.combinator)
1389             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 00:43:06.770 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>values_list</method>
<code>
836     def values_list(self, *fields, flat=False, named=False):
837         if flat and named:
838             raise TypeError("'flat' and 'named' can't be used together.")
839         if flat and len(fields) > 1:
840             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
841 
842         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
843         _fields = []
844         expressions = {}
845         counter = 1
846         for field in fields:
847             if hasattr(field, 'resolve_expression'):
848                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
849                 while True:
850                     field_id = field_id_prefix + str(counter)
851                     counter += 1
852                     if field_id not in field_names:
853                         break
854                 expressions[field_id] = field
855                 _fields.append(field_id)
856             else:
857                 _fields.append(field)
858 
859         clone = self._values(*_fields, **expressions)
860         clone._iterable_class = (
861             NamedValuesListIterable if named
862             else FlatValuesListIterable if flat
863             else ValuesListIterable
864         )
865         return clone

</code><intended_behavior>The `values_list` method should ensure that the dynamically generated `namedtuple` instances, referred to as `Row` when `named=True`, are pickleable. This would likely involve adding or ensuring that the `__reduce__` method is correctly implemented for these `namedtuple` instances. The implementation must ensure that these instances can be serialized and deserialized without losing functionality, thereby allowing these objects to be used with caching mechanisms like `cacheops`. This should be done without affecting the current functionality of returning a queryset of `namedtuple` instances for improved user accessibility and performance. The fix may involve modifying the way `namedtuple` classes are dynamically created or augmenting them with additional methods to support pickling.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
184 class QuerySet:
185     """Represent a lazy database lookup for a set of objects."""
186 
187     def __init__(self, model=None, query=None, using=None, hints=None):
188         self.model = model
189         self._db = using
190         self._hints = hints or {}
191         self._query = query or sql.Query(self.model)
192         self._result_cache = None
193         self._sticky_filter = False
194         self._for_write = False
195         self._prefetch_related_lookups = ()
196         self._prefetch_done = False
197         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
198         self._iterable_class = ModelIterable
199         self._fields = None
200         self._defer_next_filter = False
201         self._deferred_filter = None
202 
203     @property
204     def query(self):
205         if self._deferred_filter:
206             negate, args, kwargs = self._deferred_filter
207             self._filter_or_exclude_inplace(negate, args, kwargs)
208             self._deferred_filter = None
209         return self._query
210 
211     @query.setter
212     def query(self, value):
213         if value.values_select:
214             self._iterable_class = ValuesIterable
215         self._query = value
216 
217     def as_manager(cls):
218         # Address the circular dependency between `Queryset` and `Manager`.
219         from django.db.models.manager import Manager
220         manager = Manager.from_queryset(cls)()
221         manager._built_with_as_manager = True
222         return manager
223     as_manager.queryset_only = True
224     as_manager = classmethod(as_manager)
225 
226     ########################
227     # PYTHON MAGIC METHODS #
228     ########################
229 
230     def __deepcopy__(self, memo):
231         """Don't populate the QuerySet's cache."""
232         obj = self.__class__()
233         for k, v in self.__dict__.items():
234             if k == '_result_cache':
235                 obj.__dict__[k] = None
236             else:
237                 obj.__dict__[k] = copy.deepcopy(v, memo)
238         return obj
239 
240     def __getstate__(self):
241         # Force the cache to be fully populated.
242         self._fetch_all()
243         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
244 
245     def __setstate__(self, state):
246         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
247         if pickled_version:
248             if pickled_version != django.__version__:
249                 warnings.warn(
250                     "Pickled queryset instance's Django version %s does not "
251                     "match the current version %s."
252                     % (pickled_version, django.__version__),
253                     RuntimeWarning,
254                     stacklevel=2,
255                 )
256         else:
257             warnings.warn(
258                 "Pickled queryset instance's Django version is not specified.",
259                 RuntimeWarning,
260                 stacklevel=2,
261             )
262         self.__dict__.update(state)
263 
264     def __repr__(self):
265         data = list(self[:REPR_OUTPUT_SIZE + 1])
266         if len(data) > REPR_OUTPUT_SIZE:
267             data[-1] = "...(remaining elements truncated)..."
268         return '<%s %r>' % (self.__class__.__name__, data)
269 
270     def __len__(self):
271         self._fetch_all()
272         return len(self._result_cache)
273 
274     def __iter__(self):
275         """
276         The queryset iterator protocol uses three nested iterators in the
277         default case:
278             1. sql.compiler.execute_sql()
279                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
280                  using cursor.fetchmany(). This part is responsible for
281                  doing some column masking, and returning the rows in chunks.
282             2. sql.compiler.results_iter()
283                - Returns one row at time. At this point the rows are still just
284                  tuples. In some cases the return values are converted to
285                  Python values at this location.
286             3. self.iterator()
287                - Responsible for turning the rows into model objects.
288         """
289         self._fetch_all()
290         return iter(self._result_cache)
291 
292     def __bool__(self):
293         self._fetch_all()
294         return bool(self._result_cache)
295 
296     def __getitem__(self, k):
297         """Retrieve an item or slice from the set of results."""
298         if not isinstance(k, (int, slice)):
299             raise TypeError(
300                 'QuerySet indices must be integers or slices, not %s.'
301                 % type(k).__name__
302             )
303         assert ((not isinstance(k, slice) and (k >= 0)) or
304                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
305                  (k.stop is None or k.stop >= 0))), \
306             "Negative indexing is not supported."
307 
308         if self._result_cache is not None:
309             return self._result_cache[k]
310 
311         if isinstance(k, slice):
312             qs = self._chain()
313             if k.start is not None:
314                 start = int(k.start)
315             else:
316                 start = None
317             if k.stop is not None:
318                 stop = int(k.stop)
319             else:
320                 stop = None
321             qs.query.set_limits(start, stop)
322             return list(qs)[::k.step] if k.step else qs
323 
324         qs = self._chain()
325         qs.query.set_limits(k, k + 1)
326         qs._fetch_all()
327         return qs._result_cache[0]
328 
329     def __class_getitem__(cls, *args, **kwargs):
330         return cls
331 
332     def __and__(self, other):
333         self._merge_sanity_check(other)
334         if isinstance(other, EmptyQuerySet):
335             return other
336         if isinstance(self, EmptyQuerySet):
337             return self
338         combined = self._chain()
339         combined._merge_known_related_objects(other)
340         combined.query.combine(other.query, sql.AND)
341         return combined
342 
343     def __or__(self, other):
344         self._merge_sanity_check(other)
345         if isinstance(self, EmptyQuerySet):
346             return other
347         if isinstance(other, EmptyQuerySet):
348             return self
349         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
350         combined = query._chain()
351         combined._merge_known_related_objects(other)
352         if not other.query.can_filter():
353             other = other.model._base_manager.filter(pk__in=other.values('pk'))
354         combined.query.combine(other.query, sql.OR)
355         return combined
356 
357     ####################################
358     # METHODS THAT DO DATABASE QUERIES #
359     ####################################
360 
361     def _iterator(self, use_chunked_fetch, chunk_size):
362         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
363 
364     def iterator(self, chunk_size=2000):
365         """
366         An iterator over the results from applying this QuerySet to the
367         database.
368         """
369         if chunk_size <= 0:
370             raise ValueError('Chunk size must be strictly positive.')
371         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
372         return self._iterator(use_chunked_fetch, chunk_size)
373 
374     def aggregate(self, *args, **kwargs):
375         """
376         Return a dictionary containing the calculations (aggregation)
377         over the current queryset.
378 
379         If args is present the expression is passed as a kwarg using
380         the Aggregate object's default alias.
381         """
382         if self.query.distinct_fields:
383             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
384         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
385         for arg in args:
386             # The default_alias property raises TypeError if default_alias
387             # can't be set automatically or AttributeError if it isn't an
388             # attribute.
389             try:
390                 arg.default_alias
391             except (AttributeError, TypeError):
392                 raise TypeError("Complex aggregates require an alias")
393             kwargs[arg.default_alias] = arg
394 
395         query = self.query.chain()
396         for (alias, aggregate_expr) in kwargs.items():
397             query.add_annotation(aggregate_expr, alias, is_summary=True)
398             if not query.annotations[alias].contains_aggregate:
399                 raise TypeError("%s is not an aggregate expression" % alias)
400         return query.get_aggregation(self.db, kwargs)
401 
402     def count(self):
403         """
404         Perform a SELECT COUNT() and return the number of records as an
405         integer.
406 
407         If the QuerySet is already fully cached, return the length of the
408         cached results set to avoid multiple SELECT COUNT(*) calls.
409         """
410         if self._result_cache is not None:
411             return len(self._result_cache)
412 
413         return self.query.get_count(using=self.db)
414 
415     def get(self, *args, **kwargs):
416         """
417         Perform the query and return a single object matching the given
418         keyword arguments.
419         """
420         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
421         if self.query.can_filter() and not self.query.distinct_fields:
422             clone = clone.order_by()
423         limit = None
424         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
425             limit = MAX_GET_RESULTS
426             clone.query.set_limits(high=limit)
427         num = len(clone)
428         if num == 1:
429             return clone._result_cache[0]
430         if not num:
431             raise self.model.DoesNotExist(
432                 "%s matching query does not exist." %
433                 self.model._meta.object_name
434             )
435         raise self.model.MultipleObjectsReturned(
436             'get() returned more than one %s -- it returned %s!' % (
437                 self.model._meta.object_name,
438                 num if not limit or num < limit else 'more than %s' % (limit - 1),
439             )
440         )
441 
442     def create(self, **kwargs):
443         """
444         Create a new object with the given kwargs, saving it to the database
445         and returning the created object.
446         """
447         obj = self.model(**kwargs)
448         self._for_write = True
449         obj.save(force_insert=True, using=self.db)
450         return obj
451 
452     def _populate_pk_values(self, objs):
453         for obj in objs:
454             if obj.pk is None:
455                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
456 
457     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
458         """
459         Insert each of the instances into the database. Do *not* call
460         save() on each of the instances, do not send any pre/post_save
461         signals, and do not set the primary key attribute if it is an
462         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
463         Multi-table models are not supported.
464         """
465         # When you bulk insert you don't get the primary keys back (if it's an
466         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
467         # you can't insert into the child tables which references this. There
468         # are two workarounds:
469         # 1) This could be implemented if you didn't have an autoincrement pk
470         # 2) You could do it by doing O(n) normal inserts into the parent
471         #    tables to get the primary keys back and then doing a single bulk
472         #    insert into the childmost table.
473         # We currently set the primary keys on the objects when using
474         # PostgreSQL via the RETURNING ID clause. It should be possible for
475         # Oracle as well, but the semantics for extracting the primary keys is
476         # trickier so it's not done yet.
477         assert batch_size is None or batch_size > 0
478         # Check that the parents share the same concrete model with the our
479         # model to detect the inheritance pattern ConcreteGrandParent ->
480         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
481         # would not identify that case as involving multiple tables.
482         for parent in self.model._meta.get_parent_list():
483             if parent._meta.concrete_model is not self.model._meta.concrete_model:
484                 raise ValueError("Can't bulk create a multi-table inherited model")
485         if not objs:
486             return objs
487         self._for_write = True
488         connection = connections[self.db]
489         opts = self.model._meta
490         fields = opts.concrete_fields
491         objs = list(objs)
492         self._populate_pk_values(objs)
493         with transaction.atomic(using=self.db, savepoint=False):
494             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
495             if objs_with_pk:
496                 returned_columns = self._batched_insert(
497                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
498                 )
499                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
500                     for result, field in zip(results, opts.db_returning_fields):
501                         if field != opts.pk:
502                             setattr(obj_with_pk, field.attname, result)
503                 for obj_with_pk in objs_with_pk:
504                     obj_with_pk._state.adding = False
505                     obj_with_pk._state.db = self.db
506             if objs_without_pk:
507                 fields = [f for f in fields if not isinstance(f, AutoField)]
508                 returned_columns = self._batched_insert(
509                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
510                 )
511                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
512                     assert len(returned_columns) == len(objs_without_pk)
513                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
514                     for result, field in zip(results, opts.db_returning_fields):
515                         setattr(obj_without_pk, field.attname, result)
516                     obj_without_pk._state.adding = False
517                     obj_without_pk._state.db = self.db
518 
519         return objs
520 
521     def bulk_update(self, objs, fields, batch_size=None):
522         """
523         Update the given fields in each of the given objects in the database.
524         """
525         if batch_size is not None and batch_size < 0:
526             raise ValueError('Batch size must be a positive integer.')
527         if not fields:
528             raise ValueError('Field names must be given to bulk_update().')
529         objs = tuple(objs)
530         if any(obj.pk is None for obj in objs):
531             raise ValueError('All bulk_update() objects must have a primary key set.')
532         fields = [self.model._meta.get_field(name) for name in fields]
533         if any(not f.concrete or f.many_to_many for f in fields):
534             raise ValueError('bulk_update() can only be used with concrete fields.')
535         if any(f.primary_key for f in fields):
536             raise ValueError('bulk_update() cannot be used with primary key fields.')
537         if not objs:
538             return
539         # PK is used twice in the resulting update query, once in the filter
540         # and once in the WHEN. Each field will also have one CAST.
541         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
542         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
543         requires_casting = connections[self.db].features.requires_casted_case_in_updates
544         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
545         updates = []
546         for batch_objs in batches:
547             update_kwargs = {}
548             for field in fields:
549                 when_statements = []
550                 for obj in batch_objs:
551                     attr = getattr(obj, field.attname)
552                     if not isinstance(attr, Expression):
553                         attr = Value(attr, output_field=field)
554                     when_statements.append(When(pk=obj.pk, then=attr))
555                 case_statement = Case(*when_statements, output_field=field)
556                 if requires_casting:
557                     case_statement = Cast(case_statement, output_field=field)
558                 update_kwargs[field.attname] = case_statement
559             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
560         with transaction.atomic(using=self.db, savepoint=False):
561             for pks, update_kwargs in updates:
562                 self.filter(pk__in=pks).update(**update_kwargs)
563     bulk_update.alters_data = True
564 
565     def get_or_create(self, defaults=None, **kwargs):
566         """
567         Look up an object with the given kwargs, creating one if necessary.
568         Return a tuple of (object, created), where created is a boolean
569         specifying whether an object was created.
570         """
571         # The get() needs to be targeted at the write database in order
572         # to avoid potential transaction consistency problems.
573         self._for_write = True
574         try:
575             return self.get(**kwargs), False
576         except self.model.DoesNotExist:
577             params = self._extract_model_params(defaults, **kwargs)
578             # Try to create an object using passed params.
579             try:
580                 with transaction.atomic(using=self.db):
581                     params = dict(resolve_callables(params))
582                     return self.create(**params), True
583             except IntegrityError:
584                 try:
585                     return self.get(**kwargs), False
586                 except self.model.DoesNotExist:
587                     pass
588                 raise
589 
590     def update_or_create(self, defaults=None, **kwargs):
591         """
592         Look up an object with the given kwargs, updating one with defaults
593         if it exists, otherwise create a new one.
594         Return a tuple (object, created), where created is a boolean
595         specifying whether an object was created.
596         """
597         defaults = defaults or {}
598         self._for_write = True
599         with transaction.atomic(using=self.db):
600             # Lock the row so that a concurrent update is blocked until
601             # update_or_create() has performed its save.
602             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
603             if created:
604                 return obj, created
605             for k, v in resolve_callables(defaults):
606                 setattr(obj, k, v)
607             obj.save(using=self.db)
608         return obj, False
609 
610     def _extract_model_params(self, defaults, **kwargs):
611         """
612         Prepare `params` for creating a model instance based on the given
613         kwargs; for use by get_or_create().
614         """
615         defaults = defaults or {}
616         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
617         params.update(defaults)
618         property_names = self.model._meta._property_names
619         invalid_params = []
620         for param in params:
621             try:
622                 self.model._meta.get_field(param)
623             except exceptions.FieldDoesNotExist:
624                 # It's okay to use a model's property if it has a setter.
625                 if not (param in property_names and getattr(self.model, param).fset):
626                     invalid_params.append(param)
627         if invalid_params:
628             raise exceptions.FieldError(
629                 "Invalid field name(s) for model %s: '%s'." % (
630                     self.model._meta.object_name,
631                     "', '".join(sorted(invalid_params)),
632                 ))
633         return params
634 
635     def _earliest(self, *fields):
636         """
637         Return the earliest object according to fields (if given) or by the
638         model's Meta.get_latest_by.
639         """
640         if fields:
641             order_by = fields
642         else:
643             order_by = getattr(self.model._meta, 'get_latest_by')
644             if order_by and not isinstance(order_by, (tuple, list)):
645                 order_by = (order_by,)
646         if order_by is None:
647             raise ValueError(
648                 "earliest() and latest() require either fields as positional "
649                 "arguments or 'get_latest_by' in the model's Meta."
650             )
651 
652         assert not self.query.is_sliced, \
653             "Cannot change a query once a slice has been taken."
654         obj = self._chain()
655         obj.query.set_limits(high=1)
656         obj.query.clear_ordering(force_empty=True)
657         obj.query.add_ordering(*order_by)
658         return obj.get()
659 
660     def earliest(self, *fields):
661         return self._earliest(*fields)
662 
663     def latest(self, *fields):
664         return self.reverse()._earliest(*fields)
665 
666     def first(self):
667         """Return the first object of a query or None if no match is found."""
668         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
669             return obj
670 
671     def last(self):
672         """Return the last object of a query or None if no match is found."""
673         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
674             return obj
675 
676     def in_bulk(self, id_list=None, *, field_name='pk'):
677         """
678         Return a dictionary mapping each of the given IDs to the object with
679         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
680         """
681         assert not self.query.is_sliced, \
682             "Cannot use 'limit' or 'offset' with in_bulk"
683         opts = self.model._meta
684         unique_fields = [
685             constraint.fields[0]
686             for constraint in opts.total_unique_constraints
687             if len(constraint.fields) == 1
688         ]
689         if (
690             field_name != 'pk' and
691             not opts.get_field(field_name).unique and
692             field_name not in unique_fields and
693             not self.query.distinct_fields == (field_name,)
694         ):
695             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
696         if id_list is not None:
697             if not id_list:
698                 return {}
699             filter_key = '{}__in'.format(field_name)
700             batch_size = connections[self.db].features.max_query_params
701             id_list = tuple(id_list)
702             # If the database has a limit on the number of query parameters
703             # (e.g. SQLite), retrieve objects in batches if necessary.
704             if batch_size and batch_size < len(id_list):
705                 qs = ()
706                 for offset in range(0, len(id_list), batch_size):
707                     batch = id_list[offset:offset + batch_size]
708                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
709             else:
710                 qs = self.filter(**{filter_key: id_list}).order_by()
711         else:
712             qs = self._chain()
713         return {getattr(obj, field_name): obj for obj in qs}
714 
715     def delete(self):
716         """Delete the records in the current QuerySet."""
717         self._not_support_combined_queries('delete')
718         assert not self.query.is_sliced, \
719             "Cannot use 'limit' or 'offset' with delete."
720 
721         if self._fields is not None:
722             raise TypeError("Cannot call delete() after .values() or .values_list()")
723 
724         del_query = self._chain()
725 
726         # The delete is actually 2 queries - one to find related objects,
727         # and one to delete. Make sure that the discovery of related
728         # objects is performed on the same database as the deletion.
729         del_query._for_write = True
730 
731         # Disable non-supported fields.
732         del_query.query.select_for_update = False
733         del_query.query.select_related = False
734         del_query.query.clear_ordering(force_empty=True)
735 
736         collector = Collector(using=del_query.db)
737         collector.collect(del_query)
738         deleted, _rows_count = collector.delete()
739 
740         # Clear the result cache, in case this QuerySet gets reused.
741         self._result_cache = None
742         return deleted, _rows_count
743 
744     delete.alters_data = True
745     delete.queryset_only = True
746 
747     def _raw_delete(self, using):
748         """
749         Delete objects found from the given queryset in single direct SQL
750         query. No signals are sent and there is no protection for cascades.
751         """
752         query = self.query.clone()
753         query.__class__ = sql.DeleteQuery
754         cursor = query.get_compiler(using).execute_sql(CURSOR)
755         if cursor:
756             with cursor:
757                 return cursor.rowcount
758         return 0
759     _raw_delete.alters_data = True
760 
761     def update(self, **kwargs):
762         """
763         Update all elements in the current QuerySet, setting all the given
764         fields to the appropriate values.
765         """
766         self._not_support_combined_queries('update')
767         assert not self.query.is_sliced, \
768             "Cannot update a query once a slice has been taken."
769         self._for_write = True
770         query = self.query.chain(sql.UpdateQuery)
771         query.add_update_values(kwargs)
772         # Clear any annotations so that they won't be present in subqueries.
773         query.annotations = {}
774         with transaction.mark_for_rollback_on_error(using=self.db):
775             rows = query.get_compiler(self.db).execute_sql(CURSOR)
776         self._result_cache = None
777         return rows
778     update.alters_data = True
779 
780     def _update(self, values):
781         """
782         A version of update() that accepts field objects instead of field names.
783         Used primarily for model saving and not intended for use by general
784         code (it requires too much poking around at model internals to be
785         useful at that level).
786         """
787         assert not self.query.is_sliced, \
788             "Cannot update a query once a slice has been taken."
789         query = self.query.chain(sql.UpdateQuery)
790         query.add_update_fields(values)
791         # Clear any annotations so that they won't be present in subqueries.
792         query.annotations = {}
793         self._result_cache = None
794         return query.get_compiler(self.db).execute_sql(CURSOR)
795     _update.alters_data = True
796     _update.queryset_only = False
797 
798     def exists(self):
799         if self._result_cache is None:
800             return self.query.has_results(using=self.db)
801         return bool(self._result_cache)
802 
803     def _prefetch_related_objects(self):
804         # This method can only be called once the result cache has been filled.
805         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
806         self._prefetch_done = True
807 
808     def explain(self, *, format=None, **options):
809         return self.query.explain(using=self.db, format=format, **options)
810 
811     ##################################################
812     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
813     ##################################################
814 
815     def raw(self, raw_query, params=None, translations=None, using=None):
816         if using is None:
817             using = self.db
818         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
819         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
820         return qs
821 
822     def _values(self, *fields, **expressions):
823         clone = self._chain()
824         if expressions:
825             clone = clone.annotate(**expressions)
826         clone._fields = fields
827         clone.query.set_values(fields)
828         return clone
829 
830     def values(self, *fields, **expressions):
831         fields += tuple(expressions)
832         clone = self._values(*fields, **expressions)
833         clone._iterable_class = ValuesIterable
834         return clone
835 
836     def values_list(self, *fields, flat=False, named=False):
837         if flat and named:
838             raise TypeError("'flat' and 'named' can't be used together.")
839         if flat and len(fields) > 1:
840             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
841 
842         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
843         _fields = []
844         expressions = {}
845         counter = 1
846         for field in fields:
847             if hasattr(field, 'resolve_expression'):
848                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
849                 while True:
850                     field_id = field_id_prefix + str(counter)
851                     counter += 1
852                     if field_id not in field_names:
853                         break
854                 expressions[field_id] = field
855                 _fields.append(field_id)
856             else:
857                 _fields.append(field)
858 
859         clone = self._values(*_fields, **expressions)
860         clone._iterable_class = (
861             NamedValuesListIterable if named
862             else FlatValuesListIterable if flat
863             else ValuesListIterable
864         )
865         return clone
866 
867     def dates(self, field_name, kind, order='ASC'):
868         """
869         Return a list of date objects representing all available dates for
870         the given field_name, scoped to 'kind'.
871         """
872         assert kind in ('year', 'month', 'week', 'day'), \
873             "'kind' must be one of 'year', 'month', 'week', or 'day'."
874         assert order in ('ASC', 'DESC'), \
875             "'order' must be either 'ASC' or 'DESC'."
876         return self.annotate(
877             datefield=Trunc(field_name, kind, output_field=DateField()),
878             plain_field=F(field_name)
879         ).values_list(
880             'datefield', flat=True
881         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
882 
883     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
884         """
885         Return a list of datetime objects representing all available
886         datetimes for the given field_name, scoped to 'kind'.
887         """
888         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
889             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
890         assert order in ('ASC', 'DESC'), \
891             "'order' must be either 'ASC' or 'DESC'."
892         if settings.USE_TZ:
893             if tzinfo is None:
894                 tzinfo = timezone.get_current_timezone()
895         else:
896             tzinfo = None
897         return self.annotate(
898             datetimefield=Trunc(
899                 field_name,
900                 kind,
901                 output_field=DateTimeField(),
902                 tzinfo=tzinfo,
903                 is_dst=is_dst,
904             ),
905             plain_field=F(field_name)
906         ).values_list(
907             'datetimefield', flat=True
908         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
909 
910     def none(self):
911         """Return an empty QuerySet."""
912         clone = self._chain()
913         clone.query.set_empty()
914         return clone
915 
916     ##################################################################
917     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
918     ##################################################################
919 
920     def all(self):
921         """
922         Return a new QuerySet that is a copy of the current one. This allows a
923         QuerySet to proxy for a model manager in some cases.
924         """
925         return self._chain()
926 
927     def filter(self, *args, **kwargs):
928         """
929         Return a new QuerySet instance with the args ANDed to the existing
930         set.
931         """
932         self._not_support_combined_queries('filter')
933         return self._filter_or_exclude(False, args, kwargs)
934 
935     def exclude(self, *args, **kwargs):
936         """
937         Return a new QuerySet instance with NOT (args) ANDed to the existing
938         set.
939         """
940         self._not_support_combined_queries('exclude')
941         return self._filter_or_exclude(True, args, kwargs)
942 
943     def _filter_or_exclude(self, negate, args, kwargs):
944         if args or kwargs:
945             assert not self.query.is_sliced, \
946                 "Cannot filter a query once a slice has been taken."
947 
948         clone = self._chain()
949         if self._defer_next_filter:
950             self._defer_next_filter = False
951             clone._deferred_filter = negate, args, kwargs
952         else:
953             clone._filter_or_exclude_inplace(negate, args, kwargs)
954         return clone
955 
956     def _filter_or_exclude_inplace(self, negate, args, kwargs):
957         if negate:
958             self._query.add_q(~Q(*args, **kwargs))
959         else:
960             self._query.add_q(Q(*args, **kwargs))
961 
962     def complex_filter(self, filter_obj):
963         """
964         Return a new QuerySet instance with filter_obj added to the filters.
965 
966         filter_obj can be a Q object or a dictionary of keyword lookup
967         arguments.
968 
969         This exists to support framework features such as 'limit_choices_to',
970         and usually it will be more natural to use other methods.
971         """
972         if isinstance(filter_obj, Q):
973             clone = self._chain()
974             clone.query.add_q(filter_obj)
975             return clone
976         else:
977             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
978 
979     def _combinator_query(self, combinator, *other_qs, all=False):
980         # Clone the query to inherit the select list and everything
981         clone = self._chain()
982         # Clear limits and ordering so they can be reapplied
983         clone.query.clear_ordering(True)
984         clone.query.clear_limits()
985         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
986         clone.query.combinator = combinator
987         clone.query.combinator_all = all
988         return clone
989 
990     def union(self, *other_qs, all=False):
991         # If the query is an EmptyQuerySet, combine all nonempty querysets.
992         if isinstance(self, EmptyQuerySet):
993             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
994             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
995         return self._combinator_query('union', *other_qs, all=all)
996 
997     def intersection(self, *other_qs):
998         # If any query is an EmptyQuerySet, return it.
999         if isinstance(self, EmptyQuerySet):
1000             return self
1001         for other in other_qs:
1002             if isinstance(other, EmptyQuerySet):
1003                 return other
1004         return self._combinator_query('intersection', *other_qs)
1005 
1006     def difference(self, *other_qs):
1007         # If the query is an EmptyQuerySet, return it.
1008         if isinstance(self, EmptyQuerySet):
1009             return self
1010         return self._combinator_query('difference', *other_qs)
1011 
1012     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1013         """
1014         Return a new QuerySet instance that will select objects with a
1015         FOR UPDATE lock.
1016         """
1017         if nowait and skip_locked:
1018             raise ValueError('The nowait option cannot be used with skip_locked.')
1019         obj = self._chain()
1020         obj._for_write = True
1021         obj.query.select_for_update = True
1022         obj.query.select_for_update_nowait = nowait
1023         obj.query.select_for_update_skip_locked = skip_locked
1024         obj.query.select_for_update_of = of
1025         obj.query.select_for_no_key_update = no_key
1026         return obj
1027 
1028     def select_related(self, *fields):
1029         """
1030         Return a new QuerySet instance that will select related objects.
1031 
1032         If fields are specified, they must be ForeignKey fields and only those
1033         related objects are included in the selection.
1034 
1035         If select_related(None) is called, clear the list.
1036         """
1037         self._not_support_combined_queries('select_related')
1038         if self._fields is not None:
1039             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1040 
1041         obj = self._chain()
1042         if fields == (None,):
1043             obj.query.select_related = False
1044         elif fields:
1045             obj.query.add_select_related(fields)
1046         else:
1047             obj.query.select_related = True
1048         return obj
1049 
1050     def prefetch_related(self, *lookups):
1051         """
1052         Return a new QuerySet instance that will prefetch the specified
1053         Many-To-One and Many-To-Many related objects when the QuerySet is
1054         evaluated.
1055 
1056         When prefetch_related() is called more than once, append to the list of
1057         prefetch lookups. If prefetch_related(None) is called, clear the list.
1058         """
1059         self._not_support_combined_queries('prefetch_related')
1060         clone = self._chain()
1061         if lookups == (None,):
1062             clone._prefetch_related_lookups = ()
1063         else:
1064             for lookup in lookups:
1065                 if isinstance(lookup, Prefetch):
1066                     lookup = lookup.prefetch_to
1067                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1068                 if lookup in self.query._filtered_relations:
1069                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1070             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1071         return clone
1072 
1073     def annotate(self, *args, **kwargs):
1074         """
1075         Return a query set in which the returned objects have been annotated
1076         with extra data or aggregations.
1077         """
1078         self._not_support_combined_queries('annotate')
1079         return self._annotate(args, kwargs, select=True)
1080 
1081     def alias(self, *args, **kwargs):
1082         """
1083         Return a query set with added aliases for extra data or aggregations.
1084         """
1085         self._not_support_combined_queries('alias')
1086         return self._annotate(args, kwargs, select=False)
1087 
1088     def _annotate(self, args, kwargs, select=True):
1089         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1090         annotations = {}
1091         for arg in args:
1092             # The default_alias property may raise a TypeError.
1093             try:
1094                 if arg.default_alias in kwargs:
1095                     raise ValueError("The named annotation '%s' conflicts with the "
1096                                      "default name for another annotation."
1097                                      % arg.default_alias)
1098             except TypeError:
1099                 raise TypeError("Complex annotations require an alias")
1100             annotations[arg.default_alias] = arg
1101         annotations.update(kwargs)
1102 
1103         clone = self._chain()
1104         names = self._fields
1105         if names is None:
1106             names = set(chain.from_iterable(
1107                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1108                 for field in self.model._meta.get_fields()
1109             ))
1110 
1111         for alias, annotation in annotations.items():
1112             if alias in names:
1113                 raise ValueError("The annotation '%s' conflicts with a field on "
1114                                  "the model." % alias)
1115             if isinstance(annotation, FilteredRelation):
1116                 clone.query.add_filtered_relation(annotation, alias)
1117             else:
1118                 clone.query.add_annotation(
1119                     annotation, alias, is_summary=False, select=select,
1120                 )
1121         for alias, annotation in clone.query.annotations.items():
1122             if alias in annotations and annotation.contains_aggregate:
1123                 if clone._fields is None:
1124                     clone.query.group_by = True
1125                 else:
1126                     clone.query.set_group_by()
1127                 break
1128 
1129         return clone
1130 
1131     def order_by(self, *field_names):
1132         """Return a new QuerySet instance with the ordering changed."""
1133         assert not self.query.is_sliced, \
1134             "Cannot reorder a query once a slice has been taken."
1135         obj = self._chain()
1136         obj.query.clear_ordering(force_empty=False)
1137         obj.query.add_ordering(*field_names)
1138         return obj
1139 
1140     def distinct(self, *field_names):
1141         """
1142         Return a new QuerySet instance that will select only distinct results.
1143         """
1144         self._not_support_combined_queries('distinct')
1145         assert not self.query.is_sliced, \
1146             "Cannot create distinct fields once a slice has been taken."
1147         obj = self._chain()
1148         obj.query.add_distinct_fields(*field_names)
1149         return obj
1150 
1151     def extra(self, select=None, where=None, params=None, tables=None,
1152               order_by=None, select_params=None):
1153         """Add extra SQL fragments to the query."""
1154         self._not_support_combined_queries('extra')
1155         assert not self.query.is_sliced, \
1156             "Cannot change a query once a slice has been taken"
1157         clone = self._chain()
1158         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1159         return clone
1160 
1161     def reverse(self):
1162         """Reverse the ordering of the QuerySet."""
1163         if self.query.is_sliced:
1164             raise TypeError('Cannot reverse a query once a slice has been taken.')
1165         clone = self._chain()
1166         clone.query.standard_ordering = not clone.query.standard_ordering
1167         return clone
1168 
1169     def defer(self, *fields):
1170         """
1171         Defer the loading of data for certain fields until they are accessed.
1172         Add the set of deferred fields to any existing set of deferred fields.
1173         The only exception to this is if None is passed in as the only
1174         parameter, in which case removal all deferrals.
1175         """
1176         self._not_support_combined_queries('defer')
1177         if self._fields is not None:
1178             raise TypeError("Cannot call defer() after .values() or .values_list()")
1179         clone = self._chain()
1180         if fields == (None,):
1181             clone.query.clear_deferred_loading()
1182         else:
1183             clone.query.add_deferred_loading(fields)
1184         return clone
1185 
1186     def only(self, *fields):
1187         """
1188         Essentially, the opposite of defer(). Only the fields passed into this
1189         method and that are not already specified as deferred are loaded
1190         immediately when the queryset is evaluated.
1191         """
1192         self._not_support_combined_queries('only')
1193         if self._fields is not None:
1194             raise TypeError("Cannot call only() after .values() or .values_list()")
1195         if fields == (None,):
1196             # Can only pass None to defer(), not only(), as the rest option.
1197             # That won't stop people trying to do this, so let's be explicit.
1198             raise TypeError("Cannot pass None as an argument to only().")
1199         for field in fields:
1200             field = field.split(LOOKUP_SEP, 1)[0]
1201             if field in self.query._filtered_relations:
1202                 raise ValueError('only() is not supported with FilteredRelation.')
1203         clone = self._chain()
1204         clone.query.add_immediate_loading(fields)
1205         return clone
1206 
1207     def using(self, alias):
1208         """Select which database this QuerySet should execute against."""
1209         clone = self._chain()
1210         clone._db = alias
1211         return clone
1212 
1213     ###################################
1214     # PUBLIC INTROSPECTION ATTRIBUTES #
1215     ###################################
1216 
1217     @property
1218     def ordered(self):
1219         """
1220         Return True if the QuerySet is ordered -- i.e. has an order_by()
1221         clause or a default ordering on the model (or is empty).
1222         """
1223         if isinstance(self, EmptyQuerySet):
1224             return True
1225         if self.query.extra_order_by or self.query.order_by:
1226             return True
1227         elif (
1228             self.query.default_ordering and
1229             self.query.get_meta().ordering and
1230             # A default ordering doesn't affect GROUP BY queries.
1231             not self.query.group_by
1232         ):
1233             return True
1234         else:
1235             return False
1236 
1237     @property
1238     def db(self):
1239         """Return the database used if this query is executed now."""
1240         if self._for_write:
1241             return self._db or router.db_for_write(self.model, **self._hints)
1242         return self._db or router.db_for_read(self.model, **self._hints)
1243 
1244     ###################
1245     # PRIVATE METHODS #
1246     ###################
1247 
1248     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1249         """
1250         Insert a new record for the given model. This provides an interface to
1251         the InsertQuery class and is how Model.save() is implemented.
1252         """
1253         self._for_write = True
1254         if using is None:
1255             using = self.db
1256         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1257         query.insert_values(fields, objs, raw=raw)
1258         return query.get_compiler(using=using).execute_sql(returning_fields)
1259     _insert.alters_data = True
1260     _insert.queryset_only = False
1261 
1262     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1263         """
1264         Helper method for bulk_create() to insert objs one batch at a time.
1265         """
1266         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1267             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1268         ops = connections[self.db].ops
1269         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1270         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1271         inserted_rows = []
1272         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1273         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1274             if bulk_return and not ignore_conflicts:
1275                 inserted_rows.extend(self._insert(
1276                     item, fields=fields, using=self.db,
1277                     returning_fields=self.model._meta.db_returning_fields,
1278                     ignore_conflicts=ignore_conflicts,
1279                 ))
1280             else:
1281                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1282         return inserted_rows
1283 
1284     def _chain(self, **kwargs):
1285         """
1286         Return a copy of the current QuerySet that's ready for another
1287         operation.
1288         """
1289         obj = self._clone()
1290         if obj._sticky_filter:
1291             obj.query.filter_is_sticky = True
1292             obj._sticky_filter = False
1293         obj.__dict__.update(kwargs)
1294         return obj
1295 
1296     def _clone(self):
1297         """
1298         Return a copy of the current QuerySet. A lightweight alternative
1299         to deepcopy().
1300         """
1301         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1302         c._sticky_filter = self._sticky_filter
1303         c._for_write = self._for_write
1304         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1305         c._known_related_objects = self._known_related_objects
1306         c._iterable_class = self._iterable_class
1307         c._fields = self._fields
1308         return c
1309 
1310     def _fetch_all(self):
1311         if self._result_cache is None:
1312             self._result_cache = list(self._iterable_class(self))
1313         if self._prefetch_related_lookups and not self._prefetch_done:
1314             self._prefetch_related_objects()
1315 
1316     def _next_is_sticky(self):
1317         """
1318         Indicate that the next filter call and the one following that should
1319         be treated as a single filter. This is only important when it comes to
1320         determining when to reuse tables for many-to-many filters. Required so
1321         that we can filter naturally on the results of related managers.
1322 
1323         This doesn't return a clone of the current QuerySet (it returns
1324         "self"). The method is only used internally and should be immediately
1325         followed by a filter() that does create a clone.
1326         """
1327         self._sticky_filter = True
1328         return self
1329 
1330     def _merge_sanity_check(self, other):
1331         """Check that two QuerySet classes may be merged."""
1332         if self._fields is not None and (
1333                 set(self.query.values_select) != set(other.query.values_select) or
1334                 set(self.query.extra_select) != set(other.query.extra_select) or
1335                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1336             raise TypeError(
1337                 "Merging '%s' classes must involve the same values in each case."
1338                 % self.__class__.__name__
1339             )
1340 
1341     def _merge_known_related_objects(self, other):
1342         """
1343         Keep track of all known related objects from either QuerySet instance.
1344         """
1345         for field, objects in other._known_related_objects.items():
1346             self._known_related_objects.setdefault(field, {}).update(objects)
1347 
1348     def resolve_expression(self, *args, **kwargs):
1349         if self._fields and len(self._fields) > 1:
1350             # values() queryset can only be used as nested queries
1351             # if they are set up to select only a single field.
1352             raise TypeError('Cannot use multi-field values as a filter value.')
1353         query = self.query.resolve_expression(*args, **kwargs)
1354         query._db = self._db
1355         return query
1356     resolve_expression.queryset_only = True
1357 
1358     def _add_hints(self, **hints):
1359         """
1360         Update hinting information for use by routers. Add new key/values or
1361         overwrite existing key/values.
1362         """
1363         self._hints.update(hints)
1364 
1365     def _has_filters(self):
1366         """
1367         Check if this QuerySet has any filtering going on. This isn't
1368         equivalent with checking if all objects are present in results, for
1369         example, qs[1:]._has_filters() -> False.
1370         """
1371         return self.query.has_filters()
1372 
1373     @staticmethod
1374     def _validate_values_are_expressions(values, method_name):
1375         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1376         if invalid_args:
1377             raise TypeError(
1378                 'QuerySet.%s() received non-expression(s): %s.' % (
1379                     method_name,
1380                     ', '.join(invalid_args),
1381                 )
1382             )
1383 
1384     def _not_support_combined_queries(self, operation_name):
1385         if self.query.combinator:
1386             raise NotSupportedError(
1387                 'Calling QuerySet.%s() after %s() is not supported.'
1388                 % (operation_name, self.query.combinator)
1389             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 00:43:06.771 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 00:43:06.771 | INFO     | Search-only: skipping patch selection
2025-10-18 00:43:06.915 | INFO     | Task django__django-13371 completed successfully.
2025-10-18 00:43:06.916 | INFO     | No patch generated. You can try running ACR again.
