[
    {
        "rel_file_path": "pylint/lint/parallel.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pylint-dev__pylint/setup_pylint-dev__pylint__3.0/pylint/lint/parallel.py",
        "start": 1,
        "end": 173,
        "class_name": null,
        "method_name": null,
        "code": "1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\n2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE\n3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt\n4 \n5 from __future__ import annotations\n6 \n7 import functools\n8 from collections import defaultdict\n9 from collections.abc import Iterable, Sequence\n10 from typing import TYPE_CHECKING, Any\n11 \n12 import dill\n13 \n14 from pylint import reporters\n15 from pylint.lint.utils import _augment_sys_path\n16 from pylint.message import Message\n17 from pylint.typing import FileItem\n18 from pylint.utils import LinterStats, merge_stats\n19 \n20 try:\n21     import multiprocessing\n22 except ImportError:\n23     multiprocessing = None  # type: ignore[assignment]\n24 \n25 try:\n26     from concurrent.futures import ProcessPoolExecutor\n27 except ImportError:\n28     ProcessPoolExecutor = None  # type: ignore[assignment,misc]\n29 \n30 if TYPE_CHECKING:\n31     from pylint.lint import PyLinter\n32 \n33 # PyLinter object used by worker processes when checking files using parallel mode\n34 # should only be used by the worker processes\n35 _worker_linter: PyLinter | None = None\n36 \n37 \n38 def _worker_initialize(\n39     linter: bytes, extra_packages_paths: Sequence[str] | None = None\n40 ) -> None:\n41     \"\"\"Function called to initialize a worker for a Process within a concurrent Pool.\n42 \n43     :param linter: A linter-class (PyLinter) instance pickled with dill\n44     :param extra_packages_paths: Extra entries to be added to `sys.path`\n45     \"\"\"\n46     global _worker_linter  # pylint: disable=global-statement\n47     _worker_linter = dill.loads(linter)\n48     assert _worker_linter\n49 \n50     # On the worker process side the messages are just collected and passed back to\n51     # parent process as _worker_check_file function's return value\n52     _worker_linter.set_reporter(reporters.CollectingReporter())\n53     _worker_linter.open()\n54 \n55     # Re-register dynamic plugins, since the pool does not have access to the\n56     # astroid module that existed when the linter was pickled.\n57     _worker_linter.load_plugin_modules(_worker_linter._dynamic_plugins, force=True)\n58     _worker_linter.load_plugin_configuration()\n59 \n60     if extra_packages_paths:\n61         _augment_sys_path(extra_packages_paths)\n62 \n63 \n64 def _worker_check_single_file(\n65     file_item: FileItem,\n66 ) -> tuple[\n67     int,\n68     str,\n69     str,\n70     str,\n71     list[Message],\n72     LinterStats,\n73     int,\n74     defaultdict[str, list[Any]],\n75 ]:\n76     if not _worker_linter:\n77         raise RuntimeError(\"Worker linter not yet initialised\")\n78     _worker_linter.open()\n79     _worker_linter.check_single_file_item(file_item)\n80     mapreduce_data = defaultdict(list)\n81     for checker in _worker_linter.get_checkers():\n82         data = checker.get_map_data()\n83         if data is not None:\n84             mapreduce_data[checker.name].append(data)\n85     msgs = _worker_linter.reporter.messages\n86     assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)\n87     _worker_linter.reporter.reset()\n88     return (\n89         id(multiprocessing.current_process()),\n90         _worker_linter.current_name,\n91         file_item.filepath,\n92         _worker_linter.file_state.base_name,\n93         msgs,\n94         _worker_linter.stats,\n95         _worker_linter.msg_status,\n96         mapreduce_data,\n97     )\n98 \n99 \n100 def _merge_mapreduce_data(\n101     linter: PyLinter,\n102     all_mapreduce_data: defaultdict[int, list[defaultdict[str, list[Any]]]],\n103 ) -> None:\n104     \"\"\"Merges map/reduce data across workers, invoking relevant APIs on checkers.\"\"\"\n105     # First collate the data and prepare it, so we can send it to the checkers for\n106     # validation. The intent here is to collect all the mapreduce data for all checker-\n107     # runs across processes - that will then be passed to a static method on the\n108     # checkers to be reduced and further processed.\n109     collated_map_reduce_data: defaultdict[str, list[Any]] = defaultdict(list)\n110     for linter_data in all_mapreduce_data.values():\n111         for run_data in linter_data:\n112             for checker_name, data in run_data.items():\n113                 collated_map_reduce_data[checker_name].extend(data)\n114 \n115     # Send the data to checkers that support/require consolidated data\n116     original_checkers = linter.get_checkers()\n117     for checker in original_checkers:\n118         if checker.name in collated_map_reduce_data:\n119             # Assume that if the check has returned map/reduce data that it has the\n120             # reducer function\n121             checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])\n122 \n123 \n124 def check_parallel(\n125     linter: PyLinter,\n126     jobs: int,\n127     files: Iterable[FileItem],\n128     extra_packages_paths: Sequence[str] | None = None,\n129 ) -> None:\n130     \"\"\"Use the given linter to lint the files with given amount of workers (jobs).\n131 \n132     This splits the work filestream-by-filestream. If you need to do work across\n133     multiple files, as in the similarity-checker, then implement the map/reduce mixin functionality.\n134     \"\"\"\n135     # The linter is inherited by all the pool's workers, i.e. the linter\n136     # is identical to the linter object here. This is required so that\n137     # a custom PyLinter object can be used.\n138     initializer = functools.partial(\n139         _worker_initialize, extra_packages_paths=extra_packages_paths\n140     )\n141     with ProcessPoolExecutor(\n142         max_workers=jobs, initializer=initializer, initargs=(dill.dumps(linter),)\n143     ) as executor:\n144         linter.open()\n145         all_stats = []\n146         all_mapreduce_data: defaultdict[\n147             int, list[defaultdict[str, list[Any]]]\n148         ] = defaultdict(list)\n149 \n150         # Maps each file to be worked on by a single _worker_check_single_file() call,\n151         # collecting any map/reduce data by checker module so that we can 'reduce' it\n152         # later.\n153         for (\n154             worker_idx,  # used to merge map/reduce data across workers\n155             module,\n156             file_path,\n157             base_name,\n158             messages,\n159             stats,\n160             msg_status,\n161             mapreduce_data,\n162         ) in executor.map(_worker_check_single_file, files):\n163             linter.file_state.base_name = base_name\n164             linter.file_state._is_base_filestate = False\n165             linter.set_current_module(module, file_path)\n166             for msg in messages:\n167                 linter.reporter.handle_message(msg)\n168             all_stats.append(stats)\n169             all_mapreduce_data[worker_idx].append(mapreduce_data)\n170             linter.msg_status |= msg_status\n171 \n172     _merge_mapreduce_data(linter, all_mapreduce_data)\n173     linter.stats = merge_stats([linter.stats, *all_stats])\n",
        "intended_behavior": "This file should manage the distribution and execution of linting tasks across multiple processes in a way that produces consistent linting results, regardless of the number of jobs specified. The specific mechanism for splitting tasks, initializing subprocesses with correct configurations, and aggregating results needs to ensure that no checks are skipped or duplicated across processes. It might require enhancing the way tasks are partitioned and results collected to ensure consistency."
    },
    {
        "rel_file_path": "pylint/lint/run.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pylint-dev__pylint/setup_pylint-dev__pylint__3.0/pylint/lint/run.py",
        "start": 1,
        "end": 240,
        "class_name": null,
        "method_name": null,
        "code": "1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\n2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE\n3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt\n4 \n5 from __future__ import annotations\n6 \n7 import os\n8 import sys\n9 import warnings\n10 from collections.abc import Sequence\n11 from pathlib import Path\n12 from typing import ClassVar\n13 \n14 from pylint import config\n15 from pylint.checkers.utils import clear_lru_caches\n16 from pylint.config._pylint_config import (\n17     _handle_pylint_config_commands,\n18     _register_generate_config_options,\n19 )\n20 from pylint.config.config_initialization import _config_initialization\n21 from pylint.config.exceptions import ArgumentPreprocessingError\n22 from pylint.config.utils import _preprocess_options\n23 from pylint.constants import full_version\n24 from pylint.lint.base_options import _make_run_options\n25 from pylint.lint.pylinter import MANAGER, PyLinter\n26 from pylint.reporters.base_reporter import BaseReporter\n27 \n28 try:\n29     import multiprocessing\n30     from multiprocessing import synchronize  # noqa pylint: disable=unused-import\n31 except ImportError:\n32     multiprocessing = None  # type: ignore[assignment]\n33 \n34 try:\n35     from concurrent.futures import ProcessPoolExecutor\n36 except ImportError:\n37     ProcessPoolExecutor = None  # type: ignore[assignment,misc]\n38 \n39 \n40 def _query_cpu() -> int | None:\n41     \"\"\"Try to determine number of CPUs allotted in a docker container.\n42 \n43     This is based on discussion and copied from suggestions in\n44     https://bugs.python.org/issue36054.\n45     \"\"\"\n46     cpu_quota, avail_cpu = None, None\n47 \n48     if Path(\"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\").is_file():\n49         with open(\"/sys/fs/cgroup/cpu/cpu.cfs_quota_us\", encoding=\"utf-8\") as file:\n50             # Not useful for AWS Batch based jobs as result is -1, but works on local linux systems\n51             cpu_quota = int(file.read().rstrip())\n52 \n53     if (\n54         cpu_quota\n55         and cpu_quota != -1\n56         and Path(\"/sys/fs/cgroup/cpu/cpu.cfs_period_us\").is_file()\n57     ):\n58         with open(\"/sys/fs/cgroup/cpu/cpu.cfs_period_us\", encoding=\"utf-8\") as file:\n59             cpu_period = int(file.read().rstrip())\n60         # Divide quota by period and you should get num of allotted CPU to the container,\n61         # rounded down if fractional.\n62         avail_cpu = int(cpu_quota / cpu_period)\n63     elif Path(\"/sys/fs/cgroup/cpu/cpu.shares\").is_file():\n64         with open(\"/sys/fs/cgroup/cpu/cpu.shares\", encoding=\"utf-8\") as file:\n65             cpu_shares = int(file.read().rstrip())\n66         # For AWS, gives correct value * 1024.\n67         avail_cpu = int(cpu_shares / 1024)\n68 \n69     # In K8s Pods also a fraction of a single core could be available\n70     # As multiprocessing is not able to run only a \"fraction\" of process\n71     # assume we have 1 CPU available\n72     if avail_cpu == 0:\n73         avail_cpu = 1\n74 \n75     return avail_cpu\n76 \n77 \n78 def _cpu_count() -> int:\n79     \"\"\"Use sched_affinity if available for virtualized or containerized\n80     environments.\n81     \"\"\"\n82     cpu_share = _query_cpu()\n83     cpu_count = None\n84     sched_getaffinity = getattr(os, \"sched_getaffinity\", None)\n85     # pylint: disable=not-callable,using-constant-test,useless-suppression\n86     if sched_getaffinity:\n87         cpu_count = len(sched_getaffinity(0))\n88     elif multiprocessing:\n89         cpu_count = multiprocessing.cpu_count()\n90     else:\n91         cpu_count = 1\n92     if sys.platform == \"win32\":\n93         # See also https://github.com/python/cpython/issues/94242\n94         cpu_count = min(cpu_count, 56)  # pragma: no cover\n95     if cpu_share is not None:\n96         return min(cpu_share, cpu_count)\n97     return cpu_count\n98 \n99 \n100 class Run:\n101     \"\"\"Helper class to use as main for pylint with 'run(*sys.argv[1:])'.\"\"\"\n102 \n103     LinterClass = PyLinter\n104     option_groups = (\n105         (\n106             \"Commands\",\n107             \"Options which are actually commands. Options in this \\\n108 group are mutually exclusive.\",\n109         ),\n110     )\n111     _is_pylint_config: ClassVar[bool] = False\n112     \"\"\"Boolean whether or not this is a 'pylint-config' run.\n113 \n114     Used by _PylintConfigRun to make the 'pylint-config' command work.\n115     \"\"\"\n116 \n117     # pylint: disable = too-many-statements, too-many-branches\n118     def __init__(\n119         self,\n120         args: Sequence[str],\n121         reporter: BaseReporter | None = None,\n122         exit: bool = True,  # pylint: disable=redefined-builtin\n123     ) -> None:\n124         # Immediately exit if user asks for version\n125         if \"--version\" in args:\n126             print(full_version)\n127             sys.exit(0)\n128 \n129         self._rcfile: str | None = None\n130         self._output: str | None = None\n131         self._plugins: list[str] = []\n132         self.verbose: bool = False\n133 \n134         # Pre-process certain options and remove them from args list\n135         try:\n136             args = _preprocess_options(self, args)\n137         except ArgumentPreprocessingError as ex:\n138             print(ex, file=sys.stderr)\n139             sys.exit(32)\n140 \n141         # Determine configuration file\n142         if self._rcfile is None:\n143             default_file = next(config.find_default_config_files(), None)\n144             if default_file:\n145                 self._rcfile = str(default_file)\n146 \n147         self.linter = linter = self.LinterClass(\n148             _make_run_options(self),\n149             option_groups=self.option_groups,\n150             pylintrc=self._rcfile,\n151         )\n152         # register standard checkers\n153         linter.load_default_plugins()\n154         # load command line plugins\n155         linter.load_plugin_modules(self._plugins)\n156 \n157         # Register the options needed for 'pylint-config'\n158         # By not registering them by default they don't show up in the normal usage message\n159         if self._is_pylint_config:\n160             _register_generate_config_options(linter._arg_parser)\n161 \n162         args = _config_initialization(\n163             linter, args, reporter, config_file=self._rcfile, verbose_mode=self.verbose\n164         )\n165 \n166         # Handle the 'pylint-config' command\n167         if self._is_pylint_config:\n168             warnings.warn(\n169                 \"NOTE: The 'pylint-config' command is experimental and usage can change\",\n170                 UserWarning,\n171                 stacklevel=2,\n172             )\n173             code = _handle_pylint_config_commands(linter)\n174             if exit:\n175                 sys.exit(code)\n176             return\n177 \n178         # Display help messages if there are no files to lint\n179         if not args:\n180             print(linter.help())\n181             sys.exit(32)\n182 \n183         if linter.config.jobs < 0:\n184             print(\n185                 f\"Jobs number ({linter.config.jobs}) should be greater than or equal to 0\",\n186                 file=sys.stderr,\n187             )\n188             sys.exit(32)\n189         if linter.config.jobs > 1 or linter.config.jobs == 0:\n190             if ProcessPoolExecutor is None:\n191                 print(\n192                     \"concurrent.futures module is missing, fallback to single process\",\n193                     file=sys.stderr,\n194                 )\n195                 linter.set_option(\"jobs\", 1)\n196             elif linter.config.jobs == 0:\n197                 linter.config.jobs = _cpu_count()\n198 \n199         if self._output:\n200             try:\n201                 with open(self._output, \"w\", encoding=\"utf-8\") as output:\n202                     linter.reporter.out = output\n203                     linter.check(args)\n204                     score_value = linter.generate_reports()\n205             except OSError as ex:\n206                 print(ex, file=sys.stderr)\n207                 sys.exit(32)\n208         else:\n209             linter.check(args)\n210             score_value = linter.generate_reports()\n211         if linter.config.clear_cache_post_run:\n212             clear_lru_caches()\n213             MANAGER.clear_cache()\n214 \n215         if exit:\n216             if linter.config.exit_zero:\n217                 sys.exit(0)\n218             elif linter.any_fail_on_issues():\n219                 # We need to make sure we return a failing exit code in this case.\n220                 # So we use self.linter.msg_status if that is non-zero, otherwise we just return 1.\n221                 sys.exit(self.linter.msg_status or 1)\n222             elif score_value is not None:\n223                 if score_value >= linter.config.fail_under:\n224                     sys.exit(0)\n225                 else:\n226                     # We need to make sure we return a failing exit code in this case.\n227                     # So we use self.linter.msg_status if that is non-zero, otherwise we just return 1.\n228                     sys.exit(self.linter.msg_status or 1)\n229             else:\n230                 sys.exit(self.linter.msg_status)\n231 \n232 \n233 class _PylintConfigRun(Run):\n234     \"\"\"A private wrapper for the 'pylint-config' command.\"\"\"\n235 \n236     _is_pylint_config: ClassVar[bool] = True\n237     \"\"\"Boolean whether or not this is a 'pylint-config' run.\n238 \n239     Used by _PylintConfigRun to make the 'pylint-config' command work.\n240     \"\"\"\n",
        "intended_behavior": "When parsing the `--jobs` argument, this file should set up the linting process in a manner that ensures a consistent set of lint checks is applied across all specified files, independent of the number of parallel jobs. If the handling of `--jobs` directly affects which checks are run or how results are reported, this behavior needs adjustment to maintain the integrity of lint results. It might involve better integration with `parallel.py` to ensure that the parallel execution path mimics the serial path as closely as possible in terms of checks run and results reported."
    }
]