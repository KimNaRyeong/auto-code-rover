diff --git a/sklearn/ensemble/_gb_losses.py b/sklearn/ensemble/_gb_losses.py
new file mode 100644
--- /dev/null
+++ b/sklearn/ensemble/_gb_losses.py
@@ -0,0 +1,884 @@
+"""Losses and corresponding default initial estimators for gradient boosting
+decision trees.
+"""
+
+from abc import ABCMeta
+from abc import abstractmethod
+
+import numpy as np
+from scipy.special import expit
+
+from ..tree._tree import TREE_LEAF
+from ..utils.fixes import logsumexp
+from ..utils.stats import _weighted_percentile
+from ..dummy import DummyClassifier
+from ..dummy import DummyRegressor
+
+
+class LossFunction(metaclass=ABCMeta):
+    """Abstract base class for various loss functions.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+
+    Attributes
+    ----------
+    K : int
+        The number of regression trees to be induced;
+        1 for regression and binary classification;
+        ``n_classes`` for multi-class classification.
+    """
+
+    is_multi_class = False
+
+    def __init__(self, n_classes):
+        self.K = n_classes
+
+    def init_estimator(self):
+        """Default ``init`` estimator for loss function. """
+        raise NotImplementedError()
+
+    @abstractmethod
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the loss.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves).
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+
+    @abstractmethod
+    def negative_gradient(self, y, raw_predictions, **kargs):
+        """Compute the negative gradient.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            The target labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        """
+
+    def update_terminal_regions(self, tree, X, y, residual, raw_predictions,
+                                sample_weight, sample_mask,
+                                learning_rate=0.1, k=0):
+        """Update the terminal regions (=leaves) of the given tree and
+        updates the current predictions of the model. Traverses tree
+        and invokes template method `_update_terminal_region`.
+
+        Parameters
+        ----------
+        tree : tree.Tree
+            The tree object.
+        X : 2d array, shape (n, m)
+            The data array.
+        y : 1d array, shape (n,)
+            The target labels.
+        residual : 1d array, shape (n,)
+            The residuals (usually the negative gradient).
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        sample_weight : 1d array, shape (n,)
+            The weight of each sample.
+        sample_mask : 1d array, shape (n,)
+            The sample mask to be used.
+        learning_rate : float, default=0.1
+            Learning rate shrinks the contribution of each tree by
+             ``learning_rate``.
+        k : int, default=0
+            The index of the estimator being updated.
+
+        """
+        # compute leaf for each sample in ``X``.
+        terminal_regions = tree.apply(X)
+
+        # mask all which are not in sample mask.
+        masked_terminal_regions = terminal_regions.copy()
+        masked_terminal_regions[~sample_mask] = -1
+
+        # update each leaf (= perform line search)
+        for leaf in np.where(tree.children_left == TREE_LEAF)[0]:
+            self._update_terminal_region(tree, masked_terminal_regions,
+                                         leaf, X, y, residual,
+                                         raw_predictions[:, k], sample_weight)
+
+        # update predictions (both in-bag and out-of-bag)
+        raw_predictions[:, k] += \
+            learning_rate * tree.value[:, 0, 0].take(terminal_regions, axis=0)
+
+    @abstractmethod
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        """Template method for updating terminal regions (i.e., leaves)."""
+
+    @abstractmethod
+    def get_init_raw_predictions(self, X, estimator):
+        """Return the initial raw predictions.
+
+        Parameters
+        ----------
+        X : 2d array, shape (n_samples, n_features)
+            The data array.
+        estimator : estimator instance
+            The estimator to use to compute the predictions.
+
+        Returns
+        -------
+        raw_predictions : 2d array, shape (n_samples, K)
+            The initial raw predictions. K is equal to 1 for binary
+            classification and regression, and equal to the number of classes
+            for multiclass classification. ``raw_predictions`` is casted
+            into float64.
+        """
+        pass
+
+
+class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
+    """Base class for regression loss functions.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+    """
+    def __init__(self, n_classes):
+        if n_classes != 1:
+            raise ValueError("``n_classes`` must be 1 for regression but "
+                             "was %r" % n_classes)
+        super().__init__(n_classes)
+
+    def check_init_estimator(self, estimator):
+        """Make sure estimator has the required fit and predict methods.
+
+        Parameters
+        ----------
+        estimator : estimator instance
+            The init estimator to check.
+        """
+        if not (hasattr(estimator, 'fit') and hasattr(estimator, 'predict')):
+            raise ValueError(
+                "The init parameter must be a valid estimator and "
+                "support both fit and predict."
+            )
+
+    def get_init_raw_predictions(self, X, estimator):
+        predictions = estimator.predict(X)
+        return predictions.reshape(-1, 1).astype(np.float64)
+
+
+class LeastSquaresError(RegressionLossFunction):
+    """Loss function for least squares (LS) estimation.
+    Terminal regions do not need to be updated for least squares.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+    """
+
+    def init_estimator(self):
+        return DummyRegressor(strategy='mean')
+
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the least squares loss.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw_predictions (i.e. values from the tree leaves).
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+        if sample_weight is None:
+            return np.mean((y - raw_predictions.ravel()) ** 2)
+        else:
+            return (1 / sample_weight.sum() * np.sum(
+                sample_weight * ((y - raw_predictions.ravel()) ** 2)))
+
+    def negative_gradient(self, y, raw_predictions, **kargs):
+        """Compute the negative gradient.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            The target labels.
+
+        raw_predictions : 1d array, shape (n_samples,)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        """
+        return y - raw_predictions.ravel()
+
+    def update_terminal_regions(self, tree, X, y, residual, raw_predictions,
+                                sample_weight, sample_mask,
+                                learning_rate=0.1, k=0):
+        """Least squares does not need to update terminal regions.
+
+        But it has to update the predictions.
+
+        Parameters
+        ----------
+        tree : tree.Tree
+            The tree object.
+        X : 2d array, shape (n, m)
+            The data array.
+        y : 1d array, shape (n,)
+            The target labels.
+        residual : 1d array, shape (n,)
+            The residuals (usually the negative gradient).
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        sample_weight : 1d array, shape (n,)
+            The weight of each sample.
+        sample_mask : 1d array, shape (n,)
+            The sample mask to be used.
+        learning_rate : float, default=0.1
+            Learning rate shrinks the contribution of each tree by
+             ``learning_rate``.
+        k : int, default=0
+            The index of the estimator being updated.
+        """
+        # update predictions
+        raw_predictions[:, k] += learning_rate * tree.predict(X).ravel()
+
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        pass
+
+
+class LeastAbsoluteError(RegressionLossFunction):
+    """Loss function for least absolute deviation (LAD) regression.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes
+    """
+    def init_estimator(self):
+        return DummyRegressor(strategy='quantile', quantile=.5)
+
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the least absolute error.
+
+        Parameters
+        ----------
+        y : array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : array, shape (n_samples, K)
+            The raw_predictions (i.e. values from the tree leaves).
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+        if sample_weight is None:
+            return np.abs(y - raw_predictions.ravel()).mean()
+        else:
+            return (1 / sample_weight.sum() * np.sum(
+                sample_weight * np.abs(y - raw_predictions.ravel())))
+
+    def negative_gradient(self, y, raw_predictions, **kargs):
+        """Compute the negative gradient.
+
+        1.0 if y - raw_predictions > 0.0 else -1.0
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            The target labels.
+
+        raw_predictions : array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        """
+        raw_predictions = raw_predictions.ravel()
+        return 2 * (y - raw_predictions > 0) - 1
+
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        """LAD updates terminal regions to median estimates."""
+        terminal_region = np.where(terminal_regions == leaf)[0]
+        sample_weight = sample_weight.take(terminal_region, axis=0)
+        diff = (y.take(terminal_region, axis=0) -
+                raw_predictions.take(terminal_region, axis=0))
+        tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight,
+                                                      percentile=50)
+
+
+class HuberLossFunction(RegressionLossFunction):
+    """Huber loss function for robust regression.
+
+    M-Regression proposed in Friedman 2001.
+
+    References
+    ----------
+    J. Friedman, Greedy Function Approximation: A Gradient Boosting
+    Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+
+    alpha : float, default=0.9
+        Percentile at which to extract score.
+    """
+
+    def __init__(self, n_classes, alpha=0.9):
+        super().__init__(n_classes)
+        self.alpha = alpha
+        self.gamma = None
+
+    def init_estimator(self):
+        return DummyRegressor(strategy='quantile', quantile=.5)
+
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the Huber loss.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble.
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+        raw_predictions = raw_predictions.ravel()
+        diff = y - raw_predictions
+        gamma = self.gamma
+        if gamma is None:
+            if sample_weight is None:
+                gamma = np.percentile(np.abs(diff), self.alpha * 100)
+            else:
+                gamma = _weighted_percentile(np.abs(diff), sample_weight,
+                                             self.alpha * 100)
+
+        gamma_mask = np.abs(diff) <= gamma
+        if sample_weight is None:
+            sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2)
+            lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) -
+                                       gamma / 2))
+            loss = (sq_loss + lin_loss) / y.shape[0]
+        else:
+            sq_loss = np.sum(0.5 * sample_weight[gamma_mask] *
+                             diff[gamma_mask] ** 2)
+            lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *
+                              (np.abs(diff[~gamma_mask]) - gamma / 2))
+            loss = (sq_loss + lin_loss) / sample_weight.sum()
+        return loss
+
+    def negative_gradient(self, y, raw_predictions, sample_weight=None,
+                          **kargs):
+        """Compute the negative gradient.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            The target labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+        raw_predictions = raw_predictions.ravel()
+        diff = y - raw_predictions
+        if sample_weight is None:
+            gamma = np.percentile(np.abs(diff), self.alpha * 100)
+        else:
+            gamma = _weighted_percentile(np.abs(diff), sample_weight,
+                                         self.alpha * 100)
+        gamma_mask = np.abs(diff) <= gamma
+        residual = np.zeros((y.shape[0],), dtype=np.float64)
+        residual[gamma_mask] = diff[gamma_mask]
+        residual[~gamma_mask] = gamma * np.sign(diff[~gamma_mask])
+        self.gamma = gamma
+        return residual
+
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        terminal_region = np.where(terminal_regions == leaf)[0]
+        sample_weight = sample_weight.take(terminal_region, axis=0)
+        gamma = self.gamma
+        diff = (y.take(terminal_region, axis=0)
+                - raw_predictions.take(terminal_region, axis=0))
+        median = _weighted_percentile(diff, sample_weight, percentile=50)
+        diff_minus_median = diff - median
+        tree.value[leaf, 0] = median + np.mean(
+            np.sign(diff_minus_median) *
+            np.minimum(np.abs(diff_minus_median), gamma))
+
+
+class QuantileLossFunction(RegressionLossFunction):
+    """Loss function for quantile regression.
+
+    Quantile regression allows to estimate the percentiles
+    of the conditional distribution of the target.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+
+    alpha : float, optional (default = 0.9)
+        The percentile.
+    """
+    def __init__(self, n_classes, alpha=0.9):
+        super().__init__(n_classes)
+        self.alpha = alpha
+        self.percentile = alpha * 100
+
+    def init_estimator(self):
+        return DummyRegressor(strategy='quantile', quantile=self.alpha)
+
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the Quantile loss.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble.
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+        raw_predictions = raw_predictions.ravel()
+        diff = y - raw_predictions
+        alpha = self.alpha
+
+        mask = y > raw_predictions
+        if sample_weight is None:
+            loss = (alpha * diff[mask].sum() -
+                    (1 - alpha) * diff[~mask].sum()) / y.shape[0]
+        else:
+            loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -
+                    (1 - alpha) * np.sum(sample_weight[~mask] *
+                                         diff[~mask])) / sample_weight.sum())
+        return loss
+
+    def negative_gradient(self, y, raw_predictions, **kargs):
+        """Compute the negative gradient.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            The target labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw_predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        """
+        alpha = self.alpha
+        raw_predictions = raw_predictions.ravel()
+        mask = y > raw_predictions
+        return (alpha * mask) - ((1 - alpha) * ~mask)
+
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        terminal_region = np.where(terminal_regions == leaf)[0]
+        diff = (y.take(terminal_region, axis=0)
+                - raw_predictions.take(terminal_region, axis=0))
+        sample_weight = sample_weight.take(terminal_region, axis=0)
+
+        val = _weighted_percentile(diff, sample_weight, self.percentile)
+        tree.value[leaf, 0] = val
+
+
+class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
+    """Base class for classification loss functions. """
+
+    def _raw_prediction_to_proba(self, raw_predictions):
+        """Template method to convert raw predictions into probabilities.
+
+        Parameters
+        ----------
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble.
+
+        Returns
+        -------
+        probas : 2d array, shape (n_samples, K)
+            The predicted probabilities.
+        """
+
+    @abstractmethod
+    def _raw_prediction_to_decision(self, raw_predictions):
+        """Template method to convert raw predictions to decisions.
+
+        Parameters
+        ----------
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble.
+
+        Returns
+        -------
+        encoded_predictions : 2d array, shape (n_samples, K)
+            The predicted encoded labels.
+        """
+
+    def check_init_estimator(self, estimator):
+        """Make sure estimator has fit and predict_proba methods.
+
+        Parameters
+        ----------
+        estimator : estimator instance
+            The init estimator to check.
+        """
+        if not (hasattr(estimator, 'fit') and
+                hasattr(estimator, 'predict_proba')):
+            raise ValueError(
+                "The init parameter must be a valid estimator "
+                "and support both fit and predict_proba."
+            )
+
+
+class BinomialDeviance(ClassificationLossFunction):
+    """Binomial deviance loss function for binary classification.
+
+    Binary classification is a special case; here, we only need to
+    fit one tree instead of ``n_classes`` trees.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+    """
+    def __init__(self, n_classes):
+        if n_classes != 2:
+            raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
+                             .format(self.__class__.__name__, n_classes))
+        # we only need to fit one tree for binary clf.
+        super().__init__(n_classes=1)
+
+    def init_estimator(self):
+        # return the most common class, taking into account the samples
+        # weights
+        return DummyClassifier(strategy='prior')
+
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the deviance (= 2 * negative log-likelihood).
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble.
+
+        sample_weight : 1d array , shape (n_samples,), optional
+            Sample weights.
+        """
+        # logaddexp(0, v) == log(1.0 + exp(v))
+        raw_predictions = raw_predictions.ravel()
+        if sample_weight is None:
+            return -2 * np.mean((y * raw_predictions) -
+                                np.logaddexp(0, raw_predictions))
+        else:
+            return (-2 / sample_weight.sum() * np.sum(
+                sample_weight * ((y * raw_predictions) -
+                                 np.logaddexp(0, raw_predictions))))
+
+    def negative_gradient(self, y, raw_predictions, **kargs):
+        """Compute the residual (= negative gradient).
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw_predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        """
+        return y - expit(raw_predictions.ravel())
+
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        """Make a single Newton-Raphson step.
+
+        our node estimate is given by:
+
+            sum(w * (y - prob)) / sum(w * prob * (1 - prob))
+
+        we take advantage that: y - prob = residual
+        """
+        terminal_region = np.where(terminal_regions == leaf)[0]
+        residual = residual.take(terminal_region, axis=0)
+        y = y.take(terminal_region, axis=0)
+        sample_weight = sample_weight.take(terminal_region, axis=0)
+
+        numerator = np.sum(sample_weight * residual)
+        denominator = np.sum(sample_weight *
+                             (y - residual) * (1 - y + residual))
+
+        # prevents overflow and division by zero
+        if abs(denominator) < 1e-150:
+            tree.value[leaf, 0, 0] = 0.0
+        else:
+            tree.value[leaf, 0, 0] = numerator / denominator
+
+    def _raw_prediction_to_proba(self, raw_predictions):
+        proba = np.ones((raw_predictions.shape[0], 2), dtype=np.float64)
+        proba[:, 1] = expit(raw_predictions.ravel())
+        proba[:, 0] -= proba[:, 1]
+        return proba
+
+    def _raw_prediction_to_decision(self, raw_predictions):
+        proba = self._raw_prediction_to_proba(raw_predictions)
+        return np.argmax(proba, axis=1)
+
+    def get_init_raw_predictions(self, X, estimator):
+        probas = estimator.predict_proba(X)
+        proba_pos_class = probas[:, 1]
+        eps = np.finfo(np.float32).eps
+        proba_pos_class = np.clip(proba_pos_class, eps, 1 - eps)
+        # log(x / (1 - x)) is the inverse of the sigmoid (expit) function
+        raw_predictions = np.log(proba_pos_class / (1 - proba_pos_class))
+        return raw_predictions.reshape(-1, 1).astype(np.float64)
+
+
+class MultinomialDeviance(ClassificationLossFunction):
+    """Multinomial deviance loss function for multi-class classification.
+
+    For multi-class classification we need to fit ``n_classes`` trees at
+    each stage.
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+    """
+
+    is_multi_class = True
+
+    def __init__(self, n_classes):
+        if n_classes < 3:
+            raise ValueError("{0:s} requires more than 2 classes.".format(
+                self.__class__.__name__))
+        super().__init__(n_classes)
+
+    def init_estimator(self):
+        return DummyClassifier(strategy='prior')
+
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the Multinomial deviance.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble.
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+        # create one-hot label encoding
+        Y = np.zeros((y.shape[0], self.K), dtype=np.float64)
+        for k in range(self.K):
+            Y[:, k] = y == k
+
+        if sample_weight is None:
+            return np.sum(-1 * (Y * raw_predictions).sum(axis=1) +
+                          logsumexp(raw_predictions, axis=1))
+        else:
+            return np.sum(
+                -1 * sample_weight * (Y * raw_predictions).sum(axis=1) +
+                logsumexp(raw_predictions, axis=1))
+
+    def negative_gradient(self, y, raw_predictions, k=0, **kwargs):
+        """Compute negative gradient for the ``k``-th class.
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            The target labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw_predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+
+        k : int, optional default=0
+            The index of the class.
+        """
+        return y - np.nan_to_num(np.exp(raw_predictions[:, k] -
+                                        logsumexp(raw_predictions, axis=1)))
+
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        """Make a single Newton-Raphson step. """
+        terminal_region = np.where(terminal_regions == leaf)[0]
+        residual = residual.take(terminal_region, axis=0)
+        y = y.take(terminal_region, axis=0)
+        sample_weight = sample_weight.take(terminal_region, axis=0)
+
+        numerator = np.sum(sample_weight * residual)
+        numerator *= (self.K - 1) / self.K
+
+        denominator = np.sum(sample_weight * (y - residual) *
+                             (1 - y + residual))
+
+        # prevents overflow and division by zero
+        if abs(denominator) < 1e-150:
+            tree.value[leaf, 0, 0] = 0.0
+        else:
+            tree.value[leaf, 0, 0] = numerator / denominator
+
+    def _raw_prediction_to_proba(self, raw_predictions):
+        return np.nan_to_num(
+            np.exp(raw_predictions -
+                   (logsumexp(raw_predictions, axis=1)[:, np.newaxis])))
+
+    def _raw_prediction_to_decision(self, raw_predictions):
+        proba = self._raw_prediction_to_proba(raw_predictions)
+        return np.argmax(proba, axis=1)
+
+    def get_init_raw_predictions(self, X, estimator):
+        probas = estimator.predict_proba(X)
+        eps = np.finfo(np.float32).eps
+        probas = np.clip(probas, eps, 1 - eps)
+        raw_predictions = np.log(probas).astype(np.float64)
+        return raw_predictions
+
+
+class ExponentialLoss(ClassificationLossFunction):
+    """Exponential loss function for binary classification.
+
+    Same loss as AdaBoost.
+
+    References
+    ----------
+    Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
+
+    Parameters
+    ----------
+    n_classes : int
+        Number of classes.
+    """
+    def __init__(self, n_classes):
+        if n_classes != 2:
+            raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
+                             .format(self.__class__.__name__, n_classes))
+        # we only need to fit one tree for binary clf.
+        super().__init__(n_classes=1)
+
+    def init_estimator(self):
+        return DummyClassifier(strategy='prior')
+
+    def __call__(self, y, raw_predictions, sample_weight=None):
+        """Compute the exponential loss
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble.
+
+        sample_weight : 1d array, shape (n_samples,), optional
+            Sample weights.
+        """
+        raw_predictions = raw_predictions.ravel()
+        if sample_weight is None:
+            return np.mean(np.exp(-(2. * y - 1.) * raw_predictions))
+        else:
+            return (1.0 / sample_weight.sum() * np.sum(
+                sample_weight * np.exp(-(2 * y - 1) * raw_predictions)))
+
+    def negative_gradient(self, y, raw_predictions, **kargs):
+        """Compute the residual (= negative gradient).
+
+        Parameters
+        ----------
+        y : 1d array, shape (n_samples,)
+            True labels.
+
+        raw_predictions : 2d array, shape (n_samples, K)
+            The raw predictions (i.e. values from the tree leaves) of the
+            tree ensemble at iteration ``i - 1``.
+        """
+        y_ = -(2. * y - 1.)
+        return y_ * np.exp(y_ * raw_predictions.ravel())
+
+    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
+                                residual, raw_predictions, sample_weight):
+        terminal_region = np.where(terminal_regions == leaf)[0]
+        raw_predictions = raw_predictions.take(terminal_region, axis=0)
+        y = y.take(terminal_region, axis=0)
+        sample_weight = sample_weight.take(terminal_region, axis=0)
+
+        y_ = 2. * y - 1.
+
+        numerator = np.sum(y_ * sample_weight * np.exp(-y_ * raw_predictions))
+        denominator = np.sum(sample_weight * np.exp(-y_ * raw_predictions))
+
+        # prevents overflow and division by zero
+        if abs(denominator) < 1e-150:
+            tree.value[leaf, 0, 0] = 0.0
+        else:
+            tree.value[leaf, 0, 0] = numerator / denominator
+
+    def _raw_prediction_to_proba(self, raw_predictions):
+        proba = np.ones((raw_predictions.shape[0], 2), dtype=np.float64)
+        proba[:, 1] = expit(2.0 * raw_predictions.ravel())
+        proba[:, 0] -= proba[:, 1]
+        return proba
+
+    def _raw_prediction_to_decision(self, raw_predictions):
+        return (raw_predictions.ravel() >= 0).astype(np.int)
+
+    def get_init_raw_predictions(self, X, estimator):
+        probas = estimator.predict_proba(X)
+        proba_pos_class = probas[:, 1]
+        eps = np.finfo(np.float32).eps
+        proba_pos_class = np.clip(proba_pos_class, eps, 1 - eps)
+        # according to The Elements of Statistical Learning sec. 10.5, the
+        # minimizer of the exponential loss is .5 * log odds ratio. So this is
+        # the equivalent to .5 * binomial_deviance.get_init_raw_predictions()
+        raw_predictions = .5 * np.log(proba_pos_class / (1 - proba_pos_class))
+        return raw_predictions.reshape(-1, 1).astype(np.float64)
+
+
+LOSS_FUNCTIONS = {
+    'ls': LeastSquaresError,
+    'lad': LeastAbsoluteError,
+    'huber': HuberLossFunction,
+    'quantile': QuantileLossFunction,
+    'deviance': None,    # for both, multinomial and binomial
+    'exponential': ExponentialLoss,
+}
diff --git a/sklearn/ensemble/gradient_boosting.py b/sklearn/ensemble/gradient_boosting.py
--- a/sklearn/ensemble/gradient_boosting.py
+++ b/sklearn/ensemble/gradient_boosting.py
@@ -26,6 +26,8 @@
 from .base import BaseEnsemble
 from ..base import ClassifierMixin
 from ..base import RegressorMixin
+from ..base import BaseEstimator
+from ..base import is_classifier
 
 from ._gradient_boosting import predict_stages
 from ._gradient_boosting import predict_stage
@@ -44,6 +46,7 @@
 from ..tree.tree import DecisionTreeRegressor
 from ..tree._tree import DTYPE
 from ..tree._tree import TREE_LEAF
+from . import _gb_losses
 
 from ..utils import check_random_state
 from ..utils import check_array
@@ -58,6 +61,14 @@
 from ..exceptions import NotFittedError
 
 
+# FIXME: 0.23
+# All the losses and corresponding init estimators have been moved to the
+# _losses module in 0.21. We deprecate them and keep them here for now in case
+# someone has imported them. None of these losses can be used as a parameter
+# to a GBDT estimator anyway (loss param only accepts strings).
+
+@deprecated("QuantileEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class QuantileEstimator:
     """An estimator predicting the alpha-quantile of the training targets.
 
@@ -111,6 +122,8 @@ def predict(self, X):
         return y
 
 
+@deprecated("MeanEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class MeanEstimator:
     """An estimator predicting the mean of the training targets."""
     def fit(self, X, y, sample_weight=None):
@@ -152,6 +165,8 @@ def predict(self, X):
         return y
 
 
+@deprecated("LogOddsEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LogOddsEstimator:
     """An estimator predicting the log odds ratio."""
     scale = 1.0
@@ -202,11 +217,15 @@ def predict(self, X):
         return y
 
 
+@deprecated("ScaledLogOddsEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ScaledLogOddsEstimator(LogOddsEstimator):
     """Log odds ratio scaled by 0.5 -- for exponential loss. """
     scale = 0.5
 
 
+@deprecated("PriorProbablityEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class PriorProbabilityEstimator:
     """An estimator predicting the probability of each
     class in the training data.
@@ -250,8 +269,16 @@ def predict(self, X):
         return y
 
 
+@deprecated("Using ZeroEstimator is deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ZeroEstimator:
-    """An estimator that simply predicts zero. """
+    """An estimator that simply predicts zero.
+
+    .. deprecated:: 0.21
+        Using ``ZeroEstimator`` or ``init='zero'`` is deprecated in version
+        0.21 and will be removed in version 0.23.
+
+    """
 
     def fit(self, X, y, sample_weight=None):
         """Fit the estimator.
@@ -295,7 +322,13 @@ def predict(self, X):
         y.fill(0.0)
         return y
 
+    def predict_proba(self, X):
+        return self.predict(X)
+
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LossFunction(metaclass=ABCMeta):
     """Abstract base class for various loss functions.
 
@@ -403,6 +436,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
         """Template method for updating terminal regions (=leaves). """
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
     """Base class for regression loss functions.
 
@@ -418,6 +454,9 @@ def __init__(self, n_classes):
         super().__init__(n_classes)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LeastSquaresError(RegressionLossFunction):
     """Loss function for least squares (LS) estimation.
     Terminal regions need not to be updated for least squares.
@@ -501,6 +540,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
         pass
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class LeastAbsoluteError(RegressionLossFunction):
     """Loss function for least absolute deviation (LAD) regression.
 
@@ -557,6 +599,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class HuberLossFunction(RegressionLossFunction):
     """Huber loss function for robust regression.
 
@@ -660,6 +705,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
             np.minimum(np.abs(diff_minus_median), gamma))
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class QuantileLossFunction(RegressionLossFunction):
     """Loss function for quantile regression.
 
@@ -737,6 +785,9 @@ def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
         tree.value[leaf, 0] = val
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
     """Base class for classification loss functions. """
 
@@ -755,6 +806,9 @@ def _score_to_decision(self, score):
         """
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class BinomialDeviance(ClassificationLossFunction):
     """Binomial deviance loss function for binary classification.
 
@@ -846,6 +900,9 @@ def _score_to_decision(self, score):
         return np.argmax(proba, axis=1)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class MultinomialDeviance(ClassificationLossFunction):
     """Multinomial deviance loss function for multi-class classification.
 
@@ -941,6 +998,9 @@ def _score_to_decision(self, score):
         return np.argmax(proba, axis=1)
 
 
+@deprecated("All Losses in sklearn.ensemble.gradient_boosting are "
+            "deprecated in version "
+            "0.21 and will be removed in version 0.23.")
 class ExponentialLoss(ClassificationLossFunction):
     """Exponential loss function for binary classification.
 
@@ -1028,19 +1088,7 @@ def _score_to_decision(self, score):
         return (score.ravel() >= 0.0).astype(np.int)
 
 
-LOSS_FUNCTIONS = {'ls': LeastSquaresError,
-                  'lad': LeastAbsoluteError,
-                  'huber': HuberLossFunction,
-                  'quantile': QuantileLossFunction,
-                  'deviance': None,    # for both, multinomial and binomial
-                  'exponential': ExponentialLoss,
-                  }
-
-
-INIT_ESTIMATORS = {'zero': ZeroEstimator}
-
-
-class VerboseReporter:
+class VerboseReporter(object):
     """Reports verbose output to stdout.
 
     Parameters
@@ -1151,7 +1199,7 @@ def __init__(self, loss, learning_rate, n_estimators, criterion,
         self.n_iter_no_change = n_iter_no_change
         self.tol = tol
 
-    def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
+    def _fit_stage(self, i, X, y, raw_predictions, sample_weight, sample_mask,
                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
         """Fit another stage of ``n_classes_`` trees to the boosting model. """
 
@@ -1159,17 +1207,17 @@ def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
         loss = self.loss_
         original_y = y
 
-        # Need to pass a copy of y_pred to negative_gradient() because y_pred
-        # is partially updated at the end of the loop in
-        # update_terminal_regions(), and gradients need to be evaluated at
+        # Need to pass a copy of raw_predictions to negative_gradient()
+        # because raw_predictions is partially updated at the end of the loop
+        # in update_terminal_regions(), and gradients need to be evaluated at
         # iteration i - 1.
-        y_pred_copy = y_pred.copy()
+        raw_predictions_copy = raw_predictions.copy()
 
         for k in range(loss.K):
             if loss.is_multi_class:
                 y = np.array(original_y == k, dtype=np.float64)
 
-            residual = loss.negative_gradient(y, y_pred_copy, k=k,
+            residual = loss.negative_gradient(y, raw_predictions_copy, k=k,
                                               sample_weight=sample_weight)
 
             # induce regression tree on residuals
@@ -1196,14 +1244,14 @@ def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
                      check_input=False, X_idx_sorted=X_idx_sorted)
 
             # update tree leaves
-            loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
-                                         sample_weight, sample_mask,
-                                         learning_rate=self.learning_rate, k=k)
+            loss.update_terminal_regions(
+                tree.tree_, X, y, residual, raw_predictions, sample_weight,
+                sample_mask, learning_rate=self.learning_rate, k=k)
 
             # add tree to ensemble
             self.estimators_[i, k] = tree
 
-        return y_pred
+        return raw_predictions
 
     def _check_params(self):
         """Check validity of parameters and raise ValueError if not valid. """
@@ -1216,15 +1264,15 @@ def _check_params(self):
                              "was %r" % self.learning_rate)
 
         if (self.loss not in self._SUPPORTED_LOSS
-                or self.loss not in LOSS_FUNCTIONS):
+                or self.loss not in _gb_losses.LOSS_FUNCTIONS):
             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
 
         if self.loss == 'deviance':
-            loss_class = (MultinomialDeviance
+            loss_class = (_gb_losses.MultinomialDeviance
                           if len(self.classes_) > 2
-                          else BinomialDeviance)
+                          else _gb_losses.BinomialDeviance)
         else:
-            loss_class = LOSS_FUNCTIONS[self.loss]
+            loss_class = _gb_losses.LOSS_FUNCTIONS[self.loss]
 
         if self.loss in ('huber', 'quantile'):
             self.loss_ = loss_class(self.n_classes_, self.alpha)
@@ -1236,15 +1284,14 @@ def _check_params(self):
                              "was %r" % self.subsample)
 
         if self.init is not None:
-            if isinstance(self.init, str):
-                if self.init not in INIT_ESTIMATORS:
-                    raise ValueError('init="%s" is not supported' % self.init)
-            else:
-                if (not hasattr(self.init, 'fit')
-                        or not hasattr(self.init, 'predict')):
-                    raise ValueError("init=%r must be valid BaseEstimator "
-                                     "and support both fit and "
-                                     "predict" % self.init)
+            # init must be an estimator or 'zero'
+            if isinstance(self.init, BaseEstimator):
+                self.loss_.check_init_estimator(self.init)
+            elif not (isinstance(self.init, str) and self.init == 'zero'):
+                raise ValueError(
+                    "The init parameter must be an estimator or 'zero'. "
+                    "Got init={}".format(self.init)
+                )
 
         if not (0.0 < self.alpha < 1.0):
             raise ValueError("alpha must be in (0.0, 1.0) but "
@@ -1293,12 +1340,9 @@ def _check_params(self):
     def _init_state(self):
         """Initialize model state and allocate model state data structures. """
 
-        if self.init is None:
+        self.init_ = self.init
+        if self.init_ is None:
             self.init_ = self.loss_.init_estimator()
-        elif isinstance(self.init, str):
-            self.init_ = INIT_ESTIMATORS[self.init]()
-        else:
-            self.init_ = self.init
 
         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
                                     dtype=np.object)
@@ -1396,10 +1440,13 @@ def fit(self, X, y, sample_weight=None, monitor=None):
         # Check input
         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
         n_samples, self.n_features_ = X.shape
-        if sample_weight is None:
+
+        sample_weight_is_none = sample_weight is None
+        if sample_weight_is_none:
             sample_weight = np.ones(n_samples, dtype=np.float32)
         else:
             sample_weight = column_or_1d(sample_weight, warn=True)
+            sample_weight_is_none = False
 
         check_consistent_length(X, y, sample_weight)
 
@@ -1410,6 +1457,17 @@ def fit(self, X, y, sample_weight=None, monitor=None):
                 train_test_split(X, y, sample_weight,
                                  random_state=self.random_state,
                                  test_size=self.validation_fraction))
+            if is_classifier(self):
+                if self.n_classes_ != np.unique(y).shape[0]:
+                    # We choose to error here. The problem is that the init
+                    # estimator would be trained on y, which has some missing
+                    # classes now, so its predictions would not have the
+                    # correct shape.
+                    raise ValueError(
+                        'The training data after the early stopping split '
+                        'is missing some classes. Try using another random '
+                        'seed.'
+                    )
         else:
             X_val = y_val = sample_weight_val = None
 
@@ -1419,11 +1477,25 @@ def fit(self, X, y, sample_weight=None, monitor=None):
             # init state
             self._init_state()
 
-            # fit initial model - FIXME make sample_weight optional
-            self.init_.fit(X, y, sample_weight)
+            # fit initial model and initialize raw predictions
+            if self.init_ == 'zero':
+                raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
+                                           dtype=np.float64)
+            else:
+                try:
+                    self.init_.fit(X, y, sample_weight=sample_weight)
+                except TypeError:
+                    if sample_weight_is_none:
+                        self.init_.fit(X, y)
+                    else:
+                        raise ValueError(
+                            "The initial estimator {} does not support sample "
+                            "weights.".format(self.init_.__class__.__name__))
+
+                raw_predictions = \
+                    self.loss_.get_init_raw_predictions(X, self.init_)
+
 
-            # init predictions
-            y_pred = self.init_.predict(X)
             begin_at_stage = 0
 
             # The rng state must be preserved if warm_start is True
@@ -1443,7 +1515,7 @@ def fit(self, X, y, sample_weight=None, monitor=None):
             # below) are more constrained than fit. It accepts only CSR
             # matrices.
             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
-            y_pred = self._decision_function(X)
+            raw_predictions = self._raw_predict(X)
             self._resize_state()
 
         if self.presort is True and issparse(X):
@@ -1462,9 +1534,9 @@ def fit(self, X, y, sample_weight=None, monitor=None):
                                              dtype=np.int32)
 
         # fit the boosting stages
-        n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
-                                    X_val, y_val, sample_weight_val,
-                                    begin_at_stage, monitor, X_idx_sorted)
+        n_stages = self._fit_stages(
+            X, y, raw_predictions, sample_weight, self._rng, X_val, y_val,
+            sample_weight_val, begin_at_stage, monitor, X_idx_sorted)
 
         # change shape of arrays after fit (early-stopping or additional ests)
         if n_stages != self.estimators_.shape[0]:
@@ -1476,7 +1548,7 @@ def fit(self, X, y, sample_weight=None, monitor=None):
         self.n_estimators_ = n_stages
         return self
 
-    def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
+    def _fit_stages(self, X, y, raw_predictions, sample_weight, random_state,
                     X_val, y_val, sample_weight_val,
                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
         """Iteratively fits the stages.
@@ -1510,7 +1582,7 @@ def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
             loss_history = np.full(self.n_iter_no_change, np.inf)
             # We create a generator to get the predictions for X_val after
             # the addition of each successive stage
-            y_val_pred_iter = self._staged_decision_function(X_val)
+            y_val_pred_iter = self._staged_raw_predict(X_val)
 
         # perform boosting iterations
         i = begin_at_stage
@@ -1522,26 +1594,26 @@ def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
                                                   random_state)
                 # OOB score before adding this stage
                 old_oob_score = loss_(y[~sample_mask],
-                                      y_pred[~sample_mask],
+                                      raw_predictions[~sample_mask],
                                       sample_weight[~sample_mask])
 
             # fit next stage of trees
-            y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,
-                                     sample_mask, random_state, X_idx_sorted,
-                                     X_csc, X_csr)
+            raw_predictions = self._fit_stage(
+                i, X, y, raw_predictions, sample_weight, sample_mask,
+                random_state, X_idx_sorted, X_csc, X_csr)
 
             # track deviance (= loss)
             if do_oob:
                 self.train_score_[i] = loss_(y[sample_mask],
-                                             y_pred[sample_mask],
+                                             raw_predictions[sample_mask],
                                              sample_weight[sample_mask])
                 self.oob_improvement_[i] = (
                     old_oob_score - loss_(y[~sample_mask],
-                                          y_pred[~sample_mask],
+                                          raw_predictions[~sample_mask],
                                           sample_weight[~sample_mask]))
             else:
                 # no need to fancy index w/ no subsampling
-                self.train_score_[i] = loss_(y, y_pred, sample_weight)
+                self.train_score_[i] = loss_(y, raw_predictions, sample_weight)
 
             if self.verbose > 0:
                 verbose_reporter.update(i, self)
@@ -1572,26 +1644,30 @@ def _make_estimator(self, append=True):
         # we don't need _make_estimator
         raise NotImplementedError()
 
-    def _init_decision_function(self, X):
-        """Check input and compute prediction of ``init``. """
+    def _raw_predict_init(self, X):
+        """Check input and compute raw predictions of the init estimtor."""
         self._check_initialized()
         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
         if X.shape[1] != self.n_features_:
             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
                 self.n_features_, X.shape[1]))
-        score = self.init_.predict(X).astype(np.float64)
-        return score
-
-    def _decision_function(self, X):
-        # for use in inner loop, not raveling the output in single-class case,
-        # not doing input validation.
-        score = self._init_decision_function(X)
-        predict_stages(self.estimators_, X, self.learning_rate, score)
-        return score
+        if self.init_ == 'zero':
+            raw_predictions = np.zeros(shape=(X.shape[0], self.loss_.K),
+                                       dtype=np.float64)
+        else:
+            raw_predictions = self.loss_.get_init_raw_predictions(
+                X, self.init_).astype(np.float64)
+        return raw_predictions
 
+    def _raw_predict(self, X):
+        """Return the sum of the trees raw predictions (+ init estimator)."""
+        raw_predictions = self._raw_predict_init(X)
+        predict_stages(self.estimators_, X, self.learning_rate,
+                       raw_predictions)
+        return raw_predictions
 
-    def _staged_decision_function(self, X):
-        """Compute decision function of ``X`` for each iteration.
+    def _staged_raw_predict(self, X):
+        """Compute raw predictions of ``X`` for each iteration.
 
         This method allows monitoring (i.e. determine error on testing set)
         after each stage.
@@ -1605,17 +1681,18 @@ def _staged_decision_function(self, X):
 
         Returns
         -------
-        score : generator of array, shape (n_samples, k)
-            The decision function of the input samples. The order of the
+        raw_predictions : generator of array, shape (n_samples, k)
+            The raw predictions of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
             Regression and binary classification are special cases with
             ``k == 1``, otherwise ``k==n_classes``.
         """
         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
-        score = self._init_decision_function(X)
+        raw_predictions = self._raw_predict_init(X)
         for i in range(self.estimators_.shape[0]):
-            predict_stage(self.estimators_, i, X, self.learning_rate, score)
-            yield score.copy()
+            predict_stage(self.estimators_, i, X, self.learning_rate,
+                          raw_predictions)
+            yield raw_predictions.copy()
 
     @property
     def feature_importances_(self):
@@ -1793,10 +1870,11 @@ class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
 
-    init : estimator, optional
-        An estimator object that is used to compute the initial
-        predictions. ``init`` has to provide ``fit`` and ``predict``.
-        If None it uses ``loss.init_estimator``.
+    init : estimator or 'zero', optional (default=None)
+        An estimator object that is used to compute the initial predictions.
+        ``init`` has to provide `fit` and `predict_proba`. If 'zero', the
+        initial raw predictions are set to zero. By default, a
+        ``DummyEstimator`` predicting the classes priors is used.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -1984,16 +2062,17 @@ def decision_function(self, X):
         Returns
         -------
         score : array, shape (n_samples, n_classes) or (n_samples,)
-            The decision function of the input samples. The order of the
-            classes corresponds to that in the attribute `classes_`.
-            Regression and binary classification produce an array of shape
-            [n_samples].
+            The decision function of the input samples, which corresponds to
+            the raw values predicted from the trees of the ensemble . The
+            order of the classes corresponds to that in the attribute
+            `classes_`. Regression and binary classification produce an
+            array of shape [n_samples].
         """
         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
-        score = self._decision_function(X)
-        if score.shape[1] == 1:
-            return score.ravel()
-        return score
+        raw_predictions = self._raw_predict(X)
+        if raw_predictions.shape[1] == 1:
+            return raw_predictions.ravel()
+        return raw_predictions
 
     def staged_decision_function(self, X):
         """Compute decision function of ``X`` for each iteration.
@@ -2011,12 +2090,13 @@ def staged_decision_function(self, X):
         Returns
         -------
         score : generator of array, shape (n_samples, k)
-            The decision function of the input samples. The order of the
+            The decision function of the input samples, which corresponds to
+            the raw values predicted from the trees of the ensemble . The
             classes corresponds to that in the attribute `classes_`.
             Regression and binary classification are special cases with
             ``k == 1``, otherwise ``k==n_classes``.
         """
-        yield from self._staged_decision_function(X)
+        yield from self._staged_raw_predict(X)
 
     def predict(self, X):
         """Predict class for X.
@@ -2033,9 +2113,10 @@ def predict(self, X):
         y : array, shape (n_samples,)
             The predicted values.
         """
-        score = self.decision_function(X)
-        decisions = self.loss_._score_to_decision(score)
-        return self.classes_.take(decisions, axis=0)
+        raw_predictions = self.decision_function(X)
+        encoded_labels = \
+            self.loss_._raw_prediction_to_decision(raw_predictions)
+        return self.classes_.take(encoded_labels, axis=0)
 
     def staged_predict(self, X):
         """Predict class at each stage for X.
@@ -2055,9 +2136,10 @@ def staged_predict(self, X):
         y : generator of array of shape (n_samples,)
             The predicted value of the input samples.
         """
-        for score in self._staged_decision_function(X):
-            decisions = self.loss_._score_to_decision(score)
-            yield self.classes_.take(decisions, axis=0)
+        for raw_predictions in self._staged_raw_predict(X):
+            encoded_labels = \
+                self.loss_._raw_prediction_to_decision(raw_predictions)
+            yield self.classes_.take(encoded_labels, axis=0)
 
     def predict_proba(self, X):
         """Predict class probabilities for X.
@@ -2080,9 +2162,9 @@ def predict_proba(self, X):
             The class probabilities of the input samples. The order of the
             classes corresponds to that in the attribute `classes_`.
         """
-        score = self.decision_function(X)
+        raw_predictions = self.decision_function(X)
         try:
-            return self.loss_._score_to_proba(score)
+            return self.loss_._raw_prediction_to_proba(raw_predictions)
         except NotFittedError:
             raise
         except AttributeError:
@@ -2132,8 +2214,8 @@ def staged_predict_proba(self, X):
             The predicted value of the input samples.
         """
         try:
-            for score in self._staged_decision_function(X):
-                yield self.loss_._score_to_proba(score)
+            for raw_predictions in self._staged_raw_predict(X):
+                yield self.loss_._raw_prediction_to_proba(raw_predictions)
         except NotFittedError:
             raise
         except AttributeError:
@@ -2251,10 +2333,12 @@ class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
 
-    init : estimator, optional (default=None)
-        An estimator object that is used to compute the initial
-        predictions. ``init`` has to provide ``fit`` and ``predict``.
-        If None it uses ``loss.init_estimator``.
+    init : estimator or 'zero', optional (default=None)
+        An estimator object that is used to compute the initial predictions.
+        ``init`` has to provide `fit` and `predict`. If 'zero', the initial
+        raw predictions are set to zero. By default a ``DummyEstimator`` is
+        used, predicting either the average target value (for loss='ls'), or
+        a quantile for the other losses.
 
     random_state : int, RandomState instance or None, optional (default=None)
         If int, random_state is the seed used by the random number generator;
@@ -2426,7 +2510,8 @@ def predict(self, X):
             The predicted values.
         """
         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
-        return self._decision_function(X).ravel()
+        # In regression we can directly return the raw value from the trees.
+        return self._raw_predict(X).ravel()
 
     def staged_predict(self, X):
         """Predict regression target at each stage for X.
@@ -2446,8 +2531,8 @@ def staged_predict(self, X):
         y : generator of array of shape (n_samples,)
             The predicted value of the input samples.
         """
-        for y in self._staged_decision_function(X):
-            yield y.ravel()
+        for raw_predictions in self._staged_raw_predict(X):
+            yield raw_predictions.ravel()
 
     def apply(self, X):
         """Apply trees in the ensemble to X, return leaf indices.
