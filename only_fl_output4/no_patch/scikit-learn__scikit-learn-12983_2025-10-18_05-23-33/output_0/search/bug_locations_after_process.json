[
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 1937,
        "end": 1960,
        "class_name": "GradientBoostingClassifier",
        "method_name": "__init__",
        "code": "1937     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,\n1938                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n1939                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n1940                  max_depth=3, min_impurity_decrease=0.,\n1941                  min_impurity_split=None, init=None,\n1942                  random_state=None, max_features=None, verbose=0,\n1943                  max_leaf_nodes=None, warm_start=False,\n1944                  presort='auto', validation_fraction=0.1,\n1945                  n_iter_no_change=None, tol=1e-4):\n1946 \n1947         super().__init__(\n1948             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n1949             criterion=criterion, min_samples_split=min_samples_split,\n1950             min_samples_leaf=min_samples_leaf,\n1951             min_weight_fraction_leaf=min_weight_fraction_leaf,\n1952             max_depth=max_depth, init=init, subsample=subsample,\n1953             max_features=max_features,\n1954             random_state=random_state, verbose=verbose,\n1955             max_leaf_nodes=max_leaf_nodes,\n1956             min_impurity_decrease=min_impurity_decrease,\n1957             min_impurity_split=min_impurity_split,\n1958             warm_start=warm_start, presort=presort,\n1959             validation_fraction=validation_fraction,\n1960             n_iter_no_change=n_iter_no_change, tol=tol)\n",
        "intended_behavior": "The constructor (__init__) should validate the `init` parameter to ensure it can accommodate sklearn estimators correctly. Further, methods that utilize the `init` estimator for making initial predictions or setting the initial stage must ensure the output shape of these predictions aligns with the expected shape for the gradient boosting process to continue seamlessly. This might involve reshaping or appropriately aggregating the outputs from the `init` estimator. Additionally, if `init` does not support sample weights, the integration logic should handle this gracefully, allowing the gradient boosting model to be trained without passing sample weights to the `init` estimator."
    },
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 1685,
        "end": 2141,
        "class_name": "GradientBoostingClassifier",
        "method_name": null,
        "code": "1685 class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):\n1686     \"\"\"Gradient Boosting for classification.\n1687 \n1688     GB builds an additive model in a\n1689     forward stage-wise fashion; it allows for the optimization of\n1690     arbitrary differentiable loss functions. In each stage ``n_classes_``\n1691     regression trees are fit on the negative gradient of the\n1692     binomial or multinomial deviance loss function. Binary classification\n1693     is a special case where only a single regression tree is induced.\n1694 \n1695     Read more in the :ref:`User Guide <gradient_boosting>`.\n1696 \n1697     Parameters\n1698     ----------\n1699     loss : {'deviance', 'exponential'}, optional (default='deviance')\n1700         loss function to be optimized. 'deviance' refers to\n1701         deviance (= logistic regression) for classification\n1702         with probabilistic outputs. For loss 'exponential' gradient\n1703         boosting recovers the AdaBoost algorithm.\n1704 \n1705     learning_rate : float, optional (default=0.1)\n1706         learning rate shrinks the contribution of each tree by `learning_rate`.\n1707         There is a trade-off between learning_rate and n_estimators.\n1708 \n1709     n_estimators : int (default=100)\n1710         The number of boosting stages to perform. Gradient boosting\n1711         is fairly robust to over-fitting so a large number usually\n1712         results in better performance.\n1713 \n1714     subsample : float, optional (default=1.0)\n1715         The fraction of samples to be used for fitting the individual base\n1716         learners. If smaller than 1.0 this results in Stochastic Gradient\n1717         Boosting. `subsample` interacts with the parameter `n_estimators`.\n1718         Choosing `subsample < 1.0` leads to a reduction of variance\n1719         and an increase in bias.\n1720 \n1721     criterion : string, optional (default=\"friedman_mse\")\n1722         The function to measure the quality of a split. Supported criteria\n1723         are \"friedman_mse\" for the mean squared error with improvement\n1724         score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n1725         the mean absolute error. The default value of \"friedman_mse\" is\n1726         generally the best as it can provide a better approximation in\n1727         some cases.\n1728 \n1729         .. versionadded:: 0.18\n1730 \n1731     min_samples_split : int, float, optional (default=2)\n1732         The minimum number of samples required to split an internal node:\n1733 \n1734         - If int, then consider `min_samples_split` as the minimum number.\n1735         - If float, then `min_samples_split` is a fraction and\n1736           `ceil(min_samples_split * n_samples)` are the minimum\n1737           number of samples for each split.\n1738 \n1739         .. versionchanged:: 0.18\n1740            Added float values for fractions.\n1741 \n1742     min_samples_leaf : int, float, optional (default=1)\n1743         The minimum number of samples required to be at a leaf node.\n1744         A split point at any depth will only be considered if it leaves at\n1745         least ``min_samples_leaf`` training samples in each of the left and\n1746         right branches.  This may have the effect of smoothing the model,\n1747         especially in regression.\n1748 \n1749         - If int, then consider `min_samples_leaf` as the minimum number.\n1750         - If float, then `min_samples_leaf` is a fraction and\n1751           `ceil(min_samples_leaf * n_samples)` are the minimum\n1752           number of samples for each node.\n1753 \n1754         .. versionchanged:: 0.18\n1755            Added float values for fractions.\n1756 \n1757     min_weight_fraction_leaf : float, optional (default=0.)\n1758         The minimum weighted fraction of the sum total of weights (of all\n1759         the input samples) required to be at a leaf node. Samples have\n1760         equal weight when sample_weight is not provided.\n1761 \n1762     max_depth : integer, optional (default=3)\n1763         maximum depth of the individual regression estimators. The maximum\n1764         depth limits the number of nodes in the tree. Tune this parameter\n1765         for best performance; the best value depends on the interaction\n1766         of the input variables.\n1767 \n1768     min_impurity_decrease : float, optional (default=0.)\n1769         A node will be split if this split induces a decrease of the impurity\n1770         greater than or equal to this value.\n1771 \n1772         The weighted impurity decrease equation is the following::\n1773 \n1774             N_t / N * (impurity - N_t_R / N_t * right_impurity\n1775                                 - N_t_L / N_t * left_impurity)\n1776 \n1777         where ``N`` is the total number of samples, ``N_t`` is the number of\n1778         samples at the current node, ``N_t_L`` is the number of samples in the\n1779         left child, and ``N_t_R`` is the number of samples in the right child.\n1780 \n1781         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n1782         if ``sample_weight`` is passed.\n1783 \n1784         .. versionadded:: 0.19\n1785 \n1786     min_impurity_split : float, (default=1e-7)\n1787         Threshold for early stopping in tree growth. A node will split\n1788         if its impurity is above the threshold, otherwise it is a leaf.\n1789 \n1790         .. deprecated:: 0.19\n1791            ``min_impurity_split`` has been deprecated in favor of\n1792            ``min_impurity_decrease`` in 0.19. The default value of\n1793            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n1794            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n1795 \n1796     init : estimator, optional\n1797         An estimator object that is used to compute the initial\n1798         predictions. ``init`` has to provide ``fit`` and ``predict``.\n1799         If None it uses ``loss.init_estimator``.\n1800 \n1801     random_state : int, RandomState instance or None, optional (default=None)\n1802         If int, random_state is the seed used by the random number generator;\n1803         If RandomState instance, random_state is the random number generator;\n1804         If None, the random number generator is the RandomState instance used\n1805         by `np.random`.\n1806 \n1807     max_features : int, float, string or None, optional (default=None)\n1808         The number of features to consider when looking for the best split:\n1809 \n1810         - If int, then consider `max_features` features at each split.\n1811         - If float, then `max_features` is a fraction and\n1812           `int(max_features * n_features)` features are considered at each\n1813           split.\n1814         - If \"auto\", then `max_features=sqrt(n_features)`.\n1815         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n1816         - If \"log2\", then `max_features=log2(n_features)`.\n1817         - If None, then `max_features=n_features`.\n1818 \n1819         Choosing `max_features < n_features` leads to a reduction of variance\n1820         and an increase in bias.\n1821 \n1822         Note: the search for a split does not stop until at least one\n1823         valid partition of the node samples is found, even if it requires to\n1824         effectively inspect more than ``max_features`` features.\n1825 \n1826     verbose : int, default: 0\n1827         Enable verbose output. If 1 then it prints progress and performance\n1828         once in a while (the more trees the lower the frequency). If greater\n1829         than 1 then it prints progress and performance for every tree.\n1830 \n1831     max_leaf_nodes : int or None, optional (default=None)\n1832         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n1833         Best nodes are defined as relative reduction in impurity.\n1834         If None then unlimited number of leaf nodes.\n1835 \n1836     warm_start : bool, default: False\n1837         When set to ``True``, reuse the solution of the previous call to fit\n1838         and add more estimators to the ensemble, otherwise, just erase the\n1839         previous solution. See :term:`the Glossary <warm_start>`.\n1840 \n1841     presort : bool or 'auto', optional (default='auto')\n1842         Whether to presort the data to speed up the finding of best splits in\n1843         fitting. Auto mode by default will use presorting on dense data and\n1844         default to normal sorting on sparse data. Setting presort to true on\n1845         sparse data will raise an error.\n1846 \n1847         .. versionadded:: 0.17\n1848            *presort* parameter.\n1849 \n1850     validation_fraction : float, optional, default 0.1\n1851         The proportion of training data to set aside as validation set for\n1852         early stopping. Must be between 0 and 1.\n1853         Only used if ``n_iter_no_change`` is set to an integer.\n1854 \n1855         .. versionadded:: 0.20\n1856 \n1857     n_iter_no_change : int, default None\n1858         ``n_iter_no_change`` is used to decide if early stopping will be used\n1859         to terminate training when validation score is not improving. By\n1860         default it is set to None to disable early stopping. If set to a\n1861         number, it will set aside ``validation_fraction`` size of the training\n1862         data as validation and terminate training when validation score is not\n1863         improving in all of the previous ``n_iter_no_change`` numbers of\n1864         iterations.\n1865 \n1866         .. versionadded:: 0.20\n1867 \n1868     tol : float, optional, default 1e-4\n1869         Tolerance for the early stopping. When the loss is not improving\n1870         by at least tol for ``n_iter_no_change`` iterations (if set to a\n1871         number), the training stops.\n1872 \n1873         .. versionadded:: 0.20\n1874 \n1875     Attributes\n1876     ----------\n1877     n_estimators_ : int\n1878         The number of estimators as selected by early stopping (if\n1879         ``n_iter_no_change`` is specified). Otherwise it is set to\n1880         ``n_estimators``.\n1881 \n1882         .. versionadded:: 0.20\n1883 \n1884     feature_importances_ : array, shape (n_features,)\n1885         The feature importances (the higher, the more important the feature).\n1886 \n1887     oob_improvement_ : array, shape (n_estimators,)\n1888         The improvement in loss (= deviance) on the out-of-bag samples\n1889         relative to the previous iteration.\n1890         ``oob_improvement_[0]`` is the improvement in\n1891         loss of the first stage over the ``init`` estimator.\n1892 \n1893     train_score_ : array, shape (n_estimators,)\n1894         The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n1895         model at iteration ``i`` on the in-bag sample.\n1896         If ``subsample == 1`` this is the deviance on the training data.\n1897 \n1898     loss_ : LossFunction\n1899         The concrete ``LossFunction`` object.\n1900 \n1901     init_ : estimator\n1902         The estimator that provides the initial predictions.\n1903         Set via the ``init`` argument or ``loss.init_estimator``.\n1904 \n1905     estimators_ : ndarray of DecisionTreeRegressor,\\\n1906 shape (n_estimators, ``loss_.K``)\n1907         The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n1908         classification, otherwise n_classes.\n1909 \n1910     Notes\n1911     -----\n1912     The features are always randomly permuted at each split. Therefore,\n1913     the best found split may vary, even with the same training data and\n1914     ``max_features=n_features``, if the improvement of the criterion is\n1915     identical for several splits enumerated during the search of the best\n1916     split. To obtain a deterministic behaviour during fitting,\n1917     ``random_state`` has to be fixed.\n1918 \n1919     See also\n1920     --------\n1921     sklearn.tree.DecisionTreeClassifier, RandomForestClassifier\n1922     AdaBoostClassifier\n1923 \n1924     References\n1925     ----------\n1926     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n1927     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n1928 \n1929     J. Friedman, Stochastic Gradient Boosting, 1999\n1930 \n1931     T. Hastie, R. Tibshirani and J. Friedman.\n1932     Elements of Statistical Learning Ed. 2, Springer, 2009.\n1933     \"\"\"\n1934 \n1935     _SUPPORTED_LOSS = ('deviance', 'exponential')\n1936 \n1937     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,\n1938                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n1939                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n1940                  max_depth=3, min_impurity_decrease=0.,\n1941                  min_impurity_split=None, init=None,\n1942                  random_state=None, max_features=None, verbose=0,\n1943                  max_leaf_nodes=None, warm_start=False,\n1944                  presort='auto', validation_fraction=0.1,\n1945                  n_iter_no_change=None, tol=1e-4):\n1946 \n1947         super().__init__(\n1948             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n1949             criterion=criterion, min_samples_split=min_samples_split,\n1950             min_samples_leaf=min_samples_leaf,\n1951             min_weight_fraction_leaf=min_weight_fraction_leaf,\n1952             max_depth=max_depth, init=init, subsample=subsample,\n1953             max_features=max_features,\n1954             random_state=random_state, verbose=verbose,\n1955             max_leaf_nodes=max_leaf_nodes,\n1956             min_impurity_decrease=min_impurity_decrease,\n1957             min_impurity_split=min_impurity_split,\n1958             warm_start=warm_start, presort=presort,\n1959             validation_fraction=validation_fraction,\n1960             n_iter_no_change=n_iter_no_change, tol=tol)\n1961 \n1962     def _validate_y(self, y, sample_weight):\n1963         check_classification_targets(y)\n1964         self.classes_, y = np.unique(y, return_inverse=True)\n1965         n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n1966         if n_trim_classes < 2:\n1967             raise ValueError(\"y contains %d class after sample_weight \"\n1968                              \"trimmed classes with zero weights, while a \"\n1969                              \"minimum of 2 classes are required.\"\n1970                              % n_trim_classes)\n1971         self.n_classes_ = len(self.classes_)\n1972         return y\n1973 \n1974     def decision_function(self, X):\n1975         \"\"\"Compute the decision function of ``X``.\n1976 \n1977         Parameters\n1978         ----------\n1979         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n1980             The input samples. Internally, it will be converted to\n1981             ``dtype=np.float32`` and if a sparse matrix is provided\n1982             to a sparse ``csr_matrix``.\n1983 \n1984         Returns\n1985         -------\n1986         score : array, shape (n_samples, n_classes) or (n_samples,)\n1987             The decision function of the input samples. The order of the\n1988             classes corresponds to that in the attribute `classes_`.\n1989             Regression and binary classification produce an array of shape\n1990             [n_samples].\n1991         \"\"\"\n1992         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n1993         score = self._decision_function(X)\n1994         if score.shape[1] == 1:\n1995             return score.ravel()\n1996         return score\n1997 \n1998     def staged_decision_function(self, X):\n1999         \"\"\"Compute decision function of ``X`` for each iteration.\n2000 \n2001         This method allows monitoring (i.e. determine error on testing set)\n2002         after each stage.\n2003 \n2004         Parameters\n2005         ----------\n2006         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2007             The input samples. Internally, it will be converted to\n2008             ``dtype=np.float32`` and if a sparse matrix is provided\n2009             to a sparse ``csr_matrix``.\n2010 \n2011         Returns\n2012         -------\n2013         score : generator of array, shape (n_samples, k)\n2014             The decision function of the input samples. The order of the\n2015             classes corresponds to that in the attribute `classes_`.\n2016             Regression and binary classification are special cases with\n2017             ``k == 1``, otherwise ``k==n_classes``.\n2018         \"\"\"\n2019         yield from self._staged_decision_function(X)\n2020 \n2021     def predict(self, X):\n2022         \"\"\"Predict class for X.\n2023 \n2024         Parameters\n2025         ----------\n2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2027             The input samples. Internally, it will be converted to\n2028             ``dtype=np.float32`` and if a sparse matrix is provided\n2029             to a sparse ``csr_matrix``.\n2030 \n2031         Returns\n2032         -------\n2033         y : array, shape (n_samples,)\n2034             The predicted values.\n2035         \"\"\"\n2036         score = self.decision_function(X)\n2037         decisions = self.loss_._score_to_decision(score)\n2038         return self.classes_.take(decisions, axis=0)\n2039 \n2040     def staged_predict(self, X):\n2041         \"\"\"Predict class at each stage for X.\n2042 \n2043         This method allows monitoring (i.e. determine error on testing set)\n2044         after each stage.\n2045 \n2046         Parameters\n2047         ----------\n2048         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2049             The input samples. Internally, it will be converted to\n2050             ``dtype=np.float32`` and if a sparse matrix is provided\n2051             to a sparse ``csr_matrix``.\n2052 \n2053         Returns\n2054         -------\n2055         y : generator of array of shape (n_samples,)\n2056             The predicted value of the input samples.\n2057         \"\"\"\n2058         for score in self._staged_decision_function(X):\n2059             decisions = self.loss_._score_to_decision(score)\n2060             yield self.classes_.take(decisions, axis=0)\n2061 \n2062     def predict_proba(self, X):\n2063         \"\"\"Predict class probabilities for X.\n2064 \n2065         Parameters\n2066         ----------\n2067         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2068             The input samples. Internally, it will be converted to\n2069             ``dtype=np.float32`` and if a sparse matrix is provided\n2070             to a sparse ``csr_matrix``.\n2071 \n2072         Raises\n2073         ------\n2074         AttributeError\n2075             If the ``loss`` does not support probabilities.\n2076 \n2077         Returns\n2078         -------\n2079         p : array, shape (n_samples, n_classes)\n2080             The class probabilities of the input samples. The order of the\n2081             classes corresponds to that in the attribute `classes_`.\n2082         \"\"\"\n2083         score = self.decision_function(X)\n2084         try:\n2085             return self.loss_._score_to_proba(score)\n2086         except NotFittedError:\n2087             raise\n2088         except AttributeError:\n2089             raise AttributeError('loss=%r does not support predict_proba' %\n2090                                  self.loss)\n2091 \n2092     def predict_log_proba(self, X):\n2093         \"\"\"Predict class log-probabilities for X.\n2094 \n2095         Parameters\n2096         ----------\n2097         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2098             The input samples. Internally, it will be converted to\n2099             ``dtype=np.float32`` and if a sparse matrix is provided\n2100             to a sparse ``csr_matrix``.\n2101 \n2102         Raises\n2103         ------\n2104         AttributeError\n2105             If the ``loss`` does not support probabilities.\n2106 \n2107         Returns\n2108         -------\n2109         p : array, shape (n_samples, n_classes)\n2110             The class log-probabilities of the input samples. The order of the\n2111             classes corresponds to that in the attribute `classes_`.\n2112         \"\"\"\n2113         proba = self.predict_proba(X)\n2114         return np.log(proba)\n2115 \n2116     def staged_predict_proba(self, X):\n2117         \"\"\"Predict class probabilities at each stage for X.\n2118 \n2119         This method allows monitoring (i.e. determine error on testing set)\n2120         after each stage.\n2121 \n2122         Parameters\n2123         ----------\n2124         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2125             The input samples. Internally, it will be converted to\n2126             ``dtype=np.float32`` and if a sparse matrix is provided\n2127             to a sparse ``csr_matrix``.\n2128 \n2129         Returns\n2130         -------\n2131         y : generator of array of shape (n_samples,)\n2132             The predicted value of the input samples.\n2133         \"\"\"\n2134         try:\n2135             for score in self._staged_decision_function(X):\n2136                 yield self.loss_._score_to_proba(score)\n2137         except NotFittedError:\n2138             raise\n2139         except AttributeError:\n2140             raise AttributeError('loss=%r does not support predict_proba' %\n2141                                  self.loss)\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 2390,
        "end": 2411,
        "class_name": "GradientBoostingRegressor",
        "method_name": "__init__",
        "code": "2390     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,\n2391                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n2392                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n2393                  max_depth=3, min_impurity_decrease=0.,\n2394                  min_impurity_split=None, init=None, random_state=None,\n2395                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n2396                  warm_start=False, presort='auto', validation_fraction=0.1,\n2397                  n_iter_no_change=None, tol=1e-4):\n2398 \n2399         super().__init__(\n2400             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n2401             criterion=criterion, min_samples_split=min_samples_split,\n2402             min_samples_leaf=min_samples_leaf,\n2403             min_weight_fraction_leaf=min_weight_fraction_leaf,\n2404             max_depth=max_depth, init=init, subsample=subsample,\n2405             max_features=max_features,\n2406             min_impurity_decrease=min_impurity_decrease,\n2407             min_impurity_split=min_impurity_split,\n2408             random_state=random_state, alpha=alpha, verbose=verbose,\n2409             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n2410             presort=presort, validation_fraction=validation_fraction,\n2411             n_iter_no_change=n_iter_no_change, tol=tol)\n",
        "intended_behavior": "Similar to `GradientBoostingClassifier`, the handling of the `init` parameter needs to be adjusted to ensure compatibility with sklearn estimators that are used as the initial model. The code needs to ensure that the shape and scale of the initial predictions from the `init` estimator are compatible with the boosting process. It should also account for the possibility of the `init` estimator not supporting sample weights, by either not passing sample weights to it or providing a workaround that doesn\u2019t compromise the performance and functionality of the gradient boosting regressor."
    },
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 2144,
        "end": 2473,
        "class_name": "GradientBoostingRegressor",
        "method_name": null,
        "code": "2144 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):\n2145     \"\"\"Gradient Boosting for regression.\n2146 \n2147     GB builds an additive model in a forward stage-wise fashion;\n2148     it allows for the optimization of arbitrary differentiable loss functions.\n2149     In each stage a regression tree is fit on the negative gradient of the\n2150     given loss function.\n2151 \n2152     Read more in the :ref:`User Guide <gradient_boosting>`.\n2153 \n2154     Parameters\n2155     ----------\n2156     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n2157         loss function to be optimized. 'ls' refers to least squares\n2158         regression. 'lad' (least absolute deviation) is a highly robust\n2159         loss function solely based on order information of the input\n2160         variables. 'huber' is a combination of the two. 'quantile'\n2161         allows quantile regression (use `alpha` to specify the quantile).\n2162 \n2163     learning_rate : float, optional (default=0.1)\n2164         learning rate shrinks the contribution of each tree by `learning_rate`.\n2165         There is a trade-off between learning_rate and n_estimators.\n2166 \n2167     n_estimators : int (default=100)\n2168         The number of boosting stages to perform. Gradient boosting\n2169         is fairly robust to over-fitting so a large number usually\n2170         results in better performance.\n2171 \n2172     subsample : float, optional (default=1.0)\n2173         The fraction of samples to be used for fitting the individual base\n2174         learners. If smaller than 1.0 this results in Stochastic Gradient\n2175         Boosting. `subsample` interacts with the parameter `n_estimators`.\n2176         Choosing `subsample < 1.0` leads to a reduction of variance\n2177         and an increase in bias.\n2178 \n2179     criterion : string, optional (default=\"friedman_mse\")\n2180         The function to measure the quality of a split. Supported criteria\n2181         are \"friedman_mse\" for the mean squared error with improvement\n2182         score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n2183         the mean absolute error. The default value of \"friedman_mse\" is\n2184         generally the best as it can provide a better approximation in\n2185         some cases.\n2186 \n2187         .. versionadded:: 0.18\n2188 \n2189     min_samples_split : int, float, optional (default=2)\n2190         The minimum number of samples required to split an internal node:\n2191 \n2192         - If int, then consider `min_samples_split` as the minimum number.\n2193         - If float, then `min_samples_split` is a fraction and\n2194           `ceil(min_samples_split * n_samples)` are the minimum\n2195           number of samples for each split.\n2196 \n2197         .. versionchanged:: 0.18\n2198            Added float values for fractions.\n2199 \n2200     min_samples_leaf : int, float, optional (default=1)\n2201         The minimum number of samples required to be at a leaf node.\n2202         A split point at any depth will only be considered if it leaves at\n2203         least ``min_samples_leaf`` training samples in each of the left and\n2204         right branches.  This may have the effect of smoothing the model,\n2205         especially in regression.\n2206 \n2207         - If int, then consider `min_samples_leaf` as the minimum number.\n2208         - If float, then `min_samples_leaf` is a fraction and\n2209           `ceil(min_samples_leaf * n_samples)` are the minimum\n2210           number of samples for each node.\n2211 \n2212         .. versionchanged:: 0.18\n2213            Added float values for fractions.\n2214 \n2215     min_weight_fraction_leaf : float, optional (default=0.)\n2216         The minimum weighted fraction of the sum total of weights (of all\n2217         the input samples) required to be at a leaf node. Samples have\n2218         equal weight when sample_weight is not provided.\n2219 \n2220     max_depth : integer, optional (default=3)\n2221         maximum depth of the individual regression estimators. The maximum\n2222         depth limits the number of nodes in the tree. Tune this parameter\n2223         for best performance; the best value depends on the interaction\n2224         of the input variables.\n2225 \n2226     min_impurity_decrease : float, optional (default=0.)\n2227         A node will be split if this split induces a decrease of the impurity\n2228         greater than or equal to this value.\n2229 \n2230         The weighted impurity decrease equation is the following::\n2231 \n2232             N_t / N * (impurity - N_t_R / N_t * right_impurity\n2233                                 - N_t_L / N_t * left_impurity)\n2234 \n2235         where ``N`` is the total number of samples, ``N_t`` is the number of\n2236         samples at the current node, ``N_t_L`` is the number of samples in the\n2237         left child, and ``N_t_R`` is the number of samples in the right child.\n2238 \n2239         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n2240         if ``sample_weight`` is passed.\n2241 \n2242         .. versionadded:: 0.19\n2243 \n2244     min_impurity_split : float, (default=1e-7)\n2245         Threshold for early stopping in tree growth. A node will split\n2246         if its impurity is above the threshold, otherwise it is a leaf.\n2247 \n2248         .. deprecated:: 0.19\n2249            ``min_impurity_split`` has been deprecated in favor of\n2250            ``min_impurity_decrease`` in 0.19. The default value of\n2251            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n2252            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n2253 \n2254     init : estimator, optional (default=None)\n2255         An estimator object that is used to compute the initial\n2256         predictions. ``init`` has to provide ``fit`` and ``predict``.\n2257         If None it uses ``loss.init_estimator``.\n2258 \n2259     random_state : int, RandomState instance or None, optional (default=None)\n2260         If int, random_state is the seed used by the random number generator;\n2261         If RandomState instance, random_state is the random number generator;\n2262         If None, the random number generator is the RandomState instance used\n2263         by `np.random`.\n2264 \n2265     max_features : int, float, string or None, optional (default=None)\n2266         The number of features to consider when looking for the best split:\n2267 \n2268         - If int, then consider `max_features` features at each split.\n2269         - If float, then `max_features` is a fraction and\n2270           `int(max_features * n_features)` features are considered at each\n2271           split.\n2272         - If \"auto\", then `max_features=n_features`.\n2273         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n2274         - If \"log2\", then `max_features=log2(n_features)`.\n2275         - If None, then `max_features=n_features`.\n2276 \n2277         Choosing `max_features < n_features` leads to a reduction of variance\n2278         and an increase in bias.\n2279 \n2280         Note: the search for a split does not stop until at least one\n2281         valid partition of the node samples is found, even if it requires to\n2282         effectively inspect more than ``max_features`` features.\n2283 \n2284     alpha : float (default=0.9)\n2285         The alpha-quantile of the huber loss function and the quantile\n2286         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n2287 \n2288     verbose : int, default: 0\n2289         Enable verbose output. If 1 then it prints progress and performance\n2290         once in a while (the more trees the lower the frequency). If greater\n2291         than 1 then it prints progress and performance for every tree.\n2292 \n2293     max_leaf_nodes : int or None, optional (default=None)\n2294         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n2295         Best nodes are defined as relative reduction in impurity.\n2296         If None then unlimited number of leaf nodes.\n2297 \n2298     warm_start : bool, default: False\n2299         When set to ``True``, reuse the solution of the previous call to fit\n2300         and add more estimators to the ensemble, otherwise, just erase the\n2301         previous solution. See :term:`the Glossary <warm_start>`.\n2302 \n2303     presort : bool or 'auto', optional (default='auto')\n2304         Whether to presort the data to speed up the finding of best splits in\n2305         fitting. Auto mode by default will use presorting on dense data and\n2306         default to normal sorting on sparse data. Setting presort to true on\n2307         sparse data will raise an error.\n2308 \n2309         .. versionadded:: 0.17\n2310            optional parameter *presort*.\n2311 \n2312     validation_fraction : float, optional, default 0.1\n2313         The proportion of training data to set aside as validation set for\n2314         early stopping. Must be between 0 and 1.\n2315         Only used if ``n_iter_no_change`` is set to an integer.\n2316 \n2317         .. versionadded:: 0.20\n2318 \n2319     n_iter_no_change : int, default None\n2320         ``n_iter_no_change`` is used to decide if early stopping will be used\n2321         to terminate training when validation score is not improving. By\n2322         default it is set to None to disable early stopping. If set to a\n2323         number, it will set aside ``validation_fraction`` size of the training\n2324         data as validation and terminate training when validation score is not\n2325         improving in all of the previous ``n_iter_no_change`` numbers of\n2326         iterations.\n2327 \n2328         .. versionadded:: 0.20\n2329 \n2330     tol : float, optional, default 1e-4\n2331         Tolerance for the early stopping. When the loss is not improving\n2332         by at least tol for ``n_iter_no_change`` iterations (if set to a\n2333         number), the training stops.\n2334 \n2335         .. versionadded:: 0.20\n2336 \n2337 \n2338     Attributes\n2339     ----------\n2340     feature_importances_ : array, shape (n_features,)\n2341         The feature importances (the higher, the more important the feature).\n2342 \n2343     oob_improvement_ : array, shape (n_estimators,)\n2344         The improvement in loss (= deviance) on the out-of-bag samples\n2345         relative to the previous iteration.\n2346         ``oob_improvement_[0]`` is the improvement in\n2347         loss of the first stage over the ``init`` estimator.\n2348 \n2349     train_score_ : array, shape (n_estimators,)\n2350         The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n2351         model at iteration ``i`` on the in-bag sample.\n2352         If ``subsample == 1`` this is the deviance on the training data.\n2353 \n2354     loss_ : LossFunction\n2355         The concrete ``LossFunction`` object.\n2356 \n2357     init_ : estimator\n2358         The estimator that provides the initial predictions.\n2359         Set via the ``init`` argument or ``loss.init_estimator``.\n2360 \n2361     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n2362         The collection of fitted sub-estimators.\n2363 \n2364     Notes\n2365     -----\n2366     The features are always randomly permuted at each split. Therefore,\n2367     the best found split may vary, even with the same training data and\n2368     ``max_features=n_features``, if the improvement of the criterion is\n2369     identical for several splits enumerated during the search of the best\n2370     split. To obtain a deterministic behaviour during fitting,\n2371     ``random_state`` has to be fixed.\n2372 \n2373     See also\n2374     --------\n2375     DecisionTreeRegressor, RandomForestRegressor\n2376 \n2377     References\n2378     ----------\n2379     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n2380     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n2381 \n2382     J. Friedman, Stochastic Gradient Boosting, 1999\n2383 \n2384     T. Hastie, R. Tibshirani and J. Friedman.\n2385     Elements of Statistical Learning Ed. 2, Springer, 2009.\n2386     \"\"\"\n2387 \n2388     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')\n2389 \n2390     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,\n2391                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n2392                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n2393                  max_depth=3, min_impurity_decrease=0.,\n2394                  min_impurity_split=None, init=None, random_state=None,\n2395                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n2396                  warm_start=False, presort='auto', validation_fraction=0.1,\n2397                  n_iter_no_change=None, tol=1e-4):\n2398 \n2399         super().__init__(\n2400             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n2401             criterion=criterion, min_samples_split=min_samples_split,\n2402             min_samples_leaf=min_samples_leaf,\n2403             min_weight_fraction_leaf=min_weight_fraction_leaf,\n2404             max_depth=max_depth, init=init, subsample=subsample,\n2405             max_features=max_features,\n2406             min_impurity_decrease=min_impurity_decrease,\n2407             min_impurity_split=min_impurity_split,\n2408             random_state=random_state, alpha=alpha, verbose=verbose,\n2409             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n2410             presort=presort, validation_fraction=validation_fraction,\n2411             n_iter_no_change=n_iter_no_change, tol=tol)\n2412 \n2413     def predict(self, X):\n2414         \"\"\"Predict regression target for X.\n2415 \n2416         Parameters\n2417         ----------\n2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2419             The input samples. Internally, it will be converted to\n2420             ``dtype=np.float32`` and if a sparse matrix is provided\n2421             to a sparse ``csr_matrix``.\n2422 \n2423         Returns\n2424         -------\n2425         y : array, shape (n_samples,)\n2426             The predicted values.\n2427         \"\"\"\n2428         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n2429         return self._decision_function(X).ravel()\n2430 \n2431     def staged_predict(self, X):\n2432         \"\"\"Predict regression target at each stage for X.\n2433 \n2434         This method allows monitoring (i.e. determine error on testing set)\n2435         after each stage.\n2436 \n2437         Parameters\n2438         ----------\n2439         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2440             The input samples. Internally, it will be converted to\n2441             ``dtype=np.float32`` and if a sparse matrix is provided\n2442             to a sparse ``csr_matrix``.\n2443 \n2444         Returns\n2445         -------\n2446         y : generator of array of shape (n_samples,)\n2447             The predicted value of the input samples.\n2448         \"\"\"\n2449         for y in self._staged_decision_function(X):\n2450             yield y.ravel()\n2451 \n2452     def apply(self, X):\n2453         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n2454 \n2455         .. versionadded:: 0.17\n2456 \n2457         Parameters\n2458         ----------\n2459         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2460             The input samples. Internally, its dtype will be converted to\n2461             ``dtype=np.float32``. If a sparse matrix is provided, it will\n2462             be converted to a sparse ``csr_matrix``.\n2463 \n2464         Returns\n2465         -------\n2466         X_leaves : array-like, shape (n_samples, n_estimators)\n2467             For each datapoint x in X and for each tree in the ensemble,\n2468             return the index of the leaf x ends up in each estimator.\n2469         \"\"\"\n2470 \n2471         leaves = super().apply(X)\n2472         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n2473         return leaves\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]