2025-10-18 05:27:50.301 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 05:27:52.802 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=198, output_tokens=12, cost=0.002340
2025-10-18 05:27:52.803 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 05:27:52.807 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:28:02.432 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=506, output_tokens=424, cost=0.017780
2025-10-18 05:28:02.440 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:28:05.836 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=778, output_tokens=86, cost=0.010360
2025-10-18 05:28:05.838 | DEBUG    | Extracted a valid json.
2025-10-18 05:28:05.838 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"AdaBoost\")",
        "search_method_in_class(\"fit\", \"AdaBoost\")",
        "search_method_in_class(\"predict_proba\", \"AdaBoost\")",
        "search_method_in_class(\"predict\", \"AdaBoost\")",
        "search_class(\"Ensemble\")",
        "search_code(\"validate_X\")"
    ],
    "bug_locations": []
}
2025-10-18 05:28:05.886 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 05:28:05.887 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 05:28:18.175 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1260, output_tokens=607, cost=0.030810
2025-10-18 05:28:18.187 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 05:28:29.110 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2083, output_tokens=500, cost=0.035830
2025-10-18 05:28:29.120 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 05:28:38.544 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=854, output_tokens=401, cost=0.020570
2025-10-18 05:28:38.545 | DEBUG    | Extracted a valid json.
2025-10-18 05:28:38.545 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "sklearn/ensemble/weight_boosting.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file likely handles the AdaBoost algorithm and potentially others. The validation of X should be flexible enough to allow various data types beyond arrays or sparse matrices, as long as the base estimator can process the input data type. This means adjusting or extending the validate_X utility or the validation logic within fit, predict_proba, and predict methods to not outright reject input types like lists of text blobs if the base estimator can handle them. Preserve existing checks that ensure the input is compatible with the base estimator's requirements, perhaps by introducing a pre-validation step that checks if the input type is supported by the base estimator before proceeding with any further validation or processing."
        },
        {
            "file": "sklearn/ensemble/forest.py",
            "class": "",
            "method": "",
            "intended_behavior": "While this file handles Random Forests and similar ensemble methods, the validation approach extracted here might serve as a blueprint for modifications in weight_boosting.py. Ensure that any validation logic within this file allows for the flexible handling of input data types based on the capabilities of the constituent base estimators. It's about enabling a broader range of inputs without compromising on data integrity checks necessary for the algorithm to function correctly."
        },
        {
            "file": "sklearn/ensemble/gradient_boosting.py",
            "class": "",
            "method": "",
            "intended_behavior": "For gradient boosting methods, the intention is similar to above. Adjust the validate_X calls or similar validations to support a wider range of data types as inputs, provided the base estimators can work with those data types. It's crucial to maintain a balance between allowing flexibility and ensuring the input data's appropriacy for the algorithm to work as intended."
        }
    ]
}
2025-10-18 05:28:38.563 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/weight_boosting.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Weight Boosting
2 
3 This module contains weight boosting estimators for both classification and
4 regression.
5 
6 The module structure is the following:
7 
8 - The ``BaseWeightBoosting`` base class implements a common ``fit`` method
9   for all the estimators in the module. Regression and classification
10   only differ from each other in the loss function that is optimized.
11 
12 - ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for
13   classification problems.
14 
15 - ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for
16   regression problems.
17 """
18 
19 # Authors: Noel Dawe <noel@dawe.me>
20 #          Gilles Louppe <g.louppe@gmail.com>
21 #          Hamzeh Alsalhi <ha258@cornell.edu>
22 #          Arnaud Joly <arnaud.v.joly@gmail.com>
23 #
24 # License: BSD 3 clause
25 
26 from abc import ABCMeta, abstractmethod
27 
28 import numpy as np
29 
30 from scipy.special import xlogy
31 
32 from .base import BaseEnsemble
33 from ..base import ClassifierMixin, RegressorMixin, is_regressor, is_classifier
34 
35 from .forest import BaseForest
36 from ..tree import DecisionTreeClassifier, DecisionTreeRegressor
37 from ..tree.tree import BaseDecisionTree
38 from ..tree._tree import DTYPE
39 from ..utils import check_array, check_X_y, check_random_state
40 from ..utils.extmath import stable_cumsum
41 from ..metrics import accuracy_score, r2_score
42 from sklearn.utils.validation import has_fit_parameter, check_is_fitted
43 
44 __all__ = [
45     'AdaBoostClassifier',
46     'AdaBoostRegressor',
47 ]
48 
49 
50 class BaseWeightBoosting(BaseEnsemble, metaclass=ABCMeta):
51     """Base class for AdaBoost estimators.
52 
53     Warning: This class should not be used directly. Use derived classes
54     instead.
55     """
56 
57     @abstractmethod
58     def __init__(self,
59                  base_estimator=None,
60                  n_estimators=50,
61                  estimator_params=tuple(),
62                  learning_rate=1.,
63                  random_state=None):
64 
65         super().__init__(
66             base_estimator=base_estimator,
67             n_estimators=n_estimators,
68             estimator_params=estimator_params)
69 
70         self.learning_rate = learning_rate
71         self.random_state = random_state
72 
73     def fit(self, X, y, sample_weight=None):
74         """Build a boosted classifier/regressor from the training set (X, y).
75 
76         Parameters
77         ----------
78         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
79             The training input samples. Sparse matrix can be CSC, CSR, COO,
80             DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is
81             forced to DTYPE from tree._tree if the base classifier of this
82             ensemble weighted boosting classifier is a tree or forest.
83 
84         y : array-like of shape = [n_samples]
85             The target values (class labels in classification, real numbers in
86             regression).
87 
88         sample_weight : array-like of shape = [n_samples], optional
89             Sample weights. If None, the sample weights are initialized to
90             1 / n_samples.
91 
92         Returns
93         -------
94         self : object
95         """
96         # Check parameters
97         if self.learning_rate <= 0:
98             raise ValueError("learning_rate must be greater than zero")
99 
100         if (self.base_estimator is None or
101                 isinstance(self.base_estimator, (BaseDecisionTree,
102                                                  BaseForest))):
103             dtype = DTYPE
104             accept_sparse = 'csc'
105         else:
106             dtype = None
107             accept_sparse = ['csr', 'csc']
108 
109         X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,
110                          y_numeric=is_regressor(self))
111 
112         if sample_weight is None:
113             # Initialize weights to 1 / n_samples
114             sample_weight = np.empty(X.shape[0], dtype=np.float64)
115             sample_weight[:] = 1. / X.shape[0]
116         else:
117             sample_weight = check_array(sample_weight, ensure_2d=False)
118             # Normalize existing weights
119             sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)
120 
121             # Check that the sample weights sum is positive
122             if sample_weight.sum() <= 0:
123                 raise ValueError(
124                     "Attempting to fit with a non-positive "
125                     "weighted number of samples.")
126 
127         # Check parameters
128         self._validate_estimator()
129 
130         # Clear any previous fit results
131         self.estimators_ = []
132         self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)
133         self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)
134 
135         random_state = check_random_state(self.random_state)
136 
137         for iboost in range(self.n_estimators):
138             # Boosting step
139             sample_weight, estimator_weight, estimator_error = self._boost(
140                 iboost,
141                 X, y,
142                 sample_weight,
143                 random_state)
144 
145             # Early termination
146             if sample_weight is None:
147                 break
148 
149             self.estimator_weights_[iboost] = estimator_weight
150             self.estimator_errors_[iboost] = estimator_error
151 
152             # Stop if error is zero
153             if estimator_error == 0:
154                 break
155 
156             sample_weight_sum = np.sum(sample_weight)
157 
158             # Stop if the sum of sample weights has become non-positive
159             if sample_weight_sum <= 0:
160                 break
161 
162             if iboost < self.n_estimators - 1:
163                 # Normalize
164                 sample_weight /= sample_weight_sum
165 
166         return self
167 
168     @abstractmethod
169     def _boost(self, iboost, X, y, sample_weight, random_state):
170         """Implement a single boost.
171 
172         Warning: This method needs to be overridden by subclasses.
173 
174         Parameters
175         ----------
176         iboost : int
177             The index of the current boost iteration.
178 
179         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
180             The training input samples. Sparse matrix can be CSC, CSR, COO,
181             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
182 
183         y : array-like of shape = [n_samples]
184             The target values (class labels).
185 
186         sample_weight : array-like of shape = [n_samples]
187             The current sample weights.
188 
189         random_state : RandomState
190             The current random number generator
191 
192         Returns
193         -------
194         sample_weight : array-like of shape = [n_samples] or None
195             The reweighted sample weights.
196             If None then boosting has terminated early.
197 
198         estimator_weight : float
199             The weight for the current boost.
200             If None then boosting has terminated early.
201 
202         error : float
203             The classification error for the current boost.
204             If None then boosting has terminated early.
205         """
206         pass
207 
208     def staged_score(self, X, y, sample_weight=None):
209         """Return staged scores for X, y.
210 
211         This generator method yields the ensemble score after each iteration of
212         boosting and therefore allows monitoring, such as to determine the
213         score on a test set after each boost.
214 
215         Parameters
216         ----------
217         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
218             The training input samples. Sparse matrix can be CSC, CSR, COO,
219             DOK, or LIL. DOK and LIL are converted to CSR.
220 
221         y : array-like, shape = [n_samples]
222             Labels for X.
223 
224         sample_weight : array-like, shape = [n_samples], optional
225             Sample weights.
226 
227         Returns
228         -------
229         z : float
230         """
231         for y_pred in self.staged_predict(X):
232             if is_classifier(self):
233                 yield accuracy_score(y, y_pred, sample_weight=sample_weight)
234             else:
235                 yield r2_score(y, y_pred, sample_weight=sample_weight)
236 
237     @property
238     def feature_importances_(self):
239         """Return the feature importances (the higher, the more important the
240            feature).
241 
242         Returns
243         -------
244         feature_importances_ : array, shape = [n_features]
245         """
246         if self.estimators_ is None or len(self.estimators_) == 0:
247             raise ValueError("Estimator not fitted, "
248                              "call `fit` before `feature_importances_`.")
249 
250         try:
251             norm = self.estimator_weights_.sum()
252             return (sum(weight * clf.feature_importances_ for weight, clf
253                     in zip(self.estimator_weights_, self.estimators_))
254                     / norm)
255 
256         except AttributeError:
257             raise AttributeError(
258                 "Unable to compute feature importances "
259                 "since base_estimator does not have a "
260                 "feature_importances_ attribute")
261 
262     def _validate_X_predict(self, X):
263         """Ensure that X is in the proper format"""
264         if (self.base_estimator is None or
265                 isinstance(self.base_estimator,
266                            (BaseDecisionTree, BaseForest))):
267             X = check_array(X, accept_sparse='csr', dtype=DTYPE)
268 
269         else:
270             X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])
271 
272         return X
273 
274 
275 def _samme_proba(estimator, n_classes, X):
276     """Calculate algorithm 4, step 2, equation c) of Zhu et al [1].
277 
278     References
279     ----------
280     .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
281 
282     """
283     proba = estimator.predict_proba(X)
284 
285     # Displace zero probabilities so the log is defined.
286     # Also fix negative elements which may occur with
287     # negative sample weights.
288     np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
289     log_proba = np.log(proba)
290 
291     return (n_classes - 1) * (log_proba - (1. / n_classes)
292                               * log_proba.sum(axis=1)[:, np.newaxis])
293 
294 
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))
849 
850 
851 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):
852     """An AdaBoost regressor.
853 
854     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
855     regressor on the original dataset and then fits additional copies of the
856     regressor on the same dataset but where the weights of instances are
857     adjusted according to the error of the current prediction. As such,
858     subsequent regressors focus more on difficult cases.
859 
860     This class implements the algorithm known as AdaBoost.R2 [2].
861 
862     Read more in the :ref:`User Guide <adaboost>`.
863 
864     Parameters
865     ----------
866     base_estimator : object, optional (default=None)
867         The base estimator from which the boosted ensemble is built.
868         Support for sample weighting is required. If ``None``, then
869         the base estimator is ``DecisionTreeRegressor(max_depth=3)``
870 
871     n_estimators : integer, optional (default=50)
872         The maximum number of estimators at which boosting is terminated.
873         In case of perfect fit, the learning procedure is stopped early.
874 
875     learning_rate : float, optional (default=1.)
876         Learning rate shrinks the contribution of each regressor by
877         ``learning_rate``. There is a trade-off between ``learning_rate`` and
878         ``n_estimators``.
879 
880     loss : {'linear', 'square', 'exponential'}, optional (default='linear')
881         The loss function to use when updating the weights after each
882         boosting iteration.
883 
884     random_state : int, RandomState instance or None, optional (default=None)
885         If int, random_state is the seed used by the random number generator;
886         If RandomState instance, random_state is the random number generator;
887         If None, the random number generator is the RandomState instance used
888         by `np.random`.
889 
890     Attributes
891     ----------
892     estimators_ : list of classifiers
893         The collection of fitted sub-estimators.
894 
895     estimator_weights_ : array of floats
896         Weights for each estimator in the boosted ensemble.
897 
898     estimator_errors_ : array of floats
899         Regression error for each estimator in the boosted ensemble.
900 
901     feature_importances_ : array of shape = [n_features]
902         The feature importances if supported by the ``base_estimator``.
903 
904     See also
905     --------
906     AdaBoostClassifier, GradientBoostingRegressor,
907     sklearn.tree.DecisionTreeRegressor
908 
909     References
910     ----------
911     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
912            on-Line Learning and an Application to Boosting", 1995.
913 
914     .. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.
915 
916     """
917     def __init__(self,
918                  base_estimator=None,
919                  n_estimators=50,
920                  learning_rate=1.,
921                  loss='linear',
922                  random_state=None):
923 
924         super().__init__(
925             base_estimator=base_estimator,
926             n_estimators=n_estimators,
927             learning_rate=learning_rate,
928             random_state=random_state)
929 
930         self.loss = loss
931         self.random_state = random_state
932 
933     def fit(self, X, y, sample_weight=None):
934         """Build a boosted regressor from the training set (X, y).
935 
936         Parameters
937         ----------
938         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
939             The training input samples. Sparse matrix can be CSC, CSR, COO,
940             DOK, or LIL. DOK and LIL are converted to CSR.
941 
942         y : array-like of shape = [n_samples]
943             The target values (real numbers).
944 
945         sample_weight : array-like of shape = [n_samples], optional
946             Sample weights. If None, the sample weights are initialized to
947             1 / n_samples.
948 
949         Returns
950         -------
951         self : object
952         """
953         # Check loss
954         if self.loss not in ('linear', 'square', 'exponential'):
955             raise ValueError(
956                 "loss must be 'linear', 'square', or 'exponential'")
957 
958         # Fit
959         return super().fit(X, y, sample_weight)
960 
961     def _validate_estimator(self):
962         """Check the estimator and set the base_estimator_ attribute."""
963         super()._validate_estimator(
964             default=DecisionTreeRegressor(max_depth=3))
965 
966     def _boost(self, iboost, X, y, sample_weight, random_state):
967         """Implement a single boost for regression
968 
969         Perform a single boost according to the AdaBoost.R2 algorithm and
970         return the updated sample weights.
971 
972         Parameters
973         ----------
974         iboost : int
975             The index of the current boost iteration.
976 
977         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
978             The training input samples. Sparse matrix can be CSC, CSR, COO,
979             DOK, or LIL. DOK and LIL are converted to CSR.
980 
981         y : array-like of shape = [n_samples]
982             The target values (class labels in classification, real numbers in
983             regression).
984 
985         sample_weight : array-like of shape = [n_samples]
986             The current sample weights.
987 
988         random_state : RandomState
989             The current random number generator
990 
991         Returns
992         -------
993         sample_weight : array-like of shape = [n_samples] or None
994             The reweighted sample weights.
995             If None then boosting has terminated early.
996 
997         estimator_weight : float
998             The weight for the current boost.
999             If None then boosting has terminated early.
1000 
1001         estimator_error : float
1002             The regression error for the current boost.
1003             If None then boosting has terminated early.
1004         """
1005         estimator = self._make_estimator(random_state=random_state)
1006 
1007         # Weighted sampling of the training set with replacement
1008         # For NumPy >= 1.7.0 use np.random.choice
1009         cdf = stable_cumsum(sample_weight)
1010         cdf /= cdf[-1]
1011         uniform_samples = random_state.random_sample(X.shape[0])
1012         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
1013         # searchsorted returns a scalar
1014         bootstrap_idx = np.array(bootstrap_idx, copy=False)
1015 
1016         # Fit on the bootstrapped sample and obtain a prediction
1017         # for all samples in the training set
1018         estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
1019         y_predict = estimator.predict(X)
1020 
1021         error_vect = np.abs(y_predict - y)
1022         error_max = error_vect.max()
1023 
1024         if error_max != 0.:
1025             error_vect /= error_max
1026 
1027         if self.loss == 'square':
1028             error_vect **= 2
1029         elif self.loss == 'exponential':
1030             error_vect = 1. - np.exp(- error_vect)
1031 
1032         # Calculate the average loss
1033         estimator_error = (sample_weight * error_vect).sum()
1034 
1035         if estimator_error <= 0:
1036             # Stop if fit is perfect
1037             return sample_weight, 1., 0.
1038 
1039         elif estimator_error >= 0.5:
1040             # Discard current estimator only if it isn't the only one
1041             if len(self.estimators_) > 1:
1042                 self.estimators_.pop(-1)
1043             return None, None, None
1044 
1045         beta = estimator_error / (1. - estimator_error)
1046 
1047         # Boost weight using AdaBoost.R2 alg
1048         estimator_weight = self.learning_rate * np.log(1. / beta)
1049 
1050         if not iboost == self.n_estimators - 1:
1051             sample_weight *= np.power(
1052                 beta,
1053                 (1. - error_vect) * self.learning_rate)
1054 
1055         return sample_weight, estimator_weight, estimator_error
1056 
1057     def _get_median_predict(self, X, limit):
1058         # Evaluate predictions of all estimators
1059         predictions = np.array([
1060             est.predict(X) for est in self.estimators_[:limit]]).T
1061 
1062         # Sort the predictions
1063         sorted_idx = np.argsort(predictions, axis=1)
1064 
1065         # Find index of median prediction for each sample
1066         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
1067         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]
1068         median_idx = median_or_above.argmax(axis=1)
1069 
1070         median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]
1071 
1072         # Return median predictions
1073         return predictions[np.arange(X.shape[0]), median_estimators]
1074 
1075     def predict(self, X):
1076         """Predict regression value for X.
1077 
1078         The predicted regression value of an input sample is computed
1079         as the weighted median prediction of the classifiers in the ensemble.
1080 
1081         Parameters
1082         ----------
1083         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1084             The training input samples. Sparse matrix can be CSC, CSR, COO,
1085             DOK, or LIL. DOK and LIL are converted to CSR.
1086 
1087         Returns
1088         -------
1089         y : array of shape = [n_samples]
1090             The predicted regression values.
1091         """
1092         check_is_fitted(self, "estimator_weights_")
1093         X = self._validate_X_predict(X)
1094 
1095         return self._get_median_predict(X, len(self.estimators_))
1096 
1097     def staged_predict(self, X):
1098         """Return staged predictions for X.
1099 
1100         The predicted regression value of an input sample is computed
1101         as the weighted median prediction of the classifiers in the ensemble.
1102 
1103         This generator method yields the ensemble prediction after each
1104         iteration of boosting and therefore allows monitoring, such as to
1105         determine the prediction on a test set after each boost.
1106 
1107         Parameters
1108         ----------
1109         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1110             The training input samples. Sparse matrix can be CSC, CSR, COO,
1111             DOK, or LIL. DOK and LIL are converted to CSR.
1112 
1113         Returns
1114         -------
1115         y : generator of array, shape = [n_samples]
1116             The predicted regression values.
1117         """
1118         check_is_fitted(self, "estimator_weights_")
1119         X = self._validate_X_predict(X)
1120 
1121         for i, _ in enumerate(self.estimators_, 1):
1122             yield self._get_median_predict(X, limit=i)

</code><intended_behavior>This file likely handles the AdaBoost algorithm and potentially others. The validation of X should be flexible enough to allow various data types beyond arrays or sparse matrices, as long as the base estimator can process the input data type. This means adjusting or extending the validate_X utility or the validation logic within fit, predict_proba, and predict methods to not outright reject input types like lists of text blobs if the base estimator can handle them. Preserve existing checks that ensure the input is compatible with the base estimator's requirements, perhaps by introducing a pre-validation step that checks if the input type is supported by the base estimator before proceeding with any further validation or processing.</intended_behavior>, <file>sklearn/ensemble/forest.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Forest of trees-based ensemble methods
2 
3 Those methods include random forests and extremely randomized trees.
4 
5 The module structure is the following:
6 
7 - The ``BaseForest`` base class implements a common ``fit`` method for all
8   the estimators in the module. The ``fit`` method of the base ``Forest``
9   class calls the ``fit`` method of each sub-estimator on random samples
10   (with replacement, a.k.a. bootstrap) of the training set.
11 
12   The init of the sub-estimator is further delegated to the
13   ``BaseEnsemble`` constructor.
14 
15 - The ``ForestClassifier`` and ``ForestRegressor`` base classes further
16   implement the prediction logic by computing an average of the predicted
17   outcomes of the sub-estimators.
18 
19 - The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived
20   classes provide the user with concrete implementations of
21   the forest ensemble method using classical, deterministic
22   ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as
23   sub-estimator implementations.
24 
25 - The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived
26   classes provide the user with concrete implementations of the
27   forest ensemble method using the extremely randomized trees
28   ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as
29   sub-estimator implementations.
30 
31 Single and multi-output problems are both handled.
32 
33 """
34 
35 # Authors: Gilles Louppe <g.louppe@gmail.com>
36 #          Brian Holt <bdholt1@gmail.com>
37 #          Joly Arnaud <arnaud.v.joly@gmail.com>
38 #          Fares Hedayati <fares.hedayati@gmail.com>
39 #
40 # License: BSD 3 clause
41 
42 
43 from warnings import catch_warnings, simplefilter, warn
44 import threading
45 
46 from abc import ABCMeta, abstractmethod
47 import numpy as np
48 from scipy.sparse import issparse
49 from scipy.sparse import hstack as sparse_hstack
50 
51 from ..base import ClassifierMixin, RegressorMixin
52 from ..utils._joblib import Parallel, delayed
53 from ..metrics import r2_score
54 from ..preprocessing import OneHotEncoder
55 from ..tree import (DecisionTreeClassifier, DecisionTreeRegressor,
56                     ExtraTreeClassifier, ExtraTreeRegressor)
57 from ..tree._tree import DTYPE, DOUBLE
58 from ..utils import check_random_state, check_array, compute_sample_weight
59 from ..exceptions import DataConversionWarning, NotFittedError
60 from .base import BaseEnsemble, _partition_estimators
61 from ..utils.fixes import parallel_helper, _joblib_parallel_args
62 from ..utils.multiclass import check_classification_targets
63 from ..utils.validation import check_is_fitted
64 
65 __all__ = ["RandomForestClassifier",
66            "RandomForestRegressor",
67            "ExtraTreesClassifier",
68            "ExtraTreesRegressor",
69            "RandomTreesEmbedding"]
70 
71 MAX_INT = np.iinfo(np.int32).max
72 
73 
74 def _generate_sample_indices(random_state, n_samples):
75     """Private function used to _parallel_build_trees function."""
76     random_instance = check_random_state(random_state)
77     sample_indices = random_instance.randint(0, n_samples, n_samples)
78 
79     return sample_indices
80 
81 
82 def _generate_unsampled_indices(random_state, n_samples):
83     """Private function used to forest._set_oob_score function."""
84     sample_indices = _generate_sample_indices(random_state, n_samples)
85     sample_counts = np.bincount(sample_indices, minlength=n_samples)
86     unsampled_mask = sample_counts == 0
87     indices_range = np.arange(n_samples)
88     unsampled_indices = indices_range[unsampled_mask]
89 
90     return unsampled_indices
91 
92 
93 def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,
94                           verbose=0, class_weight=None):
95     """Private function used to fit a single tree in parallel."""
96     if verbose > 1:
97         print("building tree %d of %d" % (tree_idx + 1, n_trees))
98 
99     if forest.bootstrap:
100         n_samples = X.shape[0]
101         if sample_weight is None:
102             curr_sample_weight = np.ones((n_samples,), dtype=np.float64)
103         else:
104             curr_sample_weight = sample_weight.copy()
105 
106         indices = _generate_sample_indices(tree.random_state, n_samples)
107         sample_counts = np.bincount(indices, minlength=n_samples)
108         curr_sample_weight *= sample_counts
109 
110         if class_weight == 'subsample':
111             with catch_warnings():
112                 simplefilter('ignore', DeprecationWarning)
113                 curr_sample_weight *= compute_sample_weight('auto', y, indices)
114         elif class_weight == 'balanced_subsample':
115             curr_sample_weight *= compute_sample_weight('balanced', y, indices)
116 
117         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
118     else:
119         tree.fit(X, y, sample_weight=sample_weight, check_input=False)
120 
121     return tree
122 
123 
124 class BaseForest(BaseEnsemble, metaclass=ABCMeta):
125     """Base class for forests of trees.
126 
127     Warning: This class should not be used directly. Use derived classes
128     instead.
129     """
130 
131     @abstractmethod
132     def __init__(self,
133                  base_estimator,
134                  n_estimators=100,
135                  estimator_params=tuple(),
136                  bootstrap=False,
137                  oob_score=False,
138                  n_jobs=None,
139                  random_state=None,
140                  verbose=0,
141                  warm_start=False,
142                  class_weight=None):
143         super().__init__(
144             base_estimator=base_estimator,
145             n_estimators=n_estimators,
146             estimator_params=estimator_params)
147 
148         self.bootstrap = bootstrap
149         self.oob_score = oob_score
150         self.n_jobs = n_jobs
151         self.random_state = random_state
152         self.verbose = verbose
153         self.warm_start = warm_start
154         self.class_weight = class_weight
155 
156     def apply(self, X):
157         """Apply trees in the forest to X, return leaf indices.
158 
159         Parameters
160         ----------
161         X : array-like or sparse matrix, shape = [n_samples, n_features]
162             The input samples. Internally, its dtype will be converted to
163             ``dtype=np.float32``. If a sparse matrix is provided, it will be
164             converted into a sparse ``csr_matrix``.
165 
166         Returns
167         -------
168         X_leaves : array_like, shape = [n_samples, n_estimators]
169             For each datapoint x in X and for each tree in the forest,
170             return the index of the leaf x ends up in.
171         """
172         X = self._validate_X_predict(X)
173         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
174                            **_joblib_parallel_args(prefer="threads"))(
175             delayed(parallel_helper)(tree, 'apply', X, check_input=False)
176             for tree in self.estimators_)
177 
178         return np.array(results).T
179 
180     def decision_path(self, X):
181         """Return the decision path in the forest
182 
183         .. versionadded:: 0.18
184 
185         Parameters
186         ----------
187         X : array-like or sparse matrix, shape = [n_samples, n_features]
188             The input samples. Internally, its dtype will be converted to
189             ``dtype=np.float32``. If a sparse matrix is provided, it will be
190             converted into a sparse ``csr_matrix``.
191 
192         Returns
193         -------
194         indicator : sparse csr array, shape = [n_samples, n_nodes]
195             Return a node indicator matrix where non zero elements
196             indicates that the samples goes through the nodes.
197 
198         n_nodes_ptr : array of size (n_estimators + 1, )
199             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
200             gives the indicator value for the i-th estimator.
201 
202         """
203         X = self._validate_X_predict(X)
204         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
205                               **_joblib_parallel_args(prefer='threads'))(
206             delayed(parallel_helper)(tree, 'decision_path', X,
207                                      check_input=False)
208             for tree in self.estimators_)
209 
210         n_nodes = [0]
211         n_nodes.extend([i.shape[1] for i in indicators])
212         n_nodes_ptr = np.array(n_nodes).cumsum()
213 
214         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
215 
216     def fit(self, X, y, sample_weight=None):
217         """Build a forest of trees from the training set (X, y).
218 
219         Parameters
220         ----------
221         X : array-like or sparse matrix of shape = [n_samples, n_features]
222             The training input samples. Internally, its dtype will be converted
223             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
224             converted into a sparse ``csc_matrix``.
225 
226         y : array-like, shape = [n_samples] or [n_samples, n_outputs]
227             The target values (class labels in classification, real numbers in
228             regression).
229 
230         sample_weight : array-like, shape = [n_samples] or None
231             Sample weights. If None, then samples are equally weighted. Splits
232             that would create child nodes with net zero or negative weight are
233             ignored while searching for a split in each node. In the case of
234             classification, splits are also ignored if they would result in any
235             single class carrying a negative weight in either child node.
236 
237         Returns
238         -------
239         self : object
240         """
241 
242         if self.n_estimators == 'warn':
243             warn("The default value of n_estimators will change from "
244                  "10 in version 0.20 to 100 in 0.22.", FutureWarning)
245             self.n_estimators = 10
246 
247         # Validate or convert input data
248         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
249         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
250         if sample_weight is not None:
251             sample_weight = check_array(sample_weight, ensure_2d=False)
252         if issparse(X):
253             # Pre-sort indices to avoid that each individual tree of the
254             # ensemble sorts the indices.
255             X.sort_indices()
256 
257         # Remap output
258         self.n_features_ = X.shape[1]
259 
260         y = np.atleast_1d(y)
261         if y.ndim == 2 and y.shape[1] == 1:
262             warn("A column-vector y was passed when a 1d array was"
263                  " expected. Please change the shape of y to "
264                  "(n_samples,), for example using ravel().",
265                  DataConversionWarning, stacklevel=2)
266 
267         if y.ndim == 1:
268             # reshape is necessary to preserve the data contiguity against vs
269             # [:, np.newaxis] that does not.
270             y = np.reshape(y, (-1, 1))
271 
272         self.n_outputs_ = y.shape[1]
273 
274         y, expanded_class_weight = self._validate_y_class_weight(y)
275 
276         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
277             y = np.ascontiguousarray(y, dtype=DOUBLE)
278 
279         if expanded_class_weight is not None:
280             if sample_weight is not None:
281                 sample_weight = sample_weight * expanded_class_weight
282             else:
283                 sample_weight = expanded_class_weight
284 
285         # Check parameters
286         self._validate_estimator()
287 
288         if not self.bootstrap and self.oob_score:
289             raise ValueError("Out of bag estimation only available"
290                              " if bootstrap=True")
291 
292         random_state = check_random_state(self.random_state)
293 
294         if not self.warm_start or not hasattr(self, "estimators_"):
295             # Free allocated memory, if any
296             self.estimators_ = []
297 
298         n_more_estimators = self.n_estimators - len(self.estimators_)
299 
300         if n_more_estimators < 0:
301             raise ValueError('n_estimators=%d must be larger or equal to '
302                              'len(estimators_)=%d when warm_start==True'
303                              % (self.n_estimators, len(self.estimators_)))
304 
305         elif n_more_estimators == 0:
306             warn("Warm-start fitting without increasing n_estimators does not "
307                  "fit new trees.")
308         else:
309             if self.warm_start and len(self.estimators_) > 0:
310                 # We draw from the random state to get the random state we
311                 # would have got if we hadn't used a warm_start.
312                 random_state.randint(MAX_INT, size=len(self.estimators_))
313 
314             trees = [self._make_estimator(append=False,
315                                           random_state=random_state)
316                      for i in range(n_more_estimators)]
317 
318             # Parallel loop: we prefer the threading backend as the Cython code
319             # for fitting the trees is internally releasing the Python GIL
320             # making threading more efficient than multiprocessing in
321             # that case. However, for joblib 0.12+ we respect any
322             # parallel_backend contexts set at a higher level,
323             # since correctness does not rely on using threads.
324             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
325                              **_joblib_parallel_args(prefer='threads'))(
326                 delayed(_parallel_build_trees)(
327                     t, self, X, y, sample_weight, i, len(trees),
328                     verbose=self.verbose, class_weight=self.class_weight)
329                 for i, t in enumerate(trees))
330 
331             # Collect newly grown trees
332             self.estimators_.extend(trees)
333 
334         if self.oob_score:
335             self._set_oob_score(X, y)
336 
337         # Decapsulate classes_ attributes
338         if hasattr(self, "classes_") and self.n_outputs_ == 1:
339             self.n_classes_ = self.n_classes_[0]
340             self.classes_ = self.classes_[0]
341 
342         return self
343 
344     @abstractmethod
345     def _set_oob_score(self, X, y):
346         """Calculate out of bag predictions and score."""
347 
348     def _validate_y_class_weight(self, y):
349         # Default implementation
350         return y, None
351 
352     def _validate_X_predict(self, X):
353         """Validate X whenever one tries to predict, apply, predict_proba"""
354         if self.estimators_ is None or len(self.estimators_) == 0:
355             raise NotFittedError("Estimator not fitted, "
356                                  "call `fit` before exploiting the model.")
357 
358         return self.estimators_[0]._validate_X_predict(X, check_input=True)
359 
360     @property
361     def feature_importances_(self):
362         """Return the feature importances (the higher, the more important the
363            feature).
364 
365         Returns
366         -------
367         feature_importances_ : array, shape = [n_features]
368         """
369         check_is_fitted(self, 'estimators_')
370 
371         all_importances = Parallel(n_jobs=self.n_jobs,
372                                    **_joblib_parallel_args(prefer='threads'))(
373             delayed(getattr)(tree, 'feature_importances_')
374             for tree in self.estimators_)
375 
376         return sum(all_importances) / len(self.estimators_)
377 
378 
379 def _accumulate_prediction(predict, X, out, lock):
380     """This is a utility function for joblib's Parallel.
381 
382     It can't go locally in ForestClassifier or ForestRegressor, because joblib
383     complains that it cannot pickle it when placed there.
384     """
385     prediction = predict(X, check_input=False)
386     with lock:
387         if len(out) == 1:
388             out[0] += prediction
389         else:
390             for i in range(len(out)):
391                 out[i] += prediction[i]
392 
393 
394 class ForestClassifier(BaseForest, ClassifierMixin, metaclass=ABCMeta):
395     """Base class for forest of trees-based classifiers.
396 
397     Warning: This class should not be used directly. Use derived classes
398     instead.
399     """
400 
401     @abstractmethod
402     def __init__(self,
403                  base_estimator,
404                  n_estimators=100,
405                  estimator_params=tuple(),
406                  bootstrap=False,
407                  oob_score=False,
408                  n_jobs=None,
409                  random_state=None,
410                  verbose=0,
411                  warm_start=False,
412                  class_weight=None):
413         super().__init__(
414             base_estimator,
415             n_estimators=n_estimators,
416             estimator_params=estimator_params,
417             bootstrap=bootstrap,
418             oob_score=oob_score,
419             n_jobs=n_jobs,
420             random_state=random_state,
421             verbose=verbose,
422             warm_start=warm_start,
423             class_weight=class_weight)
424 
425     def _set_oob_score(self, X, y):
426         """Compute out-of-bag score"""
427         X = check_array(X, dtype=DTYPE, accept_sparse='csr')
428 
429         n_classes_ = self.n_classes_
430         n_samples = y.shape[0]
431 
432         oob_decision_function = []
433         oob_score = 0.0
434         predictions = [np.zeros((n_samples, n_classes_[k]))
435                        for k in range(self.n_outputs_)]
436 
437         for estimator in self.estimators_:
438             unsampled_indices = _generate_unsampled_indices(
439                 estimator.random_state, n_samples)
440             p_estimator = estimator.predict_proba(X[unsampled_indices, :],
441                                                   check_input=False)
442 
443             if self.n_outputs_ == 1:
444                 p_estimator = [p_estimator]
445 
446             for k in range(self.n_outputs_):
447                 predictions[k][unsampled_indices, :] += p_estimator[k]
448 
449         for k in range(self.n_outputs_):
450             if (predictions[k].sum(axis=1) == 0).any():
451                 warn("Some inputs do not have OOB scores. "
452                      "This probably means too few trees were used "
453                      "to compute any reliable oob estimates.")
454 
455             decision = (predictions[k] /
456                         predictions[k].sum(axis=1)[:, np.newaxis])
457             oob_decision_function.append(decision)
458             oob_score += np.mean(y[:, k] ==
459                                  np.argmax(predictions[k], axis=1), axis=0)
460 
461         if self.n_outputs_ == 1:
462             self.oob_decision_function_ = oob_decision_function[0]
463         else:
464             self.oob_decision_function_ = oob_decision_function
465 
466         self.oob_score_ = oob_score / self.n_outputs_
467 
468     def _validate_y_class_weight(self, y):
469         check_classification_targets(y)
470 
471         y = np.copy(y)
472         expanded_class_weight = None
473 
474         if self.class_weight is not None:
475             y_original = np.copy(y)
476 
477         self.classes_ = []
478         self.n_classes_ = []
479 
480         y_store_unique_indices = np.zeros(y.shape, dtype=np.int)
481         for k in range(self.n_outputs_):
482             classes_k, y_store_unique_indices[:, k] = np.unique(y[:, k], return_inverse=True)
483             self.classes_.append(classes_k)
484             self.n_classes_.append(classes_k.shape[0])
485         y = y_store_unique_indices
486 
487         if self.class_weight is not None:
488             valid_presets = ('balanced', 'balanced_subsample')
489             if isinstance(self.class_weight, str):
490                 if self.class_weight not in valid_presets:
491                     raise ValueError('Valid presets for class_weight include '
492                                      '"balanced" and "balanced_subsample". Given "%s".'
493                                      % self.class_weight)
494                 if self.warm_start:
495                     warn('class_weight presets "balanced" or "balanced_subsample" are '
496                          'not recommended for warm_start if the fitted data '
497                          'differs from the full dataset. In order to use '
498                          '"balanced" weights, use compute_class_weight("balanced", '
499                          'classes, y). In place of y you can use a large '
500                          'enough sample of the full training set target to '
501                          'properly estimate the class frequency '
502                          'distributions. Pass the resulting weights as the '
503                          'class_weight parameter.')
504 
505             if (self.class_weight != 'balanced_subsample' or
506                     not self.bootstrap):
507                 if self.class_weight == "balanced_subsample":
508                     class_weight = "balanced"
509                 else:
510                     class_weight = self.class_weight
511                 expanded_class_weight = compute_sample_weight(class_weight,
512                                                               y_original)
513 
514         return y, expanded_class_weight
515 
516     def predict(self, X):
517         """Predict class for X.
518 
519         The predicted class of an input sample is a vote by the trees in
520         the forest, weighted by their probability estimates. That is,
521         the predicted class is the one with highest mean probability
522         estimate across the trees.
523 
524         Parameters
525         ----------
526         X : array-like or sparse matrix of shape = [n_samples, n_features]
527             The input samples. Internally, its dtype will be converted to
528             ``dtype=np.float32``. If a sparse matrix is provided, it will be
529             converted into a sparse ``csr_matrix``.
530 
531         Returns
532         -------
533         y : array of shape = [n_samples] or [n_samples, n_outputs]
534             The predicted classes.
535         """
536         proba = self.predict_proba(X)
537 
538         if self.n_outputs_ == 1:
539             return self.classes_.take(np.argmax(proba, axis=1), axis=0)
540 
541         else:
542             n_samples = proba[0].shape[0]
543             # all dtypes should be the same, so just take the first
544             class_type = self.classes_[0].dtype
545             predictions = np.empty((n_samples, self.n_outputs_),
546                                    dtype=class_type)
547 
548             for k in range(self.n_outputs_):
549                 predictions[:, k] = self.classes_[k].take(np.argmax(proba[k],
550                                                                     axis=1),
551                                                           axis=0)
552 
553             return predictions
554 
555     def predict_proba(self, X):
556         """Predict class probabilities for X.
557 
558         The predicted class probabilities of an input sample are computed as
559         the mean predicted class probabilities of the trees in the forest. The
560         class probability of a single tree is the fraction of samples of the same
561         class in a leaf.
562 
563         Parameters
564         ----------
565         X : array-like or sparse matrix of shape = [n_samples, n_features]
566             The input samples. Internally, its dtype will be converted to
567             ``dtype=np.float32``. If a sparse matrix is provided, it will be
568             converted into a sparse ``csr_matrix``.
569 
570         Returns
571         -------
572         p : array of shape = [n_samples, n_classes], or a list of n_outputs
573             such arrays if n_outputs > 1.
574             The class probabilities of the input samples. The order of the
575             classes corresponds to that in the attribute `classes_`.
576         """
577         check_is_fitted(self, 'estimators_')
578         # Check data
579         X = self._validate_X_predict(X)
580 
581         # Assign chunk of trees to jobs
582         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
583 
584         # avoid storing the output of every estimator by summing them here
585         all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)
586                      for j in np.atleast_1d(self.n_classes_)]
587         lock = threading.Lock()
588         Parallel(n_jobs=n_jobs, verbose=self.verbose,
589                  **_joblib_parallel_args(require="sharedmem"))(
590             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,
591                                             lock)
592             for e in self.estimators_)
593 
594         for proba in all_proba:
595             proba /= len(self.estimators_)
596 
597         if len(all_proba) == 1:
598             return all_proba[0]
599         else:
600             return all_proba
601 
602     def predict_log_proba(self, X):
603         """Predict class log-probabilities for X.
604 
605         The predicted class log-probabilities of an input sample is computed as
606         the log of the mean predicted class probabilities of the trees in the
607         forest.
608 
609         Parameters
610         ----------
611         X : array-like or sparse matrix of shape = [n_samples, n_features]
612             The input samples. Internally, its dtype will be converted to
613             ``dtype=np.float32``. If a sparse matrix is provided, it will be
614             converted into a sparse ``csr_matrix``.
615 
616         Returns
617         -------
618         p : array of shape = [n_samples, n_classes], or a list of n_outputs
619             such arrays if n_outputs > 1.
620             The class probabilities of the input samples. The order of the
621             classes corresponds to that in the attribute `classes_`.
622         """
623         proba = self.predict_proba(X)
624 
625         if self.n_outputs_ == 1:
626             return np.log(proba)
627 
628         else:
629             for k in range(self.n_outputs_):
630                 proba[k] = np.log(proba[k])
631 
632             return proba
633 
634 
635 class ForestRegressor(BaseForest, RegressorMixin, metaclass=ABCMeta):
636     """Base class for forest of trees-based regressors.
637 
638     Warning: This class should not be used directly. Use derived classes
639     instead.
640     """
641 
642     @abstractmethod
643     def __init__(self,
644                  base_estimator,
645                  n_estimators=100,
646                  estimator_params=tuple(),
647                  bootstrap=False,
648                  oob_score=False,
649                  n_jobs=None,
650                  random_state=None,
651                  verbose=0,
652                  warm_start=False):
653         super().__init__(
654             base_estimator,
655             n_estimators=n_estimators,
656             estimator_params=estimator_params,
657             bootstrap=bootstrap,
658             oob_score=oob_score,
659             n_jobs=n_jobs,
660             random_state=random_state,
661             verbose=verbose,
662             warm_start=warm_start)
663 
664     def predict(self, X):
665         """Predict regression target for X.
666 
667         The predicted regression target of an input sample is computed as the
668         mean predicted regression targets of the trees in the forest.
669 
670         Parameters
671         ----------
672         X : array-like or sparse matrix of shape = [n_samples, n_features]
673             The input samples. Internally, its dtype will be converted to
674             ``dtype=np.float32``. If a sparse matrix is provided, it will be
675             converted into a sparse ``csr_matrix``.
676 
677         Returns
678         -------
679         y : array of shape = [n_samples] or [n_samples, n_outputs]
680             The predicted values.
681         """
682         check_is_fitted(self, 'estimators_')
683         # Check data
684         X = self._validate_X_predict(X)
685 
686         # Assign chunk of trees to jobs
687         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
688 
689         # avoid storing the output of every estimator by summing them here
690         if self.n_outputs_ > 1:
691             y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)
692         else:
693             y_hat = np.zeros((X.shape[0]), dtype=np.float64)
694 
695         # Parallel loop
696         lock = threading.Lock()
697         Parallel(n_jobs=n_jobs, verbose=self.verbose,
698                  **_joblib_parallel_args(require="sharedmem"))(
699             delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)
700             for e in self.estimators_)
701 
702         y_hat /= len(self.estimators_)
703 
704         return y_hat
705 
706     def _set_oob_score(self, X, y):
707         """Compute out-of-bag scores"""
708         X = check_array(X, dtype=DTYPE, accept_sparse='csr')
709 
710         n_samples = y.shape[0]
711 
712         predictions = np.zeros((n_samples, self.n_outputs_))
713         n_predictions = np.zeros((n_samples, self.n_outputs_))
714 
715         for estimator in self.estimators_:
716             unsampled_indices = _generate_unsampled_indices(
717                 estimator.random_state, n_samples)
718             p_estimator = estimator.predict(
719                 X[unsampled_indices, :], check_input=False)
720 
721             if self.n_outputs_ == 1:
722                 p_estimator = p_estimator[:, np.newaxis]
723 
724             predictions[unsampled_indices, :] += p_estimator
725             n_predictions[unsampled_indices, :] += 1
726 
727         if (n_predictions == 0).any():
728             warn("Some inputs do not have OOB scores. "
729                  "This probably means too few trees were used "
730                  "to compute any reliable oob estimates.")
731             n_predictions[n_predictions == 0] = 1
732 
733         predictions /= n_predictions
734         self.oob_prediction_ = predictions
735 
736         if self.n_outputs_ == 1:
737             self.oob_prediction_ = \
738                 self.oob_prediction_.reshape((n_samples, ))
739 
740         self.oob_score_ = 0.0
741 
742         for k in range(self.n_outputs_):
743             self.oob_score_ += r2_score(y[:, k],
744                                         predictions[:, k])
745 
746         self.oob_score_ /= self.n_outputs_
747 
748 
749 class RandomForestClassifier(ForestClassifier):
750     """A random forest classifier.
751 
752     A random forest is a meta estimator that fits a number of decision tree
753     classifiers on various sub-samples of the dataset and uses averaging to
754     improve the predictive accuracy and control over-fitting.
755     The sub-sample size is always the same as the original
756     input sample size but the samples are drawn with replacement if
757     `bootstrap=True` (default).
758 
759     Read more in the :ref:`User Guide <forest>`.
760 
761     Parameters
762     ----------
763     n_estimators : integer, optional (default=10)
764         The number of trees in the forest.
765 
766         .. versionchanged:: 0.20
767            The default value of ``n_estimators`` will change from 10 in
768            version 0.20 to 100 in version 0.22.
769 
770     criterion : string, optional (default="gini")
771         The function to measure the quality of a split. Supported criteria are
772         "gini" for the Gini impurity and "entropy" for the information gain.
773         Note: this parameter is tree-specific.
774 
775     max_depth : integer or None, optional (default=None)
776         The maximum depth of the tree. If None, then nodes are expanded until
777         all leaves are pure or until all leaves contain less than
778         min_samples_split samples.
779 
780     min_samples_split : int, float, optional (default=2)
781         The minimum number of samples required to split an internal node:
782 
783         - If int, then consider `min_samples_split` as the minimum number.
784         - If float, then `min_samples_split` is a fraction and
785           `ceil(min_samples_split * n_samples)` are the minimum
786           number of samples for each split.
787 
788         .. versionchanged:: 0.18
789            Added float values for fractions.
790 
791     min_samples_leaf : int, float, optional (default=1)
792         The minimum number of samples required to be at a leaf node.
793         A split point at any depth will only be considered if it leaves at
794         least ``min_samples_leaf`` training samples in each of the left and
795         right branches.  This may have the effect of smoothing the model,
796         especially in regression.
797 
798         - If int, then consider `min_samples_leaf` as the minimum number.
799         - If float, then `min_samples_leaf` is a fraction and
800           `ceil(min_samples_leaf * n_samples)` are the minimum
801           number of samples for each node.
802 
803         .. versionchanged:: 0.18
804            Added float values for fractions.
805 
806     min_weight_fraction_leaf : float, optional (default=0.)
807         The minimum weighted fraction of the sum total of weights (of all
808         the input samples) required to be at a leaf node. Samples have
809         equal weight when sample_weight is not provided.
810 
811     max_features : int, float, string or None, optional (default="auto")
812         The number of features to consider when looking for the best split:
813 
814         - If int, then consider `max_features` features at each split.
815         - If float, then `max_features` is a fraction and
816           `int(max_features * n_features)` features are considered at each
817           split.
818         - If "auto", then `max_features=sqrt(n_features)`.
819         - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
820         - If "log2", then `max_features=log2(n_features)`.
821         - If None, then `max_features=n_features`.
822 
823         Note: the search for a split does not stop until at least one
824         valid partition of the node samples is found, even if it requires to
825         effectively inspect more than ``max_features`` features.
826 
827     max_leaf_nodes : int or None, optional (default=None)
828         Grow trees with ``max_leaf_nodes`` in best-first fashion.
829         Best nodes are defined as relative reduction in impurity.
830         If None then unlimited number of leaf nodes.
831 
832     min_impurity_decrease : float, optional (default=0.)
833         A node will be split if this split induces a decrease of the impurity
834         greater than or equal to this value.
835 
836         The weighted impurity decrease equation is the following::
837 
838             N_t / N * (impurity - N_t_R / N_t * right_impurity
839                                 - N_t_L / N_t * left_impurity)
840 
841         where ``N`` is the total number of samples, ``N_t`` is the number of
842         samples at the current node, ``N_t_L`` is the number of samples in the
843         left child, and ``N_t_R`` is the number of samples in the right child.
844 
845         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
846         if ``sample_weight`` is passed.
847 
848         .. versionadded:: 0.19
849 
850     min_impurity_split : float, (default=1e-7)
851         Threshold for early stopping in tree growth. A node will split
852         if its impurity is above the threshold, otherwise it is a leaf.
853 
854         .. deprecated:: 0.19
855            ``min_impurity_split`` has been deprecated in favor of
856            ``min_impurity_decrease`` in 0.19. The default value of
857            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
858            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
859 
860 
861     bootstrap : boolean, optional (default=True)
862         Whether bootstrap samples are used when building trees. If False, the
863         whole datset is used to build each tree.
864 
865     oob_score : bool (default=False)
866         Whether to use out-of-bag samples to estimate
867         the generalization accuracy.
868 
869     n_jobs : int or None, optional (default=None)
870         The number of jobs to run in parallel for both `fit` and `predict`.
871         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
872         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
873         for more details.
874 
875     random_state : int, RandomState instance or None, optional (default=None)
876         If int, random_state is the seed used by the random number generator;
877         If RandomState instance, random_state is the random number generator;
878         If None, the random number generator is the RandomState instance used
879         by `np.random`.
880 
881     verbose : int, optional (default=0)
882         Controls the verbosity when fitting and predicting.
883 
884     warm_start : bool, optional (default=False)
885         When set to ``True``, reuse the solution of the previous call to fit
886         and add more estimators to the ensemble, otherwise, just fit a whole
887         new forest. See :term:`the Glossary <warm_start>`.
888 
889     class_weight : dict, list of dicts, "balanced", "balanced_subsample" or \
890     None, optional (default=None)
891         Weights associated with classes in the form ``{class_label: weight}``.
892         If not given, all classes are supposed to have weight one. For
893         multi-output problems, a list of dicts can be provided in the same
894         order as the columns of y.
895 
896         Note that for multioutput (including multilabel) weights should be
897         defined for each class of every column in its own dict. For example,
898         for four-class multilabel classification weights should be
899         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
900         [{1:1}, {2:5}, {3:1}, {4:1}].
901 
902         The "balanced" mode uses the values of y to automatically adjust
903         weights inversely proportional to class frequencies in the input data
904         as ``n_samples / (n_classes * np.bincount(y))``
905 
906         The "balanced_subsample" mode is the same as "balanced" except that
907         weights are computed based on the bootstrap sample for every tree
908         grown.
909 
910         For multi-output, the weights of each column of y will be multiplied.
911 
912         Note that these weights will be multiplied with sample_weight (passed
913         through the fit method) if sample_weight is specified.
914 
915     Attributes
916     ----------
917     estimators_ : list of DecisionTreeClassifier
918         The collection of fitted sub-estimators.
919 
920     classes_ : array of shape = [n_classes] or a list of such arrays
921         The classes labels (single output problem), or a list of arrays of
922         class labels (multi-output problem).
923 
924     n_classes_ : int or list
925         The number of classes (single output problem), or a list containing the
926         number of classes for each output (multi-output problem).
927 
928     n_features_ : int
929         The number of features when ``fit`` is performed.
930 
931     n_outputs_ : int
932         The number of outputs when ``fit`` is performed.
933 
934     feature_importances_ : array of shape = [n_features]
935         The feature importances (the higher, the more important the feature).
936 
937     oob_score_ : float
938         Score of the training dataset obtained using an out-of-bag estimate.
939 
940     oob_decision_function_ : array of shape = [n_samples, n_classes]
941         Decision function computed with out-of-bag estimate on the training
942         set. If n_estimators is small it might be possible that a data point
943         was never left out during the bootstrap. In this case,
944         `oob_decision_function_` might contain NaN.
945 
946     Examples
947     --------
948     >>> from sklearn.ensemble import RandomForestClassifier
949     >>> from sklearn.datasets import make_classification
950 
951     >>> X, y = make_classification(n_samples=1000, n_features=4,
952     ...                            n_informative=2, n_redundant=0,
953     ...                            random_state=0, shuffle=False)
954     >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,
955     ...                              random_state=0)
956     >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
957     RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
958                 max_depth=2, max_features='auto', max_leaf_nodes=None,
959                 min_impurity_decrease=0.0, min_impurity_split=None,
960                 min_samples_leaf=1, min_samples_split=2,
961                 min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
962                 oob_score=False, random_state=0, verbose=0, warm_start=False)
963     >>> print(clf.feature_importances_)
964     [0.14205973 0.76664038 0.0282433  0.06305659]
965     >>> print(clf.predict([[0, 0, 0, 0]]))
966     [1]
967 
968     Notes
969     -----
970     The default values for the parameters controlling the size of the trees
971     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
972     unpruned trees which can potentially be very large on some data sets. To
973     reduce memory consumption, the complexity and size of the trees should be
974     controlled by setting those parameter values.
975 
976     The features are always randomly permuted at each split. Therefore,
977     the best found split may vary, even with the same training data,
978     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
979     of the criterion is identical for several splits enumerated during the
980     search of the best split. To obtain a deterministic behaviour during
981     fitting, ``random_state`` has to be fixed.
982 
983     References
984     ----------
985 
986     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
987 
988     See also
989     --------
990     DecisionTreeClassifier, ExtraTreesClassifier
991     """
992     def __init__(self,
993                  n_estimators='warn',
994                  criterion="gini",
995                  max_depth=None,
996                  min_samples_split=2,
997                  min_samples_leaf=1,
998                  min_weight_fraction_leaf=0.,
999                  max_features="auto",
1000                  max_leaf_nodes=None,
1001                  min_impurity_decrease=0.,
1002                  min_impurity_split=None,
1003                  bootstrap=True,
1004                  oob_score=False,
1005                  n_jobs=None,
1006                  random_state=None,
1007                  verbose=0,
1008                  warm_start=False,
1009                  class_weight=None):
1010         super().__init__(
1011             base_estimator=DecisionTreeClassifier(),
1012             n_estimators=n_estimators,
1013             estimator_params=("criterion", "max_depth", "min_samples_split",
1014                               "min_samples_leaf", "min_weight_fraction_leaf",
1015                               "max_features", "max_leaf_nodes",
1016                               "min_impurity_decrease", "min_impurity_split",
1017                               "random_state"),
1018             bootstrap=bootstrap,
1019             oob_score=oob_score,
1020             n_jobs=n_jobs,
1021             random_state=random_state,
1022             verbose=verbose,
1023             warm_start=warm_start,
1024             class_weight=class_weight)
1025 
1026         self.criterion = criterion
1027         self.max_depth = max_depth
1028         self.min_samples_split = min_samples_split
1029         self.min_samples_leaf = min_samples_leaf
1030         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1031         self.max_features = max_features
1032         self.max_leaf_nodes = max_leaf_nodes
1033         self.min_impurity_decrease = min_impurity_decrease
1034         self.min_impurity_split = min_impurity_split
1035 
1036 
1037 class RandomForestRegressor(ForestRegressor):
1038     """A random forest regressor.
1039 
1040     A random forest is a meta estimator that fits a number of classifying
1041     decision trees on various sub-samples of the dataset and uses averaging
1042     to improve the predictive accuracy and control over-fitting.
1043     The sub-sample size is always the same as the original
1044     input sample size but the samples are drawn with replacement if
1045     `bootstrap=True` (default).
1046 
1047     Read more in the :ref:`User Guide <forest>`.
1048 
1049     Parameters
1050     ----------
1051     n_estimators : integer, optional (default=10)
1052         The number of trees in the forest.
1053 
1054         .. versionchanged:: 0.20
1055            The default value of ``n_estimators`` will change from 10 in
1056            version 0.20 to 100 in version 0.22.
1057 
1058     criterion : string, optional (default="mse")
1059         The function to measure the quality of a split. Supported criteria
1060         are "mse" for the mean squared error, which is equal to variance
1061         reduction as feature selection criterion, and "mae" for the mean
1062         absolute error.
1063 
1064         .. versionadded:: 0.18
1065            Mean Absolute Error (MAE) criterion.
1066 
1067     max_depth : integer or None, optional (default=None)
1068         The maximum depth of the tree. If None, then nodes are expanded until
1069         all leaves are pure or until all leaves contain less than
1070         min_samples_split samples.
1071 
1072     min_samples_split : int, float, optional (default=2)
1073         The minimum number of samples required to split an internal node:
1074 
1075         - If int, then consider `min_samples_split` as the minimum number.
1076         - If float, then `min_samples_split` is a fraction and
1077           `ceil(min_samples_split * n_samples)` are the minimum
1078           number of samples for each split.
1079 
1080         .. versionchanged:: 0.18
1081            Added float values for fractions.
1082 
1083     min_samples_leaf : int, float, optional (default=1)
1084         The minimum number of samples required to be at a leaf node.
1085         A split point at any depth will only be considered if it leaves at
1086         least ``min_samples_leaf`` training samples in each of the left and
1087         right branches.  This may have the effect of smoothing the model,
1088         especially in regression.
1089 
1090         - If int, then consider `min_samples_leaf` as the minimum number.
1091         - If float, then `min_samples_leaf` is a fraction and
1092           `ceil(min_samples_leaf * n_samples)` are the minimum
1093           number of samples for each node.
1094 
1095         .. versionchanged:: 0.18
1096            Added float values for fractions.
1097 
1098     min_weight_fraction_leaf : float, optional (default=0.)
1099         The minimum weighted fraction of the sum total of weights (of all
1100         the input samples) required to be at a leaf node. Samples have
1101         equal weight when sample_weight is not provided.
1102 
1103     max_features : int, float, string or None, optional (default="auto")
1104         The number of features to consider when looking for the best split:
1105 
1106         - If int, then consider `max_features` features at each split.
1107         - If float, then `max_features` is a fraction and
1108           `int(max_features * n_features)` features are considered at each
1109           split.
1110         - If "auto", then `max_features=n_features`.
1111         - If "sqrt", then `max_features=sqrt(n_features)`.
1112         - If "log2", then `max_features=log2(n_features)`.
1113         - If None, then `max_features=n_features`.
1114 
1115         Note: the search for a split does not stop until at least one
1116         valid partition of the node samples is found, even if it requires to
1117         effectively inspect more than ``max_features`` features.
1118 
1119     max_leaf_nodes : int or None, optional (default=None)
1120         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1121         Best nodes are defined as relative reduction in impurity.
1122         If None then unlimited number of leaf nodes.
1123 
1124     min_impurity_decrease : float, optional (default=0.)
1125         A node will be split if this split induces a decrease of the impurity
1126         greater than or equal to this value.
1127 
1128         The weighted impurity decrease equation is the following::
1129 
1130             N_t / N * (impurity - N_t_R / N_t * right_impurity
1131                                 - N_t_L / N_t * left_impurity)
1132 
1133         where ``N`` is the total number of samples, ``N_t`` is the number of
1134         samples at the current node, ``N_t_L`` is the number of samples in the
1135         left child, and ``N_t_R`` is the number of samples in the right child.
1136 
1137         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1138         if ``sample_weight`` is passed.
1139 
1140         .. versionadded:: 0.19
1141 
1142     min_impurity_split : float, (default=1e-7)
1143         Threshold for early stopping in tree growth. A node will split
1144         if its impurity is above the threshold, otherwise it is a leaf.
1145 
1146         .. deprecated:: 0.19
1147            ``min_impurity_split`` has been deprecated in favor of
1148            ``min_impurity_decrease`` in 0.19. The default value of
1149            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1150            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1151 
1152     bootstrap : boolean, optional (default=True)
1153         Whether bootstrap samples are used when building trees. If False, the
1154         whole datset is used to build each tree.
1155 
1156     oob_score : bool, optional (default=False)
1157         whether to use out-of-bag samples to estimate
1158         the R^2 on unseen data.
1159 
1160     n_jobs : int or None, optional (default=None)
1161         The number of jobs to run in parallel for both `fit` and `predict`.
1162         `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1163         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1164         for more details.
1165 
1166     random_state : int, RandomState instance or None, optional (default=None)
1167         If int, random_state is the seed used by the random number generator;
1168         If RandomState instance, random_state is the random number generator;
1169         If None, the random number generator is the RandomState instance used
1170         by `np.random`.
1171 
1172     verbose : int, optional (default=0)
1173         Controls the verbosity when fitting and predicting.
1174 
1175     warm_start : bool, optional (default=False)
1176         When set to ``True``, reuse the solution of the previous call to fit
1177         and add more estimators to the ensemble, otherwise, just fit a whole
1178         new forest. See :term:`the Glossary <warm_start>`.
1179 
1180     Attributes
1181     ----------
1182     estimators_ : list of DecisionTreeRegressor
1183         The collection of fitted sub-estimators.
1184 
1185     feature_importances_ : array of shape = [n_features]
1186         The feature importances (the higher, the more important the feature).
1187 
1188     n_features_ : int
1189         The number of features when ``fit`` is performed.
1190 
1191     n_outputs_ : int
1192         The number of outputs when ``fit`` is performed.
1193 
1194     oob_score_ : float
1195         Score of the training dataset obtained using an out-of-bag estimate.
1196 
1197     oob_prediction_ : array of shape = [n_samples]
1198         Prediction computed with out-of-bag estimate on the training set.
1199 
1200     Examples
1201     --------
1202     >>> from sklearn.ensemble import RandomForestRegressor
1203     >>> from sklearn.datasets import make_regression
1204 
1205     >>> X, y = make_regression(n_features=4, n_informative=2,
1206     ...                        random_state=0, shuffle=False)
1207     >>> regr = RandomForestRegressor(max_depth=2, random_state=0,
1208     ...                              n_estimators=100)
1209     >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
1210     RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,
1211                max_features='auto', max_leaf_nodes=None,
1212                min_impurity_decrease=0.0, min_impurity_split=None,
1213                min_samples_leaf=1, min_samples_split=2,
1214                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
1215                oob_score=False, random_state=0, verbose=0, warm_start=False)
1216     >>> print(regr.feature_importances_)
1217     [0.18146984 0.81473937 0.00145312 0.00233767]
1218     >>> print(regr.predict([[0, 0, 0, 0]]))
1219     [-8.32987858]
1220 
1221     Notes
1222     -----
1223     The default values for the parameters controlling the size of the trees
1224     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1225     unpruned trees which can potentially be very large on some data sets. To
1226     reduce memory consumption, the complexity and size of the trees should be
1227     controlled by setting those parameter values.
1228 
1229     The features are always randomly permuted at each split. Therefore,
1230     the best found split may vary, even with the same training data,
1231     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
1232     of the criterion is identical for several splits enumerated during the
1233     search of the best split. To obtain a deterministic behaviour during
1234     fitting, ``random_state`` has to be fixed.
1235 
1236     The default value ``max_features="auto"`` uses ``n_features``
1237     rather than ``n_features / 3``. The latter was originally suggested in
1238     [1], whereas the former was more recently justified empirically in [2].
1239 
1240     References
1241     ----------
1242 
1243     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
1244 
1245     .. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
1246            trees", Machine Learning, 63(1), 3-42, 2006.
1247 
1248     See also
1249     --------
1250     DecisionTreeRegressor, ExtraTreesRegressor
1251     """
1252     def __init__(self,
1253                  n_estimators='warn',
1254                  criterion="mse",
1255                  max_depth=None,
1256                  min_samples_split=2,
1257                  min_samples_leaf=1,
1258                  min_weight_fraction_leaf=0.,
1259                  max_features="auto",
1260                  max_leaf_nodes=None,
1261                  min_impurity_decrease=0.,
1262                  min_impurity_split=None,
1263                  bootstrap=True,
1264                  oob_score=False,
1265                  n_jobs=None,
1266                  random_state=None,
1267                  verbose=0,
1268                  warm_start=False):
1269         super().__init__(
1270             base_estimator=DecisionTreeRegressor(),
1271             n_estimators=n_estimators,
1272             estimator_params=("criterion", "max_depth", "min_samples_split",
1273                               "min_samples_leaf", "min_weight_fraction_leaf",
1274                               "max_features", "max_leaf_nodes",
1275                               "min_impurity_decrease", "min_impurity_split",
1276                               "random_state"),
1277             bootstrap=bootstrap,
1278             oob_score=oob_score,
1279             n_jobs=n_jobs,
1280             random_state=random_state,
1281             verbose=verbose,
1282             warm_start=warm_start)
1283 
1284         self.criterion = criterion
1285         self.max_depth = max_depth
1286         self.min_samples_split = min_samples_split
1287         self.min_samples_leaf = min_samples_leaf
1288         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1289         self.max_features = max_features
1290         self.max_leaf_nodes = max_leaf_nodes
1291         self.min_impurity_decrease = min_impurity_decrease
1292         self.min_impurity_split = min_impurity_split
1293 
1294 
1295 class ExtraTreesClassifier(ForestClassifier):
1296     """An extra-trees classifier.
1297 
1298     This class implements a meta estimator that fits a number of
1299     randomized decision trees (a.k.a. extra-trees) on various sub-samples
1300     of the dataset and uses averaging to improve the predictive accuracy
1301     and control over-fitting.
1302 
1303     Read more in the :ref:`User Guide <forest>`.
1304 
1305     Parameters
1306     ----------
1307     n_estimators : integer, optional (default=10)
1308         The number of trees in the forest.
1309 
1310         .. versionchanged:: 0.20
1311            The default value of ``n_estimators`` will change from 10 in
1312            version 0.20 to 100 in version 0.22.
1313 
1314     criterion : string, optional (default="gini")
1315         The function to measure the quality of a split. Supported criteria are
1316         "gini" for the Gini impurity and "entropy" for the information gain.
1317 
1318     max_depth : integer or None, optional (default=None)
1319         The maximum depth of the tree. If None, then nodes are expanded until
1320         all leaves are pure or until all leaves contain less than
1321         min_samples_split samples.
1322 
1323     min_samples_split : int, float, optional (default=2)
1324         The minimum number of samples required to split an internal node:
1325 
1326         - If int, then consider `min_samples_split` as the minimum number.
1327         - If float, then `min_samples_split` is a fraction and
1328           `ceil(min_samples_split * n_samples)` are the minimum
1329           number of samples for each split.
1330 
1331         .. versionchanged:: 0.18
1332            Added float values for fractions.
1333 
1334     min_samples_leaf : int, float, optional (default=1)
1335         The minimum number of samples required to be at a leaf node.
1336         A split point at any depth will only be considered if it leaves at
1337         least ``min_samples_leaf`` training samples in each of the left and
1338         right branches.  This may have the effect of smoothing the model,
1339         especially in regression.
1340 
1341         - If int, then consider `min_samples_leaf` as the minimum number.
1342         - If float, then `min_samples_leaf` is a fraction and
1343           `ceil(min_samples_leaf * n_samples)` are the minimum
1344           number of samples for each node.
1345 
1346         .. versionchanged:: 0.18
1347            Added float values for fractions.
1348 
1349     min_weight_fraction_leaf : float, optional (default=0.)
1350         The minimum weighted fraction of the sum total of weights (of all
1351         the input samples) required to be at a leaf node. Samples have
1352         equal weight when sample_weight is not provided.
1353 
1354     max_features : int, float, string or None, optional (default="auto")
1355         The number of features to consider when looking for the best split:
1356 
1357         - If int, then consider `max_features` features at each split.
1358         - If float, then `max_features` is a fraction and
1359           `int(max_features * n_features)` features are considered at each
1360           split.
1361         - If "auto", then `max_features=sqrt(n_features)`.
1362         - If "sqrt", then `max_features=sqrt(n_features)`.
1363         - If "log2", then `max_features=log2(n_features)`.
1364         - If None, then `max_features=n_features`.
1365 
1366         Note: the search for a split does not stop until at least one
1367         valid partition of the node samples is found, even if it requires to
1368         effectively inspect more than ``max_features`` features.
1369 
1370     max_leaf_nodes : int or None, optional (default=None)
1371         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1372         Best nodes are defined as relative reduction in impurity.
1373         If None then unlimited number of leaf nodes.
1374 
1375     min_impurity_decrease : float, optional (default=0.)
1376         A node will be split if this split induces a decrease of the impurity
1377         greater than or equal to this value.
1378 
1379         The weighted impurity decrease equation is the following::
1380 
1381             N_t / N * (impurity - N_t_R / N_t * right_impurity
1382                                 - N_t_L / N_t * left_impurity)
1383 
1384         where ``N`` is the total number of samples, ``N_t`` is the number of
1385         samples at the current node, ``N_t_L`` is the number of samples in the
1386         left child, and ``N_t_R`` is the number of samples in the right child.
1387 
1388         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1389         if ``sample_weight`` is passed.
1390 
1391         .. versionadded:: 0.19
1392 
1393     min_impurity_split : float, (default=1e-7)
1394         Threshold for early stopping in tree growth. A node will split
1395         if its impurity is above the threshold, otherwise it is a leaf.
1396 
1397         .. deprecated:: 0.19
1398            ``min_impurity_split`` has been deprecated in favor of
1399            ``min_impurity_decrease`` in 0.19. The default value of
1400            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1401            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1402 
1403     bootstrap : boolean, optional (default=False)
1404         Whether bootstrap samples are used when building trees. If False, the
1405         whole datset is used to build each tree.
1406 
1407     oob_score : bool, optional (default=False)
1408         Whether to use out-of-bag samples to estimate
1409         the generalization accuracy.
1410 
1411     n_jobs : int or None, optional (default=None)
1412         The number of jobs to run in parallel for both `fit` and `predict`.
1413         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1414         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1415         for more details.
1416 
1417     random_state : int, RandomState instance or None, optional (default=None)
1418         If int, random_state is the seed used by the random number generator;
1419         If RandomState instance, random_state is the random number generator;
1420         If None, the random number generator is the RandomState instance used
1421         by `np.random`.
1422 
1423     verbose : int, optional (default=0)
1424         Controls the verbosity when fitting and predicting.
1425 
1426     warm_start : bool, optional (default=False)
1427         When set to ``True``, reuse the solution of the previous call to fit
1428         and add more estimators to the ensemble, otherwise, just fit a whole
1429         new forest. See :term:`the Glossary <warm_start>`.
1430 
1431     class_weight : dict, list of dicts, "balanced", "balanced_subsample" or \
1432     None, optional (default=None)
1433         Weights associated with classes in the form ``{class_label: weight}``.
1434         If not given, all classes are supposed to have weight one. For
1435         multi-output problems, a list of dicts can be provided in the same
1436         order as the columns of y.
1437 
1438         Note that for multioutput (including multilabel) weights should be
1439         defined for each class of every column in its own dict. For example,
1440         for four-class multilabel classification weights should be
1441         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
1442         [{1:1}, {2:5}, {3:1}, {4:1}].
1443 
1444         The "balanced" mode uses the values of y to automatically adjust
1445         weights inversely proportional to class frequencies in the input data
1446         as ``n_samples / (n_classes * np.bincount(y))``
1447 
1448         The "balanced_subsample" mode is the same as "balanced" except that weights are
1449         computed based on the bootstrap sample for every tree grown.
1450 
1451         For multi-output, the weights of each column of y will be multiplied.
1452 
1453         Note that these weights will be multiplied with sample_weight (passed
1454         through the fit method) if sample_weight is specified.
1455 
1456     Attributes
1457     ----------
1458     estimators_ : list of DecisionTreeClassifier
1459         The collection of fitted sub-estimators.
1460 
1461     classes_ : array of shape = [n_classes] or a list of such arrays
1462         The classes labels (single output problem), or a list of arrays of
1463         class labels (multi-output problem).
1464 
1465     n_classes_ : int or list
1466         The number of classes (single output problem), or a list containing the
1467         number of classes for each output (multi-output problem).
1468 
1469     feature_importances_ : array of shape = [n_features]
1470         The feature importances (the higher, the more important the feature).
1471 
1472     n_features_ : int
1473         The number of features when ``fit`` is performed.
1474 
1475     n_outputs_ : int
1476         The number of outputs when ``fit`` is performed.
1477 
1478     oob_score_ : float
1479         Score of the training dataset obtained using an out-of-bag estimate.
1480 
1481     oob_decision_function_ : array of shape = [n_samples, n_classes]
1482         Decision function computed with out-of-bag estimate on the training
1483         set. If n_estimators is small it might be possible that a data point
1484         was never left out during the bootstrap. In this case,
1485         `oob_decision_function_` might contain NaN.
1486 
1487     Notes
1488     -----
1489     The default values for the parameters controlling the size of the trees
1490     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1491     unpruned trees which can potentially be very large on some data sets. To
1492     reduce memory consumption, the complexity and size of the trees should be
1493     controlled by setting those parameter values.
1494 
1495     References
1496     ----------
1497 
1498     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
1499            trees", Machine Learning, 63(1), 3-42, 2006.
1500 
1501     See also
1502     --------
1503     sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.
1504     RandomForestClassifier : Ensemble Classifier based on trees with optimal
1505         splits.
1506     """
1507     def __init__(self,
1508                  n_estimators='warn',
1509                  criterion="gini",
1510                  max_depth=None,
1511                  min_samples_split=2,
1512                  min_samples_leaf=1,
1513                  min_weight_fraction_leaf=0.,
1514                  max_features="auto",
1515                  max_leaf_nodes=None,
1516                  min_impurity_decrease=0.,
1517                  min_impurity_split=None,
1518                  bootstrap=False,
1519                  oob_score=False,
1520                  n_jobs=None,
1521                  random_state=None,
1522                  verbose=0,
1523                  warm_start=False,
1524                  class_weight=None):
1525         super().__init__(
1526             base_estimator=ExtraTreeClassifier(),
1527             n_estimators=n_estimators,
1528             estimator_params=("criterion", "max_depth", "min_samples_split",
1529                               "min_samples_leaf", "min_weight_fraction_leaf",
1530                               "max_features", "max_leaf_nodes",
1531                               "min_impurity_decrease", "min_impurity_split",
1532                               "random_state"),
1533             bootstrap=bootstrap,
1534             oob_score=oob_score,
1535             n_jobs=n_jobs,
1536             random_state=random_state,
1537             verbose=verbose,
1538             warm_start=warm_start,
1539             class_weight=class_weight)
1540 
1541         self.criterion = criterion
1542         self.max_depth = max_depth
1543         self.min_samples_split = min_samples_split
1544         self.min_samples_leaf = min_samples_leaf
1545         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1546         self.max_features = max_features
1547         self.max_leaf_nodes = max_leaf_nodes
1548         self.min_impurity_decrease = min_impurity_decrease
1549         self.min_impurity_split = min_impurity_split
1550 
1551 
1552 class ExtraTreesRegressor(ForestRegressor):
1553     """An extra-trees regressor.
1554 
1555     This class implements a meta estimator that fits a number of
1556     randomized decision trees (a.k.a. extra-trees) on various sub-samples
1557     of the dataset and uses averaging to improve the predictive accuracy
1558     and control over-fitting.
1559 
1560     Read more in the :ref:`User Guide <forest>`.
1561 
1562     Parameters
1563     ----------
1564     n_estimators : integer, optional (default=10)
1565         The number of trees in the forest.
1566 
1567         .. versionchanged:: 0.20
1568            The default value of ``n_estimators`` will change from 10 in
1569            version 0.20 to 100 in version 0.22.
1570 
1571     criterion : string, optional (default="mse")
1572         The function to measure the quality of a split. Supported criteria
1573         are "mse" for the mean squared error, which is equal to variance
1574         reduction as feature selection criterion, and "mae" for the mean
1575         absolute error.
1576 
1577         .. versionadded:: 0.18
1578            Mean Absolute Error (MAE) criterion.
1579 
1580     max_depth : integer or None, optional (default=None)
1581         The maximum depth of the tree. If None, then nodes are expanded until
1582         all leaves are pure or until all leaves contain less than
1583         min_samples_split samples.
1584 
1585     min_samples_split : int, float, optional (default=2)
1586         The minimum number of samples required to split an internal node:
1587 
1588         - If int, then consider `min_samples_split` as the minimum number.
1589         - If float, then `min_samples_split` is a fraction and
1590           `ceil(min_samples_split * n_samples)` are the minimum
1591           number of samples for each split.
1592 
1593         .. versionchanged:: 0.18
1594            Added float values for fractions.
1595 
1596     min_samples_leaf : int, float, optional (default=1)
1597         The minimum number of samples required to be at a leaf node.
1598         A split point at any depth will only be considered if it leaves at
1599         least ``min_samples_leaf`` training samples in each of the left and
1600         right branches.  This may have the effect of smoothing the model,
1601         especially in regression.
1602 
1603         - If int, then consider `min_samples_leaf` as the minimum number.
1604         - If float, then `min_samples_leaf` is a fraction and
1605           `ceil(min_samples_leaf * n_samples)` are the minimum
1606           number of samples for each node.
1607 
1608         .. versionchanged:: 0.18
1609            Added float values for fractions.
1610 
1611     min_weight_fraction_leaf : float, optional (default=0.)
1612         The minimum weighted fraction of the sum total of weights (of all
1613         the input samples) required to be at a leaf node. Samples have
1614         equal weight when sample_weight is not provided.
1615 
1616     max_features : int, float, string or None, optional (default="auto")
1617         The number of features to consider when looking for the best split:
1618 
1619         - If int, then consider `max_features` features at each split.
1620         - If float, then `max_features` is a fraction and
1621           `int(max_features * n_features)` features are considered at each
1622           split.
1623         - If "auto", then `max_features=n_features`.
1624         - If "sqrt", then `max_features=sqrt(n_features)`.
1625         - If "log2", then `max_features=log2(n_features)`.
1626         - If None, then `max_features=n_features`.
1627 
1628         Note: the search for a split does not stop until at least one
1629         valid partition of the node samples is found, even if it requires to
1630         effectively inspect more than ``max_features`` features.
1631 
1632     max_leaf_nodes : int or None, optional (default=None)
1633         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1634         Best nodes are defined as relative reduction in impurity.
1635         If None then unlimited number of leaf nodes.
1636 
1637     min_impurity_decrease : float, optional (default=0.)
1638         A node will be split if this split induces a decrease of the impurity
1639         greater than or equal to this value.
1640 
1641         The weighted impurity decrease equation is the following::
1642 
1643             N_t / N * (impurity - N_t_R / N_t * right_impurity
1644                                 - N_t_L / N_t * left_impurity)
1645 
1646         where ``N`` is the total number of samples, ``N_t`` is the number of
1647         samples at the current node, ``N_t_L`` is the number of samples in the
1648         left child, and ``N_t_R`` is the number of samples in the right child.
1649 
1650         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1651         if ``sample_weight`` is passed.
1652 
1653         .. versionadded:: 0.19
1654 
1655     min_impurity_split : float, (default=1e-7)
1656         Threshold for early stopping in tree growth. A node will split
1657         if its impurity is above the threshold, otherwise it is a leaf.
1658 
1659         .. deprecated:: 0.19
1660            ``min_impurity_split`` has been deprecated in favor of
1661            ``min_impurity_decrease`` in 0.19. The default value of
1662            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1663            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1664 
1665     bootstrap : boolean, optional (default=False)
1666         Whether bootstrap samples are used when building trees. If False, the
1667         whole datset is used to build each tree.
1668 
1669     oob_score : bool, optional (default=False)
1670         Whether to use out-of-bag samples to estimate the R^2 on unseen data.
1671 
1672     n_jobs : int or None, optional (default=None)
1673         The number of jobs to run in parallel for both `fit` and `predict`.
1674         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1675         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1676         for more details.
1677 
1678     random_state : int, RandomState instance or None, optional (default=None)
1679         If int, random_state is the seed used by the random number generator;
1680         If RandomState instance, random_state is the random number generator;
1681         If None, the random number generator is the RandomState instance used
1682         by `np.random`.
1683 
1684     verbose : int, optional (default=0)
1685         Controls the verbosity when fitting and predicting.
1686 
1687     warm_start : bool, optional (default=False)
1688         When set to ``True``, reuse the solution of the previous call to fit
1689         and add more estimators to the ensemble, otherwise, just fit a whole
1690         new forest. See :term:`the Glossary <warm_start>`.
1691 
1692     Attributes
1693     ----------
1694     estimators_ : list of DecisionTreeRegressor
1695         The collection of fitted sub-estimators.
1696 
1697     feature_importances_ : array of shape = [n_features]
1698         The feature importances (the higher, the more important the feature).
1699 
1700     n_features_ : int
1701         The number of features.
1702 
1703     n_outputs_ : int
1704         The number of outputs.
1705 
1706     oob_score_ : float
1707         Score of the training dataset obtained using an out-of-bag estimate.
1708 
1709     oob_prediction_ : array of shape = [n_samples]
1710         Prediction computed with out-of-bag estimate on the training set.
1711 
1712     Notes
1713     -----
1714     The default values for the parameters controlling the size of the trees
1715     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1716     unpruned trees which can potentially be very large on some data sets. To
1717     reduce memory consumption, the complexity and size of the trees should be
1718     controlled by setting those parameter values.
1719 
1720     References
1721     ----------
1722 
1723     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
1724            Machine Learning, 63(1), 3-42, 2006.
1725 
1726     See also
1727     --------
1728     sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.
1729     RandomForestRegressor: Ensemble regressor using trees with optimal splits.
1730     """
1731     def __init__(self,
1732                  n_estimators='warn',
1733                  criterion="mse",
1734                  max_depth=None,
1735                  min_samples_split=2,
1736                  min_samples_leaf=1,
1737                  min_weight_fraction_leaf=0.,
1738                  max_features="auto",
1739                  max_leaf_nodes=None,
1740                  min_impurity_decrease=0.,
1741                  min_impurity_split=None,
1742                  bootstrap=False,
1743                  oob_score=False,
1744                  n_jobs=None,
1745                  random_state=None,
1746                  verbose=0,
1747                  warm_start=False):
1748         super().__init__(
1749             base_estimator=ExtraTreeRegressor(),
1750             n_estimators=n_estimators,
1751             estimator_params=("criterion", "max_depth", "min_samples_split",
1752                               "min_samples_leaf", "min_weight_fraction_leaf",
1753                               "max_features", "max_leaf_nodes",
1754                               "min_impurity_decrease", "min_impurity_split",
1755                               "random_state"),
1756             bootstrap=bootstrap,
1757             oob_score=oob_score,
1758             n_jobs=n_jobs,
1759             random_state=random_state,
1760             verbose=verbose,
1761             warm_start=warm_start)
1762 
1763         self.criterion = criterion
1764         self.max_depth = max_depth
1765         self.min_samples_split = min_samples_split
1766         self.min_samples_leaf = min_samples_leaf
1767         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1768         self.max_features = max_features
1769         self.max_leaf_nodes = max_leaf_nodes
1770         self.min_impurity_decrease = min_impurity_decrease
1771         self.min_impurity_split = min_impurity_split
1772 
1773 
1774 class RandomTreesEmbedding(BaseForest):
1775     """An ensemble of totally random trees.
1776 
1777     An unsupervised transformation of a dataset to a high-dimensional
1778     sparse representation. A datapoint is coded according to which leaf of
1779     each tree it is sorted into. Using a one-hot encoding of the leaves,
1780     this leads to a binary coding with as many ones as there are trees in
1781     the forest.
1782 
1783     The dimensionality of the resulting representation is
1784     ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,
1785     the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.
1786 
1787     Read more in the :ref:`User Guide <random_trees_embedding>`.
1788 
1789     Parameters
1790     ----------
1791     n_estimators : integer, optional (default=10)
1792         Number of trees in the forest.
1793 
1794         .. versionchanged:: 0.20
1795            The default value of ``n_estimators`` will change from 10 in
1796            version 0.20 to 100 in version 0.22.
1797 
1798     max_depth : integer, optional (default=5)
1799         The maximum depth of each tree. If None, then nodes are expanded until
1800         all leaves are pure or until all leaves contain less than
1801         min_samples_split samples.
1802 
1803     min_samples_split : int, float, optional (default=2)
1804         The minimum number of samples required to split an internal node:
1805 
1806         - If int, then consider `min_samples_split` as the minimum number.
1807         - If float, then `min_samples_split` is a fraction and
1808           `ceil(min_samples_split * n_samples)` is the minimum
1809           number of samples for each split.
1810 
1811         .. versionchanged:: 0.18
1812            Added float values for fractions.
1813 
1814     min_samples_leaf : int, float, optional (default=1)
1815         The minimum number of samples required to be at a leaf node.
1816         A split point at any depth will only be considered if it leaves at
1817         least ``min_samples_leaf`` training samples in each of the left and
1818         right branches.  This may have the effect of smoothing the model,
1819         especially in regression.
1820 
1821         - If int, then consider `min_samples_leaf` as the minimum number.
1822         - If float, then `min_samples_leaf` is a fraction and
1823           `ceil(min_samples_leaf * n_samples)` is the minimum
1824           number of samples for each node.
1825 
1826         .. versionchanged:: 0.18
1827            Added float values for fractions.
1828 
1829     min_weight_fraction_leaf : float, optional (default=0.)
1830         The minimum weighted fraction of the sum total of weights (of all
1831         the input samples) required to be at a leaf node. Samples have
1832         equal weight when sample_weight is not provided.
1833 
1834     max_leaf_nodes : int or None, optional (default=None)
1835         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1836         Best nodes are defined as relative reduction in impurity.
1837         If None then unlimited number of leaf nodes.
1838 
1839     min_impurity_decrease : float, optional (default=0.)
1840         A node will be split if this split induces a decrease of the impurity
1841         greater than or equal to this value.
1842 
1843         The weighted impurity decrease equation is the following::
1844 
1845             N_t / N * (impurity - N_t_R / N_t * right_impurity
1846                                 - N_t_L / N_t * left_impurity)
1847 
1848         where ``N`` is the total number of samples, ``N_t`` is the number of
1849         samples at the current node, ``N_t_L`` is the number of samples in the
1850         left child, and ``N_t_R`` is the number of samples in the right child.
1851 
1852         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1853         if ``sample_weight`` is passed.
1854 
1855         .. versionadded:: 0.19
1856 
1857     min_impurity_split : float, (default=1e-7)
1858         Threshold for early stopping in tree growth. A node will split
1859         if its impurity is above the threshold, otherwise it is a leaf.
1860 
1861         .. deprecated:: 0.19
1862            ``min_impurity_split`` has been deprecated in favor of
1863            ``min_impurity_decrease`` in 0.19. The default value of
1864            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1865            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1866 
1867     sparse_output : bool, optional (default=True)
1868         Whether or not to return a sparse CSR matrix, as default behavior,
1869         or to return a dense array compatible with dense pipeline operators.
1870 
1871     n_jobs : int or None, optional (default=None)
1872         The number of jobs to run in parallel for both `fit` and `predict`.
1873         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1874         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1875         for more details.
1876 
1877     random_state : int, RandomState instance or None, optional (default=None)
1878         If int, random_state is the seed used by the random number generator;
1879         If RandomState instance, random_state is the random number generator;
1880         If None, the random number generator is the RandomState instance used
1881         by `np.random`.
1882 
1883     verbose : int, optional (default=0)
1884         Controls the verbosity when fitting and predicting.
1885 
1886     warm_start : bool, optional (default=False)
1887         When set to ``True``, reuse the solution of the previous call to fit
1888         and add more estimators to the ensemble, otherwise, just fit a whole
1889         new forest. See :term:`the Glossary <warm_start>`.
1890 
1891     Attributes
1892     ----------
1893     estimators_ : list of DecisionTreeClassifier
1894         The collection of fitted sub-estimators.
1895 
1896     References
1897     ----------
1898     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
1899            Machine Learning, 63(1), 3-42, 2006.
1900     .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  "Fast discriminative
1901            visual codebooks using randomized clustering forests"
1902            NIPS 2007
1903 
1904     """
1905 
1906     criterion = 'mse'
1907     max_features = 1
1908 
1909     def __init__(self,
1910                  n_estimators='warn',
1911                  max_depth=5,
1912                  min_samples_split=2,
1913                  min_samples_leaf=1,
1914                  min_weight_fraction_leaf=0.,
1915                  max_leaf_nodes=None,
1916                  min_impurity_decrease=0.,
1917                  min_impurity_split=None,
1918                  sparse_output=True,
1919                  n_jobs=None,
1920                  random_state=None,
1921                  verbose=0,
1922                  warm_start=False):
1923         super().__init__(
1924             base_estimator=ExtraTreeRegressor(),
1925             n_estimators=n_estimators,
1926             estimator_params=("criterion", "max_depth", "min_samples_split",
1927                               "min_samples_leaf", "min_weight_fraction_leaf",
1928                               "max_features", "max_leaf_nodes",
1929                               "min_impurity_decrease", "min_impurity_split",
1930                               "random_state"),
1931             bootstrap=False,
1932             oob_score=False,
1933             n_jobs=n_jobs,
1934             random_state=random_state,
1935             verbose=verbose,
1936             warm_start=warm_start)
1937 
1938         self.max_depth = max_depth
1939         self.min_samples_split = min_samples_split
1940         self.min_samples_leaf = min_samples_leaf
1941         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1942         self.max_leaf_nodes = max_leaf_nodes
1943         self.min_impurity_decrease = min_impurity_decrease
1944         self.min_impurity_split = min_impurity_split
1945         self.sparse_output = sparse_output
1946 
1947     def _set_oob_score(self, X, y):
1948         raise NotImplementedError("OOB score not supported by tree embedding")
1949 
1950     def fit(self, X, y=None, sample_weight=None):
1951         """Fit estimator.
1952 
1953         Parameters
1954         ----------
1955         X : array-like or sparse matrix, shape=(n_samples, n_features)
1956             The input samples. Use ``dtype=np.float32`` for maximum
1957             efficiency. Sparse matrices are also supported, use sparse
1958             ``csc_matrix`` for maximum efficiency.
1959 
1960         sample_weight : array-like, shape = [n_samples] or None
1961             Sample weights. If None, then samples are equally weighted. Splits
1962             that would create child nodes with net zero or negative weight are
1963             ignored while searching for a split in each node. In the case of
1964             classification, splits are also ignored if they would result in any
1965             single class carrying a negative weight in either child node.
1966 
1967         Returns
1968         -------
1969         self : object
1970 
1971         """
1972         self.fit_transform(X, y, sample_weight=sample_weight)
1973         return self
1974 
1975     def fit_transform(self, X, y=None, sample_weight=None):
1976         """Fit estimator and transform dataset.
1977 
1978         Parameters
1979         ----------
1980         X : array-like or sparse matrix, shape=(n_samples, n_features)
1981             Input data used to build forests. Use ``dtype=np.float32`` for
1982             maximum efficiency.
1983 
1984         sample_weight : array-like, shape = [n_samples] or None
1985             Sample weights. If None, then samples are equally weighted. Splits
1986             that would create child nodes with net zero or negative weight are
1987             ignored while searching for a split in each node. In the case of
1988             classification, splits are also ignored if they would result in any
1989             single class carrying a negative weight in either child node.
1990 
1991         Returns
1992         -------
1993         X_transformed : sparse matrix, shape=(n_samples, n_out)
1994             Transformed dataset.
1995         """
1996         X = check_array(X, accept_sparse=['csc'])
1997         if issparse(X):
1998             # Pre-sort indices to avoid that each individual tree of the
1999             # ensemble sorts the indices.
2000             X.sort_indices()
2001 
2002         rnd = check_random_state(self.random_state)
2003         y = rnd.uniform(size=X.shape[0])
2004         super().fit(X, y, sample_weight=sample_weight)
2005 
2006         self.one_hot_encoder_ = OneHotEncoder(sparse=self.sparse_output,
2007                                               categories='auto')
2008         return self.one_hot_encoder_.fit_transform(self.apply(X))
2009 
2010     def transform(self, X):
2011         """Transform dataset.
2012 
2013         Parameters
2014         ----------
2015         X : array-like or sparse matrix, shape=(n_samples, n_features)
2016             Input data to be transformed. Use ``dtype=np.float32`` for maximum
2017             efficiency. Sparse matrices are also supported, use sparse
2018             ``csr_matrix`` for maximum efficiency.
2019 
2020         Returns
2021         -------
2022         X_transformed : sparse matrix, shape=(n_samples, n_out)
2023             Transformed dataset.
2024         """
2025         check_is_fitted(self, 'one_hot_encoder_')
2026         return self.one_hot_encoder_.transform(self.apply(X))

</code><intended_behavior>While this file handles Random Forests and similar ensemble methods, the validation approach extracted here might serve as a blueprint for modifications in weight_boosting.py. Ensure that any validation logic within this file allows for the flexible handling of input data types based on the capabilities of the constituent base estimators. It's about enabling a broader range of inputs without compromising on data integrity checks necessary for the algorithm to function correctly.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Gradient Boosted Regression Trees
2 
3 This module contains methods for fitting gradient boosted regression trees for
4 both classification and regression.
5 
6 The module structure is the following:
7 
8 - The ``BaseGradientBoosting`` base class implements a common ``fit`` method
9   for all the estimators in the module. Regression and classification
10   only differ in the concrete ``LossFunction`` used.
11 
12 - ``GradientBoostingClassifier`` implements gradient boosting for
13   classification problems.
14 
15 - ``GradientBoostingRegressor`` implements gradient boosting for
16   regression problems.
17 """
18 
19 # Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,
20 #          Arnaud Joly, Jacob Schreiber
21 # License: BSD 3 clause
22 
23 from abc import ABCMeta
24 from abc import abstractmethod
25 
26 from .base import BaseEnsemble
27 from ..base import ClassifierMixin
28 from ..base import RegressorMixin
29 
30 from ._gradient_boosting import predict_stages
31 from ._gradient_boosting import predict_stage
32 from ._gradient_boosting import _random_sample_mask
33 
34 import numbers
35 import numpy as np
36 
37 from scipy.sparse import csc_matrix
38 from scipy.sparse import csr_matrix
39 from scipy.sparse import issparse
40 from scipy.special import expit
41 
42 from time import time
43 from ..model_selection import train_test_split
44 from ..tree.tree import DecisionTreeRegressor
45 from ..tree._tree import DTYPE
46 from ..tree._tree import TREE_LEAF
47 
48 from ..utils import check_random_state
49 from ..utils import check_array
50 from ..utils import check_X_y
51 from ..utils import column_or_1d
52 from ..utils import check_consistent_length
53 from ..utils import deprecated
54 from ..utils.fixes import logsumexp
55 from ..utils.stats import _weighted_percentile
56 from ..utils.validation import check_is_fitted
57 from ..utils.multiclass import check_classification_targets
58 from ..exceptions import NotFittedError
59 
60 
61 class QuantileEstimator:
62     """An estimator predicting the alpha-quantile of the training targets.
63 
64     Parameters
65     ----------
66     alpha : float
67         The quantile
68     """
69     def __init__(self, alpha=0.9):
70         if not 0 < alpha < 1.0:
71             raise ValueError("`alpha` must be in (0, 1.0) but was %r" % alpha)
72         self.alpha = alpha
73 
74     def fit(self, X, y, sample_weight=None):
75         """Fit the estimator.
76 
77         Parameters
78         ----------
79         X : {array-like, sparse matrix}, shape (n_samples, n_features)
80             Training data
81 
82         y : array, shape (n_samples, n_targets)
83             Target values. Will be cast to X's dtype if necessary
84 
85         sample_weight : numpy array of shape (n_samples,)
86             Individual weights for each sample
87         """
88         if sample_weight is None:
89             self.quantile = np.percentile(y, self.alpha * 100.0)
90         else:
91             self.quantile = _weighted_percentile(y, sample_weight,
92                                                  self.alpha * 100.0)
93 
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y
112 
113 
114 class MeanEstimator:
115     """An estimator predicting the mean of the training targets."""
116     def fit(self, X, y, sample_weight=None):
117         """Fit the estimator.
118 
119         Parameters
120         ----------
121         X : {array-like, sparse matrix}, shape (n_samples, n_features)
122             Training data
123 
124         y : array, shape (n_samples, n_targets)
125             Target values. Will be cast to X's dtype if necessary
126 
127         sample_weight : numpy array of shape (n_samples,)
128             Individual weights for each sample
129         """
130         if sample_weight is None:
131             self.mean = np.mean(y)
132         else:
133             self.mean = np.average(y, weights=sample_weight)
134 
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y
153 
154 
155 class LogOddsEstimator:
156     """An estimator predicting the log odds ratio."""
157     scale = 1.0
158 
159     def fit(self, X, y, sample_weight=None):
160         """Fit the estimator.
161 
162         Parameters
163         ----------
164         X : {array-like, sparse matrix}, shape (n_samples, n_features)
165             Training data
166 
167         y : array, shape (n_samples, n_targets)
168             Target values. Will be cast to X's dtype if necessary
169 
170         sample_weight : numpy array of shape (n_samples,)
171             Individual weights for each sample
172         """
173         # pre-cond: pos, neg are encoded as 1, 0
174         if sample_weight is None:
175             pos = np.sum(y)
176             neg = y.shape[0] - pos
177         else:
178             pos = np.sum(sample_weight * y)
179             neg = np.sum(sample_weight * (1 - y))
180 
181         if neg == 0 or pos == 0:
182             raise ValueError('y contains non binary labels.')
183         self.prior = self.scale * np.log(pos / neg)
184 
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y
203 
204 
205 class ScaledLogOddsEstimator(LogOddsEstimator):
206     """Log odds ratio scaled by 0.5 -- for exponential loss. """
207     scale = 0.5
208 
209 
210 class PriorProbabilityEstimator:
211     """An estimator predicting the probability of each
212     class in the training data.
213     """
214     def fit(self, X, y, sample_weight=None):
215         """Fit the estimator.
216 
217         Parameters
218         ----------
219         X : {array-like, sparse matrix}, shape (n_samples, n_features)
220             Training data
221 
222         y : array, shape (n_samples, n_targets)
223             Target values. Will be cast to X's dtype if necessary
224 
225         sample_weight : array, shape (n_samples,)
226             Individual weights for each sample
227         """
228         if sample_weight is None:
229             sample_weight = np.ones_like(y, dtype=np.float64)
230         class_counts = np.bincount(y, weights=sample_weight)
231         self.priors = class_counts / class_counts.sum()
232 
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y
251 
252 
253 class ZeroEstimator:
254     """An estimator that simply predicts zero. """
255 
256     def fit(self, X, y, sample_weight=None):
257         """Fit the estimator.
258 
259         Parameters
260         ----------
261         X : {array-like, sparse matrix}, shape (n_samples, n_features)
262             Training data
263 
264         y : numpy, shape (n_samples, n_targets)
265             Target values. Will be cast to X's dtype if necessary
266 
267         sample_weight : array, shape (n_samples,)
268             Individual weights for each sample
269         """
270         if np.issubdtype(y.dtype, np.signedinteger):
271             # classification
272             self.n_classes = np.unique(y).shape[0]
273             if self.n_classes == 2:
274                 self.n_classes = 1
275         else:
276             # regression
277             self.n_classes = 1
278 
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y
297 
298 
299 class LossFunction(metaclass=ABCMeta):
300     """Abstract base class for various loss functions.
301 
302     Parameters
303     ----------
304     n_classes : int
305         Number of classes
306 
307     Attributes
308     ----------
309     K : int
310         The number of regression trees to be induced;
311         1 for regression and binary classification;
312         ``n_classes`` for multi-class classification.
313     """
314 
315     is_multi_class = False
316 
317     def __init__(self, n_classes):
318         self.K = n_classes
319 
320     def init_estimator(self):
321         """Default ``init`` estimator for loss function. """
322         raise NotImplementedError()
323 
324     @abstractmethod
325     def __call__(self, y, pred, sample_weight=None):
326         """Compute the loss.
327 
328         Parameters
329         ----------
330         y : array, shape (n_samples,)
331             True labels
332 
333         pred : array, shape (n_samples,)
334             Predicted labels
335 
336         sample_weight : array-like, shape (n_samples,), optional
337             Sample weights.
338         """
339 
340     @abstractmethod
341     def negative_gradient(self, y, y_pred, **kargs):
342         """Compute the negative gradient.
343 
344         Parameters
345         ----------
346         y : array, shape (n_samples,)
347             The target labels.
348 
349         y_pred : array, shape (n_samples,)
350             The predictions.
351         """
352 
353     def update_terminal_regions(self, tree, X, y, residual, y_pred,
354                                 sample_weight, sample_mask,
355                                 learning_rate=0.1, k=0):
356         """Update the terminal regions (=leaves) of the given tree and
357         updates the current predictions of the model. Traverses tree
358         and invokes template method `_update_terminal_region`.
359 
360         Parameters
361         ----------
362         tree : tree.Tree
363             The tree object.
364         X : array, shape (n, m)
365             The data array.
366         y : array, shape (n,)
367             The target labels.
368         residual : array, shape (n,)
369             The residuals (usually the negative gradient).
370         y_pred : array, shape (n,)
371             The predictions.
372         sample_weight : array, shape (n,)
373             The weight of each sample.
374         sample_mask : array, shape (n,)
375             The sample mask to be used.
376         learning_rate : float, default=0.1
377             learning rate shrinks the contribution of each tree by
378              ``learning_rate``.
379         k : int, default 0
380             The index of the estimator being updated.
381 
382         """
383         # compute leaf for each sample in ``X``.
384         terminal_regions = tree.apply(X)
385 
386         # mask all which are not in sample mask.
387         masked_terminal_regions = terminal_regions.copy()
388         masked_terminal_regions[~sample_mask] = -1
389 
390         # update each leaf (= perform line search)
391         for leaf in np.where(tree.children_left == TREE_LEAF)[0]:
392             self._update_terminal_region(tree, masked_terminal_regions,
393                                          leaf, X, y, residual,
394                                          y_pred[:, k], sample_weight)
395 
396         # update predictions (both in-bag and out-of-bag)
397         y_pred[:, k] += (learning_rate
398                          * tree.value[:, 0, 0].take(terminal_regions, axis=0))
399 
400     @abstractmethod
401     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
402                                 residual, pred, sample_weight):
403         """Template method for updating terminal regions (=leaves). """
404 
405 
406 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
407     """Base class for regression loss functions.
408 
409     Parameters
410     ----------
411     n_classes : int
412         Number of classes
413     """
414     def __init__(self, n_classes):
415         if n_classes != 1:
416             raise ValueError("``n_classes`` must be 1 for regression but "
417                              "was %r" % n_classes)
418         super().__init__(n_classes)
419 
420 
421 class LeastSquaresError(RegressionLossFunction):
422     """Loss function for least squares (LS) estimation.
423     Terminal regions need not to be updated for least squares.
424 
425     Parameters
426     ----------
427     n_classes : int
428         Number of classes
429     """
430 
431     def init_estimator(self):
432         return MeanEstimator()
433 
434     def __call__(self, y, pred, sample_weight=None):
435         """Compute the least squares loss.
436 
437         Parameters
438         ----------
439         y : array, shape (n_samples,)
440             True labels
441 
442         pred : array, shape (n_samples,)
443             Predicted labels
444 
445         sample_weight : array-like, shape (n_samples,), optional
446             Sample weights.
447         """
448         if sample_weight is None:
449             return np.mean((y - pred.ravel()) ** 2.0)
450         else:
451             return (1.0 / sample_weight.sum() *
452                     np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))
453 
454     def negative_gradient(self, y, pred, **kargs):
455         """Compute the negative gradient.
456 
457         Parameters
458         ----------
459         y : array, shape (n_samples,)
460             The target labels.
461 
462         pred : array, shape (n_samples,)
463             The predictions.
464         """
465         return y - pred.ravel()
466 
467     def update_terminal_regions(self, tree, X, y, residual, y_pred,
468                                 sample_weight, sample_mask,
469                                 learning_rate=0.1, k=0):
470         """Least squares does not need to update terminal regions.
471 
472         But it has to update the predictions.
473 
474         Parameters
475         ----------
476         tree : tree.Tree
477             The tree object.
478         X : array, shape (n, m)
479             The data array.
480         y : array, shape (n,)
481             The target labels.
482         residual : array, shape (n,)
483             The residuals (usually the negative gradient).
484         y_pred : array, shape (n,)
485             The predictions.
486         sample_weight : array, shape (n,)
487             The weight of each sample.
488         sample_mask : array, shape (n,)
489             The sample mask to be used.
490         learning_rate : float, default=0.1
491             learning rate shrinks the contribution of each tree by
492              ``learning_rate``.
493         k : int, default 0
494             The index of the estimator being updated.
495         """
496         # update predictions
497         y_pred[:, k] += learning_rate * tree.predict(X).ravel()
498 
499     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
500                                 residual, pred, sample_weight):
501         pass
502 
503 
504 class LeastAbsoluteError(RegressionLossFunction):
505     """Loss function for least absolute deviation (LAD) regression.
506 
507     Parameters
508     ----------
509     n_classes : int
510         Number of classes
511     """
512     def init_estimator(self):
513         return QuantileEstimator(alpha=0.5)
514 
515     def __call__(self, y, pred, sample_weight=None):
516         """Compute the least absolute error.
517 
518         Parameters
519         ----------
520         y : array, shape (n_samples,)
521             True labels
522 
523         pred : array, shape (n_samples,)
524             Predicted labels
525 
526         sample_weight : array-like, shape (n_samples,), optional
527             Sample weights.
528         """
529         if sample_weight is None:
530             return np.abs(y - pred.ravel()).mean()
531         else:
532             return (1.0 / sample_weight.sum() *
533                     np.sum(sample_weight * np.abs(y - pred.ravel())))
534 
535     def negative_gradient(self, y, pred, **kargs):
536         """Compute the negative gradient.
537 
538         1.0 if y - pred > 0.0 else -1.0
539 
540         Parameters
541         ----------
542         y : array, shape (n_samples,)
543             The target labels.
544 
545         pred : array, shape (n_samples,)
546             The predictions.
547         """
548         pred = pred.ravel()
549         return 2.0 * (y - pred > 0.0) - 1.0
550 
551     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
552                                 residual, pred, sample_weight):
553         """LAD updates terminal regions to median estimates. """
554         terminal_region = np.where(terminal_regions == leaf)[0]
555         sample_weight = sample_weight.take(terminal_region, axis=0)
556         diff = y.take(terminal_region, axis=0) - pred.take(terminal_region, axis=0)
557         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)
558 
559 
560 class HuberLossFunction(RegressionLossFunction):
561     """Huber loss function for robust regression.
562 
563     M-Regression proposed in Friedman 2001.
564 
565     References
566     ----------
567     J. Friedman, Greedy Function Approximation: A Gradient Boosting
568     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
569 
570     Parameters
571     ----------
572     n_classes : int
573         Number of classes
574 
575     alpha : float
576         Percentile at which to extract score
577     """
578 
579     def __init__(self, n_classes, alpha=0.9):
580         super().__init__(n_classes)
581         self.alpha = alpha
582         self.gamma = None
583 
584     def init_estimator(self):
585         return QuantileEstimator(alpha=0.5)
586 
587     def __call__(self, y, pred, sample_weight=None):
588         """Compute the Huber loss.
589 
590         Parameters
591         ----------
592         y : array, shape (n_samples,)
593             True labels
594 
595         pred : array, shape (n_samples,)
596             Predicted labels
597 
598         sample_weight : array-like, shape (n_samples,), optional
599             Sample weights.
600         """
601         pred = pred.ravel()
602         diff = y - pred
603         gamma = self.gamma
604         if gamma is None:
605             if sample_weight is None:
606                 gamma = np.percentile(np.abs(diff), self.alpha * 100)
607             else:
608                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
609 
610         gamma_mask = np.abs(diff) <= gamma
611         if sample_weight is None:
612             sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2.0)
613             lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) - gamma / 2.0))
614             loss = (sq_loss + lin_loss) / y.shape[0]
615         else:
616             sq_loss = np.sum(0.5 * sample_weight[gamma_mask] * diff[gamma_mask] ** 2.0)
617             lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *
618                               (np.abs(diff[~gamma_mask]) - gamma / 2.0))
619             loss = (sq_loss + lin_loss) / sample_weight.sum()
620         return loss
621 
622     def negative_gradient(self, y, pred, sample_weight=None, **kargs):
623         """Compute the negative gradient.
624 
625         Parameters
626         ----------
627         y : array, shape (n_samples,)
628             The target labels.
629 
630         pred : array, shape (n_samples,)
631             The predictions.
632 
633         sample_weight : array-like, shape (n_samples,), optional
634             Sample weights.
635         """
636         pred = pred.ravel()
637         diff = y - pred
638         if sample_weight is None:
639             gamma = np.percentile(np.abs(diff), self.alpha * 100)
640         else:
641             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
642         gamma_mask = np.abs(diff) <= gamma
643         residual = np.zeros((y.shape[0],), dtype=np.float64)
644         residual[gamma_mask] = diff[gamma_mask]
645         residual[~gamma_mask] = gamma * np.sign(diff[~gamma_mask])
646         self.gamma = gamma
647         return residual
648 
649     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
650                                 residual, pred, sample_weight):
651         terminal_region = np.where(terminal_regions == leaf)[0]
652         sample_weight = sample_weight.take(terminal_region, axis=0)
653         gamma = self.gamma
654         diff = (y.take(terminal_region, axis=0)
655                 - pred.take(terminal_region, axis=0))
656         median = _weighted_percentile(diff, sample_weight, percentile=50)
657         diff_minus_median = diff - median
658         tree.value[leaf, 0] = median + np.mean(
659             np.sign(diff_minus_median) *
660             np.minimum(np.abs(diff_minus_median), gamma))
661 
662 
663 class QuantileLossFunction(RegressionLossFunction):
664     """Loss function for quantile regression.
665 
666     Quantile regression allows to estimate the percentiles
667     of the conditional distribution of the target.
668 
669     Parameters
670     ----------
671     n_classes : int
672         Number of classes.
673 
674     alpha : float, optional (default = 0.9)
675         The percentile
676     """
677     def __init__(self, n_classes, alpha=0.9):
678         super().__init__(n_classes)
679         self.alpha = alpha
680         self.percentile = alpha * 100.0
681 
682     def init_estimator(self):
683         return QuantileEstimator(self.alpha)
684 
685     def __call__(self, y, pred, sample_weight=None):
686         """Compute the Quantile loss.
687 
688         Parameters
689         ----------
690         y : array, shape (n_samples,)
691             True labels
692 
693         pred : array, shape (n_samples,)
694             Predicted labels
695 
696         sample_weight : array-like, shape (n_samples,), optional
697             Sample weights.
698         """
699         pred = pred.ravel()
700         diff = y - pred
701         alpha = self.alpha
702 
703         mask = y > pred
704         if sample_weight is None:
705             loss = (alpha * diff[mask].sum() -
706                     (1.0 - alpha) * diff[~mask].sum()) / y.shape[0]
707         else:
708             loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -
709                     (1.0 - alpha) * np.sum(sample_weight[~mask] * diff[~mask])) /
710                     sample_weight.sum())
711         return loss
712 
713     def negative_gradient(self, y, pred, **kargs):
714         """Compute the negative gradient.
715 
716         Parameters
717         ----------
718         y : array, shape (n_samples,)
719             The target labels.
720 
721         pred : array, shape (n_samples,)
722             The predictions.
723         """
724         alpha = self.alpha
725         pred = pred.ravel()
726         mask = y > pred
727         return (alpha * mask) - ((1.0 - alpha) * ~mask)
728 
729     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
730                                 residual, pred, sample_weight):
731         terminal_region = np.where(terminal_regions == leaf)[0]
732         diff = (y.take(terminal_region, axis=0)
733                 - pred.take(terminal_region, axis=0))
734         sample_weight = sample_weight.take(terminal_region, axis=0)
735 
736         val = _weighted_percentile(diff, sample_weight, self.percentile)
737         tree.value[leaf, 0] = val
738 
739 
740 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
741     """Base class for classification loss functions. """
742 
743     def _score_to_proba(self, score):
744         """Template method to convert scores to probabilities.
745 
746          the does not support probabilities raises AttributeError.
747         """
748         raise TypeError('%s does not support predict_proba' % type(self).__name__)
749 
750     @abstractmethod
751     def _score_to_decision(self, score):
752         """Template method to convert scores to decisions.
753 
754         Returns int arrays.
755         """
756 
757 
758 class BinomialDeviance(ClassificationLossFunction):
759     """Binomial deviance loss function for binary classification.
760 
761     Binary classification is a special case; here, we only need to
762     fit one tree instead of ``n_classes`` trees.
763 
764     Parameters
765     ----------
766     n_classes : int
767         Number of classes.
768     """
769     def __init__(self, n_classes):
770         if n_classes != 2:
771             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
772                              .format(self.__class__.__name__, n_classes))
773         # we only need to fit one tree for binary clf.
774         super().__init__(1)
775 
776     def init_estimator(self):
777         return LogOddsEstimator()
778 
779     def __call__(self, y, pred, sample_weight=None):
780         """Compute the deviance (= 2 * negative log-likelihood).
781 
782         Parameters
783         ----------
784         y : array, shape (n_samples,)
785             True labels
786 
787         pred : array, shape (n_samples,)
788             Predicted labels
789 
790         sample_weight : array-like, shape (n_samples,), optional
791             Sample weights.
792         """
793         # logaddexp(0, v) == log(1.0 + exp(v))
794         pred = pred.ravel()
795         if sample_weight is None:
796             return -2.0 * np.mean((y * pred) - np.logaddexp(0.0, pred))
797         else:
798             return (-2.0 / sample_weight.sum() *
799                     np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))
800 
801     def negative_gradient(self, y, pred, **kargs):
802         """Compute the residual (= negative gradient).
803 
804         Parameters
805         ----------
806         y : array, shape (n_samples,)
807             True labels
808 
809         pred : array, shape (n_samples,)
810             Predicted labels
811         """
812         return y - expit(pred.ravel())
813 
814     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
815                                 residual, pred, sample_weight):
816         """Make a single Newton-Raphson step.
817 
818         our node estimate is given by:
819 
820             sum(w * (y - prob)) / sum(w * prob * (1 - prob))
821 
822         we take advantage that: y - prob = residual
823         """
824         terminal_region = np.where(terminal_regions == leaf)[0]
825         residual = residual.take(terminal_region, axis=0)
826         y = y.take(terminal_region, axis=0)
827         sample_weight = sample_weight.take(terminal_region, axis=0)
828 
829         numerator = np.sum(sample_weight * residual)
830         denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))
831 
832         # prevents overflow and division by zero
833         if abs(denominator) < 1e-150:
834             tree.value[leaf, 0, 0] = 0.0
835         else:
836             tree.value[leaf, 0, 0] = numerator / denominator
837 
838     def _score_to_proba(self, score):
839         proba = np.ones((score.shape[0], 2), dtype=np.float64)
840         proba[:, 1] = expit(score.ravel())
841         proba[:, 0] -= proba[:, 1]
842         return proba
843 
844     def _score_to_decision(self, score):
845         proba = self._score_to_proba(score)
846         return np.argmax(proba, axis=1)
847 
848 
849 class MultinomialDeviance(ClassificationLossFunction):
850     """Multinomial deviance loss function for multi-class classification.
851 
852     For multi-class classification we need to fit ``n_classes`` trees at
853     each stage.
854 
855     Parameters
856     ----------
857     n_classes : int
858         Number of classes
859     """
860 
861     is_multi_class = True
862 
863     def __init__(self, n_classes):
864         if n_classes < 3:
865             raise ValueError("{0:s} requires more than 2 classes.".format(
866                 self.__class__.__name__))
867         super().__init__(n_classes)
868 
869     def init_estimator(self):
870         return PriorProbabilityEstimator()
871 
872     def __call__(self, y, pred, sample_weight=None):
873         """Compute the Multinomial deviance.
874 
875         Parameters
876         ----------
877         y : array, shape (n_samples,)
878             True labels
879 
880         pred : array, shape (n_samples,)
881             Predicted labels
882 
883         sample_weight : array-like, shape (n_samples,), optional
884             Sample weights.
885         """
886         # create one-hot label encoding
887         Y = np.zeros((y.shape[0], self.K), dtype=np.float64)
888         for k in range(self.K):
889             Y[:, k] = y == k
890 
891         if sample_weight is None:
892             return np.sum(-1 * (Y * pred).sum(axis=1) +
893                           logsumexp(pred, axis=1))
894         else:
895             return np.sum(-1 * sample_weight * (Y * pred).sum(axis=1) +
896                           logsumexp(pred, axis=1))
897 
898     def negative_gradient(self, y, pred, k=0, **kwargs):
899         """Compute negative gradient for the ``k``-th class.
900 
901         Parameters
902         ----------
903         y : array, shape (n_samples,)
904             The target labels.
905 
906         pred : array, shape (n_samples,)
907             The predictions.
908 
909         k : int, optional (default=0)
910             The index of the class
911         """
912         return y - np.nan_to_num(np.exp(pred[:, k] -
913                                         logsumexp(pred, axis=1)))
914 
915     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
916                                 residual, pred, sample_weight):
917         """Make a single Newton-Raphson step. """
918         terminal_region = np.where(terminal_regions == leaf)[0]
919         residual = residual.take(terminal_region, axis=0)
920         y = y.take(terminal_region, axis=0)
921         sample_weight = sample_weight.take(terminal_region, axis=0)
922 
923         numerator = np.sum(sample_weight * residual)
924         numerator *= (self.K - 1) / self.K
925 
926         denominator = np.sum(sample_weight * (y - residual) *
927                              (1.0 - y + residual))
928 
929         # prevents overflow and division by zero
930         if abs(denominator) < 1e-150:
931             tree.value[leaf, 0, 0] = 0.0
932         else:
933             tree.value[leaf, 0, 0] = numerator / denominator
934 
935     def _score_to_proba(self, score):
936         return np.nan_to_num(
937             np.exp(score - (logsumexp(score, axis=1)[:, np.newaxis])))
938 
939     def _score_to_decision(self, score):
940         proba = self._score_to_proba(score)
941         return np.argmax(proba, axis=1)
942 
943 
944 class ExponentialLoss(ClassificationLossFunction):
945     """Exponential loss function for binary classification.
946 
947     Same loss as AdaBoost.
948 
949     References
950     ----------
951     Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
952 
953     Parameters
954     ----------
955     n_classes : int
956         Number of classes.
957     """
958     def __init__(self, n_classes):
959         if n_classes != 2:
960             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
961                              .format(self.__class__.__name__, n_classes))
962         # we only need to fit one tree for binary clf.
963         super().__init__(1)
964 
965     def init_estimator(self):
966         return ScaledLogOddsEstimator()
967 
968     def __call__(self, y, pred, sample_weight=None):
969         """Compute the exponential loss
970 
971         Parameters
972         ----------
973         y : array, shape (n_samples,)
974             True labels
975 
976         pred : array, shape (n_samples,)
977             Predicted labels
978 
979         sample_weight : array-like, shape (n_samples,), optional
980             Sample weights.
981         """
982         pred = pred.ravel()
983         if sample_weight is None:
984             return np.mean(np.exp(-(2. * y - 1.) * pred))
985         else:
986             return (1.0 / sample_weight.sum() *
987                     np.sum(sample_weight * np.exp(-(2 * y - 1) * pred)))
988 
989     def negative_gradient(self, y, pred, **kargs):
990         """Compute the residual (= negative gradient).
991 
992         Parameters
993         ----------
994         y : array, shape (n_samples,)
995             True labels
996 
997         pred : array, shape (n_samples,)
998             Predicted labels
999         """
1000         y_ = -(2. * y - 1.)
1001         return y_ * np.exp(y_ * pred.ravel())
1002 
1003     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
1004                                 residual, pred, sample_weight):
1005         terminal_region = np.where(terminal_regions == leaf)[0]
1006         pred = pred.take(terminal_region, axis=0)
1007         y = y.take(terminal_region, axis=0)
1008         sample_weight = sample_weight.take(terminal_region, axis=0)
1009 
1010         y_ = 2. * y - 1.
1011 
1012         numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))
1013         denominator = np.sum(sample_weight * np.exp(-y_ * pred))
1014 
1015         # prevents overflow and division by zero
1016         if abs(denominator) < 1e-150:
1017             tree.value[leaf, 0, 0] = 0.0
1018         else:
1019             tree.value[leaf, 0, 0] = numerator / denominator
1020 
1021     def _score_to_proba(self, score):
1022         proba = np.ones((score.shape[0], 2), dtype=np.float64)
1023         proba[:, 1] = expit(2.0 * score.ravel())
1024         proba[:, 0] -= proba[:, 1]
1025         return proba
1026 
1027     def _score_to_decision(self, score):
1028         return (score.ravel() >= 0.0).astype(np.int)
1029 
1030 
1031 LOSS_FUNCTIONS = {'ls': LeastSquaresError,
1032                   'lad': LeastAbsoluteError,
1033                   'huber': HuberLossFunction,
1034                   'quantile': QuantileLossFunction,
1035                   'deviance': None,    # for both, multinomial and binomial
1036                   'exponential': ExponentialLoss,
1037                   }
1038 
1039 
1040 INIT_ESTIMATORS = {'zero': ZeroEstimator}
1041 
1042 
1043 class VerboseReporter:
1044     """Reports verbose output to stdout.
1045 
1046     Parameters
1047     ----------
1048     verbose : int
1049         Verbosity level. If ``verbose==1`` output is printed once in a while
1050         (when iteration mod verbose_mod is zero).; if larger than 1 then output
1051         is printed for each update.
1052     """
1053 
1054     def __init__(self, verbose):
1055         self.verbose = verbose
1056 
1057     def init(self, est, begin_at_stage=0):
1058         """Initialize reporter
1059 
1060         Parameters
1061         ----------
1062         est : Estimator
1063             The estimator
1064 
1065         begin_at_stage : int
1066             stage at which to begin reporting
1067         """
1068         # header fields and line format str
1069         header_fields = ['Iter', 'Train Loss']
1070         verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}']
1071         # do oob?
1072         if est.subsample < 1:
1073             header_fields.append('OOB Improve')
1074             verbose_fmt.append('{oob_impr:>16.4f}')
1075         header_fields.append('Remaining Time')
1076         verbose_fmt.append('{remaining_time:>16s}')
1077 
1078         # print the header line
1079         print(('%10s ' + '%16s ' *
1080                (len(header_fields) - 1)) % tuple(header_fields))
1081 
1082         self.verbose_fmt = ' '.join(verbose_fmt)
1083         # plot verbose info each time i % verbose_mod == 0
1084         self.verbose_mod = 1
1085         self.start_time = time()
1086         self.begin_at_stage = begin_at_stage
1087 
1088     def update(self, j, est):
1089         """Update reporter with new iteration.
1090 
1091         Parameters
1092         ----------
1093         j : int
1094             The new iteration
1095         est : Estimator
1096             The estimator
1097         """
1098         do_oob = est.subsample < 1
1099         # we need to take into account if we fit additional estimators.
1100         i = j - self.begin_at_stage  # iteration relative to the start iter
1101         if (i + 1) % self.verbose_mod == 0:
1102             oob_impr = est.oob_improvement_[j] if do_oob else 0
1103             remaining_time = ((est.n_estimators - (j + 1)) *
1104                               (time() - self.start_time) / float(i + 1))
1105             if remaining_time > 60:
1106                 remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)
1107             else:
1108                 remaining_time = '{0:.2f}s'.format(remaining_time)
1109             print(self.verbose_fmt.format(iter=j + 1,
1110                                           train_score=est.train_score_[j],
1111                                           oob_impr=oob_impr,
1112                                           remaining_time=remaining_time))
1113             if self.verbose == 1 and ((i + 1) // (self.verbose_mod * 10) > 0):
1114                 # adjust verbose frequency (powers of 10)
1115                 self.verbose_mod *= 10
1116 
1117 
1118 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1119     """Abstract base class for Gradient Boosting. """
1120 
1121     @abstractmethod
1122     def __init__(self, loss, learning_rate, n_estimators, criterion,
1123                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1124                  max_depth, min_impurity_decrease, min_impurity_split,
1125                  init, subsample, max_features,
1126                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1127                  warm_start=False, presort='auto',
1128                  validation_fraction=0.1, n_iter_no_change=None,
1129                  tol=1e-4):
1130 
1131         self.n_estimators = n_estimators
1132         self.learning_rate = learning_rate
1133         self.loss = loss
1134         self.criterion = criterion
1135         self.min_samples_split = min_samples_split
1136         self.min_samples_leaf = min_samples_leaf
1137         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1138         self.subsample = subsample
1139         self.max_features = max_features
1140         self.max_depth = max_depth
1141         self.min_impurity_decrease = min_impurity_decrease
1142         self.min_impurity_split = min_impurity_split
1143         self.init = init
1144         self.random_state = random_state
1145         self.alpha = alpha
1146         self.verbose = verbose
1147         self.max_leaf_nodes = max_leaf_nodes
1148         self.warm_start = warm_start
1149         self.presort = presort
1150         self.validation_fraction = validation_fraction
1151         self.n_iter_no_change = n_iter_no_change
1152         self.tol = tol
1153 
1154     def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
1155                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1156         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1157 
1158         assert sample_mask.dtype == np.bool
1159         loss = self.loss_
1160         original_y = y
1161 
1162         # Need to pass a copy of y_pred to negative_gradient() because y_pred
1163         # is partially updated at the end of the loop in
1164         # update_terminal_regions(), and gradients need to be evaluated at
1165         # iteration i - 1.
1166         y_pred_copy = y_pred.copy()
1167 
1168         for k in range(loss.K):
1169             if loss.is_multi_class:
1170                 y = np.array(original_y == k, dtype=np.float64)
1171 
1172             residual = loss.negative_gradient(y, y_pred_copy, k=k,
1173                                               sample_weight=sample_weight)
1174 
1175             # induce regression tree on residuals
1176             tree = DecisionTreeRegressor(
1177                 criterion=self.criterion,
1178                 splitter='best',
1179                 max_depth=self.max_depth,
1180                 min_samples_split=self.min_samples_split,
1181                 min_samples_leaf=self.min_samples_leaf,
1182                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1183                 min_impurity_decrease=self.min_impurity_decrease,
1184                 min_impurity_split=self.min_impurity_split,
1185                 max_features=self.max_features,
1186                 max_leaf_nodes=self.max_leaf_nodes,
1187                 random_state=random_state,
1188                 presort=self.presort)
1189 
1190             if self.subsample < 1.0:
1191                 # no inplace multiplication!
1192                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1193 
1194             X = X_csr if X_csr is not None else X
1195             tree.fit(X, residual, sample_weight=sample_weight,
1196                      check_input=False, X_idx_sorted=X_idx_sorted)
1197 
1198             # update tree leaves
1199             loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
1200                                          sample_weight, sample_mask,
1201                                          learning_rate=self.learning_rate, k=k)
1202 
1203             # add tree to ensemble
1204             self.estimators_[i, k] = tree
1205 
1206         return y_pred
1207 
1208     def _check_params(self):
1209         """Check validity of parameters and raise ValueError if not valid. """
1210         if self.n_estimators <= 0:
1211             raise ValueError("n_estimators must be greater than 0 but "
1212                              "was %r" % self.n_estimators)
1213 
1214         if self.learning_rate <= 0.0:
1215             raise ValueError("learning_rate must be greater than 0 but "
1216                              "was %r" % self.learning_rate)
1217 
1218         if (self.loss not in self._SUPPORTED_LOSS
1219                 or self.loss not in LOSS_FUNCTIONS):
1220             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1221 
1222         if self.loss == 'deviance':
1223             loss_class = (MultinomialDeviance
1224                           if len(self.classes_) > 2
1225                           else BinomialDeviance)
1226         else:
1227             loss_class = LOSS_FUNCTIONS[self.loss]
1228 
1229         if self.loss in ('huber', 'quantile'):
1230             self.loss_ = loss_class(self.n_classes_, self.alpha)
1231         else:
1232             self.loss_ = loss_class(self.n_classes_)
1233 
1234         if not (0.0 < self.subsample <= 1.0):
1235             raise ValueError("subsample must be in (0,1] but "
1236                              "was %r" % self.subsample)
1237 
1238         if self.init is not None:
1239             if isinstance(self.init, str):
1240                 if self.init not in INIT_ESTIMATORS:
1241                     raise ValueError('init="%s" is not supported' % self.init)
1242             else:
1243                 if (not hasattr(self.init, 'fit')
1244                         or not hasattr(self.init, 'predict')):
1245                     raise ValueError("init=%r must be valid BaseEstimator "
1246                                      "and support both fit and "
1247                                      "predict" % self.init)
1248 
1249         if not (0.0 < self.alpha < 1.0):
1250             raise ValueError("alpha must be in (0.0, 1.0) but "
1251                              "was %r" % self.alpha)
1252 
1253         if isinstance(self.max_features, str):
1254             if self.max_features == "auto":
1255                 # if is_classification
1256                 if self.n_classes_ > 1:
1257                     max_features = max(1, int(np.sqrt(self.n_features_)))
1258                 else:
1259                     # is regression
1260                     max_features = self.n_features_
1261             elif self.max_features == "sqrt":
1262                 max_features = max(1, int(np.sqrt(self.n_features_)))
1263             elif self.max_features == "log2":
1264                 max_features = max(1, int(np.log2(self.n_features_)))
1265             else:
1266                 raise ValueError("Invalid value for max_features: %r. "
1267                                  "Allowed string values are 'auto', 'sqrt' "
1268                                  "or 'log2'." % self.max_features)
1269         elif self.max_features is None:
1270             max_features = self.n_features_
1271         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1272             max_features = self.max_features
1273         else:  # float
1274             if 0. < self.max_features <= 1.:
1275                 max_features = max(int(self.max_features *
1276                                        self.n_features_), 1)
1277             else:
1278                 raise ValueError("max_features must be in (0, n_features]")
1279 
1280         self.max_features_ = max_features
1281 
1282         if not isinstance(self.n_iter_no_change,
1283                           (numbers.Integral, np.integer, type(None))):
1284             raise ValueError("n_iter_no_change should either be None or an "
1285                              "integer. %r was passed"
1286                              % self.n_iter_no_change)
1287 
1288         allowed_presort = ('auto', True, False)
1289         if self.presort not in allowed_presort:
1290             raise ValueError("'presort' should be in {}. Got {!r} instead."
1291                              .format(allowed_presort, self.presort))
1292 
1293     def _init_state(self):
1294         """Initialize model state and allocate model state data structures. """
1295 
1296         if self.init is None:
1297             self.init_ = self.loss_.init_estimator()
1298         elif isinstance(self.init, str):
1299             self.init_ = INIT_ESTIMATORS[self.init]()
1300         else:
1301             self.init_ = self.init
1302 
1303         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1304                                     dtype=np.object)
1305         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1306         # do oob?
1307         if self.subsample < 1.0:
1308             self.oob_improvement_ = np.zeros((self.n_estimators),
1309                                              dtype=np.float64)
1310 
1311     def _clear_state(self):
1312         """Clear the state of the gradient boosting model. """
1313         if hasattr(self, 'estimators_'):
1314             self.estimators_ = np.empty((0, 0), dtype=np.object)
1315         if hasattr(self, 'train_score_'):
1316             del self.train_score_
1317         if hasattr(self, 'oob_improvement_'):
1318             del self.oob_improvement_
1319         if hasattr(self, 'init_'):
1320             del self.init_
1321         if hasattr(self, '_rng'):
1322             del self._rng
1323 
1324     def _resize_state(self):
1325         """Add additional ``n_estimators`` entries to all attributes. """
1326         # self.n_estimators is the number of additional est to fit
1327         total_n_estimators = self.n_estimators
1328         if total_n_estimators < self.estimators_.shape[0]:
1329             raise ValueError('resize with smaller n_estimators %d < %d' %
1330                              (total_n_estimators, self.estimators_[0]))
1331 
1332         self.estimators_ = np.resize(self.estimators_,
1333                                      (total_n_estimators, self.loss_.K))
1334         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1335         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1336             # if do oob resize arrays or create new if not available
1337             if hasattr(self, 'oob_improvement_'):
1338                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1339                                                   total_n_estimators)
1340             else:
1341                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1342                                                  dtype=np.float64)
1343 
1344     def _is_initialized(self):
1345         return len(getattr(self, 'estimators_', [])) > 0
1346 
1347     def _check_initialized(self):
1348         """Check that the estimator is initialized, raising an error if not."""
1349         check_is_fitted(self, 'estimators_')
1350 
1351     @property
1352     @deprecated("Attribute n_features was deprecated in version 0.19 and "
1353                 "will be removed in 0.21.")
1354     def n_features(self):
1355         return self.n_features_
1356 
1357     def fit(self, X, y, sample_weight=None, monitor=None):
1358         """Fit the gradient boosting model.
1359 
1360         Parameters
1361         ----------
1362         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1363             The input samples. Internally, it will be converted to
1364             ``dtype=np.float32`` and if a sparse matrix is provided
1365             to a sparse ``csr_matrix``.
1366 
1367         y : array-like, shape (n_samples,)
1368             Target values (strings or integers in classification, real numbers
1369             in regression)
1370             For classification, labels must correspond to classes.
1371 
1372         sample_weight : array-like, shape (n_samples,) or None
1373             Sample weights. If None, then samples are equally weighted. Splits
1374             that would create child nodes with net zero or negative weight are
1375             ignored while searching for a split in each node. In the case of
1376             classification, splits are also ignored if they would result in any
1377             single class carrying a negative weight in either child node.
1378 
1379         monitor : callable, optional
1380             The monitor is called after each iteration with the current
1381             iteration, a reference to the estimator and the local variables of
1382             ``_fit_stages`` as keyword arguments ``callable(i, self,
1383             locals())``. If the callable returns ``True`` the fitting procedure
1384             is stopped. The monitor can be used for various things such as
1385             computing held-out estimates, early stopping, model introspect, and
1386             snapshoting.
1387 
1388         Returns
1389         -------
1390         self : object
1391         """
1392         # if not warmstart - clear the estimator state
1393         if not self.warm_start:
1394             self._clear_state()
1395 
1396         # Check input
1397         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1398         n_samples, self.n_features_ = X.shape
1399         if sample_weight is None:
1400             sample_weight = np.ones(n_samples, dtype=np.float32)
1401         else:
1402             sample_weight = column_or_1d(sample_weight, warn=True)
1403 
1404         check_consistent_length(X, y, sample_weight)
1405 
1406         y = self._validate_y(y, sample_weight)
1407 
1408         if self.n_iter_no_change is not None:
1409             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1410                 train_test_split(X, y, sample_weight,
1411                                  random_state=self.random_state,
1412                                  test_size=self.validation_fraction))
1413         else:
1414             X_val = y_val = sample_weight_val = None
1415 
1416         self._check_params()
1417 
1418         if not self._is_initialized():
1419             # init state
1420             self._init_state()
1421 
1422             # fit initial model - FIXME make sample_weight optional
1423             self.init_.fit(X, y, sample_weight)
1424 
1425             # init predictions
1426             y_pred = self.init_.predict(X)
1427             begin_at_stage = 0
1428 
1429             # The rng state must be preserved if warm_start is True
1430             self._rng = check_random_state(self.random_state)
1431 
1432         else:
1433             # add more estimators to fitted model
1434             # invariant: warm_start = True
1435             if self.n_estimators < self.estimators_.shape[0]:
1436                 raise ValueError('n_estimators=%d must be larger or equal to '
1437                                  'estimators_.shape[0]=%d when '
1438                                  'warm_start==True'
1439                                  % (self.n_estimators,
1440                                     self.estimators_.shape[0]))
1441             begin_at_stage = self.estimators_.shape[0]
1442             # The requirements of _decision_function (called in two lines
1443             # below) are more constrained than fit. It accepts only CSR
1444             # matrices.
1445             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1446             y_pred = self._decision_function(X)
1447             self._resize_state()
1448 
1449         if self.presort is True and issparse(X):
1450             raise ValueError(
1451                 "Presorting is not supported for sparse matrices.")
1452 
1453         presort = self.presort
1454         # Allow presort to be 'auto', which means True if the dataset is dense,
1455         # otherwise it will be False.
1456         if presort == 'auto':
1457             presort = not issparse(X)
1458 
1459         X_idx_sorted = None
1460         if presort:
1461             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1462                                              dtype=np.int32)
1463 
1464         # fit the boosting stages
1465         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
1466                                     X_val, y_val, sample_weight_val,
1467                                     begin_at_stage, monitor, X_idx_sorted)
1468 
1469         # change shape of arrays after fit (early-stopping or additional ests)
1470         if n_stages != self.estimators_.shape[0]:
1471             self.estimators_ = self.estimators_[:n_stages]
1472             self.train_score_ = self.train_score_[:n_stages]
1473             if hasattr(self, 'oob_improvement_'):
1474                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1475 
1476         self.n_estimators_ = n_stages
1477         return self
1478 
1479     def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
1480                     X_val, y_val, sample_weight_val,
1481                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1482         """Iteratively fits the stages.
1483 
1484         For each stage it computes the progress (OOB, train score)
1485         and delegates to ``_fit_stage``.
1486         Returns the number of stages fit; might differ from ``n_estimators``
1487         due to early stopping.
1488         """
1489         n_samples = X.shape[0]
1490         do_oob = self.subsample < 1.0
1491         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1492         n_inbag = max(1, int(self.subsample * n_samples))
1493         loss_ = self.loss_
1494 
1495         # Set min_weight_leaf from min_weight_fraction_leaf
1496         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1497             min_weight_leaf = (self.min_weight_fraction_leaf *
1498                                np.sum(sample_weight))
1499         else:
1500             min_weight_leaf = 0.
1501 
1502         if self.verbose:
1503             verbose_reporter = VerboseReporter(self.verbose)
1504             verbose_reporter.init(self, begin_at_stage)
1505 
1506         X_csc = csc_matrix(X) if issparse(X) else None
1507         X_csr = csr_matrix(X) if issparse(X) else None
1508 
1509         if self.n_iter_no_change is not None:
1510             loss_history = np.full(self.n_iter_no_change, np.inf)
1511             # We create a generator to get the predictions for X_val after
1512             # the addition of each successive stage
1513             y_val_pred_iter = self._staged_decision_function(X_val)
1514 
1515         # perform boosting iterations
1516         i = begin_at_stage
1517         for i in range(begin_at_stage, self.n_estimators):
1518 
1519             # subsampling
1520             if do_oob:
1521                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1522                                                   random_state)
1523                 # OOB score before adding this stage
1524                 old_oob_score = loss_(y[~sample_mask],
1525                                       y_pred[~sample_mask],
1526                                       sample_weight[~sample_mask])
1527 
1528             # fit next stage of trees
1529             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,
1530                                      sample_mask, random_state, X_idx_sorted,
1531                                      X_csc, X_csr)
1532 
1533             # track deviance (= loss)
1534             if do_oob:
1535                 self.train_score_[i] = loss_(y[sample_mask],
1536                                              y_pred[sample_mask],
1537                                              sample_weight[sample_mask])
1538                 self.oob_improvement_[i] = (
1539                     old_oob_score - loss_(y[~sample_mask],
1540                                           y_pred[~sample_mask],
1541                                           sample_weight[~sample_mask]))
1542             else:
1543                 # no need to fancy index w/ no subsampling
1544                 self.train_score_[i] = loss_(y, y_pred, sample_weight)
1545 
1546             if self.verbose > 0:
1547                 verbose_reporter.update(i, self)
1548 
1549             if monitor is not None:
1550                 early_stopping = monitor(i, self, locals())
1551                 if early_stopping:
1552                     break
1553 
1554             # We also provide an early stopping based on the score from
1555             # validation set (X_val, y_val), if n_iter_no_change is set
1556             if self.n_iter_no_change is not None:
1557                 # By calling next(y_val_pred_iter), we get the predictions
1558                 # for X_val after the addition of the current stage
1559                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1560                                         sample_weight_val)
1561 
1562                 # Require validation_score to be better (less) than at least
1563                 # one of the last n_iter_no_change evaluations
1564                 if np.any(validation_loss + self.tol < loss_history):
1565                     loss_history[i % len(loss_history)] = validation_loss
1566                 else:
1567                     break
1568 
1569         return i + 1
1570 
1571     def _make_estimator(self, append=True):
1572         # we don't need _make_estimator
1573         raise NotImplementedError()
1574 
1575     def _init_decision_function(self, X):
1576         """Check input and compute prediction of ``init``. """
1577         self._check_initialized()
1578         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1579         if X.shape[1] != self.n_features_:
1580             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1581                 self.n_features_, X.shape[1]))
1582         score = self.init_.predict(X).astype(np.float64)
1583         return score
1584 
1585     def _decision_function(self, X):
1586         # for use in inner loop, not raveling the output in single-class case,
1587         # not doing input validation.
1588         score = self._init_decision_function(X)
1589         predict_stages(self.estimators_, X, self.learning_rate, score)
1590         return score
1591 
1592 
1593     def _staged_decision_function(self, X):
1594         """Compute decision function of ``X`` for each iteration.
1595 
1596         This method allows monitoring (i.e. determine error on testing set)
1597         after each stage.
1598 
1599         Parameters
1600         ----------
1601         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1602             The input samples. Internally, it will be converted to
1603             ``dtype=np.float32`` and if a sparse matrix is provided
1604             to a sparse ``csr_matrix``.
1605 
1606         Returns
1607         -------
1608         score : generator of array, shape (n_samples, k)
1609             The decision function of the input samples. The order of the
1610             classes corresponds to that in the attribute `classes_`.
1611             Regression and binary classification are special cases with
1612             ``k == 1``, otherwise ``k==n_classes``.
1613         """
1614         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1615         score = self._init_decision_function(X)
1616         for i in range(self.estimators_.shape[0]):
1617             predict_stage(self.estimators_, i, X, self.learning_rate, score)
1618             yield score.copy()
1619 
1620     @property
1621     def feature_importances_(self):
1622         """Return the feature importances (the higher, the more important the
1623            feature).
1624 
1625         Returns
1626         -------
1627         feature_importances_ : array, shape (n_features,)
1628         """
1629         self._check_initialized()
1630 
1631         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1632         for stage in self.estimators_:
1633             stage_sum = sum(tree.tree_.compute_feature_importances(
1634                 normalize=False) for tree in stage) / len(stage)
1635             total_sum += stage_sum
1636 
1637         importances = total_sum / total_sum.sum()
1638         return importances
1639 
1640     def _validate_y(self, y, sample_weight):
1641         # 'sample_weight' is not utilised but is used for
1642         # consistency with similar method _validate_y of GBC
1643         self.n_classes_ = 1
1644         if y.dtype.kind == 'O':
1645             y = y.astype(np.float64)
1646         # Default implementation
1647         return y
1648 
1649     def apply(self, X):
1650         """Apply trees in the ensemble to X, return leaf indices.
1651 
1652         .. versionadded:: 0.17
1653 
1654         Parameters
1655         ----------
1656         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1657             The input samples. Internally, its dtype will be converted to
1658             ``dtype=np.float32``. If a sparse matrix is provided, it will
1659             be converted to a sparse ``csr_matrix``.
1660 
1661         Returns
1662         -------
1663         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1664             For each datapoint x in X and for each tree in the ensemble,
1665             return the index of the leaf x ends up in each estimator.
1666             In the case of binary classification n_classes is 1.
1667         """
1668 
1669         self._check_initialized()
1670         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1671 
1672         # n_classes will be equal to 1 in the binary classification or the
1673         # regression case.
1674         n_estimators, n_classes = self.estimators_.shape
1675         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1676 
1677         for i in range(n_estimators):
1678             for j in range(n_classes):
1679                 estimator = self.estimators_[i, j]
1680                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1681 
1682         return leaves
1683 
1684 
1685 class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
1686     """Gradient Boosting for classification.
1687 
1688     GB builds an additive model in a
1689     forward stage-wise fashion; it allows for the optimization of
1690     arbitrary differentiable loss functions. In each stage ``n_classes_``
1691     regression trees are fit on the negative gradient of the
1692     binomial or multinomial deviance loss function. Binary classification
1693     is a special case where only a single regression tree is induced.
1694 
1695     Read more in the :ref:`User Guide <gradient_boosting>`.
1696 
1697     Parameters
1698     ----------
1699     loss : {'deviance', 'exponential'}, optional (default='deviance')
1700         loss function to be optimized. 'deviance' refers to
1701         deviance (= logistic regression) for classification
1702         with probabilistic outputs. For loss 'exponential' gradient
1703         boosting recovers the AdaBoost algorithm.
1704 
1705     learning_rate : float, optional (default=0.1)
1706         learning rate shrinks the contribution of each tree by `learning_rate`.
1707         There is a trade-off between learning_rate and n_estimators.
1708 
1709     n_estimators : int (default=100)
1710         The number of boosting stages to perform. Gradient boosting
1711         is fairly robust to over-fitting so a large number usually
1712         results in better performance.
1713 
1714     subsample : float, optional (default=1.0)
1715         The fraction of samples to be used for fitting the individual base
1716         learners. If smaller than 1.0 this results in Stochastic Gradient
1717         Boosting. `subsample` interacts with the parameter `n_estimators`.
1718         Choosing `subsample < 1.0` leads to a reduction of variance
1719         and an increase in bias.
1720 
1721     criterion : string, optional (default="friedman_mse")
1722         The function to measure the quality of a split. Supported criteria
1723         are "friedman_mse" for the mean squared error with improvement
1724         score by Friedman, "mse" for mean squared error, and "mae" for
1725         the mean absolute error. The default value of "friedman_mse" is
1726         generally the best as it can provide a better approximation in
1727         some cases.
1728 
1729         .. versionadded:: 0.18
1730 
1731     min_samples_split : int, float, optional (default=2)
1732         The minimum number of samples required to split an internal node:
1733 
1734         - If int, then consider `min_samples_split` as the minimum number.
1735         - If float, then `min_samples_split` is a fraction and
1736           `ceil(min_samples_split * n_samples)` are the minimum
1737           number of samples for each split.
1738 
1739         .. versionchanged:: 0.18
1740            Added float values for fractions.
1741 
1742     min_samples_leaf : int, float, optional (default=1)
1743         The minimum number of samples required to be at a leaf node.
1744         A split point at any depth will only be considered if it leaves at
1745         least ``min_samples_leaf`` training samples in each of the left and
1746         right branches.  This may have the effect of smoothing the model,
1747         especially in regression.
1748 
1749         - If int, then consider `min_samples_leaf` as the minimum number.
1750         - If float, then `min_samples_leaf` is a fraction and
1751           `ceil(min_samples_leaf * n_samples)` are the minimum
1752           number of samples for each node.
1753 
1754         .. versionchanged:: 0.18
1755            Added float values for fractions.
1756 
1757     min_weight_fraction_leaf : float, optional (default=0.)
1758         The minimum weighted fraction of the sum total of weights (of all
1759         the input samples) required to be at a leaf node. Samples have
1760         equal weight when sample_weight is not provided.
1761 
1762     max_depth : integer, optional (default=3)
1763         maximum depth of the individual regression estimators. The maximum
1764         depth limits the number of nodes in the tree. Tune this parameter
1765         for best performance; the best value depends on the interaction
1766         of the input variables.
1767 
1768     min_impurity_decrease : float, optional (default=0.)
1769         A node will be split if this split induces a decrease of the impurity
1770         greater than or equal to this value.
1771 
1772         The weighted impurity decrease equation is the following::
1773 
1774             N_t / N * (impurity - N_t_R / N_t * right_impurity
1775                                 - N_t_L / N_t * left_impurity)
1776 
1777         where ``N`` is the total number of samples, ``N_t`` is the number of
1778         samples at the current node, ``N_t_L`` is the number of samples in the
1779         left child, and ``N_t_R`` is the number of samples in the right child.
1780 
1781         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1782         if ``sample_weight`` is passed.
1783 
1784         .. versionadded:: 0.19
1785 
1786     min_impurity_split : float, (default=1e-7)
1787         Threshold for early stopping in tree growth. A node will split
1788         if its impurity is above the threshold, otherwise it is a leaf.
1789 
1790         .. deprecated:: 0.19
1791            ``min_impurity_split`` has been deprecated in favor of
1792            ``min_impurity_decrease`` in 0.19. The default value of
1793            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1794            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1795 
1796     init : estimator, optional
1797         An estimator object that is used to compute the initial
1798         predictions. ``init`` has to provide ``fit`` and ``predict``.
1799         If None it uses ``loss.init_estimator``.
1800 
1801     random_state : int, RandomState instance or None, optional (default=None)
1802         If int, random_state is the seed used by the random number generator;
1803         If RandomState instance, random_state is the random number generator;
1804         If None, the random number generator is the RandomState instance used
1805         by `np.random`.
1806 
1807     max_features : int, float, string or None, optional (default=None)
1808         The number of features to consider when looking for the best split:
1809 
1810         - If int, then consider `max_features` features at each split.
1811         - If float, then `max_features` is a fraction and
1812           `int(max_features * n_features)` features are considered at each
1813           split.
1814         - If "auto", then `max_features=sqrt(n_features)`.
1815         - If "sqrt", then `max_features=sqrt(n_features)`.
1816         - If "log2", then `max_features=log2(n_features)`.
1817         - If None, then `max_features=n_features`.
1818 
1819         Choosing `max_features < n_features` leads to a reduction of variance
1820         and an increase in bias.
1821 
1822         Note: the search for a split does not stop until at least one
1823         valid partition of the node samples is found, even if it requires to
1824         effectively inspect more than ``max_features`` features.
1825 
1826     verbose : int, default: 0
1827         Enable verbose output. If 1 then it prints progress and performance
1828         once in a while (the more trees the lower the frequency). If greater
1829         than 1 then it prints progress and performance for every tree.
1830 
1831     max_leaf_nodes : int or None, optional (default=None)
1832         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1833         Best nodes are defined as relative reduction in impurity.
1834         If None then unlimited number of leaf nodes.
1835 
1836     warm_start : bool, default: False
1837         When set to ``True``, reuse the solution of the previous call to fit
1838         and add more estimators to the ensemble, otherwise, just erase the
1839         previous solution. See :term:`the Glossary <warm_start>`.
1840 
1841     presort : bool or 'auto', optional (default='auto')
1842         Whether to presort the data to speed up the finding of best splits in
1843         fitting. Auto mode by default will use presorting on dense data and
1844         default to normal sorting on sparse data. Setting presort to true on
1845         sparse data will raise an error.
1846 
1847         .. versionadded:: 0.17
1848            *presort* parameter.
1849 
1850     validation_fraction : float, optional, default 0.1
1851         The proportion of training data to set aside as validation set for
1852         early stopping. Must be between 0 and 1.
1853         Only used if ``n_iter_no_change`` is set to an integer.
1854 
1855         .. versionadded:: 0.20
1856 
1857     n_iter_no_change : int, default None
1858         ``n_iter_no_change`` is used to decide if early stopping will be used
1859         to terminate training when validation score is not improving. By
1860         default it is set to None to disable early stopping. If set to a
1861         number, it will set aside ``validation_fraction`` size of the training
1862         data as validation and terminate training when validation score is not
1863         improving in all of the previous ``n_iter_no_change`` numbers of
1864         iterations.
1865 
1866         .. versionadded:: 0.20
1867 
1868     tol : float, optional, default 1e-4
1869         Tolerance for the early stopping. When the loss is not improving
1870         by at least tol for ``n_iter_no_change`` iterations (if set to a
1871         number), the training stops.
1872 
1873         .. versionadded:: 0.20
1874 
1875     Attributes
1876     ----------
1877     n_estimators_ : int
1878         The number of estimators as selected by early stopping (if
1879         ``n_iter_no_change`` is specified). Otherwise it is set to
1880         ``n_estimators``.
1881 
1882         .. versionadded:: 0.20
1883 
1884     feature_importances_ : array, shape (n_features,)
1885         The feature importances (the higher, the more important the feature).
1886 
1887     oob_improvement_ : array, shape (n_estimators,)
1888         The improvement in loss (= deviance) on the out-of-bag samples
1889         relative to the previous iteration.
1890         ``oob_improvement_[0]`` is the improvement in
1891         loss of the first stage over the ``init`` estimator.
1892 
1893     train_score_ : array, shape (n_estimators,)
1894         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
1895         model at iteration ``i`` on the in-bag sample.
1896         If ``subsample == 1`` this is the deviance on the training data.
1897 
1898     loss_ : LossFunction
1899         The concrete ``LossFunction`` object.
1900 
1901     init_ : estimator
1902         The estimator that provides the initial predictions.
1903         Set via the ``init`` argument or ``loss.init_estimator``.
1904 
1905     estimators_ : ndarray of DecisionTreeRegressor,\
1906 shape (n_estimators, ``loss_.K``)
1907         The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary
1908         classification, otherwise n_classes.
1909 
1910     Notes
1911     -----
1912     The features are always randomly permuted at each split. Therefore,
1913     the best found split may vary, even with the same training data and
1914     ``max_features=n_features``, if the improvement of the criterion is
1915     identical for several splits enumerated during the search of the best
1916     split. To obtain a deterministic behaviour during fitting,
1917     ``random_state`` has to be fixed.
1918 
1919     See also
1920     --------
1921     sklearn.tree.DecisionTreeClassifier, RandomForestClassifier
1922     AdaBoostClassifier
1923 
1924     References
1925     ----------
1926     J. Friedman, Greedy Function Approximation: A Gradient Boosting
1927     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
1928 
1929     J. Friedman, Stochastic Gradient Boosting, 1999
1930 
1931     T. Hastie, R. Tibshirani and J. Friedman.
1932     Elements of Statistical Learning Ed. 2, Springer, 2009.
1933     """
1934 
1935     _SUPPORTED_LOSS = ('deviance', 'exponential')
1936 
1937     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
1938                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
1939                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
1940                  max_depth=3, min_impurity_decrease=0.,
1941                  min_impurity_split=None, init=None,
1942                  random_state=None, max_features=None, verbose=0,
1943                  max_leaf_nodes=None, warm_start=False,
1944                  presort='auto', validation_fraction=0.1,
1945                  n_iter_no_change=None, tol=1e-4):
1946 
1947         super().__init__(
1948             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
1949             criterion=criterion, min_samples_split=min_samples_split,
1950             min_samples_leaf=min_samples_leaf,
1951             min_weight_fraction_leaf=min_weight_fraction_leaf,
1952             max_depth=max_depth, init=init, subsample=subsample,
1953             max_features=max_features,
1954             random_state=random_state, verbose=verbose,
1955             max_leaf_nodes=max_leaf_nodes,
1956             min_impurity_decrease=min_impurity_decrease,
1957             min_impurity_split=min_impurity_split,
1958             warm_start=warm_start, presort=presort,
1959             validation_fraction=validation_fraction,
1960             n_iter_no_change=n_iter_no_change, tol=tol)
1961 
1962     def _validate_y(self, y, sample_weight):
1963         check_classification_targets(y)
1964         self.classes_, y = np.unique(y, return_inverse=True)
1965         n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))
1966         if n_trim_classes < 2:
1967             raise ValueError("y contains %d class after sample_weight "
1968                              "trimmed classes with zero weights, while a "
1969                              "minimum of 2 classes are required."
1970                              % n_trim_classes)
1971         self.n_classes_ = len(self.classes_)
1972         return y
1973 
1974     def decision_function(self, X):
1975         """Compute the decision function of ``X``.
1976 
1977         Parameters
1978         ----------
1979         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1980             The input samples. Internally, it will be converted to
1981             ``dtype=np.float32`` and if a sparse matrix is provided
1982             to a sparse ``csr_matrix``.
1983 
1984         Returns
1985         -------
1986         score : array, shape (n_samples, n_classes) or (n_samples,)
1987             The decision function of the input samples. The order of the
1988             classes corresponds to that in the attribute `classes_`.
1989             Regression and binary classification produce an array of shape
1990             [n_samples].
1991         """
1992         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1993         score = self._decision_function(X)
1994         if score.shape[1] == 1:
1995             return score.ravel()
1996         return score
1997 
1998     def staged_decision_function(self, X):
1999         """Compute decision function of ``X`` for each iteration.
2000 
2001         This method allows monitoring (i.e. determine error on testing set)
2002         after each stage.
2003 
2004         Parameters
2005         ----------
2006         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2007             The input samples. Internally, it will be converted to
2008             ``dtype=np.float32`` and if a sparse matrix is provided
2009             to a sparse ``csr_matrix``.
2010 
2011         Returns
2012         -------
2013         score : generator of array, shape (n_samples, k)
2014             The decision function of the input samples. The order of the
2015             classes corresponds to that in the attribute `classes_`.
2016             Regression and binary classification are special cases with
2017             ``k == 1``, otherwise ``k==n_classes``.
2018         """
2019         yield from self._staged_decision_function(X)
2020 
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)
2039 
2040     def staged_predict(self, X):
2041         """Predict class at each stage for X.
2042 
2043         This method allows monitoring (i.e. determine error on testing set)
2044         after each stage.
2045 
2046         Parameters
2047         ----------
2048         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2049             The input samples. Internally, it will be converted to
2050             ``dtype=np.float32`` and if a sparse matrix is provided
2051             to a sparse ``csr_matrix``.
2052 
2053         Returns
2054         -------
2055         y : generator of array of shape (n_samples,)
2056             The predicted value of the input samples.
2057         """
2058         for score in self._staged_decision_function(X):
2059             decisions = self.loss_._score_to_decision(score)
2060             yield self.classes_.take(decisions, axis=0)
2061 
2062     def predict_proba(self, X):
2063         """Predict class probabilities for X.
2064 
2065         Parameters
2066         ----------
2067         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2068             The input samples. Internally, it will be converted to
2069             ``dtype=np.float32`` and if a sparse matrix is provided
2070             to a sparse ``csr_matrix``.
2071 
2072         Raises
2073         ------
2074         AttributeError
2075             If the ``loss`` does not support probabilities.
2076 
2077         Returns
2078         -------
2079         p : array, shape (n_samples, n_classes)
2080             The class probabilities of the input samples. The order of the
2081             classes corresponds to that in the attribute `classes_`.
2082         """
2083         score = self.decision_function(X)
2084         try:
2085             return self.loss_._score_to_proba(score)
2086         except NotFittedError:
2087             raise
2088         except AttributeError:
2089             raise AttributeError('loss=%r does not support predict_proba' %
2090                                  self.loss)
2091 
2092     def predict_log_proba(self, X):
2093         """Predict class log-probabilities for X.
2094 
2095         Parameters
2096         ----------
2097         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2098             The input samples. Internally, it will be converted to
2099             ``dtype=np.float32`` and if a sparse matrix is provided
2100             to a sparse ``csr_matrix``.
2101 
2102         Raises
2103         ------
2104         AttributeError
2105             If the ``loss`` does not support probabilities.
2106 
2107         Returns
2108         -------
2109         p : array, shape (n_samples, n_classes)
2110             The class log-probabilities of the input samples. The order of the
2111             classes corresponds to that in the attribute `classes_`.
2112         """
2113         proba = self.predict_proba(X)
2114         return np.log(proba)
2115 
2116     def staged_predict_proba(self, X):
2117         """Predict class probabilities at each stage for X.
2118 
2119         This method allows monitoring (i.e. determine error on testing set)
2120         after each stage.
2121 
2122         Parameters
2123         ----------
2124         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2125             The input samples. Internally, it will be converted to
2126             ``dtype=np.float32`` and if a sparse matrix is provided
2127             to a sparse ``csr_matrix``.
2128 
2129         Returns
2130         -------
2131         y : generator of array of shape (n_samples,)
2132             The predicted value of the input samples.
2133         """
2134         try:
2135             for score in self._staged_decision_function(X):
2136                 yield self.loss_._score_to_proba(score)
2137         except NotFittedError:
2138             raise
2139         except AttributeError:
2140             raise AttributeError('loss=%r does not support predict_proba' %
2141                                  self.loss)
2142 
2143 
2144 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
2145     """Gradient Boosting for regression.
2146 
2147     GB builds an additive model in a forward stage-wise fashion;
2148     it allows for the optimization of arbitrary differentiable loss functions.
2149     In each stage a regression tree is fit on the negative gradient of the
2150     given loss function.
2151 
2152     Read more in the :ref:`User Guide <gradient_boosting>`.
2153 
2154     Parameters
2155     ----------
2156     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')
2157         loss function to be optimized. 'ls' refers to least squares
2158         regression. 'lad' (least absolute deviation) is a highly robust
2159         loss function solely based on order information of the input
2160         variables. 'huber' is a combination of the two. 'quantile'
2161         allows quantile regression (use `alpha` to specify the quantile).
2162 
2163     learning_rate : float, optional (default=0.1)
2164         learning rate shrinks the contribution of each tree by `learning_rate`.
2165         There is a trade-off between learning_rate and n_estimators.
2166 
2167     n_estimators : int (default=100)
2168         The number of boosting stages to perform. Gradient boosting
2169         is fairly robust to over-fitting so a large number usually
2170         results in better performance.
2171 
2172     subsample : float, optional (default=1.0)
2173         The fraction of samples to be used for fitting the individual base
2174         learners. If smaller than 1.0 this results in Stochastic Gradient
2175         Boosting. `subsample` interacts with the parameter `n_estimators`.
2176         Choosing `subsample < 1.0` leads to a reduction of variance
2177         and an increase in bias.
2178 
2179     criterion : string, optional (default="friedman_mse")
2180         The function to measure the quality of a split. Supported criteria
2181         are "friedman_mse" for the mean squared error with improvement
2182         score by Friedman, "mse" for mean squared error, and "mae" for
2183         the mean absolute error. The default value of "friedman_mse" is
2184         generally the best as it can provide a better approximation in
2185         some cases.
2186 
2187         .. versionadded:: 0.18
2188 
2189     min_samples_split : int, float, optional (default=2)
2190         The minimum number of samples required to split an internal node:
2191 
2192         - If int, then consider `min_samples_split` as the minimum number.
2193         - If float, then `min_samples_split` is a fraction and
2194           `ceil(min_samples_split * n_samples)` are the minimum
2195           number of samples for each split.
2196 
2197         .. versionchanged:: 0.18
2198            Added float values for fractions.
2199 
2200     min_samples_leaf : int, float, optional (default=1)
2201         The minimum number of samples required to be at a leaf node.
2202         A split point at any depth will only be considered if it leaves at
2203         least ``min_samples_leaf`` training samples in each of the left and
2204         right branches.  This may have the effect of smoothing the model,
2205         especially in regression.
2206 
2207         - If int, then consider `min_samples_leaf` as the minimum number.
2208         - If float, then `min_samples_leaf` is a fraction and
2209           `ceil(min_samples_leaf * n_samples)` are the minimum
2210           number of samples for each node.
2211 
2212         .. versionchanged:: 0.18
2213            Added float values for fractions.
2214 
2215     min_weight_fraction_leaf : float, optional (default=0.)
2216         The minimum weighted fraction of the sum total of weights (of all
2217         the input samples) required to be at a leaf node. Samples have
2218         equal weight when sample_weight is not provided.
2219 
2220     max_depth : integer, optional (default=3)
2221         maximum depth of the individual regression estimators. The maximum
2222         depth limits the number of nodes in the tree. Tune this parameter
2223         for best performance; the best value depends on the interaction
2224         of the input variables.
2225 
2226     min_impurity_decrease : float, optional (default=0.)
2227         A node will be split if this split induces a decrease of the impurity
2228         greater than or equal to this value.
2229 
2230         The weighted impurity decrease equation is the following::
2231 
2232             N_t / N * (impurity - N_t_R / N_t * right_impurity
2233                                 - N_t_L / N_t * left_impurity)
2234 
2235         where ``N`` is the total number of samples, ``N_t`` is the number of
2236         samples at the current node, ``N_t_L`` is the number of samples in the
2237         left child, and ``N_t_R`` is the number of samples in the right child.
2238 
2239         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2240         if ``sample_weight`` is passed.
2241 
2242         .. versionadded:: 0.19
2243 
2244     min_impurity_split : float, (default=1e-7)
2245         Threshold for early stopping in tree growth. A node will split
2246         if its impurity is above the threshold, otherwise it is a leaf.
2247 
2248         .. deprecated:: 0.19
2249            ``min_impurity_split`` has been deprecated in favor of
2250            ``min_impurity_decrease`` in 0.19. The default value of
2251            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
2252            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
2253 
2254     init : estimator, optional (default=None)
2255         An estimator object that is used to compute the initial
2256         predictions. ``init`` has to provide ``fit`` and ``predict``.
2257         If None it uses ``loss.init_estimator``.
2258 
2259     random_state : int, RandomState instance or None, optional (default=None)
2260         If int, random_state is the seed used by the random number generator;
2261         If RandomState instance, random_state is the random number generator;
2262         If None, the random number generator is the RandomState instance used
2263         by `np.random`.
2264 
2265     max_features : int, float, string or None, optional (default=None)
2266         The number of features to consider when looking for the best split:
2267 
2268         - If int, then consider `max_features` features at each split.
2269         - If float, then `max_features` is a fraction and
2270           `int(max_features * n_features)` features are considered at each
2271           split.
2272         - If "auto", then `max_features=n_features`.
2273         - If "sqrt", then `max_features=sqrt(n_features)`.
2274         - If "log2", then `max_features=log2(n_features)`.
2275         - If None, then `max_features=n_features`.
2276 
2277         Choosing `max_features < n_features` leads to a reduction of variance
2278         and an increase in bias.
2279 
2280         Note: the search for a split does not stop until at least one
2281         valid partition of the node samples is found, even if it requires to
2282         effectively inspect more than ``max_features`` features.
2283 
2284     alpha : float (default=0.9)
2285         The alpha-quantile of the huber loss function and the quantile
2286         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
2287 
2288     verbose : int, default: 0
2289         Enable verbose output. If 1 then it prints progress and performance
2290         once in a while (the more trees the lower the frequency). If greater
2291         than 1 then it prints progress and performance for every tree.
2292 
2293     max_leaf_nodes : int or None, optional (default=None)
2294         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2295         Best nodes are defined as relative reduction in impurity.
2296         If None then unlimited number of leaf nodes.
2297 
2298     warm_start : bool, default: False
2299         When set to ``True``, reuse the solution of the previous call to fit
2300         and add more estimators to the ensemble, otherwise, just erase the
2301         previous solution. See :term:`the Glossary <warm_start>`.
2302 
2303     presort : bool or 'auto', optional (default='auto')
2304         Whether to presort the data to speed up the finding of best splits in
2305         fitting. Auto mode by default will use presorting on dense data and
2306         default to normal sorting on sparse data. Setting presort to true on
2307         sparse data will raise an error.
2308 
2309         .. versionadded:: 0.17
2310            optional parameter *presort*.
2311 
2312     validation_fraction : float, optional, default 0.1
2313         The proportion of training data to set aside as validation set for
2314         early stopping. Must be between 0 and 1.
2315         Only used if ``n_iter_no_change`` is set to an integer.
2316 
2317         .. versionadded:: 0.20
2318 
2319     n_iter_no_change : int, default None
2320         ``n_iter_no_change`` is used to decide if early stopping will be used
2321         to terminate training when validation score is not improving. By
2322         default it is set to None to disable early stopping. If set to a
2323         number, it will set aside ``validation_fraction`` size of the training
2324         data as validation and terminate training when validation score is not
2325         improving in all of the previous ``n_iter_no_change`` numbers of
2326         iterations.
2327 
2328         .. versionadded:: 0.20
2329 
2330     tol : float, optional, default 1e-4
2331         Tolerance for the early stopping. When the loss is not improving
2332         by at least tol for ``n_iter_no_change`` iterations (if set to a
2333         number), the training stops.
2334 
2335         .. versionadded:: 0.20
2336 
2337 
2338     Attributes
2339     ----------
2340     feature_importances_ : array, shape (n_features,)
2341         The feature importances (the higher, the more important the feature).
2342 
2343     oob_improvement_ : array, shape (n_estimators,)
2344         The improvement in loss (= deviance) on the out-of-bag samples
2345         relative to the previous iteration.
2346         ``oob_improvement_[0]`` is the improvement in
2347         loss of the first stage over the ``init`` estimator.
2348 
2349     train_score_ : array, shape (n_estimators,)
2350         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
2351         model at iteration ``i`` on the in-bag sample.
2352         If ``subsample == 1`` this is the deviance on the training data.
2353 
2354     loss_ : LossFunction
2355         The concrete ``LossFunction`` object.
2356 
2357     init_ : estimator
2358         The estimator that provides the initial predictions.
2359         Set via the ``init`` argument or ``loss.init_estimator``.
2360 
2361     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)
2362         The collection of fitted sub-estimators.
2363 
2364     Notes
2365     -----
2366     The features are always randomly permuted at each split. Therefore,
2367     the best found split may vary, even with the same training data and
2368     ``max_features=n_features``, if the improvement of the criterion is
2369     identical for several splits enumerated during the search of the best
2370     split. To obtain a deterministic behaviour during fitting,
2371     ``random_state`` has to be fixed.
2372 
2373     See also
2374     --------
2375     DecisionTreeRegressor, RandomForestRegressor
2376 
2377     References
2378     ----------
2379     J. Friedman, Greedy Function Approximation: A Gradient Boosting
2380     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
2381 
2382     J. Friedman, Stochastic Gradient Boosting, 1999
2383 
2384     T. Hastie, R. Tibshirani and J. Friedman.
2385     Elements of Statistical Learning Ed. 2, Springer, 2009.
2386     """
2387 
2388     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')
2389 
2390     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
2391                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
2392                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
2393                  max_depth=3, min_impurity_decrease=0.,
2394                  min_impurity_split=None, init=None, random_state=None,
2395                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
2396                  warm_start=False, presort='auto', validation_fraction=0.1,
2397                  n_iter_no_change=None, tol=1e-4):
2398 
2399         super().__init__(
2400             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
2401             criterion=criterion, min_samples_split=min_samples_split,
2402             min_samples_leaf=min_samples_leaf,
2403             min_weight_fraction_leaf=min_weight_fraction_leaf,
2404             max_depth=max_depth, init=init, subsample=subsample,
2405             max_features=max_features,
2406             min_impurity_decrease=min_impurity_decrease,
2407             min_impurity_split=min_impurity_split,
2408             random_state=random_state, alpha=alpha, verbose=verbose,
2409             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,
2410             presort=presort, validation_fraction=validation_fraction,
2411             n_iter_no_change=n_iter_no_change, tol=tol)
2412 
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()
2430 
2431     def staged_predict(self, X):
2432         """Predict regression target at each stage for X.
2433 
2434         This method allows monitoring (i.e. determine error on testing set)
2435         after each stage.
2436 
2437         Parameters
2438         ----------
2439         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2440             The input samples. Internally, it will be converted to
2441             ``dtype=np.float32`` and if a sparse matrix is provided
2442             to a sparse ``csr_matrix``.
2443 
2444         Returns
2445         -------
2446         y : generator of array of shape (n_samples,)
2447             The predicted value of the input samples.
2448         """
2449         for y in self._staged_decision_function(X):
2450             yield y.ravel()
2451 
2452     def apply(self, X):
2453         """Apply trees in the ensemble to X, return leaf indices.
2454 
2455         .. versionadded:: 0.17
2456 
2457         Parameters
2458         ----------
2459         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2460             The input samples. Internally, its dtype will be converted to
2461             ``dtype=np.float32``. If a sparse matrix is provided, it will
2462             be converted to a sparse ``csr_matrix``.
2463 
2464         Returns
2465         -------
2466         X_leaves : array-like, shape (n_samples, n_estimators)
2467             For each datapoint x in X and for each tree in the ensemble,
2468             return the index of the leaf x ends up in each estimator.
2469         """
2470 
2471         leaves = super().apply(X)
2472         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])
2473         return leaves

</code><intended_behavior>For gradient boosting methods, the intention is similar to above. Adjust the validate_X calls or similar validations to support a wider range of data types as inputs, provided the base estimators can work with those data types. It's crucial to maintain a balance between allowing flexibility and ensuring the input data's appropriacy for the algorithm to work as intended.</intended_behavior>]
2025-10-18 05:28:38.564 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/weight_boosting.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Weight Boosting
2 
3 This module contains weight boosting estimators for both classification and
4 regression.
5 
6 The module structure is the following:
7 
8 - The ``BaseWeightBoosting`` base class implements a common ``fit`` method
9   for all the estimators in the module. Regression and classification
10   only differ from each other in the loss function that is optimized.
11 
12 - ``AdaBoostClassifier`` implements adaptive boosting (AdaBoost-SAMME) for
13   classification problems.
14 
15 - ``AdaBoostRegressor`` implements adaptive boosting (AdaBoost.R2) for
16   regression problems.
17 """
18 
19 # Authors: Noel Dawe <noel@dawe.me>
20 #          Gilles Louppe <g.louppe@gmail.com>
21 #          Hamzeh Alsalhi <ha258@cornell.edu>
22 #          Arnaud Joly <arnaud.v.joly@gmail.com>
23 #
24 # License: BSD 3 clause
25 
26 from abc import ABCMeta, abstractmethod
27 
28 import numpy as np
29 
30 from scipy.special import xlogy
31 
32 from .base import BaseEnsemble
33 from ..base import ClassifierMixin, RegressorMixin, is_regressor, is_classifier
34 
35 from .forest import BaseForest
36 from ..tree import DecisionTreeClassifier, DecisionTreeRegressor
37 from ..tree.tree import BaseDecisionTree
38 from ..tree._tree import DTYPE
39 from ..utils import check_array, check_X_y, check_random_state
40 from ..utils.extmath import stable_cumsum
41 from ..metrics import accuracy_score, r2_score
42 from sklearn.utils.validation import has_fit_parameter, check_is_fitted
43 
44 __all__ = [
45     'AdaBoostClassifier',
46     'AdaBoostRegressor',
47 ]
48 
49 
50 class BaseWeightBoosting(BaseEnsemble, metaclass=ABCMeta):
51     """Base class for AdaBoost estimators.
52 
53     Warning: This class should not be used directly. Use derived classes
54     instead.
55     """
56 
57     @abstractmethod
58     def __init__(self,
59                  base_estimator=None,
60                  n_estimators=50,
61                  estimator_params=tuple(),
62                  learning_rate=1.,
63                  random_state=None):
64 
65         super().__init__(
66             base_estimator=base_estimator,
67             n_estimators=n_estimators,
68             estimator_params=estimator_params)
69 
70         self.learning_rate = learning_rate
71         self.random_state = random_state
72 
73     def fit(self, X, y, sample_weight=None):
74         """Build a boosted classifier/regressor from the training set (X, y).
75 
76         Parameters
77         ----------
78         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
79             The training input samples. Sparse matrix can be CSC, CSR, COO,
80             DOK, or LIL. COO, DOK, and LIL are converted to CSR. The dtype is
81             forced to DTYPE from tree._tree if the base classifier of this
82             ensemble weighted boosting classifier is a tree or forest.
83 
84         y : array-like of shape = [n_samples]
85             The target values (class labels in classification, real numbers in
86             regression).
87 
88         sample_weight : array-like of shape = [n_samples], optional
89             Sample weights. If None, the sample weights are initialized to
90             1 / n_samples.
91 
92         Returns
93         -------
94         self : object
95         """
96         # Check parameters
97         if self.learning_rate <= 0:
98             raise ValueError("learning_rate must be greater than zero")
99 
100         if (self.base_estimator is None or
101                 isinstance(self.base_estimator, (BaseDecisionTree,
102                                                  BaseForest))):
103             dtype = DTYPE
104             accept_sparse = 'csc'
105         else:
106             dtype = None
107             accept_sparse = ['csr', 'csc']
108 
109         X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,
110                          y_numeric=is_regressor(self))
111 
112         if sample_weight is None:
113             # Initialize weights to 1 / n_samples
114             sample_weight = np.empty(X.shape[0], dtype=np.float64)
115             sample_weight[:] = 1. / X.shape[0]
116         else:
117             sample_weight = check_array(sample_weight, ensure_2d=False)
118             # Normalize existing weights
119             sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)
120 
121             # Check that the sample weights sum is positive
122             if sample_weight.sum() <= 0:
123                 raise ValueError(
124                     "Attempting to fit with a non-positive "
125                     "weighted number of samples.")
126 
127         # Check parameters
128         self._validate_estimator()
129 
130         # Clear any previous fit results
131         self.estimators_ = []
132         self.estimator_weights_ = np.zeros(self.n_estimators, dtype=np.float64)
133         self.estimator_errors_ = np.ones(self.n_estimators, dtype=np.float64)
134 
135         random_state = check_random_state(self.random_state)
136 
137         for iboost in range(self.n_estimators):
138             # Boosting step
139             sample_weight, estimator_weight, estimator_error = self._boost(
140                 iboost,
141                 X, y,
142                 sample_weight,
143                 random_state)
144 
145             # Early termination
146             if sample_weight is None:
147                 break
148 
149             self.estimator_weights_[iboost] = estimator_weight
150             self.estimator_errors_[iboost] = estimator_error
151 
152             # Stop if error is zero
153             if estimator_error == 0:
154                 break
155 
156             sample_weight_sum = np.sum(sample_weight)
157 
158             # Stop if the sum of sample weights has become non-positive
159             if sample_weight_sum <= 0:
160                 break
161 
162             if iboost < self.n_estimators - 1:
163                 # Normalize
164                 sample_weight /= sample_weight_sum
165 
166         return self
167 
168     @abstractmethod
169     def _boost(self, iboost, X, y, sample_weight, random_state):
170         """Implement a single boost.
171 
172         Warning: This method needs to be overridden by subclasses.
173 
174         Parameters
175         ----------
176         iboost : int
177             The index of the current boost iteration.
178 
179         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
180             The training input samples. Sparse matrix can be CSC, CSR, COO,
181             DOK, or LIL. COO, DOK, and LIL are converted to CSR.
182 
183         y : array-like of shape = [n_samples]
184             The target values (class labels).
185 
186         sample_weight : array-like of shape = [n_samples]
187             The current sample weights.
188 
189         random_state : RandomState
190             The current random number generator
191 
192         Returns
193         -------
194         sample_weight : array-like of shape = [n_samples] or None
195             The reweighted sample weights.
196             If None then boosting has terminated early.
197 
198         estimator_weight : float
199             The weight for the current boost.
200             If None then boosting has terminated early.
201 
202         error : float
203             The classification error for the current boost.
204             If None then boosting has terminated early.
205         """
206         pass
207 
208     def staged_score(self, X, y, sample_weight=None):
209         """Return staged scores for X, y.
210 
211         This generator method yields the ensemble score after each iteration of
212         boosting and therefore allows monitoring, such as to determine the
213         score on a test set after each boost.
214 
215         Parameters
216         ----------
217         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
218             The training input samples. Sparse matrix can be CSC, CSR, COO,
219             DOK, or LIL. DOK and LIL are converted to CSR.
220 
221         y : array-like, shape = [n_samples]
222             Labels for X.
223 
224         sample_weight : array-like, shape = [n_samples], optional
225             Sample weights.
226 
227         Returns
228         -------
229         z : float
230         """
231         for y_pred in self.staged_predict(X):
232             if is_classifier(self):
233                 yield accuracy_score(y, y_pred, sample_weight=sample_weight)
234             else:
235                 yield r2_score(y, y_pred, sample_weight=sample_weight)
236 
237     @property
238     def feature_importances_(self):
239         """Return the feature importances (the higher, the more important the
240            feature).
241 
242         Returns
243         -------
244         feature_importances_ : array, shape = [n_features]
245         """
246         if self.estimators_ is None or len(self.estimators_) == 0:
247             raise ValueError("Estimator not fitted, "
248                              "call `fit` before `feature_importances_`.")
249 
250         try:
251             norm = self.estimator_weights_.sum()
252             return (sum(weight * clf.feature_importances_ for weight, clf
253                     in zip(self.estimator_weights_, self.estimators_))
254                     / norm)
255 
256         except AttributeError:
257             raise AttributeError(
258                 "Unable to compute feature importances "
259                 "since base_estimator does not have a "
260                 "feature_importances_ attribute")
261 
262     def _validate_X_predict(self, X):
263         """Ensure that X is in the proper format"""
264         if (self.base_estimator is None or
265                 isinstance(self.base_estimator,
266                            (BaseDecisionTree, BaseForest))):
267             X = check_array(X, accept_sparse='csr', dtype=DTYPE)
268 
269         else:
270             X = check_array(X, accept_sparse=['csr', 'csc', 'coo'])
271 
272         return X
273 
274 
275 def _samme_proba(estimator, n_classes, X):
276     """Calculate algorithm 4, step 2, equation c) of Zhu et al [1].
277 
278     References
279     ----------
280     .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
281 
282     """
283     proba = estimator.predict_proba(X)
284 
285     # Displace zero probabilities so the log is defined.
286     # Also fix negative elements which may occur with
287     # negative sample weights.
288     np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
289     log_proba = np.log(proba)
290 
291     return (n_classes - 1) * (log_proba - (1. / n_classes)
292                               * log_proba.sum(axis=1)[:, np.newaxis])
293 
294 
295 class AdaBoostClassifier(BaseWeightBoosting, ClassifierMixin):
296     """An AdaBoost classifier.
297 
298     An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
299     classifier on the original dataset and then fits additional copies of the
300     classifier on the same dataset but where the weights of incorrectly
301     classified instances are adjusted such that subsequent classifiers focus
302     more on difficult cases.
303 
304     This class implements the algorithm known as AdaBoost-SAMME [2].
305 
306     Read more in the :ref:`User Guide <adaboost>`.
307 
308     Parameters
309     ----------
310     base_estimator : object, optional (default=None)
311         The base estimator from which the boosted ensemble is built.
312         Support for sample weighting is required, as well as proper
313         ``classes_`` and ``n_classes_`` attributes. If ``None``, then
314         the base estimator is ``DecisionTreeClassifier(max_depth=1)``
315 
316     n_estimators : integer, optional (default=50)
317         The maximum number of estimators at which boosting is terminated.
318         In case of perfect fit, the learning procedure is stopped early.
319 
320     learning_rate : float, optional (default=1.)
321         Learning rate shrinks the contribution of each classifier by
322         ``learning_rate``. There is a trade-off between ``learning_rate`` and
323         ``n_estimators``.
324 
325     algorithm : {'SAMME', 'SAMME.R'}, optional (default='SAMME.R')
326         If 'SAMME.R' then use the SAMME.R real boosting algorithm.
327         ``base_estimator`` must support calculation of class probabilities.
328         If 'SAMME' then use the SAMME discrete boosting algorithm.
329         The SAMME.R algorithm typically converges faster than SAMME,
330         achieving a lower test error with fewer boosting iterations.
331 
332     random_state : int, RandomState instance or None, optional (default=None)
333         If int, random_state is the seed used by the random number generator;
334         If RandomState instance, random_state is the random number generator;
335         If None, the random number generator is the RandomState instance used
336         by `np.random`.
337 
338     Attributes
339     ----------
340     estimators_ : list of classifiers
341         The collection of fitted sub-estimators.
342 
343     classes_ : array of shape = [n_classes]
344         The classes labels.
345 
346     n_classes_ : int
347         The number of classes.
348 
349     estimator_weights_ : array of floats
350         Weights for each estimator in the boosted ensemble.
351 
352     estimator_errors_ : array of floats
353         Classification error for each estimator in the boosted
354         ensemble.
355 
356     feature_importances_ : array of shape = [n_features]
357         The feature importances if supported by the ``base_estimator``.
358 
359     See also
360     --------
361     AdaBoostRegressor, GradientBoostingClassifier,
362     sklearn.tree.DecisionTreeClassifier
363 
364     References
365     ----------
366     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
367            on-Line Learning and an Application to Boosting", 1995.
368 
369     .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, "Multi-class AdaBoost", 2009.
370 
371     """
372     def __init__(self,
373                  base_estimator=None,
374                  n_estimators=50,
375                  learning_rate=1.,
376                  algorithm='SAMME.R',
377                  random_state=None):
378 
379         super().__init__(
380             base_estimator=base_estimator,
381             n_estimators=n_estimators,
382             learning_rate=learning_rate,
383             random_state=random_state)
384 
385         self.algorithm = algorithm
386 
387     def fit(self, X, y, sample_weight=None):
388         """Build a boosted classifier from the training set (X, y).
389 
390         Parameters
391         ----------
392         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
393             The training input samples. Sparse matrix can be CSC, CSR, COO,
394             DOK, or LIL. DOK and LIL are converted to CSR.
395 
396         y : array-like of shape = [n_samples]
397             The target values (class labels).
398 
399         sample_weight : array-like of shape = [n_samples], optional
400             Sample weights. If None, the sample weights are initialized to
401             ``1 / n_samples``.
402 
403         Returns
404         -------
405         self : object
406         """
407         # Check that algorithm is supported
408         if self.algorithm not in ('SAMME', 'SAMME.R'):
409             raise ValueError("algorithm %s is not supported" % self.algorithm)
410 
411         # Fit
412         return super().fit(X, y, sample_weight)
413 
414     def _validate_estimator(self):
415         """Check the estimator and set the base_estimator_ attribute."""
416         super()._validate_estimator(
417             default=DecisionTreeClassifier(max_depth=1))
418 
419         #  SAMME-R requires predict_proba-enabled base estimators
420         if self.algorithm == 'SAMME.R':
421             if not hasattr(self.base_estimator_, 'predict_proba'):
422                 raise TypeError(
423                     "AdaBoostClassifier with algorithm='SAMME.R' requires "
424                     "that the weak learner supports the calculation of class "
425                     "probabilities with a predict_proba method.\n"
426                     "Please change the base estimator or set "
427                     "algorithm='SAMME' instead.")
428         if not has_fit_parameter(self.base_estimator_, "sample_weight"):
429             raise ValueError("%s doesn't support sample_weight."
430                              % self.base_estimator_.__class__.__name__)
431 
432     def _boost(self, iboost, X, y, sample_weight, random_state):
433         """Implement a single boost.
434 
435         Perform a single boost according to the real multi-class SAMME.R
436         algorithm or to the discrete SAMME algorithm and return the updated
437         sample weights.
438 
439         Parameters
440         ----------
441         iboost : int
442             The index of the current boost iteration.
443 
444         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
445             The training input samples. Sparse matrix can be CSC, CSR, COO,
446             DOK, or LIL. DOK and LIL are converted to CSR.
447 
448         y : array-like of shape = [n_samples]
449             The target values (class labels).
450 
451         sample_weight : array-like of shape = [n_samples]
452             The current sample weights.
453 
454         random_state : RandomState
455             The current random number generator
456 
457         Returns
458         -------
459         sample_weight : array-like of shape = [n_samples] or None
460             The reweighted sample weights.
461             If None then boosting has terminated early.
462 
463         estimator_weight : float
464             The weight for the current boost.
465             If None then boosting has terminated early.
466 
467         estimator_error : float
468             The classification error for the current boost.
469             If None then boosting has terminated early.
470         """
471         if self.algorithm == 'SAMME.R':
472             return self._boost_real(iboost, X, y, sample_weight, random_state)
473 
474         else:  # elif self.algorithm == "SAMME":
475             return self._boost_discrete(iboost, X, y, sample_weight,
476                                         random_state)
477 
478     def _boost_real(self, iboost, X, y, sample_weight, random_state):
479         """Implement a single boost using the SAMME.R real algorithm."""
480         estimator = self._make_estimator(random_state=random_state)
481 
482         estimator.fit(X, y, sample_weight=sample_weight)
483 
484         y_predict_proba = estimator.predict_proba(X)
485 
486         if iboost == 0:
487             self.classes_ = getattr(estimator, 'classes_', None)
488             self.n_classes_ = len(self.classes_)
489 
490         y_predict = self.classes_.take(np.argmax(y_predict_proba, axis=1),
491                                        axis=0)
492 
493         # Instances incorrectly classified
494         incorrect = y_predict != y
495 
496         # Error fraction
497         estimator_error = np.mean(
498             np.average(incorrect, weights=sample_weight, axis=0))
499 
500         # Stop if classification is perfect
501         if estimator_error <= 0:
502             return sample_weight, 1., 0.
503 
504         # Construct y coding as described in Zhu et al [2]:
505         #
506         #    y_k = 1 if c == k else -1 / (K - 1)
507         #
508         # where K == n_classes_ and c, k in [0, K) are indices along the second
509         # axis of the y coding with c being the index corresponding to the true
510         # class label.
511         n_classes = self.n_classes_
512         classes = self.classes_
513         y_codes = np.array([-1. / (n_classes - 1), 1.])
514         y_coding = y_codes.take(classes == y[:, np.newaxis])
515 
516         # Displace zero probabilities so the log is defined.
517         # Also fix negative elements which may occur with
518         # negative sample weights.
519         proba = y_predict_proba  # alias for readability
520         np.clip(proba, np.finfo(proba.dtype).eps, None, out=proba)
521 
522         # Boost weight using multi-class AdaBoost SAMME.R alg
523         estimator_weight = (-1. * self.learning_rate
524                             * ((n_classes - 1.) / n_classes)
525                             * xlogy(y_coding, y_predict_proba).sum(axis=1))
526 
527         # Only boost the weights if it will fit again
528         if not iboost == self.n_estimators - 1:
529             # Only boost positive weights
530             sample_weight *= np.exp(estimator_weight *
531                                     ((sample_weight > 0) |
532                                      (estimator_weight < 0)))
533 
534         return sample_weight, 1., estimator_error
535 
536     def _boost_discrete(self, iboost, X, y, sample_weight, random_state):
537         """Implement a single boost using the SAMME discrete algorithm."""
538         estimator = self._make_estimator(random_state=random_state)
539 
540         estimator.fit(X, y, sample_weight=sample_weight)
541 
542         y_predict = estimator.predict(X)
543 
544         if iboost == 0:
545             self.classes_ = getattr(estimator, 'classes_', None)
546             self.n_classes_ = len(self.classes_)
547 
548         # Instances incorrectly classified
549         incorrect = y_predict != y
550 
551         # Error fraction
552         estimator_error = np.mean(
553             np.average(incorrect, weights=sample_weight, axis=0))
554 
555         # Stop if classification is perfect
556         if estimator_error <= 0:
557             return sample_weight, 1., 0.
558 
559         n_classes = self.n_classes_
560 
561         # Stop if the error is at least as bad as random guessing
562         if estimator_error >= 1. - (1. / n_classes):
563             self.estimators_.pop(-1)
564             if len(self.estimators_) == 0:
565                 raise ValueError('BaseClassifier in AdaBoostClassifier '
566                                  'ensemble is worse than random, ensemble '
567                                  'can not be fit.')
568             return None, None, None
569 
570         # Boost weight using multi-class AdaBoost SAMME alg
571         estimator_weight = self.learning_rate * (
572             np.log((1. - estimator_error) / estimator_error) +
573             np.log(n_classes - 1.))
574 
575         # Only boost the weights if I will fit again
576         if not iboost == self.n_estimators - 1:
577             # Only boost positive weights
578             sample_weight *= np.exp(estimator_weight * incorrect *
579                                     ((sample_weight > 0) |
580                                      (estimator_weight < 0)))
581 
582         return sample_weight, estimator_weight, estimator_error
583 
584     def predict(self, X):
585         """Predict classes for X.
586 
587         The predicted class of an input sample is computed as the weighted mean
588         prediction of the classifiers in the ensemble.
589 
590         Parameters
591         ----------
592         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
593             The training input samples. Sparse matrix can be CSC, CSR, COO,
594             DOK, or LIL. DOK and LIL are converted to CSR.
595 
596         Returns
597         -------
598         y : array of shape = [n_samples]
599             The predicted classes.
600         """
601         pred = self.decision_function(X)
602 
603         if self.n_classes_ == 2:
604             return self.classes_.take(pred > 0, axis=0)
605 
606         return self.classes_.take(np.argmax(pred, axis=1), axis=0)
607 
608     def staged_predict(self, X):
609         """Return staged predictions for X.
610 
611         The predicted class of an input sample is computed as the weighted mean
612         prediction of the classifiers in the ensemble.
613 
614         This generator method yields the ensemble prediction after each
615         iteration of boosting and therefore allows monitoring, such as to
616         determine the prediction on a test set after each boost.
617 
618         Parameters
619         ----------
620         X : array-like of shape = [n_samples, n_features]
621             The input samples.
622 
623         Returns
624         -------
625         y : generator of array, shape = [n_samples]
626             The predicted classes.
627         """
628         n_classes = self.n_classes_
629         classes = self.classes_
630 
631         if n_classes == 2:
632             for pred in self.staged_decision_function(X):
633                 yield np.array(classes.take(pred > 0, axis=0))
634 
635         else:
636             for pred in self.staged_decision_function(X):
637                 yield np.array(classes.take(
638                     np.argmax(pred, axis=1), axis=0))
639 
640     def decision_function(self, X):
641         """Compute the decision function of ``X``.
642 
643         Parameters
644         ----------
645         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
646             The training input samples. Sparse matrix can be CSC, CSR, COO,
647             DOK, or LIL. DOK and LIL are converted to CSR.
648 
649         Returns
650         -------
651         score : array, shape = [n_samples, k]
652             The decision function of the input samples. The order of
653             outputs is the same of that of the `classes_` attribute.
654             Binary classification is a special cases with ``k == 1``,
655             otherwise ``k==n_classes``. For binary classification,
656             values closer to -1 or 1 mean more like the first or second
657             class in ``classes_``, respectively.
658         """
659         check_is_fitted(self, "n_classes_")
660         X = self._validate_X_predict(X)
661 
662         n_classes = self.n_classes_
663         classes = self.classes_[:, np.newaxis]
664 
665         if self.algorithm == 'SAMME.R':
666             # The weights are all 1. for SAMME.R
667             pred = sum(_samme_proba(estimator, n_classes, X)
668                        for estimator in self.estimators_)
669         else:   # self.algorithm == "SAMME"
670             pred = sum((estimator.predict(X) == classes).T * w
671                        for estimator, w in zip(self.estimators_,
672                                                self.estimator_weights_))
673 
674         pred /= self.estimator_weights_.sum()
675         if n_classes == 2:
676             pred[:, 0] *= -1
677             return pred.sum(axis=1)
678         return pred
679 
680     def staged_decision_function(self, X):
681         """Compute decision function of ``X`` for each boosting iteration.
682 
683         This method allows monitoring (i.e. determine error on testing set)
684         after each boosting iteration.
685 
686         Parameters
687         ----------
688         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
689             The training input samples. Sparse matrix can be CSC, CSR, COO,
690             DOK, or LIL. DOK and LIL are converted to CSR.
691 
692         Returns
693         -------
694         score : generator of array, shape = [n_samples, k]
695             The decision function of the input samples. The order of
696             outputs is the same of that of the `classes_` attribute.
697             Binary classification is a special cases with ``k == 1``,
698             otherwise ``k==n_classes``. For binary classification,
699             values closer to -1 or 1 mean more like the first or second
700             class in ``classes_``, respectively.
701         """
702         check_is_fitted(self, "n_classes_")
703         X = self._validate_X_predict(X)
704 
705         n_classes = self.n_classes_
706         classes = self.classes_[:, np.newaxis]
707         pred = None
708         norm = 0.
709 
710         for weight, estimator in zip(self.estimator_weights_,
711                                      self.estimators_):
712             norm += weight
713 
714             if self.algorithm == 'SAMME.R':
715                 # The weights are all 1. for SAMME.R
716                 current_pred = _samme_proba(estimator, n_classes, X)
717             else:  # elif self.algorithm == "SAMME":
718                 current_pred = estimator.predict(X)
719                 current_pred = (current_pred == classes).T * weight
720 
721             if pred is None:
722                 pred = current_pred
723             else:
724                 pred += current_pred
725 
726             if n_classes == 2:
727                 tmp_pred = np.copy(pred)
728                 tmp_pred[:, 0] *= -1
729                 yield (tmp_pred / norm).sum(axis=1)
730             else:
731                 yield pred / norm
732 
733     def predict_proba(self, X):
734         """Predict class probabilities for X.
735 
736         The predicted class probabilities of an input sample is computed as
737         the weighted mean predicted class probabilities of the classifiers
738         in the ensemble.
739 
740         Parameters
741         ----------
742         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
743             The training input samples. Sparse matrix can be CSC, CSR, COO,
744             DOK, or LIL. DOK and LIL are converted to CSR.
745 
746         Returns
747         -------
748         p : array of shape = [n_samples, n_classes]
749             The class probabilities of the input samples. The order of
750             outputs is the same of that of the `classes_` attribute.
751         """
752         check_is_fitted(self, "n_classes_")
753 
754         n_classes = self.n_classes_
755         X = self._validate_X_predict(X)
756 
757         if n_classes == 1:
758             return np.ones((X.shape[0], 1))
759 
760         if self.algorithm == 'SAMME.R':
761             # The weights are all 1. for SAMME.R
762             proba = sum(_samme_proba(estimator, n_classes, X)
763                         for estimator in self.estimators_)
764         else:   # self.algorithm == "SAMME"
765             proba = sum(estimator.predict_proba(X) * w
766                         for estimator, w in zip(self.estimators_,
767                                                 self.estimator_weights_))
768 
769         proba /= self.estimator_weights_.sum()
770         proba = np.exp((1. / (n_classes - 1)) * proba)
771         normalizer = proba.sum(axis=1)[:, np.newaxis]
772         normalizer[normalizer == 0.0] = 1.0
773         proba /= normalizer
774 
775         return proba
776 
777     def staged_predict_proba(self, X):
778         """Predict class probabilities for X.
779 
780         The predicted class probabilities of an input sample is computed as
781         the weighted mean predicted class probabilities of the classifiers
782         in the ensemble.
783 
784         This generator method yields the ensemble predicted class probabilities
785         after each iteration of boosting and therefore allows monitoring, such
786         as to determine the predicted class probabilities on a test set after
787         each boost.
788 
789         Parameters
790         ----------
791         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
792             The training input samples. Sparse matrix can be CSC, CSR, COO,
793             DOK, or LIL. DOK and LIL are converted to CSR.
794 
795         Returns
796         -------
797         p : generator of array, shape = [n_samples]
798             The class probabilities of the input samples. The order of
799             outputs is the same of that of the `classes_` attribute.
800         """
801         X = self._validate_X_predict(X)
802 
803         n_classes = self.n_classes_
804         proba = None
805         norm = 0.
806 
807         for weight, estimator in zip(self.estimator_weights_,
808                                      self.estimators_):
809             norm += weight
810 
811             if self.algorithm == 'SAMME.R':
812                 # The weights are all 1. for SAMME.R
813                 current_proba = _samme_proba(estimator, n_classes, X)
814             else:  # elif self.algorithm == "SAMME":
815                 current_proba = estimator.predict_proba(X) * weight
816 
817             if proba is None:
818                 proba = current_proba
819             else:
820                 proba += current_proba
821 
822             real_proba = np.exp((1. / (n_classes - 1)) * (proba / norm))
823             normalizer = real_proba.sum(axis=1)[:, np.newaxis]
824             normalizer[normalizer == 0.0] = 1.0
825             real_proba /= normalizer
826 
827             yield real_proba
828 
829     def predict_log_proba(self, X):
830         """Predict class log-probabilities for X.
831 
832         The predicted class log-probabilities of an input sample is computed as
833         the weighted mean predicted class log-probabilities of the classifiers
834         in the ensemble.
835 
836         Parameters
837         ----------
838         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
839             The training input samples. Sparse matrix can be CSC, CSR, COO,
840             DOK, or LIL. DOK and LIL are converted to CSR.
841 
842         Returns
843         -------
844         p : array of shape = [n_samples, n_classes]
845             The class probabilities of the input samples. The order of
846             outputs is the same of that of the `classes_` attribute.
847         """
848         return np.log(self.predict_proba(X))
849 
850 
851 class AdaBoostRegressor(BaseWeightBoosting, RegressorMixin):
852     """An AdaBoost regressor.
853 
854     An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
855     regressor on the original dataset and then fits additional copies of the
856     regressor on the same dataset but where the weights of instances are
857     adjusted according to the error of the current prediction. As such,
858     subsequent regressors focus more on difficult cases.
859 
860     This class implements the algorithm known as AdaBoost.R2 [2].
861 
862     Read more in the :ref:`User Guide <adaboost>`.
863 
864     Parameters
865     ----------
866     base_estimator : object, optional (default=None)
867         The base estimator from which the boosted ensemble is built.
868         Support for sample weighting is required. If ``None``, then
869         the base estimator is ``DecisionTreeRegressor(max_depth=3)``
870 
871     n_estimators : integer, optional (default=50)
872         The maximum number of estimators at which boosting is terminated.
873         In case of perfect fit, the learning procedure is stopped early.
874 
875     learning_rate : float, optional (default=1.)
876         Learning rate shrinks the contribution of each regressor by
877         ``learning_rate``. There is a trade-off between ``learning_rate`` and
878         ``n_estimators``.
879 
880     loss : {'linear', 'square', 'exponential'}, optional (default='linear')
881         The loss function to use when updating the weights after each
882         boosting iteration.
883 
884     random_state : int, RandomState instance or None, optional (default=None)
885         If int, random_state is the seed used by the random number generator;
886         If RandomState instance, random_state is the random number generator;
887         If None, the random number generator is the RandomState instance used
888         by `np.random`.
889 
890     Attributes
891     ----------
892     estimators_ : list of classifiers
893         The collection of fitted sub-estimators.
894 
895     estimator_weights_ : array of floats
896         Weights for each estimator in the boosted ensemble.
897 
898     estimator_errors_ : array of floats
899         Regression error for each estimator in the boosted ensemble.
900 
901     feature_importances_ : array of shape = [n_features]
902         The feature importances if supported by the ``base_estimator``.
903 
904     See also
905     --------
906     AdaBoostClassifier, GradientBoostingRegressor,
907     sklearn.tree.DecisionTreeRegressor
908 
909     References
910     ----------
911     .. [1] Y. Freund, R. Schapire, "A Decision-Theoretic Generalization of
912            on-Line Learning and an Application to Boosting", 1995.
913 
914     .. [2] H. Drucker, "Improving Regressors using Boosting Techniques", 1997.
915 
916     """
917     def __init__(self,
918                  base_estimator=None,
919                  n_estimators=50,
920                  learning_rate=1.,
921                  loss='linear',
922                  random_state=None):
923 
924         super().__init__(
925             base_estimator=base_estimator,
926             n_estimators=n_estimators,
927             learning_rate=learning_rate,
928             random_state=random_state)
929 
930         self.loss = loss
931         self.random_state = random_state
932 
933     def fit(self, X, y, sample_weight=None):
934         """Build a boosted regressor from the training set (X, y).
935 
936         Parameters
937         ----------
938         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
939             The training input samples. Sparse matrix can be CSC, CSR, COO,
940             DOK, or LIL. DOK and LIL are converted to CSR.
941 
942         y : array-like of shape = [n_samples]
943             The target values (real numbers).
944 
945         sample_weight : array-like of shape = [n_samples], optional
946             Sample weights. If None, the sample weights are initialized to
947             1 / n_samples.
948 
949         Returns
950         -------
951         self : object
952         """
953         # Check loss
954         if self.loss not in ('linear', 'square', 'exponential'):
955             raise ValueError(
956                 "loss must be 'linear', 'square', or 'exponential'")
957 
958         # Fit
959         return super().fit(X, y, sample_weight)
960 
961     def _validate_estimator(self):
962         """Check the estimator and set the base_estimator_ attribute."""
963         super()._validate_estimator(
964             default=DecisionTreeRegressor(max_depth=3))
965 
966     def _boost(self, iboost, X, y, sample_weight, random_state):
967         """Implement a single boost for regression
968 
969         Perform a single boost according to the AdaBoost.R2 algorithm and
970         return the updated sample weights.
971 
972         Parameters
973         ----------
974         iboost : int
975             The index of the current boost iteration.
976 
977         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
978             The training input samples. Sparse matrix can be CSC, CSR, COO,
979             DOK, or LIL. DOK and LIL are converted to CSR.
980 
981         y : array-like of shape = [n_samples]
982             The target values (class labels in classification, real numbers in
983             regression).
984 
985         sample_weight : array-like of shape = [n_samples]
986             The current sample weights.
987 
988         random_state : RandomState
989             The current random number generator
990 
991         Returns
992         -------
993         sample_weight : array-like of shape = [n_samples] or None
994             The reweighted sample weights.
995             If None then boosting has terminated early.
996 
997         estimator_weight : float
998             The weight for the current boost.
999             If None then boosting has terminated early.
1000 
1001         estimator_error : float
1002             The regression error for the current boost.
1003             If None then boosting has terminated early.
1004         """
1005         estimator = self._make_estimator(random_state=random_state)
1006 
1007         # Weighted sampling of the training set with replacement
1008         # For NumPy >= 1.7.0 use np.random.choice
1009         cdf = stable_cumsum(sample_weight)
1010         cdf /= cdf[-1]
1011         uniform_samples = random_state.random_sample(X.shape[0])
1012         bootstrap_idx = cdf.searchsorted(uniform_samples, side='right')
1013         # searchsorted returns a scalar
1014         bootstrap_idx = np.array(bootstrap_idx, copy=False)
1015 
1016         # Fit on the bootstrapped sample and obtain a prediction
1017         # for all samples in the training set
1018         estimator.fit(X[bootstrap_idx], y[bootstrap_idx])
1019         y_predict = estimator.predict(X)
1020 
1021         error_vect = np.abs(y_predict - y)
1022         error_max = error_vect.max()
1023 
1024         if error_max != 0.:
1025             error_vect /= error_max
1026 
1027         if self.loss == 'square':
1028             error_vect **= 2
1029         elif self.loss == 'exponential':
1030             error_vect = 1. - np.exp(- error_vect)
1031 
1032         # Calculate the average loss
1033         estimator_error = (sample_weight * error_vect).sum()
1034 
1035         if estimator_error <= 0:
1036             # Stop if fit is perfect
1037             return sample_weight, 1., 0.
1038 
1039         elif estimator_error >= 0.5:
1040             # Discard current estimator only if it isn't the only one
1041             if len(self.estimators_) > 1:
1042                 self.estimators_.pop(-1)
1043             return None, None, None
1044 
1045         beta = estimator_error / (1. - estimator_error)
1046 
1047         # Boost weight using AdaBoost.R2 alg
1048         estimator_weight = self.learning_rate * np.log(1. / beta)
1049 
1050         if not iboost == self.n_estimators - 1:
1051             sample_weight *= np.power(
1052                 beta,
1053                 (1. - error_vect) * self.learning_rate)
1054 
1055         return sample_weight, estimator_weight, estimator_error
1056 
1057     def _get_median_predict(self, X, limit):
1058         # Evaluate predictions of all estimators
1059         predictions = np.array([
1060             est.predict(X) for est in self.estimators_[:limit]]).T
1061 
1062         # Sort the predictions
1063         sorted_idx = np.argsort(predictions, axis=1)
1064 
1065         # Find index of median prediction for each sample
1066         weight_cdf = stable_cumsum(self.estimator_weights_[sorted_idx], axis=1)
1067         median_or_above = weight_cdf >= 0.5 * weight_cdf[:, -1][:, np.newaxis]
1068         median_idx = median_or_above.argmax(axis=1)
1069 
1070         median_estimators = sorted_idx[np.arange(X.shape[0]), median_idx]
1071 
1072         # Return median predictions
1073         return predictions[np.arange(X.shape[0]), median_estimators]
1074 
1075     def predict(self, X):
1076         """Predict regression value for X.
1077 
1078         The predicted regression value of an input sample is computed
1079         as the weighted median prediction of the classifiers in the ensemble.
1080 
1081         Parameters
1082         ----------
1083         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1084             The training input samples. Sparse matrix can be CSC, CSR, COO,
1085             DOK, or LIL. DOK and LIL are converted to CSR.
1086 
1087         Returns
1088         -------
1089         y : array of shape = [n_samples]
1090             The predicted regression values.
1091         """
1092         check_is_fitted(self, "estimator_weights_")
1093         X = self._validate_X_predict(X)
1094 
1095         return self._get_median_predict(X, len(self.estimators_))
1096 
1097     def staged_predict(self, X):
1098         """Return staged predictions for X.
1099 
1100         The predicted regression value of an input sample is computed
1101         as the weighted median prediction of the classifiers in the ensemble.
1102 
1103         This generator method yields the ensemble prediction after each
1104         iteration of boosting and therefore allows monitoring, such as to
1105         determine the prediction on a test set after each boost.
1106 
1107         Parameters
1108         ----------
1109         X : {array-like, sparse matrix} of shape = [n_samples, n_features]
1110             The training input samples. Sparse matrix can be CSC, CSR, COO,
1111             DOK, or LIL. DOK and LIL are converted to CSR.
1112 
1113         Returns
1114         -------
1115         y : generator of array, shape = [n_samples]
1116             The predicted regression values.
1117         """
1118         check_is_fitted(self, "estimator_weights_")
1119         X = self._validate_X_predict(X)
1120 
1121         for i, _ in enumerate(self.estimators_, 1):
1122             yield self._get_median_predict(X, limit=i)

</code><intended_behavior>This file likely handles the AdaBoost algorithm and potentially others. The validation of X should be flexible enough to allow various data types beyond arrays or sparse matrices, as long as the base estimator can process the input data type. This means adjusting or extending the validate_X utility or the validation logic within fit, predict_proba, and predict methods to not outright reject input types like lists of text blobs if the base estimator can handle them. Preserve existing checks that ensure the input is compatible with the base estimator's requirements, perhaps by introducing a pre-validation step that checks if the input type is supported by the base estimator before proceeding with any further validation or processing.</intended_behavior>, <file>sklearn/ensemble/forest.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Forest of trees-based ensemble methods
2 
3 Those methods include random forests and extremely randomized trees.
4 
5 The module structure is the following:
6 
7 - The ``BaseForest`` base class implements a common ``fit`` method for all
8   the estimators in the module. The ``fit`` method of the base ``Forest``
9   class calls the ``fit`` method of each sub-estimator on random samples
10   (with replacement, a.k.a. bootstrap) of the training set.
11 
12   The init of the sub-estimator is further delegated to the
13   ``BaseEnsemble`` constructor.
14 
15 - The ``ForestClassifier`` and ``ForestRegressor`` base classes further
16   implement the prediction logic by computing an average of the predicted
17   outcomes of the sub-estimators.
18 
19 - The ``RandomForestClassifier`` and ``RandomForestRegressor`` derived
20   classes provide the user with concrete implementations of
21   the forest ensemble method using classical, deterministic
22   ``DecisionTreeClassifier`` and ``DecisionTreeRegressor`` as
23   sub-estimator implementations.
24 
25 - The ``ExtraTreesClassifier`` and ``ExtraTreesRegressor`` derived
26   classes provide the user with concrete implementations of the
27   forest ensemble method using the extremely randomized trees
28   ``ExtraTreeClassifier`` and ``ExtraTreeRegressor`` as
29   sub-estimator implementations.
30 
31 Single and multi-output problems are both handled.
32 
33 """
34 
35 # Authors: Gilles Louppe <g.louppe@gmail.com>
36 #          Brian Holt <bdholt1@gmail.com>
37 #          Joly Arnaud <arnaud.v.joly@gmail.com>
38 #          Fares Hedayati <fares.hedayati@gmail.com>
39 #
40 # License: BSD 3 clause
41 
42 
43 from warnings import catch_warnings, simplefilter, warn
44 import threading
45 
46 from abc import ABCMeta, abstractmethod
47 import numpy as np
48 from scipy.sparse import issparse
49 from scipy.sparse import hstack as sparse_hstack
50 
51 from ..base import ClassifierMixin, RegressorMixin
52 from ..utils._joblib import Parallel, delayed
53 from ..metrics import r2_score
54 from ..preprocessing import OneHotEncoder
55 from ..tree import (DecisionTreeClassifier, DecisionTreeRegressor,
56                     ExtraTreeClassifier, ExtraTreeRegressor)
57 from ..tree._tree import DTYPE, DOUBLE
58 from ..utils import check_random_state, check_array, compute_sample_weight
59 from ..exceptions import DataConversionWarning, NotFittedError
60 from .base import BaseEnsemble, _partition_estimators
61 from ..utils.fixes import parallel_helper, _joblib_parallel_args
62 from ..utils.multiclass import check_classification_targets
63 from ..utils.validation import check_is_fitted
64 
65 __all__ = ["RandomForestClassifier",
66            "RandomForestRegressor",
67            "ExtraTreesClassifier",
68            "ExtraTreesRegressor",
69            "RandomTreesEmbedding"]
70 
71 MAX_INT = np.iinfo(np.int32).max
72 
73 
74 def _generate_sample_indices(random_state, n_samples):
75     """Private function used to _parallel_build_trees function."""
76     random_instance = check_random_state(random_state)
77     sample_indices = random_instance.randint(0, n_samples, n_samples)
78 
79     return sample_indices
80 
81 
82 def _generate_unsampled_indices(random_state, n_samples):
83     """Private function used to forest._set_oob_score function."""
84     sample_indices = _generate_sample_indices(random_state, n_samples)
85     sample_counts = np.bincount(sample_indices, minlength=n_samples)
86     unsampled_mask = sample_counts == 0
87     indices_range = np.arange(n_samples)
88     unsampled_indices = indices_range[unsampled_mask]
89 
90     return unsampled_indices
91 
92 
93 def _parallel_build_trees(tree, forest, X, y, sample_weight, tree_idx, n_trees,
94                           verbose=0, class_weight=None):
95     """Private function used to fit a single tree in parallel."""
96     if verbose > 1:
97         print("building tree %d of %d" % (tree_idx + 1, n_trees))
98 
99     if forest.bootstrap:
100         n_samples = X.shape[0]
101         if sample_weight is None:
102             curr_sample_weight = np.ones((n_samples,), dtype=np.float64)
103         else:
104             curr_sample_weight = sample_weight.copy()
105 
106         indices = _generate_sample_indices(tree.random_state, n_samples)
107         sample_counts = np.bincount(indices, minlength=n_samples)
108         curr_sample_weight *= sample_counts
109 
110         if class_weight == 'subsample':
111             with catch_warnings():
112                 simplefilter('ignore', DeprecationWarning)
113                 curr_sample_weight *= compute_sample_weight('auto', y, indices)
114         elif class_weight == 'balanced_subsample':
115             curr_sample_weight *= compute_sample_weight('balanced', y, indices)
116 
117         tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)
118     else:
119         tree.fit(X, y, sample_weight=sample_weight, check_input=False)
120 
121     return tree
122 
123 
124 class BaseForest(BaseEnsemble, metaclass=ABCMeta):
125     """Base class for forests of trees.
126 
127     Warning: This class should not be used directly. Use derived classes
128     instead.
129     """
130 
131     @abstractmethod
132     def __init__(self,
133                  base_estimator,
134                  n_estimators=100,
135                  estimator_params=tuple(),
136                  bootstrap=False,
137                  oob_score=False,
138                  n_jobs=None,
139                  random_state=None,
140                  verbose=0,
141                  warm_start=False,
142                  class_weight=None):
143         super().__init__(
144             base_estimator=base_estimator,
145             n_estimators=n_estimators,
146             estimator_params=estimator_params)
147 
148         self.bootstrap = bootstrap
149         self.oob_score = oob_score
150         self.n_jobs = n_jobs
151         self.random_state = random_state
152         self.verbose = verbose
153         self.warm_start = warm_start
154         self.class_weight = class_weight
155 
156     def apply(self, X):
157         """Apply trees in the forest to X, return leaf indices.
158 
159         Parameters
160         ----------
161         X : array-like or sparse matrix, shape = [n_samples, n_features]
162             The input samples. Internally, its dtype will be converted to
163             ``dtype=np.float32``. If a sparse matrix is provided, it will be
164             converted into a sparse ``csr_matrix``.
165 
166         Returns
167         -------
168         X_leaves : array_like, shape = [n_samples, n_estimators]
169             For each datapoint x in X and for each tree in the forest,
170             return the index of the leaf x ends up in.
171         """
172         X = self._validate_X_predict(X)
173         results = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
174                            **_joblib_parallel_args(prefer="threads"))(
175             delayed(parallel_helper)(tree, 'apply', X, check_input=False)
176             for tree in self.estimators_)
177 
178         return np.array(results).T
179 
180     def decision_path(self, X):
181         """Return the decision path in the forest
182 
183         .. versionadded:: 0.18
184 
185         Parameters
186         ----------
187         X : array-like or sparse matrix, shape = [n_samples, n_features]
188             The input samples. Internally, its dtype will be converted to
189             ``dtype=np.float32``. If a sparse matrix is provided, it will be
190             converted into a sparse ``csr_matrix``.
191 
192         Returns
193         -------
194         indicator : sparse csr array, shape = [n_samples, n_nodes]
195             Return a node indicator matrix where non zero elements
196             indicates that the samples goes through the nodes.
197 
198         n_nodes_ptr : array of size (n_estimators + 1, )
199             The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
200             gives the indicator value for the i-th estimator.
201 
202         """
203         X = self._validate_X_predict(X)
204         indicators = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
205                               **_joblib_parallel_args(prefer='threads'))(
206             delayed(parallel_helper)(tree, 'decision_path', X,
207                                      check_input=False)
208             for tree in self.estimators_)
209 
210         n_nodes = [0]
211         n_nodes.extend([i.shape[1] for i in indicators])
212         n_nodes_ptr = np.array(n_nodes).cumsum()
213 
214         return sparse_hstack(indicators).tocsr(), n_nodes_ptr
215 
216     def fit(self, X, y, sample_weight=None):
217         """Build a forest of trees from the training set (X, y).
218 
219         Parameters
220         ----------
221         X : array-like or sparse matrix of shape = [n_samples, n_features]
222             The training input samples. Internally, its dtype will be converted
223             to ``dtype=np.float32``. If a sparse matrix is provided, it will be
224             converted into a sparse ``csc_matrix``.
225 
226         y : array-like, shape = [n_samples] or [n_samples, n_outputs]
227             The target values (class labels in classification, real numbers in
228             regression).
229 
230         sample_weight : array-like, shape = [n_samples] or None
231             Sample weights. If None, then samples are equally weighted. Splits
232             that would create child nodes with net zero or negative weight are
233             ignored while searching for a split in each node. In the case of
234             classification, splits are also ignored if they would result in any
235             single class carrying a negative weight in either child node.
236 
237         Returns
238         -------
239         self : object
240         """
241 
242         if self.n_estimators == 'warn':
243             warn("The default value of n_estimators will change from "
244                  "10 in version 0.20 to 100 in 0.22.", FutureWarning)
245             self.n_estimators = 10
246 
247         # Validate or convert input data
248         X = check_array(X, accept_sparse="csc", dtype=DTYPE)
249         y = check_array(y, accept_sparse='csc', ensure_2d=False, dtype=None)
250         if sample_weight is not None:
251             sample_weight = check_array(sample_weight, ensure_2d=False)
252         if issparse(X):
253             # Pre-sort indices to avoid that each individual tree of the
254             # ensemble sorts the indices.
255             X.sort_indices()
256 
257         # Remap output
258         self.n_features_ = X.shape[1]
259 
260         y = np.atleast_1d(y)
261         if y.ndim == 2 and y.shape[1] == 1:
262             warn("A column-vector y was passed when a 1d array was"
263                  " expected. Please change the shape of y to "
264                  "(n_samples,), for example using ravel().",
265                  DataConversionWarning, stacklevel=2)
266 
267         if y.ndim == 1:
268             # reshape is necessary to preserve the data contiguity against vs
269             # [:, np.newaxis] that does not.
270             y = np.reshape(y, (-1, 1))
271 
272         self.n_outputs_ = y.shape[1]
273 
274         y, expanded_class_weight = self._validate_y_class_weight(y)
275 
276         if getattr(y, "dtype", None) != DOUBLE or not y.flags.contiguous:
277             y = np.ascontiguousarray(y, dtype=DOUBLE)
278 
279         if expanded_class_weight is not None:
280             if sample_weight is not None:
281                 sample_weight = sample_weight * expanded_class_weight
282             else:
283                 sample_weight = expanded_class_weight
284 
285         # Check parameters
286         self._validate_estimator()
287 
288         if not self.bootstrap and self.oob_score:
289             raise ValueError("Out of bag estimation only available"
290                              " if bootstrap=True")
291 
292         random_state = check_random_state(self.random_state)
293 
294         if not self.warm_start or not hasattr(self, "estimators_"):
295             # Free allocated memory, if any
296             self.estimators_ = []
297 
298         n_more_estimators = self.n_estimators - len(self.estimators_)
299 
300         if n_more_estimators < 0:
301             raise ValueError('n_estimators=%d must be larger or equal to '
302                              'len(estimators_)=%d when warm_start==True'
303                              % (self.n_estimators, len(self.estimators_)))
304 
305         elif n_more_estimators == 0:
306             warn("Warm-start fitting without increasing n_estimators does not "
307                  "fit new trees.")
308         else:
309             if self.warm_start and len(self.estimators_) > 0:
310                 # We draw from the random state to get the random state we
311                 # would have got if we hadn't used a warm_start.
312                 random_state.randint(MAX_INT, size=len(self.estimators_))
313 
314             trees = [self._make_estimator(append=False,
315                                           random_state=random_state)
316                      for i in range(n_more_estimators)]
317 
318             # Parallel loop: we prefer the threading backend as the Cython code
319             # for fitting the trees is internally releasing the Python GIL
320             # making threading more efficient than multiprocessing in
321             # that case. However, for joblib 0.12+ we respect any
322             # parallel_backend contexts set at a higher level,
323             # since correctness does not rely on using threads.
324             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,
325                              **_joblib_parallel_args(prefer='threads'))(
326                 delayed(_parallel_build_trees)(
327                     t, self, X, y, sample_weight, i, len(trees),
328                     verbose=self.verbose, class_weight=self.class_weight)
329                 for i, t in enumerate(trees))
330 
331             # Collect newly grown trees
332             self.estimators_.extend(trees)
333 
334         if self.oob_score:
335             self._set_oob_score(X, y)
336 
337         # Decapsulate classes_ attributes
338         if hasattr(self, "classes_") and self.n_outputs_ == 1:
339             self.n_classes_ = self.n_classes_[0]
340             self.classes_ = self.classes_[0]
341 
342         return self
343 
344     @abstractmethod
345     def _set_oob_score(self, X, y):
346         """Calculate out of bag predictions and score."""
347 
348     def _validate_y_class_weight(self, y):
349         # Default implementation
350         return y, None
351 
352     def _validate_X_predict(self, X):
353         """Validate X whenever one tries to predict, apply, predict_proba"""
354         if self.estimators_ is None or len(self.estimators_) == 0:
355             raise NotFittedError("Estimator not fitted, "
356                                  "call `fit` before exploiting the model.")
357 
358         return self.estimators_[0]._validate_X_predict(X, check_input=True)
359 
360     @property
361     def feature_importances_(self):
362         """Return the feature importances (the higher, the more important the
363            feature).
364 
365         Returns
366         -------
367         feature_importances_ : array, shape = [n_features]
368         """
369         check_is_fitted(self, 'estimators_')
370 
371         all_importances = Parallel(n_jobs=self.n_jobs,
372                                    **_joblib_parallel_args(prefer='threads'))(
373             delayed(getattr)(tree, 'feature_importances_')
374             for tree in self.estimators_)
375 
376         return sum(all_importances) / len(self.estimators_)
377 
378 
379 def _accumulate_prediction(predict, X, out, lock):
380     """This is a utility function for joblib's Parallel.
381 
382     It can't go locally in ForestClassifier or ForestRegressor, because joblib
383     complains that it cannot pickle it when placed there.
384     """
385     prediction = predict(X, check_input=False)
386     with lock:
387         if len(out) == 1:
388             out[0] += prediction
389         else:
390             for i in range(len(out)):
391                 out[i] += prediction[i]
392 
393 
394 class ForestClassifier(BaseForest, ClassifierMixin, metaclass=ABCMeta):
395     """Base class for forest of trees-based classifiers.
396 
397     Warning: This class should not be used directly. Use derived classes
398     instead.
399     """
400 
401     @abstractmethod
402     def __init__(self,
403                  base_estimator,
404                  n_estimators=100,
405                  estimator_params=tuple(),
406                  bootstrap=False,
407                  oob_score=False,
408                  n_jobs=None,
409                  random_state=None,
410                  verbose=0,
411                  warm_start=False,
412                  class_weight=None):
413         super().__init__(
414             base_estimator,
415             n_estimators=n_estimators,
416             estimator_params=estimator_params,
417             bootstrap=bootstrap,
418             oob_score=oob_score,
419             n_jobs=n_jobs,
420             random_state=random_state,
421             verbose=verbose,
422             warm_start=warm_start,
423             class_weight=class_weight)
424 
425     def _set_oob_score(self, X, y):
426         """Compute out-of-bag score"""
427         X = check_array(X, dtype=DTYPE, accept_sparse='csr')
428 
429         n_classes_ = self.n_classes_
430         n_samples = y.shape[0]
431 
432         oob_decision_function = []
433         oob_score = 0.0
434         predictions = [np.zeros((n_samples, n_classes_[k]))
435                        for k in range(self.n_outputs_)]
436 
437         for estimator in self.estimators_:
438             unsampled_indices = _generate_unsampled_indices(
439                 estimator.random_state, n_samples)
440             p_estimator = estimator.predict_proba(X[unsampled_indices, :],
441                                                   check_input=False)
442 
443             if self.n_outputs_ == 1:
444                 p_estimator = [p_estimator]
445 
446             for k in range(self.n_outputs_):
447                 predictions[k][unsampled_indices, :] += p_estimator[k]
448 
449         for k in range(self.n_outputs_):
450             if (predictions[k].sum(axis=1) == 0).any():
451                 warn("Some inputs do not have OOB scores. "
452                      "This probably means too few trees were used "
453                      "to compute any reliable oob estimates.")
454 
455             decision = (predictions[k] /
456                         predictions[k].sum(axis=1)[:, np.newaxis])
457             oob_decision_function.append(decision)
458             oob_score += np.mean(y[:, k] ==
459                                  np.argmax(predictions[k], axis=1), axis=0)
460 
461         if self.n_outputs_ == 1:
462             self.oob_decision_function_ = oob_decision_function[0]
463         else:
464             self.oob_decision_function_ = oob_decision_function
465 
466         self.oob_score_ = oob_score / self.n_outputs_
467 
468     def _validate_y_class_weight(self, y):
469         check_classification_targets(y)
470 
471         y = np.copy(y)
472         expanded_class_weight = None
473 
474         if self.class_weight is not None:
475             y_original = np.copy(y)
476 
477         self.classes_ = []
478         self.n_classes_ = []
479 
480         y_store_unique_indices = np.zeros(y.shape, dtype=np.int)
481         for k in range(self.n_outputs_):
482             classes_k, y_store_unique_indices[:, k] = np.unique(y[:, k], return_inverse=True)
483             self.classes_.append(classes_k)
484             self.n_classes_.append(classes_k.shape[0])
485         y = y_store_unique_indices
486 
487         if self.class_weight is not None:
488             valid_presets = ('balanced', 'balanced_subsample')
489             if isinstance(self.class_weight, str):
490                 if self.class_weight not in valid_presets:
491                     raise ValueError('Valid presets for class_weight include '
492                                      '"balanced" and "balanced_subsample". Given "%s".'
493                                      % self.class_weight)
494                 if self.warm_start:
495                     warn('class_weight presets "balanced" or "balanced_subsample" are '
496                          'not recommended for warm_start if the fitted data '
497                          'differs from the full dataset. In order to use '
498                          '"balanced" weights, use compute_class_weight("balanced", '
499                          'classes, y). In place of y you can use a large '
500                          'enough sample of the full training set target to '
501                          'properly estimate the class frequency '
502                          'distributions. Pass the resulting weights as the '
503                          'class_weight parameter.')
504 
505             if (self.class_weight != 'balanced_subsample' or
506                     not self.bootstrap):
507                 if self.class_weight == "balanced_subsample":
508                     class_weight = "balanced"
509                 else:
510                     class_weight = self.class_weight
511                 expanded_class_weight = compute_sample_weight(class_weight,
512                                                               y_original)
513 
514         return y, expanded_class_weight
515 
516     def predict(self, X):
517         """Predict class for X.
518 
519         The predicted class of an input sample is a vote by the trees in
520         the forest, weighted by their probability estimates. That is,
521         the predicted class is the one with highest mean probability
522         estimate across the trees.
523 
524         Parameters
525         ----------
526         X : array-like or sparse matrix of shape = [n_samples, n_features]
527             The input samples. Internally, its dtype will be converted to
528             ``dtype=np.float32``. If a sparse matrix is provided, it will be
529             converted into a sparse ``csr_matrix``.
530 
531         Returns
532         -------
533         y : array of shape = [n_samples] or [n_samples, n_outputs]
534             The predicted classes.
535         """
536         proba = self.predict_proba(X)
537 
538         if self.n_outputs_ == 1:
539             return self.classes_.take(np.argmax(proba, axis=1), axis=0)
540 
541         else:
542             n_samples = proba[0].shape[0]
543             # all dtypes should be the same, so just take the first
544             class_type = self.classes_[0].dtype
545             predictions = np.empty((n_samples, self.n_outputs_),
546                                    dtype=class_type)
547 
548             for k in range(self.n_outputs_):
549                 predictions[:, k] = self.classes_[k].take(np.argmax(proba[k],
550                                                                     axis=1),
551                                                           axis=0)
552 
553             return predictions
554 
555     def predict_proba(self, X):
556         """Predict class probabilities for X.
557 
558         The predicted class probabilities of an input sample are computed as
559         the mean predicted class probabilities of the trees in the forest. The
560         class probability of a single tree is the fraction of samples of the same
561         class in a leaf.
562 
563         Parameters
564         ----------
565         X : array-like or sparse matrix of shape = [n_samples, n_features]
566             The input samples. Internally, its dtype will be converted to
567             ``dtype=np.float32``. If a sparse matrix is provided, it will be
568             converted into a sparse ``csr_matrix``.
569 
570         Returns
571         -------
572         p : array of shape = [n_samples, n_classes], or a list of n_outputs
573             such arrays if n_outputs > 1.
574             The class probabilities of the input samples. The order of the
575             classes corresponds to that in the attribute `classes_`.
576         """
577         check_is_fitted(self, 'estimators_')
578         # Check data
579         X = self._validate_X_predict(X)
580 
581         # Assign chunk of trees to jobs
582         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
583 
584         # avoid storing the output of every estimator by summing them here
585         all_proba = [np.zeros((X.shape[0], j), dtype=np.float64)
586                      for j in np.atleast_1d(self.n_classes_)]
587         lock = threading.Lock()
588         Parallel(n_jobs=n_jobs, verbose=self.verbose,
589                  **_joblib_parallel_args(require="sharedmem"))(
590             delayed(_accumulate_prediction)(e.predict_proba, X, all_proba,
591                                             lock)
592             for e in self.estimators_)
593 
594         for proba in all_proba:
595             proba /= len(self.estimators_)
596 
597         if len(all_proba) == 1:
598             return all_proba[0]
599         else:
600             return all_proba
601 
602     def predict_log_proba(self, X):
603         """Predict class log-probabilities for X.
604 
605         The predicted class log-probabilities of an input sample is computed as
606         the log of the mean predicted class probabilities of the trees in the
607         forest.
608 
609         Parameters
610         ----------
611         X : array-like or sparse matrix of shape = [n_samples, n_features]
612             The input samples. Internally, its dtype will be converted to
613             ``dtype=np.float32``. If a sparse matrix is provided, it will be
614             converted into a sparse ``csr_matrix``.
615 
616         Returns
617         -------
618         p : array of shape = [n_samples, n_classes], or a list of n_outputs
619             such arrays if n_outputs > 1.
620             The class probabilities of the input samples. The order of the
621             classes corresponds to that in the attribute `classes_`.
622         """
623         proba = self.predict_proba(X)
624 
625         if self.n_outputs_ == 1:
626             return np.log(proba)
627 
628         else:
629             for k in range(self.n_outputs_):
630                 proba[k] = np.log(proba[k])
631 
632             return proba
633 
634 
635 class ForestRegressor(BaseForest, RegressorMixin, metaclass=ABCMeta):
636     """Base class for forest of trees-based regressors.
637 
638     Warning: This class should not be used directly. Use derived classes
639     instead.
640     """
641 
642     @abstractmethod
643     def __init__(self,
644                  base_estimator,
645                  n_estimators=100,
646                  estimator_params=tuple(),
647                  bootstrap=False,
648                  oob_score=False,
649                  n_jobs=None,
650                  random_state=None,
651                  verbose=0,
652                  warm_start=False):
653         super().__init__(
654             base_estimator,
655             n_estimators=n_estimators,
656             estimator_params=estimator_params,
657             bootstrap=bootstrap,
658             oob_score=oob_score,
659             n_jobs=n_jobs,
660             random_state=random_state,
661             verbose=verbose,
662             warm_start=warm_start)
663 
664     def predict(self, X):
665         """Predict regression target for X.
666 
667         The predicted regression target of an input sample is computed as the
668         mean predicted regression targets of the trees in the forest.
669 
670         Parameters
671         ----------
672         X : array-like or sparse matrix of shape = [n_samples, n_features]
673             The input samples. Internally, its dtype will be converted to
674             ``dtype=np.float32``. If a sparse matrix is provided, it will be
675             converted into a sparse ``csr_matrix``.
676 
677         Returns
678         -------
679         y : array of shape = [n_samples] or [n_samples, n_outputs]
680             The predicted values.
681         """
682         check_is_fitted(self, 'estimators_')
683         # Check data
684         X = self._validate_X_predict(X)
685 
686         # Assign chunk of trees to jobs
687         n_jobs, _, _ = _partition_estimators(self.n_estimators, self.n_jobs)
688 
689         # avoid storing the output of every estimator by summing them here
690         if self.n_outputs_ > 1:
691             y_hat = np.zeros((X.shape[0], self.n_outputs_), dtype=np.float64)
692         else:
693             y_hat = np.zeros((X.shape[0]), dtype=np.float64)
694 
695         # Parallel loop
696         lock = threading.Lock()
697         Parallel(n_jobs=n_jobs, verbose=self.verbose,
698                  **_joblib_parallel_args(require="sharedmem"))(
699             delayed(_accumulate_prediction)(e.predict, X, [y_hat], lock)
700             for e in self.estimators_)
701 
702         y_hat /= len(self.estimators_)
703 
704         return y_hat
705 
706     def _set_oob_score(self, X, y):
707         """Compute out-of-bag scores"""
708         X = check_array(X, dtype=DTYPE, accept_sparse='csr')
709 
710         n_samples = y.shape[0]
711 
712         predictions = np.zeros((n_samples, self.n_outputs_))
713         n_predictions = np.zeros((n_samples, self.n_outputs_))
714 
715         for estimator in self.estimators_:
716             unsampled_indices = _generate_unsampled_indices(
717                 estimator.random_state, n_samples)
718             p_estimator = estimator.predict(
719                 X[unsampled_indices, :], check_input=False)
720 
721             if self.n_outputs_ == 1:
722                 p_estimator = p_estimator[:, np.newaxis]
723 
724             predictions[unsampled_indices, :] += p_estimator
725             n_predictions[unsampled_indices, :] += 1
726 
727         if (n_predictions == 0).any():
728             warn("Some inputs do not have OOB scores. "
729                  "This probably means too few trees were used "
730                  "to compute any reliable oob estimates.")
731             n_predictions[n_predictions == 0] = 1
732 
733         predictions /= n_predictions
734         self.oob_prediction_ = predictions
735 
736         if self.n_outputs_ == 1:
737             self.oob_prediction_ = \
738                 self.oob_prediction_.reshape((n_samples, ))
739 
740         self.oob_score_ = 0.0
741 
742         for k in range(self.n_outputs_):
743             self.oob_score_ += r2_score(y[:, k],
744                                         predictions[:, k])
745 
746         self.oob_score_ /= self.n_outputs_
747 
748 
749 class RandomForestClassifier(ForestClassifier):
750     """A random forest classifier.
751 
752     A random forest is a meta estimator that fits a number of decision tree
753     classifiers on various sub-samples of the dataset and uses averaging to
754     improve the predictive accuracy and control over-fitting.
755     The sub-sample size is always the same as the original
756     input sample size but the samples are drawn with replacement if
757     `bootstrap=True` (default).
758 
759     Read more in the :ref:`User Guide <forest>`.
760 
761     Parameters
762     ----------
763     n_estimators : integer, optional (default=10)
764         The number of trees in the forest.
765 
766         .. versionchanged:: 0.20
767            The default value of ``n_estimators`` will change from 10 in
768            version 0.20 to 100 in version 0.22.
769 
770     criterion : string, optional (default="gini")
771         The function to measure the quality of a split. Supported criteria are
772         "gini" for the Gini impurity and "entropy" for the information gain.
773         Note: this parameter is tree-specific.
774 
775     max_depth : integer or None, optional (default=None)
776         The maximum depth of the tree. If None, then nodes are expanded until
777         all leaves are pure or until all leaves contain less than
778         min_samples_split samples.
779 
780     min_samples_split : int, float, optional (default=2)
781         The minimum number of samples required to split an internal node:
782 
783         - If int, then consider `min_samples_split` as the minimum number.
784         - If float, then `min_samples_split` is a fraction and
785           `ceil(min_samples_split * n_samples)` are the minimum
786           number of samples for each split.
787 
788         .. versionchanged:: 0.18
789            Added float values for fractions.
790 
791     min_samples_leaf : int, float, optional (default=1)
792         The minimum number of samples required to be at a leaf node.
793         A split point at any depth will only be considered if it leaves at
794         least ``min_samples_leaf`` training samples in each of the left and
795         right branches.  This may have the effect of smoothing the model,
796         especially in regression.
797 
798         - If int, then consider `min_samples_leaf` as the minimum number.
799         - If float, then `min_samples_leaf` is a fraction and
800           `ceil(min_samples_leaf * n_samples)` are the minimum
801           number of samples for each node.
802 
803         .. versionchanged:: 0.18
804            Added float values for fractions.
805 
806     min_weight_fraction_leaf : float, optional (default=0.)
807         The minimum weighted fraction of the sum total of weights (of all
808         the input samples) required to be at a leaf node. Samples have
809         equal weight when sample_weight is not provided.
810 
811     max_features : int, float, string or None, optional (default="auto")
812         The number of features to consider when looking for the best split:
813 
814         - If int, then consider `max_features` features at each split.
815         - If float, then `max_features` is a fraction and
816           `int(max_features * n_features)` features are considered at each
817           split.
818         - If "auto", then `max_features=sqrt(n_features)`.
819         - If "sqrt", then `max_features=sqrt(n_features)` (same as "auto").
820         - If "log2", then `max_features=log2(n_features)`.
821         - If None, then `max_features=n_features`.
822 
823         Note: the search for a split does not stop until at least one
824         valid partition of the node samples is found, even if it requires to
825         effectively inspect more than ``max_features`` features.
826 
827     max_leaf_nodes : int or None, optional (default=None)
828         Grow trees with ``max_leaf_nodes`` in best-first fashion.
829         Best nodes are defined as relative reduction in impurity.
830         If None then unlimited number of leaf nodes.
831 
832     min_impurity_decrease : float, optional (default=0.)
833         A node will be split if this split induces a decrease of the impurity
834         greater than or equal to this value.
835 
836         The weighted impurity decrease equation is the following::
837 
838             N_t / N * (impurity - N_t_R / N_t * right_impurity
839                                 - N_t_L / N_t * left_impurity)
840 
841         where ``N`` is the total number of samples, ``N_t`` is the number of
842         samples at the current node, ``N_t_L`` is the number of samples in the
843         left child, and ``N_t_R`` is the number of samples in the right child.
844 
845         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
846         if ``sample_weight`` is passed.
847 
848         .. versionadded:: 0.19
849 
850     min_impurity_split : float, (default=1e-7)
851         Threshold for early stopping in tree growth. A node will split
852         if its impurity is above the threshold, otherwise it is a leaf.
853 
854         .. deprecated:: 0.19
855            ``min_impurity_split`` has been deprecated in favor of
856            ``min_impurity_decrease`` in 0.19. The default value of
857            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
858            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
859 
860 
861     bootstrap : boolean, optional (default=True)
862         Whether bootstrap samples are used when building trees. If False, the
863         whole datset is used to build each tree.
864 
865     oob_score : bool (default=False)
866         Whether to use out-of-bag samples to estimate
867         the generalization accuracy.
868 
869     n_jobs : int or None, optional (default=None)
870         The number of jobs to run in parallel for both `fit` and `predict`.
871         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
872         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
873         for more details.
874 
875     random_state : int, RandomState instance or None, optional (default=None)
876         If int, random_state is the seed used by the random number generator;
877         If RandomState instance, random_state is the random number generator;
878         If None, the random number generator is the RandomState instance used
879         by `np.random`.
880 
881     verbose : int, optional (default=0)
882         Controls the verbosity when fitting and predicting.
883 
884     warm_start : bool, optional (default=False)
885         When set to ``True``, reuse the solution of the previous call to fit
886         and add more estimators to the ensemble, otherwise, just fit a whole
887         new forest. See :term:`the Glossary <warm_start>`.
888 
889     class_weight : dict, list of dicts, "balanced", "balanced_subsample" or \
890     None, optional (default=None)
891         Weights associated with classes in the form ``{class_label: weight}``.
892         If not given, all classes are supposed to have weight one. For
893         multi-output problems, a list of dicts can be provided in the same
894         order as the columns of y.
895 
896         Note that for multioutput (including multilabel) weights should be
897         defined for each class of every column in its own dict. For example,
898         for four-class multilabel classification weights should be
899         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
900         [{1:1}, {2:5}, {3:1}, {4:1}].
901 
902         The "balanced" mode uses the values of y to automatically adjust
903         weights inversely proportional to class frequencies in the input data
904         as ``n_samples / (n_classes * np.bincount(y))``
905 
906         The "balanced_subsample" mode is the same as "balanced" except that
907         weights are computed based on the bootstrap sample for every tree
908         grown.
909 
910         For multi-output, the weights of each column of y will be multiplied.
911 
912         Note that these weights will be multiplied with sample_weight (passed
913         through the fit method) if sample_weight is specified.
914 
915     Attributes
916     ----------
917     estimators_ : list of DecisionTreeClassifier
918         The collection of fitted sub-estimators.
919 
920     classes_ : array of shape = [n_classes] or a list of such arrays
921         The classes labels (single output problem), or a list of arrays of
922         class labels (multi-output problem).
923 
924     n_classes_ : int or list
925         The number of classes (single output problem), or a list containing the
926         number of classes for each output (multi-output problem).
927 
928     n_features_ : int
929         The number of features when ``fit`` is performed.
930 
931     n_outputs_ : int
932         The number of outputs when ``fit`` is performed.
933 
934     feature_importances_ : array of shape = [n_features]
935         The feature importances (the higher, the more important the feature).
936 
937     oob_score_ : float
938         Score of the training dataset obtained using an out-of-bag estimate.
939 
940     oob_decision_function_ : array of shape = [n_samples, n_classes]
941         Decision function computed with out-of-bag estimate on the training
942         set. If n_estimators is small it might be possible that a data point
943         was never left out during the bootstrap. In this case,
944         `oob_decision_function_` might contain NaN.
945 
946     Examples
947     --------
948     >>> from sklearn.ensemble import RandomForestClassifier
949     >>> from sklearn.datasets import make_classification
950 
951     >>> X, y = make_classification(n_samples=1000, n_features=4,
952     ...                            n_informative=2, n_redundant=0,
953     ...                            random_state=0, shuffle=False)
954     >>> clf = RandomForestClassifier(n_estimators=100, max_depth=2,
955     ...                              random_state=0)
956     >>> clf.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
957     RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
958                 max_depth=2, max_features='auto', max_leaf_nodes=None,
959                 min_impurity_decrease=0.0, min_impurity_split=None,
960                 min_samples_leaf=1, min_samples_split=2,
961                 min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
962                 oob_score=False, random_state=0, verbose=0, warm_start=False)
963     >>> print(clf.feature_importances_)
964     [0.14205973 0.76664038 0.0282433  0.06305659]
965     >>> print(clf.predict([[0, 0, 0, 0]]))
966     [1]
967 
968     Notes
969     -----
970     The default values for the parameters controlling the size of the trees
971     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
972     unpruned trees which can potentially be very large on some data sets. To
973     reduce memory consumption, the complexity and size of the trees should be
974     controlled by setting those parameter values.
975 
976     The features are always randomly permuted at each split. Therefore,
977     the best found split may vary, even with the same training data,
978     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
979     of the criterion is identical for several splits enumerated during the
980     search of the best split. To obtain a deterministic behaviour during
981     fitting, ``random_state`` has to be fixed.
982 
983     References
984     ----------
985 
986     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
987 
988     See also
989     --------
990     DecisionTreeClassifier, ExtraTreesClassifier
991     """
992     def __init__(self,
993                  n_estimators='warn',
994                  criterion="gini",
995                  max_depth=None,
996                  min_samples_split=2,
997                  min_samples_leaf=1,
998                  min_weight_fraction_leaf=0.,
999                  max_features="auto",
1000                  max_leaf_nodes=None,
1001                  min_impurity_decrease=0.,
1002                  min_impurity_split=None,
1003                  bootstrap=True,
1004                  oob_score=False,
1005                  n_jobs=None,
1006                  random_state=None,
1007                  verbose=0,
1008                  warm_start=False,
1009                  class_weight=None):
1010         super().__init__(
1011             base_estimator=DecisionTreeClassifier(),
1012             n_estimators=n_estimators,
1013             estimator_params=("criterion", "max_depth", "min_samples_split",
1014                               "min_samples_leaf", "min_weight_fraction_leaf",
1015                               "max_features", "max_leaf_nodes",
1016                               "min_impurity_decrease", "min_impurity_split",
1017                               "random_state"),
1018             bootstrap=bootstrap,
1019             oob_score=oob_score,
1020             n_jobs=n_jobs,
1021             random_state=random_state,
1022             verbose=verbose,
1023             warm_start=warm_start,
1024             class_weight=class_weight)
1025 
1026         self.criterion = criterion
1027         self.max_depth = max_depth
1028         self.min_samples_split = min_samples_split
1029         self.min_samples_leaf = min_samples_leaf
1030         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1031         self.max_features = max_features
1032         self.max_leaf_nodes = max_leaf_nodes
1033         self.min_impurity_decrease = min_impurity_decrease
1034         self.min_impurity_split = min_impurity_split
1035 
1036 
1037 class RandomForestRegressor(ForestRegressor):
1038     """A random forest regressor.
1039 
1040     A random forest is a meta estimator that fits a number of classifying
1041     decision trees on various sub-samples of the dataset and uses averaging
1042     to improve the predictive accuracy and control over-fitting.
1043     The sub-sample size is always the same as the original
1044     input sample size but the samples are drawn with replacement if
1045     `bootstrap=True` (default).
1046 
1047     Read more in the :ref:`User Guide <forest>`.
1048 
1049     Parameters
1050     ----------
1051     n_estimators : integer, optional (default=10)
1052         The number of trees in the forest.
1053 
1054         .. versionchanged:: 0.20
1055            The default value of ``n_estimators`` will change from 10 in
1056            version 0.20 to 100 in version 0.22.
1057 
1058     criterion : string, optional (default="mse")
1059         The function to measure the quality of a split. Supported criteria
1060         are "mse" for the mean squared error, which is equal to variance
1061         reduction as feature selection criterion, and "mae" for the mean
1062         absolute error.
1063 
1064         .. versionadded:: 0.18
1065            Mean Absolute Error (MAE) criterion.
1066 
1067     max_depth : integer or None, optional (default=None)
1068         The maximum depth of the tree. If None, then nodes are expanded until
1069         all leaves are pure or until all leaves contain less than
1070         min_samples_split samples.
1071 
1072     min_samples_split : int, float, optional (default=2)
1073         The minimum number of samples required to split an internal node:
1074 
1075         - If int, then consider `min_samples_split` as the minimum number.
1076         - If float, then `min_samples_split` is a fraction and
1077           `ceil(min_samples_split * n_samples)` are the minimum
1078           number of samples for each split.
1079 
1080         .. versionchanged:: 0.18
1081            Added float values for fractions.
1082 
1083     min_samples_leaf : int, float, optional (default=1)
1084         The minimum number of samples required to be at a leaf node.
1085         A split point at any depth will only be considered if it leaves at
1086         least ``min_samples_leaf`` training samples in each of the left and
1087         right branches.  This may have the effect of smoothing the model,
1088         especially in regression.
1089 
1090         - If int, then consider `min_samples_leaf` as the minimum number.
1091         - If float, then `min_samples_leaf` is a fraction and
1092           `ceil(min_samples_leaf * n_samples)` are the minimum
1093           number of samples for each node.
1094 
1095         .. versionchanged:: 0.18
1096            Added float values for fractions.
1097 
1098     min_weight_fraction_leaf : float, optional (default=0.)
1099         The minimum weighted fraction of the sum total of weights (of all
1100         the input samples) required to be at a leaf node. Samples have
1101         equal weight when sample_weight is not provided.
1102 
1103     max_features : int, float, string or None, optional (default="auto")
1104         The number of features to consider when looking for the best split:
1105 
1106         - If int, then consider `max_features` features at each split.
1107         - If float, then `max_features` is a fraction and
1108           `int(max_features * n_features)` features are considered at each
1109           split.
1110         - If "auto", then `max_features=n_features`.
1111         - If "sqrt", then `max_features=sqrt(n_features)`.
1112         - If "log2", then `max_features=log2(n_features)`.
1113         - If None, then `max_features=n_features`.
1114 
1115         Note: the search for a split does not stop until at least one
1116         valid partition of the node samples is found, even if it requires to
1117         effectively inspect more than ``max_features`` features.
1118 
1119     max_leaf_nodes : int or None, optional (default=None)
1120         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1121         Best nodes are defined as relative reduction in impurity.
1122         If None then unlimited number of leaf nodes.
1123 
1124     min_impurity_decrease : float, optional (default=0.)
1125         A node will be split if this split induces a decrease of the impurity
1126         greater than or equal to this value.
1127 
1128         The weighted impurity decrease equation is the following::
1129 
1130             N_t / N * (impurity - N_t_R / N_t * right_impurity
1131                                 - N_t_L / N_t * left_impurity)
1132 
1133         where ``N`` is the total number of samples, ``N_t`` is the number of
1134         samples at the current node, ``N_t_L`` is the number of samples in the
1135         left child, and ``N_t_R`` is the number of samples in the right child.
1136 
1137         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1138         if ``sample_weight`` is passed.
1139 
1140         .. versionadded:: 0.19
1141 
1142     min_impurity_split : float, (default=1e-7)
1143         Threshold for early stopping in tree growth. A node will split
1144         if its impurity is above the threshold, otherwise it is a leaf.
1145 
1146         .. deprecated:: 0.19
1147            ``min_impurity_split`` has been deprecated in favor of
1148            ``min_impurity_decrease`` in 0.19. The default value of
1149            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1150            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1151 
1152     bootstrap : boolean, optional (default=True)
1153         Whether bootstrap samples are used when building trees. If False, the
1154         whole datset is used to build each tree.
1155 
1156     oob_score : bool, optional (default=False)
1157         whether to use out-of-bag samples to estimate
1158         the R^2 on unseen data.
1159 
1160     n_jobs : int or None, optional (default=None)
1161         The number of jobs to run in parallel for both `fit` and `predict`.
1162         `None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1163         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1164         for more details.
1165 
1166     random_state : int, RandomState instance or None, optional (default=None)
1167         If int, random_state is the seed used by the random number generator;
1168         If RandomState instance, random_state is the random number generator;
1169         If None, the random number generator is the RandomState instance used
1170         by `np.random`.
1171 
1172     verbose : int, optional (default=0)
1173         Controls the verbosity when fitting and predicting.
1174 
1175     warm_start : bool, optional (default=False)
1176         When set to ``True``, reuse the solution of the previous call to fit
1177         and add more estimators to the ensemble, otherwise, just fit a whole
1178         new forest. See :term:`the Glossary <warm_start>`.
1179 
1180     Attributes
1181     ----------
1182     estimators_ : list of DecisionTreeRegressor
1183         The collection of fitted sub-estimators.
1184 
1185     feature_importances_ : array of shape = [n_features]
1186         The feature importances (the higher, the more important the feature).
1187 
1188     n_features_ : int
1189         The number of features when ``fit`` is performed.
1190 
1191     n_outputs_ : int
1192         The number of outputs when ``fit`` is performed.
1193 
1194     oob_score_ : float
1195         Score of the training dataset obtained using an out-of-bag estimate.
1196 
1197     oob_prediction_ : array of shape = [n_samples]
1198         Prediction computed with out-of-bag estimate on the training set.
1199 
1200     Examples
1201     --------
1202     >>> from sklearn.ensemble import RandomForestRegressor
1203     >>> from sklearn.datasets import make_regression
1204 
1205     >>> X, y = make_regression(n_features=4, n_informative=2,
1206     ...                        random_state=0, shuffle=False)
1207     >>> regr = RandomForestRegressor(max_depth=2, random_state=0,
1208     ...                              n_estimators=100)
1209     >>> regr.fit(X, y)  # doctest: +NORMALIZE_WHITESPACE
1210     RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=2,
1211                max_features='auto', max_leaf_nodes=None,
1212                min_impurity_decrease=0.0, min_impurity_split=None,
1213                min_samples_leaf=1, min_samples_split=2,
1214                min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,
1215                oob_score=False, random_state=0, verbose=0, warm_start=False)
1216     >>> print(regr.feature_importances_)
1217     [0.18146984 0.81473937 0.00145312 0.00233767]
1218     >>> print(regr.predict([[0, 0, 0, 0]]))
1219     [-8.32987858]
1220 
1221     Notes
1222     -----
1223     The default values for the parameters controlling the size of the trees
1224     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1225     unpruned trees which can potentially be very large on some data sets. To
1226     reduce memory consumption, the complexity and size of the trees should be
1227     controlled by setting those parameter values.
1228 
1229     The features are always randomly permuted at each split. Therefore,
1230     the best found split may vary, even with the same training data,
1231     ``max_features=n_features`` and ``bootstrap=False``, if the improvement
1232     of the criterion is identical for several splits enumerated during the
1233     search of the best split. To obtain a deterministic behaviour during
1234     fitting, ``random_state`` has to be fixed.
1235 
1236     The default value ``max_features="auto"`` uses ``n_features``
1237     rather than ``n_features / 3``. The latter was originally suggested in
1238     [1], whereas the former was more recently justified empirically in [2].
1239 
1240     References
1241     ----------
1242 
1243     .. [1] L. Breiman, "Random Forests", Machine Learning, 45(1), 5-32, 2001.
1244 
1245     .. [2] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
1246            trees", Machine Learning, 63(1), 3-42, 2006.
1247 
1248     See also
1249     --------
1250     DecisionTreeRegressor, ExtraTreesRegressor
1251     """
1252     def __init__(self,
1253                  n_estimators='warn',
1254                  criterion="mse",
1255                  max_depth=None,
1256                  min_samples_split=2,
1257                  min_samples_leaf=1,
1258                  min_weight_fraction_leaf=0.,
1259                  max_features="auto",
1260                  max_leaf_nodes=None,
1261                  min_impurity_decrease=0.,
1262                  min_impurity_split=None,
1263                  bootstrap=True,
1264                  oob_score=False,
1265                  n_jobs=None,
1266                  random_state=None,
1267                  verbose=0,
1268                  warm_start=False):
1269         super().__init__(
1270             base_estimator=DecisionTreeRegressor(),
1271             n_estimators=n_estimators,
1272             estimator_params=("criterion", "max_depth", "min_samples_split",
1273                               "min_samples_leaf", "min_weight_fraction_leaf",
1274                               "max_features", "max_leaf_nodes",
1275                               "min_impurity_decrease", "min_impurity_split",
1276                               "random_state"),
1277             bootstrap=bootstrap,
1278             oob_score=oob_score,
1279             n_jobs=n_jobs,
1280             random_state=random_state,
1281             verbose=verbose,
1282             warm_start=warm_start)
1283 
1284         self.criterion = criterion
1285         self.max_depth = max_depth
1286         self.min_samples_split = min_samples_split
1287         self.min_samples_leaf = min_samples_leaf
1288         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1289         self.max_features = max_features
1290         self.max_leaf_nodes = max_leaf_nodes
1291         self.min_impurity_decrease = min_impurity_decrease
1292         self.min_impurity_split = min_impurity_split
1293 
1294 
1295 class ExtraTreesClassifier(ForestClassifier):
1296     """An extra-trees classifier.
1297 
1298     This class implements a meta estimator that fits a number of
1299     randomized decision trees (a.k.a. extra-trees) on various sub-samples
1300     of the dataset and uses averaging to improve the predictive accuracy
1301     and control over-fitting.
1302 
1303     Read more in the :ref:`User Guide <forest>`.
1304 
1305     Parameters
1306     ----------
1307     n_estimators : integer, optional (default=10)
1308         The number of trees in the forest.
1309 
1310         .. versionchanged:: 0.20
1311            The default value of ``n_estimators`` will change from 10 in
1312            version 0.20 to 100 in version 0.22.
1313 
1314     criterion : string, optional (default="gini")
1315         The function to measure the quality of a split. Supported criteria are
1316         "gini" for the Gini impurity and "entropy" for the information gain.
1317 
1318     max_depth : integer or None, optional (default=None)
1319         The maximum depth of the tree. If None, then nodes are expanded until
1320         all leaves are pure or until all leaves contain less than
1321         min_samples_split samples.
1322 
1323     min_samples_split : int, float, optional (default=2)
1324         The minimum number of samples required to split an internal node:
1325 
1326         - If int, then consider `min_samples_split` as the minimum number.
1327         - If float, then `min_samples_split` is a fraction and
1328           `ceil(min_samples_split * n_samples)` are the minimum
1329           number of samples for each split.
1330 
1331         .. versionchanged:: 0.18
1332            Added float values for fractions.
1333 
1334     min_samples_leaf : int, float, optional (default=1)
1335         The minimum number of samples required to be at a leaf node.
1336         A split point at any depth will only be considered if it leaves at
1337         least ``min_samples_leaf`` training samples in each of the left and
1338         right branches.  This may have the effect of smoothing the model,
1339         especially in regression.
1340 
1341         - If int, then consider `min_samples_leaf` as the minimum number.
1342         - If float, then `min_samples_leaf` is a fraction and
1343           `ceil(min_samples_leaf * n_samples)` are the minimum
1344           number of samples for each node.
1345 
1346         .. versionchanged:: 0.18
1347            Added float values for fractions.
1348 
1349     min_weight_fraction_leaf : float, optional (default=0.)
1350         The minimum weighted fraction of the sum total of weights (of all
1351         the input samples) required to be at a leaf node. Samples have
1352         equal weight when sample_weight is not provided.
1353 
1354     max_features : int, float, string or None, optional (default="auto")
1355         The number of features to consider when looking for the best split:
1356 
1357         - If int, then consider `max_features` features at each split.
1358         - If float, then `max_features` is a fraction and
1359           `int(max_features * n_features)` features are considered at each
1360           split.
1361         - If "auto", then `max_features=sqrt(n_features)`.
1362         - If "sqrt", then `max_features=sqrt(n_features)`.
1363         - If "log2", then `max_features=log2(n_features)`.
1364         - If None, then `max_features=n_features`.
1365 
1366         Note: the search for a split does not stop until at least one
1367         valid partition of the node samples is found, even if it requires to
1368         effectively inspect more than ``max_features`` features.
1369 
1370     max_leaf_nodes : int or None, optional (default=None)
1371         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1372         Best nodes are defined as relative reduction in impurity.
1373         If None then unlimited number of leaf nodes.
1374 
1375     min_impurity_decrease : float, optional (default=0.)
1376         A node will be split if this split induces a decrease of the impurity
1377         greater than or equal to this value.
1378 
1379         The weighted impurity decrease equation is the following::
1380 
1381             N_t / N * (impurity - N_t_R / N_t * right_impurity
1382                                 - N_t_L / N_t * left_impurity)
1383 
1384         where ``N`` is the total number of samples, ``N_t`` is the number of
1385         samples at the current node, ``N_t_L`` is the number of samples in the
1386         left child, and ``N_t_R`` is the number of samples in the right child.
1387 
1388         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1389         if ``sample_weight`` is passed.
1390 
1391         .. versionadded:: 0.19
1392 
1393     min_impurity_split : float, (default=1e-7)
1394         Threshold for early stopping in tree growth. A node will split
1395         if its impurity is above the threshold, otherwise it is a leaf.
1396 
1397         .. deprecated:: 0.19
1398            ``min_impurity_split`` has been deprecated in favor of
1399            ``min_impurity_decrease`` in 0.19. The default value of
1400            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1401            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1402 
1403     bootstrap : boolean, optional (default=False)
1404         Whether bootstrap samples are used when building trees. If False, the
1405         whole datset is used to build each tree.
1406 
1407     oob_score : bool, optional (default=False)
1408         Whether to use out-of-bag samples to estimate
1409         the generalization accuracy.
1410 
1411     n_jobs : int or None, optional (default=None)
1412         The number of jobs to run in parallel for both `fit` and `predict`.
1413         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1414         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1415         for more details.
1416 
1417     random_state : int, RandomState instance or None, optional (default=None)
1418         If int, random_state is the seed used by the random number generator;
1419         If RandomState instance, random_state is the random number generator;
1420         If None, the random number generator is the RandomState instance used
1421         by `np.random`.
1422 
1423     verbose : int, optional (default=0)
1424         Controls the verbosity when fitting and predicting.
1425 
1426     warm_start : bool, optional (default=False)
1427         When set to ``True``, reuse the solution of the previous call to fit
1428         and add more estimators to the ensemble, otherwise, just fit a whole
1429         new forest. See :term:`the Glossary <warm_start>`.
1430 
1431     class_weight : dict, list of dicts, "balanced", "balanced_subsample" or \
1432     None, optional (default=None)
1433         Weights associated with classes in the form ``{class_label: weight}``.
1434         If not given, all classes are supposed to have weight one. For
1435         multi-output problems, a list of dicts can be provided in the same
1436         order as the columns of y.
1437 
1438         Note that for multioutput (including multilabel) weights should be
1439         defined for each class of every column in its own dict. For example,
1440         for four-class multilabel classification weights should be
1441         [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
1442         [{1:1}, {2:5}, {3:1}, {4:1}].
1443 
1444         The "balanced" mode uses the values of y to automatically adjust
1445         weights inversely proportional to class frequencies in the input data
1446         as ``n_samples / (n_classes * np.bincount(y))``
1447 
1448         The "balanced_subsample" mode is the same as "balanced" except that weights are
1449         computed based on the bootstrap sample for every tree grown.
1450 
1451         For multi-output, the weights of each column of y will be multiplied.
1452 
1453         Note that these weights will be multiplied with sample_weight (passed
1454         through the fit method) if sample_weight is specified.
1455 
1456     Attributes
1457     ----------
1458     estimators_ : list of DecisionTreeClassifier
1459         The collection of fitted sub-estimators.
1460 
1461     classes_ : array of shape = [n_classes] or a list of such arrays
1462         The classes labels (single output problem), or a list of arrays of
1463         class labels (multi-output problem).
1464 
1465     n_classes_ : int or list
1466         The number of classes (single output problem), or a list containing the
1467         number of classes for each output (multi-output problem).
1468 
1469     feature_importances_ : array of shape = [n_features]
1470         The feature importances (the higher, the more important the feature).
1471 
1472     n_features_ : int
1473         The number of features when ``fit`` is performed.
1474 
1475     n_outputs_ : int
1476         The number of outputs when ``fit`` is performed.
1477 
1478     oob_score_ : float
1479         Score of the training dataset obtained using an out-of-bag estimate.
1480 
1481     oob_decision_function_ : array of shape = [n_samples, n_classes]
1482         Decision function computed with out-of-bag estimate on the training
1483         set. If n_estimators is small it might be possible that a data point
1484         was never left out during the bootstrap. In this case,
1485         `oob_decision_function_` might contain NaN.
1486 
1487     Notes
1488     -----
1489     The default values for the parameters controlling the size of the trees
1490     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1491     unpruned trees which can potentially be very large on some data sets. To
1492     reduce memory consumption, the complexity and size of the trees should be
1493     controlled by setting those parameter values.
1494 
1495     References
1496     ----------
1497 
1498     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized
1499            trees", Machine Learning, 63(1), 3-42, 2006.
1500 
1501     See also
1502     --------
1503     sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.
1504     RandomForestClassifier : Ensemble Classifier based on trees with optimal
1505         splits.
1506     """
1507     def __init__(self,
1508                  n_estimators='warn',
1509                  criterion="gini",
1510                  max_depth=None,
1511                  min_samples_split=2,
1512                  min_samples_leaf=1,
1513                  min_weight_fraction_leaf=0.,
1514                  max_features="auto",
1515                  max_leaf_nodes=None,
1516                  min_impurity_decrease=0.,
1517                  min_impurity_split=None,
1518                  bootstrap=False,
1519                  oob_score=False,
1520                  n_jobs=None,
1521                  random_state=None,
1522                  verbose=0,
1523                  warm_start=False,
1524                  class_weight=None):
1525         super().__init__(
1526             base_estimator=ExtraTreeClassifier(),
1527             n_estimators=n_estimators,
1528             estimator_params=("criterion", "max_depth", "min_samples_split",
1529                               "min_samples_leaf", "min_weight_fraction_leaf",
1530                               "max_features", "max_leaf_nodes",
1531                               "min_impurity_decrease", "min_impurity_split",
1532                               "random_state"),
1533             bootstrap=bootstrap,
1534             oob_score=oob_score,
1535             n_jobs=n_jobs,
1536             random_state=random_state,
1537             verbose=verbose,
1538             warm_start=warm_start,
1539             class_weight=class_weight)
1540 
1541         self.criterion = criterion
1542         self.max_depth = max_depth
1543         self.min_samples_split = min_samples_split
1544         self.min_samples_leaf = min_samples_leaf
1545         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1546         self.max_features = max_features
1547         self.max_leaf_nodes = max_leaf_nodes
1548         self.min_impurity_decrease = min_impurity_decrease
1549         self.min_impurity_split = min_impurity_split
1550 
1551 
1552 class ExtraTreesRegressor(ForestRegressor):
1553     """An extra-trees regressor.
1554 
1555     This class implements a meta estimator that fits a number of
1556     randomized decision trees (a.k.a. extra-trees) on various sub-samples
1557     of the dataset and uses averaging to improve the predictive accuracy
1558     and control over-fitting.
1559 
1560     Read more in the :ref:`User Guide <forest>`.
1561 
1562     Parameters
1563     ----------
1564     n_estimators : integer, optional (default=10)
1565         The number of trees in the forest.
1566 
1567         .. versionchanged:: 0.20
1568            The default value of ``n_estimators`` will change from 10 in
1569            version 0.20 to 100 in version 0.22.
1570 
1571     criterion : string, optional (default="mse")
1572         The function to measure the quality of a split. Supported criteria
1573         are "mse" for the mean squared error, which is equal to variance
1574         reduction as feature selection criterion, and "mae" for the mean
1575         absolute error.
1576 
1577         .. versionadded:: 0.18
1578            Mean Absolute Error (MAE) criterion.
1579 
1580     max_depth : integer or None, optional (default=None)
1581         The maximum depth of the tree. If None, then nodes are expanded until
1582         all leaves are pure or until all leaves contain less than
1583         min_samples_split samples.
1584 
1585     min_samples_split : int, float, optional (default=2)
1586         The minimum number of samples required to split an internal node:
1587 
1588         - If int, then consider `min_samples_split` as the minimum number.
1589         - If float, then `min_samples_split` is a fraction and
1590           `ceil(min_samples_split * n_samples)` are the minimum
1591           number of samples for each split.
1592 
1593         .. versionchanged:: 0.18
1594            Added float values for fractions.
1595 
1596     min_samples_leaf : int, float, optional (default=1)
1597         The minimum number of samples required to be at a leaf node.
1598         A split point at any depth will only be considered if it leaves at
1599         least ``min_samples_leaf`` training samples in each of the left and
1600         right branches.  This may have the effect of smoothing the model,
1601         especially in regression.
1602 
1603         - If int, then consider `min_samples_leaf` as the minimum number.
1604         - If float, then `min_samples_leaf` is a fraction and
1605           `ceil(min_samples_leaf * n_samples)` are the minimum
1606           number of samples for each node.
1607 
1608         .. versionchanged:: 0.18
1609            Added float values for fractions.
1610 
1611     min_weight_fraction_leaf : float, optional (default=0.)
1612         The minimum weighted fraction of the sum total of weights (of all
1613         the input samples) required to be at a leaf node. Samples have
1614         equal weight when sample_weight is not provided.
1615 
1616     max_features : int, float, string or None, optional (default="auto")
1617         The number of features to consider when looking for the best split:
1618 
1619         - If int, then consider `max_features` features at each split.
1620         - If float, then `max_features` is a fraction and
1621           `int(max_features * n_features)` features are considered at each
1622           split.
1623         - If "auto", then `max_features=n_features`.
1624         - If "sqrt", then `max_features=sqrt(n_features)`.
1625         - If "log2", then `max_features=log2(n_features)`.
1626         - If None, then `max_features=n_features`.
1627 
1628         Note: the search for a split does not stop until at least one
1629         valid partition of the node samples is found, even if it requires to
1630         effectively inspect more than ``max_features`` features.
1631 
1632     max_leaf_nodes : int or None, optional (default=None)
1633         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1634         Best nodes are defined as relative reduction in impurity.
1635         If None then unlimited number of leaf nodes.
1636 
1637     min_impurity_decrease : float, optional (default=0.)
1638         A node will be split if this split induces a decrease of the impurity
1639         greater than or equal to this value.
1640 
1641         The weighted impurity decrease equation is the following::
1642 
1643             N_t / N * (impurity - N_t_R / N_t * right_impurity
1644                                 - N_t_L / N_t * left_impurity)
1645 
1646         where ``N`` is the total number of samples, ``N_t`` is the number of
1647         samples at the current node, ``N_t_L`` is the number of samples in the
1648         left child, and ``N_t_R`` is the number of samples in the right child.
1649 
1650         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1651         if ``sample_weight`` is passed.
1652 
1653         .. versionadded:: 0.19
1654 
1655     min_impurity_split : float, (default=1e-7)
1656         Threshold for early stopping in tree growth. A node will split
1657         if its impurity is above the threshold, otherwise it is a leaf.
1658 
1659         .. deprecated:: 0.19
1660            ``min_impurity_split`` has been deprecated in favor of
1661            ``min_impurity_decrease`` in 0.19. The default value of
1662            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1663            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1664 
1665     bootstrap : boolean, optional (default=False)
1666         Whether bootstrap samples are used when building trees. If False, the
1667         whole datset is used to build each tree.
1668 
1669     oob_score : bool, optional (default=False)
1670         Whether to use out-of-bag samples to estimate the R^2 on unseen data.
1671 
1672     n_jobs : int or None, optional (default=None)
1673         The number of jobs to run in parallel for both `fit` and `predict`.
1674         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1675         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1676         for more details.
1677 
1678     random_state : int, RandomState instance or None, optional (default=None)
1679         If int, random_state is the seed used by the random number generator;
1680         If RandomState instance, random_state is the random number generator;
1681         If None, the random number generator is the RandomState instance used
1682         by `np.random`.
1683 
1684     verbose : int, optional (default=0)
1685         Controls the verbosity when fitting and predicting.
1686 
1687     warm_start : bool, optional (default=False)
1688         When set to ``True``, reuse the solution of the previous call to fit
1689         and add more estimators to the ensemble, otherwise, just fit a whole
1690         new forest. See :term:`the Glossary <warm_start>`.
1691 
1692     Attributes
1693     ----------
1694     estimators_ : list of DecisionTreeRegressor
1695         The collection of fitted sub-estimators.
1696 
1697     feature_importances_ : array of shape = [n_features]
1698         The feature importances (the higher, the more important the feature).
1699 
1700     n_features_ : int
1701         The number of features.
1702 
1703     n_outputs_ : int
1704         The number of outputs.
1705 
1706     oob_score_ : float
1707         Score of the training dataset obtained using an out-of-bag estimate.
1708 
1709     oob_prediction_ : array of shape = [n_samples]
1710         Prediction computed with out-of-bag estimate on the training set.
1711 
1712     Notes
1713     -----
1714     The default values for the parameters controlling the size of the trees
1715     (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and
1716     unpruned trees which can potentially be very large on some data sets. To
1717     reduce memory consumption, the complexity and size of the trees should be
1718     controlled by setting those parameter values.
1719 
1720     References
1721     ----------
1722 
1723     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
1724            Machine Learning, 63(1), 3-42, 2006.
1725 
1726     See also
1727     --------
1728     sklearn.tree.ExtraTreeRegressor: Base estimator for this ensemble.
1729     RandomForestRegressor: Ensemble regressor using trees with optimal splits.
1730     """
1731     def __init__(self,
1732                  n_estimators='warn',
1733                  criterion="mse",
1734                  max_depth=None,
1735                  min_samples_split=2,
1736                  min_samples_leaf=1,
1737                  min_weight_fraction_leaf=0.,
1738                  max_features="auto",
1739                  max_leaf_nodes=None,
1740                  min_impurity_decrease=0.,
1741                  min_impurity_split=None,
1742                  bootstrap=False,
1743                  oob_score=False,
1744                  n_jobs=None,
1745                  random_state=None,
1746                  verbose=0,
1747                  warm_start=False):
1748         super().__init__(
1749             base_estimator=ExtraTreeRegressor(),
1750             n_estimators=n_estimators,
1751             estimator_params=("criterion", "max_depth", "min_samples_split",
1752                               "min_samples_leaf", "min_weight_fraction_leaf",
1753                               "max_features", "max_leaf_nodes",
1754                               "min_impurity_decrease", "min_impurity_split",
1755                               "random_state"),
1756             bootstrap=bootstrap,
1757             oob_score=oob_score,
1758             n_jobs=n_jobs,
1759             random_state=random_state,
1760             verbose=verbose,
1761             warm_start=warm_start)
1762 
1763         self.criterion = criterion
1764         self.max_depth = max_depth
1765         self.min_samples_split = min_samples_split
1766         self.min_samples_leaf = min_samples_leaf
1767         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1768         self.max_features = max_features
1769         self.max_leaf_nodes = max_leaf_nodes
1770         self.min_impurity_decrease = min_impurity_decrease
1771         self.min_impurity_split = min_impurity_split
1772 
1773 
1774 class RandomTreesEmbedding(BaseForest):
1775     """An ensemble of totally random trees.
1776 
1777     An unsupervised transformation of a dataset to a high-dimensional
1778     sparse representation. A datapoint is coded according to which leaf of
1779     each tree it is sorted into. Using a one-hot encoding of the leaves,
1780     this leads to a binary coding with as many ones as there are trees in
1781     the forest.
1782 
1783     The dimensionality of the resulting representation is
1784     ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,
1785     the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.
1786 
1787     Read more in the :ref:`User Guide <random_trees_embedding>`.
1788 
1789     Parameters
1790     ----------
1791     n_estimators : integer, optional (default=10)
1792         Number of trees in the forest.
1793 
1794         .. versionchanged:: 0.20
1795            The default value of ``n_estimators`` will change from 10 in
1796            version 0.20 to 100 in version 0.22.
1797 
1798     max_depth : integer, optional (default=5)
1799         The maximum depth of each tree. If None, then nodes are expanded until
1800         all leaves are pure or until all leaves contain less than
1801         min_samples_split samples.
1802 
1803     min_samples_split : int, float, optional (default=2)
1804         The minimum number of samples required to split an internal node:
1805 
1806         - If int, then consider `min_samples_split` as the minimum number.
1807         - If float, then `min_samples_split` is a fraction and
1808           `ceil(min_samples_split * n_samples)` is the minimum
1809           number of samples for each split.
1810 
1811         .. versionchanged:: 0.18
1812            Added float values for fractions.
1813 
1814     min_samples_leaf : int, float, optional (default=1)
1815         The minimum number of samples required to be at a leaf node.
1816         A split point at any depth will only be considered if it leaves at
1817         least ``min_samples_leaf`` training samples in each of the left and
1818         right branches.  This may have the effect of smoothing the model,
1819         especially in regression.
1820 
1821         - If int, then consider `min_samples_leaf` as the minimum number.
1822         - If float, then `min_samples_leaf` is a fraction and
1823           `ceil(min_samples_leaf * n_samples)` is the minimum
1824           number of samples for each node.
1825 
1826         .. versionchanged:: 0.18
1827            Added float values for fractions.
1828 
1829     min_weight_fraction_leaf : float, optional (default=0.)
1830         The minimum weighted fraction of the sum total of weights (of all
1831         the input samples) required to be at a leaf node. Samples have
1832         equal weight when sample_weight is not provided.
1833 
1834     max_leaf_nodes : int or None, optional (default=None)
1835         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1836         Best nodes are defined as relative reduction in impurity.
1837         If None then unlimited number of leaf nodes.
1838 
1839     min_impurity_decrease : float, optional (default=0.)
1840         A node will be split if this split induces a decrease of the impurity
1841         greater than or equal to this value.
1842 
1843         The weighted impurity decrease equation is the following::
1844 
1845             N_t / N * (impurity - N_t_R / N_t * right_impurity
1846                                 - N_t_L / N_t * left_impurity)
1847 
1848         where ``N`` is the total number of samples, ``N_t`` is the number of
1849         samples at the current node, ``N_t_L`` is the number of samples in the
1850         left child, and ``N_t_R`` is the number of samples in the right child.
1851 
1852         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1853         if ``sample_weight`` is passed.
1854 
1855         .. versionadded:: 0.19
1856 
1857     min_impurity_split : float, (default=1e-7)
1858         Threshold for early stopping in tree growth. A node will split
1859         if its impurity is above the threshold, otherwise it is a leaf.
1860 
1861         .. deprecated:: 0.19
1862            ``min_impurity_split`` has been deprecated in favor of
1863            ``min_impurity_decrease`` in 0.19. The default value of
1864            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1865            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1866 
1867     sparse_output : bool, optional (default=True)
1868         Whether or not to return a sparse CSR matrix, as default behavior,
1869         or to return a dense array compatible with dense pipeline operators.
1870 
1871     n_jobs : int or None, optional (default=None)
1872         The number of jobs to run in parallel for both `fit` and `predict`.
1873         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.
1874         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`
1875         for more details.
1876 
1877     random_state : int, RandomState instance or None, optional (default=None)
1878         If int, random_state is the seed used by the random number generator;
1879         If RandomState instance, random_state is the random number generator;
1880         If None, the random number generator is the RandomState instance used
1881         by `np.random`.
1882 
1883     verbose : int, optional (default=0)
1884         Controls the verbosity when fitting and predicting.
1885 
1886     warm_start : bool, optional (default=False)
1887         When set to ``True``, reuse the solution of the previous call to fit
1888         and add more estimators to the ensemble, otherwise, just fit a whole
1889         new forest. See :term:`the Glossary <warm_start>`.
1890 
1891     Attributes
1892     ----------
1893     estimators_ : list of DecisionTreeClassifier
1894         The collection of fitted sub-estimators.
1895 
1896     References
1897     ----------
1898     .. [1] P. Geurts, D. Ernst., and L. Wehenkel, "Extremely randomized trees",
1899            Machine Learning, 63(1), 3-42, 2006.
1900     .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  "Fast discriminative
1901            visual codebooks using randomized clustering forests"
1902            NIPS 2007
1903 
1904     """
1905 
1906     criterion = 'mse'
1907     max_features = 1
1908 
1909     def __init__(self,
1910                  n_estimators='warn',
1911                  max_depth=5,
1912                  min_samples_split=2,
1913                  min_samples_leaf=1,
1914                  min_weight_fraction_leaf=0.,
1915                  max_leaf_nodes=None,
1916                  min_impurity_decrease=0.,
1917                  min_impurity_split=None,
1918                  sparse_output=True,
1919                  n_jobs=None,
1920                  random_state=None,
1921                  verbose=0,
1922                  warm_start=False):
1923         super().__init__(
1924             base_estimator=ExtraTreeRegressor(),
1925             n_estimators=n_estimators,
1926             estimator_params=("criterion", "max_depth", "min_samples_split",
1927                               "min_samples_leaf", "min_weight_fraction_leaf",
1928                               "max_features", "max_leaf_nodes",
1929                               "min_impurity_decrease", "min_impurity_split",
1930                               "random_state"),
1931             bootstrap=False,
1932             oob_score=False,
1933             n_jobs=n_jobs,
1934             random_state=random_state,
1935             verbose=verbose,
1936             warm_start=warm_start)
1937 
1938         self.max_depth = max_depth
1939         self.min_samples_split = min_samples_split
1940         self.min_samples_leaf = min_samples_leaf
1941         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1942         self.max_leaf_nodes = max_leaf_nodes
1943         self.min_impurity_decrease = min_impurity_decrease
1944         self.min_impurity_split = min_impurity_split
1945         self.sparse_output = sparse_output
1946 
1947     def _set_oob_score(self, X, y):
1948         raise NotImplementedError("OOB score not supported by tree embedding")
1949 
1950     def fit(self, X, y=None, sample_weight=None):
1951         """Fit estimator.
1952 
1953         Parameters
1954         ----------
1955         X : array-like or sparse matrix, shape=(n_samples, n_features)
1956             The input samples. Use ``dtype=np.float32`` for maximum
1957             efficiency. Sparse matrices are also supported, use sparse
1958             ``csc_matrix`` for maximum efficiency.
1959 
1960         sample_weight : array-like, shape = [n_samples] or None
1961             Sample weights. If None, then samples are equally weighted. Splits
1962             that would create child nodes with net zero or negative weight are
1963             ignored while searching for a split in each node. In the case of
1964             classification, splits are also ignored if they would result in any
1965             single class carrying a negative weight in either child node.
1966 
1967         Returns
1968         -------
1969         self : object
1970 
1971         """
1972         self.fit_transform(X, y, sample_weight=sample_weight)
1973         return self
1974 
1975     def fit_transform(self, X, y=None, sample_weight=None):
1976         """Fit estimator and transform dataset.
1977 
1978         Parameters
1979         ----------
1980         X : array-like or sparse matrix, shape=(n_samples, n_features)
1981             Input data used to build forests. Use ``dtype=np.float32`` for
1982             maximum efficiency.
1983 
1984         sample_weight : array-like, shape = [n_samples] or None
1985             Sample weights. If None, then samples are equally weighted. Splits
1986             that would create child nodes with net zero or negative weight are
1987             ignored while searching for a split in each node. In the case of
1988             classification, splits are also ignored if they would result in any
1989             single class carrying a negative weight in either child node.
1990 
1991         Returns
1992         -------
1993         X_transformed : sparse matrix, shape=(n_samples, n_out)
1994             Transformed dataset.
1995         """
1996         X = check_array(X, accept_sparse=['csc'])
1997         if issparse(X):
1998             # Pre-sort indices to avoid that each individual tree of the
1999             # ensemble sorts the indices.
2000             X.sort_indices()
2001 
2002         rnd = check_random_state(self.random_state)
2003         y = rnd.uniform(size=X.shape[0])
2004         super().fit(X, y, sample_weight=sample_weight)
2005 
2006         self.one_hot_encoder_ = OneHotEncoder(sparse=self.sparse_output,
2007                                               categories='auto')
2008         return self.one_hot_encoder_.fit_transform(self.apply(X))
2009 
2010     def transform(self, X):
2011         """Transform dataset.
2012 
2013         Parameters
2014         ----------
2015         X : array-like or sparse matrix, shape=(n_samples, n_features)
2016             Input data to be transformed. Use ``dtype=np.float32`` for maximum
2017             efficiency. Sparse matrices are also supported, use sparse
2018             ``csr_matrix`` for maximum efficiency.
2019 
2020         Returns
2021         -------
2022         X_transformed : sparse matrix, shape=(n_samples, n_out)
2023             Transformed dataset.
2024         """
2025         check_is_fitted(self, 'one_hot_encoder_')
2026         return self.one_hot_encoder_.transform(self.apply(X))

</code><intended_behavior>While this file handles Random Forests and similar ensemble methods, the validation approach extracted here might serve as a blueprint for modifications in weight_boosting.py. Ensure that any validation logic within this file allows for the flexible handling of input data types based on the capabilities of the constituent base estimators. It's about enabling a broader range of inputs without compromising on data integrity checks necessary for the algorithm to function correctly.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Gradient Boosted Regression Trees
2 
3 This module contains methods for fitting gradient boosted regression trees for
4 both classification and regression.
5 
6 The module structure is the following:
7 
8 - The ``BaseGradientBoosting`` base class implements a common ``fit`` method
9   for all the estimators in the module. Regression and classification
10   only differ in the concrete ``LossFunction`` used.
11 
12 - ``GradientBoostingClassifier`` implements gradient boosting for
13   classification problems.
14 
15 - ``GradientBoostingRegressor`` implements gradient boosting for
16   regression problems.
17 """
18 
19 # Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,
20 #          Arnaud Joly, Jacob Schreiber
21 # License: BSD 3 clause
22 
23 from abc import ABCMeta
24 from abc import abstractmethod
25 
26 from .base import BaseEnsemble
27 from ..base import ClassifierMixin
28 from ..base import RegressorMixin
29 
30 from ._gradient_boosting import predict_stages
31 from ._gradient_boosting import predict_stage
32 from ._gradient_boosting import _random_sample_mask
33 
34 import numbers
35 import numpy as np
36 
37 from scipy.sparse import csc_matrix
38 from scipy.sparse import csr_matrix
39 from scipy.sparse import issparse
40 from scipy.special import expit
41 
42 from time import time
43 from ..model_selection import train_test_split
44 from ..tree.tree import DecisionTreeRegressor
45 from ..tree._tree import DTYPE
46 from ..tree._tree import TREE_LEAF
47 
48 from ..utils import check_random_state
49 from ..utils import check_array
50 from ..utils import check_X_y
51 from ..utils import column_or_1d
52 from ..utils import check_consistent_length
53 from ..utils import deprecated
54 from ..utils.fixes import logsumexp
55 from ..utils.stats import _weighted_percentile
56 from ..utils.validation import check_is_fitted
57 from ..utils.multiclass import check_classification_targets
58 from ..exceptions import NotFittedError
59 
60 
61 class QuantileEstimator:
62     """An estimator predicting the alpha-quantile of the training targets.
63 
64     Parameters
65     ----------
66     alpha : float
67         The quantile
68     """
69     def __init__(self, alpha=0.9):
70         if not 0 < alpha < 1.0:
71             raise ValueError("`alpha` must be in (0, 1.0) but was %r" % alpha)
72         self.alpha = alpha
73 
74     def fit(self, X, y, sample_weight=None):
75         """Fit the estimator.
76 
77         Parameters
78         ----------
79         X : {array-like, sparse matrix}, shape (n_samples, n_features)
80             Training data
81 
82         y : array, shape (n_samples, n_targets)
83             Target values. Will be cast to X's dtype if necessary
84 
85         sample_weight : numpy array of shape (n_samples,)
86             Individual weights for each sample
87         """
88         if sample_weight is None:
89             self.quantile = np.percentile(y, self.alpha * 100.0)
90         else:
91             self.quantile = _weighted_percentile(y, sample_weight,
92                                                  self.alpha * 100.0)
93 
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y
112 
113 
114 class MeanEstimator:
115     """An estimator predicting the mean of the training targets."""
116     def fit(self, X, y, sample_weight=None):
117         """Fit the estimator.
118 
119         Parameters
120         ----------
121         X : {array-like, sparse matrix}, shape (n_samples, n_features)
122             Training data
123 
124         y : array, shape (n_samples, n_targets)
125             Target values. Will be cast to X's dtype if necessary
126 
127         sample_weight : numpy array of shape (n_samples,)
128             Individual weights for each sample
129         """
130         if sample_weight is None:
131             self.mean = np.mean(y)
132         else:
133             self.mean = np.average(y, weights=sample_weight)
134 
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y
153 
154 
155 class LogOddsEstimator:
156     """An estimator predicting the log odds ratio."""
157     scale = 1.0
158 
159     def fit(self, X, y, sample_weight=None):
160         """Fit the estimator.
161 
162         Parameters
163         ----------
164         X : {array-like, sparse matrix}, shape (n_samples, n_features)
165             Training data
166 
167         y : array, shape (n_samples, n_targets)
168             Target values. Will be cast to X's dtype if necessary
169 
170         sample_weight : numpy array of shape (n_samples,)
171             Individual weights for each sample
172         """
173         # pre-cond: pos, neg are encoded as 1, 0
174         if sample_weight is None:
175             pos = np.sum(y)
176             neg = y.shape[0] - pos
177         else:
178             pos = np.sum(sample_weight * y)
179             neg = np.sum(sample_weight * (1 - y))
180 
181         if neg == 0 or pos == 0:
182             raise ValueError('y contains non binary labels.')
183         self.prior = self.scale * np.log(pos / neg)
184 
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y
203 
204 
205 class ScaledLogOddsEstimator(LogOddsEstimator):
206     """Log odds ratio scaled by 0.5 -- for exponential loss. """
207     scale = 0.5
208 
209 
210 class PriorProbabilityEstimator:
211     """An estimator predicting the probability of each
212     class in the training data.
213     """
214     def fit(self, X, y, sample_weight=None):
215         """Fit the estimator.
216 
217         Parameters
218         ----------
219         X : {array-like, sparse matrix}, shape (n_samples, n_features)
220             Training data
221 
222         y : array, shape (n_samples, n_targets)
223             Target values. Will be cast to X's dtype if necessary
224 
225         sample_weight : array, shape (n_samples,)
226             Individual weights for each sample
227         """
228         if sample_weight is None:
229             sample_weight = np.ones_like(y, dtype=np.float64)
230         class_counts = np.bincount(y, weights=sample_weight)
231         self.priors = class_counts / class_counts.sum()
232 
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y
251 
252 
253 class ZeroEstimator:
254     """An estimator that simply predicts zero. """
255 
256     def fit(self, X, y, sample_weight=None):
257         """Fit the estimator.
258 
259         Parameters
260         ----------
261         X : {array-like, sparse matrix}, shape (n_samples, n_features)
262             Training data
263 
264         y : numpy, shape (n_samples, n_targets)
265             Target values. Will be cast to X's dtype if necessary
266 
267         sample_weight : array, shape (n_samples,)
268             Individual weights for each sample
269         """
270         if np.issubdtype(y.dtype, np.signedinteger):
271             # classification
272             self.n_classes = np.unique(y).shape[0]
273             if self.n_classes == 2:
274                 self.n_classes = 1
275         else:
276             # regression
277             self.n_classes = 1
278 
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y
297 
298 
299 class LossFunction(metaclass=ABCMeta):
300     """Abstract base class for various loss functions.
301 
302     Parameters
303     ----------
304     n_classes : int
305         Number of classes
306 
307     Attributes
308     ----------
309     K : int
310         The number of regression trees to be induced;
311         1 for regression and binary classification;
312         ``n_classes`` for multi-class classification.
313     """
314 
315     is_multi_class = False
316 
317     def __init__(self, n_classes):
318         self.K = n_classes
319 
320     def init_estimator(self):
321         """Default ``init`` estimator for loss function. """
322         raise NotImplementedError()
323 
324     @abstractmethod
325     def __call__(self, y, pred, sample_weight=None):
326         """Compute the loss.
327 
328         Parameters
329         ----------
330         y : array, shape (n_samples,)
331             True labels
332 
333         pred : array, shape (n_samples,)
334             Predicted labels
335 
336         sample_weight : array-like, shape (n_samples,), optional
337             Sample weights.
338         """
339 
340     @abstractmethod
341     def negative_gradient(self, y, y_pred, **kargs):
342         """Compute the negative gradient.
343 
344         Parameters
345         ----------
346         y : array, shape (n_samples,)
347             The target labels.
348 
349         y_pred : array, shape (n_samples,)
350             The predictions.
351         """
352 
353     def update_terminal_regions(self, tree, X, y, residual, y_pred,
354                                 sample_weight, sample_mask,
355                                 learning_rate=0.1, k=0):
356         """Update the terminal regions (=leaves) of the given tree and
357         updates the current predictions of the model. Traverses tree
358         and invokes template method `_update_terminal_region`.
359 
360         Parameters
361         ----------
362         tree : tree.Tree
363             The tree object.
364         X : array, shape (n, m)
365             The data array.
366         y : array, shape (n,)
367             The target labels.
368         residual : array, shape (n,)
369             The residuals (usually the negative gradient).
370         y_pred : array, shape (n,)
371             The predictions.
372         sample_weight : array, shape (n,)
373             The weight of each sample.
374         sample_mask : array, shape (n,)
375             The sample mask to be used.
376         learning_rate : float, default=0.1
377             learning rate shrinks the contribution of each tree by
378              ``learning_rate``.
379         k : int, default 0
380             The index of the estimator being updated.
381 
382         """
383         # compute leaf for each sample in ``X``.
384         terminal_regions = tree.apply(X)
385 
386         # mask all which are not in sample mask.
387         masked_terminal_regions = terminal_regions.copy()
388         masked_terminal_regions[~sample_mask] = -1
389 
390         # update each leaf (= perform line search)
391         for leaf in np.where(tree.children_left == TREE_LEAF)[0]:
392             self._update_terminal_region(tree, masked_terminal_regions,
393                                          leaf, X, y, residual,
394                                          y_pred[:, k], sample_weight)
395 
396         # update predictions (both in-bag and out-of-bag)
397         y_pred[:, k] += (learning_rate
398                          * tree.value[:, 0, 0].take(terminal_regions, axis=0))
399 
400     @abstractmethod
401     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
402                                 residual, pred, sample_weight):
403         """Template method for updating terminal regions (=leaves). """
404 
405 
406 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
407     """Base class for regression loss functions.
408 
409     Parameters
410     ----------
411     n_classes : int
412         Number of classes
413     """
414     def __init__(self, n_classes):
415         if n_classes != 1:
416             raise ValueError("``n_classes`` must be 1 for regression but "
417                              "was %r" % n_classes)
418         super().__init__(n_classes)
419 
420 
421 class LeastSquaresError(RegressionLossFunction):
422     """Loss function for least squares (LS) estimation.
423     Terminal regions need not to be updated for least squares.
424 
425     Parameters
426     ----------
427     n_classes : int
428         Number of classes
429     """
430 
431     def init_estimator(self):
432         return MeanEstimator()
433 
434     def __call__(self, y, pred, sample_weight=None):
435         """Compute the least squares loss.
436 
437         Parameters
438         ----------
439         y : array, shape (n_samples,)
440             True labels
441 
442         pred : array, shape (n_samples,)
443             Predicted labels
444 
445         sample_weight : array-like, shape (n_samples,), optional
446             Sample weights.
447         """
448         if sample_weight is None:
449             return np.mean((y - pred.ravel()) ** 2.0)
450         else:
451             return (1.0 / sample_weight.sum() *
452                     np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))
453 
454     def negative_gradient(self, y, pred, **kargs):
455         """Compute the negative gradient.
456 
457         Parameters
458         ----------
459         y : array, shape (n_samples,)
460             The target labels.
461 
462         pred : array, shape (n_samples,)
463             The predictions.
464         """
465         return y - pred.ravel()
466 
467     def update_terminal_regions(self, tree, X, y, residual, y_pred,
468                                 sample_weight, sample_mask,
469                                 learning_rate=0.1, k=0):
470         """Least squares does not need to update terminal regions.
471 
472         But it has to update the predictions.
473 
474         Parameters
475         ----------
476         tree : tree.Tree
477             The tree object.
478         X : array, shape (n, m)
479             The data array.
480         y : array, shape (n,)
481             The target labels.
482         residual : array, shape (n,)
483             The residuals (usually the negative gradient).
484         y_pred : array, shape (n,)
485             The predictions.
486         sample_weight : array, shape (n,)
487             The weight of each sample.
488         sample_mask : array, shape (n,)
489             The sample mask to be used.
490         learning_rate : float, default=0.1
491             learning rate shrinks the contribution of each tree by
492              ``learning_rate``.
493         k : int, default 0
494             The index of the estimator being updated.
495         """
496         # update predictions
497         y_pred[:, k] += learning_rate * tree.predict(X).ravel()
498 
499     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
500                                 residual, pred, sample_weight):
501         pass
502 
503 
504 class LeastAbsoluteError(RegressionLossFunction):
505     """Loss function for least absolute deviation (LAD) regression.
506 
507     Parameters
508     ----------
509     n_classes : int
510         Number of classes
511     """
512     def init_estimator(self):
513         return QuantileEstimator(alpha=0.5)
514 
515     def __call__(self, y, pred, sample_weight=None):
516         """Compute the least absolute error.
517 
518         Parameters
519         ----------
520         y : array, shape (n_samples,)
521             True labels
522 
523         pred : array, shape (n_samples,)
524             Predicted labels
525 
526         sample_weight : array-like, shape (n_samples,), optional
527             Sample weights.
528         """
529         if sample_weight is None:
530             return np.abs(y - pred.ravel()).mean()
531         else:
532             return (1.0 / sample_weight.sum() *
533                     np.sum(sample_weight * np.abs(y - pred.ravel())))
534 
535     def negative_gradient(self, y, pred, **kargs):
536         """Compute the negative gradient.
537 
538         1.0 if y - pred > 0.0 else -1.0
539 
540         Parameters
541         ----------
542         y : array, shape (n_samples,)
543             The target labels.
544 
545         pred : array, shape (n_samples,)
546             The predictions.
547         """
548         pred = pred.ravel()
549         return 2.0 * (y - pred > 0.0) - 1.0
550 
551     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
552                                 residual, pred, sample_weight):
553         """LAD updates terminal regions to median estimates. """
554         terminal_region = np.where(terminal_regions == leaf)[0]
555         sample_weight = sample_weight.take(terminal_region, axis=0)
556         diff = y.take(terminal_region, axis=0) - pred.take(terminal_region, axis=0)
557         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)
558 
559 
560 class HuberLossFunction(RegressionLossFunction):
561     """Huber loss function for robust regression.
562 
563     M-Regression proposed in Friedman 2001.
564 
565     References
566     ----------
567     J. Friedman, Greedy Function Approximation: A Gradient Boosting
568     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
569 
570     Parameters
571     ----------
572     n_classes : int
573         Number of classes
574 
575     alpha : float
576         Percentile at which to extract score
577     """
578 
579     def __init__(self, n_classes, alpha=0.9):
580         super().__init__(n_classes)
581         self.alpha = alpha
582         self.gamma = None
583 
584     def init_estimator(self):
585         return QuantileEstimator(alpha=0.5)
586 
587     def __call__(self, y, pred, sample_weight=None):
588         """Compute the Huber loss.
589 
590         Parameters
591         ----------
592         y : array, shape (n_samples,)
593             True labels
594 
595         pred : array, shape (n_samples,)
596             Predicted labels
597 
598         sample_weight : array-like, shape (n_samples,), optional
599             Sample weights.
600         """
601         pred = pred.ravel()
602         diff = y - pred
603         gamma = self.gamma
604         if gamma is None:
605             if sample_weight is None:
606                 gamma = np.percentile(np.abs(diff), self.alpha * 100)
607             else:
608                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
609 
610         gamma_mask = np.abs(diff) <= gamma
611         if sample_weight is None:
612             sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2.0)
613             lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) - gamma / 2.0))
614             loss = (sq_loss + lin_loss) / y.shape[0]
615         else:
616             sq_loss = np.sum(0.5 * sample_weight[gamma_mask] * diff[gamma_mask] ** 2.0)
617             lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *
618                               (np.abs(diff[~gamma_mask]) - gamma / 2.0))
619             loss = (sq_loss + lin_loss) / sample_weight.sum()
620         return loss
621 
622     def negative_gradient(self, y, pred, sample_weight=None, **kargs):
623         """Compute the negative gradient.
624 
625         Parameters
626         ----------
627         y : array, shape (n_samples,)
628             The target labels.
629 
630         pred : array, shape (n_samples,)
631             The predictions.
632 
633         sample_weight : array-like, shape (n_samples,), optional
634             Sample weights.
635         """
636         pred = pred.ravel()
637         diff = y - pred
638         if sample_weight is None:
639             gamma = np.percentile(np.abs(diff), self.alpha * 100)
640         else:
641             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
642         gamma_mask = np.abs(diff) <= gamma
643         residual = np.zeros((y.shape[0],), dtype=np.float64)
644         residual[gamma_mask] = diff[gamma_mask]
645         residual[~gamma_mask] = gamma * np.sign(diff[~gamma_mask])
646         self.gamma = gamma
647         return residual
648 
649     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
650                                 residual, pred, sample_weight):
651         terminal_region = np.where(terminal_regions == leaf)[0]
652         sample_weight = sample_weight.take(terminal_region, axis=0)
653         gamma = self.gamma
654         diff = (y.take(terminal_region, axis=0)
655                 - pred.take(terminal_region, axis=0))
656         median = _weighted_percentile(diff, sample_weight, percentile=50)
657         diff_minus_median = diff - median
658         tree.value[leaf, 0] = median + np.mean(
659             np.sign(diff_minus_median) *
660             np.minimum(np.abs(diff_minus_median), gamma))
661 
662 
663 class QuantileLossFunction(RegressionLossFunction):
664     """Loss function for quantile regression.
665 
666     Quantile regression allows to estimate the percentiles
667     of the conditional distribution of the target.
668 
669     Parameters
670     ----------
671     n_classes : int
672         Number of classes.
673 
674     alpha : float, optional (default = 0.9)
675         The percentile
676     """
677     def __init__(self, n_classes, alpha=0.9):
678         super().__init__(n_classes)
679         self.alpha = alpha
680         self.percentile = alpha * 100.0
681 
682     def init_estimator(self):
683         return QuantileEstimator(self.alpha)
684 
685     def __call__(self, y, pred, sample_weight=None):
686         """Compute the Quantile loss.
687 
688         Parameters
689         ----------
690         y : array, shape (n_samples,)
691             True labels
692 
693         pred : array, shape (n_samples,)
694             Predicted labels
695 
696         sample_weight : array-like, shape (n_samples,), optional
697             Sample weights.
698         """
699         pred = pred.ravel()
700         diff = y - pred
701         alpha = self.alpha
702 
703         mask = y > pred
704         if sample_weight is None:
705             loss = (alpha * diff[mask].sum() -
706                     (1.0 - alpha) * diff[~mask].sum()) / y.shape[0]
707         else:
708             loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -
709                     (1.0 - alpha) * np.sum(sample_weight[~mask] * diff[~mask])) /
710                     sample_weight.sum())
711         return loss
712 
713     def negative_gradient(self, y, pred, **kargs):
714         """Compute the negative gradient.
715 
716         Parameters
717         ----------
718         y : array, shape (n_samples,)
719             The target labels.
720 
721         pred : array, shape (n_samples,)
722             The predictions.
723         """
724         alpha = self.alpha
725         pred = pred.ravel()
726         mask = y > pred
727         return (alpha * mask) - ((1.0 - alpha) * ~mask)
728 
729     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
730                                 residual, pred, sample_weight):
731         terminal_region = np.where(terminal_regions == leaf)[0]
732         diff = (y.take(terminal_region, axis=0)
733                 - pred.take(terminal_region, axis=0))
734         sample_weight = sample_weight.take(terminal_region, axis=0)
735 
736         val = _weighted_percentile(diff, sample_weight, self.percentile)
737         tree.value[leaf, 0] = val
738 
739 
740 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
741     """Base class for classification loss functions. """
742 
743     def _score_to_proba(self, score):
744         """Template method to convert scores to probabilities.
745 
746          the does not support probabilities raises AttributeError.
747         """
748         raise TypeError('%s does not support predict_proba' % type(self).__name__)
749 
750     @abstractmethod
751     def _score_to_decision(self, score):
752         """Template method to convert scores to decisions.
753 
754         Returns int arrays.
755         """
756 
757 
758 class BinomialDeviance(ClassificationLossFunction):
759     """Binomial deviance loss function for binary classification.
760 
761     Binary classification is a special case; here, we only need to
762     fit one tree instead of ``n_classes`` trees.
763 
764     Parameters
765     ----------
766     n_classes : int
767         Number of classes.
768     """
769     def __init__(self, n_classes):
770         if n_classes != 2:
771             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
772                              .format(self.__class__.__name__, n_classes))
773         # we only need to fit one tree for binary clf.
774         super().__init__(1)
775 
776     def init_estimator(self):
777         return LogOddsEstimator()
778 
779     def __call__(self, y, pred, sample_weight=None):
780         """Compute the deviance (= 2 * negative log-likelihood).
781 
782         Parameters
783         ----------
784         y : array, shape (n_samples,)
785             True labels
786 
787         pred : array, shape (n_samples,)
788             Predicted labels
789 
790         sample_weight : array-like, shape (n_samples,), optional
791             Sample weights.
792         """
793         # logaddexp(0, v) == log(1.0 + exp(v))
794         pred = pred.ravel()
795         if sample_weight is None:
796             return -2.0 * np.mean((y * pred) - np.logaddexp(0.0, pred))
797         else:
798             return (-2.0 / sample_weight.sum() *
799                     np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))
800 
801     def negative_gradient(self, y, pred, **kargs):
802         """Compute the residual (= negative gradient).
803 
804         Parameters
805         ----------
806         y : array, shape (n_samples,)
807             True labels
808 
809         pred : array, shape (n_samples,)
810             Predicted labels
811         """
812         return y - expit(pred.ravel())
813 
814     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
815                                 residual, pred, sample_weight):
816         """Make a single Newton-Raphson step.
817 
818         our node estimate is given by:
819 
820             sum(w * (y - prob)) / sum(w * prob * (1 - prob))
821 
822         we take advantage that: y - prob = residual
823         """
824         terminal_region = np.where(terminal_regions == leaf)[0]
825         residual = residual.take(terminal_region, axis=0)
826         y = y.take(terminal_region, axis=0)
827         sample_weight = sample_weight.take(terminal_region, axis=0)
828 
829         numerator = np.sum(sample_weight * residual)
830         denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))
831 
832         # prevents overflow and division by zero
833         if abs(denominator) < 1e-150:
834             tree.value[leaf, 0, 0] = 0.0
835         else:
836             tree.value[leaf, 0, 0] = numerator / denominator
837 
838     def _score_to_proba(self, score):
839         proba = np.ones((score.shape[0], 2), dtype=np.float64)
840         proba[:, 1] = expit(score.ravel())
841         proba[:, 0] -= proba[:, 1]
842         return proba
843 
844     def _score_to_decision(self, score):
845         proba = self._score_to_proba(score)
846         return np.argmax(proba, axis=1)
847 
848 
849 class MultinomialDeviance(ClassificationLossFunction):
850     """Multinomial deviance loss function for multi-class classification.
851 
852     For multi-class classification we need to fit ``n_classes`` trees at
853     each stage.
854 
855     Parameters
856     ----------
857     n_classes : int
858         Number of classes
859     """
860 
861     is_multi_class = True
862 
863     def __init__(self, n_classes):
864         if n_classes < 3:
865             raise ValueError("{0:s} requires more than 2 classes.".format(
866                 self.__class__.__name__))
867         super().__init__(n_classes)
868 
869     def init_estimator(self):
870         return PriorProbabilityEstimator()
871 
872     def __call__(self, y, pred, sample_weight=None):
873         """Compute the Multinomial deviance.
874 
875         Parameters
876         ----------
877         y : array, shape (n_samples,)
878             True labels
879 
880         pred : array, shape (n_samples,)
881             Predicted labels
882 
883         sample_weight : array-like, shape (n_samples,), optional
884             Sample weights.
885         """
886         # create one-hot label encoding
887         Y = np.zeros((y.shape[0], self.K), dtype=np.float64)
888         for k in range(self.K):
889             Y[:, k] = y == k
890 
891         if sample_weight is None:
892             return np.sum(-1 * (Y * pred).sum(axis=1) +
893                           logsumexp(pred, axis=1))
894         else:
895             return np.sum(-1 * sample_weight * (Y * pred).sum(axis=1) +
896                           logsumexp(pred, axis=1))
897 
898     def negative_gradient(self, y, pred, k=0, **kwargs):
899         """Compute negative gradient for the ``k``-th class.
900 
901         Parameters
902         ----------
903         y : array, shape (n_samples,)
904             The target labels.
905 
906         pred : array, shape (n_samples,)
907             The predictions.
908 
909         k : int, optional (default=0)
910             The index of the class
911         """
912         return y - np.nan_to_num(np.exp(pred[:, k] -
913                                         logsumexp(pred, axis=1)))
914 
915     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
916                                 residual, pred, sample_weight):
917         """Make a single Newton-Raphson step. """
918         terminal_region = np.where(terminal_regions == leaf)[0]
919         residual = residual.take(terminal_region, axis=0)
920         y = y.take(terminal_region, axis=0)
921         sample_weight = sample_weight.take(terminal_region, axis=0)
922 
923         numerator = np.sum(sample_weight * residual)
924         numerator *= (self.K - 1) / self.K
925 
926         denominator = np.sum(sample_weight * (y - residual) *
927                              (1.0 - y + residual))
928 
929         # prevents overflow and division by zero
930         if abs(denominator) < 1e-150:
931             tree.value[leaf, 0, 0] = 0.0
932         else:
933             tree.value[leaf, 0, 0] = numerator / denominator
934 
935     def _score_to_proba(self, score):
936         return np.nan_to_num(
937             np.exp(score - (logsumexp(score, axis=1)[:, np.newaxis])))
938 
939     def _score_to_decision(self, score):
940         proba = self._score_to_proba(score)
941         return np.argmax(proba, axis=1)
942 
943 
944 class ExponentialLoss(ClassificationLossFunction):
945     """Exponential loss function for binary classification.
946 
947     Same loss as AdaBoost.
948 
949     References
950     ----------
951     Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
952 
953     Parameters
954     ----------
955     n_classes : int
956         Number of classes.
957     """
958     def __init__(self, n_classes):
959         if n_classes != 2:
960             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
961                              .format(self.__class__.__name__, n_classes))
962         # we only need to fit one tree for binary clf.
963         super().__init__(1)
964 
965     def init_estimator(self):
966         return ScaledLogOddsEstimator()
967 
968     def __call__(self, y, pred, sample_weight=None):
969         """Compute the exponential loss
970 
971         Parameters
972         ----------
973         y : array, shape (n_samples,)
974             True labels
975 
976         pred : array, shape (n_samples,)
977             Predicted labels
978 
979         sample_weight : array-like, shape (n_samples,), optional
980             Sample weights.
981         """
982         pred = pred.ravel()
983         if sample_weight is None:
984             return np.mean(np.exp(-(2. * y - 1.) * pred))
985         else:
986             return (1.0 / sample_weight.sum() *
987                     np.sum(sample_weight * np.exp(-(2 * y - 1) * pred)))
988 
989     def negative_gradient(self, y, pred, **kargs):
990         """Compute the residual (= negative gradient).
991 
992         Parameters
993         ----------
994         y : array, shape (n_samples,)
995             True labels
996 
997         pred : array, shape (n_samples,)
998             Predicted labels
999         """
1000         y_ = -(2. * y - 1.)
1001         return y_ * np.exp(y_ * pred.ravel())
1002 
1003     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
1004                                 residual, pred, sample_weight):
1005         terminal_region = np.where(terminal_regions == leaf)[0]
1006         pred = pred.take(terminal_region, axis=0)
1007         y = y.take(terminal_region, axis=0)
1008         sample_weight = sample_weight.take(terminal_region, axis=0)
1009 
1010         y_ = 2. * y - 1.
1011 
1012         numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))
1013         denominator = np.sum(sample_weight * np.exp(-y_ * pred))
1014 
1015         # prevents overflow and division by zero
1016         if abs(denominator) < 1e-150:
1017             tree.value[leaf, 0, 0] = 0.0
1018         else:
1019             tree.value[leaf, 0, 0] = numerator / denominator
1020 
1021     def _score_to_proba(self, score):
1022         proba = np.ones((score.shape[0], 2), dtype=np.float64)
1023         proba[:, 1] = expit(2.0 * score.ravel())
1024         proba[:, 0] -= proba[:, 1]
1025         return proba
1026 
1027     def _score_to_decision(self, score):
1028         return (score.ravel() >= 0.0).astype(np.int)
1029 
1030 
1031 LOSS_FUNCTIONS = {'ls': LeastSquaresError,
1032                   'lad': LeastAbsoluteError,
1033                   'huber': HuberLossFunction,
1034                   'quantile': QuantileLossFunction,
1035                   'deviance': None,    # for both, multinomial and binomial
1036                   'exponential': ExponentialLoss,
1037                   }
1038 
1039 
1040 INIT_ESTIMATORS = {'zero': ZeroEstimator}
1041 
1042 
1043 class VerboseReporter:
1044     """Reports verbose output to stdout.
1045 
1046     Parameters
1047     ----------
1048     verbose : int
1049         Verbosity level. If ``verbose==1`` output is printed once in a while
1050         (when iteration mod verbose_mod is zero).; if larger than 1 then output
1051         is printed for each update.
1052     """
1053 
1054     def __init__(self, verbose):
1055         self.verbose = verbose
1056 
1057     def init(self, est, begin_at_stage=0):
1058         """Initialize reporter
1059 
1060         Parameters
1061         ----------
1062         est : Estimator
1063             The estimator
1064 
1065         begin_at_stage : int
1066             stage at which to begin reporting
1067         """
1068         # header fields and line format str
1069         header_fields = ['Iter', 'Train Loss']
1070         verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}']
1071         # do oob?
1072         if est.subsample < 1:
1073             header_fields.append('OOB Improve')
1074             verbose_fmt.append('{oob_impr:>16.4f}')
1075         header_fields.append('Remaining Time')
1076         verbose_fmt.append('{remaining_time:>16s}')
1077 
1078         # print the header line
1079         print(('%10s ' + '%16s ' *
1080                (len(header_fields) - 1)) % tuple(header_fields))
1081 
1082         self.verbose_fmt = ' '.join(verbose_fmt)
1083         # plot verbose info each time i % verbose_mod == 0
1084         self.verbose_mod = 1
1085         self.start_time = time()
1086         self.begin_at_stage = begin_at_stage
1087 
1088     def update(self, j, est):
1089         """Update reporter with new iteration.
1090 
1091         Parameters
1092         ----------
1093         j : int
1094             The new iteration
1095         est : Estimator
1096             The estimator
1097         """
1098         do_oob = est.subsample < 1
1099         # we need to take into account if we fit additional estimators.
1100         i = j - self.begin_at_stage  # iteration relative to the start iter
1101         if (i + 1) % self.verbose_mod == 0:
1102             oob_impr = est.oob_improvement_[j] if do_oob else 0
1103             remaining_time = ((est.n_estimators - (j + 1)) *
1104                               (time() - self.start_time) / float(i + 1))
1105             if remaining_time > 60:
1106                 remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)
1107             else:
1108                 remaining_time = '{0:.2f}s'.format(remaining_time)
1109             print(self.verbose_fmt.format(iter=j + 1,
1110                                           train_score=est.train_score_[j],
1111                                           oob_impr=oob_impr,
1112                                           remaining_time=remaining_time))
1113             if self.verbose == 1 and ((i + 1) // (self.verbose_mod * 10) > 0):
1114                 # adjust verbose frequency (powers of 10)
1115                 self.verbose_mod *= 10
1116 
1117 
1118 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1119     """Abstract base class for Gradient Boosting. """
1120 
1121     @abstractmethod
1122     def __init__(self, loss, learning_rate, n_estimators, criterion,
1123                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1124                  max_depth, min_impurity_decrease, min_impurity_split,
1125                  init, subsample, max_features,
1126                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1127                  warm_start=False, presort='auto',
1128                  validation_fraction=0.1, n_iter_no_change=None,
1129                  tol=1e-4):
1130 
1131         self.n_estimators = n_estimators
1132         self.learning_rate = learning_rate
1133         self.loss = loss
1134         self.criterion = criterion
1135         self.min_samples_split = min_samples_split
1136         self.min_samples_leaf = min_samples_leaf
1137         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1138         self.subsample = subsample
1139         self.max_features = max_features
1140         self.max_depth = max_depth
1141         self.min_impurity_decrease = min_impurity_decrease
1142         self.min_impurity_split = min_impurity_split
1143         self.init = init
1144         self.random_state = random_state
1145         self.alpha = alpha
1146         self.verbose = verbose
1147         self.max_leaf_nodes = max_leaf_nodes
1148         self.warm_start = warm_start
1149         self.presort = presort
1150         self.validation_fraction = validation_fraction
1151         self.n_iter_no_change = n_iter_no_change
1152         self.tol = tol
1153 
1154     def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
1155                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1156         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1157 
1158         assert sample_mask.dtype == np.bool
1159         loss = self.loss_
1160         original_y = y
1161 
1162         # Need to pass a copy of y_pred to negative_gradient() because y_pred
1163         # is partially updated at the end of the loop in
1164         # update_terminal_regions(), and gradients need to be evaluated at
1165         # iteration i - 1.
1166         y_pred_copy = y_pred.copy()
1167 
1168         for k in range(loss.K):
1169             if loss.is_multi_class:
1170                 y = np.array(original_y == k, dtype=np.float64)
1171 
1172             residual = loss.negative_gradient(y, y_pred_copy, k=k,
1173                                               sample_weight=sample_weight)
1174 
1175             # induce regression tree on residuals
1176             tree = DecisionTreeRegressor(
1177                 criterion=self.criterion,
1178                 splitter='best',
1179                 max_depth=self.max_depth,
1180                 min_samples_split=self.min_samples_split,
1181                 min_samples_leaf=self.min_samples_leaf,
1182                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1183                 min_impurity_decrease=self.min_impurity_decrease,
1184                 min_impurity_split=self.min_impurity_split,
1185                 max_features=self.max_features,
1186                 max_leaf_nodes=self.max_leaf_nodes,
1187                 random_state=random_state,
1188                 presort=self.presort)
1189 
1190             if self.subsample < 1.0:
1191                 # no inplace multiplication!
1192                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1193 
1194             X = X_csr if X_csr is not None else X
1195             tree.fit(X, residual, sample_weight=sample_weight,
1196                      check_input=False, X_idx_sorted=X_idx_sorted)
1197 
1198             # update tree leaves
1199             loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
1200                                          sample_weight, sample_mask,
1201                                          learning_rate=self.learning_rate, k=k)
1202 
1203             # add tree to ensemble
1204             self.estimators_[i, k] = tree
1205 
1206         return y_pred
1207 
1208     def _check_params(self):
1209         """Check validity of parameters and raise ValueError if not valid. """
1210         if self.n_estimators <= 0:
1211             raise ValueError("n_estimators must be greater than 0 but "
1212                              "was %r" % self.n_estimators)
1213 
1214         if self.learning_rate <= 0.0:
1215             raise ValueError("learning_rate must be greater than 0 but "
1216                              "was %r" % self.learning_rate)
1217 
1218         if (self.loss not in self._SUPPORTED_LOSS
1219                 or self.loss not in LOSS_FUNCTIONS):
1220             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1221 
1222         if self.loss == 'deviance':
1223             loss_class = (MultinomialDeviance
1224                           if len(self.classes_) > 2
1225                           else BinomialDeviance)
1226         else:
1227             loss_class = LOSS_FUNCTIONS[self.loss]
1228 
1229         if self.loss in ('huber', 'quantile'):
1230             self.loss_ = loss_class(self.n_classes_, self.alpha)
1231         else:
1232             self.loss_ = loss_class(self.n_classes_)
1233 
1234         if not (0.0 < self.subsample <= 1.0):
1235             raise ValueError("subsample must be in (0,1] but "
1236                              "was %r" % self.subsample)
1237 
1238         if self.init is not None:
1239             if isinstance(self.init, str):
1240                 if self.init not in INIT_ESTIMATORS:
1241                     raise ValueError('init="%s" is not supported' % self.init)
1242             else:
1243                 if (not hasattr(self.init, 'fit')
1244                         or not hasattr(self.init, 'predict')):
1245                     raise ValueError("init=%r must be valid BaseEstimator "
1246                                      "and support both fit and "
1247                                      "predict" % self.init)
1248 
1249         if not (0.0 < self.alpha < 1.0):
1250             raise ValueError("alpha must be in (0.0, 1.0) but "
1251                              "was %r" % self.alpha)
1252 
1253         if isinstance(self.max_features, str):
1254             if self.max_features == "auto":
1255                 # if is_classification
1256                 if self.n_classes_ > 1:
1257                     max_features = max(1, int(np.sqrt(self.n_features_)))
1258                 else:
1259                     # is regression
1260                     max_features = self.n_features_
1261             elif self.max_features == "sqrt":
1262                 max_features = max(1, int(np.sqrt(self.n_features_)))
1263             elif self.max_features == "log2":
1264                 max_features = max(1, int(np.log2(self.n_features_)))
1265             else:
1266                 raise ValueError("Invalid value for max_features: %r. "
1267                                  "Allowed string values are 'auto', 'sqrt' "
1268                                  "or 'log2'." % self.max_features)
1269         elif self.max_features is None:
1270             max_features = self.n_features_
1271         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1272             max_features = self.max_features
1273         else:  # float
1274             if 0. < self.max_features <= 1.:
1275                 max_features = max(int(self.max_features *
1276                                        self.n_features_), 1)
1277             else:
1278                 raise ValueError("max_features must be in (0, n_features]")
1279 
1280         self.max_features_ = max_features
1281 
1282         if not isinstance(self.n_iter_no_change,
1283                           (numbers.Integral, np.integer, type(None))):
1284             raise ValueError("n_iter_no_change should either be None or an "
1285                              "integer. %r was passed"
1286                              % self.n_iter_no_change)
1287 
1288         allowed_presort = ('auto', True, False)
1289         if self.presort not in allowed_presort:
1290             raise ValueError("'presort' should be in {}. Got {!r} instead."
1291                              .format(allowed_presort, self.presort))
1292 
1293     def _init_state(self):
1294         """Initialize model state and allocate model state data structures. """
1295 
1296         if self.init is None:
1297             self.init_ = self.loss_.init_estimator()
1298         elif isinstance(self.init, str):
1299             self.init_ = INIT_ESTIMATORS[self.init]()
1300         else:
1301             self.init_ = self.init
1302 
1303         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1304                                     dtype=np.object)
1305         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1306         # do oob?
1307         if self.subsample < 1.0:
1308             self.oob_improvement_ = np.zeros((self.n_estimators),
1309                                              dtype=np.float64)
1310 
1311     def _clear_state(self):
1312         """Clear the state of the gradient boosting model. """
1313         if hasattr(self, 'estimators_'):
1314             self.estimators_ = np.empty((0, 0), dtype=np.object)
1315         if hasattr(self, 'train_score_'):
1316             del self.train_score_
1317         if hasattr(self, 'oob_improvement_'):
1318             del self.oob_improvement_
1319         if hasattr(self, 'init_'):
1320             del self.init_
1321         if hasattr(self, '_rng'):
1322             del self._rng
1323 
1324     def _resize_state(self):
1325         """Add additional ``n_estimators`` entries to all attributes. """
1326         # self.n_estimators is the number of additional est to fit
1327         total_n_estimators = self.n_estimators
1328         if total_n_estimators < self.estimators_.shape[0]:
1329             raise ValueError('resize with smaller n_estimators %d < %d' %
1330                              (total_n_estimators, self.estimators_[0]))
1331 
1332         self.estimators_ = np.resize(self.estimators_,
1333                                      (total_n_estimators, self.loss_.K))
1334         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1335         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1336             # if do oob resize arrays or create new if not available
1337             if hasattr(self, 'oob_improvement_'):
1338                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1339                                                   total_n_estimators)
1340             else:
1341                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1342                                                  dtype=np.float64)
1343 
1344     def _is_initialized(self):
1345         return len(getattr(self, 'estimators_', [])) > 0
1346 
1347     def _check_initialized(self):
1348         """Check that the estimator is initialized, raising an error if not."""
1349         check_is_fitted(self, 'estimators_')
1350 
1351     @property
1352     @deprecated("Attribute n_features was deprecated in version 0.19 and "
1353                 "will be removed in 0.21.")
1354     def n_features(self):
1355         return self.n_features_
1356 
1357     def fit(self, X, y, sample_weight=None, monitor=None):
1358         """Fit the gradient boosting model.
1359 
1360         Parameters
1361         ----------
1362         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1363             The input samples. Internally, it will be converted to
1364             ``dtype=np.float32`` and if a sparse matrix is provided
1365             to a sparse ``csr_matrix``.
1366 
1367         y : array-like, shape (n_samples,)
1368             Target values (strings or integers in classification, real numbers
1369             in regression)
1370             For classification, labels must correspond to classes.
1371 
1372         sample_weight : array-like, shape (n_samples,) or None
1373             Sample weights. If None, then samples are equally weighted. Splits
1374             that would create child nodes with net zero or negative weight are
1375             ignored while searching for a split in each node. In the case of
1376             classification, splits are also ignored if they would result in any
1377             single class carrying a negative weight in either child node.
1378 
1379         monitor : callable, optional
1380             The monitor is called after each iteration with the current
1381             iteration, a reference to the estimator and the local variables of
1382             ``_fit_stages`` as keyword arguments ``callable(i, self,
1383             locals())``. If the callable returns ``True`` the fitting procedure
1384             is stopped. The monitor can be used for various things such as
1385             computing held-out estimates, early stopping, model introspect, and
1386             snapshoting.
1387 
1388         Returns
1389         -------
1390         self : object
1391         """
1392         # if not warmstart - clear the estimator state
1393         if not self.warm_start:
1394             self._clear_state()
1395 
1396         # Check input
1397         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1398         n_samples, self.n_features_ = X.shape
1399         if sample_weight is None:
1400             sample_weight = np.ones(n_samples, dtype=np.float32)
1401         else:
1402             sample_weight = column_or_1d(sample_weight, warn=True)
1403 
1404         check_consistent_length(X, y, sample_weight)
1405 
1406         y = self._validate_y(y, sample_weight)
1407 
1408         if self.n_iter_no_change is not None:
1409             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1410                 train_test_split(X, y, sample_weight,
1411                                  random_state=self.random_state,
1412                                  test_size=self.validation_fraction))
1413         else:
1414             X_val = y_val = sample_weight_val = None
1415 
1416         self._check_params()
1417 
1418         if not self._is_initialized():
1419             # init state
1420             self._init_state()
1421 
1422             # fit initial model - FIXME make sample_weight optional
1423             self.init_.fit(X, y, sample_weight)
1424 
1425             # init predictions
1426             y_pred = self.init_.predict(X)
1427             begin_at_stage = 0
1428 
1429             # The rng state must be preserved if warm_start is True
1430             self._rng = check_random_state(self.random_state)
1431 
1432         else:
1433             # add more estimators to fitted model
1434             # invariant: warm_start = True
1435             if self.n_estimators < self.estimators_.shape[0]:
1436                 raise ValueError('n_estimators=%d must be larger or equal to '
1437                                  'estimators_.shape[0]=%d when '
1438                                  'warm_start==True'
1439                                  % (self.n_estimators,
1440                                     self.estimators_.shape[0]))
1441             begin_at_stage = self.estimators_.shape[0]
1442             # The requirements of _decision_function (called in two lines
1443             # below) are more constrained than fit. It accepts only CSR
1444             # matrices.
1445             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1446             y_pred = self._decision_function(X)
1447             self._resize_state()
1448 
1449         if self.presort is True and issparse(X):
1450             raise ValueError(
1451                 "Presorting is not supported for sparse matrices.")
1452 
1453         presort = self.presort
1454         # Allow presort to be 'auto', which means True if the dataset is dense,
1455         # otherwise it will be False.
1456         if presort == 'auto':
1457             presort = not issparse(X)
1458 
1459         X_idx_sorted = None
1460         if presort:
1461             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1462                                              dtype=np.int32)
1463 
1464         # fit the boosting stages
1465         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
1466                                     X_val, y_val, sample_weight_val,
1467                                     begin_at_stage, monitor, X_idx_sorted)
1468 
1469         # change shape of arrays after fit (early-stopping or additional ests)
1470         if n_stages != self.estimators_.shape[0]:
1471             self.estimators_ = self.estimators_[:n_stages]
1472             self.train_score_ = self.train_score_[:n_stages]
1473             if hasattr(self, 'oob_improvement_'):
1474                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1475 
1476         self.n_estimators_ = n_stages
1477         return self
1478 
1479     def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
1480                     X_val, y_val, sample_weight_val,
1481                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1482         """Iteratively fits the stages.
1483 
1484         For each stage it computes the progress (OOB, train score)
1485         and delegates to ``_fit_stage``.
1486         Returns the number of stages fit; might differ from ``n_estimators``
1487         due to early stopping.
1488         """
1489         n_samples = X.shape[0]
1490         do_oob = self.subsample < 1.0
1491         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1492         n_inbag = max(1, int(self.subsample * n_samples))
1493         loss_ = self.loss_
1494 
1495         # Set min_weight_leaf from min_weight_fraction_leaf
1496         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1497             min_weight_leaf = (self.min_weight_fraction_leaf *
1498                                np.sum(sample_weight))
1499         else:
1500             min_weight_leaf = 0.
1501 
1502         if self.verbose:
1503             verbose_reporter = VerboseReporter(self.verbose)
1504             verbose_reporter.init(self, begin_at_stage)
1505 
1506         X_csc = csc_matrix(X) if issparse(X) else None
1507         X_csr = csr_matrix(X) if issparse(X) else None
1508 
1509         if self.n_iter_no_change is not None:
1510             loss_history = np.full(self.n_iter_no_change, np.inf)
1511             # We create a generator to get the predictions for X_val after
1512             # the addition of each successive stage
1513             y_val_pred_iter = self._staged_decision_function(X_val)
1514 
1515         # perform boosting iterations
1516         i = begin_at_stage
1517         for i in range(begin_at_stage, self.n_estimators):
1518 
1519             # subsampling
1520             if do_oob:
1521                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1522                                                   random_state)
1523                 # OOB score before adding this stage
1524                 old_oob_score = loss_(y[~sample_mask],
1525                                       y_pred[~sample_mask],
1526                                       sample_weight[~sample_mask])
1527 
1528             # fit next stage of trees
1529             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,
1530                                      sample_mask, random_state, X_idx_sorted,
1531                                      X_csc, X_csr)
1532 
1533             # track deviance (= loss)
1534             if do_oob:
1535                 self.train_score_[i] = loss_(y[sample_mask],
1536                                              y_pred[sample_mask],
1537                                              sample_weight[sample_mask])
1538                 self.oob_improvement_[i] = (
1539                     old_oob_score - loss_(y[~sample_mask],
1540                                           y_pred[~sample_mask],
1541                                           sample_weight[~sample_mask]))
1542             else:
1543                 # no need to fancy index w/ no subsampling
1544                 self.train_score_[i] = loss_(y, y_pred, sample_weight)
1545 
1546             if self.verbose > 0:
1547                 verbose_reporter.update(i, self)
1548 
1549             if monitor is not None:
1550                 early_stopping = monitor(i, self, locals())
1551                 if early_stopping:
1552                     break
1553 
1554             # We also provide an early stopping based on the score from
1555             # validation set (X_val, y_val), if n_iter_no_change is set
1556             if self.n_iter_no_change is not None:
1557                 # By calling next(y_val_pred_iter), we get the predictions
1558                 # for X_val after the addition of the current stage
1559                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1560                                         sample_weight_val)
1561 
1562                 # Require validation_score to be better (less) than at least
1563                 # one of the last n_iter_no_change evaluations
1564                 if np.any(validation_loss + self.tol < loss_history):
1565                     loss_history[i % len(loss_history)] = validation_loss
1566                 else:
1567                     break
1568 
1569         return i + 1
1570 
1571     def _make_estimator(self, append=True):
1572         # we don't need _make_estimator
1573         raise NotImplementedError()
1574 
1575     def _init_decision_function(self, X):
1576         """Check input and compute prediction of ``init``. """
1577         self._check_initialized()
1578         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1579         if X.shape[1] != self.n_features_:
1580             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1581                 self.n_features_, X.shape[1]))
1582         score = self.init_.predict(X).astype(np.float64)
1583         return score
1584 
1585     def _decision_function(self, X):
1586         # for use in inner loop, not raveling the output in single-class case,
1587         # not doing input validation.
1588         score = self._init_decision_function(X)
1589         predict_stages(self.estimators_, X, self.learning_rate, score)
1590         return score
1591 
1592 
1593     def _staged_decision_function(self, X):
1594         """Compute decision function of ``X`` for each iteration.
1595 
1596         This method allows monitoring (i.e. determine error on testing set)
1597         after each stage.
1598 
1599         Parameters
1600         ----------
1601         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1602             The input samples. Internally, it will be converted to
1603             ``dtype=np.float32`` and if a sparse matrix is provided
1604             to a sparse ``csr_matrix``.
1605 
1606         Returns
1607         -------
1608         score : generator of array, shape (n_samples, k)
1609             The decision function of the input samples. The order of the
1610             classes corresponds to that in the attribute `classes_`.
1611             Regression and binary classification are special cases with
1612             ``k == 1``, otherwise ``k==n_classes``.
1613         """
1614         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1615         score = self._init_decision_function(X)
1616         for i in range(self.estimators_.shape[0]):
1617             predict_stage(self.estimators_, i, X, self.learning_rate, score)
1618             yield score.copy()
1619 
1620     @property
1621     def feature_importances_(self):
1622         """Return the feature importances (the higher, the more important the
1623            feature).
1624 
1625         Returns
1626         -------
1627         feature_importances_ : array, shape (n_features,)
1628         """
1629         self._check_initialized()
1630 
1631         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1632         for stage in self.estimators_:
1633             stage_sum = sum(tree.tree_.compute_feature_importances(
1634                 normalize=False) for tree in stage) / len(stage)
1635             total_sum += stage_sum
1636 
1637         importances = total_sum / total_sum.sum()
1638         return importances
1639 
1640     def _validate_y(self, y, sample_weight):
1641         # 'sample_weight' is not utilised but is used for
1642         # consistency with similar method _validate_y of GBC
1643         self.n_classes_ = 1
1644         if y.dtype.kind == 'O':
1645             y = y.astype(np.float64)
1646         # Default implementation
1647         return y
1648 
1649     def apply(self, X):
1650         """Apply trees in the ensemble to X, return leaf indices.
1651 
1652         .. versionadded:: 0.17
1653 
1654         Parameters
1655         ----------
1656         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1657             The input samples. Internally, its dtype will be converted to
1658             ``dtype=np.float32``. If a sparse matrix is provided, it will
1659             be converted to a sparse ``csr_matrix``.
1660 
1661         Returns
1662         -------
1663         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1664             For each datapoint x in X and for each tree in the ensemble,
1665             return the index of the leaf x ends up in each estimator.
1666             In the case of binary classification n_classes is 1.
1667         """
1668 
1669         self._check_initialized()
1670         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1671 
1672         # n_classes will be equal to 1 in the binary classification or the
1673         # regression case.
1674         n_estimators, n_classes = self.estimators_.shape
1675         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1676 
1677         for i in range(n_estimators):
1678             for j in range(n_classes):
1679                 estimator = self.estimators_[i, j]
1680                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1681 
1682         return leaves
1683 
1684 
1685 class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
1686     """Gradient Boosting for classification.
1687 
1688     GB builds an additive model in a
1689     forward stage-wise fashion; it allows for the optimization of
1690     arbitrary differentiable loss functions. In each stage ``n_classes_``
1691     regression trees are fit on the negative gradient of the
1692     binomial or multinomial deviance loss function. Binary classification
1693     is a special case where only a single regression tree is induced.
1694 
1695     Read more in the :ref:`User Guide <gradient_boosting>`.
1696 
1697     Parameters
1698     ----------
1699     loss : {'deviance', 'exponential'}, optional (default='deviance')
1700         loss function to be optimized. 'deviance' refers to
1701         deviance (= logistic regression) for classification
1702         with probabilistic outputs. For loss 'exponential' gradient
1703         boosting recovers the AdaBoost algorithm.
1704 
1705     learning_rate : float, optional (default=0.1)
1706         learning rate shrinks the contribution of each tree by `learning_rate`.
1707         There is a trade-off between learning_rate and n_estimators.
1708 
1709     n_estimators : int (default=100)
1710         The number of boosting stages to perform. Gradient boosting
1711         is fairly robust to over-fitting so a large number usually
1712         results in better performance.
1713 
1714     subsample : float, optional (default=1.0)
1715         The fraction of samples to be used for fitting the individual base
1716         learners. If smaller than 1.0 this results in Stochastic Gradient
1717         Boosting. `subsample` interacts with the parameter `n_estimators`.
1718         Choosing `subsample < 1.0` leads to a reduction of variance
1719         and an increase in bias.
1720 
1721     criterion : string, optional (default="friedman_mse")
1722         The function to measure the quality of a split. Supported criteria
1723         are "friedman_mse" for the mean squared error with improvement
1724         score by Friedman, "mse" for mean squared error, and "mae" for
1725         the mean absolute error. The default value of "friedman_mse" is
1726         generally the best as it can provide a better approximation in
1727         some cases.
1728 
1729         .. versionadded:: 0.18
1730 
1731     min_samples_split : int, float, optional (default=2)
1732         The minimum number of samples required to split an internal node:
1733 
1734         - If int, then consider `min_samples_split` as the minimum number.
1735         - If float, then `min_samples_split` is a fraction and
1736           `ceil(min_samples_split * n_samples)` are the minimum
1737           number of samples for each split.
1738 
1739         .. versionchanged:: 0.18
1740            Added float values for fractions.
1741 
1742     min_samples_leaf : int, float, optional (default=1)
1743         The minimum number of samples required to be at a leaf node.
1744         A split point at any depth will only be considered if it leaves at
1745         least ``min_samples_leaf`` training samples in each of the left and
1746         right branches.  This may have the effect of smoothing the model,
1747         especially in regression.
1748 
1749         - If int, then consider `min_samples_leaf` as the minimum number.
1750         - If float, then `min_samples_leaf` is a fraction and
1751           `ceil(min_samples_leaf * n_samples)` are the minimum
1752           number of samples for each node.
1753 
1754         .. versionchanged:: 0.18
1755            Added float values for fractions.
1756 
1757     min_weight_fraction_leaf : float, optional (default=0.)
1758         The minimum weighted fraction of the sum total of weights (of all
1759         the input samples) required to be at a leaf node. Samples have
1760         equal weight when sample_weight is not provided.
1761 
1762     max_depth : integer, optional (default=3)
1763         maximum depth of the individual regression estimators. The maximum
1764         depth limits the number of nodes in the tree. Tune this parameter
1765         for best performance; the best value depends on the interaction
1766         of the input variables.
1767 
1768     min_impurity_decrease : float, optional (default=0.)
1769         A node will be split if this split induces a decrease of the impurity
1770         greater than or equal to this value.
1771 
1772         The weighted impurity decrease equation is the following::
1773 
1774             N_t / N * (impurity - N_t_R / N_t * right_impurity
1775                                 - N_t_L / N_t * left_impurity)
1776 
1777         where ``N`` is the total number of samples, ``N_t`` is the number of
1778         samples at the current node, ``N_t_L`` is the number of samples in the
1779         left child, and ``N_t_R`` is the number of samples in the right child.
1780 
1781         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1782         if ``sample_weight`` is passed.
1783 
1784         .. versionadded:: 0.19
1785 
1786     min_impurity_split : float, (default=1e-7)
1787         Threshold for early stopping in tree growth. A node will split
1788         if its impurity is above the threshold, otherwise it is a leaf.
1789 
1790         .. deprecated:: 0.19
1791            ``min_impurity_split`` has been deprecated in favor of
1792            ``min_impurity_decrease`` in 0.19. The default value of
1793            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1794            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1795 
1796     init : estimator, optional
1797         An estimator object that is used to compute the initial
1798         predictions. ``init`` has to provide ``fit`` and ``predict``.
1799         If None it uses ``loss.init_estimator``.
1800 
1801     random_state : int, RandomState instance or None, optional (default=None)
1802         If int, random_state is the seed used by the random number generator;
1803         If RandomState instance, random_state is the random number generator;
1804         If None, the random number generator is the RandomState instance used
1805         by `np.random`.
1806 
1807     max_features : int, float, string or None, optional (default=None)
1808         The number of features to consider when looking for the best split:
1809 
1810         - If int, then consider `max_features` features at each split.
1811         - If float, then `max_features` is a fraction and
1812           `int(max_features * n_features)` features are considered at each
1813           split.
1814         - If "auto", then `max_features=sqrt(n_features)`.
1815         - If "sqrt", then `max_features=sqrt(n_features)`.
1816         - If "log2", then `max_features=log2(n_features)`.
1817         - If None, then `max_features=n_features`.
1818 
1819         Choosing `max_features < n_features` leads to a reduction of variance
1820         and an increase in bias.
1821 
1822         Note: the search for a split does not stop until at least one
1823         valid partition of the node samples is found, even if it requires to
1824         effectively inspect more than ``max_features`` features.
1825 
1826     verbose : int, default: 0
1827         Enable verbose output. If 1 then it prints progress and performance
1828         once in a while (the more trees the lower the frequency). If greater
1829         than 1 then it prints progress and performance for every tree.
1830 
1831     max_leaf_nodes : int or None, optional (default=None)
1832         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1833         Best nodes are defined as relative reduction in impurity.
1834         If None then unlimited number of leaf nodes.
1835 
1836     warm_start : bool, default: False
1837         When set to ``True``, reuse the solution of the previous call to fit
1838         and add more estimators to the ensemble, otherwise, just erase the
1839         previous solution. See :term:`the Glossary <warm_start>`.
1840 
1841     presort : bool or 'auto', optional (default='auto')
1842         Whether to presort the data to speed up the finding of best splits in
1843         fitting. Auto mode by default will use presorting on dense data and
1844         default to normal sorting on sparse data. Setting presort to true on
1845         sparse data will raise an error.
1846 
1847         .. versionadded:: 0.17
1848            *presort* parameter.
1849 
1850     validation_fraction : float, optional, default 0.1
1851         The proportion of training data to set aside as validation set for
1852         early stopping. Must be between 0 and 1.
1853         Only used if ``n_iter_no_change`` is set to an integer.
1854 
1855         .. versionadded:: 0.20
1856 
1857     n_iter_no_change : int, default None
1858         ``n_iter_no_change`` is used to decide if early stopping will be used
1859         to terminate training when validation score is not improving. By
1860         default it is set to None to disable early stopping. If set to a
1861         number, it will set aside ``validation_fraction`` size of the training
1862         data as validation and terminate training when validation score is not
1863         improving in all of the previous ``n_iter_no_change`` numbers of
1864         iterations.
1865 
1866         .. versionadded:: 0.20
1867 
1868     tol : float, optional, default 1e-4
1869         Tolerance for the early stopping. When the loss is not improving
1870         by at least tol for ``n_iter_no_change`` iterations (if set to a
1871         number), the training stops.
1872 
1873         .. versionadded:: 0.20
1874 
1875     Attributes
1876     ----------
1877     n_estimators_ : int
1878         The number of estimators as selected by early stopping (if
1879         ``n_iter_no_change`` is specified). Otherwise it is set to
1880         ``n_estimators``.
1881 
1882         .. versionadded:: 0.20
1883 
1884     feature_importances_ : array, shape (n_features,)
1885         The feature importances (the higher, the more important the feature).
1886 
1887     oob_improvement_ : array, shape (n_estimators,)
1888         The improvement in loss (= deviance) on the out-of-bag samples
1889         relative to the previous iteration.
1890         ``oob_improvement_[0]`` is the improvement in
1891         loss of the first stage over the ``init`` estimator.
1892 
1893     train_score_ : array, shape (n_estimators,)
1894         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
1895         model at iteration ``i`` on the in-bag sample.
1896         If ``subsample == 1`` this is the deviance on the training data.
1897 
1898     loss_ : LossFunction
1899         The concrete ``LossFunction`` object.
1900 
1901     init_ : estimator
1902         The estimator that provides the initial predictions.
1903         Set via the ``init`` argument or ``loss.init_estimator``.
1904 
1905     estimators_ : ndarray of DecisionTreeRegressor,\
1906 shape (n_estimators, ``loss_.K``)
1907         The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary
1908         classification, otherwise n_classes.
1909 
1910     Notes
1911     -----
1912     The features are always randomly permuted at each split. Therefore,
1913     the best found split may vary, even with the same training data and
1914     ``max_features=n_features``, if the improvement of the criterion is
1915     identical for several splits enumerated during the search of the best
1916     split. To obtain a deterministic behaviour during fitting,
1917     ``random_state`` has to be fixed.
1918 
1919     See also
1920     --------
1921     sklearn.tree.DecisionTreeClassifier, RandomForestClassifier
1922     AdaBoostClassifier
1923 
1924     References
1925     ----------
1926     J. Friedman, Greedy Function Approximation: A Gradient Boosting
1927     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
1928 
1929     J. Friedman, Stochastic Gradient Boosting, 1999
1930 
1931     T. Hastie, R. Tibshirani and J. Friedman.
1932     Elements of Statistical Learning Ed. 2, Springer, 2009.
1933     """
1934 
1935     _SUPPORTED_LOSS = ('deviance', 'exponential')
1936 
1937     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
1938                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
1939                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
1940                  max_depth=3, min_impurity_decrease=0.,
1941                  min_impurity_split=None, init=None,
1942                  random_state=None, max_features=None, verbose=0,
1943                  max_leaf_nodes=None, warm_start=False,
1944                  presort='auto', validation_fraction=0.1,
1945                  n_iter_no_change=None, tol=1e-4):
1946 
1947         super().__init__(
1948             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
1949             criterion=criterion, min_samples_split=min_samples_split,
1950             min_samples_leaf=min_samples_leaf,
1951             min_weight_fraction_leaf=min_weight_fraction_leaf,
1952             max_depth=max_depth, init=init, subsample=subsample,
1953             max_features=max_features,
1954             random_state=random_state, verbose=verbose,
1955             max_leaf_nodes=max_leaf_nodes,
1956             min_impurity_decrease=min_impurity_decrease,
1957             min_impurity_split=min_impurity_split,
1958             warm_start=warm_start, presort=presort,
1959             validation_fraction=validation_fraction,
1960             n_iter_no_change=n_iter_no_change, tol=tol)
1961 
1962     def _validate_y(self, y, sample_weight):
1963         check_classification_targets(y)
1964         self.classes_, y = np.unique(y, return_inverse=True)
1965         n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))
1966         if n_trim_classes < 2:
1967             raise ValueError("y contains %d class after sample_weight "
1968                              "trimmed classes with zero weights, while a "
1969                              "minimum of 2 classes are required."
1970                              % n_trim_classes)
1971         self.n_classes_ = len(self.classes_)
1972         return y
1973 
1974     def decision_function(self, X):
1975         """Compute the decision function of ``X``.
1976 
1977         Parameters
1978         ----------
1979         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1980             The input samples. Internally, it will be converted to
1981             ``dtype=np.float32`` and if a sparse matrix is provided
1982             to a sparse ``csr_matrix``.
1983 
1984         Returns
1985         -------
1986         score : array, shape (n_samples, n_classes) or (n_samples,)
1987             The decision function of the input samples. The order of the
1988             classes corresponds to that in the attribute `classes_`.
1989             Regression and binary classification produce an array of shape
1990             [n_samples].
1991         """
1992         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1993         score = self._decision_function(X)
1994         if score.shape[1] == 1:
1995             return score.ravel()
1996         return score
1997 
1998     def staged_decision_function(self, X):
1999         """Compute decision function of ``X`` for each iteration.
2000 
2001         This method allows monitoring (i.e. determine error on testing set)
2002         after each stage.
2003 
2004         Parameters
2005         ----------
2006         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2007             The input samples. Internally, it will be converted to
2008             ``dtype=np.float32`` and if a sparse matrix is provided
2009             to a sparse ``csr_matrix``.
2010 
2011         Returns
2012         -------
2013         score : generator of array, shape (n_samples, k)
2014             The decision function of the input samples. The order of the
2015             classes corresponds to that in the attribute `classes_`.
2016             Regression and binary classification are special cases with
2017             ``k == 1``, otherwise ``k==n_classes``.
2018         """
2019         yield from self._staged_decision_function(X)
2020 
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)
2039 
2040     def staged_predict(self, X):
2041         """Predict class at each stage for X.
2042 
2043         This method allows monitoring (i.e. determine error on testing set)
2044         after each stage.
2045 
2046         Parameters
2047         ----------
2048         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2049             The input samples. Internally, it will be converted to
2050             ``dtype=np.float32`` and if a sparse matrix is provided
2051             to a sparse ``csr_matrix``.
2052 
2053         Returns
2054         -------
2055         y : generator of array of shape (n_samples,)
2056             The predicted value of the input samples.
2057         """
2058         for score in self._staged_decision_function(X):
2059             decisions = self.loss_._score_to_decision(score)
2060             yield self.classes_.take(decisions, axis=0)
2061 
2062     def predict_proba(self, X):
2063         """Predict class probabilities for X.
2064 
2065         Parameters
2066         ----------
2067         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2068             The input samples. Internally, it will be converted to
2069             ``dtype=np.float32`` and if a sparse matrix is provided
2070             to a sparse ``csr_matrix``.
2071 
2072         Raises
2073         ------
2074         AttributeError
2075             If the ``loss`` does not support probabilities.
2076 
2077         Returns
2078         -------
2079         p : array, shape (n_samples, n_classes)
2080             The class probabilities of the input samples. The order of the
2081             classes corresponds to that in the attribute `classes_`.
2082         """
2083         score = self.decision_function(X)
2084         try:
2085             return self.loss_._score_to_proba(score)
2086         except NotFittedError:
2087             raise
2088         except AttributeError:
2089             raise AttributeError('loss=%r does not support predict_proba' %
2090                                  self.loss)
2091 
2092     def predict_log_proba(self, X):
2093         """Predict class log-probabilities for X.
2094 
2095         Parameters
2096         ----------
2097         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2098             The input samples. Internally, it will be converted to
2099             ``dtype=np.float32`` and if a sparse matrix is provided
2100             to a sparse ``csr_matrix``.
2101 
2102         Raises
2103         ------
2104         AttributeError
2105             If the ``loss`` does not support probabilities.
2106 
2107         Returns
2108         -------
2109         p : array, shape (n_samples, n_classes)
2110             The class log-probabilities of the input samples. The order of the
2111             classes corresponds to that in the attribute `classes_`.
2112         """
2113         proba = self.predict_proba(X)
2114         return np.log(proba)
2115 
2116     def staged_predict_proba(self, X):
2117         """Predict class probabilities at each stage for X.
2118 
2119         This method allows monitoring (i.e. determine error on testing set)
2120         after each stage.
2121 
2122         Parameters
2123         ----------
2124         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2125             The input samples. Internally, it will be converted to
2126             ``dtype=np.float32`` and if a sparse matrix is provided
2127             to a sparse ``csr_matrix``.
2128 
2129         Returns
2130         -------
2131         y : generator of array of shape (n_samples,)
2132             The predicted value of the input samples.
2133         """
2134         try:
2135             for score in self._staged_decision_function(X):
2136                 yield self.loss_._score_to_proba(score)
2137         except NotFittedError:
2138             raise
2139         except AttributeError:
2140             raise AttributeError('loss=%r does not support predict_proba' %
2141                                  self.loss)
2142 
2143 
2144 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
2145     """Gradient Boosting for regression.
2146 
2147     GB builds an additive model in a forward stage-wise fashion;
2148     it allows for the optimization of arbitrary differentiable loss functions.
2149     In each stage a regression tree is fit on the negative gradient of the
2150     given loss function.
2151 
2152     Read more in the :ref:`User Guide <gradient_boosting>`.
2153 
2154     Parameters
2155     ----------
2156     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')
2157         loss function to be optimized. 'ls' refers to least squares
2158         regression. 'lad' (least absolute deviation) is a highly robust
2159         loss function solely based on order information of the input
2160         variables. 'huber' is a combination of the two. 'quantile'
2161         allows quantile regression (use `alpha` to specify the quantile).
2162 
2163     learning_rate : float, optional (default=0.1)
2164         learning rate shrinks the contribution of each tree by `learning_rate`.
2165         There is a trade-off between learning_rate and n_estimators.
2166 
2167     n_estimators : int (default=100)
2168         The number of boosting stages to perform. Gradient boosting
2169         is fairly robust to over-fitting so a large number usually
2170         results in better performance.
2171 
2172     subsample : float, optional (default=1.0)
2173         The fraction of samples to be used for fitting the individual base
2174         learners. If smaller than 1.0 this results in Stochastic Gradient
2175         Boosting. `subsample` interacts with the parameter `n_estimators`.
2176         Choosing `subsample < 1.0` leads to a reduction of variance
2177         and an increase in bias.
2178 
2179     criterion : string, optional (default="friedman_mse")
2180         The function to measure the quality of a split. Supported criteria
2181         are "friedman_mse" for the mean squared error with improvement
2182         score by Friedman, "mse" for mean squared error, and "mae" for
2183         the mean absolute error. The default value of "friedman_mse" is
2184         generally the best as it can provide a better approximation in
2185         some cases.
2186 
2187         .. versionadded:: 0.18
2188 
2189     min_samples_split : int, float, optional (default=2)
2190         The minimum number of samples required to split an internal node:
2191 
2192         - If int, then consider `min_samples_split` as the minimum number.
2193         - If float, then `min_samples_split` is a fraction and
2194           `ceil(min_samples_split * n_samples)` are the minimum
2195           number of samples for each split.
2196 
2197         .. versionchanged:: 0.18
2198            Added float values for fractions.
2199 
2200     min_samples_leaf : int, float, optional (default=1)
2201         The minimum number of samples required to be at a leaf node.
2202         A split point at any depth will only be considered if it leaves at
2203         least ``min_samples_leaf`` training samples in each of the left and
2204         right branches.  This may have the effect of smoothing the model,
2205         especially in regression.
2206 
2207         - If int, then consider `min_samples_leaf` as the minimum number.
2208         - If float, then `min_samples_leaf` is a fraction and
2209           `ceil(min_samples_leaf * n_samples)` are the minimum
2210           number of samples for each node.
2211 
2212         .. versionchanged:: 0.18
2213            Added float values for fractions.
2214 
2215     min_weight_fraction_leaf : float, optional (default=0.)
2216         The minimum weighted fraction of the sum total of weights (of all
2217         the input samples) required to be at a leaf node. Samples have
2218         equal weight when sample_weight is not provided.
2219 
2220     max_depth : integer, optional (default=3)
2221         maximum depth of the individual regression estimators. The maximum
2222         depth limits the number of nodes in the tree. Tune this parameter
2223         for best performance; the best value depends on the interaction
2224         of the input variables.
2225 
2226     min_impurity_decrease : float, optional (default=0.)
2227         A node will be split if this split induces a decrease of the impurity
2228         greater than or equal to this value.
2229 
2230         The weighted impurity decrease equation is the following::
2231 
2232             N_t / N * (impurity - N_t_R / N_t * right_impurity
2233                                 - N_t_L / N_t * left_impurity)
2234 
2235         where ``N`` is the total number of samples, ``N_t`` is the number of
2236         samples at the current node, ``N_t_L`` is the number of samples in the
2237         left child, and ``N_t_R`` is the number of samples in the right child.
2238 
2239         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2240         if ``sample_weight`` is passed.
2241 
2242         .. versionadded:: 0.19
2243 
2244     min_impurity_split : float, (default=1e-7)
2245         Threshold for early stopping in tree growth. A node will split
2246         if its impurity is above the threshold, otherwise it is a leaf.
2247 
2248         .. deprecated:: 0.19
2249            ``min_impurity_split`` has been deprecated in favor of
2250            ``min_impurity_decrease`` in 0.19. The default value of
2251            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
2252            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
2253 
2254     init : estimator, optional (default=None)
2255         An estimator object that is used to compute the initial
2256         predictions. ``init`` has to provide ``fit`` and ``predict``.
2257         If None it uses ``loss.init_estimator``.
2258 
2259     random_state : int, RandomState instance or None, optional (default=None)
2260         If int, random_state is the seed used by the random number generator;
2261         If RandomState instance, random_state is the random number generator;
2262         If None, the random number generator is the RandomState instance used
2263         by `np.random`.
2264 
2265     max_features : int, float, string or None, optional (default=None)
2266         The number of features to consider when looking for the best split:
2267 
2268         - If int, then consider `max_features` features at each split.
2269         - If float, then `max_features` is a fraction and
2270           `int(max_features * n_features)` features are considered at each
2271           split.
2272         - If "auto", then `max_features=n_features`.
2273         - If "sqrt", then `max_features=sqrt(n_features)`.
2274         - If "log2", then `max_features=log2(n_features)`.
2275         - If None, then `max_features=n_features`.
2276 
2277         Choosing `max_features < n_features` leads to a reduction of variance
2278         and an increase in bias.
2279 
2280         Note: the search for a split does not stop until at least one
2281         valid partition of the node samples is found, even if it requires to
2282         effectively inspect more than ``max_features`` features.
2283 
2284     alpha : float (default=0.9)
2285         The alpha-quantile of the huber loss function and the quantile
2286         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
2287 
2288     verbose : int, default: 0
2289         Enable verbose output. If 1 then it prints progress and performance
2290         once in a while (the more trees the lower the frequency). If greater
2291         than 1 then it prints progress and performance for every tree.
2292 
2293     max_leaf_nodes : int or None, optional (default=None)
2294         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2295         Best nodes are defined as relative reduction in impurity.
2296         If None then unlimited number of leaf nodes.
2297 
2298     warm_start : bool, default: False
2299         When set to ``True``, reuse the solution of the previous call to fit
2300         and add more estimators to the ensemble, otherwise, just erase the
2301         previous solution. See :term:`the Glossary <warm_start>`.
2302 
2303     presort : bool or 'auto', optional (default='auto')
2304         Whether to presort the data to speed up the finding of best splits in
2305         fitting. Auto mode by default will use presorting on dense data and
2306         default to normal sorting on sparse data. Setting presort to true on
2307         sparse data will raise an error.
2308 
2309         .. versionadded:: 0.17
2310            optional parameter *presort*.
2311 
2312     validation_fraction : float, optional, default 0.1
2313         The proportion of training data to set aside as validation set for
2314         early stopping. Must be between 0 and 1.
2315         Only used if ``n_iter_no_change`` is set to an integer.
2316 
2317         .. versionadded:: 0.20
2318 
2319     n_iter_no_change : int, default None
2320         ``n_iter_no_change`` is used to decide if early stopping will be used
2321         to terminate training when validation score is not improving. By
2322         default it is set to None to disable early stopping. If set to a
2323         number, it will set aside ``validation_fraction`` size of the training
2324         data as validation and terminate training when validation score is not
2325         improving in all of the previous ``n_iter_no_change`` numbers of
2326         iterations.
2327 
2328         .. versionadded:: 0.20
2329 
2330     tol : float, optional, default 1e-4
2331         Tolerance for the early stopping. When the loss is not improving
2332         by at least tol for ``n_iter_no_change`` iterations (if set to a
2333         number), the training stops.
2334 
2335         .. versionadded:: 0.20
2336 
2337 
2338     Attributes
2339     ----------
2340     feature_importances_ : array, shape (n_features,)
2341         The feature importances (the higher, the more important the feature).
2342 
2343     oob_improvement_ : array, shape (n_estimators,)
2344         The improvement in loss (= deviance) on the out-of-bag samples
2345         relative to the previous iteration.
2346         ``oob_improvement_[0]`` is the improvement in
2347         loss of the first stage over the ``init`` estimator.
2348 
2349     train_score_ : array, shape (n_estimators,)
2350         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
2351         model at iteration ``i`` on the in-bag sample.
2352         If ``subsample == 1`` this is the deviance on the training data.
2353 
2354     loss_ : LossFunction
2355         The concrete ``LossFunction`` object.
2356 
2357     init_ : estimator
2358         The estimator that provides the initial predictions.
2359         Set via the ``init`` argument or ``loss.init_estimator``.
2360 
2361     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)
2362         The collection of fitted sub-estimators.
2363 
2364     Notes
2365     -----
2366     The features are always randomly permuted at each split. Therefore,
2367     the best found split may vary, even with the same training data and
2368     ``max_features=n_features``, if the improvement of the criterion is
2369     identical for several splits enumerated during the search of the best
2370     split. To obtain a deterministic behaviour during fitting,
2371     ``random_state`` has to be fixed.
2372 
2373     See also
2374     --------
2375     DecisionTreeRegressor, RandomForestRegressor
2376 
2377     References
2378     ----------
2379     J. Friedman, Greedy Function Approximation: A Gradient Boosting
2380     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
2381 
2382     J. Friedman, Stochastic Gradient Boosting, 1999
2383 
2384     T. Hastie, R. Tibshirani and J. Friedman.
2385     Elements of Statistical Learning Ed. 2, Springer, 2009.
2386     """
2387 
2388     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')
2389 
2390     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
2391                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
2392                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
2393                  max_depth=3, min_impurity_decrease=0.,
2394                  min_impurity_split=None, init=None, random_state=None,
2395                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
2396                  warm_start=False, presort='auto', validation_fraction=0.1,
2397                  n_iter_no_change=None, tol=1e-4):
2398 
2399         super().__init__(
2400             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
2401             criterion=criterion, min_samples_split=min_samples_split,
2402             min_samples_leaf=min_samples_leaf,
2403             min_weight_fraction_leaf=min_weight_fraction_leaf,
2404             max_depth=max_depth, init=init, subsample=subsample,
2405             max_features=max_features,
2406             min_impurity_decrease=min_impurity_decrease,
2407             min_impurity_split=min_impurity_split,
2408             random_state=random_state, alpha=alpha, verbose=verbose,
2409             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,
2410             presort=presort, validation_fraction=validation_fraction,
2411             n_iter_no_change=n_iter_no_change, tol=tol)
2412 
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()
2430 
2431     def staged_predict(self, X):
2432         """Predict regression target at each stage for X.
2433 
2434         This method allows monitoring (i.e. determine error on testing set)
2435         after each stage.
2436 
2437         Parameters
2438         ----------
2439         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2440             The input samples. Internally, it will be converted to
2441             ``dtype=np.float32`` and if a sparse matrix is provided
2442             to a sparse ``csr_matrix``.
2443 
2444         Returns
2445         -------
2446         y : generator of array of shape (n_samples,)
2447             The predicted value of the input samples.
2448         """
2449         for y in self._staged_decision_function(X):
2450             yield y.ravel()
2451 
2452     def apply(self, X):
2453         """Apply trees in the ensemble to X, return leaf indices.
2454 
2455         .. versionadded:: 0.17
2456 
2457         Parameters
2458         ----------
2459         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2460             The input samples. Internally, its dtype will be converted to
2461             ``dtype=np.float32``. If a sparse matrix is provided, it will
2462             be converted to a sparse ``csr_matrix``.
2463 
2464         Returns
2465         -------
2466         X_leaves : array-like, shape (n_samples, n_estimators)
2467             For each datapoint x in X and for each tree in the ensemble,
2468             return the index of the leaf x ends up in each estimator.
2469         """
2470 
2471         leaves = super().apply(X)
2472         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])
2473         return leaves

</code><intended_behavior>For gradient boosting methods, the intention is similar to above. Adjust the validate_X calls or similar validations to support a wider range of data types as inputs, provided the base estimators can work with those data types. It's crucial to maintain a balance between allowing flexibility and ensuring the input data's appropriacy for the algorithm to work as intended.</intended_behavior>]
2025-10-18 05:28:38.564 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 05:28:38.565 | INFO     | Search-only: skipping patch selection
2025-10-18 05:28:38.663 | INFO     | Task scikit-learn__scikit-learn-13174 completed successfully.
2025-10-18 05:28:38.664 | INFO     | No patch generated. You can try running ACR again.
