[
    {
        "rel_file_path": "sphinx/builders/__init__.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/sphinx-doc__sphinx/setup_sphinx-doc__sphinx__3.5/sphinx/builders/__init__.py",
        "start": 53,
        "end": 619,
        "class_name": "Builder",
        "method_name": null,
        "code": "53 class Builder:\n54     \"\"\"\n55     Builds target formats from the reST sources.\n56     \"\"\"\n57 \n58     #: The builder's name, for the -b command line option.\n59     name = ''\n60     #: The builder's output format, or '' if no document output is produced.\n61     format = ''\n62     #: The message emitted upon successful build completion. This can be a\n63     #: printf-style template string with the following keys: ``outdir``,\n64     #: ``project``\n65     epilog = ''\n66 \n67     #: default translator class for the builder.  This can be overridden by\n68     #: :py:meth:`app.set_translator()`.\n69     default_translator_class = None  # type: Type[nodes.NodeVisitor]\n70     # doctree versioning method\n71     versioning_method = 'none'\n72     versioning_compare = False\n73     # allow parallel write_doc() calls\n74     allow_parallel = False\n75     # support translation\n76     use_message_catalog = True\n77 \n78     #: The list of MIME types of image formats supported by the builder.\n79     #: Image files are searched in the order in which they appear here.\n80     supported_image_types = []  # type: List[str]\n81     #: The builder supports remote images or not.\n82     supported_remote_images = False\n83     #: The builder supports data URIs or not.\n84     supported_data_uri_images = False\n85 \n86     def __init__(self, app: \"Sphinx\") -> None:\n87         self.srcdir = app.srcdir\n88         self.confdir = app.confdir\n89         self.outdir = app.outdir\n90         self.doctreedir = app.doctreedir\n91         ensuredir(self.doctreedir)\n92 \n93         self.app = app              # type: Sphinx\n94         self.env = None             # type: BuildEnvironment\n95         self.events = app.events    # type: EventManager\n96         self.config = app.config    # type: Config\n97         self.tags = app.tags        # type: Tags\n98         self.tags.add(self.format)\n99         self.tags.add(self.name)\n100         self.tags.add(\"format_%s\" % self.format)\n101         self.tags.add(\"builder_%s\" % self.name)\n102 \n103         # images that need to be copied over (source -> dest)\n104         self.images = {}  # type: Dict[str, str]\n105         # basename of images directory\n106         self.imagedir = \"\"\n107         # relative path to image directory from current docname (used at writing docs)\n108         self.imgpath = \"\"\n109 \n110         # these get set later\n111         self.parallel_ok = False\n112         self.finish_tasks = None  # type: Any\n113 \n114     def set_environment(self, env: BuildEnvironment) -> None:\n115         \"\"\"Store BuildEnvironment object.\"\"\"\n116         self.env = env\n117         self.env.set_versioning_method(self.versioning_method,\n118                                        self.versioning_compare)\n119 \n120     def get_translator_class(self, *args: Any) -> \"Type[nodes.NodeVisitor]\":\n121         \"\"\"Return a class of translator.\"\"\"\n122         return self.app.registry.get_translator_class(self)\n123 \n124     def create_translator(self, *args: Any) -> nodes.NodeVisitor:\n125         \"\"\"Return an instance of translator.\n126 \n127         This method returns an instance of ``default_translator_class`` by default.\n128         Users can replace the translator class with ``app.set_translator()`` API.\n129         \"\"\"\n130         return self.app.registry.create_translator(self, *args)\n131 \n132     # helper methods\n133     def init(self) -> None:\n134         \"\"\"Load necessary templates and perform initialization.  The default\n135         implementation does nothing.\n136         \"\"\"\n137         pass\n138 \n139     def create_template_bridge(self) -> None:\n140         \"\"\"Return the template bridge configured.\"\"\"\n141         if self.config.template_bridge:\n142             self.templates = import_object(self.config.template_bridge,\n143                                            'template_bridge setting')()\n144         else:\n145             from sphinx.jinja2glue import BuiltinTemplateLoader\n146             self.templates = BuiltinTemplateLoader()\n147 \n148     def get_target_uri(self, docname: str, typ: str = None) -> str:\n149         \"\"\"Return the target URI for a document name.\n150 \n151         *typ* can be used to qualify the link characteristic for individual\n152         builders.\n153         \"\"\"\n154         raise NotImplementedError\n155 \n156     def get_relative_uri(self, from_: str, to: str, typ: str = None) -> str:\n157         \"\"\"Return a relative URI between two source filenames.\n158 \n159         May raise environment.NoUri if there's no way to return a sensible URI.\n160         \"\"\"\n161         return relative_uri(self.get_target_uri(from_),\n162                             self.get_target_uri(to, typ))\n163 \n164     def get_outdated_docs(self) -> Union[str, Iterable[str]]:\n165         \"\"\"Return an iterable of output files that are outdated, or a string\n166         describing what an update build will build.\n167 \n168         If the builder does not output individual files corresponding to\n169         source files, return a string here.  If it does, return an iterable\n170         of those files that need to be written.\n171         \"\"\"\n172         raise NotImplementedError\n173 \n174     def get_asset_paths(self) -> List[str]:\n175         \"\"\"Return list of paths for assets (ex. templates, CSS, etc.).\"\"\"\n176         return []\n177 \n178     def post_process_images(self, doctree: Node) -> None:\n179         \"\"\"Pick the best candidate for all image URIs.\"\"\"\n180         images = ImageAdapter(self.env)\n181         for node in doctree.traverse(nodes.image):\n182             if '?' in node['candidates']:\n183                 # don't rewrite nonlocal image URIs\n184                 continue\n185             if '*' not in node['candidates']:\n186                 for imgtype in self.supported_image_types:\n187                     candidate = node['candidates'].get(imgtype, None)\n188                     if candidate:\n189                         break\n190                 else:\n191                     mimetypes = sorted(node['candidates'])\n192                     image_uri = images.get_original_image_uri(node['uri'])\n193                     if mimetypes:\n194                         logger.warning(__('a suitable image for %s builder not found: '\n195                                           '%s (%s)'),\n196                                        self.name, mimetypes, image_uri, location=node)\n197                     else:\n198                         logger.warning(__('a suitable image for %s builder not found: %s'),\n199                                        self.name, image_uri, location=node)\n200                     continue\n201                 node['uri'] = candidate\n202             else:\n203                 candidate = node['uri']\n204             if candidate not in self.env.images:\n205                 # non-existing URI; let it alone\n206                 continue\n207             self.images[candidate] = self.env.images[candidate][1]\n208 \n209     # compile po methods\n210 \n211     def compile_catalogs(self, catalogs: Set[CatalogInfo], message: str) -> None:\n212         if not self.config.gettext_auto_build:\n213             return\n214 \n215         def cat2relpath(cat: CatalogInfo) -> str:\n216             return relpath(cat.mo_path, self.env.srcdir).replace(path.sep, SEP)\n217 \n218         logger.info(bold(__('building [mo]: ')) + message)\n219         for catalog in status_iterator(catalogs, __('writing output... '), \"darkgreen\",\n220                                        len(catalogs), self.app.verbosity,\n221                                        stringify_func=cat2relpath):\n222             catalog.write_mo(self.config.language)\n223 \n224     def compile_all_catalogs(self) -> None:\n225         repo = CatalogRepository(self.srcdir, self.config.locale_dirs,\n226                                  self.config.language, self.config.source_encoding)\n227         message = __('all of %d po files') % len(list(repo.catalogs))\n228         self.compile_catalogs(set(repo.catalogs), message)\n229 \n230     def compile_specific_catalogs(self, specified_files: List[str]) -> None:\n231         def to_domain(fpath: str) -> str:\n232             docname = self.env.path2doc(path.abspath(fpath))\n233             if docname:\n234                 return docname_to_domain(docname, self.config.gettext_compact)\n235             else:\n236                 return None\n237 \n238         catalogs = set()\n239         domains = set(map(to_domain, specified_files))\n240         repo = CatalogRepository(self.srcdir, self.config.locale_dirs,\n241                                  self.config.language, self.config.source_encoding)\n242         for catalog in repo.catalogs:\n243             if catalog.domain in domains and catalog.is_outdated():\n244                 catalogs.add(catalog)\n245         message = __('targets for %d po files that are specified') % len(catalogs)\n246         self.compile_catalogs(catalogs, message)\n247 \n248     def compile_update_catalogs(self) -> None:\n249         repo = CatalogRepository(self.srcdir, self.config.locale_dirs,\n250                                  self.config.language, self.config.source_encoding)\n251         catalogs = {c for c in repo.catalogs if c.is_outdated()}\n252         message = __('targets for %d po files that are out of date') % len(catalogs)\n253         self.compile_catalogs(catalogs, message)\n254 \n255     # build methods\n256 \n257     def build_all(self) -> None:\n258         \"\"\"Build all source files.\"\"\"\n259         self.build(None, summary=__('all source files'), method='all')\n260 \n261     def build_specific(self, filenames: List[str]) -> None:\n262         \"\"\"Only rebuild as much as needed for changes in the *filenames*.\"\"\"\n263         # bring the filenames to the canonical format, that is,\n264         # relative to the source directory and without source_suffix.\n265         dirlen = len(self.srcdir) + 1\n266         to_write = []\n267         suffixes = None  # type: Tuple[str]\n268         suffixes = tuple(self.config.source_suffix)  # type: ignore\n269         for filename in filenames:\n270             filename = path.normpath(path.abspath(filename))\n271             if not filename.startswith(self.srcdir):\n272                 logger.warning(__('file %r given on command line is not under the '\n273                                   'source directory, ignoring'), filename)\n274                 continue\n275             if not path.isfile(filename):\n276                 logger.warning(__('file %r given on command line does not exist, '\n277                                   'ignoring'), filename)\n278                 continue\n279             filename = filename[dirlen:]\n280             for suffix in suffixes:\n281                 if filename.endswith(suffix):\n282                     filename = filename[:-len(suffix)]\n283                     break\n284             filename = filename.replace(path.sep, SEP)\n285             to_write.append(filename)\n286         self.build(to_write, method='specific',\n287                    summary=__('%d source files given on command line') % len(to_write))\n288 \n289     def build_update(self) -> None:\n290         \"\"\"Only rebuild what was changed or added since last build.\"\"\"\n291         to_build = self.get_outdated_docs()\n292         if isinstance(to_build, str):\n293             self.build(['__all__'], to_build)\n294         else:\n295             to_build = list(to_build)\n296             self.build(to_build,\n297                        summary=__('targets for %d source files that are out of date') %\n298                        len(to_build))\n299 \n300     def build(self, docnames: Iterable[str], summary: str = None, method: str = 'update') -> None:  # NOQA\n301         \"\"\"Main build method.\n302 \n303         First updates the environment, and then calls :meth:`write`.\n304         \"\"\"\n305         if summary:\n306             logger.info(bold(__('building [%s]: ') % self.name) + summary)\n307 \n308         # while reading, collect all warnings from docutils\n309         with logging.pending_warnings():\n310             updated_docnames = set(self.read())\n311 \n312         doccount = len(updated_docnames)\n313         logger.info(bold(__('looking for now-outdated files... ')), nonl=True)\n314         for docname in self.env.check_dependents(self.app, updated_docnames):\n315             updated_docnames.add(docname)\n316         outdated = len(updated_docnames) - doccount\n317         if outdated:\n318             logger.info(__('%d found'), outdated)\n319         else:\n320             logger.info(__('none found'))\n321 \n322         if updated_docnames:\n323             # save the environment\n324             from sphinx.application import ENV_PICKLE_FILENAME\n325             with progress_message(__('pickling environment')):\n326                 with open(path.join(self.doctreedir, ENV_PICKLE_FILENAME), 'wb') as f:\n327                     pickle.dump(self.env, f, pickle.HIGHEST_PROTOCOL)\n328 \n329             # global actions\n330             self.app.phase = BuildPhase.CONSISTENCY_CHECK\n331             with progress_message(__('checking consistency')):\n332                 self.env.check_consistency()\n333         else:\n334             if method == 'update' and not docnames:\n335                 logger.info(bold(__('no targets are out of date.')))\n336                 return\n337 \n338         self.app.phase = BuildPhase.RESOLVING\n339 \n340         # filter \"docnames\" (list of outdated files) by the updated\n341         # found_docs of the environment; this will remove docs that\n342         # have since been removed\n343         if docnames and docnames != ['__all__']:\n344             docnames = set(docnames) & self.env.found_docs\n345 \n346         # determine if we can write in parallel\n347         if parallel_available and self.app.parallel > 1 and self.allow_parallel:\n348             self.parallel_ok = self.app.is_parallel_allowed('write')\n349         else:\n350             self.parallel_ok = False\n351 \n352         #  create a task executor to use for misc. \"finish-up\" tasks\n353         # if self.parallel_ok:\n354         #     self.finish_tasks = ParallelTasks(self.app.parallel)\n355         # else:\n356         # for now, just execute them serially\n357         self.finish_tasks = SerialTasks()\n358 \n359         # write all \"normal\" documents (or everything for some builders)\n360         self.write(docnames, list(updated_docnames), method)\n361 \n362         # finish (write static files etc.)\n363         self.finish()\n364 \n365         # wait for all tasks\n366         self.finish_tasks.join()\n367 \n368     def read(self) -> List[str]:\n369         \"\"\"(Re-)read all files new or changed since last update.\n370 \n371         Store all environment docnames in the canonical format (ie using SEP as\n372         a separator in place of os.path.sep).\n373         \"\"\"\n374         logger.info(bold(__('updating environment: ')), nonl=True)\n375 \n376         self.env.find_files(self.config, self)\n377         updated = (self.env.config_status != CONFIG_OK)\n378         added, changed, removed = self.env.get_outdated_files(updated)\n379 \n380         # allow user intervention as well\n381         for docs in self.events.emit('env-get-outdated', self.env, added, changed, removed):\n382             changed.update(set(docs) & self.env.found_docs)\n383 \n384         # if files were added or removed, all documents with globbed toctrees\n385         # must be reread\n386         if added or removed:\n387             # ... but not those that already were removed\n388             changed.update(self.env.glob_toctrees & self.env.found_docs)\n389 \n390         if updated:  # explain the change iff build config status was not ok\n391             reason = (CONFIG_CHANGED_REASON.get(self.env.config_status, '') +\n392                       (self.env.config_status_extra or ''))\n393             logger.info('[%s] ', reason, nonl=True)\n394 \n395         logger.info(__('%s added, %s changed, %s removed'),\n396                     len(added), len(changed), len(removed))\n397 \n398         # clear all files no longer present\n399         for docname in removed:\n400             self.events.emit('env-purge-doc', self.env, docname)\n401             self.env.clear_doc(docname)\n402 \n403         # read all new and changed files\n404         docnames = sorted(added | changed)\n405         # allow changing and reordering the list of docs to read\n406         self.events.emit('env-before-read-docs', self.env, docnames)\n407 \n408         # check if we should do parallel or serial read\n409         if parallel_available and len(docnames) > 5 and self.app.parallel > 1:\n410             par_ok = self.app.is_parallel_allowed('read')\n411         else:\n412             par_ok = False\n413 \n414         if par_ok:\n415             self._read_parallel(docnames, nproc=self.app.parallel)\n416         else:\n417             self._read_serial(docnames)\n418 \n419         if self.config.master_doc not in self.env.all_docs:\n420             raise SphinxError('master file %s not found' %\n421                               self.env.doc2path(self.config.master_doc))\n422 \n423         for retval in self.events.emit('env-updated', self.env):\n424             if retval is not None:\n425                 docnames.extend(retval)\n426 \n427         # workaround: marked as okay to call builder.read() twice in same process\n428         self.env.config_status = CONFIG_OK\n429 \n430         return sorted(docnames)\n431 \n432     def _read_serial(self, docnames: List[str]) -> None:\n433         for docname in status_iterator(docnames, __('reading sources... '), \"purple\",\n434                                        len(docnames), self.app.verbosity):\n435             # remove all inventory entries for that file\n436             self.events.emit('env-purge-doc', self.env, docname)\n437             self.env.clear_doc(docname)\n438             self.read_doc(docname)\n439 \n440     def _read_parallel(self, docnames: List[str], nproc: int) -> None:\n441         # clear all outdated docs at once\n442         for docname in docnames:\n443             self.events.emit('env-purge-doc', self.env, docname)\n444             self.env.clear_doc(docname)\n445 \n446         def read_process(docs: List[str]) -> bytes:\n447             self.env.app = self.app\n448             for docname in docs:\n449                 self.read_doc(docname)\n450             # allow pickling self to send it back\n451             return pickle.dumps(self.env, pickle.HIGHEST_PROTOCOL)\n452 \n453         def merge(docs: List[str], otherenv: bytes) -> None:\n454             env = pickle.loads(otherenv)\n455             self.env.merge_info_from(docs, env, self.app)\n456 \n457         tasks = ParallelTasks(nproc)\n458         chunks = make_chunks(docnames, nproc)\n459 \n460         for chunk in status_iterator(chunks, __('reading sources... '), \"purple\",\n461                                      len(chunks), self.app.verbosity):\n462             tasks.add_task(read_process, chunk, merge)\n463 \n464         # make sure all threads have finished\n465         logger.info(bold(__('waiting for workers...')))\n466         tasks.join()\n467 \n468     def read_doc(self, docname: str) -> None:\n469         \"\"\"Parse a file and add/update inventory entries for the doctree.\"\"\"\n470         self.env.prepare_settings(docname)\n471 \n472         # Add confdir/docutils.conf to dependencies list if exists\n473         docutilsconf = path.join(self.confdir, 'docutils.conf')\n474         if path.isfile(docutilsconf):\n475             self.env.note_dependency(docutilsconf)\n476 \n477         with sphinx_domains(self.env), rst.default_role(docname, self.config.default_role):\n478             doctree = read_doc(self.app, self.env, self.env.doc2path(docname))\n479 \n480         # store time of reading, for outdated files detection\n481         # (Some filesystems have coarse timestamp resolution;\n482         # therefore time.time() can be older than filesystem's timestamp.\n483         # For example, FAT32 has 2sec timestamp resolution.)\n484         self.env.all_docs[docname] = max(time.time(),\n485                                          path.getmtime(self.env.doc2path(docname)))\n486 \n487         # cleanup\n488         self.env.temp_data.clear()\n489         self.env.ref_context.clear()\n490 \n491         self.write_doctree(docname, doctree)\n492 \n493     def write_doctree(self, docname: str, doctree: nodes.document) -> None:\n494         \"\"\"Write the doctree to a file.\"\"\"\n495         # make it picklable\n496         doctree.reporter = None\n497         doctree.transformer = None\n498         doctree.settings.warning_stream = None\n499         doctree.settings.env = None\n500         doctree.settings.record_dependencies = None\n501 \n502         doctree_filename = path.join(self.doctreedir, docname + '.doctree')\n503         ensuredir(path.dirname(doctree_filename))\n504         with open(doctree_filename, 'wb') as f:\n505             pickle.dump(doctree, f, pickle.HIGHEST_PROTOCOL)\n506 \n507     def write(self, build_docnames: Iterable[str], updated_docnames: Sequence[str], method: str = 'update') -> None:  # NOQA\n508         if build_docnames is None or build_docnames == ['__all__']:\n509             # build_all\n510             build_docnames = self.env.found_docs\n511         if method == 'update':\n512             # build updated ones as well\n513             docnames = set(build_docnames) | set(updated_docnames)\n514         else:\n515             docnames = set(build_docnames)\n516         logger.debug(__('docnames to write: %s'), ', '.join(sorted(docnames)))\n517 \n518         # add all toctree-containing files that may have changed\n519         for docname in list(docnames):\n520             for tocdocname in self.env.files_to_rebuild.get(docname, set()):\n521                 if tocdocname in self.env.found_docs:\n522                     docnames.add(tocdocname)\n523         docnames.add(self.config.master_doc)\n524 \n525         with progress_message(__('preparing documents')):\n526             self.prepare_writing(docnames)\n527 \n528         if self.parallel_ok:\n529             # number of subprocesses is parallel-1 because the main process\n530             # is busy loading doctrees and doing write_doc_serialized()\n531             self._write_parallel(sorted(docnames),\n532                                  nproc=self.app.parallel - 1)\n533         else:\n534             self._write_serial(sorted(docnames))\n535 \n536     def _write_serial(self, docnames: Sequence[str]) -> None:\n537         with logging.pending_warnings():\n538             for docname in status_iterator(docnames, __('writing output... '), \"darkgreen\",\n539                                            len(docnames), self.app.verbosity):\n540                 self.app.phase = BuildPhase.RESOLVING\n541                 doctree = self.env.get_and_resolve_doctree(docname, self)\n542                 self.app.phase = BuildPhase.WRITING\n543                 self.write_doc_serialized(docname, doctree)\n544                 self.write_doc(docname, doctree)\n545 \n546     def _write_parallel(self, docnames: Sequence[str], nproc: int) -> None:\n547         def write_process(docs: List[Tuple[str, nodes.document]]) -> None:\n548             self.app.phase = BuildPhase.WRITING\n549             for docname, doctree in docs:\n550                 self.write_doc(docname, doctree)\n551 \n552         # warm up caches/compile templates using the first document\n553         firstname, docnames = docnames[0], docnames[1:]\n554         self.app.phase = BuildPhase.RESOLVING\n555         doctree = self.env.get_and_resolve_doctree(firstname, self)\n556         self.app.phase = BuildPhase.WRITING\n557         self.write_doc_serialized(firstname, doctree)\n558         self.write_doc(firstname, doctree)\n559 \n560         tasks = ParallelTasks(nproc)\n561         chunks = make_chunks(docnames, nproc)\n562 \n563         self.app.phase = BuildPhase.RESOLVING\n564         for chunk in status_iterator(chunks, __('writing output... '), \"darkgreen\",\n565                                      len(chunks), self.app.verbosity):\n566             arg = []\n567             for i, docname in enumerate(chunk):\n568                 doctree = self.env.get_and_resolve_doctree(docname, self)\n569                 self.write_doc_serialized(docname, doctree)\n570                 arg.append((docname, doctree))\n571             tasks.add_task(write_process, arg)\n572 \n573         # make sure all threads have finished\n574         logger.info(bold(__('waiting for workers...')))\n575         tasks.join()\n576 \n577     def prepare_writing(self, docnames: Set[str]) -> None:\n578         \"\"\"A place where you can add logic before :meth:`write_doc` is run\"\"\"\n579         raise NotImplementedError\n580 \n581     def write_doc(self, docname: str, doctree: nodes.document) -> None:\n582         \"\"\"Where you actually write something to the filesystem.\"\"\"\n583         raise NotImplementedError\n584 \n585     def write_doc_serialized(self, docname: str, doctree: nodes.document) -> None:\n586         \"\"\"Handle parts of write_doc that must be called in the main process\n587         if parallel build is active.\n588         \"\"\"\n589         pass\n590 \n591     def finish(self) -> None:\n592         \"\"\"Finish the building process.\n593 \n594         The default implementation does nothing.\n595         \"\"\"\n596         pass\n597 \n598     def cleanup(self) -> None:\n599         \"\"\"Cleanup any resources.\n600 \n601         The default implementation does nothing.\n602         \"\"\"\n603         pass\n604 \n605     def get_builder_config(self, option: str, default: str) -> Any:\n606         \"\"\"Return a builder specific option.\n607 \n608         This method allows customization of common builder settings by\n609         inserting the name of the current builder in the option key.\n610         If the key does not exist, use default as builder name.\n611         \"\"\"\n612         # At the moment, only XXX_use_index is looked up this way.\n613         # Every new builder variant must be registered in Config.config_values.\n614         try:\n615             optname = '%s_%s' % (self.name, option)\n616             return getattr(self.config, optname)\n617         except AttributeError:\n618             optname = '%s_%s' % (default, option)\n619             return getattr(self.config, optname)\n",
        "intended_behavior": "A new method, let\u2019s name it normalize_path, should be introduced in the Builder class. This method should take a file path as an input and apply path normalization to it using os.path.normpath or a similar approach. The method should handle various inputs, such as absolute paths, relative paths, and paths with mixed separators (UNIX and Windows), to produce a consistent, canonical form of the path. This normalization should occur before any file-related operations, such as copying or linking files, to ensure that duplicate files are not created for the same resource. This functionality is intended to preserve existing features while preventing the creation of duplicate files from different references to the same file."
    },
    {
        "rel_file_path": "sphinx/builders/__init__.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/sphinx-doc__sphinx/setup_sphinx-doc__sphinx__3.5/sphinx/builders/__init__.py",
        "start": 53,
        "end": 619,
        "class_name": "Builder",
        "method_name": null,
        "code": "53 class Builder:\n54     \"\"\"\n55     Builds target formats from the reST sources.\n56     \"\"\"\n57 \n58     #: The builder's name, for the -b command line option.\n59     name = ''\n60     #: The builder's output format, or '' if no document output is produced.\n61     format = ''\n62     #: The message emitted upon successful build completion. This can be a\n63     #: printf-style template string with the following keys: ``outdir``,\n64     #: ``project``\n65     epilog = ''\n66 \n67     #: default translator class for the builder.  This can be overridden by\n68     #: :py:meth:`app.set_translator()`.\n69     default_translator_class = None  # type: Type[nodes.NodeVisitor]\n70     # doctree versioning method\n71     versioning_method = 'none'\n72     versioning_compare = False\n73     # allow parallel write_doc() calls\n74     allow_parallel = False\n75     # support translation\n76     use_message_catalog = True\n77 \n78     #: The list of MIME types of image formats supported by the builder.\n79     #: Image files are searched in the order in which they appear here.\n80     supported_image_types = []  # type: List[str]\n81     #: The builder supports remote images or not.\n82     supported_remote_images = False\n83     #: The builder supports data URIs or not.\n84     supported_data_uri_images = False\n85 \n86     def __init__(self, app: \"Sphinx\") -> None:\n87         self.srcdir = app.srcdir\n88         self.confdir = app.confdir\n89         self.outdir = app.outdir\n90         self.doctreedir = app.doctreedir\n91         ensuredir(self.doctreedir)\n92 \n93         self.app = app              # type: Sphinx\n94         self.env = None             # type: BuildEnvironment\n95         self.events = app.events    # type: EventManager\n96         self.config = app.config    # type: Config\n97         self.tags = app.tags        # type: Tags\n98         self.tags.add(self.format)\n99         self.tags.add(self.name)\n100         self.tags.add(\"format_%s\" % self.format)\n101         self.tags.add(\"builder_%s\" % self.name)\n102 \n103         # images that need to be copied over (source -> dest)\n104         self.images = {}  # type: Dict[str, str]\n105         # basename of images directory\n106         self.imagedir = \"\"\n107         # relative path to image directory from current docname (used at writing docs)\n108         self.imgpath = \"\"\n109 \n110         # these get set later\n111         self.parallel_ok = False\n112         self.finish_tasks = None  # type: Any\n113 \n114     def set_environment(self, env: BuildEnvironment) -> None:\n115         \"\"\"Store BuildEnvironment object.\"\"\"\n116         self.env = env\n117         self.env.set_versioning_method(self.versioning_method,\n118                                        self.versioning_compare)\n119 \n120     def get_translator_class(self, *args: Any) -> \"Type[nodes.NodeVisitor]\":\n121         \"\"\"Return a class of translator.\"\"\"\n122         return self.app.registry.get_translator_class(self)\n123 \n124     def create_translator(self, *args: Any) -> nodes.NodeVisitor:\n125         \"\"\"Return an instance of translator.\n126 \n127         This method returns an instance of ``default_translator_class`` by default.\n128         Users can replace the translator class with ``app.set_translator()`` API.\n129         \"\"\"\n130         return self.app.registry.create_translator(self, *args)\n131 \n132     # helper methods\n133     def init(self) -> None:\n134         \"\"\"Load necessary templates and perform initialization.  The default\n135         implementation does nothing.\n136         \"\"\"\n137         pass\n138 \n139     def create_template_bridge(self) -> None:\n140         \"\"\"Return the template bridge configured.\"\"\"\n141         if self.config.template_bridge:\n142             self.templates = import_object(self.config.template_bridge,\n143                                            'template_bridge setting')()\n144         else:\n145             from sphinx.jinja2glue import BuiltinTemplateLoader\n146             self.templates = BuiltinTemplateLoader()\n147 \n148     def get_target_uri(self, docname: str, typ: str = None) -> str:\n149         \"\"\"Return the target URI for a document name.\n150 \n151         *typ* can be used to qualify the link characteristic for individual\n152         builders.\n153         \"\"\"\n154         raise NotImplementedError\n155 \n156     def get_relative_uri(self, from_: str, to: str, typ: str = None) -> str:\n157         \"\"\"Return a relative URI between two source filenames.\n158 \n159         May raise environment.NoUri if there's no way to return a sensible URI.\n160         \"\"\"\n161         return relative_uri(self.get_target_uri(from_),\n162                             self.get_target_uri(to, typ))\n163 \n164     def get_outdated_docs(self) -> Union[str, Iterable[str]]:\n165         \"\"\"Return an iterable of output files that are outdated, or a string\n166         describing what an update build will build.\n167 \n168         If the builder does not output individual files corresponding to\n169         source files, return a string here.  If it does, return an iterable\n170         of those files that need to be written.\n171         \"\"\"\n172         raise NotImplementedError\n173 \n174     def get_asset_paths(self) -> List[str]:\n175         \"\"\"Return list of paths for assets (ex. templates, CSS, etc.).\"\"\"\n176         return []\n177 \n178     def post_process_images(self, doctree: Node) -> None:\n179         \"\"\"Pick the best candidate for all image URIs.\"\"\"\n180         images = ImageAdapter(self.env)\n181         for node in doctree.traverse(nodes.image):\n182             if '?' in node['candidates']:\n183                 # don't rewrite nonlocal image URIs\n184                 continue\n185             if '*' not in node['candidates']:\n186                 for imgtype in self.supported_image_types:\n187                     candidate = node['candidates'].get(imgtype, None)\n188                     if candidate:\n189                         break\n190                 else:\n191                     mimetypes = sorted(node['candidates'])\n192                     image_uri = images.get_original_image_uri(node['uri'])\n193                     if mimetypes:\n194                         logger.warning(__('a suitable image for %s builder not found: '\n195                                           '%s (%s)'),\n196                                        self.name, mimetypes, image_uri, location=node)\n197                     else:\n198                         logger.warning(__('a suitable image for %s builder not found: %s'),\n199                                        self.name, image_uri, location=node)\n200                     continue\n201                 node['uri'] = candidate\n202             else:\n203                 candidate = node['uri']\n204             if candidate not in self.env.images:\n205                 # non-existing URI; let it alone\n206                 continue\n207             self.images[candidate] = self.env.images[candidate][1]\n208 \n209     # compile po methods\n210 \n211     def compile_catalogs(self, catalogs: Set[CatalogInfo], message: str) -> None:\n212         if not self.config.gettext_auto_build:\n213             return\n214 \n215         def cat2relpath(cat: CatalogInfo) -> str:\n216             return relpath(cat.mo_path, self.env.srcdir).replace(path.sep, SEP)\n217 \n218         logger.info(bold(__('building [mo]: ')) + message)\n219         for catalog in status_iterator(catalogs, __('writing output... '), \"darkgreen\",\n220                                        len(catalogs), self.app.verbosity,\n221                                        stringify_func=cat2relpath):\n222             catalog.write_mo(self.config.language)\n223 \n224     def compile_all_catalogs(self) -> None:\n225         repo = CatalogRepository(self.srcdir, self.config.locale_dirs,\n226                                  self.config.language, self.config.source_encoding)\n227         message = __('all of %d po files') % len(list(repo.catalogs))\n228         self.compile_catalogs(set(repo.catalogs), message)\n229 \n230     def compile_specific_catalogs(self, specified_files: List[str]) -> None:\n231         def to_domain(fpath: str) -> str:\n232             docname = self.env.path2doc(path.abspath(fpath))\n233             if docname:\n234                 return docname_to_domain(docname, self.config.gettext_compact)\n235             else:\n236                 return None\n237 \n238         catalogs = set()\n239         domains = set(map(to_domain, specified_files))\n240         repo = CatalogRepository(self.srcdir, self.config.locale_dirs,\n241                                  self.config.language, self.config.source_encoding)\n242         for catalog in repo.catalogs:\n243             if catalog.domain in domains and catalog.is_outdated():\n244                 catalogs.add(catalog)\n245         message = __('targets for %d po files that are specified') % len(catalogs)\n246         self.compile_catalogs(catalogs, message)\n247 \n248     def compile_update_catalogs(self) -> None:\n249         repo = CatalogRepository(self.srcdir, self.config.locale_dirs,\n250                                  self.config.language, self.config.source_encoding)\n251         catalogs = {c for c in repo.catalogs if c.is_outdated()}\n252         message = __('targets for %d po files that are out of date') % len(catalogs)\n253         self.compile_catalogs(catalogs, message)\n254 \n255     # build methods\n256 \n257     def build_all(self) -> None:\n258         \"\"\"Build all source files.\"\"\"\n259         self.build(None, summary=__('all source files'), method='all')\n260 \n261     def build_specific(self, filenames: List[str]) -> None:\n262         \"\"\"Only rebuild as much as needed for changes in the *filenames*.\"\"\"\n263         # bring the filenames to the canonical format, that is,\n264         # relative to the source directory and without source_suffix.\n265         dirlen = len(self.srcdir) + 1\n266         to_write = []\n267         suffixes = None  # type: Tuple[str]\n268         suffixes = tuple(self.config.source_suffix)  # type: ignore\n269         for filename in filenames:\n270             filename = path.normpath(path.abspath(filename))\n271             if not filename.startswith(self.srcdir):\n272                 logger.warning(__('file %r given on command line is not under the '\n273                                   'source directory, ignoring'), filename)\n274                 continue\n275             if not path.isfile(filename):\n276                 logger.warning(__('file %r given on command line does not exist, '\n277                                   'ignoring'), filename)\n278                 continue\n279             filename = filename[dirlen:]\n280             for suffix in suffixes:\n281                 if filename.endswith(suffix):\n282                     filename = filename[:-len(suffix)]\n283                     break\n284             filename = filename.replace(path.sep, SEP)\n285             to_write.append(filename)\n286         self.build(to_write, method='specific',\n287                    summary=__('%d source files given on command line') % len(to_write))\n288 \n289     def build_update(self) -> None:\n290         \"\"\"Only rebuild what was changed or added since last build.\"\"\"\n291         to_build = self.get_outdated_docs()\n292         if isinstance(to_build, str):\n293             self.build(['__all__'], to_build)\n294         else:\n295             to_build = list(to_build)\n296             self.build(to_build,\n297                        summary=__('targets for %d source files that are out of date') %\n298                        len(to_build))\n299 \n300     def build(self, docnames: Iterable[str], summary: str = None, method: str = 'update') -> None:  # NOQA\n301         \"\"\"Main build method.\n302 \n303         First updates the environment, and then calls :meth:`write`.\n304         \"\"\"\n305         if summary:\n306             logger.info(bold(__('building [%s]: ') % self.name) + summary)\n307 \n308         # while reading, collect all warnings from docutils\n309         with logging.pending_warnings():\n310             updated_docnames = set(self.read())\n311 \n312         doccount = len(updated_docnames)\n313         logger.info(bold(__('looking for now-outdated files... ')), nonl=True)\n314         for docname in self.env.check_dependents(self.app, updated_docnames):\n315             updated_docnames.add(docname)\n316         outdated = len(updated_docnames) - doccount\n317         if outdated:\n318             logger.info(__('%d found'), outdated)\n319         else:\n320             logger.info(__('none found'))\n321 \n322         if updated_docnames:\n323             # save the environment\n324             from sphinx.application import ENV_PICKLE_FILENAME\n325             with progress_message(__('pickling environment')):\n326                 with open(path.join(self.doctreedir, ENV_PICKLE_FILENAME), 'wb') as f:\n327                     pickle.dump(self.env, f, pickle.HIGHEST_PROTOCOL)\n328 \n329             # global actions\n330             self.app.phase = BuildPhase.CONSISTENCY_CHECK\n331             with progress_message(__('checking consistency')):\n332                 self.env.check_consistency()\n333         else:\n334             if method == 'update' and not docnames:\n335                 logger.info(bold(__('no targets are out of date.')))\n336                 return\n337 \n338         self.app.phase = BuildPhase.RESOLVING\n339 \n340         # filter \"docnames\" (list of outdated files) by the updated\n341         # found_docs of the environment; this will remove docs that\n342         # have since been removed\n343         if docnames and docnames != ['__all__']:\n344             docnames = set(docnames) & self.env.found_docs\n345 \n346         # determine if we can write in parallel\n347         if parallel_available and self.app.parallel > 1 and self.allow_parallel:\n348             self.parallel_ok = self.app.is_parallel_allowed('write')\n349         else:\n350             self.parallel_ok = False\n351 \n352         #  create a task executor to use for misc. \"finish-up\" tasks\n353         # if self.parallel_ok:\n354         #     self.finish_tasks = ParallelTasks(self.app.parallel)\n355         # else:\n356         # for now, just execute them serially\n357         self.finish_tasks = SerialTasks()\n358 \n359         # write all \"normal\" documents (or everything for some builders)\n360         self.write(docnames, list(updated_docnames), method)\n361 \n362         # finish (write static files etc.)\n363         self.finish()\n364 \n365         # wait for all tasks\n366         self.finish_tasks.join()\n367 \n368     def read(self) -> List[str]:\n369         \"\"\"(Re-)read all files new or changed since last update.\n370 \n371         Store all environment docnames in the canonical format (ie using SEP as\n372         a separator in place of os.path.sep).\n373         \"\"\"\n374         logger.info(bold(__('updating environment: ')), nonl=True)\n375 \n376         self.env.find_files(self.config, self)\n377         updated = (self.env.config_status != CONFIG_OK)\n378         added, changed, removed = self.env.get_outdated_files(updated)\n379 \n380         # allow user intervention as well\n381         for docs in self.events.emit('env-get-outdated', self.env, added, changed, removed):\n382             changed.update(set(docs) & self.env.found_docs)\n383 \n384         # if files were added or removed, all documents with globbed toctrees\n385         # must be reread\n386         if added or removed:\n387             # ... but not those that already were removed\n388             changed.update(self.env.glob_toctrees & self.env.found_docs)\n389 \n390         if updated:  # explain the change iff build config status was not ok\n391             reason = (CONFIG_CHANGED_REASON.get(self.env.config_status, '') +\n392                       (self.env.config_status_extra or ''))\n393             logger.info('[%s] ', reason, nonl=True)\n394 \n395         logger.info(__('%s added, %s changed, %s removed'),\n396                     len(added), len(changed), len(removed))\n397 \n398         # clear all files no longer present\n399         for docname in removed:\n400             self.events.emit('env-purge-doc', self.env, docname)\n401             self.env.clear_doc(docname)\n402 \n403         # read all new and changed files\n404         docnames = sorted(added | changed)\n405         # allow changing and reordering the list of docs to read\n406         self.events.emit('env-before-read-docs', self.env, docnames)\n407 \n408         # check if we should do parallel or serial read\n409         if parallel_available and len(docnames) > 5 and self.app.parallel > 1:\n410             par_ok = self.app.is_parallel_allowed('read')\n411         else:\n412             par_ok = False\n413 \n414         if par_ok:\n415             self._read_parallel(docnames, nproc=self.app.parallel)\n416         else:\n417             self._read_serial(docnames)\n418 \n419         if self.config.master_doc not in self.env.all_docs:\n420             raise SphinxError('master file %s not found' %\n421                               self.env.doc2path(self.config.master_doc))\n422 \n423         for retval in self.events.emit('env-updated', self.env):\n424             if retval is not None:\n425                 docnames.extend(retval)\n426 \n427         # workaround: marked as okay to call builder.read() twice in same process\n428         self.env.config_status = CONFIG_OK\n429 \n430         return sorted(docnames)\n431 \n432     def _read_serial(self, docnames: List[str]) -> None:\n433         for docname in status_iterator(docnames, __('reading sources... '), \"purple\",\n434                                        len(docnames), self.app.verbosity):\n435             # remove all inventory entries for that file\n436             self.events.emit('env-purge-doc', self.env, docname)\n437             self.env.clear_doc(docname)\n438             self.read_doc(docname)\n439 \n440     def _read_parallel(self, docnames: List[str], nproc: int) -> None:\n441         # clear all outdated docs at once\n442         for docname in docnames:\n443             self.events.emit('env-purge-doc', self.env, docname)\n444             self.env.clear_doc(docname)\n445 \n446         def read_process(docs: List[str]) -> bytes:\n447             self.env.app = self.app\n448             for docname in docs:\n449                 self.read_doc(docname)\n450             # allow pickling self to send it back\n451             return pickle.dumps(self.env, pickle.HIGHEST_PROTOCOL)\n452 \n453         def merge(docs: List[str], otherenv: bytes) -> None:\n454             env = pickle.loads(otherenv)\n455             self.env.merge_info_from(docs, env, self.app)\n456 \n457         tasks = ParallelTasks(nproc)\n458         chunks = make_chunks(docnames, nproc)\n459 \n460         for chunk in status_iterator(chunks, __('reading sources... '), \"purple\",\n461                                      len(chunks), self.app.verbosity):\n462             tasks.add_task(read_process, chunk, merge)\n463 \n464         # make sure all threads have finished\n465         logger.info(bold(__('waiting for workers...')))\n466         tasks.join()\n467 \n468     def read_doc(self, docname: str) -> None:\n469         \"\"\"Parse a file and add/update inventory entries for the doctree.\"\"\"\n470         self.env.prepare_settings(docname)\n471 \n472         # Add confdir/docutils.conf to dependencies list if exists\n473         docutilsconf = path.join(self.confdir, 'docutils.conf')\n474         if path.isfile(docutilsconf):\n475             self.env.note_dependency(docutilsconf)\n476 \n477         with sphinx_domains(self.env), rst.default_role(docname, self.config.default_role):\n478             doctree = read_doc(self.app, self.env, self.env.doc2path(docname))\n479 \n480         # store time of reading, for outdated files detection\n481         # (Some filesystems have coarse timestamp resolution;\n482         # therefore time.time() can be older than filesystem's timestamp.\n483         # For example, FAT32 has 2sec timestamp resolution.)\n484         self.env.all_docs[docname] = max(time.time(),\n485                                          path.getmtime(self.env.doc2path(docname)))\n486 \n487         # cleanup\n488         self.env.temp_data.clear()\n489         self.env.ref_context.clear()\n490 \n491         self.write_doctree(docname, doctree)\n492 \n493     def write_doctree(self, docname: str, doctree: nodes.document) -> None:\n494         \"\"\"Write the doctree to a file.\"\"\"\n495         # make it picklable\n496         doctree.reporter = None\n497         doctree.transformer = None\n498         doctree.settings.warning_stream = None\n499         doctree.settings.env = None\n500         doctree.settings.record_dependencies = None\n501 \n502         doctree_filename = path.join(self.doctreedir, docname + '.doctree')\n503         ensuredir(path.dirname(doctree_filename))\n504         with open(doctree_filename, 'wb') as f:\n505             pickle.dump(doctree, f, pickle.HIGHEST_PROTOCOL)\n506 \n507     def write(self, build_docnames: Iterable[str], updated_docnames: Sequence[str], method: str = 'update') -> None:  # NOQA\n508         if build_docnames is None or build_docnames == ['__all__']:\n509             # build_all\n510             build_docnames = self.env.found_docs\n511         if method == 'update':\n512             # build updated ones as well\n513             docnames = set(build_docnames) | set(updated_docnames)\n514         else:\n515             docnames = set(build_docnames)\n516         logger.debug(__('docnames to write: %s'), ', '.join(sorted(docnames)))\n517 \n518         # add all toctree-containing files that may have changed\n519         for docname in list(docnames):\n520             for tocdocname in self.env.files_to_rebuild.get(docname, set()):\n521                 if tocdocname in self.env.found_docs:\n522                     docnames.add(tocdocname)\n523         docnames.add(self.config.master_doc)\n524 \n525         with progress_message(__('preparing documents')):\n526             self.prepare_writing(docnames)\n527 \n528         if self.parallel_ok:\n529             # number of subprocesses is parallel-1 because the main process\n530             # is busy loading doctrees and doing write_doc_serialized()\n531             self._write_parallel(sorted(docnames),\n532                                  nproc=self.app.parallel - 1)\n533         else:\n534             self._write_serial(sorted(docnames))\n535 \n536     def _write_serial(self, docnames: Sequence[str]) -> None:\n537         with logging.pending_warnings():\n538             for docname in status_iterator(docnames, __('writing output... '), \"darkgreen\",\n539                                            len(docnames), self.app.verbosity):\n540                 self.app.phase = BuildPhase.RESOLVING\n541                 doctree = self.env.get_and_resolve_doctree(docname, self)\n542                 self.app.phase = BuildPhase.WRITING\n543                 self.write_doc_serialized(docname, doctree)\n544                 self.write_doc(docname, doctree)\n545 \n546     def _write_parallel(self, docnames: Sequence[str], nproc: int) -> None:\n547         def write_process(docs: List[Tuple[str, nodes.document]]) -> None:\n548             self.app.phase = BuildPhase.WRITING\n549             for docname, doctree in docs:\n550                 self.write_doc(docname, doctree)\n551 \n552         # warm up caches/compile templates using the first document\n553         firstname, docnames = docnames[0], docnames[1:]\n554         self.app.phase = BuildPhase.RESOLVING\n555         doctree = self.env.get_and_resolve_doctree(firstname, self)\n556         self.app.phase = BuildPhase.WRITING\n557         self.write_doc_serialized(firstname, doctree)\n558         self.write_doc(firstname, doctree)\n559 \n560         tasks = ParallelTasks(nproc)\n561         chunks = make_chunks(docnames, nproc)\n562 \n563         self.app.phase = BuildPhase.RESOLVING\n564         for chunk in status_iterator(chunks, __('writing output... '), \"darkgreen\",\n565                                      len(chunks), self.app.verbosity):\n566             arg = []\n567             for i, docname in enumerate(chunk):\n568                 doctree = self.env.get_and_resolve_doctree(docname, self)\n569                 self.write_doc_serialized(docname, doctree)\n570                 arg.append((docname, doctree))\n571             tasks.add_task(write_process, arg)\n572 \n573         # make sure all threads have finished\n574         logger.info(bold(__('waiting for workers...')))\n575         tasks.join()\n576 \n577     def prepare_writing(self, docnames: Set[str]) -> None:\n578         \"\"\"A place where you can add logic before :meth:`write_doc` is run\"\"\"\n579         raise NotImplementedError\n580 \n581     def write_doc(self, docname: str, doctree: nodes.document) -> None:\n582         \"\"\"Where you actually write something to the filesystem.\"\"\"\n583         raise NotImplementedError\n584 \n585     def write_doc_serialized(self, docname: str, doctree: nodes.document) -> None:\n586         \"\"\"Handle parts of write_doc that must be called in the main process\n587         if parallel build is active.\n588         \"\"\"\n589         pass\n590 \n591     def finish(self) -> None:\n592         \"\"\"Finish the building process.\n593 \n594         The default implementation does nothing.\n595         \"\"\"\n596         pass\n597 \n598     def cleanup(self) -> None:\n599         \"\"\"Cleanup any resources.\n600 \n601         The default implementation does nothing.\n602         \"\"\"\n603         pass\n604 \n605     def get_builder_config(self, option: str, default: str) -> Any:\n606         \"\"\"Return a builder specific option.\n607 \n608         This method allows customization of common builder settings by\n609         inserting the name of the current builder in the option key.\n610         If the key does not exist, use default as builder name.\n611         \"\"\"\n612         # At the moment, only XXX_use_index is looked up this way.\n613         # Every new builder variant must be registered in Config.config_values.\n614         try:\n615             optname = '%s_%s' % (self.name, option)\n616             return getattr(self.config, optname)\n617         except AttributeError:\n618             optname = '%s_%s' % (default, option)\n619             return getattr(self.config, optname)\n",
        "intended_behavior": "Within methods that handle writing to the file system or processing output files (e.g., write_doc, post_process_images), ensure that the normalize_path method is called for any file paths before proceeding with their processing. This ensures that all paths are consistently normalized, reducing the chance of duplicates due to differing path references. This adjustment preserves the method's ability to process and output files appropriately while ensuring that the same resource referenced in varying ways is treated as identical, thus avoiding duplicate file creation in the output directory."
    }
]