[
    {
        "rel_file_path": "django/db/models/expressions.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/models/expressions.py",
        "start": 1199,
        "end": 1231,
        "class_name": "Exists",
        "method_name": null,
        "code": "1199 class Exists(Subquery):\n1200     template = 'EXISTS(%(subquery)s)'\n1201     output_field = fields.BooleanField()\n1202 \n1203     def __init__(self, queryset, negated=False, **kwargs):\n1204         self.negated = negated\n1205         super().__init__(queryset, **kwargs)\n1206 \n1207     def __invert__(self):\n1208         clone = self.copy()\n1209         clone.negated = not self.negated\n1210         return clone\n1211 \n1212     def as_sql(self, compiler, connection, template=None, **extra_context):\n1213         query = self.query.exists(using=connection.alias)\n1214         sql, params = super().as_sql(\n1215             compiler,\n1216             connection,\n1217             template=template,\n1218             query=query,\n1219             **extra_context,\n1220         )\n1221         if self.negated:\n1222             sql = 'NOT {}'.format(sql)\n1223         return sql, params\n1224 \n1225     def select_format(self, compiler, sql, params):\n1226         # Wrap EXISTS() with a CASE WHEN expression if a database backend\n1227         # (e.g. Oracle) doesn't support boolean expression in SELECT or GROUP\n1228         # BY list.\n1229         if not compiler.connection.features.supports_boolean_expr_in_select_clause:\n1230             sql = 'CASE WHEN {} THEN 1 ELSE 0 END'.format(sql)\n1231         return sql, params\n",
        "intended_behavior": "Ensure that when `Exists` is constructed with an empty queryset, especially when negated, the resulting SQL maintains the correct WHERE clause structure. The `as_sql` method, which generates the SQL for the EXISTS expression, should be reviewed to confirm it handles empty querysets in a way that does not lead to the entire WHERE block being omitted. This means likely adding logic to preserve a placeholder or no-op condition in the SQL when faced with an empty queryset, ensuring the WHERE clause is still present even if it's logically empty."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/models/query.py",
        "start": 1061,
        "end": 1065,
        "class_name": "QuerySet",
        "method_name": "none",
        "code": "1061     def none(self):\n1062         \"\"\"Return an empty QuerySet.\"\"\"\n1063         clone = self._chain()\n1064         clone.query.set_empty()\n1065         return clone\n",
        "intended_behavior": "The method should ensure that when `none` is called, and the resulting QuerySet is used within an `Exists` expression, the `EmptyResultSet` is handled in such a way that it doesn't lead to the unintended removal of WHERE clause in the composed SQL query. This might involve marking the QuerySet in a specific way that `Exists` can detect and handle appropriately, ensuring that the intended logic of 'no results possible' is encoded into the SQL without dropping the WHERE clause entirely."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/models/query.py",
        "start": 176,
        "end": 1561,
        "class_name": "QuerySet",
        "method_name": null,
        "code": "176 class QuerySet:\n177     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n178 \n179     def __init__(self, model=None, query=None, using=None, hints=None):\n180         self.model = model\n181         self._db = using\n182         self._hints = hints or {}\n183         self._query = query or sql.Query(self.model)\n184         self._result_cache = None\n185         self._sticky_filter = False\n186         self._for_write = False\n187         self._prefetch_related_lookups = ()\n188         self._prefetch_done = False\n189         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n190         self._iterable_class = ModelIterable\n191         self._fields = None\n192         self._defer_next_filter = False\n193         self._deferred_filter = None\n194 \n195     @property\n196     def query(self):\n197         if self._deferred_filter:\n198             negate, args, kwargs = self._deferred_filter\n199             self._filter_or_exclude_inplace(negate, args, kwargs)\n200             self._deferred_filter = None\n201         return self._query\n202 \n203     @query.setter\n204     def query(self, value):\n205         if value.values_select:\n206             self._iterable_class = ValuesIterable\n207         self._query = value\n208 \n209     def as_manager(cls):\n210         # Address the circular dependency between `Queryset` and `Manager`.\n211         from django.db.models.manager import Manager\n212         manager = Manager.from_queryset(cls)()\n213         manager._built_with_as_manager = True\n214         return manager\n215     as_manager.queryset_only = True\n216     as_manager = classmethod(as_manager)\n217 \n218     ########################\n219     # PYTHON MAGIC METHODS #\n220     ########################\n221 \n222     def __deepcopy__(self, memo):\n223         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n224         obj = self.__class__()\n225         for k, v in self.__dict__.items():\n226             if k == '_result_cache':\n227                 obj.__dict__[k] = None\n228             else:\n229                 obj.__dict__[k] = copy.deepcopy(v, memo)\n230         return obj\n231 \n232     def __getstate__(self):\n233         # Force the cache to be fully populated.\n234         self._fetch_all()\n235         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}\n236 \n237     def __setstate__(self, state):\n238         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n239         if pickled_version:\n240             if pickled_version != django.__version__:\n241                 warnings.warn(\n242                     \"Pickled queryset instance's Django version %s does not \"\n243                     \"match the current version %s.\"\n244                     % (pickled_version, django.__version__),\n245                     RuntimeWarning,\n246                     stacklevel=2,\n247                 )\n248         else:\n249             warnings.warn(\n250                 \"Pickled queryset instance's Django version is not specified.\",\n251                 RuntimeWarning,\n252                 stacklevel=2,\n253             )\n254         self.__dict__.update(state)\n255 \n256     def __repr__(self):\n257         data = list(self[:REPR_OUTPUT_SIZE + 1])\n258         if len(data) > REPR_OUTPUT_SIZE:\n259             data[-1] = \"...(remaining elements truncated)...\"\n260         return '<%s %r>' % (self.__class__.__name__, data)\n261 \n262     def __len__(self):\n263         self._fetch_all()\n264         return len(self._result_cache)\n265 \n266     def __iter__(self):\n267         \"\"\"\n268         The queryset iterator protocol uses three nested iterators in the\n269         default case:\n270             1. sql.compiler.execute_sql()\n271                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n272                  using cursor.fetchmany(). This part is responsible for\n273                  doing some column masking, and returning the rows in chunks.\n274             2. sql.compiler.results_iter()\n275                - Returns one row at time. At this point the rows are still just\n276                  tuples. In some cases the return values are converted to\n277                  Python values at this location.\n278             3. self.iterator()\n279                - Responsible for turning the rows into model objects.\n280         \"\"\"\n281         self._fetch_all()\n282         return iter(self._result_cache)\n283 \n284     def __bool__(self):\n285         self._fetch_all()\n286         return bool(self._result_cache)\n287 \n288     def __getitem__(self, k):\n289         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n290         if not isinstance(k, (int, slice)):\n291             raise TypeError(\n292                 'QuerySet indices must be integers or slices, not %s.'\n293                 % type(k).__name__\n294             )\n295         if (\n296             (isinstance(k, int) and k < 0) or\n297             (isinstance(k, slice) and (\n298                 (k.start is not None and k.start < 0) or\n299                 (k.stop is not None and k.stop < 0)\n300             ))\n301         ):\n302             raise ValueError('Negative indexing is not supported.')\n303 \n304         if self._result_cache is not None:\n305             return self._result_cache[k]\n306 \n307         if isinstance(k, slice):\n308             qs = self._chain()\n309             if k.start is not None:\n310                 start = int(k.start)\n311             else:\n312                 start = None\n313             if k.stop is not None:\n314                 stop = int(k.stop)\n315             else:\n316                 stop = None\n317             qs.query.set_limits(start, stop)\n318             return list(qs)[::k.step] if k.step else qs\n319 \n320         qs = self._chain()\n321         qs.query.set_limits(k, k + 1)\n322         qs._fetch_all()\n323         return qs._result_cache[0]\n324 \n325     def __class_getitem__(cls, *args, **kwargs):\n326         return cls\n327 \n328     def __and__(self, other):\n329         self._check_operator_queryset(other, '&')\n330         self._merge_sanity_check(other)\n331         if isinstance(other, EmptyQuerySet):\n332             return other\n333         if isinstance(self, EmptyQuerySet):\n334             return self\n335         combined = self._chain()\n336         combined._merge_known_related_objects(other)\n337         combined.query.combine(other.query, sql.AND)\n338         return combined\n339 \n340     def __or__(self, other):\n341         self._check_operator_queryset(other, '|')\n342         self._merge_sanity_check(other)\n343         if isinstance(self, EmptyQuerySet):\n344             return other\n345         if isinstance(other, EmptyQuerySet):\n346             return self\n347         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n348         combined = query._chain()\n349         combined._merge_known_related_objects(other)\n350         if not other.query.can_filter():\n351             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n352         combined.query.combine(other.query, sql.OR)\n353         return combined\n354 \n355     ####################################\n356     # METHODS THAT DO DATABASE QUERIES #\n357     ####################################\n358 \n359     def _iterator(self, use_chunked_fetch, chunk_size):\n360         iterable = self._iterable_class(\n361             self,\n362             chunked_fetch=use_chunked_fetch,\n363             chunk_size=chunk_size or 2000,\n364         )\n365         if not self._prefetch_related_lookups or chunk_size is None:\n366             yield from iterable\n367             return\n368 \n369         iterator = iter(iterable)\n370         while results := list(islice(iterator, chunk_size)):\n371             prefetch_related_objects(results, *self._prefetch_related_lookups)\n372             yield from results\n373 \n374     def iterator(self, chunk_size=None):\n375         \"\"\"\n376         An iterator over the results from applying this QuerySet to the\n377         database. chunk_size must be provided for QuerySets that prefetch\n378         related objects. Otherwise, a default chunk_size of 2000 is supplied.\n379         \"\"\"\n380         if chunk_size is None:\n381             if self._prefetch_related_lookups:\n382                 # When the deprecation ends, replace with:\n383                 # raise ValueError(\n384                 #     'chunk_size must be provided when using '\n385                 #     'QuerySet.iterator() after prefetch_related().'\n386                 # )\n387                 warnings.warn(\n388                     'Using QuerySet.iterator() after prefetch_related() '\n389                     'without specifying chunk_size is deprecated.',\n390                     category=RemovedInDjango50Warning,\n391                     stacklevel=2,\n392                 )\n393         elif chunk_size <= 0:\n394             raise ValueError('Chunk size must be strictly positive.')\n395         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n396         return self._iterator(use_chunked_fetch, chunk_size)\n397 \n398     def aggregate(self, *args, **kwargs):\n399         \"\"\"\n400         Return a dictionary containing the calculations (aggregation)\n401         over the current queryset.\n402 \n403         If args is present the expression is passed as a kwarg using\n404         the Aggregate object's default alias.\n405         \"\"\"\n406         if self.query.distinct_fields:\n407             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n408         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n409         for arg in args:\n410             # The default_alias property raises TypeError if default_alias\n411             # can't be set automatically or AttributeError if it isn't an\n412             # attribute.\n413             try:\n414                 arg.default_alias\n415             except (AttributeError, TypeError):\n416                 raise TypeError(\"Complex aggregates require an alias\")\n417             kwargs[arg.default_alias] = arg\n418 \n419         query = self.query.chain()\n420         for (alias, aggregate_expr) in kwargs.items():\n421             query.add_annotation(aggregate_expr, alias, is_summary=True)\n422             annotation = query.annotations[alias]\n423             if not annotation.contains_aggregate:\n424                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n425             for expr in annotation.get_source_expressions():\n426                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:\n427                     name = expr.refs\n428                     raise exceptions.FieldError(\n429                         \"Cannot compute %s('%s'): '%s' is an aggregate\"\n430                         % (annotation.name, name, name)\n431                     )\n432         return query.get_aggregation(self.db, kwargs)\n433 \n434     def count(self):\n435         \"\"\"\n436         Perform a SELECT COUNT() and return the number of records as an\n437         integer.\n438 \n439         If the QuerySet is already fully cached, return the length of the\n440         cached results set to avoid multiple SELECT COUNT(*) calls.\n441         \"\"\"\n442         if self._result_cache is not None:\n443             return len(self._result_cache)\n444 \n445         return self.query.get_count(using=self.db)\n446 \n447     def get(self, *args, **kwargs):\n448         \"\"\"\n449         Perform the query and return a single object matching the given\n450         keyword arguments.\n451         \"\"\"\n452         if self.query.combinator and (args or kwargs):\n453             raise NotSupportedError(\n454                 'Calling QuerySet.get(...) with filters after %s() is not '\n455                 'supported.' % self.query.combinator\n456             )\n457         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)\n458         if self.query.can_filter() and not self.query.distinct_fields:\n459             clone = clone.order_by()\n460         limit = None\n461         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:\n462             limit = MAX_GET_RESULTS\n463             clone.query.set_limits(high=limit)\n464         num = len(clone)\n465         if num == 1:\n466             return clone._result_cache[0]\n467         if not num:\n468             raise self.model.DoesNotExist(\n469                 \"%s matching query does not exist.\" %\n470                 self.model._meta.object_name\n471             )\n472         raise self.model.MultipleObjectsReturned(\n473             'get() returned more than one %s -- it returned %s!' % (\n474                 self.model._meta.object_name,\n475                 num if not limit or num < limit else 'more than %s' % (limit - 1),\n476             )\n477         )\n478 \n479     def create(self, **kwargs):\n480         \"\"\"\n481         Create a new object with the given kwargs, saving it to the database\n482         and returning the created object.\n483         \"\"\"\n484         obj = self.model(**kwargs)\n485         self._for_write = True\n486         obj.save(force_insert=True, using=self.db)\n487         return obj\n488 \n489     def _prepare_for_bulk_create(self, objs):\n490         for obj in objs:\n491             if obj.pk is None:\n492                 # Populate new PK values.\n493                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n494             obj._prepare_related_fields_for_save(operation_name='bulk_create')\n495 \n496     def _check_bulk_create_options(self, ignore_conflicts, update_conflicts, update_fields, unique_fields):\n497         if ignore_conflicts and update_conflicts:\n498             raise ValueError(\n499                 'ignore_conflicts and update_conflicts are mutually exclusive.'\n500             )\n501         db_features = connections[self.db].features\n502         if ignore_conflicts:\n503             if not db_features.supports_ignore_conflicts:\n504                 raise NotSupportedError(\n505                     'This database backend does not support ignoring conflicts.'\n506                 )\n507             return OnConflict.IGNORE\n508         elif update_conflicts:\n509             if not db_features.supports_update_conflicts:\n510                 raise NotSupportedError(\n511                     'This database backend does not support updating conflicts.'\n512                 )\n513             if not update_fields:\n514                 raise ValueError(\n515                     'Fields that will be updated when a row insertion fails '\n516                     'on conflicts must be provided.'\n517                 )\n518             if unique_fields and not db_features.supports_update_conflicts_with_target:\n519                 raise NotSupportedError(\n520                     'This database backend does not support updating '\n521                     'conflicts with specifying unique fields that can trigger '\n522                     'the upsert.'\n523                 )\n524             if not unique_fields and db_features.supports_update_conflicts_with_target:\n525                 raise ValueError(\n526                     'Unique fields that can trigger the upsert must be '\n527                     'provided.'\n528                 )\n529             # Updating primary keys and non-concrete fields is forbidden.\n530             update_fields = [self.model._meta.get_field(name) for name in update_fields]\n531             if any(not f.concrete or f.many_to_many for f in update_fields):\n532                 raise ValueError(\n533                     'bulk_create() can only be used with concrete fields in '\n534                     'update_fields.'\n535                 )\n536             if any(f.primary_key for f in update_fields):\n537                 raise ValueError(\n538                     'bulk_create() cannot be used with primary keys in '\n539                     'update_fields.'\n540                 )\n541             if unique_fields:\n542                 # Primary key is allowed in unique_fields.\n543                 unique_fields = [\n544                     self.model._meta.get_field(name)\n545                     for name in unique_fields if name != 'pk'\n546                 ]\n547                 if any(not f.concrete or f.many_to_many for f in unique_fields):\n548                     raise ValueError(\n549                         'bulk_create() can only be used with concrete fields '\n550                         'in unique_fields.'\n551                     )\n552             return OnConflict.UPDATE\n553         return None\n554 \n555     def bulk_create(\n556         self, objs, batch_size=None, ignore_conflicts=False,\n557         update_conflicts=False, update_fields=None, unique_fields=None,\n558     ):\n559         \"\"\"\n560         Insert each of the instances into the database. Do *not* call\n561         save() on each of the instances, do not send any pre/post_save\n562         signals, and do not set the primary key attribute if it is an\n563         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n564         Multi-table models are not supported.\n565         \"\"\"\n566         # When you bulk insert you don't get the primary keys back (if it's an\n567         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n568         # you can't insert into the child tables which references this. There\n569         # are two workarounds:\n570         # 1) This could be implemented if you didn't have an autoincrement pk\n571         # 2) You could do it by doing O(n) normal inserts into the parent\n572         #    tables to get the primary keys back and then doing a single bulk\n573         #    insert into the childmost table.\n574         # We currently set the primary keys on the objects when using\n575         # PostgreSQL via the RETURNING ID clause. It should be possible for\n576         # Oracle as well, but the semantics for extracting the primary keys is\n577         # trickier so it's not done yet.\n578         if batch_size is not None and batch_size <= 0:\n579             raise ValueError('Batch size must be a positive integer.')\n580         # Check that the parents share the same concrete model with the our\n581         # model to detect the inheritance pattern ConcreteGrandParent ->\n582         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n583         # would not identify that case as involving multiple tables.\n584         for parent in self.model._meta.get_parent_list():\n585             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n586                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n587         if not objs:\n588             return objs\n589         on_conflict = self._check_bulk_create_options(\n590             ignore_conflicts,\n591             update_conflicts,\n592             update_fields,\n593             unique_fields,\n594         )\n595         self._for_write = True\n596         opts = self.model._meta\n597         fields = opts.concrete_fields\n598         objs = list(objs)\n599         self._prepare_for_bulk_create(objs)\n600         with transaction.atomic(using=self.db, savepoint=False):\n601             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n602             if objs_with_pk:\n603                 returned_columns = self._batched_insert(\n604                     objs_with_pk,\n605                     fields,\n606                     batch_size,\n607                     on_conflict=on_conflict,\n608                     update_fields=update_fields,\n609                     unique_fields=unique_fields,\n610                 )\n611                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):\n612                     for result, field in zip(results, opts.db_returning_fields):\n613                         if field != opts.pk:\n614                             setattr(obj_with_pk, field.attname, result)\n615                 for obj_with_pk in objs_with_pk:\n616                     obj_with_pk._state.adding = False\n617                     obj_with_pk._state.db = self.db\n618             if objs_without_pk:\n619                 fields = [f for f in fields if not isinstance(f, AutoField)]\n620                 returned_columns = self._batched_insert(\n621                     objs_without_pk,\n622                     fields,\n623                     batch_size,\n624                     on_conflict=on_conflict,\n625                     update_fields=update_fields,\n626                     unique_fields=unique_fields,\n627                 )\n628                 connection = connections[self.db]\n629                 if connection.features.can_return_rows_from_bulk_insert and on_conflict is None:\n630                     assert len(returned_columns) == len(objs_without_pk)\n631                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):\n632                     for result, field in zip(results, opts.db_returning_fields):\n633                         setattr(obj_without_pk, field.attname, result)\n634                     obj_without_pk._state.adding = False\n635                     obj_without_pk._state.db = self.db\n636 \n637         return objs\n638 \n639     def bulk_update(self, objs, fields, batch_size=None):\n640         \"\"\"\n641         Update the given fields in each of the given objects in the database.\n642         \"\"\"\n643         if batch_size is not None and batch_size < 0:\n644             raise ValueError('Batch size must be a positive integer.')\n645         if not fields:\n646             raise ValueError('Field names must be given to bulk_update().')\n647         objs = tuple(objs)\n648         if any(obj.pk is None for obj in objs):\n649             raise ValueError('All bulk_update() objects must have a primary key set.')\n650         fields = [self.model._meta.get_field(name) for name in fields]\n651         if any(not f.concrete or f.many_to_many for f in fields):\n652             raise ValueError('bulk_update() can only be used with concrete fields.')\n653         if any(f.primary_key for f in fields):\n654             raise ValueError('bulk_update() cannot be used with primary key fields.')\n655         if not objs:\n656             return 0\n657         for obj in objs:\n658             obj._prepare_related_fields_for_save(operation_name='bulk_update', fields=fields)\n659         # PK is used twice in the resulting update query, once in the filter\n660         # and once in the WHEN. Each field will also have one CAST.\n661         connection = connections[self.db]\n662         max_batch_size = connection.ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n663         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n664         requires_casting = connection.features.requires_casted_case_in_updates\n665         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n666         updates = []\n667         for batch_objs in batches:\n668             update_kwargs = {}\n669             for field in fields:\n670                 when_statements = []\n671                 for obj in batch_objs:\n672                     attr = getattr(obj, field.attname)\n673                     if not hasattr(attr, 'resolve_expression'):\n674                         attr = Value(attr, output_field=field)\n675                     when_statements.append(When(pk=obj.pk, then=attr))\n676                 case_statement = Case(*when_statements, output_field=field)\n677                 if requires_casting:\n678                     case_statement = Cast(case_statement, output_field=field)\n679                 update_kwargs[field.attname] = case_statement\n680             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n681         rows_updated = 0\n682         with transaction.atomic(using=self.db, savepoint=False):\n683             for pks, update_kwargs in updates:\n684                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)\n685         return rows_updated\n686     bulk_update.alters_data = True\n687 \n688     def get_or_create(self, defaults=None, **kwargs):\n689         \"\"\"\n690         Look up an object with the given kwargs, creating one if necessary.\n691         Return a tuple of (object, created), where created is a boolean\n692         specifying whether an object was created.\n693         \"\"\"\n694         # The get() needs to be targeted at the write database in order\n695         # to avoid potential transaction consistency problems.\n696         self._for_write = True\n697         try:\n698             return self.get(**kwargs), False\n699         except self.model.DoesNotExist:\n700             params = self._extract_model_params(defaults, **kwargs)\n701             # Try to create an object using passed params.\n702             try:\n703                 with transaction.atomic(using=self.db):\n704                     params = dict(resolve_callables(params))\n705                     return self.create(**params), True\n706             except IntegrityError:\n707                 try:\n708                     return self.get(**kwargs), False\n709                 except self.model.DoesNotExist:\n710                     pass\n711                 raise\n712 \n713     def update_or_create(self, defaults=None, **kwargs):\n714         \"\"\"\n715         Look up an object with the given kwargs, updating one with defaults\n716         if it exists, otherwise create a new one.\n717         Return a tuple (object, created), where created is a boolean\n718         specifying whether an object was created.\n719         \"\"\"\n720         defaults = defaults or {}\n721         self._for_write = True\n722         with transaction.atomic(using=self.db):\n723             # Lock the row so that a concurrent update is blocked until\n724             # update_or_create() has performed its save.\n725             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)\n726             if created:\n727                 return obj, created\n728             for k, v in resolve_callables(defaults):\n729                 setattr(obj, k, v)\n730             obj.save(using=self.db)\n731         return obj, False\n732 \n733     def _extract_model_params(self, defaults, **kwargs):\n734         \"\"\"\n735         Prepare `params` for creating a model instance based on the given\n736         kwargs; for use by get_or_create().\n737         \"\"\"\n738         defaults = defaults or {}\n739         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n740         params.update(defaults)\n741         property_names = self.model._meta._property_names\n742         invalid_params = []\n743         for param in params:\n744             try:\n745                 self.model._meta.get_field(param)\n746             except exceptions.FieldDoesNotExist:\n747                 # It's okay to use a model's property if it has a setter.\n748                 if not (param in property_names and getattr(self.model, param).fset):\n749                     invalid_params.append(param)\n750         if invalid_params:\n751             raise exceptions.FieldError(\n752                 \"Invalid field name(s) for model %s: '%s'.\" % (\n753                     self.model._meta.object_name,\n754                     \"', '\".join(sorted(invalid_params)),\n755                 ))\n756         return params\n757 \n758     def _earliest(self, *fields):\n759         \"\"\"\n760         Return the earliest object according to fields (if given) or by the\n761         model's Meta.get_latest_by.\n762         \"\"\"\n763         if fields:\n764             order_by = fields\n765         else:\n766             order_by = getattr(self.model._meta, 'get_latest_by')\n767             if order_by and not isinstance(order_by, (tuple, list)):\n768                 order_by = (order_by,)\n769         if order_by is None:\n770             raise ValueError(\n771                 \"earliest() and latest() require either fields as positional \"\n772                 \"arguments or 'get_latest_by' in the model's Meta.\"\n773             )\n774         obj = self._chain()\n775         obj.query.set_limits(high=1)\n776         obj.query.clear_ordering(force=True)\n777         obj.query.add_ordering(*order_by)\n778         return obj.get()\n779 \n780     def earliest(self, *fields):\n781         if self.query.is_sliced:\n782             raise TypeError('Cannot change a query once a slice has been taken.')\n783         return self._earliest(*fields)\n784 \n785     def latest(self, *fields):\n786         if self.query.is_sliced:\n787             raise TypeError('Cannot change a query once a slice has been taken.')\n788         return self.reverse()._earliest(*fields)\n789 \n790     def first(self):\n791         \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n792         for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n793             return obj\n794 \n795     def last(self):\n796         \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n797         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n798             return obj\n799 \n800     def in_bulk(self, id_list=None, *, field_name='pk'):\n801         \"\"\"\n802         Return a dictionary mapping each of the given IDs to the object with\n803         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n804         \"\"\"\n805         if self.query.is_sliced:\n806             raise TypeError(\"Cannot use 'limit' or 'offset' with in_bulk().\")\n807         opts = self.model._meta\n808         unique_fields = [\n809             constraint.fields[0]\n810             for constraint in opts.total_unique_constraints\n811             if len(constraint.fields) == 1\n812         ]\n813         if (\n814             field_name != 'pk' and\n815             not opts.get_field(field_name).unique and\n816             field_name not in unique_fields and\n817             self.query.distinct_fields != (field_name,)\n818         ):\n819             raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n820         if id_list is not None:\n821             if not id_list:\n822                 return {}\n823             filter_key = '{}__in'.format(field_name)\n824             batch_size = connections[self.db].features.max_query_params\n825             id_list = tuple(id_list)\n826             # If the database has a limit on the number of query parameters\n827             # (e.g. SQLite), retrieve objects in batches if necessary.\n828             if batch_size and batch_size < len(id_list):\n829                 qs = ()\n830                 for offset in range(0, len(id_list), batch_size):\n831                     batch = id_list[offset:offset + batch_size]\n832                     qs += tuple(self.filter(**{filter_key: batch}).order_by())\n833             else:\n834                 qs = self.filter(**{filter_key: id_list}).order_by()\n835         else:\n836             qs = self._chain()\n837         return {getattr(obj, field_name): obj for obj in qs}\n838 \n839     def delete(self):\n840         \"\"\"Delete the records in the current QuerySet.\"\"\"\n841         self._not_support_combined_queries('delete')\n842         if self.query.is_sliced:\n843             raise TypeError(\"Cannot use 'limit' or 'offset' with delete().\")\n844         if self.query.distinct or self.query.distinct_fields:\n845             raise TypeError('Cannot call delete() after .distinct().')\n846         if self._fields is not None:\n847             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n848 \n849         del_query = self._chain()\n850 \n851         # The delete is actually 2 queries - one to find related objects,\n852         # and one to delete. Make sure that the discovery of related\n853         # objects is performed on the same database as the deletion.\n854         del_query._for_write = True\n855 \n856         # Disable non-supported fields.\n857         del_query.query.select_for_update = False\n858         del_query.query.select_related = False\n859         del_query.query.clear_ordering(force=True)\n860 \n861         collector = Collector(using=del_query.db, origin=self)\n862         collector.collect(del_query)\n863         deleted, _rows_count = collector.delete()\n864 \n865         # Clear the result cache, in case this QuerySet gets reused.\n866         self._result_cache = None\n867         return deleted, _rows_count\n868 \n869     delete.alters_data = True\n870     delete.queryset_only = True\n871 \n872     def _raw_delete(self, using):\n873         \"\"\"\n874         Delete objects found from the given queryset in single direct SQL\n875         query. No signals are sent and there is no protection for cascades.\n876         \"\"\"\n877         query = self.query.clone()\n878         query.__class__ = sql.DeleteQuery\n879         cursor = query.get_compiler(using).execute_sql(CURSOR)\n880         if cursor:\n881             with cursor:\n882                 return cursor.rowcount\n883         return 0\n884     _raw_delete.alters_data = True\n885 \n886     def update(self, **kwargs):\n887         \"\"\"\n888         Update all elements in the current QuerySet, setting all the given\n889         fields to the appropriate values.\n890         \"\"\"\n891         self._not_support_combined_queries('update')\n892         if self.query.is_sliced:\n893             raise TypeError('Cannot update a query once a slice has been taken.')\n894         self._for_write = True\n895         query = self.query.chain(sql.UpdateQuery)\n896         query.add_update_values(kwargs)\n897         # Clear any annotations so that they won't be present in subqueries.\n898         query.annotations = {}\n899         with transaction.mark_for_rollback_on_error(using=self.db):\n900             rows = query.get_compiler(self.db).execute_sql(CURSOR)\n901         self._result_cache = None\n902         return rows\n903     update.alters_data = True\n904 \n905     def _update(self, values):\n906         \"\"\"\n907         A version of update() that accepts field objects instead of field names.\n908         Used primarily for model saving and not intended for use by general\n909         code (it requires too much poking around at model internals to be\n910         useful at that level).\n911         \"\"\"\n912         if self.query.is_sliced:\n913             raise TypeError('Cannot update a query once a slice has been taken.')\n914         query = self.query.chain(sql.UpdateQuery)\n915         query.add_update_fields(values)\n916         # Clear any annotations so that they won't be present in subqueries.\n917         query.annotations = {}\n918         self._result_cache = None\n919         return query.get_compiler(self.db).execute_sql(CURSOR)\n920     _update.alters_data = True\n921     _update.queryset_only = False\n922 \n923     def exists(self):\n924         if self._result_cache is None:\n925             return self.query.has_results(using=self.db)\n926         return bool(self._result_cache)\n927 \n928     def contains(self, obj):\n929         \"\"\"Return True if the queryset contains an object.\"\"\"\n930         self._not_support_combined_queries('contains')\n931         if self._fields is not None:\n932             raise TypeError(\n933                 'Cannot call QuerySet.contains() after .values() or '\n934                 '.values_list().'\n935             )\n936         try:\n937             if obj._meta.concrete_model != self.model._meta.concrete_model:\n938                 return False\n939         except AttributeError:\n940             raise TypeError(\"'obj' must be a model instance.\")\n941         if obj.pk is None:\n942             raise ValueError(\n943                 'QuerySet.contains() cannot be used on unsaved objects.'\n944             )\n945         if self._result_cache is not None:\n946             return obj in self._result_cache\n947         return self.filter(pk=obj.pk).exists()\n948 \n949     def _prefetch_related_objects(self):\n950         # This method can only be called once the result cache has been filled.\n951         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n952         self._prefetch_done = True\n953 \n954     def explain(self, *, format=None, **options):\n955         return self.query.explain(using=self.db, format=format, **options)\n956 \n957     ##################################################\n958     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n959     ##################################################\n960 \n961     def raw(self, raw_query, params=(), translations=None, using=None):\n962         if using is None:\n963             using = self.db\n964         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)\n965         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n966         return qs\n967 \n968     def _values(self, *fields, **expressions):\n969         clone = self._chain()\n970         if expressions:\n971             clone = clone.annotate(**expressions)\n972         clone._fields = fields\n973         clone.query.set_values(fields)\n974         return clone\n975 \n976     def values(self, *fields, **expressions):\n977         fields += tuple(expressions)\n978         clone = self._values(*fields, **expressions)\n979         clone._iterable_class = ValuesIterable\n980         return clone\n981 \n982     def values_list(self, *fields, flat=False, named=False):\n983         if flat and named:\n984             raise TypeError(\"'flat' and 'named' can't be used together.\")\n985         if flat and len(fields) > 1:\n986             raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n987 \n988         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}\n989         _fields = []\n990         expressions = {}\n991         counter = 1\n992         for field in fields:\n993             if hasattr(field, 'resolve_expression'):\n994                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())\n995                 while True:\n996                     field_id = field_id_prefix + str(counter)\n997                     counter += 1\n998                     if field_id not in field_names:\n999                         break\n1000                 expressions[field_id] = field\n1001                 _fields.append(field_id)\n1002             else:\n1003                 _fields.append(field)\n1004 \n1005         clone = self._values(*_fields, **expressions)\n1006         clone._iterable_class = (\n1007             NamedValuesListIterable if named\n1008             else FlatValuesListIterable if flat\n1009             else ValuesListIterable\n1010         )\n1011         return clone\n1012 \n1013     def dates(self, field_name, kind, order='ASC'):\n1014         \"\"\"\n1015         Return a list of date objects representing all available dates for\n1016         the given field_name, scoped to 'kind'.\n1017         \"\"\"\n1018         if kind not in ('year', 'month', 'week', 'day'):\n1019             raise ValueError(\"'kind' must be one of 'year', 'month', 'week', or 'day'.\")\n1020         if order not in ('ASC', 'DESC'):\n1021             raise ValueError(\"'order' must be either 'ASC' or 'DESC'.\")\n1022         return self.annotate(\n1023             datefield=Trunc(field_name, kind, output_field=DateField()),\n1024             plain_field=F(field_name)\n1025         ).values_list(\n1026             'datefield', flat=True\n1027         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')\n1028 \n1029     # RemovedInDjango50Warning: when the deprecation ends, remove is_dst\n1030     # argument.\n1031     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=timezone.NOT_PASSED):\n1032         \"\"\"\n1033         Return a list of datetime objects representing all available\n1034         datetimes for the given field_name, scoped to 'kind'.\n1035         \"\"\"\n1036         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):\n1037             raise ValueError(\n1038                 \"'kind' must be one of 'year', 'month', 'week', 'day', \"\n1039                 \"'hour', 'minute', or 'second'.\"\n1040             )\n1041         if order not in ('ASC', 'DESC'):\n1042             raise ValueError(\"'order' must be either 'ASC' or 'DESC'.\")\n1043         if settings.USE_TZ:\n1044             if tzinfo is None:\n1045                 tzinfo = timezone.get_current_timezone()\n1046         else:\n1047             tzinfo = None\n1048         return self.annotate(\n1049             datetimefield=Trunc(\n1050                 field_name,\n1051                 kind,\n1052                 output_field=DateTimeField(),\n1053                 tzinfo=tzinfo,\n1054                 is_dst=is_dst,\n1055             ),\n1056             plain_field=F(field_name)\n1057         ).values_list(\n1058             'datetimefield', flat=True\n1059         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')\n1060 \n1061     def none(self):\n1062         \"\"\"Return an empty QuerySet.\"\"\"\n1063         clone = self._chain()\n1064         clone.query.set_empty()\n1065         return clone\n1066 \n1067     ##################################################################\n1068     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n1069     ##################################################################\n1070 \n1071     def all(self):\n1072         \"\"\"\n1073         Return a new QuerySet that is a copy of the current one. This allows a\n1074         QuerySet to proxy for a model manager in some cases.\n1075         \"\"\"\n1076         return self._chain()\n1077 \n1078     def filter(self, *args, **kwargs):\n1079         \"\"\"\n1080         Return a new QuerySet instance with the args ANDed to the existing\n1081         set.\n1082         \"\"\"\n1083         self._not_support_combined_queries('filter')\n1084         return self._filter_or_exclude(False, args, kwargs)\n1085 \n1086     def exclude(self, *args, **kwargs):\n1087         \"\"\"\n1088         Return a new QuerySet instance with NOT (args) ANDed to the existing\n1089         set.\n1090         \"\"\"\n1091         self._not_support_combined_queries('exclude')\n1092         return self._filter_or_exclude(True, args, kwargs)\n1093 \n1094     def _filter_or_exclude(self, negate, args, kwargs):\n1095         if (args or kwargs) and self.query.is_sliced:\n1096             raise TypeError('Cannot filter a query once a slice has been taken.')\n1097         clone = self._chain()\n1098         if self._defer_next_filter:\n1099             self._defer_next_filter = False\n1100             clone._deferred_filter = negate, args, kwargs\n1101         else:\n1102             clone._filter_or_exclude_inplace(negate, args, kwargs)\n1103         return clone\n1104 \n1105     def _filter_or_exclude_inplace(self, negate, args, kwargs):\n1106         if negate:\n1107             self._query.add_q(~Q(*args, **kwargs))\n1108         else:\n1109             self._query.add_q(Q(*args, **kwargs))\n1110 \n1111     def complex_filter(self, filter_obj):\n1112         \"\"\"\n1113         Return a new QuerySet instance with filter_obj added to the filters.\n1114 \n1115         filter_obj can be a Q object or a dictionary of keyword lookup\n1116         arguments.\n1117 \n1118         This exists to support framework features such as 'limit_choices_to',\n1119         and usually it will be more natural to use other methods.\n1120         \"\"\"\n1121         if isinstance(filter_obj, Q):\n1122             clone = self._chain()\n1123             clone.query.add_q(filter_obj)\n1124             return clone\n1125         else:\n1126             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)\n1127 \n1128     def _combinator_query(self, combinator, *other_qs, all=False):\n1129         # Clone the query to inherit the select list and everything\n1130         clone = self._chain()\n1131         # Clear limits and ordering so they can be reapplied\n1132         clone.query.clear_ordering(force=True)\n1133         clone.query.clear_limits()\n1134         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)\n1135         clone.query.combinator = combinator\n1136         clone.query.combinator_all = all\n1137         return clone\n1138 \n1139     def union(self, *other_qs, all=False):\n1140         # If the query is an EmptyQuerySet, combine all nonempty querysets.\n1141         if isinstance(self, EmptyQuerySet):\n1142             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n1143             if not qs:\n1144                 return self\n1145             if len(qs) == 1:\n1146                 return qs[0]\n1147             return qs[0]._combinator_query('union', *qs[1:], all=all)\n1148         return self._combinator_query('union', *other_qs, all=all)\n1149 \n1150     def intersection(self, *other_qs):\n1151         # If any query is an EmptyQuerySet, return it.\n1152         if isinstance(self, EmptyQuerySet):\n1153             return self\n1154         for other in other_qs:\n1155             if isinstance(other, EmptyQuerySet):\n1156                 return other\n1157         return self._combinator_query('intersection', *other_qs)\n1158 \n1159     def difference(self, *other_qs):\n1160         # If the query is an EmptyQuerySet, return it.\n1161         if isinstance(self, EmptyQuerySet):\n1162             return self\n1163         return self._combinator_query('difference', *other_qs)\n1164 \n1165     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):\n1166         \"\"\"\n1167         Return a new QuerySet instance that will select objects with a\n1168         FOR UPDATE lock.\n1169         \"\"\"\n1170         if nowait and skip_locked:\n1171             raise ValueError('The nowait option cannot be used with skip_locked.')\n1172         obj = self._chain()\n1173         obj._for_write = True\n1174         obj.query.select_for_update = True\n1175         obj.query.select_for_update_nowait = nowait\n1176         obj.query.select_for_update_skip_locked = skip_locked\n1177         obj.query.select_for_update_of = of\n1178         obj.query.select_for_no_key_update = no_key\n1179         return obj\n1180 \n1181     def select_related(self, *fields):\n1182         \"\"\"\n1183         Return a new QuerySet instance that will select related objects.\n1184 \n1185         If fields are specified, they must be ForeignKey fields and only those\n1186         related objects are included in the selection.\n1187 \n1188         If select_related(None) is called, clear the list.\n1189         \"\"\"\n1190         self._not_support_combined_queries('select_related')\n1191         if self._fields is not None:\n1192             raise TypeError(\"Cannot call select_related() after .values() or .values_list()\")\n1193 \n1194         obj = self._chain()\n1195         if fields == (None,):\n1196             obj.query.select_related = False\n1197         elif fields:\n1198             obj.query.add_select_related(fields)\n1199         else:\n1200             obj.query.select_related = True\n1201         return obj\n1202 \n1203     def prefetch_related(self, *lookups):\n1204         \"\"\"\n1205         Return a new QuerySet instance that will prefetch the specified\n1206         Many-To-One and Many-To-Many related objects when the QuerySet is\n1207         evaluated.\n1208 \n1209         When prefetch_related() is called more than once, append to the list of\n1210         prefetch lookups. If prefetch_related(None) is called, clear the list.\n1211         \"\"\"\n1212         self._not_support_combined_queries('prefetch_related')\n1213         clone = self._chain()\n1214         if lookups == (None,):\n1215             clone._prefetch_related_lookups = ()\n1216         else:\n1217             for lookup in lookups:\n1218                 if isinstance(lookup, Prefetch):\n1219                     lookup = lookup.prefetch_to\n1220                 lookup = lookup.split(LOOKUP_SEP, 1)[0]\n1221                 if lookup in self.query._filtered_relations:\n1222                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')\n1223             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n1224         return clone\n1225 \n1226     def annotate(self, *args, **kwargs):\n1227         \"\"\"\n1228         Return a query set in which the returned objects have been annotated\n1229         with extra data or aggregations.\n1230         \"\"\"\n1231         self._not_support_combined_queries('annotate')\n1232         return self._annotate(args, kwargs, select=True)\n1233 \n1234     def alias(self, *args, **kwargs):\n1235         \"\"\"\n1236         Return a query set with added aliases for extra data or aggregations.\n1237         \"\"\"\n1238         self._not_support_combined_queries('alias')\n1239         return self._annotate(args, kwargs, select=False)\n1240 \n1241     def _annotate(self, args, kwargs, select=True):\n1242         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n1243         annotations = {}\n1244         for arg in args:\n1245             # The default_alias property may raise a TypeError.\n1246             try:\n1247                 if arg.default_alias in kwargs:\n1248                     raise ValueError(\"The named annotation '%s' conflicts with the \"\n1249                                      \"default name for another annotation.\"\n1250                                      % arg.default_alias)\n1251             except TypeError:\n1252                 raise TypeError(\"Complex annotations require an alias\")\n1253             annotations[arg.default_alias] = arg\n1254         annotations.update(kwargs)\n1255 \n1256         clone = self._chain()\n1257         names = self._fields\n1258         if names is None:\n1259             names = set(chain.from_iterable(\n1260                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n1261                 for field in self.model._meta.get_fields()\n1262             ))\n1263 \n1264         for alias, annotation in annotations.items():\n1265             if alias in names:\n1266                 raise ValueError(\"The annotation '%s' conflicts with a field on \"\n1267                                  \"the model.\" % alias)\n1268             if isinstance(annotation, FilteredRelation):\n1269                 clone.query.add_filtered_relation(annotation, alias)\n1270             else:\n1271                 clone.query.add_annotation(\n1272                     annotation, alias, is_summary=False, select=select,\n1273                 )\n1274         for alias, annotation in clone.query.annotations.items():\n1275             if alias in annotations and annotation.contains_aggregate:\n1276                 if clone._fields is None:\n1277                     clone.query.group_by = True\n1278                 else:\n1279                     clone.query.set_group_by()\n1280                 break\n1281 \n1282         return clone\n1283 \n1284     def order_by(self, *field_names):\n1285         \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n1286         if self.query.is_sliced:\n1287             raise TypeError('Cannot reorder a query once a slice has been taken.')\n1288         obj = self._chain()\n1289         obj.query.clear_ordering(force=True, clear_default=False)\n1290         obj.query.add_ordering(*field_names)\n1291         return obj\n1292 \n1293     def distinct(self, *field_names):\n1294         \"\"\"\n1295         Return a new QuerySet instance that will select only distinct results.\n1296         \"\"\"\n1297         self._not_support_combined_queries('distinct')\n1298         if self.query.is_sliced:\n1299             raise TypeError('Cannot create distinct fields once a slice has been taken.')\n1300         obj = self._chain()\n1301         obj.query.add_distinct_fields(*field_names)\n1302         return obj\n1303 \n1304     def extra(self, select=None, where=None, params=None, tables=None,\n1305               order_by=None, select_params=None):\n1306         \"\"\"Add extra SQL fragments to the query.\"\"\"\n1307         self._not_support_combined_queries('extra')\n1308         if self.query.is_sliced:\n1309             raise TypeError('Cannot change a query once a slice has been taken.')\n1310         clone = self._chain()\n1311         clone.query.add_extra(select, select_params, where, params, tables, order_by)\n1312         return clone\n1313 \n1314     def reverse(self):\n1315         \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n1316         if self.query.is_sliced:\n1317             raise TypeError('Cannot reverse a query once a slice has been taken.')\n1318         clone = self._chain()\n1319         clone.query.standard_ordering = not clone.query.standard_ordering\n1320         return clone\n1321 \n1322     def defer(self, *fields):\n1323         \"\"\"\n1324         Defer the loading of data for certain fields until they are accessed.\n1325         Add the set of deferred fields to any existing set of deferred fields.\n1326         The only exception to this is if None is passed in as the only\n1327         parameter, in which case removal all deferrals.\n1328         \"\"\"\n1329         self._not_support_combined_queries('defer')\n1330         if self._fields is not None:\n1331             raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n1332         clone = self._chain()\n1333         if fields == (None,):\n1334             clone.query.clear_deferred_loading()\n1335         else:\n1336             clone.query.add_deferred_loading(fields)\n1337         return clone\n1338 \n1339     def only(self, *fields):\n1340         \"\"\"\n1341         Essentially, the opposite of defer(). Only the fields passed into this\n1342         method and that are not already specified as deferred are loaded\n1343         immediately when the queryset is evaluated.\n1344         \"\"\"\n1345         self._not_support_combined_queries('only')\n1346         if self._fields is not None:\n1347             raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n1348         if fields == (None,):\n1349             # Can only pass None to defer(), not only(), as the rest option.\n1350             # That won't stop people trying to do this, so let's be explicit.\n1351             raise TypeError(\"Cannot pass None as an argument to only().\")\n1352         for field in fields:\n1353             field = field.split(LOOKUP_SEP, 1)[0]\n1354             if field in self.query._filtered_relations:\n1355                 raise ValueError('only() is not supported with FilteredRelation.')\n1356         clone = self._chain()\n1357         clone.query.add_immediate_loading(fields)\n1358         return clone\n1359 \n1360     def using(self, alias):\n1361         \"\"\"Select which database this QuerySet should execute against.\"\"\"\n1362         clone = self._chain()\n1363         clone._db = alias\n1364         return clone\n1365 \n1366     ###################################\n1367     # PUBLIC INTROSPECTION ATTRIBUTES #\n1368     ###################################\n1369 \n1370     @property\n1371     def ordered(self):\n1372         \"\"\"\n1373         Return True if the QuerySet is ordered -- i.e. has an order_by()\n1374         clause or a default ordering on the model (or is empty).\n1375         \"\"\"\n1376         if isinstance(self, EmptyQuerySet):\n1377             return True\n1378         if self.query.extra_order_by or self.query.order_by:\n1379             return True\n1380         elif (\n1381             self.query.default_ordering and\n1382             self.query.get_meta().ordering and\n1383             # A default ordering doesn't affect GROUP BY queries.\n1384             not self.query.group_by\n1385         ):\n1386             return True\n1387         else:\n1388             return False\n1389 \n1390     @property\n1391     def db(self):\n1392         \"\"\"Return the database used if this query is executed now.\"\"\"\n1393         if self._for_write:\n1394             return self._db or router.db_for_write(self.model, **self._hints)\n1395         return self._db or router.db_for_read(self.model, **self._hints)\n1396 \n1397     ###################\n1398     # PRIVATE METHODS #\n1399     ###################\n1400 \n1401     def _insert(\n1402         self, objs, fields, returning_fields=None, raw=False, using=None,\n1403         on_conflict=None, update_fields=None, unique_fields=None,\n1404     ):\n1405         \"\"\"\n1406         Insert a new record for the given model. This provides an interface to\n1407         the InsertQuery class and is how Model.save() is implemented.\n1408         \"\"\"\n1409         self._for_write = True\n1410         if using is None:\n1411             using = self.db\n1412         query = sql.InsertQuery(\n1413             self.model,\n1414             on_conflict=on_conflict,\n1415             update_fields=update_fields,\n1416             unique_fields=unique_fields,\n1417         )\n1418         query.insert_values(fields, objs, raw=raw)\n1419         return query.get_compiler(using=using).execute_sql(returning_fields)\n1420     _insert.alters_data = True\n1421     _insert.queryset_only = False\n1422 \n1423     def _batched_insert(\n1424         self, objs, fields, batch_size, on_conflict=None, update_fields=None,\n1425         unique_fields=None,\n1426     ):\n1427         \"\"\"\n1428         Helper method for bulk_create() to insert objs one batch at a time.\n1429         \"\"\"\n1430         connection = connections[self.db]\n1431         ops = connection.ops\n1432         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)\n1433         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n1434         inserted_rows = []\n1435         bulk_return = connection.features.can_return_rows_from_bulk_insert\n1436         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n1437             if bulk_return and on_conflict is None:\n1438                 inserted_rows.extend(self._insert(\n1439                     item, fields=fields, using=self.db,\n1440                     returning_fields=self.model._meta.db_returning_fields,\n1441                 ))\n1442             else:\n1443                 self._insert(\n1444                     item,\n1445                     fields=fields,\n1446                     using=self.db,\n1447                     on_conflict=on_conflict,\n1448                     update_fields=update_fields,\n1449                     unique_fields=unique_fields,\n1450                 )\n1451         return inserted_rows\n1452 \n1453     def _chain(self):\n1454         \"\"\"\n1455         Return a copy of the current QuerySet that's ready for another\n1456         operation.\n1457         \"\"\"\n1458         obj = self._clone()\n1459         if obj._sticky_filter:\n1460             obj.query.filter_is_sticky = True\n1461             obj._sticky_filter = False\n1462         return obj\n1463 \n1464     def _clone(self):\n1465         \"\"\"\n1466         Return a copy of the current QuerySet. A lightweight alternative\n1467         to deepcopy().\n1468         \"\"\"\n1469         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)\n1470         c._sticky_filter = self._sticky_filter\n1471         c._for_write = self._for_write\n1472         c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n1473         c._known_related_objects = self._known_related_objects\n1474         c._iterable_class = self._iterable_class\n1475         c._fields = self._fields\n1476         return c\n1477 \n1478     def _fetch_all(self):\n1479         if self._result_cache is None:\n1480             self._result_cache = list(self._iterable_class(self))\n1481         if self._prefetch_related_lookups and not self._prefetch_done:\n1482             self._prefetch_related_objects()\n1483 \n1484     def _next_is_sticky(self):\n1485         \"\"\"\n1486         Indicate that the next filter call and the one following that should\n1487         be treated as a single filter. This is only important when it comes to\n1488         determining when to reuse tables for many-to-many filters. Required so\n1489         that we can filter naturally on the results of related managers.\n1490 \n1491         This doesn't return a clone of the current QuerySet (it returns\n1492         \"self\"). The method is only used internally and should be immediately\n1493         followed by a filter() that does create a clone.\n1494         \"\"\"\n1495         self._sticky_filter = True\n1496         return self\n1497 \n1498     def _merge_sanity_check(self, other):\n1499         \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n1500         if self._fields is not None and (\n1501                 set(self.query.values_select) != set(other.query.values_select) or\n1502                 set(self.query.extra_select) != set(other.query.extra_select) or\n1503                 set(self.query.annotation_select) != set(other.query.annotation_select)):\n1504             raise TypeError(\n1505                 \"Merging '%s' classes must involve the same values in each case.\"\n1506                 % self.__class__.__name__\n1507             )\n1508 \n1509     def _merge_known_related_objects(self, other):\n1510         \"\"\"\n1511         Keep track of all known related objects from either QuerySet instance.\n1512         \"\"\"\n1513         for field, objects in other._known_related_objects.items():\n1514             self._known_related_objects.setdefault(field, {}).update(objects)\n1515 \n1516     def resolve_expression(self, *args, **kwargs):\n1517         if self._fields and len(self._fields) > 1:\n1518             # values() queryset can only be used as nested queries\n1519             # if they are set up to select only a single field.\n1520             raise TypeError('Cannot use multi-field values as a filter value.')\n1521         query = self.query.resolve_expression(*args, **kwargs)\n1522         query._db = self._db\n1523         return query\n1524     resolve_expression.queryset_only = True\n1525 \n1526     def _add_hints(self, **hints):\n1527         \"\"\"\n1528         Update hinting information for use by routers. Add new key/values or\n1529         overwrite existing key/values.\n1530         \"\"\"\n1531         self._hints.update(hints)\n1532 \n1533     def _has_filters(self):\n1534         \"\"\"\n1535         Check if this QuerySet has any filtering going on. This isn't\n1536         equivalent with checking if all objects are present in results, for\n1537         example, qs[1:]._has_filters() -> False.\n1538         \"\"\"\n1539         return self.query.has_filters()\n1540 \n1541     @staticmethod\n1542     def _validate_values_are_expressions(values, method_name):\n1543         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))\n1544         if invalid_args:\n1545             raise TypeError(\n1546                 'QuerySet.%s() received non-expression(s): %s.' % (\n1547                     method_name,\n1548                     ', '.join(invalid_args),\n1549                 )\n1550             )\n1551 \n1552     def _not_support_combined_queries(self, operation_name):\n1553         if self.query.combinator:\n1554             raise NotSupportedError(\n1555                 'Calling QuerySet.%s() after %s() is not supported.'\n1556                 % (operation_name, self.query.combinator)\n1557             )\n1558 \n1559     def _check_operator_queryset(self, other, operator_):\n1560         if self.query.combinator or other.query.combinator:\n1561             raise TypeError(f'Cannot use {operator_} operator with combined queryset.')\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "django/db/models/sql/compiler.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__4.1/django/db/models/sql/compiler.py",
        "start": 1,
        "end": 1700,
        "class_name": null,
        "method_name": null,
        "code": "1 import collections\n2 import json\n3 import re\n4 from functools import partial\n5 from itertools import chain\n6 \n7 from django.core.exceptions import EmptyResultSet, FieldError\n8 from django.db import DatabaseError, NotSupportedError\n9 from django.db.models.constants import LOOKUP_SEP\n10 from django.db.models.expressions import F, OrderBy, RawSQL, Ref, Value\n11 from django.db.models.functions import Cast, Random\n12 from django.db.models.query_utils import select_related_descend\n13 from django.db.models.sql.constants import (\n14     CURSOR, GET_ITERATOR_CHUNK_SIZE, MULTI, NO_RESULTS, ORDER_DIR, SINGLE,\n15 )\n16 from django.db.models.sql.query import Query, get_order_dir\n17 from django.db.transaction import TransactionManagementError\n18 from django.utils.functional import cached_property\n19 from django.utils.hashable import make_hashable\n20 from django.utils.regex_helper import _lazy_re_compile\n21 \n22 \n23 class SQLCompiler:\n24     # Multiline ordering SQL clause may appear from RawSQL.\n25     ordering_parts = _lazy_re_compile(\n26         r'^(.*)\\s(?:ASC|DESC).*',\n27         re.MULTILINE | re.DOTALL,\n28     )\n29 \n30     def __init__(self, query, connection, using, elide_empty=True):\n31         self.query = query\n32         self.connection = connection\n33         self.using = using\n34         # Some queries, e.g. coalesced aggregation, need to be executed even if\n35         # they would return an empty result set.\n36         self.elide_empty = elide_empty\n37         self.quote_cache = {'*': '*'}\n38         # The select, klass_info, and annotations are needed by QuerySet.iterator()\n39         # these are set as a side-effect of executing the query. Note that we calculate\n40         # separately a list of extra select columns needed for grammatical correctness\n41         # of the query, but these columns are not included in self.select.\n42         self.select = None\n43         self.annotation_col_map = None\n44         self.klass_info = None\n45         self._meta_ordering = None\n46 \n47     def __repr__(self):\n48         return (\n49             f'<{self.__class__.__qualname__} '\n50             f'model={self.query.model.__qualname__} '\n51             f'connection={self.connection!r} using={self.using!r}>'\n52         )\n53 \n54     def setup_query(self):\n55         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):\n56             self.query.get_initial_alias()\n57         self.select, self.klass_info, self.annotation_col_map = self.get_select()\n58         self.col_count = len(self.select)\n59 \n60     def pre_sql_setup(self):\n61         \"\"\"\n62         Do any necessary class setup immediately prior to producing SQL. This\n63         is for things that can't necessarily be done in __init__ because we\n64         might not have all the pieces in place at that time.\n65         \"\"\"\n66         self.setup_query()\n67         order_by = self.get_order_by()\n68         self.where, self.having = self.query.where.split_having()\n69         extra_select = self.get_extra_select(order_by, self.select)\n70         self.has_extra_select = bool(extra_select)\n71         group_by = self.get_group_by(self.select + extra_select, order_by)\n72         return extra_select, order_by, group_by\n73 \n74     def get_group_by(self, select, order_by):\n75         \"\"\"\n76         Return a list of 2-tuples of form (sql, params).\n77 \n78         The logic of what exactly the GROUP BY clause contains is hard\n79         to describe in other words than \"if it passes the test suite,\n80         then it is correct\".\n81         \"\"\"\n82         # Some examples:\n83         #     SomeModel.objects.annotate(Count('somecol'))\n84         #     GROUP BY: all fields of the model\n85         #\n86         #    SomeModel.objects.values('name').annotate(Count('somecol'))\n87         #    GROUP BY: name\n88         #\n89         #    SomeModel.objects.annotate(Count('somecol')).values('name')\n90         #    GROUP BY: all cols of the model\n91         #\n92         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')\n93         #    GROUP BY: name, pk\n94         #\n95         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')\n96         #    GROUP BY: name, pk\n97         #\n98         # In fact, the self.query.group_by is the minimal set to GROUP BY. It\n99         # can't be ever restricted to a smaller set, but additional columns in\n100         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately\n101         # the end result is that it is impossible to force the query to have\n102         # a chosen GROUP BY clause - you can almost do this by using the form:\n103         #     .values(*wanted_cols).annotate(AnAggregate())\n104         # but any later annotations, extra selects, values calls that\n105         # refer some column outside of the wanted_cols, order_by, or even\n106         # filter calls can alter the GROUP BY clause.\n107 \n108         # The query.group_by is either None (no GROUP BY at all), True\n109         # (group by select fields), or a list of expressions to be added\n110         # to the group by.\n111         if self.query.group_by is None:\n112             return []\n113         expressions = []\n114         if self.query.group_by is not True:\n115             # If the group by is set to a list (by .values() call most likely),\n116             # then we need to add everything in it to the GROUP BY clause.\n117             # Backwards compatibility hack for setting query.group_by. Remove\n118             # when  we have public API way of forcing the GROUP BY clause.\n119             # Converts string references to expressions.\n120             for expr in self.query.group_by:\n121                 if not hasattr(expr, 'as_sql'):\n122                     expressions.append(self.query.resolve_ref(expr))\n123                 else:\n124                     expressions.append(expr)\n125         # Note that even if the group_by is set, it is only the minimal\n126         # set to group by. So, we need to add cols in select, order_by, and\n127         # having into the select in any case.\n128         ref_sources = {\n129             expr.source for expr in expressions if isinstance(expr, Ref)\n130         }\n131         for expr, _, _ in select:\n132             # Skip members of the select clause that are already included\n133             # by reference.\n134             if expr in ref_sources:\n135                 continue\n136             cols = expr.get_group_by_cols()\n137             for col in cols:\n138                 expressions.append(col)\n139         if not self._meta_ordering:\n140             for expr, (sql, params, is_ref) in order_by:\n141                 # Skip references to the SELECT clause, as all expressions in\n142                 # the SELECT clause are already part of the GROUP BY.\n143                 if not is_ref:\n144                     expressions.extend(expr.get_group_by_cols())\n145         having_group_by = self.having.get_group_by_cols() if self.having else ()\n146         for expr in having_group_by:\n147             expressions.append(expr)\n148         result = []\n149         seen = set()\n150         expressions = self.collapse_group_by(expressions, having_group_by)\n151 \n152         for expr in expressions:\n153             sql, params = self.compile(expr)\n154             sql, params = expr.select_format(self, sql, params)\n155             params_hash = make_hashable(params)\n156             if (sql, params_hash) not in seen:\n157                 result.append((sql, params))\n158                 seen.add((sql, params_hash))\n159         return result\n160 \n161     def collapse_group_by(self, expressions, having):\n162         # If the DB can group by primary key, then group by the primary key of\n163         # query's main model. Note that for PostgreSQL the GROUP BY clause must\n164         # include the primary key of every table, but for MySQL it is enough to\n165         # have the main table's primary key.\n166         if self.connection.features.allows_group_by_pk:\n167             # Determine if the main model's primary key is in the query.\n168             pk = None\n169             for expr in expressions:\n170                 # Is this a reference to query's base table primary key? If the\n171                 # expression isn't a Col-like, then skip the expression.\n172                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and\n173                         getattr(expr, 'alias', None) == self.query.base_table):\n174                     pk = expr\n175                     break\n176             # If the main model's primary key is in the query, group by that\n177             # field, HAVING expressions, and expressions associated with tables\n178             # that don't have a primary key included in the grouped columns.\n179             if pk:\n180                 pk_aliases = {\n181                     expr.alias for expr in expressions\n182                     if hasattr(expr, 'target') and expr.target.primary_key\n183                 }\n184                 expressions = [pk] + [\n185                     expr for expr in expressions\n186                     if expr in having or (\n187                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases\n188                     )\n189                 ]\n190         elif self.connection.features.allows_group_by_selected_pks:\n191             # Filter out all expressions associated with a table's primary key\n192             # present in the grouped columns. This is done by identifying all\n193             # tables that have their primary key included in the grouped\n194             # columns and removing non-primary key columns referring to them.\n195             # Unmanaged models are excluded because they could be representing\n196             # database views on which the optimization might not be allowed.\n197             pks = {\n198                 expr for expr in expressions\n199                 if (\n200                     hasattr(expr, 'target') and\n201                     expr.target.primary_key and\n202                     self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)\n203                 )\n204             }\n205             aliases = {expr.alias for expr in pks}\n206             expressions = [\n207                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases\n208             ]\n209         return expressions\n210 \n211     def get_select(self):\n212         \"\"\"\n213         Return three values:\n214         - a list of 3-tuples of (expression, (sql, params), alias)\n215         - a klass_info structure,\n216         - a dictionary of annotations\n217 \n218         The (sql, params) is what the expression will produce, and alias is the\n219         \"AS alias\" for the column (possibly None).\n220 \n221         The klass_info structure contains the following information:\n222         - The base model of the query.\n223         - Which columns for that model are present in the query (by\n224           position of the select clause).\n225         - related_klass_infos: [f, klass_info] to descent into\n226 \n227         The annotations is a dictionary of {'attname': column position} values.\n228         \"\"\"\n229         select = []\n230         klass_info = None\n231         annotations = {}\n232         select_idx = 0\n233         for alias, (sql, params) in self.query.extra_select.items():\n234             annotations[alias] = select_idx\n235             select.append((RawSQL(sql, params), alias))\n236             select_idx += 1\n237         assert not (self.query.select and self.query.default_cols)\n238         if self.query.default_cols:\n239             cols = self.get_default_columns()\n240         else:\n241             # self.query.select is a special case. These columns never go to\n242             # any model.\n243             cols = self.query.select\n244         if cols:\n245             select_list = []\n246             for col in cols:\n247                 select_list.append(select_idx)\n248                 select.append((col, None))\n249                 select_idx += 1\n250             klass_info = {\n251                 'model': self.query.model,\n252                 'select_fields': select_list,\n253             }\n254         for alias, annotation in self.query.annotation_select.items():\n255             annotations[alias] = select_idx\n256             select.append((annotation, alias))\n257             select_idx += 1\n258 \n259         if self.query.select_related:\n260             related_klass_infos = self.get_related_selections(select)\n261             klass_info['related_klass_infos'] = related_klass_infos\n262 \n263             def get_select_from_parent(klass_info):\n264                 for ki in klass_info['related_klass_infos']:\n265                     if ki['from_parent']:\n266                         ki['select_fields'] = (klass_info['select_fields'] +\n267                                                ki['select_fields'])\n268                     get_select_from_parent(ki)\n269             get_select_from_parent(klass_info)\n270 \n271         ret = []\n272         for col, alias in select:\n273             try:\n274                 sql, params = self.compile(col)\n275             except EmptyResultSet:\n276                 empty_result_set_value = getattr(col, 'empty_result_set_value', NotImplemented)\n277                 if empty_result_set_value is NotImplemented:\n278                     # Select a predicate that's always False.\n279                     sql, params = '0', ()\n280                 else:\n281                     sql, params = self.compile(Value(empty_result_set_value))\n282             else:\n283                 sql, params = col.select_format(self, sql, params)\n284             ret.append((col, (sql, params), alias))\n285         return ret, klass_info, annotations\n286 \n287     def _order_by_pairs(self):\n288         if self.query.extra_order_by:\n289             ordering = self.query.extra_order_by\n290         elif not self.query.default_ordering:\n291             ordering = self.query.order_by\n292         elif self.query.order_by:\n293             ordering = self.query.order_by\n294         elif self.query.get_meta().ordering:\n295             ordering = self.query.get_meta().ordering\n296             self._meta_ordering = ordering\n297         else:\n298             ordering = []\n299         if self.query.standard_ordering:\n300             default_order, _ = ORDER_DIR['ASC']\n301         else:\n302             default_order, _ = ORDER_DIR['DESC']\n303 \n304         for field in ordering:\n305             if hasattr(field, 'resolve_expression'):\n306                 if isinstance(field, Value):\n307                     # output_field must be resolved for constants.\n308                     field = Cast(field, field.output_field)\n309                 if not isinstance(field, OrderBy):\n310                     field = field.asc()\n311                 if not self.query.standard_ordering:\n312                     field = field.copy()\n313                     field.reverse_ordering()\n314                 yield field, False\n315                 continue\n316             if field == '?':  # random\n317                 yield OrderBy(Random()), False\n318                 continue\n319 \n320             col, order = get_order_dir(field, default_order)\n321             descending = order == 'DESC'\n322 \n323             if col in self.query.annotation_select:\n324                 # Reference to expression in SELECT clause\n325                 yield (\n326                     OrderBy(\n327                         Ref(col, self.query.annotation_select[col]),\n328                         descending=descending,\n329                     ),\n330                     True,\n331                 )\n332                 continue\n333             if col in self.query.annotations:\n334                 # References to an expression which is masked out of the SELECT\n335                 # clause.\n336                 if self.query.combinator and self.select:\n337                     # Don't use the resolved annotation because other\n338                     # combinated queries might define it differently.\n339                     expr = F(col)\n340                 else:\n341                     expr = self.query.annotations[col]\n342                     if isinstance(expr, Value):\n343                         # output_field must be resolved for constants.\n344                         expr = Cast(expr, expr.output_field)\n345                 yield OrderBy(expr, descending=descending), False\n346                 continue\n347 \n348             if '.' in field:\n349                 # This came in through an extra(order_by=...) addition. Pass it\n350                 # on verbatim.\n351                 table, col = col.split('.', 1)\n352                 yield (\n353                     OrderBy(\n354                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),\n355                         descending=descending,\n356                     ),\n357                     False,\n358                 )\n359                 continue\n360 \n361             if self.query.extra and col in self.query.extra:\n362                 if col in self.query.extra_select:\n363                     yield (\n364                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),\n365                         True,\n366                     )\n367                 else:\n368                     yield (\n369                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),\n370                         False,\n371                     )\n372             else:\n373                 if self.query.combinator and self.select:\n374                     # Don't use the first model's field because other\n375                     # combinated queries might define it differently.\n376                     yield OrderBy(F(col), descending=descending), False\n377                 else:\n378                     # 'col' is of the form 'field' or 'field1__field2' or\n379                     # '-field1__field2__field', etc.\n380                     yield from self.find_ordering_name(\n381                         field, self.query.get_meta(), default_order=default_order,\n382                     )\n383 \n384     def get_order_by(self):\n385         \"\"\"\n386         Return a list of 2-tuples of the form (expr, (sql, params, is_ref)) for\n387         the ORDER BY clause.\n388 \n389         The order_by clause can alter the select clause (for example it can add\n390         aliases to clauses that do not yet have one, or it can add totally new\n391         select clauses).\n392         \"\"\"\n393         result = []\n394         seen = set()\n395 \n396         for expr, is_ref in self._order_by_pairs():\n397             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)\n398             if self.query.combinator and self.select:\n399                 src = resolved.get_source_expressions()[0]\n400                 expr_src = expr.get_source_expressions()[0]\n401                 # Relabel order by columns to raw numbers if this is a combined\n402                 # query; necessary since the columns can't be referenced by the\n403                 # fully qualified name and the simple column names may collide.\n404                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):\n405                     if is_ref and col_alias == src.refs:\n406                         src = src.source\n407                     elif col_alias and not (\n408                         isinstance(expr_src, F) and col_alias == expr_src.name\n409                     ):\n410                         continue\n411                     if src == sel_expr:\n412                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])\n413                         break\n414                 else:\n415                     if col_alias:\n416                         raise DatabaseError('ORDER BY term does not match any column in the result set.')\n417                     # Add column used in ORDER BY clause to the selected\n418                     # columns and to each combined query.\n419                     order_by_idx = len(self.query.select) + 1\n420                     col_name = f'__orderbycol{order_by_idx}'\n421                     for q in self.query.combined_queries:\n422                         q.add_annotation(expr_src, col_name)\n423                     self.query.add_select_col(resolved, col_name)\n424                     resolved.set_source_expressions([RawSQL(f'{order_by_idx}', ())])\n425             sql, params = self.compile(resolved)\n426             # Don't add the same column twice, but the order direction is\n427             # not taken into account so we strip it. When this entire method\n428             # is refactored into expressions, then we can check each part as we\n429             # generate it.\n430             without_ordering = self.ordering_parts.search(sql)[1]\n431             params_hash = make_hashable(params)\n432             if (without_ordering, params_hash) in seen:\n433                 continue\n434             seen.add((without_ordering, params_hash))\n435             result.append((resolved, (sql, params, is_ref)))\n436         return result\n437 \n438     def get_extra_select(self, order_by, select):\n439         extra_select = []\n440         if self.query.distinct and not self.query.distinct_fields:\n441             select_sql = [t[1] for t in select]\n442             for expr, (sql, params, is_ref) in order_by:\n443                 without_ordering = self.ordering_parts.search(sql)[1]\n444                 if not is_ref and (without_ordering, params) not in select_sql:\n445                     extra_select.append((expr, (without_ordering, params), None))\n446         return extra_select\n447 \n448     def quote_name_unless_alias(self, name):\n449         \"\"\"\n450         A wrapper around connection.ops.quote_name that doesn't quote aliases\n451         for table names. This avoids problems with some SQL dialects that treat\n452         quoted strings specially (e.g. PostgreSQL).\n453         \"\"\"\n454         if name in self.quote_cache:\n455             return self.quote_cache[name]\n456         if ((name in self.query.alias_map and name not in self.query.table_map) or\n457                 name in self.query.extra_select or (\n458                     self.query.external_aliases.get(name) and name not in self.query.table_map)):\n459             self.quote_cache[name] = name\n460             return name\n461         r = self.connection.ops.quote_name(name)\n462         self.quote_cache[name] = r\n463         return r\n464 \n465     def compile(self, node):\n466         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)\n467         if vendor_impl:\n468             sql, params = vendor_impl(self, self.connection)\n469         else:\n470             sql, params = node.as_sql(self, self.connection)\n471         return sql, params\n472 \n473     def get_combinator_sql(self, combinator, all):\n474         features = self.connection.features\n475         compilers = [\n476             query.get_compiler(self.using, self.connection, self.elide_empty)\n477             for query in self.query.combined_queries if not query.is_empty()\n478         ]\n479         if not features.supports_slicing_ordering_in_compound:\n480             for query, compiler in zip(self.query.combined_queries, compilers):\n481                 if query.low_mark or query.high_mark:\n482                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')\n483                 if compiler.get_order_by():\n484                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')\n485         parts = ()\n486         for compiler in compilers:\n487             try:\n488                 # If the columns list is limited, then all combined queries\n489                 # must have the same columns list. Set the selects defined on\n490                 # the query on all combined queries, if not already set.\n491                 if not compiler.query.values_select and self.query.values_select:\n492                     compiler.query = compiler.query.clone()\n493                     compiler.query.set_values((\n494                         *self.query.extra_select,\n495                         *self.query.values_select,\n496                         *self.query.annotation_select,\n497                     ))\n498                 part_sql, part_args = compiler.as_sql()\n499                 if compiler.query.combinator:\n500                     # Wrap in a subquery if wrapping in parentheses isn't\n501                     # supported.\n502                     if not features.supports_parentheses_in_compound:\n503                         part_sql = 'SELECT * FROM ({})'.format(part_sql)\n504                     # Add parentheses when combining with compound query if not\n505                     # already added for all compound queries.\n506                     elif (\n507                         self.query.subquery or\n508                         not features.supports_slicing_ordering_in_compound\n509                     ):\n510                         part_sql = '({})'.format(part_sql)\n511                 parts += ((part_sql, part_args),)\n512             except EmptyResultSet:\n513                 # Omit the empty queryset with UNION and with DIFFERENCE if the\n514                 # first queryset is nonempty.\n515                 if combinator == 'union' or (combinator == 'difference' and parts):\n516                     continue\n517                 raise\n518         if not parts:\n519             raise EmptyResultSet\n520         combinator_sql = self.connection.ops.set_operators[combinator]\n521         if all and combinator == 'union':\n522             combinator_sql += ' ALL'\n523         braces = '{}'\n524         if not self.query.subquery and features.supports_slicing_ordering_in_compound:\n525             braces = '({})'\n526         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))\n527         result = [' {} '.format(combinator_sql).join(sql_parts)]\n528         params = []\n529         for part in args_parts:\n530             params.extend(part)\n531         return result, params\n532 \n533     def as_sql(self, with_limits=True, with_col_aliases=False):\n534         \"\"\"\n535         Create the SQL for this query. Return the SQL string and list of\n536         parameters.\n537 \n538         If 'with_limits' is False, any limit/offset information is not included\n539         in the query.\n540         \"\"\"\n541         refcounts_before = self.query.alias_refcount.copy()\n542         try:\n543             extra_select, order_by, group_by = self.pre_sql_setup()\n544             for_update_part = None\n545             # Is a LIMIT/OFFSET clause needed?\n546             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)\n547             combinator = self.query.combinator\n548             features = self.connection.features\n549             if combinator:\n550                 if not getattr(features, 'supports_select_{}'.format(combinator)):\n551                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))\n552                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)\n553             else:\n554                 distinct_fields, distinct_params = self.get_distinct()\n555                 # This must come after 'select', 'ordering', and 'distinct'\n556                 # (see docstring of get_from_clause() for details).\n557                 from_, f_params = self.get_from_clause()\n558                 try:\n559                     where, w_params = self.compile(self.where) if self.where is not None else ('', [])\n560                 except EmptyResultSet:\n561                     if self.elide_empty:\n562                         raise\n563                     # Use a predicate that's always False.\n564                     where, w_params = '0 = 1', []\n565                 having, h_params = self.compile(self.having) if self.having is not None else (\"\", [])\n566                 result = ['SELECT']\n567                 params = []\n568 \n569                 if self.query.distinct:\n570                     distinct_result, distinct_params = self.connection.ops.distinct_sql(\n571                         distinct_fields,\n572                         distinct_params,\n573                     )\n574                     result += distinct_result\n575                     params += distinct_params\n576 \n577                 out_cols = []\n578                 col_idx = 1\n579                 for _, (s_sql, s_params), alias in self.select + extra_select:\n580                     if alias:\n581                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))\n582                     elif with_col_aliases:\n583                         s_sql = '%s AS %s' % (\n584                             s_sql,\n585                             self.connection.ops.quote_name('col%d' % col_idx),\n586                         )\n587                         col_idx += 1\n588                     params.extend(s_params)\n589                     out_cols.append(s_sql)\n590 \n591                 result += [', '.join(out_cols), 'FROM', *from_]\n592                 params.extend(f_params)\n593 \n594                 if self.query.select_for_update and self.connection.features.has_select_for_update:\n595                     if self.connection.get_autocommit():\n596                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')\n597 \n598                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:\n599                         raise NotSupportedError(\n600                             'LIMIT/OFFSET is not supported with '\n601                             'select_for_update on this database backend.'\n602                         )\n603                     nowait = self.query.select_for_update_nowait\n604                     skip_locked = self.query.select_for_update_skip_locked\n605                     of = self.query.select_for_update_of\n606                     no_key = self.query.select_for_no_key_update\n607                     # If it's a NOWAIT/SKIP LOCKED/OF/NO KEY query but the\n608                     # backend doesn't support it, raise NotSupportedError to\n609                     # prevent a possible deadlock.\n610                     if nowait and not self.connection.features.has_select_for_update_nowait:\n611                         raise NotSupportedError('NOWAIT is not supported on this database backend.')\n612                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:\n613                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')\n614                     elif of and not self.connection.features.has_select_for_update_of:\n615                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')\n616                     elif no_key and not self.connection.features.has_select_for_no_key_update:\n617                         raise NotSupportedError(\n618                             'FOR NO KEY UPDATE is not supported on this '\n619                             'database backend.'\n620                         )\n621                     for_update_part = self.connection.ops.for_update_sql(\n622                         nowait=nowait,\n623                         skip_locked=skip_locked,\n624                         of=self.get_select_for_update_of_arguments(),\n625                         no_key=no_key,\n626                     )\n627 \n628                 if for_update_part and self.connection.features.for_update_after_from:\n629                     result.append(for_update_part)\n630 \n631                 if where:\n632                     result.append('WHERE %s' % where)\n633                     params.extend(w_params)\n634 \n635                 grouping = []\n636                 for g_sql, g_params in group_by:\n637                     grouping.append(g_sql)\n638                     params.extend(g_params)\n639                 if grouping:\n640                     if distinct_fields:\n641                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')\n642                     order_by = order_by or self.connection.ops.force_no_ordering()\n643                     result.append('GROUP BY %s' % ', '.join(grouping))\n644                     if self._meta_ordering:\n645                         order_by = None\n646                 if having:\n647                     result.append('HAVING %s' % having)\n648                     params.extend(h_params)\n649 \n650             if self.query.explain_info:\n651                 result.insert(0, self.connection.ops.explain_query_prefix(\n652                     self.query.explain_info.format,\n653                     **self.query.explain_info.options\n654                 ))\n655 \n656             if order_by:\n657                 ordering = []\n658                 for _, (o_sql, o_params, _) in order_by:\n659                     ordering.append(o_sql)\n660                     params.extend(o_params)\n661                 result.append('ORDER BY %s' % ', '.join(ordering))\n662 \n663             if with_limit_offset:\n664                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))\n665 \n666             if for_update_part and not self.connection.features.for_update_after_from:\n667                 result.append(for_update_part)\n668 \n669             if self.query.subquery and extra_select:\n670                 # If the query is used as a subquery, the extra selects would\n671                 # result in more columns than the left-hand side expression is\n672                 # expecting. This can happen when a subquery uses a combination\n673                 # of order_by() and distinct(), forcing the ordering expressions\n674                 # to be selected as well. Wrap the query in another subquery\n675                 # to exclude extraneous selects.\n676                 sub_selects = []\n677                 sub_params = []\n678                 for index, (select, _, alias) in enumerate(self.select, start=1):\n679                     if not alias and with_col_aliases:\n680                         alias = 'col%d' % index\n681                     if alias:\n682                         sub_selects.append(\"%s.%s\" % (\n683                             self.connection.ops.quote_name('subquery'),\n684                             self.connection.ops.quote_name(alias),\n685                         ))\n686                     else:\n687                         select_clone = select.relabeled_clone({select.alias: 'subquery'})\n688                         subselect, subparams = select_clone.as_sql(self, self.connection)\n689                         sub_selects.append(subselect)\n690                         sub_params.extend(subparams)\n691                 return 'SELECT %s FROM (%s) subquery' % (\n692                     ', '.join(sub_selects),\n693                     ' '.join(result),\n694                 ), tuple(sub_params + params)\n695 \n696             return ' '.join(result), tuple(params)\n697         finally:\n698             # Finally do cleanup - get rid of the joins we created above.\n699             self.query.reset_refcounts(refcounts_before)\n700 \n701     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):\n702         \"\"\"\n703         Compute the default columns for selecting every field in the base\n704         model. Will sometimes be called to pull in related models (e.g. via\n705         select_related), in which case \"opts\" and \"start_alias\" will be given\n706         to provide a starting point for the traversal.\n707 \n708         Return a list of strings, quoted appropriately for use in SQL\n709         directly, as well as a set of aliases used in the select statement (if\n710         'as_pairs' is True, return a list of (alias, col_name) pairs instead\n711         of strings as the first component and None as the second component).\n712         \"\"\"\n713         result = []\n714         if opts is None:\n715             opts = self.query.get_meta()\n716         only_load = self.deferred_to_columns()\n717         start_alias = start_alias or self.query.get_initial_alias()\n718         # The 'seen_models' is used to optimize checking the needed parent\n719         # alias for a given field. This also includes None -> start_alias to\n720         # be used by local fields.\n721         seen_models = {None: start_alias}\n722 \n723         for field in opts.concrete_fields:\n724             model = field.model._meta.concrete_model\n725             # A proxy model will have a different model and concrete_model. We\n726             # will assign None if the field belongs to this model.\n727             if model == opts.model:\n728                 model = None\n729             if from_parent and model is not None and issubclass(\n730                     from_parent._meta.concrete_model, model._meta.concrete_model):\n731                 # Avoid loading data for already loaded parents.\n732                 # We end up here in the case select_related() resolution\n733                 # proceeds from parent model to child model. In that case the\n734                 # parent model data is already present in the SELECT clause,\n735                 # and we want to avoid reloading the same data again.\n736                 continue\n737             if field.model in only_load and field.attname not in only_load[field.model]:\n738                 continue\n739             alias = self.query.join_parent_model(opts, model, start_alias,\n740                                                  seen_models)\n741             column = field.get_col(alias)\n742             result.append(column)\n743         return result\n744 \n745     def get_distinct(self):\n746         \"\"\"\n747         Return a quoted list of fields to use in DISTINCT ON part of the query.\n748 \n749         This method can alter the tables in the query, and thus it must be\n750         called before get_from_clause().\n751         \"\"\"\n752         result = []\n753         params = []\n754         opts = self.query.get_meta()\n755 \n756         for name in self.query.distinct_fields:\n757             parts = name.split(LOOKUP_SEP)\n758             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)\n759             targets, alias, _ = self.query.trim_joins(targets, joins, path)\n760             for target in targets:\n761                 if name in self.query.annotation_select:\n762                     result.append(self.connection.ops.quote_name(name))\n763                 else:\n764                     r, p = self.compile(transform_function(target, alias))\n765                     result.append(r)\n766                     params.append(p)\n767         return result, params\n768 \n769     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',\n770                            already_seen=None):\n771         \"\"\"\n772         Return the table alias (the name might be ambiguous, the alias will\n773         not be) and column name for ordering by the given 'name' parameter.\n774         The 'name' is of the form 'field1__field2__...__fieldN'.\n775         \"\"\"\n776         name, order = get_order_dir(name, default_order)\n777         descending = order == 'DESC'\n778         pieces = name.split(LOOKUP_SEP)\n779         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)\n780 \n781         # If we get to this point and the field is a relation to another model,\n782         # append the default ordering for that model unless it is the pk\n783         # shortcut or the attribute name of the field that is specified.\n784         if (\n785             field.is_relation and\n786             opts.ordering and\n787             getattr(field, 'attname', None) != pieces[-1] and\n788             name != 'pk'\n789         ):\n790             # Firstly, avoid infinite loops.\n791             already_seen = already_seen or set()\n792             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)\n793             if join_tuple in already_seen:\n794                 raise FieldError('Infinite loop caused by ordering.')\n795             already_seen.add(join_tuple)\n796 \n797             results = []\n798             for item in opts.ordering:\n799                 if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):\n800                     item = item.desc() if descending else item.asc()\n801                 if isinstance(item, OrderBy):\n802                     results.append((item, False))\n803                     continue\n804                 results.extend(self.find_ordering_name(item, opts, alias,\n805                                                        order, already_seen))\n806             return results\n807         targets, alias, _ = self.query.trim_joins(targets, joins, path)\n808         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]\n809 \n810     def _setup_joins(self, pieces, opts, alias):\n811         \"\"\"\n812         Helper method for get_order_by() and get_distinct().\n813 \n814         get_ordering() and get_distinct() must produce same target columns on\n815         same input, as the prefixes of get_ordering() and get_distinct() must\n816         match. Executing SQL where this is not true is an error.\n817         \"\"\"\n818         alias = alias or self.query.get_initial_alias()\n819         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)\n820         alias = joins[-1]\n821         return field, targets, alias, joins, path, opts, transform_function\n822 \n823     def get_from_clause(self):\n824         \"\"\"\n825         Return a list of strings that are joined together to go after the\n826         \"FROM\" part of the query, as well as a list any extra parameters that\n827         need to be included. Subclasses, can override this to create a\n828         from-clause via a \"select\".\n829 \n830         This should only be called after any SQL construction methods that\n831         might change the tables that are needed. This means the select columns,\n832         ordering, and distinct must be done first.\n833         \"\"\"\n834         result = []\n835         params = []\n836         for alias in tuple(self.query.alias_map):\n837             if not self.query.alias_refcount[alias]:\n838                 continue\n839             try:\n840                 from_clause = self.query.alias_map[alias]\n841             except KeyError:\n842                 # Extra tables can end up in self.tables, but not in the\n843                 # alias_map if they aren't in a join. That's OK. We skip them.\n844                 continue\n845             clause_sql, clause_params = self.compile(from_clause)\n846             result.append(clause_sql)\n847             params.extend(clause_params)\n848         for t in self.query.extra_tables:\n849             alias, _ = self.query.table_alias(t)\n850             # Only add the alias if it's not already present (the table_alias()\n851             # call increments the refcount, so an alias refcount of one means\n852             # this is the only reference).\n853             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:\n854                 result.append(', %s' % self.quote_name_unless_alias(alias))\n855         return result, params\n856 \n857     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,\n858                                requested=None, restricted=None):\n859         \"\"\"\n860         Fill in the information needed for a select_related query. The current\n861         depth is measured as the number of connections away from the root model\n862         (for example, cur_depth=1 means we are looking at models with direct\n863         connections to the root model).\n864         \"\"\"\n865         def _get_field_choices():\n866             direct_choices = (f.name for f in opts.fields if f.is_relation)\n867             reverse_choices = (\n868                 f.field.related_query_name()\n869                 for f in opts.related_objects if f.field.unique\n870             )\n871             return chain(direct_choices, reverse_choices, self.query._filtered_relations)\n872 \n873         related_klass_infos = []\n874         if not restricted and cur_depth > self.query.max_depth:\n875             # We've recursed far enough; bail out.\n876             return related_klass_infos\n877 \n878         if not opts:\n879             opts = self.query.get_meta()\n880             root_alias = self.query.get_initial_alias()\n881         only_load = self.query.get_loaded_field_names()\n882 \n883         # Setup for the case when only particular related fields should be\n884         # included in the related selection.\n885         fields_found = set()\n886         if requested is None:\n887             restricted = isinstance(self.query.select_related, dict)\n888             if restricted:\n889                 requested = self.query.select_related\n890 \n891         def get_related_klass_infos(klass_info, related_klass_infos):\n892             klass_info['related_klass_infos'] = related_klass_infos\n893 \n894         for f in opts.fields:\n895             field_model = f.model._meta.concrete_model\n896             fields_found.add(f.name)\n897 \n898             if restricted:\n899                 next = requested.get(f.name, {})\n900                 if not f.is_relation:\n901                     # If a non-related field is used like a relation,\n902                     # or if a single non-relational field is given.\n903                     if next or f.name in requested:\n904                         raise FieldError(\n905                             \"Non-relational field given in select_related: '%s'. \"\n906                             \"Choices are: %s\" % (\n907                                 f.name,\n908                                 \", \".join(_get_field_choices()) or '(none)',\n909                             )\n910                         )\n911             else:\n912                 next = False\n913 \n914             if not select_related_descend(f, restricted, requested,\n915                                           only_load.get(field_model)):\n916                 continue\n917             klass_info = {\n918                 'model': f.remote_field.model,\n919                 'field': f,\n920                 'reverse': False,\n921                 'local_setter': f.set_cached_value,\n922                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,\n923                 'from_parent': False,\n924             }\n925             related_klass_infos.append(klass_info)\n926             select_fields = []\n927             _, _, _, joins, _, _ = self.query.setup_joins(\n928                 [f.name], opts, root_alias)\n929             alias = joins[-1]\n930             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)\n931             for col in columns:\n932                 select_fields.append(len(select))\n933                 select.append((col, None))\n934             klass_info['select_fields'] = select_fields\n935             next_klass_infos = self.get_related_selections(\n936                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)\n937             get_related_klass_infos(klass_info, next_klass_infos)\n938 \n939         if restricted:\n940             related_fields = [\n941                 (o.field, o.related_model)\n942                 for o in opts.related_objects\n943                 if o.field.unique and not o.many_to_many\n944             ]\n945             for f, model in related_fields:\n946                 if not select_related_descend(f, restricted, requested,\n947                                               only_load.get(model), reverse=True):\n948                     continue\n949 \n950                 related_field_name = f.related_query_name()\n951                 fields_found.add(related_field_name)\n952 \n953                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)\n954                 alias = join_info.joins[-1]\n955                 from_parent = issubclass(model, opts.model) and model is not opts.model\n956                 klass_info = {\n957                     'model': model,\n958                     'field': f,\n959                     'reverse': True,\n960                     'local_setter': f.remote_field.set_cached_value,\n961                     'remote_setter': f.set_cached_value,\n962                     'from_parent': from_parent,\n963                 }\n964                 related_klass_infos.append(klass_info)\n965                 select_fields = []\n966                 columns = self.get_default_columns(\n967                     start_alias=alias, opts=model._meta, from_parent=opts.model)\n968                 for col in columns:\n969                     select_fields.append(len(select))\n970                     select.append((col, None))\n971                 klass_info['select_fields'] = select_fields\n972                 next = requested.get(f.related_query_name(), {})\n973                 next_klass_infos = self.get_related_selections(\n974                     select, model._meta, alias, cur_depth + 1,\n975                     next, restricted)\n976                 get_related_klass_infos(klass_info, next_klass_infos)\n977 \n978             def local_setter(obj, from_obj):\n979                 # Set a reverse fk object when relation is non-empty.\n980                 if from_obj:\n981                     f.remote_field.set_cached_value(from_obj, obj)\n982 \n983             def remote_setter(name, obj, from_obj):\n984                 setattr(from_obj, name, obj)\n985 \n986             for name in list(requested):\n987                 # Filtered relations work only on the topmost level.\n988                 if cur_depth > 1:\n989                     break\n990                 if name in self.query._filtered_relations:\n991                     fields_found.add(name)\n992                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)\n993                     model = join_opts.model\n994                     alias = joins[-1]\n995                     from_parent = issubclass(model, opts.model) and model is not opts.model\n996                     klass_info = {\n997                         'model': model,\n998                         'field': f,\n999                         'reverse': True,\n1000                         'local_setter': local_setter,\n1001                         'remote_setter': partial(remote_setter, name),\n1002                         'from_parent': from_parent,\n1003                     }\n1004                     related_klass_infos.append(klass_info)\n1005                     select_fields = []\n1006                     columns = self.get_default_columns(\n1007                         start_alias=alias, opts=model._meta,\n1008                         from_parent=opts.model,\n1009                     )\n1010                     for col in columns:\n1011                         select_fields.append(len(select))\n1012                         select.append((col, None))\n1013                     klass_info['select_fields'] = select_fields\n1014                     next_requested = requested.get(name, {})\n1015                     next_klass_infos = self.get_related_selections(\n1016                         select, opts=model._meta, root_alias=alias,\n1017                         cur_depth=cur_depth + 1, requested=next_requested,\n1018                         restricted=restricted,\n1019                     )\n1020                     get_related_klass_infos(klass_info, next_klass_infos)\n1021             fields_not_found = set(requested).difference(fields_found)\n1022             if fields_not_found:\n1023                 invalid_fields = (\"'%s'\" % s for s in fields_not_found)\n1024                 raise FieldError(\n1025                     'Invalid field name(s) given in select_related: %s. '\n1026                     'Choices are: %s' % (\n1027                         ', '.join(invalid_fields),\n1028                         ', '.join(_get_field_choices()) or '(none)',\n1029                     )\n1030                 )\n1031         return related_klass_infos\n1032 \n1033     def get_select_for_update_of_arguments(self):\n1034         \"\"\"\n1035         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of\n1036         the query.\n1037         \"\"\"\n1038         def _get_parent_klass_info(klass_info):\n1039             concrete_model = klass_info['model']._meta.concrete_model\n1040             for parent_model, parent_link in concrete_model._meta.parents.items():\n1041                 parent_list = parent_model._meta.get_parent_list()\n1042                 yield {\n1043                     'model': parent_model,\n1044                     'field': parent_link,\n1045                     'reverse': False,\n1046                     'select_fields': [\n1047                         select_index\n1048                         for select_index in klass_info['select_fields']\n1049                         # Selected columns from a model or its parents.\n1050                         if (\n1051                             self.select[select_index][0].target.model == parent_model or\n1052                             self.select[select_index][0].target.model in parent_list\n1053                         )\n1054                     ],\n1055                 }\n1056 \n1057         def _get_first_selected_col_from_model(klass_info):\n1058             \"\"\"\n1059             Find the first selected column from a model. If it doesn't exist,\n1060             don't lock a model.\n1061 \n1062             select_fields is filled recursively, so it also contains fields\n1063             from the parent models.\n1064             \"\"\"\n1065             concrete_model = klass_info['model']._meta.concrete_model\n1066             for select_index in klass_info['select_fields']:\n1067                 if self.select[select_index][0].target.model == concrete_model:\n1068                     return self.select[select_index][0]\n1069 \n1070         def _get_field_choices():\n1071             \"\"\"Yield all allowed field paths in breadth-first search order.\"\"\"\n1072             queue = collections.deque([(None, self.klass_info)])\n1073             while queue:\n1074                 parent_path, klass_info = queue.popleft()\n1075                 if parent_path is None:\n1076                     path = []\n1077                     yield 'self'\n1078                 else:\n1079                     field = klass_info['field']\n1080                     if klass_info['reverse']:\n1081                         field = field.remote_field\n1082                     path = parent_path + [field.name]\n1083                     yield LOOKUP_SEP.join(path)\n1084                 queue.extend(\n1085                     (path, klass_info)\n1086                     for klass_info in _get_parent_klass_info(klass_info)\n1087                 )\n1088                 queue.extend(\n1089                     (path, klass_info)\n1090                     for klass_info in klass_info.get('related_klass_infos', [])\n1091                 )\n1092         if not self.klass_info:\n1093             return []\n1094         result = []\n1095         invalid_names = []\n1096         for name in self.query.select_for_update_of:\n1097             klass_info = self.klass_info\n1098             if name == 'self':\n1099                 col = _get_first_selected_col_from_model(klass_info)\n1100             else:\n1101                 for part in name.split(LOOKUP_SEP):\n1102                     klass_infos = (\n1103                         *klass_info.get('related_klass_infos', []),\n1104                         *_get_parent_klass_info(klass_info),\n1105                     )\n1106                     for related_klass_info in klass_infos:\n1107                         field = related_klass_info['field']\n1108                         if related_klass_info['reverse']:\n1109                             field = field.remote_field\n1110                         if field.name == part:\n1111                             klass_info = related_klass_info\n1112                             break\n1113                     else:\n1114                         klass_info = None\n1115                         break\n1116                 if klass_info is None:\n1117                     invalid_names.append(name)\n1118                     continue\n1119                 col = _get_first_selected_col_from_model(klass_info)\n1120             if col is not None:\n1121                 if self.connection.features.select_for_update_of_column:\n1122                     result.append(self.compile(col)[0])\n1123                 else:\n1124                     result.append(self.quote_name_unless_alias(col.alias))\n1125         if invalid_names:\n1126             raise FieldError(\n1127                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '\n1128                 'Only relational fields followed in the query are allowed. '\n1129                 'Choices are: %s.' % (\n1130                     ', '.join(invalid_names),\n1131                     ', '.join(_get_field_choices()),\n1132                 )\n1133             )\n1134         return result\n1135 \n1136     def deferred_to_columns(self):\n1137         \"\"\"\n1138         Convert the self.deferred_loading data structure to mapping of table\n1139         names to sets of column names which are to be loaded. Return the\n1140         dictionary.\n1141         \"\"\"\n1142         columns = {}\n1143         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)\n1144         return columns\n1145 \n1146     def get_converters(self, expressions):\n1147         converters = {}\n1148         for i, expression in enumerate(expressions):\n1149             if expression:\n1150                 backend_converters = self.connection.ops.get_db_converters(expression)\n1151                 field_converters = expression.get_db_converters(self.connection)\n1152                 if backend_converters or field_converters:\n1153                     converters[i] = (backend_converters + field_converters, expression)\n1154         return converters\n1155 \n1156     def apply_converters(self, rows, converters):\n1157         connection = self.connection\n1158         converters = list(converters.items())\n1159         for row in map(list, rows):\n1160             for pos, (convs, expression) in converters:\n1161                 value = row[pos]\n1162                 for converter in convs:\n1163                     value = converter(value, expression, connection)\n1164                 row[pos] = value\n1165             yield row\n1166 \n1167     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,\n1168                      chunk_size=GET_ITERATOR_CHUNK_SIZE):\n1169         \"\"\"Return an iterator over the results from executing this query.\"\"\"\n1170         if results is None:\n1171             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)\n1172         fields = [s[0] for s in self.select[0:self.col_count]]\n1173         converters = self.get_converters(fields)\n1174         rows = chain.from_iterable(results)\n1175         if converters:\n1176             rows = self.apply_converters(rows, converters)\n1177             if tuple_expected:\n1178                 rows = map(tuple, rows)\n1179         return rows\n1180 \n1181     def has_results(self):\n1182         \"\"\"\n1183         Backends (e.g. NoSQL) can override this in order to use optimized\n1184         versions of \"query has any results.\"\n1185         \"\"\"\n1186         return bool(self.execute_sql(SINGLE))\n1187 \n1188     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):\n1189         \"\"\"\n1190         Run the query against the database and return the result(s). The\n1191         return value is a single data item if result_type is SINGLE, or an\n1192         iterator over the results if the result_type is MULTI.\n1193 \n1194         result_type is either MULTI (use fetchmany() to retrieve all rows),\n1195         SINGLE (only retrieve a single row), or None. In this last case, the\n1196         cursor is returned if any query is executed, since it's used by\n1197         subclasses such as InsertQuery). It's possible, however, that no query\n1198         is needed, as the filters describe an empty set. In that case, None is\n1199         returned, to avoid any unnecessary database interaction.\n1200         \"\"\"\n1201         result_type = result_type or NO_RESULTS\n1202         try:\n1203             sql, params = self.as_sql()\n1204             if not sql:\n1205                 raise EmptyResultSet\n1206         except EmptyResultSet:\n1207             if result_type == MULTI:\n1208                 return iter([])\n1209             else:\n1210                 return\n1211         if chunked_fetch:\n1212             cursor = self.connection.chunked_cursor()\n1213         else:\n1214             cursor = self.connection.cursor()\n1215         try:\n1216             cursor.execute(sql, params)\n1217         except Exception:\n1218             # Might fail for server-side cursors (e.g. connection closed)\n1219             cursor.close()\n1220             raise\n1221 \n1222         if result_type == CURSOR:\n1223             # Give the caller the cursor to process and close.\n1224             return cursor\n1225         if result_type == SINGLE:\n1226             try:\n1227                 val = cursor.fetchone()\n1228                 if val:\n1229                     return val[0:self.col_count]\n1230                 return val\n1231             finally:\n1232                 # done with the cursor\n1233                 cursor.close()\n1234         if result_type == NO_RESULTS:\n1235             cursor.close()\n1236             return\n1237 \n1238         result = cursor_iter(\n1239             cursor, self.connection.features.empty_fetchmany_value,\n1240             self.col_count if self.has_extra_select else None,\n1241             chunk_size,\n1242         )\n1243         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:\n1244             # If we are using non-chunked reads, we return the same data\n1245             # structure as normally, but ensure it is all read into memory\n1246             # before going any further. Use chunked_fetch if requested,\n1247             # unless the database doesn't support it.\n1248             return list(result)\n1249         return result\n1250 \n1251     def as_subquery_condition(self, alias, columns, compiler):\n1252         qn = compiler.quote_name_unless_alias\n1253         qn2 = self.connection.ops.quote_name\n1254 \n1255         for index, select_col in enumerate(self.query.select):\n1256             lhs_sql, lhs_params = self.compile(select_col)\n1257             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))\n1258             self.query.where.add(\n1259                 RawSQL('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')\n1260 \n1261         sql, params = self.as_sql()\n1262         return 'EXISTS (%s)' % sql, params\n1263 \n1264     def explain_query(self):\n1265         result = list(self.execute_sql())\n1266         # Some backends return 1 item tuples with strings, and others return\n1267         # tuples with integers and strings. Flatten them out into strings.\n1268         output_formatter = json.dumps if self.query.explain_info.format == 'json' else str\n1269         for row in result[0]:\n1270             if not isinstance(row, str):\n1271                 yield ' '.join(output_formatter(c) for c in row)\n1272             else:\n1273                 yield row\n1274 \n1275 \n1276 class SQLInsertCompiler(SQLCompiler):\n1277     returning_fields = None\n1278     returning_params = tuple()\n1279 \n1280     def field_as_sql(self, field, val):\n1281         \"\"\"\n1282         Take a field and a value intended to be saved on that field, and\n1283         return placeholder SQL and accompanying params. Check for raw values,\n1284         expressions, and fields with get_placeholder() defined in that order.\n1285 \n1286         When field is None, consider the value raw and use it as the\n1287         placeholder, with no corresponding parameters returned.\n1288         \"\"\"\n1289         if field is None:\n1290             # A field value of None means the value is raw.\n1291             sql, params = val, []\n1292         elif hasattr(val, 'as_sql'):\n1293             # This is an expression, let's compile it.\n1294             sql, params = self.compile(val)\n1295         elif hasattr(field, 'get_placeholder'):\n1296             # Some fields (e.g. geo fields) need special munging before\n1297             # they can be inserted.\n1298             sql, params = field.get_placeholder(val, self, self.connection), [val]\n1299         else:\n1300             # Return the common case for the placeholder\n1301             sql, params = '%s', [val]\n1302 \n1303         # The following hook is only used by Oracle Spatial, which sometimes\n1304         # needs to yield 'NULL' and [] as its placeholder and params instead\n1305         # of '%s' and [None]. The 'NULL' placeholder is produced earlier by\n1306         # OracleOperations.get_geom_placeholder(). The following line removes\n1307         # the corresponding None parameter. See ticket #10888.\n1308         params = self.connection.ops.modify_insert_params(sql, params)\n1309 \n1310         return sql, params\n1311 \n1312     def prepare_value(self, field, value):\n1313         \"\"\"\n1314         Prepare a value to be used in a query by resolving it if it is an\n1315         expression and otherwise calling the field's get_db_prep_save().\n1316         \"\"\"\n1317         if hasattr(value, 'resolve_expression'):\n1318             value = value.resolve_expression(self.query, allow_joins=False, for_save=True)\n1319             # Don't allow values containing Col expressions. They refer to\n1320             # existing columns on a row, but in the case of insert the row\n1321             # doesn't exist yet.\n1322             if value.contains_column_references:\n1323                 raise ValueError(\n1324                     'Failed to insert expression \"%s\" on %s. F() expressions '\n1325                     'can only be used to update, not to insert.' % (value, field)\n1326                 )\n1327             if value.contains_aggregate:\n1328                 raise FieldError(\n1329                     'Aggregate functions are not allowed in this query '\n1330                     '(%s=%r).' % (field.name, value)\n1331                 )\n1332             if value.contains_over_clause:\n1333                 raise FieldError(\n1334                     'Window expressions are not allowed in this query (%s=%r).'\n1335                     % (field.name, value)\n1336                 )\n1337         else:\n1338             value = field.get_db_prep_save(value, connection=self.connection)\n1339         return value\n1340 \n1341     def pre_save_val(self, field, obj):\n1342         \"\"\"\n1343         Get the given field's value off the given obj. pre_save() is used for\n1344         things like auto_now on DateTimeField. Skip it if this is a raw query.\n1345         \"\"\"\n1346         if self.query.raw:\n1347             return getattr(obj, field.attname)\n1348         return field.pre_save(obj, add=True)\n1349 \n1350     def assemble_as_sql(self, fields, value_rows):\n1351         \"\"\"\n1352         Take a sequence of N fields and a sequence of M rows of values, and\n1353         generate placeholder SQL and parameters for each field and value.\n1354         Return a pair containing:\n1355          * a sequence of M rows of N SQL placeholder strings, and\n1356          * a sequence of M rows of corresponding parameter values.\n1357 \n1358         Each placeholder string may contain any number of '%s' interpolation\n1359         strings, and each parameter row will contain exactly as many params\n1360         as the total number of '%s's in the corresponding placeholder row.\n1361         \"\"\"\n1362         if not value_rows:\n1363             return [], []\n1364 \n1365         # list of (sql, [params]) tuples for each object to be saved\n1366         # Shape: [n_objs][n_fields][2]\n1367         rows_of_fields_as_sql = (\n1368             (self.field_as_sql(field, v) for field, v in zip(fields, row))\n1369             for row in value_rows\n1370         )\n1371 \n1372         # tuple like ([sqls], [[params]s]) for each object to be saved\n1373         # Shape: [n_objs][2][n_fields]\n1374         sql_and_param_pair_rows = (zip(*row) for row in rows_of_fields_as_sql)\n1375 \n1376         # Extract separate lists for placeholders and params.\n1377         # Each of these has shape [n_objs][n_fields]\n1378         placeholder_rows, param_rows = zip(*sql_and_param_pair_rows)\n1379 \n1380         # Params for each field are still lists, and need to be flattened.\n1381         param_rows = [[p for ps in row for p in ps] for row in param_rows]\n1382 \n1383         return placeholder_rows, param_rows\n1384 \n1385     def as_sql(self):\n1386         # We don't need quote_name_unless_alias() here, since these are all\n1387         # going to be column names (so we can avoid the extra overhead).\n1388         qn = self.connection.ops.quote_name\n1389         opts = self.query.get_meta()\n1390         insert_statement = self.connection.ops.insert_statement(\n1391             on_conflict=self.query.on_conflict,\n1392         )\n1393         result = ['%s %s' % (insert_statement, qn(opts.db_table))]\n1394         fields = self.query.fields or [opts.pk]\n1395         result.append('(%s)' % ', '.join(qn(f.column) for f in fields))\n1396 \n1397         if self.query.fields:\n1398             value_rows = [\n1399                 [self.prepare_value(field, self.pre_save_val(field, obj)) for field in fields]\n1400                 for obj in self.query.objs\n1401             ]\n1402         else:\n1403             # An empty object.\n1404             value_rows = [[self.connection.ops.pk_default_value()] for _ in self.query.objs]\n1405             fields = [None]\n1406 \n1407         # Currently the backends just accept values when generating bulk\n1408         # queries and generate their own placeholders. Doing that isn't\n1409         # necessary and it should be possible to use placeholders and\n1410         # expressions in bulk inserts too.\n1411         can_bulk = (not self.returning_fields and self.connection.features.has_bulk_insert)\n1412 \n1413         placeholder_rows, param_rows = self.assemble_as_sql(fields, value_rows)\n1414 \n1415         on_conflict_suffix_sql = self.connection.ops.on_conflict_suffix_sql(\n1416             fields,\n1417             self.query.on_conflict,\n1418             self.query.update_fields,\n1419             self.query.unique_fields,\n1420         )\n1421         if self.returning_fields and self.connection.features.can_return_columns_from_insert:\n1422             if self.connection.features.can_return_rows_from_bulk_insert:\n1423                 result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n1424                 params = param_rows\n1425             else:\n1426                 result.append(\"VALUES (%s)\" % \", \".join(placeholder_rows[0]))\n1427                 params = [param_rows[0]]\n1428             if on_conflict_suffix_sql:\n1429                 result.append(on_conflict_suffix_sql)\n1430             # Skip empty r_sql to allow subclasses to customize behavior for\n1431             # 3rd party backends. Refs #19096.\n1432             r_sql, self.returning_params = self.connection.ops.return_insert_columns(self.returning_fields)\n1433             if r_sql:\n1434                 result.append(r_sql)\n1435                 params += [self.returning_params]\n1436             return [(\" \".join(result), tuple(chain.from_iterable(params)))]\n1437 \n1438         if can_bulk:\n1439             result.append(self.connection.ops.bulk_insert_sql(fields, placeholder_rows))\n1440             if on_conflict_suffix_sql:\n1441                 result.append(on_conflict_suffix_sql)\n1442             return [(\" \".join(result), tuple(p for ps in param_rows for p in ps))]\n1443         else:\n1444             if on_conflict_suffix_sql:\n1445                 result.append(on_conflict_suffix_sql)\n1446             return [\n1447                 (\" \".join(result + [\"VALUES (%s)\" % \", \".join(p)]), vals)\n1448                 for p, vals in zip(placeholder_rows, param_rows)\n1449             ]\n1450 \n1451     def execute_sql(self, returning_fields=None):\n1452         assert not (\n1453             returning_fields and len(self.query.objs) != 1 and\n1454             not self.connection.features.can_return_rows_from_bulk_insert\n1455         )\n1456         opts = self.query.get_meta()\n1457         self.returning_fields = returning_fields\n1458         with self.connection.cursor() as cursor:\n1459             for sql, params in self.as_sql():\n1460                 cursor.execute(sql, params)\n1461             if not self.returning_fields:\n1462                 return []\n1463             if self.connection.features.can_return_rows_from_bulk_insert and len(self.query.objs) > 1:\n1464                 rows = self.connection.ops.fetch_returned_insert_rows(cursor)\n1465             elif self.connection.features.can_return_columns_from_insert:\n1466                 assert len(self.query.objs) == 1\n1467                 rows = [self.connection.ops.fetch_returned_insert_columns(\n1468                     cursor, self.returning_params,\n1469                 )]\n1470             else:\n1471                 rows = [(self.connection.ops.last_insert_id(\n1472                     cursor, opts.db_table, opts.pk.column,\n1473                 ),)]\n1474         cols = [field.get_col(opts.db_table) for field in self.returning_fields]\n1475         converters = self.get_converters(cols)\n1476         if converters:\n1477             rows = list(self.apply_converters(rows, converters))\n1478         return rows\n1479 \n1480 \n1481 class SQLDeleteCompiler(SQLCompiler):\n1482     @cached_property\n1483     def single_alias(self):\n1484         # Ensure base table is in aliases.\n1485         self.query.get_initial_alias()\n1486         return sum(self.query.alias_refcount[t] > 0 for t in self.query.alias_map) == 1\n1487 \n1488     @classmethod\n1489     def _expr_refs_base_model(cls, expr, base_model):\n1490         if isinstance(expr, Query):\n1491             return expr.model == base_model\n1492         if not hasattr(expr, 'get_source_expressions'):\n1493             return False\n1494         return any(\n1495             cls._expr_refs_base_model(source_expr, base_model)\n1496             for source_expr in expr.get_source_expressions()\n1497         )\n1498 \n1499     @cached_property\n1500     def contains_self_reference_subquery(self):\n1501         return any(\n1502             self._expr_refs_base_model(expr, self.query.model)\n1503             for expr in chain(self.query.annotations.values(), self.query.where.children)\n1504         )\n1505 \n1506     def _as_sql(self, query):\n1507         result = [\n1508             'DELETE FROM %s' % self.quote_name_unless_alias(query.base_table)\n1509         ]\n1510         where, params = self.compile(query.where)\n1511         if where:\n1512             result.append('WHERE %s' % where)\n1513         return ' '.join(result), tuple(params)\n1514 \n1515     def as_sql(self):\n1516         \"\"\"\n1517         Create the SQL for this query. Return the SQL string and list of\n1518         parameters.\n1519         \"\"\"\n1520         if self.single_alias and not self.contains_self_reference_subquery:\n1521             return self._as_sql(self.query)\n1522         innerq = self.query.clone()\n1523         innerq.__class__ = Query\n1524         innerq.clear_select_clause()\n1525         pk = self.query.model._meta.pk\n1526         innerq.select = [\n1527             pk.get_col(self.query.get_initial_alias())\n1528         ]\n1529         outerq = Query(self.query.model)\n1530         if not self.connection.features.update_can_self_select:\n1531             # Force the materialization of the inner query to allow reference\n1532             # to the target table on MySQL.\n1533             sql, params = innerq.get_compiler(connection=self.connection).as_sql()\n1534             innerq = RawSQL('SELECT * FROM (%s) subquery' % sql, params)\n1535         outerq.add_filter('pk__in', innerq)\n1536         return self._as_sql(outerq)\n1537 \n1538 \n1539 class SQLUpdateCompiler(SQLCompiler):\n1540     def as_sql(self):\n1541         \"\"\"\n1542         Create the SQL for this query. Return the SQL string and list of\n1543         parameters.\n1544         \"\"\"\n1545         self.pre_sql_setup()\n1546         if not self.query.values:\n1547             return '', ()\n1548         qn = self.quote_name_unless_alias\n1549         values, update_params = [], []\n1550         for field, model, val in self.query.values:\n1551             if hasattr(val, 'resolve_expression'):\n1552                 val = val.resolve_expression(self.query, allow_joins=False, for_save=True)\n1553                 if val.contains_aggregate:\n1554                     raise FieldError(\n1555                         'Aggregate functions are not allowed in this query '\n1556                         '(%s=%r).' % (field.name, val)\n1557                     )\n1558                 if val.contains_over_clause:\n1559                     raise FieldError(\n1560                         'Window expressions are not allowed in this query '\n1561                         '(%s=%r).' % (field.name, val)\n1562                     )\n1563             elif hasattr(val, 'prepare_database_save'):\n1564                 if field.remote_field:\n1565                     val = field.get_db_prep_save(\n1566                         val.prepare_database_save(field),\n1567                         connection=self.connection,\n1568                     )\n1569                 else:\n1570                     raise TypeError(\n1571                         \"Tried to update field %s with a model instance, %r. \"\n1572                         \"Use a value compatible with %s.\"\n1573                         % (field, val, field.__class__.__name__)\n1574                     )\n1575             else:\n1576                 val = field.get_db_prep_save(val, connection=self.connection)\n1577 \n1578             # Getting the placeholder for the field.\n1579             if hasattr(field, 'get_placeholder'):\n1580                 placeholder = field.get_placeholder(val, self, self.connection)\n1581             else:\n1582                 placeholder = '%s'\n1583             name = field.column\n1584             if hasattr(val, 'as_sql'):\n1585                 sql, params = self.compile(val)\n1586                 values.append('%s = %s' % (qn(name), placeholder % sql))\n1587                 update_params.extend(params)\n1588             elif val is not None:\n1589                 values.append('%s = %s' % (qn(name), placeholder))\n1590                 update_params.append(val)\n1591             else:\n1592                 values.append('%s = NULL' % qn(name))\n1593         table = self.query.base_table\n1594         result = [\n1595             'UPDATE %s SET' % qn(table),\n1596             ', '.join(values),\n1597         ]\n1598         where, params = self.compile(self.query.where)\n1599         if where:\n1600             result.append('WHERE %s' % where)\n1601         return ' '.join(result), tuple(update_params + params)\n1602 \n1603     def execute_sql(self, result_type):\n1604         \"\"\"\n1605         Execute the specified update. Return the number of rows affected by\n1606         the primary update query. The \"primary update query\" is the first\n1607         non-empty query that is executed. Row counts for any subsequent,\n1608         related queries are not available.\n1609         \"\"\"\n1610         cursor = super().execute_sql(result_type)\n1611         try:\n1612             rows = cursor.rowcount if cursor else 0\n1613             is_empty = cursor is None\n1614         finally:\n1615             if cursor:\n1616                 cursor.close()\n1617         for query in self.query.get_related_updates():\n1618             aux_rows = query.get_compiler(self.using).execute_sql(result_type)\n1619             if is_empty and aux_rows:\n1620                 rows = aux_rows\n1621                 is_empty = False\n1622         return rows\n1623 \n1624     def pre_sql_setup(self):\n1625         \"\"\"\n1626         If the update depends on results from other tables, munge the \"where\"\n1627         conditions to match the format required for (portable) SQL updates.\n1628 \n1629         If multiple updates are required, pull out the id values to update at\n1630         this point so that they don't change as a result of the progressive\n1631         updates.\n1632         \"\"\"\n1633         refcounts_before = self.query.alias_refcount.copy()\n1634         # Ensure base table is in the query\n1635         self.query.get_initial_alias()\n1636         count = self.query.count_active_tables()\n1637         if not self.query.related_updates and count == 1:\n1638             return\n1639         query = self.query.chain(klass=Query)\n1640         query.select_related = False\n1641         query.clear_ordering(force=True)\n1642         query.extra = {}\n1643         query.select = []\n1644         query.add_fields([query.get_meta().pk.name])\n1645         super().pre_sql_setup()\n1646 \n1647         must_pre_select = count > 1 and not self.connection.features.update_can_self_select\n1648 \n1649         # Now we adjust the current query: reset the where clause and get rid\n1650         # of all the tables we don't need (since they're in the sub-select).\n1651         self.query.clear_where()\n1652         if self.query.related_updates or must_pre_select:\n1653             # Either we're using the idents in multiple update queries (so\n1654             # don't want them to change), or the db backend doesn't support\n1655             # selecting from the updating table (e.g. MySQL).\n1656             idents = []\n1657             for rows in query.get_compiler(self.using).execute_sql(MULTI):\n1658                 idents.extend(r[0] for r in rows)\n1659             self.query.add_filter('pk__in', idents)\n1660             self.query.related_ids = idents\n1661         else:\n1662             # The fast path. Filters and updates in one query.\n1663             self.query.add_filter('pk__in', query)\n1664         self.query.reset_refcounts(refcounts_before)\n1665 \n1666 \n1667 class SQLAggregateCompiler(SQLCompiler):\n1668     def as_sql(self):\n1669         \"\"\"\n1670         Create the SQL for this query. Return the SQL string and list of\n1671         parameters.\n1672         \"\"\"\n1673         sql, params = [], []\n1674         for annotation in self.query.annotation_select.values():\n1675             ann_sql, ann_params = self.compile(annotation)\n1676             ann_sql, ann_params = annotation.select_format(self, ann_sql, ann_params)\n1677             sql.append(ann_sql)\n1678             params.extend(ann_params)\n1679         self.col_count = len(self.query.annotation_select)\n1680         sql = ', '.join(sql)\n1681         params = tuple(params)\n1682 \n1683         inner_query_sql, inner_query_params = self.query.inner_query.get_compiler(\n1684             self.using, elide_empty=self.elide_empty,\n1685         ).as_sql(with_col_aliases=True)\n1686         sql = 'SELECT %s FROM (%s) subquery' % (sql, inner_query_sql)\n1687         params = params + inner_query_params\n1688         return sql, params\n1689 \n1690 \n1691 def cursor_iter(cursor, sentinel, col_count, itersize):\n1692     \"\"\"\n1693     Yield blocks of rows from a cursor and ensure the cursor is closed when\n1694     done.\n1695     \"\"\"\n1696     try:\n1697         for rows in iter((lambda: cursor.fetchmany(itersize)), sentinel):\n1698             yield rows if col_count is None else [r[:col_count] for r in rows]\n1699     finally:\n1700         cursor.close()\n",
        "intended_behavior": "This is a speculative location based on the usage of `EmptyResultSet` across the codebase, particularly in SQL compiler logic. A revision might be necessary to ensure that when compiling queries involving an `Exists` subquery that has been negated and is based on an `none()` queryset, the compiler preserves the WHERE block appropriately, potentially as a no-op or always-false condition, thereby conveying the intended semantics of the original queryset filtering criteria without erroneously excluding necessary SQL syntax."
    }
]