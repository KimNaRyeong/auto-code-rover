[
    {
        "rel_file_path": "sklearn/metrics/scorer.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/metrics/scorer.py",
        "start": 67,
        "end": 98,
        "class_name": null,
        "method_name": "__call__",
        "code": "67     def __call__(self, estimator, X, y_true, sample_weight=None):\n68         \"\"\"Evaluate predicted target values for X relative to y_true.\n69 \n70         Parameters\n71         ----------\n72         estimator : object\n73             Trained estimator to use for scoring. Must have a predict_proba\n74             method; the output of that is used to compute the score.\n75 \n76         X : array-like or sparse matrix\n77             Test data that will be fed to estimator.predict.\n78 \n79         y_true : array-like\n80             Gold standard target values for X.\n81 \n82         sample_weight : array-like, optional (default=None)\n83             Sample weights.\n84 \n85         Returns\n86         -------\n87         score : float\n88             Score function applied to prediction of estimator on X.\n89         \"\"\"\n90 \n91         y_pred = estimator.predict(X)\n92         if sample_weight is not None:\n93             return self._sign * self._score_func(y_true, y_pred,\n94                                                  sample_weight=sample_weight,\n95                                                  **self._kwargs)\n96         else:\n97             return self._sign * self._score_func(y_true, y_pred,\n98                                                  **self._kwargs)\n",
        "intended_behavior": "The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation."
    },
    {
        "rel_file_path": "sklearn/metrics/scorer.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/metrics/scorer.py",
        "start": 102,
        "end": 135,
        "class_name": null,
        "method_name": "__call__",
        "code": "102     def __call__(self, clf, X, y, sample_weight=None):\n103         \"\"\"Evaluate predicted probabilities for X relative to y_true.\n104 \n105         Parameters\n106         ----------\n107         clf : object\n108             Trained classifier to use for scoring. Must have a predict_proba\n109             method; the output of that is used to compute the score.\n110 \n111         X : array-like or sparse matrix\n112             Test data that will be fed to clf.predict_proba.\n113 \n114         y : array-like\n115             Gold standard target values for X. These must be class labels,\n116             not probabilities.\n117 \n118         sample_weight : array-like, optional (default=None)\n119             Sample weights.\n120 \n121         Returns\n122         -------\n123         score : float\n124             Score function applied to prediction of estimator on X.\n125         \"\"\"\n126         y_type = type_of_target(y)\n127         y_pred = clf.predict_proba(X)\n128         if y_type == \"binary\":\n129             y_pred = y_pred[:, 1]\n130         if sample_weight is not None:\n131             return self._sign * self._score_func(y, y_pred,\n132                                                  sample_weight=sample_weight,\n133                                                  **self._kwargs)\n134         else:\n135             return self._sign * self._score_func(y, y_pred, **self._kwargs)\n",
        "intended_behavior": "The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation."
    },
    {
        "rel_file_path": "sklearn/metrics/scorer.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/metrics/scorer.py",
        "start": 142,
        "end": 195,
        "class_name": null,
        "method_name": "__call__",
        "code": "142     def __call__(self, clf, X, y, sample_weight=None):\n143         \"\"\"Evaluate decision function output for X relative to y_true.\n144 \n145         Parameters\n146         ----------\n147         clf : object\n148             Trained classifier to use for scoring. Must have either a\n149             decision_function method or a predict_proba method; the output of\n150             that is used to compute the score.\n151 \n152         X : array-like or sparse matrix\n153             Test data that will be fed to clf.decision_function or\n154             clf.predict_proba.\n155 \n156         y : array-like\n157             Gold standard target values for X. These must be class labels,\n158             not decision function values.\n159 \n160         sample_weight : array-like, optional (default=None)\n161             Sample weights.\n162 \n163         Returns\n164         -------\n165         score : float\n166             Score function applied to prediction of estimator on X.\n167         \"\"\"\n168         y_type = type_of_target(y)\n169         if y_type not in (\"binary\", \"multilabel-indicator\"):\n170             raise ValueError(\"{0} format is not supported\".format(y_type))\n171 \n172         if is_regressor(clf):\n173             y_pred = clf.predict(X)\n174         else:\n175             try:\n176                 y_pred = clf.decision_function(X)\n177 \n178                 # For multi-output multi-class estimator\n179                 if isinstance(y_pred, list):\n180                     y_pred = np.vstack([p for p in y_pred]).T\n181 \n182             except (NotImplementedError, AttributeError):\n183                 y_pred = clf.predict_proba(X)\n184 \n185                 if y_type == \"binary\":\n186                     y_pred = y_pred[:, 1]\n187                 elif isinstance(y_pred, list):\n188                     y_pred = np.vstack([p[:, -1] for p in y_pred]).T\n189 \n190         if sample_weight is not None:\n191             return self._sign * self._score_func(y, y_pred,\n192                                                  sample_weight=sample_weight,\n193                                                  **self._kwargs)\n194         else:\n195             return self._sign * self._score_func(y, y_pred, **self._kwargs)\n",
        "intended_behavior": "The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation."
    },
    {
        "rel_file_path": "sklearn/metrics/scorer.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/metrics/scorer.py",
        "start": 67,
        "end": 98,
        "class_name": "_PredictScorer",
        "method_name": "__call__",
        "code": "67     def __call__(self, estimator, X, y_true, sample_weight=None):\n68         \"\"\"Evaluate predicted target values for X relative to y_true.\n69 \n70         Parameters\n71         ----------\n72         estimator : object\n73             Trained estimator to use for scoring. Must have a predict_proba\n74             method; the output of that is used to compute the score.\n75 \n76         X : array-like or sparse matrix\n77             Test data that will be fed to estimator.predict.\n78 \n79         y_true : array-like\n80             Gold standard target values for X.\n81 \n82         sample_weight : array-like, optional (default=None)\n83             Sample weights.\n84 \n85         Returns\n86         -------\n87         score : float\n88             Score function applied to prediction of estimator on X.\n89         \"\"\"\n90 \n91         y_pred = estimator.predict(X)\n92         if sample_weight is not None:\n93             return self._sign * self._score_func(y_true, y_pred,\n94                                                  sample_weight=sample_weight,\n95                                                  **self._kwargs)\n96         else:\n97             return self._sign * self._score_func(y_true, y_pred,\n98                                                  **self._kwargs)\n",
        "intended_behavior": "The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation."
    },
    {
        "rel_file_path": "sklearn/metrics/scorer.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/metrics/scorer.py",
        "start": 102,
        "end": 135,
        "class_name": "_ProbaScorer",
        "method_name": "__call__",
        "code": "102     def __call__(self, clf, X, y, sample_weight=None):\n103         \"\"\"Evaluate predicted probabilities for X relative to y_true.\n104 \n105         Parameters\n106         ----------\n107         clf : object\n108             Trained classifier to use for scoring. Must have a predict_proba\n109             method; the output of that is used to compute the score.\n110 \n111         X : array-like or sparse matrix\n112             Test data that will be fed to clf.predict_proba.\n113 \n114         y : array-like\n115             Gold standard target values for X. These must be class labels,\n116             not probabilities.\n117 \n118         sample_weight : array-like, optional (default=None)\n119             Sample weights.\n120 \n121         Returns\n122         -------\n123         score : float\n124             Score function applied to prediction of estimator on X.\n125         \"\"\"\n126         y_type = type_of_target(y)\n127         y_pred = clf.predict_proba(X)\n128         if y_type == \"binary\":\n129             y_pred = y_pred[:, 1]\n130         if sample_weight is not None:\n131             return self._sign * self._score_func(y, y_pred,\n132                                                  sample_weight=sample_weight,\n133                                                  **self._kwargs)\n134         else:\n135             return self._sign * self._score_func(y, y_pred, **self._kwargs)\n",
        "intended_behavior": "The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation."
    },
    {
        "rel_file_path": "sklearn/metrics/scorer.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/metrics/scorer.py",
        "start": 142,
        "end": 195,
        "class_name": "_ThresholdScorer",
        "method_name": "__call__",
        "code": "142     def __call__(self, clf, X, y, sample_weight=None):\n143         \"\"\"Evaluate decision function output for X relative to y_true.\n144 \n145         Parameters\n146         ----------\n147         clf : object\n148             Trained classifier to use for scoring. Must have either a\n149             decision_function method or a predict_proba method; the output of\n150             that is used to compute the score.\n151 \n152         X : array-like or sparse matrix\n153             Test data that will be fed to clf.decision_function or\n154             clf.predict_proba.\n155 \n156         y : array-like\n157             Gold standard target values for X. These must be class labels,\n158             not decision function values.\n159 \n160         sample_weight : array-like, optional (default=None)\n161             Sample weights.\n162 \n163         Returns\n164         -------\n165         score : float\n166             Score function applied to prediction of estimator on X.\n167         \"\"\"\n168         y_type = type_of_target(y)\n169         if y_type not in (\"binary\", \"multilabel-indicator\"):\n170             raise ValueError(\"{0} format is not supported\".format(y_type))\n171 \n172         if is_regressor(clf):\n173             y_pred = clf.predict(X)\n174         else:\n175             try:\n176                 y_pred = clf.decision_function(X)\n177 \n178                 # For multi-output multi-class estimator\n179                 if isinstance(y_pred, list):\n180                     y_pred = np.vstack([p for p in y_pred]).T\n181 \n182             except (NotImplementedError, AttributeError):\n183                 y_pred = clf.predict_proba(X)\n184 \n185                 if y_type == \"binary\":\n186                     y_pred = y_pred[:, 1]\n187                 elif isinstance(y_pred, list):\n188                     y_pred = np.vstack([p[:, -1] for p in y_pred]).T\n189 \n190         if sample_weight is not None:\n191             return self._sign * self._score_func(y, y_pred,\n192                                                  sample_weight=sample_weight,\n193                                                  **self._kwargs)\n194         else:\n195             return self._sign * self._score_func(y, y_pred, **self._kwargs)\n",
        "intended_behavior": "The intended behavior should include validation to check if the estimator is capable of producing the expected type of predictions (e.g., binary probabilities for `roc_auc` scoring). If the estimator does not naturally support the scoring method (as might be the case with `GMM` or `GaussianMixture` for `roc_auc`), a more descriptive error message should be thrown to guide the user. This validation should preserve the functionality of allowing any classifiers that can output probabilities or decision function values to be scored appropriately while preventing or handling more gracefully the cases where an unsupervised model is incorrectly used for supervised tasks. For supervised models that do not support `predict_proba` or `decision_function`, an alternative handling or suggestion in the error message should advise on the correct use or model adaptation."
    },
    {
        "rel_file_path": "sklearn/model_selection/_search.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/model_selection/_search.py",
        "start": 818,
        "end": 1130,
        "class_name": "GridSearchCV",
        "method_name": null,
        "code": "818 class GridSearchCV(BaseSearchCV):\n819     \"\"\"Exhaustive search over specified parameter values for an estimator.\n820 \n821     Important members are fit, predict.\n822 \n823     GridSearchCV implements a \"fit\" and a \"score\" method.\n824     It also implements \"predict\", \"predict_proba\", \"decision_function\",\n825     \"transform\" and \"inverse_transform\" if they are implemented in the\n826     estimator used.\n827 \n828     The parameters of the estimator used to apply these methods are optimized\n829     by cross-validated grid-search over a parameter grid.\n830 \n831     Read more in the :ref:`User Guide <grid_search>`.\n832 \n833     Parameters\n834     ----------\n835     estimator : estimator object.\n836         This is assumed to implement the scikit-learn estimator interface.\n837         Either estimator needs to provide a ``score`` function,\n838         or ``scoring`` must be passed.\n839 \n840     param_grid : dict or list of dictionaries\n841         Dictionary with parameters names (string) as keys and lists of\n842         parameter settings to try as values, or a list of such\n843         dictionaries, in which case the grids spanned by each dictionary\n844         in the list are explored. This enables searching over any sequence\n845         of parameter settings.\n846 \n847     scoring : string, callable, list/tuple, dict or None, default: None\n848         A single string (see :ref:`scoring_parameter`) or a callable\n849         (see :ref:`scoring`) to evaluate the predictions on the test set.\n850 \n851         For evaluating multiple metrics, either give a list of (unique) strings\n852         or a dict with names as keys and callables as values.\n853 \n854         NOTE that when using custom scorers, each scorer should return a single\n855         value. Metric functions returning a list/array of values can be wrapped\n856         into multiple scorers that return one value each.\n857 \n858         See :ref:`multimetric_grid_search` for an example.\n859 \n860         If None, the estimator's default scorer (if available) is used.\n861 \n862     n_jobs : int or None, optional (default=None)\n863         Number of jobs to run in parallel.\n864         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n865         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n866         for more details.\n867 \n868     pre_dispatch : int, or string, optional\n869         Controls the number of jobs that get dispatched during parallel\n870         execution. Reducing this number can be useful to avoid an\n871         explosion of memory consumption when more jobs get dispatched\n872         than CPUs can process. This parameter can be:\n873 \n874             - None, in which case all the jobs are immediately\n875               created and spawned. Use this for lightweight and\n876               fast-running jobs, to avoid delays due to on-demand\n877               spawning of the jobs\n878 \n879             - An int, giving the exact number of total jobs that are\n880               spawned\n881 \n882             - A string, giving an expression as a function of n_jobs,\n883               as in '2*n_jobs'\n884 \n885     iid : boolean, default='warn'\n886         If True, return the average score across folds, weighted by the number\n887         of samples in each test set. In this case, the data is assumed to be\n888         identically distributed across the folds, and the loss minimized is\n889         the total loss per sample, and not the mean loss across the folds. If\n890         False, return the average score across folds. Default is True, but\n891         will change to False in version 0.22, to correspond to the standard\n892         definition of cross-validation.\n893 \n894         .. versionchanged:: 0.20\n895             Parameter ``iid`` will change from True to False by default in\n896             version 0.22, and will be removed in 0.24.\n897 \n898     cv : int, cross-validation generator or an iterable, optional\n899         Determines the cross-validation splitting strategy.\n900         Possible inputs for cv are:\n901 \n902         - None, to use the default 3-fold cross validation,\n903         - integer, to specify the number of folds in a `(Stratified)KFold`,\n904         - :term:`CV splitter`,\n905         - An iterable yielding (train, test) splits as arrays of indices.\n906 \n907         For integer/None inputs, if the estimator is a classifier and ``y`` is\n908         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n909         other cases, :class:`KFold` is used.\n910 \n911         Refer :ref:`User Guide <cross_validation>` for the various\n912         cross-validation strategies that can be used here.\n913 \n914         .. versionchanged:: 0.20\n915             ``cv`` default value if None will change from 3-fold to 5-fold\n916             in v0.22.\n917 \n918     refit : boolean, or string, default=True\n919         Refit an estimator using the best found parameters on the whole\n920         dataset.\n921 \n922         For multiple metric evaluation, this needs to be a string denoting the\n923         scorer is used to find the best parameters for refitting the estimator\n924         at the end.\n925 \n926         The refitted estimator is made available at the ``best_estimator_``\n927         attribute and permits using ``predict`` directly on this\n928         ``GridSearchCV`` instance.\n929 \n930         Also for multiple metric evaluation, the attributes ``best_index_``,\n931         ``best_score_`` and ``best_params_`` will only be available if\n932         ``refit`` is set and all of them will be determined w.r.t this specific\n933         scorer.\n934 \n935         See ``scoring`` parameter to know more about multiple metric\n936         evaluation.\n937 \n938     verbose : integer\n939         Controls the verbosity: the higher, the more messages.\n940 \n941     error_score : 'raise' or numeric\n942         Value to assign to the score if an error occurs in estimator fitting.\n943         If set to 'raise', the error is raised. If a numeric value is given,\n944         FitFailedWarning is raised. This parameter does not affect the refit\n945         step, which will always raise the error. Default is 'raise' but from\n946         version 0.22 it will change to np.nan.\n947 \n948     return_train_score : boolean, default=False\n949         If ``False``, the ``cv_results_`` attribute will not include training\n950         scores.\n951         Computing training scores is used to get insights on how different\n952         parameter settings impact the overfitting/underfitting trade-off.\n953         However computing the scores on the training set can be computationally\n954         expensive and is not strictly required to select the parameters that\n955         yield the best generalization performance.\n956 \n957 \n958     Examples\n959     --------\n960     >>> from sklearn import svm, datasets\n961     >>> from sklearn.model_selection import GridSearchCV\n962     >>> iris = datasets.load_iris()\n963     >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n964     >>> svc = svm.SVC(gamma=\"scale\")\n965     >>> clf = GridSearchCV(svc, parameters, cv=5)\n966     >>> clf.fit(iris.data, iris.target)\n967     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n968     GridSearchCV(cv=5, error_score=...,\n969            estimator=SVC(C=1.0, cache_size=..., class_weight=..., coef0=...,\n970                          decision_function_shape='ovr', degree=..., gamma=...,\n971                          kernel='rbf', max_iter=-1, probability=False,\n972                          random_state=None, shrinking=True, tol=...,\n973                          verbose=False),\n974            iid=..., n_jobs=None,\n975            param_grid=..., pre_dispatch=..., refit=..., return_train_score=...,\n976            scoring=..., verbose=...)\n977     >>> sorted(clf.cv_results_.keys())\n978     ...                             # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n979     ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n980      'param_C', 'param_kernel', 'params',...\n981      'rank_test_score', 'split0_test_score',...\n982      'split2_test_score', ...\n983      'std_fit_time', 'std_score_time', 'std_test_score']\n984 \n985     Attributes\n986     ----------\n987     cv_results_ : dict of numpy (masked) ndarrays\n988         A dict with keys as column headers and values as columns, that can be\n989         imported into a pandas ``DataFrame``.\n990 \n991         For instance the below given table\n992 \n993         +------------+-----------+------------+-----------------+---+---------+\n994         |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n995         +============+===========+============+=================+===+=========+\n996         |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n997         +------------+-----------+------------+-----------------+---+---------+\n998         |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n999         +------------+-----------+------------+-----------------+---+---------+\n1000         |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n1001         +------------+-----------+------------+-----------------+---+---------+\n1002         |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n1003         +------------+-----------+------------+-----------------+---+---------+\n1004 \n1005         will be represented by a ``cv_results_`` dict of::\n1006 \n1007             {\n1008             'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n1009                                          mask = [False False False False]...)\n1010             'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n1011                                         mask = [ True  True False False]...),\n1012             'param_degree': masked_array(data = [2.0 3.0 -- --],\n1013                                          mask = [False False  True  True]...),\n1014             'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n1015             'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n1016             'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n1017             'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n1018             'rank_test_score'    : [2, 4, 3, 1],\n1019             'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n1020             'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n1021             'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n1022             'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n1023             'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n1024             'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n1025             'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n1026             'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n1027             'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n1028             }\n1029 \n1030         NOTE\n1031 \n1032         The key ``'params'`` is used to store a list of parameter\n1033         settings dicts for all the parameter candidates.\n1034 \n1035         The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n1036         ``std_score_time`` are all in seconds.\n1037 \n1038         For multi-metric evaluation, the scores for all the scorers are\n1039         available in the ``cv_results_`` dict at the keys ending with that\n1040         scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n1041         above. ('split0_test_precision', 'mean_train_precision' etc.)\n1042 \n1043     best_estimator_ : estimator or dict\n1044         Estimator that was chosen by the search, i.e. estimator\n1045         which gave highest score (or smallest loss if specified)\n1046         on the left out data. Not available if ``refit=False``.\n1047 \n1048         See ``refit`` parameter for more information on allowed values.\n1049 \n1050     best_score_ : float\n1051         Mean cross-validated score of the best_estimator\n1052 \n1053         For multi-metric evaluation, this is present only if ``refit`` is\n1054         specified.\n1055 \n1056     best_params_ : dict\n1057         Parameter setting that gave the best results on the hold out data.\n1058 \n1059         For multi-metric evaluation, this is present only if ``refit`` is\n1060         specified.\n1061 \n1062     best_index_ : int\n1063         The index (of the ``cv_results_`` arrays) which corresponds to the best\n1064         candidate parameter setting.\n1065 \n1066         The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n1067         the parameter setting for the best model, that gives the highest\n1068         mean score (``search.best_score_``).\n1069 \n1070         For multi-metric evaluation, this is present only if ``refit`` is\n1071         specified.\n1072 \n1073     scorer_ : function or a dict\n1074         Scorer function used on the held out data to choose the best\n1075         parameters for the model.\n1076 \n1077         For multi-metric evaluation, this attribute holds the validated\n1078         ``scoring`` dict which maps the scorer key to the scorer callable.\n1079 \n1080     n_splits_ : int\n1081         The number of cross-validation splits (folds/iterations).\n1082 \n1083     refit_time_ : float\n1084         Seconds used for refitting the best model on the whole dataset.\n1085 \n1086         This is present only if ``refit`` is not False.\n1087 \n1088     Notes\n1089     ------\n1090     The parameters selected are those that maximize the score of the left out\n1091     data, unless an explicit score is passed in which case it is used instead.\n1092 \n1093     If `n_jobs` was set to a value higher than one, the data is copied for each\n1094     point in the grid (and not `n_jobs` times). This is done for efficiency\n1095     reasons if individual jobs take very little time, but may raise errors if\n1096     the dataset is large and not enough memory is available.  A workaround in\n1097     this case is to set `pre_dispatch`. Then, the memory is copied only\n1098     `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n1099     n_jobs`.\n1100 \n1101     See Also\n1102     ---------\n1103     :class:`ParameterGrid`:\n1104         generates all the combinations of a hyperparameter grid.\n1105 \n1106     :func:`sklearn.model_selection.train_test_split`:\n1107         utility function to split the data into a development set usable\n1108         for fitting a GridSearchCV instance and an evaluation set for\n1109         its final evaluation.\n1110 \n1111     :func:`sklearn.metrics.make_scorer`:\n1112         Make a scorer from a performance metric or loss function.\n1113 \n1114     \"\"\"\n1115 \n1116     def __init__(self, estimator, param_grid, scoring=None,\n1117                  n_jobs=None, iid='warn', refit=True, cv='warn', verbose=0,\n1118                  pre_dispatch='2*n_jobs', error_score='raise-deprecating',\n1119                  return_train_score=False):\n1120         super(GridSearchCV, self).__init__(\n1121             estimator=estimator, scoring=scoring,\n1122             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,\n1123             pre_dispatch=pre_dispatch, error_score=error_score,\n1124             return_train_score=return_train_score)\n1125         self.param_grid = param_grid\n1126         _check_param_grid(param_grid)\n1127 \n1128     def _run_search(self, evaluate_candidates):\n1129         \"\"\"Search all candidates in param_grid\"\"\"\n1130         evaluate_candidates(ParameterGrid(self.param_grid))\n",
        "intended_behavior": "Given that the issue also stems from a misunderstanding or misuse of `GridSearchCV` with a non-typical classifier (`GMM`), it's crucial that `GridSearchCV` performs a check on the estimator to ensure it is suitable for the type of scoring being requested (`roc_auc`). This could involve verifying that the estimator adheres to the interface required for the specified scoring strategy. The behavior should ensure that users are immediately notified when there is a mismatch between the estimator's capabilities and the requirements of the scoring method, without altering the existing functionality of `GridSearchCV` for compatible model and scorer combinations. This might involve extending the existing validation within `GridSearchCV` to more comprehensively cover the capabilities of the provided estimator relative to the scoring method utilized."
    }
]