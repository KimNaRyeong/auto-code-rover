2025-10-17 22:59:18.502 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-17 22:59:22.405 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=147, output_tokens=12, cost=0.001830
2025-10-17 22:59:22.406 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-17 22:59:22.410 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 22:59:31.009 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=456, output_tokens=371, cost=0.015690
2025-10-17 22:59:31.018 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 22:59:33.165 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=725, output_tokens=34, cost=0.008270
2025-10-17 22:59:33.166 | DEBUG    | Extracted a valid json.
2025-10-17 22:59:33.166 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"filter\")",
        "search_code(\".filter(\")"
    ],
    "bug_locations": []
}
2025-10-17 22:59:33.356 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-17 22:59:33.357 | DEBUG    | <Agent search> Analyzing search results.
2025-10-17 22:59:48.511 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1798, output_tokens=749, cost=0.040450
2025-10-17 22:59:48.519 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-17 22:59:53.585 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2763, output_tokens=175, cost=0.032880
2025-10-17 22:59:53.593 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-17 22:59:58.692 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=529, output_tokens=181, cost=0.010720
2025-10-17 22:59:58.693 | DEBUG    | Extracted a valid json.
2025-10-17 22:59:58.699 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "filter",
            "intended_behavior": "This method needs to be enhanced to support parsing and interpreting expressions, such as `F('some_field').lower() == 'anssi'`, as valid filtering criteria. The modification must extend the method's capabilities to interpret these expressions correctly and convert them into the corresponding SQL where clauses without breaking the existing functionality for simple keyword arguments. The enhancement should enable third-party apps to create query syntax extensions, promoting a more unified and powerful ORM experience. This will likely involve implementing additional parsing logic to distinguish between simple keyword arguments and more complex expression arguments, as well as interfacing with Django's underlying query construction logic to accurately translate these expressions into SQL."
        }
    ]
}
2025-10-17 22:59:58.710 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>filter</method>
<code>
888     def filter(self, *args, **kwargs):
889         """
890         Return a new QuerySet instance with the args ANDed to the existing
891         set.
892         """
893         self._not_support_combined_queries('filter')
894         return self._filter_or_exclude(False, *args, **kwargs)

</code><intended_behavior>This method needs to be enhanced to support parsing and interpreting expressions, such as `F('some_field').lower() == 'anssi'`, as valid filtering criteria. The modification must extend the method's capabilities to interpret these expressions correctly and convert them into the corresponding SQL where clauses without breaking the existing functionality for simple keyword arguments. The enhancement should enable third-party apps to create query syntax extensions, promoting a more unified and powerful ORM experience. This will likely involve implementing additional parsing logic to distinguish between simple keyword arguments and more complex expression arguments, as well as interfacing with Django's underlying query construction logic to accurately translate these expressions into SQL.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
188 class QuerySet:
189     """Represent a lazy database lookup for a set of objects."""
190 
191     def __init__(self, model=None, query=None, using=None, hints=None):
192         self.model = model
193         self._db = using
194         self._hints = hints or {}
195         self.query = query or sql.Query(self.model)
196         self._result_cache = None
197         self._sticky_filter = False
198         self._for_write = False
199         self._prefetch_related_lookups = ()
200         self._prefetch_done = False
201         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
202         self._iterable_class = ModelIterable
203         self._fields = None
204 
205     def as_manager(cls):
206         # Address the circular dependency between `Queryset` and `Manager`.
207         from django.db.models.manager import Manager
208         manager = Manager.from_queryset(cls)()
209         manager._built_with_as_manager = True
210         return manager
211     as_manager.queryset_only = True
212     as_manager = classmethod(as_manager)
213 
214     ########################
215     # PYTHON MAGIC METHODS #
216     ########################
217 
218     def __deepcopy__(self, memo):
219         """Don't populate the QuerySet's cache."""
220         obj = self.__class__()
221         for k, v in self.__dict__.items():
222             if k == '_result_cache':
223                 obj.__dict__[k] = None
224             else:
225                 obj.__dict__[k] = copy.deepcopy(v, memo)
226         return obj
227 
228     def __getstate__(self):
229         # Force the cache to be fully populated.
230         self._fetch_all()
231         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
232 
233     def __setstate__(self, state):
234         msg = None
235         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
236         if pickled_version:
237             current_version = get_version()
238             if current_version != pickled_version:
239                 msg = (
240                     "Pickled queryset instance's Django version %s does not "
241                     "match the current version %s." % (pickled_version, current_version)
242                 )
243         else:
244             msg = "Pickled queryset instance's Django version is not specified."
245 
246         if msg:
247             warnings.warn(msg, RuntimeWarning, stacklevel=2)
248 
249         self.__dict__.update(state)
250 
251     def __repr__(self):
252         data = list(self[:REPR_OUTPUT_SIZE + 1])
253         if len(data) > REPR_OUTPUT_SIZE:
254             data[-1] = "...(remaining elements truncated)..."
255         return '<%s %r>' % (self.__class__.__name__, data)
256 
257     def __len__(self):
258         self._fetch_all()
259         return len(self._result_cache)
260 
261     def __iter__(self):
262         """
263         The queryset iterator protocol uses three nested iterators in the
264         default case:
265             1. sql.compiler.execute_sql()
266                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
267                  using cursor.fetchmany(). This part is responsible for
268                  doing some column masking, and returning the rows in chunks.
269             2. sql.compiler.results_iter()
270                - Returns one row at time. At this point the rows are still just
271                  tuples. In some cases the return values are converted to
272                  Python values at this location.
273             3. self.iterator()
274                - Responsible for turning the rows into model objects.
275         """
276         self._fetch_all()
277         return iter(self._result_cache)
278 
279     def __bool__(self):
280         self._fetch_all()
281         return bool(self._result_cache)
282 
283     def __getitem__(self, k):
284         """Retrieve an item or slice from the set of results."""
285         if not isinstance(k, (int, slice)):
286             raise TypeError(
287                 'QuerySet indices must be integers or slices, not %s.'
288                 % type(k).__name__
289             )
290         assert ((not isinstance(k, slice) and (k >= 0)) or
291                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
292                  (k.stop is None or k.stop >= 0))), \
293             "Negative indexing is not supported."
294 
295         if self._result_cache is not None:
296             return self._result_cache[k]
297 
298         if isinstance(k, slice):
299             qs = self._chain()
300             if k.start is not None:
301                 start = int(k.start)
302             else:
303                 start = None
304             if k.stop is not None:
305                 stop = int(k.stop)
306             else:
307                 stop = None
308             qs.query.set_limits(start, stop)
309             return list(qs)[::k.step] if k.step else qs
310 
311         qs = self._chain()
312         qs.query.set_limits(k, k + 1)
313         qs._fetch_all()
314         return qs._result_cache[0]
315 
316     def __and__(self, other):
317         self._merge_sanity_check(other)
318         if isinstance(other, EmptyQuerySet):
319             return other
320         if isinstance(self, EmptyQuerySet):
321             return self
322         combined = self._chain()
323         combined._merge_known_related_objects(other)
324         combined.query.combine(other.query, sql.AND)
325         return combined
326 
327     def __or__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(self, EmptyQuerySet):
330             return other
331         if isinstance(other, EmptyQuerySet):
332             return self
333         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
334         combined = query._chain()
335         combined._merge_known_related_objects(other)
336         if not other.query.can_filter():
337             other = other.model._base_manager.filter(pk__in=other.values('pk'))
338         combined.query.combine(other.query, sql.OR)
339         return combined
340 
341     ####################################
342     # METHODS THAT DO DATABASE QUERIES #
343     ####################################
344 
345     def _iterator(self, use_chunked_fetch, chunk_size):
346         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
347 
348     def iterator(self, chunk_size=2000):
349         """
350         An iterator over the results from applying this QuerySet to the
351         database.
352         """
353         if chunk_size <= 0:
354             raise ValueError('Chunk size must be strictly positive.')
355         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
356         return self._iterator(use_chunked_fetch, chunk_size)
357 
358     def aggregate(self, *args, **kwargs):
359         """
360         Return a dictionary containing the calculations (aggregation)
361         over the current queryset.
362 
363         If args is present the expression is passed as a kwarg using
364         the Aggregate object's default alias.
365         """
366         if self.query.distinct_fields:
367             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
368         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
369         for arg in args:
370             # The default_alias property raises TypeError if default_alias
371             # can't be set automatically or AttributeError if it isn't an
372             # attribute.
373             try:
374                 arg.default_alias
375             except (AttributeError, TypeError):
376                 raise TypeError("Complex aggregates require an alias")
377             kwargs[arg.default_alias] = arg
378 
379         query = self.query.chain()
380         for (alias, aggregate_expr) in kwargs.items():
381             query.add_annotation(aggregate_expr, alias, is_summary=True)
382             if not query.annotations[alias].contains_aggregate:
383                 raise TypeError("%s is not an aggregate expression" % alias)
384         return query.get_aggregation(self.db, kwargs)
385 
386     def count(self):
387         """
388         Perform a SELECT COUNT() and return the number of records as an
389         integer.
390 
391         If the QuerySet is already fully cached, return the length of the
392         cached results set to avoid multiple SELECT COUNT(*) calls.
393         """
394         if self._result_cache is not None:
395             return len(self._result_cache)
396 
397         return self.query.get_count(using=self.db)
398 
399     def get(self, *args, **kwargs):
400         """
401         Perform the query and return a single object matching the given
402         keyword arguments.
403         """
404         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
405         if self.query.can_filter() and not self.query.distinct_fields:
406             clone = clone.order_by()
407         limit = None
408         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
409             limit = MAX_GET_RESULTS
410             clone.query.set_limits(high=limit)
411         num = len(clone)
412         if num == 1:
413             return clone._result_cache[0]
414         if not num:
415             raise self.model.DoesNotExist(
416                 "%s matching query does not exist." %
417                 self.model._meta.object_name
418             )
419         raise self.model.MultipleObjectsReturned(
420             'get() returned more than one %s -- it returned %s!' % (
421                 self.model._meta.object_name,
422                 num if not limit or num < limit else 'more than %s' % (limit - 1),
423             )
424         )
425 
426     def create(self, **kwargs):
427         """
428         Create a new object with the given kwargs, saving it to the database
429         and returning the created object.
430         """
431         obj = self.model(**kwargs)
432         self._for_write = True
433         obj.save(force_insert=True, using=self.db)
434         return obj
435 
436     def _populate_pk_values(self, objs):
437         for obj in objs:
438             if obj.pk is None:
439                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
440 
441     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
442         """
443         Insert each of the instances into the database. Do *not* call
444         save() on each of the instances, do not send any pre/post_save
445         signals, and do not set the primary key attribute if it is an
446         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
447         Multi-table models are not supported.
448         """
449         # When you bulk insert you don't get the primary keys back (if it's an
450         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
451         # you can't insert into the child tables which references this. There
452         # are two workarounds:
453         # 1) This could be implemented if you didn't have an autoincrement pk
454         # 2) You could do it by doing O(n) normal inserts into the parent
455         #    tables to get the primary keys back and then doing a single bulk
456         #    insert into the childmost table.
457         # We currently set the primary keys on the objects when using
458         # PostgreSQL via the RETURNING ID clause. It should be possible for
459         # Oracle as well, but the semantics for extracting the primary keys is
460         # trickier so it's not done yet.
461         assert batch_size is None or batch_size > 0
462         # Check that the parents share the same concrete model with the our
463         # model to detect the inheritance pattern ConcreteGrandParent ->
464         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
465         # would not identify that case as involving multiple tables.
466         for parent in self.model._meta.get_parent_list():
467             if parent._meta.concrete_model is not self.model._meta.concrete_model:
468                 raise ValueError("Can't bulk create a multi-table inherited model")
469         if not objs:
470             return objs
471         self._for_write = True
472         connection = connections[self.db]
473         fields = self.model._meta.concrete_fields
474         objs = list(objs)
475         self._populate_pk_values(objs)
476         with transaction.atomic(using=self.db, savepoint=False):
477             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
478             if objs_with_pk:
479                 self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
480                 for obj_with_pk in objs_with_pk:
481                     obj_with_pk._state.adding = False
482                     obj_with_pk._state.db = self.db
483             if objs_without_pk:
484                 fields = [f for f in fields if not isinstance(f, AutoField)]
485                 ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
486                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
487                     assert len(ids) == len(objs_without_pk)
488                 for obj_without_pk, pk in zip(objs_without_pk, ids):
489                     obj_without_pk.pk = pk
490                     obj_without_pk._state.adding = False
491                     obj_without_pk._state.db = self.db
492 
493         return objs
494 
495     def bulk_update(self, objs, fields, batch_size=None):
496         """
497         Update the given fields in each of the given objects in the database.
498         """
499         if batch_size is not None and batch_size < 0:
500             raise ValueError('Batch size must be a positive integer.')
501         if not fields:
502             raise ValueError('Field names must be given to bulk_update().')
503         objs = tuple(objs)
504         if any(obj.pk is None for obj in objs):
505             raise ValueError('All bulk_update() objects must have a primary key set.')
506         fields = [self.model._meta.get_field(name) for name in fields]
507         if any(not f.concrete or f.many_to_many for f in fields):
508             raise ValueError('bulk_update() can only be used with concrete fields.')
509         if any(f.primary_key for f in fields):
510             raise ValueError('bulk_update() cannot be used with primary key fields.')
511         if not objs:
512             return
513         # PK is used twice in the resulting update query, once in the filter
514         # and once in the WHEN. Each field will also have one CAST.
515         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
516         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
517         requires_casting = connections[self.db].features.requires_casted_case_in_updates
518         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
519         updates = []
520         for batch_objs in batches:
521             update_kwargs = {}
522             for field in fields:
523                 when_statements = []
524                 for obj in batch_objs:
525                     attr = getattr(obj, field.attname)
526                     if not isinstance(attr, Expression):
527                         attr = Value(attr, output_field=field)
528                     when_statements.append(When(pk=obj.pk, then=attr))
529                 case_statement = Case(*when_statements, output_field=field)
530                 if requires_casting:
531                     case_statement = Cast(case_statement, output_field=field)
532                 update_kwargs[field.attname] = case_statement
533             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
534         with transaction.atomic(using=self.db, savepoint=False):
535             for pks, update_kwargs in updates:
536                 self.filter(pk__in=pks).update(**update_kwargs)
537     bulk_update.alters_data = True
538 
539     def get_or_create(self, defaults=None, **kwargs):
540         """
541         Look up an object with the given kwargs, creating one if necessary.
542         Return a tuple of (object, created), where created is a boolean
543         specifying whether an object was created.
544         """
545         # The get() needs to be targeted at the write database in order
546         # to avoid potential transaction consistency problems.
547         self._for_write = True
548         try:
549             return self.get(**kwargs), False
550         except self.model.DoesNotExist:
551             params = self._extract_model_params(defaults, **kwargs)
552             return self._create_object_from_params(kwargs, params)
553 
554     def update_or_create(self, defaults=None, **kwargs):
555         """
556         Look up an object with the given kwargs, updating one with defaults
557         if it exists, otherwise create a new one.
558         Return a tuple (object, created), where created is a boolean
559         specifying whether an object was created.
560         """
561         defaults = defaults or {}
562         self._for_write = True
563         with transaction.atomic(using=self.db):
564             try:
565                 obj = self.select_for_update().get(**kwargs)
566             except self.model.DoesNotExist:
567                 params = self._extract_model_params(defaults, **kwargs)
568                 # Lock the row so that a concurrent update is blocked until
569                 # after update_or_create() has performed its save.
570                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
571                 if created:
572                     return obj, created
573             for k, v in defaults.items():
574                 setattr(obj, k, v() if callable(v) else v)
575             obj.save(using=self.db)
576         return obj, False
577 
578     def _create_object_from_params(self, lookup, params, lock=False):
579         """
580         Try to create an object using passed params. Used by get_or_create()
581         and update_or_create().
582         """
583         try:
584             with transaction.atomic(using=self.db):
585                 params = {k: v() if callable(v) else v for k, v in params.items()}
586                 obj = self.create(**params)
587             return obj, True
588         except IntegrityError as e:
589             try:
590                 qs = self.select_for_update() if lock else self
591                 return qs.get(**lookup), False
592             except self.model.DoesNotExist:
593                 pass
594             raise e
595 
596     def _extract_model_params(self, defaults, **kwargs):
597         """
598         Prepare `params` for creating a model instance based on the given
599         kwargs; for use by get_or_create() and update_or_create().
600         """
601         defaults = defaults or {}
602         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
603         params.update(defaults)
604         property_names = self.model._meta._property_names
605         invalid_params = []
606         for param in params:
607             try:
608                 self.model._meta.get_field(param)
609             except exceptions.FieldDoesNotExist:
610                 # It's okay to use a model's property if it has a setter.
611                 if not (param in property_names and getattr(self.model, param).fset):
612                     invalid_params.append(param)
613         if invalid_params:
614             raise exceptions.FieldError(
615                 "Invalid field name(s) for model %s: '%s'." % (
616                     self.model._meta.object_name,
617                     "', '".join(sorted(invalid_params)),
618                 ))
619         return params
620 
621     def _earliest(self, *fields):
622         """
623         Return the earliest object according to fields (if given) or by the
624         model's Meta.get_latest_by.
625         """
626         if fields:
627             order_by = fields
628         else:
629             order_by = getattr(self.model._meta, 'get_latest_by')
630             if order_by and not isinstance(order_by, (tuple, list)):
631                 order_by = (order_by,)
632         if order_by is None:
633             raise ValueError(
634                 "earliest() and latest() require either fields as positional "
635                 "arguments or 'get_latest_by' in the model's Meta."
636             )
637 
638         assert not self.query.is_sliced, \
639             "Cannot change a query once a slice has been taken."
640         obj = self._chain()
641         obj.query.set_limits(high=1)
642         obj.query.clear_ordering(force_empty=True)
643         obj.query.add_ordering(*order_by)
644         return obj.get()
645 
646     def earliest(self, *fields):
647         return self._earliest(*fields)
648 
649     def latest(self, *fields):
650         return self.reverse()._earliest(*fields)
651 
652     def first(self):
653         """Return the first object of a query or None if no match is found."""
654         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
655             return obj
656 
657     def last(self):
658         """Return the last object of a query or None if no match is found."""
659         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
660             return obj
661 
662     def in_bulk(self, id_list=None, *, field_name='pk'):
663         """
664         Return a dictionary mapping each of the given IDs to the object with
665         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
666         """
667         assert not self.query.is_sliced, \
668             "Cannot use 'limit' or 'offset' with in_bulk"
669         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
670             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
671         if id_list is not None:
672             if not id_list:
673                 return {}
674             filter_key = '{}__in'.format(field_name)
675             batch_size = connections[self.db].features.max_query_params
676             id_list = tuple(id_list)
677             # If the database has a limit on the number of query parameters
678             # (e.g. SQLite), retrieve objects in batches if necessary.
679             if batch_size and batch_size < len(id_list):
680                 qs = ()
681                 for offset in range(0, len(id_list), batch_size):
682                     batch = id_list[offset:offset + batch_size]
683                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
684             else:
685                 qs = self.filter(**{filter_key: id_list}).order_by()
686         else:
687             qs = self._chain()
688         return {getattr(obj, field_name): obj for obj in qs}
689 
690     def delete(self):
691         """Delete the records in the current QuerySet."""
692         assert not self.query.is_sliced, \
693             "Cannot use 'limit' or 'offset' with delete."
694 
695         if self._fields is not None:
696             raise TypeError("Cannot call delete() after .values() or .values_list()")
697 
698         del_query = self._chain()
699 
700         # The delete is actually 2 queries - one to find related objects,
701         # and one to delete. Make sure that the discovery of related
702         # objects is performed on the same database as the deletion.
703         del_query._for_write = True
704 
705         # Disable non-supported fields.
706         del_query.query.select_for_update = False
707         del_query.query.select_related = False
708         del_query.query.clear_ordering(force_empty=True)
709 
710         collector = Collector(using=del_query.db)
711         collector.collect(del_query)
712         deleted, _rows_count = collector.delete()
713 
714         # Clear the result cache, in case this QuerySet gets reused.
715         self._result_cache = None
716         return deleted, _rows_count
717 
718     delete.alters_data = True
719     delete.queryset_only = True
720 
721     def _raw_delete(self, using):
722         """
723         Delete objects found from the given queryset in single direct SQL
724         query. No signals are sent and there is no protection for cascades.
725         """
726         return sql.DeleteQuery(self.model).delete_qs(self, using)
727     _raw_delete.alters_data = True
728 
729     def update(self, **kwargs):
730         """
731         Update all elements in the current QuerySet, setting all the given
732         fields to the appropriate values.
733         """
734         assert not self.query.is_sliced, \
735             "Cannot update a query once a slice has been taken."
736         self._for_write = True
737         query = self.query.chain(sql.UpdateQuery)
738         query.add_update_values(kwargs)
739         # Clear any annotations so that they won't be present in subqueries.
740         query.annotations = {}
741         with transaction.mark_for_rollback_on_error(using=self.db):
742             rows = query.get_compiler(self.db).execute_sql(CURSOR)
743         self._result_cache = None
744         return rows
745     update.alters_data = True
746 
747     def _update(self, values):
748         """
749         A version of update() that accepts field objects instead of field names.
750         Used primarily for model saving and not intended for use by general
751         code (it requires too much poking around at model internals to be
752         useful at that level).
753         """
754         assert not self.query.is_sliced, \
755             "Cannot update a query once a slice has been taken."
756         query = self.query.chain(sql.UpdateQuery)
757         query.add_update_fields(values)
758         # Clear any annotations so that they won't be present in subqueries.
759         query.annotations = {}
760         self._result_cache = None
761         return query.get_compiler(self.db).execute_sql(CURSOR)
762     _update.alters_data = True
763     _update.queryset_only = False
764 
765     def exists(self):
766         if self._result_cache is None:
767             return self.query.has_results(using=self.db)
768         return bool(self._result_cache)
769 
770     def _prefetch_related_objects(self):
771         # This method can only be called once the result cache has been filled.
772         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
773         self._prefetch_done = True
774 
775     def explain(self, *, format=None, **options):
776         return self.query.explain(using=self.db, format=format, **options)
777 
778     ##################################################
779     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
780     ##################################################
781 
782     def raw(self, raw_query, params=None, translations=None, using=None):
783         if using is None:
784             using = self.db
785         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
786         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
787         return qs
788 
789     def _values(self, *fields, **expressions):
790         clone = self._chain()
791         if expressions:
792             clone = clone.annotate(**expressions)
793         clone._fields = fields
794         clone.query.set_values(fields)
795         return clone
796 
797     def values(self, *fields, **expressions):
798         fields += tuple(expressions)
799         clone = self._values(*fields, **expressions)
800         clone._iterable_class = ValuesIterable
801         return clone
802 
803     def values_list(self, *fields, flat=False, named=False):
804         if flat and named:
805             raise TypeError("'flat' and 'named' can't be used together.")
806         if flat and len(fields) > 1:
807             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
808 
809         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
810         _fields = []
811         expressions = {}
812         counter = 1
813         for field in fields:
814             if hasattr(field, 'resolve_expression'):
815                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
816                 while True:
817                     field_id = field_id_prefix + str(counter)
818                     counter += 1
819                     if field_id not in field_names:
820                         break
821                 expressions[field_id] = field
822                 _fields.append(field_id)
823             else:
824                 _fields.append(field)
825 
826         clone = self._values(*_fields, **expressions)
827         clone._iterable_class = (
828             NamedValuesListIterable if named
829             else FlatValuesListIterable if flat
830             else ValuesListIterable
831         )
832         return clone
833 
834     def dates(self, field_name, kind, order='ASC'):
835         """
836         Return a list of date objects representing all available dates for
837         the given field_name, scoped to 'kind'.
838         """
839         assert kind in ('year', 'month', 'week', 'day'), \
840             "'kind' must be one of 'year', 'month', 'week', or 'day'."
841         assert order in ('ASC', 'DESC'), \
842             "'order' must be either 'ASC' or 'DESC'."
843         return self.annotate(
844             datefield=Trunc(field_name, kind, output_field=DateField()),
845             plain_field=F(field_name)
846         ).values_list(
847             'datefield', flat=True
848         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
849 
850     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
851         """
852         Return a list of datetime objects representing all available
853         datetimes for the given field_name, scoped to 'kind'.
854         """
855         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
856             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
857         assert order in ('ASC', 'DESC'), \
858             "'order' must be either 'ASC' or 'DESC'."
859         if settings.USE_TZ:
860             if tzinfo is None:
861                 tzinfo = timezone.get_current_timezone()
862         else:
863             tzinfo = None
864         return self.annotate(
865             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
866             plain_field=F(field_name)
867         ).values_list(
868             'datetimefield', flat=True
869         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
870 
871     def none(self):
872         """Return an empty QuerySet."""
873         clone = self._chain()
874         clone.query.set_empty()
875         return clone
876 
877     ##################################################################
878     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
879     ##################################################################
880 
881     def all(self):
882         """
883         Return a new QuerySet that is a copy of the current one. This allows a
884         QuerySet to proxy for a model manager in some cases.
885         """
886         return self._chain()
887 
888     def filter(self, *args, **kwargs):
889         """
890         Return a new QuerySet instance with the args ANDed to the existing
891         set.
892         """
893         self._not_support_combined_queries('filter')
894         return self._filter_or_exclude(False, *args, **kwargs)
895 
896     def exclude(self, *args, **kwargs):
897         """
898         Return a new QuerySet instance with NOT (args) ANDed to the existing
899         set.
900         """
901         self._not_support_combined_queries('exclude')
902         return self._filter_or_exclude(True, *args, **kwargs)
903 
904     def _filter_or_exclude(self, negate, *args, **kwargs):
905         if args or kwargs:
906             assert not self.query.is_sliced, \
907                 "Cannot filter a query once a slice has been taken."
908 
909         clone = self._chain()
910         if negate:
911             clone.query.add_q(~Q(*args, **kwargs))
912         else:
913             clone.query.add_q(Q(*args, **kwargs))
914         return clone
915 
916     def complex_filter(self, filter_obj):
917         """
918         Return a new QuerySet instance with filter_obj added to the filters.
919 
920         filter_obj can be a Q object or a dictionary of keyword lookup
921         arguments.
922 
923         This exists to support framework features such as 'limit_choices_to',
924         and usually it will be more natural to use other methods.
925         """
926         if isinstance(filter_obj, Q):
927             clone = self._chain()
928             clone.query.add_q(filter_obj)
929             return clone
930         else:
931             return self._filter_or_exclude(None, **filter_obj)
932 
933     def _combinator_query(self, combinator, *other_qs, all=False):
934         # Clone the query to inherit the select list and everything
935         clone = self._chain()
936         # Clear limits and ordering so they can be reapplied
937         clone.query.clear_ordering(True)
938         clone.query.clear_limits()
939         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
940         clone.query.combinator = combinator
941         clone.query.combinator_all = all
942         return clone
943 
944     def union(self, *other_qs, all=False):
945         # If the query is an EmptyQuerySet, combine all nonempty querysets.
946         if isinstance(self, EmptyQuerySet):
947             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
948             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
949         return self._combinator_query('union', *other_qs, all=all)
950 
951     def intersection(self, *other_qs):
952         # If any query is an EmptyQuerySet, return it.
953         if isinstance(self, EmptyQuerySet):
954             return self
955         for other in other_qs:
956             if isinstance(other, EmptyQuerySet):
957                 return other
958         return self._combinator_query('intersection', *other_qs)
959 
960     def difference(self, *other_qs):
961         # If the query is an EmptyQuerySet, return it.
962         if isinstance(self, EmptyQuerySet):
963             return self
964         return self._combinator_query('difference', *other_qs)
965 
966     def select_for_update(self, nowait=False, skip_locked=False, of=()):
967         """
968         Return a new QuerySet instance that will select objects with a
969         FOR UPDATE lock.
970         """
971         if nowait and skip_locked:
972             raise ValueError('The nowait option cannot be used with skip_locked.')
973         obj = self._chain()
974         obj._for_write = True
975         obj.query.select_for_update = True
976         obj.query.select_for_update_nowait = nowait
977         obj.query.select_for_update_skip_locked = skip_locked
978         obj.query.select_for_update_of = of
979         return obj
980 
981     def select_related(self, *fields):
982         """
983         Return a new QuerySet instance that will select related objects.
984 
985         If fields are specified, they must be ForeignKey fields and only those
986         related objects are included in the selection.
987 
988         If select_related(None) is called, clear the list.
989         """
990         self._not_support_combined_queries('select_related')
991         if self._fields is not None:
992             raise TypeError("Cannot call select_related() after .values() or .values_list()")
993 
994         obj = self._chain()
995         if fields == (None,):
996             obj.query.select_related = False
997         elif fields:
998             obj.query.add_select_related(fields)
999         else:
1000             obj.query.select_related = True
1001         return obj
1002 
1003     def prefetch_related(self, *lookups):
1004         """
1005         Return a new QuerySet instance that will prefetch the specified
1006         Many-To-One and Many-To-Many related objects when the QuerySet is
1007         evaluated.
1008 
1009         When prefetch_related() is called more than once, append to the list of
1010         prefetch lookups. If prefetch_related(None) is called, clear the list.
1011         """
1012         self._not_support_combined_queries('prefetch_related')
1013         clone = self._chain()
1014         if lookups == (None,):
1015             clone._prefetch_related_lookups = ()
1016         else:
1017             for lookup in lookups:
1018                 if isinstance(lookup, Prefetch):
1019                     lookup = lookup.prefetch_to
1020                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1021                 if lookup in self.query._filtered_relations:
1022                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1023             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1024         return clone
1025 
1026     def annotate(self, *args, **kwargs):
1027         """
1028         Return a query set in which the returned objects have been annotated
1029         with extra data or aggregations.
1030         """
1031         self._not_support_combined_queries('annotate')
1032         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1033         annotations = {}
1034         for arg in args:
1035             # The default_alias property may raise a TypeError.
1036             try:
1037                 if arg.default_alias in kwargs:
1038                     raise ValueError("The named annotation '%s' conflicts with the "
1039                                      "default name for another annotation."
1040                                      % arg.default_alias)
1041             except TypeError:
1042                 raise TypeError("Complex annotations require an alias")
1043             annotations[arg.default_alias] = arg
1044         annotations.update(kwargs)
1045 
1046         clone = self._chain()
1047         names = self._fields
1048         if names is None:
1049             names = set(chain.from_iterable(
1050                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1051                 for field in self.model._meta.get_fields()
1052             ))
1053 
1054         for alias, annotation in annotations.items():
1055             if alias in names:
1056                 raise ValueError("The annotation '%s' conflicts with a field on "
1057                                  "the model." % alias)
1058             if isinstance(annotation, FilteredRelation):
1059                 clone.query.add_filtered_relation(annotation, alias)
1060             else:
1061                 clone.query.add_annotation(annotation, alias, is_summary=False)
1062 
1063         for alias, annotation in clone.query.annotations.items():
1064             if alias in annotations and annotation.contains_aggregate:
1065                 if clone._fields is None:
1066                     clone.query.group_by = True
1067                 else:
1068                     clone.query.set_group_by()
1069                 break
1070 
1071         return clone
1072 
1073     def order_by(self, *field_names):
1074         """Return a new QuerySet instance with the ordering changed."""
1075         assert not self.query.is_sliced, \
1076             "Cannot reorder a query once a slice has been taken."
1077         obj = self._chain()
1078         obj.query.clear_ordering(force_empty=False)
1079         obj.query.add_ordering(*field_names)
1080         return obj
1081 
1082     def distinct(self, *field_names):
1083         """
1084         Return a new QuerySet instance that will select only distinct results.
1085         """
1086         assert not self.query.is_sliced, \
1087             "Cannot create distinct fields once a slice has been taken."
1088         obj = self._chain()
1089         obj.query.add_distinct_fields(*field_names)
1090         return obj
1091 
1092     def extra(self, select=None, where=None, params=None, tables=None,
1093               order_by=None, select_params=None):
1094         """Add extra SQL fragments to the query."""
1095         self._not_support_combined_queries('extra')
1096         assert not self.query.is_sliced, \
1097             "Cannot change a query once a slice has been taken"
1098         clone = self._chain()
1099         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1100         return clone
1101 
1102     def reverse(self):
1103         """Reverse the ordering of the QuerySet."""
1104         if self.query.is_sliced:
1105             raise TypeError('Cannot reverse a query once a slice has been taken.')
1106         clone = self._chain()
1107         clone.query.standard_ordering = not clone.query.standard_ordering
1108         return clone
1109 
1110     def defer(self, *fields):
1111         """
1112         Defer the loading of data for certain fields until they are accessed.
1113         Add the set of deferred fields to any existing set of deferred fields.
1114         The only exception to this is if None is passed in as the only
1115         parameter, in which case removal all deferrals.
1116         """
1117         self._not_support_combined_queries('defer')
1118         if self._fields is not None:
1119             raise TypeError("Cannot call defer() after .values() or .values_list()")
1120         clone = self._chain()
1121         if fields == (None,):
1122             clone.query.clear_deferred_loading()
1123         else:
1124             clone.query.add_deferred_loading(fields)
1125         return clone
1126 
1127     def only(self, *fields):
1128         """
1129         Essentially, the opposite of defer(). Only the fields passed into this
1130         method and that are not already specified as deferred are loaded
1131         immediately when the queryset is evaluated.
1132         """
1133         self._not_support_combined_queries('only')
1134         if self._fields is not None:
1135             raise TypeError("Cannot call only() after .values() or .values_list()")
1136         if fields == (None,):
1137             # Can only pass None to defer(), not only(), as the rest option.
1138             # That won't stop people trying to do this, so let's be explicit.
1139             raise TypeError("Cannot pass None as an argument to only().")
1140         for field in fields:
1141             field = field.split(LOOKUP_SEP, 1)[0]
1142             if field in self.query._filtered_relations:
1143                 raise ValueError('only() is not supported with FilteredRelation.')
1144         clone = self._chain()
1145         clone.query.add_immediate_loading(fields)
1146         return clone
1147 
1148     def using(self, alias):
1149         """Select which database this QuerySet should execute against."""
1150         clone = self._chain()
1151         clone._db = alias
1152         return clone
1153 
1154     ###################################
1155     # PUBLIC INTROSPECTION ATTRIBUTES #
1156     ###################################
1157 
1158     @property
1159     def ordered(self):
1160         """
1161         Return True if the QuerySet is ordered -- i.e. has an order_by()
1162         clause or a default ordering on the model (or is empty).
1163         """
1164         if isinstance(self, EmptyQuerySet):
1165             return True
1166         if self.query.extra_order_by or self.query.order_by:
1167             return True
1168         elif self.query.default_ordering and self.query.get_meta().ordering:
1169             return True
1170         else:
1171             return False
1172 
1173     @property
1174     def db(self):
1175         """Return the database used if this query is executed now."""
1176         if self._for_write:
1177             return self._db or router.db_for_write(self.model, **self._hints)
1178         return self._db or router.db_for_read(self.model, **self._hints)
1179 
1180     ###################
1181     # PRIVATE METHODS #
1182     ###################
1183 
1184     def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):
1185         """
1186         Insert a new record for the given model. This provides an interface to
1187         the InsertQuery class and is how Model.save() is implemented.
1188         """
1189         self._for_write = True
1190         if using is None:
1191             using = self.db
1192         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1193         query.insert_values(fields, objs, raw=raw)
1194         return query.get_compiler(using=using).execute_sql(return_id)
1195     _insert.alters_data = True
1196     _insert.queryset_only = False
1197 
1198     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1199         """
1200         Helper method for bulk_create() to insert objs one batch at a time.
1201         """
1202         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1203             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1204         ops = connections[self.db].ops
1205         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1206         inserted_ids = []
1207         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1208         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1209             if bulk_return and not ignore_conflicts:
1210                 inserted_id = self._insert(
1211                     item, fields=fields, using=self.db, return_id=True,
1212                     ignore_conflicts=ignore_conflicts,
1213                 )
1214                 if isinstance(inserted_id, list):
1215                     inserted_ids.extend(inserted_id)
1216                 else:
1217                     inserted_ids.append(inserted_id)
1218             else:
1219                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1220         return inserted_ids
1221 
1222     def _chain(self, **kwargs):
1223         """
1224         Return a copy of the current QuerySet that's ready for another
1225         operation.
1226         """
1227         obj = self._clone()
1228         if obj._sticky_filter:
1229             obj.query.filter_is_sticky = True
1230             obj._sticky_filter = False
1231         obj.__dict__.update(kwargs)
1232         return obj
1233 
1234     def _clone(self):
1235         """
1236         Return a copy of the current QuerySet. A lightweight alternative
1237         to deepcopy().
1238         """
1239         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1240         c._sticky_filter = self._sticky_filter
1241         c._for_write = self._for_write
1242         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1243         c._known_related_objects = self._known_related_objects
1244         c._iterable_class = self._iterable_class
1245         c._fields = self._fields
1246         return c
1247 
1248     def _fetch_all(self):
1249         if self._result_cache is None:
1250             self._result_cache = list(self._iterable_class(self))
1251         if self._prefetch_related_lookups and not self._prefetch_done:
1252             self._prefetch_related_objects()
1253 
1254     def _next_is_sticky(self):
1255         """
1256         Indicate that the next filter call and the one following that should
1257         be treated as a single filter. This is only important when it comes to
1258         determining when to reuse tables for many-to-many filters. Required so
1259         that we can filter naturally on the results of related managers.
1260 
1261         This doesn't return a clone of the current QuerySet (it returns
1262         "self"). The method is only used internally and should be immediately
1263         followed by a filter() that does create a clone.
1264         """
1265         self._sticky_filter = True
1266         return self
1267 
1268     def _merge_sanity_check(self, other):
1269         """Check that two QuerySet classes may be merged."""
1270         if self._fields is not None and (
1271                 set(self.query.values_select) != set(other.query.values_select) or
1272                 set(self.query.extra_select) != set(other.query.extra_select) or
1273                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1274             raise TypeError(
1275                 "Merging '%s' classes must involve the same values in each case."
1276                 % self.__class__.__name__
1277             )
1278 
1279     def _merge_known_related_objects(self, other):
1280         """
1281         Keep track of all known related objects from either QuerySet instance.
1282         """
1283         for field, objects in other._known_related_objects.items():
1284             self._known_related_objects.setdefault(field, {}).update(objects)
1285 
1286     def resolve_expression(self, *args, **kwargs):
1287         if self._fields and len(self._fields) > 1:
1288             # values() queryset can only be used as nested queries
1289             # if they are set up to select only a single field.
1290             raise TypeError('Cannot use multi-field values as a filter value.')
1291         query = self.query.resolve_expression(*args, **kwargs)
1292         query._db = self._db
1293         return query
1294     resolve_expression.queryset_only = True
1295 
1296     def _add_hints(self, **hints):
1297         """
1298         Update hinting information for use by routers. Add new key/values or
1299         overwrite existing key/values.
1300         """
1301         self._hints.update(hints)
1302 
1303     def _has_filters(self):
1304         """
1305         Check if this QuerySet has any filtering going on. This isn't
1306         equivalent with checking if all objects are present in results, for
1307         example, qs[1:]._has_filters() -> False.
1308         """
1309         return self.query.has_filters()
1310 
1311     @staticmethod
1312     def _validate_values_are_expressions(values, method_name):
1313         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1314         if invalid_args:
1315             raise TypeError(
1316                 'QuerySet.%s() received non-expression(s): %s.' % (
1317                     method_name,
1318                     ', '.join(invalid_args),
1319                 )
1320             )
1321 
1322     def _not_support_combined_queries(self, operation_name):
1323         if self.query.combinator:
1324             raise NotSupportedError(
1325                 'Calling QuerySet.%s() after %s() is not supported.'
1326                 % (operation_name, self.query.combinator)
1327             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 22:59:58.711 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>filter</method>
<code>
888     def filter(self, *args, **kwargs):
889         """
890         Return a new QuerySet instance with the args ANDed to the existing
891         set.
892         """
893         self._not_support_combined_queries('filter')
894         return self._filter_or_exclude(False, *args, **kwargs)

</code><intended_behavior>This method needs to be enhanced to support parsing and interpreting expressions, such as `F('some_field').lower() == 'anssi'`, as valid filtering criteria. The modification must extend the method's capabilities to interpret these expressions correctly and convert them into the corresponding SQL where clauses without breaking the existing functionality for simple keyword arguments. The enhancement should enable third-party apps to create query syntax extensions, promoting a more unified and powerful ORM experience. This will likely involve implementing additional parsing logic to distinguish between simple keyword arguments and more complex expression arguments, as well as interfacing with Django's underlying query construction logic to accurately translate these expressions into SQL.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
188 class QuerySet:
189     """Represent a lazy database lookup for a set of objects."""
190 
191     def __init__(self, model=None, query=None, using=None, hints=None):
192         self.model = model
193         self._db = using
194         self._hints = hints or {}
195         self.query = query or sql.Query(self.model)
196         self._result_cache = None
197         self._sticky_filter = False
198         self._for_write = False
199         self._prefetch_related_lookups = ()
200         self._prefetch_done = False
201         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
202         self._iterable_class = ModelIterable
203         self._fields = None
204 
205     def as_manager(cls):
206         # Address the circular dependency between `Queryset` and `Manager`.
207         from django.db.models.manager import Manager
208         manager = Manager.from_queryset(cls)()
209         manager._built_with_as_manager = True
210         return manager
211     as_manager.queryset_only = True
212     as_manager = classmethod(as_manager)
213 
214     ########################
215     # PYTHON MAGIC METHODS #
216     ########################
217 
218     def __deepcopy__(self, memo):
219         """Don't populate the QuerySet's cache."""
220         obj = self.__class__()
221         for k, v in self.__dict__.items():
222             if k == '_result_cache':
223                 obj.__dict__[k] = None
224             else:
225                 obj.__dict__[k] = copy.deepcopy(v, memo)
226         return obj
227 
228     def __getstate__(self):
229         # Force the cache to be fully populated.
230         self._fetch_all()
231         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
232 
233     def __setstate__(self, state):
234         msg = None
235         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
236         if pickled_version:
237             current_version = get_version()
238             if current_version != pickled_version:
239                 msg = (
240                     "Pickled queryset instance's Django version %s does not "
241                     "match the current version %s." % (pickled_version, current_version)
242                 )
243         else:
244             msg = "Pickled queryset instance's Django version is not specified."
245 
246         if msg:
247             warnings.warn(msg, RuntimeWarning, stacklevel=2)
248 
249         self.__dict__.update(state)
250 
251     def __repr__(self):
252         data = list(self[:REPR_OUTPUT_SIZE + 1])
253         if len(data) > REPR_OUTPUT_SIZE:
254             data[-1] = "...(remaining elements truncated)..."
255         return '<%s %r>' % (self.__class__.__name__, data)
256 
257     def __len__(self):
258         self._fetch_all()
259         return len(self._result_cache)
260 
261     def __iter__(self):
262         """
263         The queryset iterator protocol uses three nested iterators in the
264         default case:
265             1. sql.compiler.execute_sql()
266                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
267                  using cursor.fetchmany(). This part is responsible for
268                  doing some column masking, and returning the rows in chunks.
269             2. sql.compiler.results_iter()
270                - Returns one row at time. At this point the rows are still just
271                  tuples. In some cases the return values are converted to
272                  Python values at this location.
273             3. self.iterator()
274                - Responsible for turning the rows into model objects.
275         """
276         self._fetch_all()
277         return iter(self._result_cache)
278 
279     def __bool__(self):
280         self._fetch_all()
281         return bool(self._result_cache)
282 
283     def __getitem__(self, k):
284         """Retrieve an item or slice from the set of results."""
285         if not isinstance(k, (int, slice)):
286             raise TypeError(
287                 'QuerySet indices must be integers or slices, not %s.'
288                 % type(k).__name__
289             )
290         assert ((not isinstance(k, slice) and (k >= 0)) or
291                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
292                  (k.stop is None or k.stop >= 0))), \
293             "Negative indexing is not supported."
294 
295         if self._result_cache is not None:
296             return self._result_cache[k]
297 
298         if isinstance(k, slice):
299             qs = self._chain()
300             if k.start is not None:
301                 start = int(k.start)
302             else:
303                 start = None
304             if k.stop is not None:
305                 stop = int(k.stop)
306             else:
307                 stop = None
308             qs.query.set_limits(start, stop)
309             return list(qs)[::k.step] if k.step else qs
310 
311         qs = self._chain()
312         qs.query.set_limits(k, k + 1)
313         qs._fetch_all()
314         return qs._result_cache[0]
315 
316     def __and__(self, other):
317         self._merge_sanity_check(other)
318         if isinstance(other, EmptyQuerySet):
319             return other
320         if isinstance(self, EmptyQuerySet):
321             return self
322         combined = self._chain()
323         combined._merge_known_related_objects(other)
324         combined.query.combine(other.query, sql.AND)
325         return combined
326 
327     def __or__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(self, EmptyQuerySet):
330             return other
331         if isinstance(other, EmptyQuerySet):
332             return self
333         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
334         combined = query._chain()
335         combined._merge_known_related_objects(other)
336         if not other.query.can_filter():
337             other = other.model._base_manager.filter(pk__in=other.values('pk'))
338         combined.query.combine(other.query, sql.OR)
339         return combined
340 
341     ####################################
342     # METHODS THAT DO DATABASE QUERIES #
343     ####################################
344 
345     def _iterator(self, use_chunked_fetch, chunk_size):
346         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
347 
348     def iterator(self, chunk_size=2000):
349         """
350         An iterator over the results from applying this QuerySet to the
351         database.
352         """
353         if chunk_size <= 0:
354             raise ValueError('Chunk size must be strictly positive.')
355         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
356         return self._iterator(use_chunked_fetch, chunk_size)
357 
358     def aggregate(self, *args, **kwargs):
359         """
360         Return a dictionary containing the calculations (aggregation)
361         over the current queryset.
362 
363         If args is present the expression is passed as a kwarg using
364         the Aggregate object's default alias.
365         """
366         if self.query.distinct_fields:
367             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
368         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
369         for arg in args:
370             # The default_alias property raises TypeError if default_alias
371             # can't be set automatically or AttributeError if it isn't an
372             # attribute.
373             try:
374                 arg.default_alias
375             except (AttributeError, TypeError):
376                 raise TypeError("Complex aggregates require an alias")
377             kwargs[arg.default_alias] = arg
378 
379         query = self.query.chain()
380         for (alias, aggregate_expr) in kwargs.items():
381             query.add_annotation(aggregate_expr, alias, is_summary=True)
382             if not query.annotations[alias].contains_aggregate:
383                 raise TypeError("%s is not an aggregate expression" % alias)
384         return query.get_aggregation(self.db, kwargs)
385 
386     def count(self):
387         """
388         Perform a SELECT COUNT() and return the number of records as an
389         integer.
390 
391         If the QuerySet is already fully cached, return the length of the
392         cached results set to avoid multiple SELECT COUNT(*) calls.
393         """
394         if self._result_cache is not None:
395             return len(self._result_cache)
396 
397         return self.query.get_count(using=self.db)
398 
399     def get(self, *args, **kwargs):
400         """
401         Perform the query and return a single object matching the given
402         keyword arguments.
403         """
404         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
405         if self.query.can_filter() and not self.query.distinct_fields:
406             clone = clone.order_by()
407         limit = None
408         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
409             limit = MAX_GET_RESULTS
410             clone.query.set_limits(high=limit)
411         num = len(clone)
412         if num == 1:
413             return clone._result_cache[0]
414         if not num:
415             raise self.model.DoesNotExist(
416                 "%s matching query does not exist." %
417                 self.model._meta.object_name
418             )
419         raise self.model.MultipleObjectsReturned(
420             'get() returned more than one %s -- it returned %s!' % (
421                 self.model._meta.object_name,
422                 num if not limit or num < limit else 'more than %s' % (limit - 1),
423             )
424         )
425 
426     def create(self, **kwargs):
427         """
428         Create a new object with the given kwargs, saving it to the database
429         and returning the created object.
430         """
431         obj = self.model(**kwargs)
432         self._for_write = True
433         obj.save(force_insert=True, using=self.db)
434         return obj
435 
436     def _populate_pk_values(self, objs):
437         for obj in objs:
438             if obj.pk is None:
439                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
440 
441     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
442         """
443         Insert each of the instances into the database. Do *not* call
444         save() on each of the instances, do not send any pre/post_save
445         signals, and do not set the primary key attribute if it is an
446         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
447         Multi-table models are not supported.
448         """
449         # When you bulk insert you don't get the primary keys back (if it's an
450         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
451         # you can't insert into the child tables which references this. There
452         # are two workarounds:
453         # 1) This could be implemented if you didn't have an autoincrement pk
454         # 2) You could do it by doing O(n) normal inserts into the parent
455         #    tables to get the primary keys back and then doing a single bulk
456         #    insert into the childmost table.
457         # We currently set the primary keys on the objects when using
458         # PostgreSQL via the RETURNING ID clause. It should be possible for
459         # Oracle as well, but the semantics for extracting the primary keys is
460         # trickier so it's not done yet.
461         assert batch_size is None or batch_size > 0
462         # Check that the parents share the same concrete model with the our
463         # model to detect the inheritance pattern ConcreteGrandParent ->
464         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
465         # would not identify that case as involving multiple tables.
466         for parent in self.model._meta.get_parent_list():
467             if parent._meta.concrete_model is not self.model._meta.concrete_model:
468                 raise ValueError("Can't bulk create a multi-table inherited model")
469         if not objs:
470             return objs
471         self._for_write = True
472         connection = connections[self.db]
473         fields = self.model._meta.concrete_fields
474         objs = list(objs)
475         self._populate_pk_values(objs)
476         with transaction.atomic(using=self.db, savepoint=False):
477             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
478             if objs_with_pk:
479                 self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
480                 for obj_with_pk in objs_with_pk:
481                     obj_with_pk._state.adding = False
482                     obj_with_pk._state.db = self.db
483             if objs_without_pk:
484                 fields = [f for f in fields if not isinstance(f, AutoField)]
485                 ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)
486                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
487                     assert len(ids) == len(objs_without_pk)
488                 for obj_without_pk, pk in zip(objs_without_pk, ids):
489                     obj_without_pk.pk = pk
490                     obj_without_pk._state.adding = False
491                     obj_without_pk._state.db = self.db
492 
493         return objs
494 
495     def bulk_update(self, objs, fields, batch_size=None):
496         """
497         Update the given fields in each of the given objects in the database.
498         """
499         if batch_size is not None and batch_size < 0:
500             raise ValueError('Batch size must be a positive integer.')
501         if not fields:
502             raise ValueError('Field names must be given to bulk_update().')
503         objs = tuple(objs)
504         if any(obj.pk is None for obj in objs):
505             raise ValueError('All bulk_update() objects must have a primary key set.')
506         fields = [self.model._meta.get_field(name) for name in fields]
507         if any(not f.concrete or f.many_to_many for f in fields):
508             raise ValueError('bulk_update() can only be used with concrete fields.')
509         if any(f.primary_key for f in fields):
510             raise ValueError('bulk_update() cannot be used with primary key fields.')
511         if not objs:
512             return
513         # PK is used twice in the resulting update query, once in the filter
514         # and once in the WHEN. Each field will also have one CAST.
515         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
516         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
517         requires_casting = connections[self.db].features.requires_casted_case_in_updates
518         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
519         updates = []
520         for batch_objs in batches:
521             update_kwargs = {}
522             for field in fields:
523                 when_statements = []
524                 for obj in batch_objs:
525                     attr = getattr(obj, field.attname)
526                     if not isinstance(attr, Expression):
527                         attr = Value(attr, output_field=field)
528                     when_statements.append(When(pk=obj.pk, then=attr))
529                 case_statement = Case(*when_statements, output_field=field)
530                 if requires_casting:
531                     case_statement = Cast(case_statement, output_field=field)
532                 update_kwargs[field.attname] = case_statement
533             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
534         with transaction.atomic(using=self.db, savepoint=False):
535             for pks, update_kwargs in updates:
536                 self.filter(pk__in=pks).update(**update_kwargs)
537     bulk_update.alters_data = True
538 
539     def get_or_create(self, defaults=None, **kwargs):
540         """
541         Look up an object with the given kwargs, creating one if necessary.
542         Return a tuple of (object, created), where created is a boolean
543         specifying whether an object was created.
544         """
545         # The get() needs to be targeted at the write database in order
546         # to avoid potential transaction consistency problems.
547         self._for_write = True
548         try:
549             return self.get(**kwargs), False
550         except self.model.DoesNotExist:
551             params = self._extract_model_params(defaults, **kwargs)
552             return self._create_object_from_params(kwargs, params)
553 
554     def update_or_create(self, defaults=None, **kwargs):
555         """
556         Look up an object with the given kwargs, updating one with defaults
557         if it exists, otherwise create a new one.
558         Return a tuple (object, created), where created is a boolean
559         specifying whether an object was created.
560         """
561         defaults = defaults or {}
562         self._for_write = True
563         with transaction.atomic(using=self.db):
564             try:
565                 obj = self.select_for_update().get(**kwargs)
566             except self.model.DoesNotExist:
567                 params = self._extract_model_params(defaults, **kwargs)
568                 # Lock the row so that a concurrent update is blocked until
569                 # after update_or_create() has performed its save.
570                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
571                 if created:
572                     return obj, created
573             for k, v in defaults.items():
574                 setattr(obj, k, v() if callable(v) else v)
575             obj.save(using=self.db)
576         return obj, False
577 
578     def _create_object_from_params(self, lookup, params, lock=False):
579         """
580         Try to create an object using passed params. Used by get_or_create()
581         and update_or_create().
582         """
583         try:
584             with transaction.atomic(using=self.db):
585                 params = {k: v() if callable(v) else v for k, v in params.items()}
586                 obj = self.create(**params)
587             return obj, True
588         except IntegrityError as e:
589             try:
590                 qs = self.select_for_update() if lock else self
591                 return qs.get(**lookup), False
592             except self.model.DoesNotExist:
593                 pass
594             raise e
595 
596     def _extract_model_params(self, defaults, **kwargs):
597         """
598         Prepare `params` for creating a model instance based on the given
599         kwargs; for use by get_or_create() and update_or_create().
600         """
601         defaults = defaults or {}
602         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
603         params.update(defaults)
604         property_names = self.model._meta._property_names
605         invalid_params = []
606         for param in params:
607             try:
608                 self.model._meta.get_field(param)
609             except exceptions.FieldDoesNotExist:
610                 # It's okay to use a model's property if it has a setter.
611                 if not (param in property_names and getattr(self.model, param).fset):
612                     invalid_params.append(param)
613         if invalid_params:
614             raise exceptions.FieldError(
615                 "Invalid field name(s) for model %s: '%s'." % (
616                     self.model._meta.object_name,
617                     "', '".join(sorted(invalid_params)),
618                 ))
619         return params
620 
621     def _earliest(self, *fields):
622         """
623         Return the earliest object according to fields (if given) or by the
624         model's Meta.get_latest_by.
625         """
626         if fields:
627             order_by = fields
628         else:
629             order_by = getattr(self.model._meta, 'get_latest_by')
630             if order_by and not isinstance(order_by, (tuple, list)):
631                 order_by = (order_by,)
632         if order_by is None:
633             raise ValueError(
634                 "earliest() and latest() require either fields as positional "
635                 "arguments or 'get_latest_by' in the model's Meta."
636             )
637 
638         assert not self.query.is_sliced, \
639             "Cannot change a query once a slice has been taken."
640         obj = self._chain()
641         obj.query.set_limits(high=1)
642         obj.query.clear_ordering(force_empty=True)
643         obj.query.add_ordering(*order_by)
644         return obj.get()
645 
646     def earliest(self, *fields):
647         return self._earliest(*fields)
648 
649     def latest(self, *fields):
650         return self.reverse()._earliest(*fields)
651 
652     def first(self):
653         """Return the first object of a query or None if no match is found."""
654         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
655             return obj
656 
657     def last(self):
658         """Return the last object of a query or None if no match is found."""
659         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
660             return obj
661 
662     def in_bulk(self, id_list=None, *, field_name='pk'):
663         """
664         Return a dictionary mapping each of the given IDs to the object with
665         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
666         """
667         assert not self.query.is_sliced, \
668             "Cannot use 'limit' or 'offset' with in_bulk"
669         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
670             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
671         if id_list is not None:
672             if not id_list:
673                 return {}
674             filter_key = '{}__in'.format(field_name)
675             batch_size = connections[self.db].features.max_query_params
676             id_list = tuple(id_list)
677             # If the database has a limit on the number of query parameters
678             # (e.g. SQLite), retrieve objects in batches if necessary.
679             if batch_size and batch_size < len(id_list):
680                 qs = ()
681                 for offset in range(0, len(id_list), batch_size):
682                     batch = id_list[offset:offset + batch_size]
683                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
684             else:
685                 qs = self.filter(**{filter_key: id_list}).order_by()
686         else:
687             qs = self._chain()
688         return {getattr(obj, field_name): obj for obj in qs}
689 
690     def delete(self):
691         """Delete the records in the current QuerySet."""
692         assert not self.query.is_sliced, \
693             "Cannot use 'limit' or 'offset' with delete."
694 
695         if self._fields is not None:
696             raise TypeError("Cannot call delete() after .values() or .values_list()")
697 
698         del_query = self._chain()
699 
700         # The delete is actually 2 queries - one to find related objects,
701         # and one to delete. Make sure that the discovery of related
702         # objects is performed on the same database as the deletion.
703         del_query._for_write = True
704 
705         # Disable non-supported fields.
706         del_query.query.select_for_update = False
707         del_query.query.select_related = False
708         del_query.query.clear_ordering(force_empty=True)
709 
710         collector = Collector(using=del_query.db)
711         collector.collect(del_query)
712         deleted, _rows_count = collector.delete()
713 
714         # Clear the result cache, in case this QuerySet gets reused.
715         self._result_cache = None
716         return deleted, _rows_count
717 
718     delete.alters_data = True
719     delete.queryset_only = True
720 
721     def _raw_delete(self, using):
722         """
723         Delete objects found from the given queryset in single direct SQL
724         query. No signals are sent and there is no protection for cascades.
725         """
726         return sql.DeleteQuery(self.model).delete_qs(self, using)
727     _raw_delete.alters_data = True
728 
729     def update(self, **kwargs):
730         """
731         Update all elements in the current QuerySet, setting all the given
732         fields to the appropriate values.
733         """
734         assert not self.query.is_sliced, \
735             "Cannot update a query once a slice has been taken."
736         self._for_write = True
737         query = self.query.chain(sql.UpdateQuery)
738         query.add_update_values(kwargs)
739         # Clear any annotations so that they won't be present in subqueries.
740         query.annotations = {}
741         with transaction.mark_for_rollback_on_error(using=self.db):
742             rows = query.get_compiler(self.db).execute_sql(CURSOR)
743         self._result_cache = None
744         return rows
745     update.alters_data = True
746 
747     def _update(self, values):
748         """
749         A version of update() that accepts field objects instead of field names.
750         Used primarily for model saving and not intended for use by general
751         code (it requires too much poking around at model internals to be
752         useful at that level).
753         """
754         assert not self.query.is_sliced, \
755             "Cannot update a query once a slice has been taken."
756         query = self.query.chain(sql.UpdateQuery)
757         query.add_update_fields(values)
758         # Clear any annotations so that they won't be present in subqueries.
759         query.annotations = {}
760         self._result_cache = None
761         return query.get_compiler(self.db).execute_sql(CURSOR)
762     _update.alters_data = True
763     _update.queryset_only = False
764 
765     def exists(self):
766         if self._result_cache is None:
767             return self.query.has_results(using=self.db)
768         return bool(self._result_cache)
769 
770     def _prefetch_related_objects(self):
771         # This method can only be called once the result cache has been filled.
772         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
773         self._prefetch_done = True
774 
775     def explain(self, *, format=None, **options):
776         return self.query.explain(using=self.db, format=format, **options)
777 
778     ##################################################
779     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
780     ##################################################
781 
782     def raw(self, raw_query, params=None, translations=None, using=None):
783         if using is None:
784             using = self.db
785         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
786         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
787         return qs
788 
789     def _values(self, *fields, **expressions):
790         clone = self._chain()
791         if expressions:
792             clone = clone.annotate(**expressions)
793         clone._fields = fields
794         clone.query.set_values(fields)
795         return clone
796 
797     def values(self, *fields, **expressions):
798         fields += tuple(expressions)
799         clone = self._values(*fields, **expressions)
800         clone._iterable_class = ValuesIterable
801         return clone
802 
803     def values_list(self, *fields, flat=False, named=False):
804         if flat and named:
805             raise TypeError("'flat' and 'named' can't be used together.")
806         if flat and len(fields) > 1:
807             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
808 
809         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
810         _fields = []
811         expressions = {}
812         counter = 1
813         for field in fields:
814             if hasattr(field, 'resolve_expression'):
815                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
816                 while True:
817                     field_id = field_id_prefix + str(counter)
818                     counter += 1
819                     if field_id not in field_names:
820                         break
821                 expressions[field_id] = field
822                 _fields.append(field_id)
823             else:
824                 _fields.append(field)
825 
826         clone = self._values(*_fields, **expressions)
827         clone._iterable_class = (
828             NamedValuesListIterable if named
829             else FlatValuesListIterable if flat
830             else ValuesListIterable
831         )
832         return clone
833 
834     def dates(self, field_name, kind, order='ASC'):
835         """
836         Return a list of date objects representing all available dates for
837         the given field_name, scoped to 'kind'.
838         """
839         assert kind in ('year', 'month', 'week', 'day'), \
840             "'kind' must be one of 'year', 'month', 'week', or 'day'."
841         assert order in ('ASC', 'DESC'), \
842             "'order' must be either 'ASC' or 'DESC'."
843         return self.annotate(
844             datefield=Trunc(field_name, kind, output_field=DateField()),
845             plain_field=F(field_name)
846         ).values_list(
847             'datefield', flat=True
848         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
849 
850     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
851         """
852         Return a list of datetime objects representing all available
853         datetimes for the given field_name, scoped to 'kind'.
854         """
855         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
856             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
857         assert order in ('ASC', 'DESC'), \
858             "'order' must be either 'ASC' or 'DESC'."
859         if settings.USE_TZ:
860             if tzinfo is None:
861                 tzinfo = timezone.get_current_timezone()
862         else:
863             tzinfo = None
864         return self.annotate(
865             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
866             plain_field=F(field_name)
867         ).values_list(
868             'datetimefield', flat=True
869         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
870 
871     def none(self):
872         """Return an empty QuerySet."""
873         clone = self._chain()
874         clone.query.set_empty()
875         return clone
876 
877     ##################################################################
878     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
879     ##################################################################
880 
881     def all(self):
882         """
883         Return a new QuerySet that is a copy of the current one. This allows a
884         QuerySet to proxy for a model manager in some cases.
885         """
886         return self._chain()
887 
888     def filter(self, *args, **kwargs):
889         """
890         Return a new QuerySet instance with the args ANDed to the existing
891         set.
892         """
893         self._not_support_combined_queries('filter')
894         return self._filter_or_exclude(False, *args, **kwargs)
895 
896     def exclude(self, *args, **kwargs):
897         """
898         Return a new QuerySet instance with NOT (args) ANDed to the existing
899         set.
900         """
901         self._not_support_combined_queries('exclude')
902         return self._filter_or_exclude(True, *args, **kwargs)
903 
904     def _filter_or_exclude(self, negate, *args, **kwargs):
905         if args or kwargs:
906             assert not self.query.is_sliced, \
907                 "Cannot filter a query once a slice has been taken."
908 
909         clone = self._chain()
910         if negate:
911             clone.query.add_q(~Q(*args, **kwargs))
912         else:
913             clone.query.add_q(Q(*args, **kwargs))
914         return clone
915 
916     def complex_filter(self, filter_obj):
917         """
918         Return a new QuerySet instance with filter_obj added to the filters.
919 
920         filter_obj can be a Q object or a dictionary of keyword lookup
921         arguments.
922 
923         This exists to support framework features such as 'limit_choices_to',
924         and usually it will be more natural to use other methods.
925         """
926         if isinstance(filter_obj, Q):
927             clone = self._chain()
928             clone.query.add_q(filter_obj)
929             return clone
930         else:
931             return self._filter_or_exclude(None, **filter_obj)
932 
933     def _combinator_query(self, combinator, *other_qs, all=False):
934         # Clone the query to inherit the select list and everything
935         clone = self._chain()
936         # Clear limits and ordering so they can be reapplied
937         clone.query.clear_ordering(True)
938         clone.query.clear_limits()
939         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
940         clone.query.combinator = combinator
941         clone.query.combinator_all = all
942         return clone
943 
944     def union(self, *other_qs, all=False):
945         # If the query is an EmptyQuerySet, combine all nonempty querysets.
946         if isinstance(self, EmptyQuerySet):
947             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
948             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
949         return self._combinator_query('union', *other_qs, all=all)
950 
951     def intersection(self, *other_qs):
952         # If any query is an EmptyQuerySet, return it.
953         if isinstance(self, EmptyQuerySet):
954             return self
955         for other in other_qs:
956             if isinstance(other, EmptyQuerySet):
957                 return other
958         return self._combinator_query('intersection', *other_qs)
959 
960     def difference(self, *other_qs):
961         # If the query is an EmptyQuerySet, return it.
962         if isinstance(self, EmptyQuerySet):
963             return self
964         return self._combinator_query('difference', *other_qs)
965 
966     def select_for_update(self, nowait=False, skip_locked=False, of=()):
967         """
968         Return a new QuerySet instance that will select objects with a
969         FOR UPDATE lock.
970         """
971         if nowait and skip_locked:
972             raise ValueError('The nowait option cannot be used with skip_locked.')
973         obj = self._chain()
974         obj._for_write = True
975         obj.query.select_for_update = True
976         obj.query.select_for_update_nowait = nowait
977         obj.query.select_for_update_skip_locked = skip_locked
978         obj.query.select_for_update_of = of
979         return obj
980 
981     def select_related(self, *fields):
982         """
983         Return a new QuerySet instance that will select related objects.
984 
985         If fields are specified, they must be ForeignKey fields and only those
986         related objects are included in the selection.
987 
988         If select_related(None) is called, clear the list.
989         """
990         self._not_support_combined_queries('select_related')
991         if self._fields is not None:
992             raise TypeError("Cannot call select_related() after .values() or .values_list()")
993 
994         obj = self._chain()
995         if fields == (None,):
996             obj.query.select_related = False
997         elif fields:
998             obj.query.add_select_related(fields)
999         else:
1000             obj.query.select_related = True
1001         return obj
1002 
1003     def prefetch_related(self, *lookups):
1004         """
1005         Return a new QuerySet instance that will prefetch the specified
1006         Many-To-One and Many-To-Many related objects when the QuerySet is
1007         evaluated.
1008 
1009         When prefetch_related() is called more than once, append to the list of
1010         prefetch lookups. If prefetch_related(None) is called, clear the list.
1011         """
1012         self._not_support_combined_queries('prefetch_related')
1013         clone = self._chain()
1014         if lookups == (None,):
1015             clone._prefetch_related_lookups = ()
1016         else:
1017             for lookup in lookups:
1018                 if isinstance(lookup, Prefetch):
1019                     lookup = lookup.prefetch_to
1020                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1021                 if lookup in self.query._filtered_relations:
1022                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1023             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1024         return clone
1025 
1026     def annotate(self, *args, **kwargs):
1027         """
1028         Return a query set in which the returned objects have been annotated
1029         with extra data or aggregations.
1030         """
1031         self._not_support_combined_queries('annotate')
1032         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1033         annotations = {}
1034         for arg in args:
1035             # The default_alias property may raise a TypeError.
1036             try:
1037                 if arg.default_alias in kwargs:
1038                     raise ValueError("The named annotation '%s' conflicts with the "
1039                                      "default name for another annotation."
1040                                      % arg.default_alias)
1041             except TypeError:
1042                 raise TypeError("Complex annotations require an alias")
1043             annotations[arg.default_alias] = arg
1044         annotations.update(kwargs)
1045 
1046         clone = self._chain()
1047         names = self._fields
1048         if names is None:
1049             names = set(chain.from_iterable(
1050                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1051                 for field in self.model._meta.get_fields()
1052             ))
1053 
1054         for alias, annotation in annotations.items():
1055             if alias in names:
1056                 raise ValueError("The annotation '%s' conflicts with a field on "
1057                                  "the model." % alias)
1058             if isinstance(annotation, FilteredRelation):
1059                 clone.query.add_filtered_relation(annotation, alias)
1060             else:
1061                 clone.query.add_annotation(annotation, alias, is_summary=False)
1062 
1063         for alias, annotation in clone.query.annotations.items():
1064             if alias in annotations and annotation.contains_aggregate:
1065                 if clone._fields is None:
1066                     clone.query.group_by = True
1067                 else:
1068                     clone.query.set_group_by()
1069                 break
1070 
1071         return clone
1072 
1073     def order_by(self, *field_names):
1074         """Return a new QuerySet instance with the ordering changed."""
1075         assert not self.query.is_sliced, \
1076             "Cannot reorder a query once a slice has been taken."
1077         obj = self._chain()
1078         obj.query.clear_ordering(force_empty=False)
1079         obj.query.add_ordering(*field_names)
1080         return obj
1081 
1082     def distinct(self, *field_names):
1083         """
1084         Return a new QuerySet instance that will select only distinct results.
1085         """
1086         assert not self.query.is_sliced, \
1087             "Cannot create distinct fields once a slice has been taken."
1088         obj = self._chain()
1089         obj.query.add_distinct_fields(*field_names)
1090         return obj
1091 
1092     def extra(self, select=None, where=None, params=None, tables=None,
1093               order_by=None, select_params=None):
1094         """Add extra SQL fragments to the query."""
1095         self._not_support_combined_queries('extra')
1096         assert not self.query.is_sliced, \
1097             "Cannot change a query once a slice has been taken"
1098         clone = self._chain()
1099         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1100         return clone
1101 
1102     def reverse(self):
1103         """Reverse the ordering of the QuerySet."""
1104         if self.query.is_sliced:
1105             raise TypeError('Cannot reverse a query once a slice has been taken.')
1106         clone = self._chain()
1107         clone.query.standard_ordering = not clone.query.standard_ordering
1108         return clone
1109 
1110     def defer(self, *fields):
1111         """
1112         Defer the loading of data for certain fields until they are accessed.
1113         Add the set of deferred fields to any existing set of deferred fields.
1114         The only exception to this is if None is passed in as the only
1115         parameter, in which case removal all deferrals.
1116         """
1117         self._not_support_combined_queries('defer')
1118         if self._fields is not None:
1119             raise TypeError("Cannot call defer() after .values() or .values_list()")
1120         clone = self._chain()
1121         if fields == (None,):
1122             clone.query.clear_deferred_loading()
1123         else:
1124             clone.query.add_deferred_loading(fields)
1125         return clone
1126 
1127     def only(self, *fields):
1128         """
1129         Essentially, the opposite of defer(). Only the fields passed into this
1130         method and that are not already specified as deferred are loaded
1131         immediately when the queryset is evaluated.
1132         """
1133         self._not_support_combined_queries('only')
1134         if self._fields is not None:
1135             raise TypeError("Cannot call only() after .values() or .values_list()")
1136         if fields == (None,):
1137             # Can only pass None to defer(), not only(), as the rest option.
1138             # That won't stop people trying to do this, so let's be explicit.
1139             raise TypeError("Cannot pass None as an argument to only().")
1140         for field in fields:
1141             field = field.split(LOOKUP_SEP, 1)[0]
1142             if field in self.query._filtered_relations:
1143                 raise ValueError('only() is not supported with FilteredRelation.')
1144         clone = self._chain()
1145         clone.query.add_immediate_loading(fields)
1146         return clone
1147 
1148     def using(self, alias):
1149         """Select which database this QuerySet should execute against."""
1150         clone = self._chain()
1151         clone._db = alias
1152         return clone
1153 
1154     ###################################
1155     # PUBLIC INTROSPECTION ATTRIBUTES #
1156     ###################################
1157 
1158     @property
1159     def ordered(self):
1160         """
1161         Return True if the QuerySet is ordered -- i.e. has an order_by()
1162         clause or a default ordering on the model (or is empty).
1163         """
1164         if isinstance(self, EmptyQuerySet):
1165             return True
1166         if self.query.extra_order_by or self.query.order_by:
1167             return True
1168         elif self.query.default_ordering and self.query.get_meta().ordering:
1169             return True
1170         else:
1171             return False
1172 
1173     @property
1174     def db(self):
1175         """Return the database used if this query is executed now."""
1176         if self._for_write:
1177             return self._db or router.db_for_write(self.model, **self._hints)
1178         return self._db or router.db_for_read(self.model, **self._hints)
1179 
1180     ###################
1181     # PRIVATE METHODS #
1182     ###################
1183 
1184     def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):
1185         """
1186         Insert a new record for the given model. This provides an interface to
1187         the InsertQuery class and is how Model.save() is implemented.
1188         """
1189         self._for_write = True
1190         if using is None:
1191             using = self.db
1192         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1193         query.insert_values(fields, objs, raw=raw)
1194         return query.get_compiler(using=using).execute_sql(return_id)
1195     _insert.alters_data = True
1196     _insert.queryset_only = False
1197 
1198     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1199         """
1200         Helper method for bulk_create() to insert objs one batch at a time.
1201         """
1202         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1203             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1204         ops = connections[self.db].ops
1205         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1206         inserted_ids = []
1207         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1208         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1209             if bulk_return and not ignore_conflicts:
1210                 inserted_id = self._insert(
1211                     item, fields=fields, using=self.db, return_id=True,
1212                     ignore_conflicts=ignore_conflicts,
1213                 )
1214                 if isinstance(inserted_id, list):
1215                     inserted_ids.extend(inserted_id)
1216                 else:
1217                     inserted_ids.append(inserted_id)
1218             else:
1219                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1220         return inserted_ids
1221 
1222     def _chain(self, **kwargs):
1223         """
1224         Return a copy of the current QuerySet that's ready for another
1225         operation.
1226         """
1227         obj = self._clone()
1228         if obj._sticky_filter:
1229             obj.query.filter_is_sticky = True
1230             obj._sticky_filter = False
1231         obj.__dict__.update(kwargs)
1232         return obj
1233 
1234     def _clone(self):
1235         """
1236         Return a copy of the current QuerySet. A lightweight alternative
1237         to deepcopy().
1238         """
1239         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1240         c._sticky_filter = self._sticky_filter
1241         c._for_write = self._for_write
1242         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1243         c._known_related_objects = self._known_related_objects
1244         c._iterable_class = self._iterable_class
1245         c._fields = self._fields
1246         return c
1247 
1248     def _fetch_all(self):
1249         if self._result_cache is None:
1250             self._result_cache = list(self._iterable_class(self))
1251         if self._prefetch_related_lookups and not self._prefetch_done:
1252             self._prefetch_related_objects()
1253 
1254     def _next_is_sticky(self):
1255         """
1256         Indicate that the next filter call and the one following that should
1257         be treated as a single filter. This is only important when it comes to
1258         determining when to reuse tables for many-to-many filters. Required so
1259         that we can filter naturally on the results of related managers.
1260 
1261         This doesn't return a clone of the current QuerySet (it returns
1262         "self"). The method is only used internally and should be immediately
1263         followed by a filter() that does create a clone.
1264         """
1265         self._sticky_filter = True
1266         return self
1267 
1268     def _merge_sanity_check(self, other):
1269         """Check that two QuerySet classes may be merged."""
1270         if self._fields is not None and (
1271                 set(self.query.values_select) != set(other.query.values_select) or
1272                 set(self.query.extra_select) != set(other.query.extra_select) or
1273                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1274             raise TypeError(
1275                 "Merging '%s' classes must involve the same values in each case."
1276                 % self.__class__.__name__
1277             )
1278 
1279     def _merge_known_related_objects(self, other):
1280         """
1281         Keep track of all known related objects from either QuerySet instance.
1282         """
1283         for field, objects in other._known_related_objects.items():
1284             self._known_related_objects.setdefault(field, {}).update(objects)
1285 
1286     def resolve_expression(self, *args, **kwargs):
1287         if self._fields and len(self._fields) > 1:
1288             # values() queryset can only be used as nested queries
1289             # if they are set up to select only a single field.
1290             raise TypeError('Cannot use multi-field values as a filter value.')
1291         query = self.query.resolve_expression(*args, **kwargs)
1292         query._db = self._db
1293         return query
1294     resolve_expression.queryset_only = True
1295 
1296     def _add_hints(self, **hints):
1297         """
1298         Update hinting information for use by routers. Add new key/values or
1299         overwrite existing key/values.
1300         """
1301         self._hints.update(hints)
1302 
1303     def _has_filters(self):
1304         """
1305         Check if this QuerySet has any filtering going on. This isn't
1306         equivalent with checking if all objects are present in results, for
1307         example, qs[1:]._has_filters() -> False.
1308         """
1309         return self.query.has_filters()
1310 
1311     @staticmethod
1312     def _validate_values_are_expressions(values, method_name):
1313         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1314         if invalid_args:
1315             raise TypeError(
1316                 'QuerySet.%s() received non-expression(s): %s.' % (
1317                     method_name,
1318                     ', '.join(invalid_args),
1319                 )
1320             )
1321 
1322     def _not_support_combined_queries(self, operation_name):
1323         if self.query.combinator:
1324             raise NotSupportedError(
1325                 'Calling QuerySet.%s() after %s() is not supported.'
1326                 % (operation_name, self.query.combinator)
1327             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-17 22:59:58.711 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-17 22:59:58.711 | INFO     | Search-only: skipping patch selection
2025-10-17 22:59:58.937 | INFO     | Task django__django-8119 completed successfully.
2025-10-17 22:59:58.938 | INFO     | No patch generated. You can try running ACR again.
