[
    {
        "rel_file_path": "testing/test_terminal.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pytest-dev__pytest/setup_pytest-dev__pytest__5.4/testing/test_terminal.py",
        "start": 1,
        "end": 2163,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 terminal reporting of the full testing process.\n3 \"\"\"\n4 import collections\n5 import os\n6 import sys\n7 import textwrap\n8 from io import StringIO\n9 from typing import Dict\n10 from typing import List\n11 from typing import Tuple\n12 \n13 import pluggy\n14 import py\n15 \n16 import _pytest.config\n17 import pytest\n18 from _pytest.config import ExitCode\n19 from _pytest.pytester import Testdir\n20 from _pytest.reports import BaseReport\n21 from _pytest.terminal import _folded_skips\n22 from _pytest.terminal import _get_line_with_reprcrash_message\n23 from _pytest.terminal import _plugin_nameversions\n24 from _pytest.terminal import getreportopt\n25 from _pytest.terminal import TerminalReporter\n26 \n27 DistInfo = collections.namedtuple(\"DistInfo\", [\"project_name\", \"version\"])\n28 \n29 \n30 TRANS_FNMATCH = str.maketrans({\"[\": \"[[]\", \"]\": \"[]]\"})\n31 \n32 \n33 class Option:\n34     def __init__(self, verbosity=0):\n35         self.verbosity = verbosity\n36 \n37     @property\n38     def args(self):\n39         values = []\n40         values.append(\"--verbosity=%d\" % self.verbosity)\n41         return values\n42 \n43 \n44 @pytest.fixture(\n45     params=[Option(verbosity=0), Option(verbosity=1), Option(verbosity=-1)],\n46     ids=[\"default\", \"verbose\", \"quiet\"],\n47 )\n48 def option(request):\n49     return request.param\n50 \n51 \n52 @pytest.mark.parametrize(\n53     \"input,expected\",\n54     [\n55         ([DistInfo(project_name=\"test\", version=1)], [\"test-1\"]),\n56         ([DistInfo(project_name=\"pytest-test\", version=1)], [\"test-1\"]),\n57         (\n58             [\n59                 DistInfo(project_name=\"test\", version=1),\n60                 DistInfo(project_name=\"test\", version=1),\n61             ],\n62             [\"test-1\"],\n63         ),\n64     ],\n65     ids=[\"normal\", \"prefix-strip\", \"deduplicate\"],\n66 )\n67 def test_plugin_nameversion(input, expected):\n68     pluginlist = [(None, x) for x in input]\n69     result = _plugin_nameversions(pluginlist)\n70     assert result == expected\n71 \n72 \n73 class TestTerminal:\n74     def test_pass_skip_fail(self, testdir, option):\n75         testdir.makepyfile(\n76             \"\"\"\n77             import pytest\n78             def test_ok():\n79                 pass\n80             def test_skip():\n81                 pytest.skip(\"xx\")\n82             def test_func():\n83                 assert 0\n84         \"\"\"\n85         )\n86         result = testdir.runpytest(*option.args)\n87         if option.verbosity > 0:\n88             result.stdout.fnmatch_lines(\n89                 [\n90                     \"*test_pass_skip_fail.py::test_ok PASS*\",\n91                     \"*test_pass_skip_fail.py::test_skip SKIP*\",\n92                     \"*test_pass_skip_fail.py::test_func FAIL*\",\n93                 ]\n94             )\n95         elif option.verbosity == 0:\n96             result.stdout.fnmatch_lines([\"*test_pass_skip_fail.py .sF*\"])\n97         else:\n98             result.stdout.fnmatch_lines([\".sF*\"])\n99         result.stdout.fnmatch_lines(\n100             [\"    def test_func():\", \">       assert 0\", \"E       assert 0\"]\n101         )\n102 \n103     def test_internalerror(self, testdir, linecomp):\n104         modcol = testdir.getmodulecol(\"def test_one(): pass\")\n105         rep = TerminalReporter(modcol.config, file=linecomp.stringio)\n106         with pytest.raises(ValueError) as excinfo:\n107             raise ValueError(\"hello\")\n108         rep.pytest_internalerror(excinfo.getrepr())\n109         linecomp.assert_contains_lines([\"INTERNALERROR> *ValueError*hello*\"])\n110 \n111     def test_writeline(self, testdir, linecomp):\n112         modcol = testdir.getmodulecol(\"def test_one(): pass\")\n113         rep = TerminalReporter(modcol.config, file=linecomp.stringio)\n114         rep.write_fspath_result(modcol.nodeid, \".\")\n115         rep.write_line(\"hello world\")\n116         lines = linecomp.stringio.getvalue().split(\"\\n\")\n117         assert not lines[0]\n118         assert lines[1].endswith(modcol.name + \" .\")\n119         assert lines[2] == \"hello world\"\n120 \n121     def test_show_runtest_logstart(self, testdir, linecomp):\n122         item = testdir.getitem(\"def test_func(): pass\")\n123         tr = TerminalReporter(item.config, file=linecomp.stringio)\n124         item.config.pluginmanager.register(tr)\n125         location = item.reportinfo()\n126         tr.config.hook.pytest_runtest_logstart(\n127             nodeid=item.nodeid, location=location, fspath=str(item.fspath)\n128         )\n129         linecomp.assert_contains_lines([\"*test_show_runtest_logstart.py*\"])\n130 \n131     def test_runtest_location_shown_before_test_starts(self, testdir):\n132         testdir.makepyfile(\n133             \"\"\"\n134             def test_1():\n135                 import time\n136                 time.sleep(20)\n137         \"\"\"\n138         )\n139         child = testdir.spawn_pytest(\"\")\n140         child.expect(\".*test_runtest_location.*py\")\n141         child.sendeof()\n142         child.kill(15)\n143 \n144     def test_report_collect_after_half_a_second(self, testdir):\n145         \"\"\"Test for \"collecting\" being updated after 0.5s\"\"\"\n146 \n147         testdir.makepyfile(\n148             **{\n149                 \"test1.py\": \"\"\"\n150                 import _pytest.terminal\n151 \n152                 _pytest.terminal.REPORT_COLLECTING_RESOLUTION = 0\n153 \n154                 def test_1():\n155                     pass\n156                     \"\"\",\n157                 \"test2.py\": \"def test_2(): pass\",\n158             }\n159         )\n160         # Explicitly test colored output.\n161         testdir.monkeypatch.setenv(\"PY_COLORS\", \"1\")\n162 \n163         child = testdir.spawn_pytest(\"-v test1.py test2.py\")\n164         child.expect(r\"collecting \\.\\.\\.\")\n165         child.expect(r\"collecting 1 item\")\n166         child.expect(r\"collecting 2 items\")\n167         child.expect(r\"collected 2 items\")\n168         rest = child.read().decode(\"utf8\")\n169         assert \"= \\x1b[32m\\x1b[1m2 passed\\x1b[0m\\x1b[32m in\" in rest\n170 \n171     def test_itemreport_subclasses_show_subclassed_file(self, testdir):\n172         testdir.makepyfile(\n173             **{\n174                 \"tests/test_p1\": \"\"\"\n175             class BaseTests(object):\n176                 fail = False\n177 \n178                 def test_p1(self):\n179                     if self.fail: assert 0\n180                 \"\"\",\n181                 \"tests/test_p2\": \"\"\"\n182             from test_p1 import BaseTests\n183 \n184             class TestMore(BaseTests): pass\n185                 \"\"\",\n186                 \"tests/test_p3.py\": \"\"\"\n187             from test_p1 import BaseTests\n188 \n189             BaseTests.fail = True\n190 \n191             class TestMore(BaseTests): pass\n192         \"\"\",\n193             }\n194         )\n195         result = testdir.runpytest(\"tests/test_p2.py\", \"--rootdir=tests\")\n196         result.stdout.fnmatch_lines([\"tests/test_p2.py .*\", \"=* 1 passed in *\"])\n197 \n198         result = testdir.runpytest(\"-vv\", \"-rA\", \"tests/test_p2.py\", \"--rootdir=tests\")\n199         result.stdout.fnmatch_lines(\n200             [\n201                 \"tests/test_p2.py::TestMore::test_p1 <- test_p1.py PASSED *\",\n202                 \"*= short test summary info =*\",\n203                 \"PASSED tests/test_p2.py::TestMore::test_p1\",\n204             ]\n205         )\n206         result = testdir.runpytest(\"-vv\", \"-rA\", \"tests/test_p3.py\", \"--rootdir=tests\")\n207         result.stdout.fnmatch_lines(\n208             [\n209                 \"tests/test_p3.py::TestMore::test_p1 <- test_p1.py FAILED *\",\n210                 \"*_ TestMore.test_p1 _*\",\n211                 \"    def test_p1(self):\",\n212                 \">       if self.fail: assert 0\",\n213                 \"E       assert 0\",\n214                 \"\",\n215                 \"tests/test_p1.py:5: AssertionError\",\n216                 \"*= short test summary info =*\",\n217                 \"FAILED tests/test_p3.py::TestMore::test_p1 - assert 0\",\n218                 \"*= 1 failed in *\",\n219             ]\n220         )\n221 \n222     def test_itemreport_directclasses_not_shown_as_subclasses(self, testdir):\n223         a = testdir.mkpydir(\"a123\")\n224         a.join(\"test_hello123.py\").write(\n225             textwrap.dedent(\n226                 \"\"\"\\\n227                 class TestClass(object):\n228                     def test_method(self):\n229                         pass\n230                 \"\"\"\n231             )\n232         )\n233         result = testdir.runpytest(\"-vv\")\n234         assert result.ret == 0\n235         result.stdout.fnmatch_lines([\"*a123/test_hello123.py*PASS*\"])\n236         result.stdout.no_fnmatch_line(\"* <- *\")\n237 \n238     @pytest.mark.parametrize(\"fulltrace\", (\"\", \"--fulltrace\"))\n239     def test_keyboard_interrupt(self, testdir, fulltrace):\n240         testdir.makepyfile(\n241             \"\"\"\n242             def test_foobar():\n243                 assert 0\n244             def test_spamegg():\n245                 import py; pytest.skip('skip me please!')\n246             def test_interrupt_me():\n247                 raise KeyboardInterrupt   # simulating the user\n248         \"\"\"\n249         )\n250 \n251         result = testdir.runpytest(fulltrace, no_reraise_ctrlc=True)\n252         result.stdout.fnmatch_lines(\n253             [\n254                 \"    def test_foobar():\",\n255                 \">       assert 0\",\n256                 \"E       assert 0\",\n257                 \"*_keyboard_interrupt.py:6: KeyboardInterrupt*\",\n258             ]\n259         )\n260         if fulltrace:\n261             result.stdout.fnmatch_lines(\n262                 [\"*raise KeyboardInterrupt   # simulating the user*\"]\n263             )\n264         else:\n265             result.stdout.fnmatch_lines(\n266                 [\"(to show a full traceback on KeyboardInterrupt use --full-trace)\"]\n267             )\n268         result.stdout.fnmatch_lines([\"*KeyboardInterrupt*\"])\n269 \n270     def test_keyboard_in_sessionstart(self, testdir):\n271         testdir.makeconftest(\n272             \"\"\"\n273             def pytest_sessionstart():\n274                 raise KeyboardInterrupt\n275         \"\"\"\n276         )\n277         testdir.makepyfile(\n278             \"\"\"\n279             def test_foobar():\n280                 pass\n281         \"\"\"\n282         )\n283 \n284         result = testdir.runpytest(no_reraise_ctrlc=True)\n285         assert result.ret == 2\n286         result.stdout.fnmatch_lines([\"*KeyboardInterrupt*\"])\n287 \n288     def test_collect_single_item(self, testdir):\n289         \"\"\"Use singular 'item' when reporting a single test item\"\"\"\n290         testdir.makepyfile(\n291             \"\"\"\n292             def test_foobar():\n293                 pass\n294         \"\"\"\n295         )\n296         result = testdir.runpytest()\n297         result.stdout.fnmatch_lines([\"collected 1 item\"])\n298 \n299     def test_rewrite(self, testdir, monkeypatch):\n300         config = testdir.parseconfig()\n301         f = StringIO()\n302         monkeypatch.setattr(f, \"isatty\", lambda *args: True)\n303         tr = TerminalReporter(config, f)\n304         tr._tw.fullwidth = 10\n305         tr.write(\"hello\")\n306         tr.rewrite(\"hey\", erase=True)\n307         assert f.getvalue() == \"hello\" + \"\\r\" + \"hey\" + (6 * \" \")\n308 \n309     def test_report_teststatus_explicit_markup(\n310         self, testdir: Testdir, color_mapping\n311     ) -> None:\n312         \"\"\"Test that TerminalReporter handles markup explicitly provided by\n313         a pytest_report_teststatus hook.\"\"\"\n314         testdir.monkeypatch.setenv(\"PY_COLORS\", \"1\")\n315         testdir.makeconftest(\n316             \"\"\"\n317             def pytest_report_teststatus(report):\n318                 return 'foo', 'F', ('FOO', {'red': True})\n319         \"\"\"\n320         )\n321         testdir.makepyfile(\n322             \"\"\"\n323             def test_foobar():\n324                 pass\n325         \"\"\"\n326         )\n327         result = testdir.runpytest(\"-v\")\n328         result.stdout.fnmatch_lines(\n329             color_mapping.format_for_fnmatch([\"*{red}FOO{reset}*\"])\n330         )\n331 \n332 \n333 class TestCollectonly:\n334     def test_collectonly_basic(self, testdir):\n335         testdir.makepyfile(\n336             \"\"\"\n337             def test_func():\n338                 pass\n339         \"\"\"\n340         )\n341         result = testdir.runpytest(\"--collect-only\")\n342         result.stdout.fnmatch_lines(\n343             [\"<Module test_collectonly_basic.py>\", \"  <Function test_func>\"]\n344         )\n345 \n346     def test_collectonly_skipped_module(self, testdir):\n347         testdir.makepyfile(\n348             \"\"\"\n349             import pytest\n350             pytest.skip(\"hello\")\n351         \"\"\"\n352         )\n353         result = testdir.runpytest(\"--collect-only\", \"-rs\")\n354         result.stdout.fnmatch_lines([\"*ERROR collecting*\"])\n355 \n356     def test_collectonly_display_test_description(self, testdir):\n357         testdir.makepyfile(\n358             \"\"\"\n359             def test_with_description():\n360                 \\\"\"\" This test has a description.\n361                 \\\"\"\"\n362                 assert True\n363         \"\"\"\n364         )\n365         result = testdir.runpytest(\"--collect-only\", \"--verbose\")\n366         result.stdout.fnmatch_lines([\"    This test has a description.\"])\n367 \n368     def test_collectonly_failed_module(self, testdir):\n369         testdir.makepyfile(\"\"\"raise ValueError(0)\"\"\")\n370         result = testdir.runpytest(\"--collect-only\")\n371         result.stdout.fnmatch_lines([\"*raise ValueError*\", \"*1 error*\"])\n372 \n373     def test_collectonly_fatal(self, testdir):\n374         testdir.makeconftest(\n375             \"\"\"\n376             def pytest_collectstart(collector):\n377                 assert 0, \"urgs\"\n378         \"\"\"\n379         )\n380         result = testdir.runpytest(\"--collect-only\")\n381         result.stdout.fnmatch_lines([\"*INTERNAL*args*\"])\n382         assert result.ret == 3\n383 \n384     def test_collectonly_simple(self, testdir):\n385         p = testdir.makepyfile(\n386             \"\"\"\n387             def test_func1():\n388                 pass\n389             class TestClass(object):\n390                 def test_method(self):\n391                     pass\n392         \"\"\"\n393         )\n394         result = testdir.runpytest(\"--collect-only\", p)\n395         # assert stderr.startswith(\"inserting into sys.path\")\n396         assert result.ret == 0\n397         result.stdout.fnmatch_lines(\n398             [\n399                 \"*<Module *.py>\",\n400                 \"* <Function test_func1>\",\n401                 \"* <Class TestClass>\",\n402                 \"*   <Function test_method>\",\n403             ]\n404         )\n405 \n406     def test_collectonly_error(self, testdir):\n407         p = testdir.makepyfile(\"import Errlkjqweqwe\")\n408         result = testdir.runpytest(\"--collect-only\", p)\n409         assert result.ret == 2\n410         result.stdout.fnmatch_lines(\n411             textwrap.dedent(\n412                 \"\"\"\\\n413                 *ERROR*\n414                 *ImportError*\n415                 *No module named *Errlk*\n416                 *1 error*\n417                 \"\"\"\n418             ).strip()\n419         )\n420 \n421     def test_collectonly_missing_path(self, testdir):\n422         \"\"\"this checks issue 115,\n423             failure in parseargs will cause session\n424             not to have the items attribute\n425         \"\"\"\n426         result = testdir.runpytest(\"--collect-only\", \"uhm_missing_path\")\n427         assert result.ret == 4\n428         result.stderr.fnmatch_lines([\"*ERROR: file not found*\"])\n429 \n430     def test_collectonly_quiet(self, testdir):\n431         testdir.makepyfile(\"def test_foo(): pass\")\n432         result = testdir.runpytest(\"--collect-only\", \"-q\")\n433         result.stdout.fnmatch_lines([\"*test_foo*\"])\n434 \n435     def test_collectonly_more_quiet(self, testdir):\n436         testdir.makepyfile(test_fun=\"def test_foo(): pass\")\n437         result = testdir.runpytest(\"--collect-only\", \"-qq\")\n438         result.stdout.fnmatch_lines([\"*test_fun.py: 1*\"])\n439 \n440 \n441 class TestFixtureReporting:\n442     def test_setup_fixture_error(self, testdir):\n443         testdir.makepyfile(\n444             \"\"\"\n445             def setup_function(function):\n446                 print(\"setup func\")\n447                 assert 0\n448             def test_nada():\n449                 pass\n450         \"\"\"\n451         )\n452         result = testdir.runpytest()\n453         result.stdout.fnmatch_lines(\n454             [\n455                 \"*ERROR at setup of test_nada*\",\n456                 \"*setup_function(function):*\",\n457                 \"*setup func*\",\n458                 \"*assert 0*\",\n459                 \"*1 error*\",\n460             ]\n461         )\n462         assert result.ret != 0\n463 \n464     def test_teardown_fixture_error(self, testdir):\n465         testdir.makepyfile(\n466             \"\"\"\n467             def test_nada():\n468                 pass\n469             def teardown_function(function):\n470                 print(\"teardown func\")\n471                 assert 0\n472         \"\"\"\n473         )\n474         result = testdir.runpytest()\n475         result.stdout.fnmatch_lines(\n476             [\n477                 \"*ERROR at teardown*\",\n478                 \"*teardown_function(function):*\",\n479                 \"*assert 0*\",\n480                 \"*Captured stdout*\",\n481                 \"*teardown func*\",\n482                 \"*1 passed*1 error*\",\n483             ]\n484         )\n485 \n486     def test_teardown_fixture_error_and_test_failure(self, testdir):\n487         testdir.makepyfile(\n488             \"\"\"\n489             def test_fail():\n490                 assert 0, \"failingfunc\"\n491 \n492             def teardown_function(function):\n493                 print(\"teardown func\")\n494                 assert False\n495         \"\"\"\n496         )\n497         result = testdir.runpytest()\n498         result.stdout.fnmatch_lines(\n499             [\n500                 \"*ERROR at teardown of test_fail*\",\n501                 \"*teardown_function(function):*\",\n502                 \"*assert False*\",\n503                 \"*Captured stdout*\",\n504                 \"*teardown func*\",\n505                 \"*test_fail*\",\n506                 \"*def test_fail():\",\n507                 \"*failingfunc*\",\n508                 \"*1 failed*1 error*\",\n509             ]\n510         )\n511 \n512     def test_setup_teardown_output_and_test_failure(self, testdir):\n513         \"\"\" Test for issue #442 \"\"\"\n514         testdir.makepyfile(\n515             \"\"\"\n516             def setup_function(function):\n517                 print(\"setup func\")\n518 \n519             def test_fail():\n520                 assert 0, \"failingfunc\"\n521 \n522             def teardown_function(function):\n523                 print(\"teardown func\")\n524         \"\"\"\n525         )\n526         result = testdir.runpytest()\n527         result.stdout.fnmatch_lines(\n528             [\n529                 \"*test_fail*\",\n530                 \"*def test_fail():\",\n531                 \"*failingfunc*\",\n532                 \"*Captured stdout setup*\",\n533                 \"*setup func*\",\n534                 \"*Captured stdout teardown*\",\n535                 \"*teardown func*\",\n536                 \"*1 failed*\",\n537             ]\n538         )\n539 \n540 \n541 class TestTerminalFunctional:\n542     def test_deselected(self, testdir):\n543         testpath = testdir.makepyfile(\n544             \"\"\"\n545                 def test_one():\n546                     pass\n547                 def test_two():\n548                     pass\n549                 def test_three():\n550                     pass\n551            \"\"\"\n552         )\n553         result = testdir.runpytest(\"-k\", \"test_two:\", testpath)\n554         result.stdout.fnmatch_lines(\n555             [\"collected 3 items / 1 deselected / 2 selected\", \"*test_deselected.py ..*\"]\n556         )\n557         assert result.ret == 0\n558 \n559     def test_deselected_with_hookwrapper(self, testdir):\n560         testpath = testdir.makeconftest(\n561             \"\"\"\n562             import pytest\n563 \n564             @pytest.hookimpl(hookwrapper=True)\n565             def pytest_collection_modifyitems(config, items):\n566                 yield\n567                 deselected = items.pop()\n568                 config.hook.pytest_deselected(items=[deselected])\n569             \"\"\"\n570         )\n571         testpath = testdir.makepyfile(\n572             \"\"\"\n573                 def test_one():\n574                     pass\n575                 def test_two():\n576                     pass\n577                 def test_three():\n578                     pass\n579            \"\"\"\n580         )\n581         result = testdir.runpytest(testpath)\n582         result.stdout.fnmatch_lines(\n583             [\n584                 \"collected 3 items / 1 deselected / 2 selected\",\n585                 \"*= 2 passed, 1 deselected in*\",\n586             ]\n587         )\n588         assert result.ret == 0\n589 \n590     def test_show_deselected_items_using_markexpr_before_test_execution(self, testdir):\n591         testdir.makepyfile(\n592             test_show_deselected=\"\"\"\n593             import pytest\n594 \n595             @pytest.mark.foo\n596             def test_foobar():\n597                 pass\n598 \n599             @pytest.mark.bar\n600             def test_bar():\n601                 pass\n602 \n603             def test_pass():\n604                 pass\n605         \"\"\"\n606         )\n607         result = testdir.runpytest(\"-m\", \"not foo\")\n608         result.stdout.fnmatch_lines(\n609             [\n610                 \"collected 3 items / 1 deselected / 2 selected\",\n611                 \"*test_show_deselected.py ..*\",\n612                 \"*= 2 passed, 1 deselected in * =*\",\n613             ]\n614         )\n615         result.stdout.no_fnmatch_line(\"*= 1 deselected =*\")\n616         assert result.ret == 0\n617 \n618     def test_no_skip_summary_if_failure(self, testdir):\n619         testdir.makepyfile(\n620             \"\"\"\n621             import pytest\n622             def test_ok():\n623                 pass\n624             def test_fail():\n625                 assert 0\n626             def test_skip():\n627                 pytest.skip(\"dontshow\")\n628         \"\"\"\n629         )\n630         result = testdir.runpytest()\n631         assert result.stdout.str().find(\"skip test summary\") == -1\n632         assert result.ret == 1\n633 \n634     def test_passes(self, testdir):\n635         p1 = testdir.makepyfile(\n636             \"\"\"\n637             def test_passes():\n638                 pass\n639             class TestClass(object):\n640                 def test_method(self):\n641                     pass\n642         \"\"\"\n643         )\n644         old = p1.dirpath().chdir()\n645         try:\n646             result = testdir.runpytest()\n647         finally:\n648             old.chdir()\n649         result.stdout.fnmatch_lines([\"test_passes.py ..*\", \"* 2 pass*\"])\n650         assert result.ret == 0\n651 \n652     def test_header_trailer_info(self, testdir, request):\n653         testdir.monkeypatch.delenv(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\")\n654         testdir.makepyfile(\n655             \"\"\"\n656             def test_passes():\n657                 pass\n658         \"\"\"\n659         )\n660         result = testdir.runpytest()\n661         verinfo = \".\".join(map(str, sys.version_info[:3]))\n662         result.stdout.fnmatch_lines(\n663             [\n664                 \"*===== test session starts ====*\",\n665                 \"platform %s -- Python %s*pytest-%s*py-%s*pluggy-%s\"\n666                 % (\n667                     sys.platform,\n668                     verinfo,\n669                     pytest.__version__,\n670                     py.__version__,\n671                     pluggy.__version__,\n672                 ),\n673                 \"*test_header_trailer_info.py .*\",\n674                 \"=* 1 passed*in *.[0-9][0-9]s *=\",\n675             ]\n676         )\n677         if request.config.pluginmanager.list_plugin_distinfo():\n678             result.stdout.fnmatch_lines([\"plugins: *\"])\n679 \n680     def test_header(self, testdir):\n681         testdir.tmpdir.join(\"tests\").ensure_dir()\n682         testdir.tmpdir.join(\"gui\").ensure_dir()\n683 \n684         # no ini file\n685         result = testdir.runpytest()\n686         result.stdout.fnmatch_lines([\"rootdir: *test_header0\"])\n687 \n688         # with inifile\n689         testdir.makeini(\"\"\"[pytest]\"\"\")\n690         result = testdir.runpytest()\n691         result.stdout.fnmatch_lines([\"rootdir: *test_header0, inifile: tox.ini\"])\n692 \n693         # with testpaths option, and not passing anything in the command-line\n694         testdir.makeini(\n695             \"\"\"\n696             [pytest]\n697             testpaths = tests gui\n698         \"\"\"\n699         )\n700         result = testdir.runpytest()\n701         result.stdout.fnmatch_lines(\n702             [\"rootdir: *test_header0, inifile: tox.ini, testpaths: tests, gui\"]\n703         )\n704 \n705         # with testpaths option, passing directory in command-line: do not show testpaths then\n706         result = testdir.runpytest(\"tests\")\n707         result.stdout.fnmatch_lines([\"rootdir: *test_header0, inifile: tox.ini\"])\n708 \n709     def test_showlocals(self, testdir):\n710         p1 = testdir.makepyfile(\n711             \"\"\"\n712             def test_showlocals():\n713                 x = 3\n714                 y = \"x\" * 5000\n715                 assert 0\n716         \"\"\"\n717         )\n718         result = testdir.runpytest(p1, \"-l\")\n719         result.stdout.fnmatch_lines(\n720             [\n721                 # \"_ _ * Locals *\",\n722                 \"x* = 3\",\n723                 \"y* = 'xxxxxx*\",\n724             ]\n725         )\n726 \n727     def test_showlocals_short(self, testdir):\n728         p1 = testdir.makepyfile(\n729             \"\"\"\n730             def test_showlocals_short():\n731                 x = 3\n732                 y = \"xxxx\"\n733                 assert 0\n734         \"\"\"\n735         )\n736         result = testdir.runpytest(p1, \"-l\", \"--tb=short\")\n737         result.stdout.fnmatch_lines(\n738             [\n739                 \"test_showlocals_short.py:*\",\n740                 \"    assert 0\",\n741                 \"E   assert 0\",\n742                 \"        x          = 3\",\n743                 \"        y          = 'xxxx'\",\n744             ]\n745         )\n746 \n747     @pytest.fixture\n748     def verbose_testfile(self, testdir):\n749         return testdir.makepyfile(\n750             \"\"\"\n751             import pytest\n752             def test_fail():\n753                 raise ValueError()\n754             def test_pass():\n755                 pass\n756             class TestClass(object):\n757                 def test_skip(self):\n758                     pytest.skip(\"hello\")\n759             def test_gen():\n760                 def check(x):\n761                     assert x == 1\n762                 yield check, 0\n763         \"\"\"\n764         )\n765 \n766     def test_verbose_reporting(self, verbose_testfile, testdir):\n767         result = testdir.runpytest(\n768             verbose_testfile, \"-v\", \"-Walways::pytest.PytestWarning\"\n769         )\n770         result.stdout.fnmatch_lines(\n771             [\n772                 \"*test_verbose_reporting.py::test_fail *FAIL*\",\n773                 \"*test_verbose_reporting.py::test_pass *PASS*\",\n774                 \"*test_verbose_reporting.py::TestClass::test_skip *SKIP*\",\n775                 \"*test_verbose_reporting.py::test_gen *XFAIL*\",\n776             ]\n777         )\n778         assert result.ret == 1\n779 \n780     def test_verbose_reporting_xdist(self, verbose_testfile, testdir, pytestconfig):\n781         if not pytestconfig.pluginmanager.get_plugin(\"xdist\"):\n782             pytest.skip(\"xdist plugin not installed\")\n783 \n784         testdir.monkeypatch.delenv(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\")\n785         result = testdir.runpytest(\n786             verbose_testfile, \"-v\", \"-n 1\", \"-Walways::pytest.PytestWarning\"\n787         )\n788         result.stdout.fnmatch_lines(\n789             [\"*FAIL*test_verbose_reporting_xdist.py::test_fail*\"]\n790         )\n791         assert result.ret == 1\n792 \n793     def test_quiet_reporting(self, testdir):\n794         p1 = testdir.makepyfile(\"def test_pass(): pass\")\n795         result = testdir.runpytest(p1, \"-q\")\n796         s = result.stdout.str()\n797         assert \"test session starts\" not in s\n798         assert p1.basename not in s\n799         assert \"===\" not in s\n800         assert \"passed\" in s\n801 \n802     def test_more_quiet_reporting(self, testdir):\n803         p1 = testdir.makepyfile(\"def test_pass(): pass\")\n804         result = testdir.runpytest(p1, \"-qq\")\n805         s = result.stdout.str()\n806         assert \"test session starts\" not in s\n807         assert p1.basename not in s\n808         assert \"===\" not in s\n809         assert \"passed\" not in s\n810 \n811     @pytest.mark.parametrize(\n812         \"params\", [(), (\"--collect-only\",)], ids=[\"no-params\", \"collect-only\"]\n813     )\n814     def test_report_collectionfinish_hook(self, testdir, params):\n815         testdir.makeconftest(\n816             \"\"\"\n817             def pytest_report_collectionfinish(config, startdir, items):\n818                 return ['hello from hook: {0} items'.format(len(items))]\n819         \"\"\"\n820         )\n821         testdir.makepyfile(\n822             \"\"\"\n823             import pytest\n824             @pytest.mark.parametrize('i', range(3))\n825             def test(i):\n826                 pass\n827         \"\"\"\n828         )\n829         result = testdir.runpytest(*params)\n830         result.stdout.fnmatch_lines([\"collected 3 items\", \"hello from hook: 3 items\"])\n831 \n832     def test_summary_f_alias(self, testdir):\n833         \"\"\"Test that 'f' and 'F' report chars are aliases and don't show up twice in the summary (#6334)\"\"\"\n834         testdir.makepyfile(\n835             \"\"\"\n836             def test():\n837                 assert False\n838             \"\"\"\n839         )\n840         result = testdir.runpytest(\"-rfF\")\n841         expected = \"FAILED test_summary_f_alias.py::test - assert False\"\n842         result.stdout.fnmatch_lines([expected])\n843         assert result.stdout.lines.count(expected) == 1\n844 \n845     def test_summary_s_alias(self, testdir):\n846         \"\"\"Test that 's' and 'S' report chars are aliases and don't show up twice in the summary\"\"\"\n847         testdir.makepyfile(\n848             \"\"\"\n849             import pytest\n850 \n851             @pytest.mark.skip\n852             def test():\n853                 pass\n854             \"\"\"\n855         )\n856         result = testdir.runpytest(\"-rsS\")\n857         expected = \"SKIPPED [1] test_summary_s_alias.py:3: unconditional skip\"\n858         result.stdout.fnmatch_lines([expected])\n859         assert result.stdout.lines.count(expected) == 1\n860 \n861 \n862 def test_fail_extra_reporting(testdir, monkeypatch):\n863     monkeypatch.setenv(\"COLUMNS\", \"80\")\n864     testdir.makepyfile(\"def test_this(): assert 0, 'this_failed' * 100\")\n865     result = testdir.runpytest(\"-rN\")\n866     result.stdout.no_fnmatch_line(\"*short test summary*\")\n867     result = testdir.runpytest()\n868     result.stdout.fnmatch_lines(\n869         [\n870             \"*test summary*\",\n871             \"FAILED test_fail_extra_reporting.py::test_this - AssertionError: this_failedt...\",\n872         ]\n873     )\n874 \n875 \n876 def test_fail_reporting_on_pass(testdir):\n877     testdir.makepyfile(\"def test_this(): assert 1\")\n878     result = testdir.runpytest(\"-rf\")\n879     result.stdout.no_fnmatch_line(\"*short test summary*\")\n880 \n881 \n882 def test_pass_extra_reporting(testdir):\n883     testdir.makepyfile(\"def test_this(): assert 1\")\n884     result = testdir.runpytest()\n885     result.stdout.no_fnmatch_line(\"*short test summary*\")\n886     result = testdir.runpytest(\"-rp\")\n887     result.stdout.fnmatch_lines([\"*test summary*\", \"PASS*test_pass_extra_reporting*\"])\n888 \n889 \n890 def test_pass_reporting_on_fail(testdir):\n891     testdir.makepyfile(\"def test_this(): assert 0\")\n892     result = testdir.runpytest(\"-rp\")\n893     result.stdout.no_fnmatch_line(\"*short test summary*\")\n894 \n895 \n896 def test_pass_output_reporting(testdir):\n897     testdir.makepyfile(\n898         \"\"\"\n899         def setup_module():\n900             print(\"setup_module\")\n901 \n902         def teardown_module():\n903             print(\"teardown_module\")\n904 \n905         def test_pass_has_output():\n906             print(\"Four score and seven years ago...\")\n907 \n908         def test_pass_no_output():\n909             pass\n910     \"\"\"\n911     )\n912     result = testdir.runpytest()\n913     s = result.stdout.str()\n914     assert \"test_pass_has_output\" not in s\n915     assert \"Four score and seven years ago...\" not in s\n916     assert \"test_pass_no_output\" not in s\n917     result = testdir.runpytest(\"-rPp\")\n918     result.stdout.fnmatch_lines(\n919         [\n920             \"*= PASSES =*\",\n921             \"*_ test_pass_has_output _*\",\n922             \"*- Captured stdout setup -*\",\n923             \"setup_module\",\n924             \"*- Captured stdout call -*\",\n925             \"Four score and seven years ago...\",\n926             \"*- Captured stdout teardown -*\",\n927             \"teardown_module\",\n928             \"*= short test summary info =*\",\n929             \"PASSED test_pass_output_reporting.py::test_pass_has_output\",\n930             \"PASSED test_pass_output_reporting.py::test_pass_no_output\",\n931             \"*= 2 passed in *\",\n932         ]\n933     )\n934 \n935 \n936 def test_color_yes(testdir, color_mapping):\n937     p1 = testdir.makepyfile(\n938         \"\"\"\n939         def fail():\n940             assert 0\n941 \n942         def test_this():\n943             fail()\n944         \"\"\"\n945     )\n946     result = testdir.runpytest(\"--color=yes\", str(p1))\n947     color_mapping.requires_ordered_markup(result)\n948     result.stdout.fnmatch_lines(\n949         color_mapping.format_for_fnmatch(\n950             [\n951                 \"{bold}=*= test session starts =*={reset}\",\n952                 \"collected 1 item\",\n953                 \"\",\n954                 \"test_color_yes.py {red}F{reset}{red} * [100%]{reset}\",\n955                 \"\",\n956                 \"=*= FAILURES =*=\",\n957                 \"{red}{bold}_*_ test_this _*_{reset}\",\n958                 \"\",\n959                 \"    {kw}def{hl-reset} {function}test_this{hl-reset}():\",\n960                 \">       fail()\",\n961                 \"\",\n962                 \"{bold}{red}test_color_yes.py{reset}:5: \",\n963                 \"_ _ * _ _*\",\n964                 \"\",\n965                 \"    {kw}def{hl-reset} {function}fail{hl-reset}():\",\n966                 \">       {kw}assert{hl-reset} {number}0{hl-reset}\",\n967                 \"{bold}{red}E       assert 0{reset}\",\n968                 \"\",\n969                 \"{bold}{red}test_color_yes.py{reset}:2: AssertionError\",\n970                 \"{red}=*= {red}{bold}1 failed{reset}{red} in *s{reset}{red} =*={reset}\",\n971             ]\n972         )\n973     )\n974     result = testdir.runpytest(\"--color=yes\", \"--tb=short\", str(p1))\n975     result.stdout.fnmatch_lines(\n976         color_mapping.format_for_fnmatch(\n977             [\n978                 \"{bold}=*= test session starts =*={reset}\",\n979                 \"collected 1 item\",\n980                 \"\",\n981                 \"test_color_yes.py {red}F{reset}{red} * [100%]{reset}\",\n982                 \"\",\n983                 \"=*= FAILURES =*=\",\n984                 \"{red}{bold}_*_ test_this _*_{reset}\",\n985                 \"{bold}{red}test_color_yes.py{reset}:5: in test_this\",\n986                 \"    fail()\",\n987                 \"{bold}{red}test_color_yes.py{reset}:2: in fail\",\n988                 \"    {kw}assert{hl-reset} {number}0{hl-reset}\",\n989                 \"{bold}{red}E   assert 0{reset}\",\n990                 \"{red}=*= {red}{bold}1 failed{reset}{red} in *s{reset}{red} =*={reset}\",\n991             ]\n992         )\n993     )\n994 \n995 \n996 def test_color_no(testdir):\n997     testdir.makepyfile(\"def test_this(): assert 1\")\n998     result = testdir.runpytest(\"--color=no\")\n999     assert \"test session starts\" in result.stdout.str()\n1000     result.stdout.no_fnmatch_line(\"*\\x1b[1m*\")\n1001 \n1002 \n1003 @pytest.mark.parametrize(\"verbose\", [True, False])\n1004 def test_color_yes_collection_on_non_atty(testdir, verbose):\n1005     \"\"\"skip collect progress report when working on non-terminals.\n1006     #1397\n1007     \"\"\"\n1008     testdir.makepyfile(\n1009         \"\"\"\n1010         import pytest\n1011         @pytest.mark.parametrize('i', range(10))\n1012         def test_this(i):\n1013             assert 1\n1014     \"\"\"\n1015     )\n1016     args = [\"--color=yes\"]\n1017     if verbose:\n1018         args.append(\"-vv\")\n1019     result = testdir.runpytest(*args)\n1020     assert \"test session starts\" in result.stdout.str()\n1021     assert \"\\x1b[1m\" in result.stdout.str()\n1022     result.stdout.no_fnmatch_line(\"*collecting 10 items*\")\n1023     if verbose:\n1024         assert \"collecting ...\" in result.stdout.str()\n1025     assert \"collected 10 items\" in result.stdout.str()\n1026 \n1027 \n1028 def test_getreportopt():\n1029     from _pytest.terminal import _REPORTCHARS_DEFAULT\n1030 \n1031     class Config:\n1032         class Option:\n1033             reportchars = _REPORTCHARS_DEFAULT\n1034             disable_warnings = False\n1035 \n1036         option = Option()\n1037 \n1038     config = Config()\n1039 \n1040     assert _REPORTCHARS_DEFAULT == \"fE\"\n1041 \n1042     # Default.\n1043     assert getreportopt(config) == \"wfE\"\n1044 \n1045     config.option.reportchars = \"sf\"\n1046     assert getreportopt(config) == \"wsf\"\n1047 \n1048     config.option.reportchars = \"sfxw\"\n1049     assert getreportopt(config) == \"sfxw\"\n1050 \n1051     config.option.reportchars = \"a\"\n1052     assert getreportopt(config) == \"wsxXEf\"\n1053 \n1054     config.option.reportchars = \"N\"\n1055     assert getreportopt(config) == \"w\"\n1056 \n1057     config.option.reportchars = \"NwfE\"\n1058     assert getreportopt(config) == \"wfE\"\n1059 \n1060     config.option.reportchars = \"NfENx\"\n1061     assert getreportopt(config) == \"wx\"\n1062 \n1063     # Now with --disable-warnings.\n1064     config.option.disable_warnings = True\n1065     config.option.reportchars = \"a\"\n1066     assert getreportopt(config) == \"sxXEf\"\n1067 \n1068     config.option.reportchars = \"sfx\"\n1069     assert getreportopt(config) == \"sfx\"\n1070 \n1071     config.option.reportchars = \"sfxw\"\n1072     assert getreportopt(config) == \"sfx\"\n1073 \n1074     config.option.reportchars = \"a\"\n1075     assert getreportopt(config) == \"sxXEf\"\n1076 \n1077     config.option.reportchars = \"A\"\n1078     assert getreportopt(config) == \"PpsxXEf\"\n1079 \n1080     config.option.reportchars = \"AN\"\n1081     assert getreportopt(config) == \"\"\n1082 \n1083     config.option.reportchars = \"NwfE\"\n1084     assert getreportopt(config) == \"fE\"\n1085 \n1086 \n1087 def test_terminalreporter_reportopt_addopts(testdir):\n1088     testdir.makeini(\"[pytest]\\naddopts=-rs\")\n1089     testdir.makepyfile(\n1090         \"\"\"\n1091         import pytest\n1092 \n1093         @pytest.fixture\n1094         def tr(request):\n1095             tr = request.config.pluginmanager.getplugin(\"terminalreporter\")\n1096             return tr\n1097         def test_opt(tr):\n1098             assert tr.hasopt('skipped')\n1099             assert not tr.hasopt('qwe')\n1100     \"\"\"\n1101     )\n1102     result = testdir.runpytest()\n1103     result.stdout.fnmatch_lines([\"*1 passed*\"])\n1104 \n1105 \n1106 def test_tbstyle_short(testdir):\n1107     p = testdir.makepyfile(\n1108         \"\"\"\n1109         import pytest\n1110 \n1111         @pytest.fixture\n1112         def arg(request):\n1113             return 42\n1114         def test_opt(arg):\n1115             x = 0\n1116             assert x\n1117     \"\"\"\n1118     )\n1119     result = testdir.runpytest(\"--tb=short\")\n1120     s = result.stdout.str()\n1121     assert \"arg = 42\" not in s\n1122     assert \"x = 0\" not in s\n1123     result.stdout.fnmatch_lines([\"*%s:8*\" % p.basename, \"    assert x\", \"E   assert*\"])\n1124     result = testdir.runpytest()\n1125     s = result.stdout.str()\n1126     assert \"x = 0\" in s\n1127     assert \"assert x\" in s\n1128 \n1129 \n1130 def test_traceconfig(testdir):\n1131     result = testdir.runpytest(\"--traceconfig\")\n1132     result.stdout.fnmatch_lines([\"*active plugins*\"])\n1133     assert result.ret == ExitCode.NO_TESTS_COLLECTED\n1134 \n1135 \n1136 class TestGenericReporting:\n1137     \"\"\" this test class can be subclassed with a different option\n1138         provider to run e.g. distributed tests.\n1139     \"\"\"\n1140 \n1141     def test_collect_fail(self, testdir, option):\n1142         testdir.makepyfile(\"import xyz\\n\")\n1143         result = testdir.runpytest(*option.args)\n1144         result.stdout.fnmatch_lines(\n1145             [\"ImportError while importing*\", \"*No module named *xyz*\", \"*1 error*\"]\n1146         )\n1147 \n1148     def test_maxfailures(self, testdir, option):\n1149         testdir.makepyfile(\n1150             \"\"\"\n1151             def test_1():\n1152                 assert 0\n1153             def test_2():\n1154                 assert 0\n1155             def test_3():\n1156                 assert 0\n1157         \"\"\"\n1158         )\n1159         result = testdir.runpytest(\"--maxfail=2\", *option.args)\n1160         result.stdout.fnmatch_lines(\n1161             [\n1162                 \"*def test_1():*\",\n1163                 \"*def test_2():*\",\n1164                 \"*! stopping after 2 failures !*\",\n1165                 \"*2 failed*\",\n1166             ]\n1167         )\n1168 \n1169     def test_maxfailures_with_interrupted(self, testdir):\n1170         testdir.makepyfile(\n1171             \"\"\"\n1172             def test(request):\n1173                 request.session.shouldstop = \"session_interrupted\"\n1174                 assert 0\n1175         \"\"\"\n1176         )\n1177         result = testdir.runpytest(\"--maxfail=1\", \"-ra\")\n1178         result.stdout.fnmatch_lines(\n1179             [\n1180                 \"*= short test summary info =*\",\n1181                 \"FAILED *\",\n1182                 \"*! stopping after 1 failures !*\",\n1183                 \"*! session_interrupted !*\",\n1184                 \"*= 1 failed in*\",\n1185             ]\n1186         )\n1187 \n1188     def test_tb_option(self, testdir, option):\n1189         testdir.makepyfile(\n1190             \"\"\"\n1191             import pytest\n1192             def g():\n1193                 raise IndexError\n1194             def test_func():\n1195                 print(6*7)\n1196                 g()  # --calling--\n1197         \"\"\"\n1198         )\n1199         for tbopt in [\"long\", \"short\", \"no\"]:\n1200             print(\"testing --tb=%s...\" % tbopt)\n1201             result = testdir.runpytest(\"-rN\", \"--tb=%s\" % tbopt)\n1202             s = result.stdout.str()\n1203             if tbopt == \"long\":\n1204                 assert \"print(6*7)\" in s\n1205             else:\n1206                 assert \"print(6*7)\" not in s\n1207             if tbopt != \"no\":\n1208                 assert \"--calling--\" in s\n1209                 assert \"IndexError\" in s\n1210             else:\n1211                 assert \"FAILURES\" not in s\n1212                 assert \"--calling--\" not in s\n1213                 assert \"IndexError\" not in s\n1214 \n1215     def test_tb_crashline(self, testdir, option):\n1216         p = testdir.makepyfile(\n1217             \"\"\"\n1218             import pytest\n1219             def g():\n1220                 raise IndexError\n1221             def test_func1():\n1222                 print(6*7)\n1223                 g()  # --calling--\n1224             def test_func2():\n1225                 assert 0, \"hello\"\n1226         \"\"\"\n1227         )\n1228         result = testdir.runpytest(\"--tb=line\")\n1229         bn = p.basename\n1230         result.stdout.fnmatch_lines(\n1231             [\"*%s:3: IndexError*\" % bn, \"*%s:8: AssertionError: hello*\" % bn]\n1232         )\n1233         s = result.stdout.str()\n1234         assert \"def test_func2\" not in s\n1235 \n1236     def test_pytest_report_header(self, testdir, option):\n1237         testdir.makeconftest(\n1238             \"\"\"\n1239             def pytest_sessionstart(session):\n1240                 session.config._somevalue = 42\n1241             def pytest_report_header(config):\n1242                 return \"hello: %s\" % config._somevalue\n1243         \"\"\"\n1244         )\n1245         testdir.mkdir(\"a\").join(\"conftest.py\").write(\n1246             \"\"\"\n1247 def pytest_report_header(config, startdir):\n1248     return [\"line1\", str(startdir)]\n1249 \"\"\"\n1250         )\n1251         result = testdir.runpytest(\"a\")\n1252         result.stdout.fnmatch_lines([\"*hello: 42*\", \"line1\", str(testdir.tmpdir)])\n1253 \n1254     def test_show_capture(self, testdir):\n1255         testdir.makepyfile(\n1256             \"\"\"\n1257             import sys\n1258             import logging\n1259             def test_one():\n1260                 sys.stdout.write('!This is stdout!')\n1261                 sys.stderr.write('!This is stderr!')\n1262                 logging.warning('!This is a warning log msg!')\n1263                 assert False, 'Something failed'\n1264         \"\"\"\n1265         )\n1266 \n1267         result = testdir.runpytest(\"--tb=short\")\n1268         result.stdout.fnmatch_lines(\n1269             [\n1270                 \"!This is stdout!\",\n1271                 \"!This is stderr!\",\n1272                 \"*WARNING*!This is a warning log msg!\",\n1273             ]\n1274         )\n1275 \n1276         result = testdir.runpytest(\"--show-capture=all\", \"--tb=short\")\n1277         result.stdout.fnmatch_lines(\n1278             [\n1279                 \"!This is stdout!\",\n1280                 \"!This is stderr!\",\n1281                 \"*WARNING*!This is a warning log msg!\",\n1282             ]\n1283         )\n1284 \n1285         stdout = testdir.runpytest(\"--show-capture=stdout\", \"--tb=short\").stdout.str()\n1286         assert \"!This is stderr!\" not in stdout\n1287         assert \"!This is stdout!\" in stdout\n1288         assert \"!This is a warning log msg!\" not in stdout\n1289 \n1290         stdout = testdir.runpytest(\"--show-capture=stderr\", \"--tb=short\").stdout.str()\n1291         assert \"!This is stdout!\" not in stdout\n1292         assert \"!This is stderr!\" in stdout\n1293         assert \"!This is a warning log msg!\" not in stdout\n1294 \n1295         stdout = testdir.runpytest(\"--show-capture=log\", \"--tb=short\").stdout.str()\n1296         assert \"!This is stdout!\" not in stdout\n1297         assert \"!This is stderr!\" not in stdout\n1298         assert \"!This is a warning log msg!\" in stdout\n1299 \n1300         stdout = testdir.runpytest(\"--show-capture=no\", \"--tb=short\").stdout.str()\n1301         assert \"!This is stdout!\" not in stdout\n1302         assert \"!This is stderr!\" not in stdout\n1303         assert \"!This is a warning log msg!\" not in stdout\n1304 \n1305     def test_show_capture_with_teardown_logs(self, testdir):\n1306         \"\"\"Ensure that the capturing of teardown logs honor --show-capture setting\"\"\"\n1307         testdir.makepyfile(\n1308             \"\"\"\n1309             import logging\n1310             import sys\n1311             import pytest\n1312 \n1313             @pytest.fixture(scope=\"function\", autouse=\"True\")\n1314             def hook_each_test(request):\n1315                 yield\n1316                 sys.stdout.write(\"!stdout!\")\n1317                 sys.stderr.write(\"!stderr!\")\n1318                 logging.warning(\"!log!\")\n1319 \n1320             def test_func():\n1321                 assert False\n1322         \"\"\"\n1323         )\n1324 \n1325         result = testdir.runpytest(\"--show-capture=stdout\", \"--tb=short\").stdout.str()\n1326         assert \"!stdout!\" in result\n1327         assert \"!stderr!\" not in result\n1328         assert \"!log!\" not in result\n1329 \n1330         result = testdir.runpytest(\"--show-capture=stderr\", \"--tb=short\").stdout.str()\n1331         assert \"!stdout!\" not in result\n1332         assert \"!stderr!\" in result\n1333         assert \"!log!\" not in result\n1334 \n1335         result = testdir.runpytest(\"--show-capture=log\", \"--tb=short\").stdout.str()\n1336         assert \"!stdout!\" not in result\n1337         assert \"!stderr!\" not in result\n1338         assert \"!log!\" in result\n1339 \n1340         result = testdir.runpytest(\"--show-capture=no\", \"--tb=short\").stdout.str()\n1341         assert \"!stdout!\" not in result\n1342         assert \"!stderr!\" not in result\n1343         assert \"!log!\" not in result\n1344 \n1345 \n1346 @pytest.mark.xfail(\"not hasattr(os, 'dup')\")\n1347 def test_fdopen_kept_alive_issue124(testdir):\n1348     testdir.makepyfile(\n1349         \"\"\"\n1350         import os, sys\n1351         k = []\n1352         def test_open_file_and_keep_alive(capfd):\n1353             stdout = os.fdopen(1, 'w', 1)\n1354             k.append(stdout)\n1355 \n1356         def test_close_kept_alive_file():\n1357             stdout = k.pop()\n1358             stdout.close()\n1359     \"\"\"\n1360     )\n1361     result = testdir.runpytest()\n1362     result.stdout.fnmatch_lines([\"*2 passed*\"])\n1363 \n1364 \n1365 def test_tbstyle_native_setup_error(testdir):\n1366     testdir.makepyfile(\n1367         \"\"\"\n1368         import pytest\n1369         @pytest.fixture\n1370         def setup_error_fixture():\n1371             raise Exception(\"error in exception\")\n1372 \n1373         def test_error_fixture(setup_error_fixture):\n1374             pass\n1375     \"\"\"\n1376     )\n1377     result = testdir.runpytest(\"--tb=native\")\n1378     result.stdout.fnmatch_lines(\n1379         ['*File *test_tbstyle_native_setup_error.py\", line *, in setup_error_fixture*']\n1380     )\n1381 \n1382 \n1383 def test_terminal_summary(testdir):\n1384     testdir.makeconftest(\n1385         \"\"\"\n1386         def pytest_terminal_summary(terminalreporter, exitstatus):\n1387             w = terminalreporter\n1388             w.section(\"hello\")\n1389             w.line(\"world\")\n1390             w.line(\"exitstatus: {0}\".format(exitstatus))\n1391     \"\"\"\n1392     )\n1393     result = testdir.runpytest()\n1394     result.stdout.fnmatch_lines(\n1395         \"\"\"\n1396         *==== hello ====*\n1397         world\n1398         exitstatus: 5\n1399     \"\"\"\n1400     )\n1401 \n1402 \n1403 @pytest.mark.filterwarnings(\"default\")\n1404 def test_terminal_summary_warnings_are_displayed(testdir):\n1405     \"\"\"Test that warnings emitted during pytest_terminal_summary are displayed.\n1406     (#1305).\n1407     \"\"\"\n1408     testdir.makeconftest(\n1409         \"\"\"\n1410         import warnings\n1411         def pytest_terminal_summary(terminalreporter):\n1412             warnings.warn(UserWarning('internal warning'))\n1413     \"\"\"\n1414     )\n1415     testdir.makepyfile(\n1416         \"\"\"\n1417         def test_failure():\n1418             import warnings\n1419             warnings.warn(\"warning_from_\" + \"test\")\n1420             assert 0\n1421     \"\"\"\n1422     )\n1423     result = testdir.runpytest(\"-ra\")\n1424     result.stdout.fnmatch_lines(\n1425         [\n1426             \"*= warnings summary =*\",\n1427             \"*warning_from_test*\",\n1428             \"*= short test summary info =*\",\n1429             \"*= warnings summary (final) =*\",\n1430             \"*conftest.py:3:*internal warning\",\n1431             \"*== 1 failed, 2 warnings in *\",\n1432         ]\n1433     )\n1434     result.stdout.no_fnmatch_line(\"*None*\")\n1435     stdout = result.stdout.str()\n1436     assert stdout.count(\"warning_from_test\") == 1\n1437     assert stdout.count(\"=== warnings summary \") == 2\n1438 \n1439 \n1440 @pytest.mark.filterwarnings(\"default\")\n1441 def test_terminal_summary_warnings_header_once(testdir):\n1442     testdir.makepyfile(\n1443         \"\"\"\n1444         def test_failure():\n1445             import warnings\n1446             warnings.warn(\"warning_from_\" + \"test\")\n1447             assert 0\n1448     \"\"\"\n1449     )\n1450     result = testdir.runpytest(\"-ra\")\n1451     result.stdout.fnmatch_lines(\n1452         [\n1453             \"*= warnings summary =*\",\n1454             \"*warning_from_test*\",\n1455             \"*= short test summary info =*\",\n1456             \"*== 1 failed, 1 warning in *\",\n1457         ]\n1458     )\n1459     result.stdout.no_fnmatch_line(\"*None*\")\n1460     stdout = result.stdout.str()\n1461     assert stdout.count(\"warning_from_test\") == 1\n1462     assert stdout.count(\"=== warnings summary \") == 1\n1463 \n1464 \n1465 @pytest.fixture(scope=\"session\")\n1466 def tr() -> TerminalReporter:\n1467     config = _pytest.config._prepareconfig()\n1468     return TerminalReporter(config)\n1469 \n1470 \n1471 @pytest.mark.parametrize(\n1472     \"exp_color, exp_line, stats_arg\",\n1473     [\n1474         # The method under test only cares about the length of each\n1475         # dict value, not the actual contents, so tuples of anything\n1476         # suffice\n1477         # Important statuses -- the highest priority of these always wins\n1478         (\"red\", [(\"1 failed\", {\"bold\": True, \"red\": True})], {\"failed\": (1,)}),\n1479         (\n1480             \"red\",\n1481             [\n1482                 (\"1 failed\", {\"bold\": True, \"red\": True}),\n1483                 (\"1 passed\", {\"bold\": False, \"green\": True}),\n1484             ],\n1485             {\"failed\": (1,), \"passed\": (1,)},\n1486         ),\n1487         (\"red\", [(\"1 error\", {\"bold\": True, \"red\": True})], {\"error\": (1,)}),\n1488         (\"red\", [(\"2 errors\", {\"bold\": True, \"red\": True})], {\"error\": (1, 2)}),\n1489         (\n1490             \"red\",\n1491             [\n1492                 (\"1 passed\", {\"bold\": False, \"green\": True}),\n1493                 (\"1 error\", {\"bold\": True, \"red\": True}),\n1494             ],\n1495             {\"error\": (1,), \"passed\": (1,)},\n1496         ),\n1497         # (a status that's not known to the code)\n1498         (\"yellow\", [(\"1 weird\", {\"bold\": True, \"yellow\": True})], {\"weird\": (1,)}),\n1499         (\n1500             \"yellow\",\n1501             [\n1502                 (\"1 passed\", {\"bold\": False, \"green\": True}),\n1503                 (\"1 weird\", {\"bold\": True, \"yellow\": True}),\n1504             ],\n1505             {\"weird\": (1,), \"passed\": (1,)},\n1506         ),\n1507         (\"yellow\", [(\"1 warning\", {\"bold\": True, \"yellow\": True})], {\"warnings\": (1,)}),\n1508         (\n1509             \"yellow\",\n1510             [\n1511                 (\"1 passed\", {\"bold\": False, \"green\": True}),\n1512                 (\"1 warning\", {\"bold\": True, \"yellow\": True}),\n1513             ],\n1514             {\"warnings\": (1,), \"passed\": (1,)},\n1515         ),\n1516         (\n1517             \"green\",\n1518             [(\"5 passed\", {\"bold\": True, \"green\": True})],\n1519             {\"passed\": (1, 2, 3, 4, 5)},\n1520         ),\n1521         # \"Boring\" statuses.  These have no effect on the color of the summary\n1522         # line.  Thus, if *every* test has a boring status, the summary line stays\n1523         # at its default color, i.e. yellow, to warn the user that the test run\n1524         # produced no useful information\n1525         (\"yellow\", [(\"1 skipped\", {\"bold\": True, \"yellow\": True})], {\"skipped\": (1,)}),\n1526         (\n1527             \"green\",\n1528             [\n1529                 (\"1 passed\", {\"bold\": True, \"green\": True}),\n1530                 (\"1 skipped\", {\"bold\": False, \"yellow\": True}),\n1531             ],\n1532             {\"skipped\": (1,), \"passed\": (1,)},\n1533         ),\n1534         (\n1535             \"yellow\",\n1536             [(\"1 deselected\", {\"bold\": True, \"yellow\": True})],\n1537             {\"deselected\": (1,)},\n1538         ),\n1539         (\n1540             \"green\",\n1541             [\n1542                 (\"1 passed\", {\"bold\": True, \"green\": True}),\n1543                 (\"1 deselected\", {\"bold\": False, \"yellow\": True}),\n1544             ],\n1545             {\"deselected\": (1,), \"passed\": (1,)},\n1546         ),\n1547         (\"yellow\", [(\"1 xfailed\", {\"bold\": True, \"yellow\": True})], {\"xfailed\": (1,)}),\n1548         (\n1549             \"green\",\n1550             [\n1551                 (\"1 passed\", {\"bold\": True, \"green\": True}),\n1552                 (\"1 xfailed\", {\"bold\": False, \"yellow\": True}),\n1553             ],\n1554             {\"xfailed\": (1,), \"passed\": (1,)},\n1555         ),\n1556         (\"yellow\", [(\"1 xpassed\", {\"bold\": True, \"yellow\": True})], {\"xpassed\": (1,)}),\n1557         (\n1558             \"yellow\",\n1559             [\n1560                 (\"1 passed\", {\"bold\": False, \"green\": True}),\n1561                 (\"1 xpassed\", {\"bold\": True, \"yellow\": True}),\n1562             ],\n1563             {\"xpassed\": (1,), \"passed\": (1,)},\n1564         ),\n1565         # Likewise if no tests were found at all\n1566         (\"yellow\", [(\"no tests ran\", {\"yellow\": True})], {}),\n1567         # Test the empty-key special case\n1568         (\"yellow\", [(\"no tests ran\", {\"yellow\": True})], {\"\": (1,)}),\n1569         (\n1570             \"green\",\n1571             [(\"1 passed\", {\"bold\": True, \"green\": True})],\n1572             {\"\": (1,), \"passed\": (1,)},\n1573         ),\n1574         # A couple more complex combinations\n1575         (\n1576             \"red\",\n1577             [\n1578                 (\"1 failed\", {\"bold\": True, \"red\": True}),\n1579                 (\"2 passed\", {\"bold\": False, \"green\": True}),\n1580                 (\"3 xfailed\", {\"bold\": False, \"yellow\": True}),\n1581             ],\n1582             {\"passed\": (1, 2), \"failed\": (1,), \"xfailed\": (1, 2, 3)},\n1583         ),\n1584         (\n1585             \"green\",\n1586             [\n1587                 (\"1 passed\", {\"bold\": True, \"green\": True}),\n1588                 (\"2 skipped\", {\"bold\": False, \"yellow\": True}),\n1589                 (\"3 deselected\", {\"bold\": False, \"yellow\": True}),\n1590                 (\"2 xfailed\", {\"bold\": False, \"yellow\": True}),\n1591             ],\n1592             {\n1593                 \"passed\": (1,),\n1594                 \"skipped\": (1, 2),\n1595                 \"deselected\": (1, 2, 3),\n1596                 \"xfailed\": (1, 2),\n1597             },\n1598         ),\n1599     ],\n1600 )\n1601 def test_summary_stats(\n1602     tr: TerminalReporter,\n1603     exp_line: List[Tuple[str, Dict[str, bool]]],\n1604     exp_color: str,\n1605     stats_arg: Dict[str, List],\n1606 ) -> None:\n1607     tr.stats = stats_arg\n1608 \n1609     # Fake \"_is_last_item\" to be True.\n1610     class fake_session:\n1611         testscollected = 0\n1612 \n1613     tr._session = fake_session  # type: ignore[assignment]  # noqa: F821\n1614     assert tr._is_last_item\n1615 \n1616     # Reset cache.\n1617     tr._main_color = None\n1618 \n1619     print(\"Based on stats: %s\" % stats_arg)\n1620     print('Expect summary: \"{}\"; with color \"{}\"'.format(exp_line, exp_color))\n1621     (line, color) = tr.build_summary_stats_line()\n1622     print('Actually got:   \"{}\"; with color \"{}\"'.format(line, color))\n1623     assert line == exp_line\n1624     assert color == exp_color\n1625 \n1626 \n1627 def test_skip_counting_towards_summary(tr):\n1628     class DummyReport(BaseReport):\n1629         count_towards_summary = True\n1630 \n1631     r1 = DummyReport()\n1632     r2 = DummyReport()\n1633     tr.stats = {\"failed\": (r1, r2)}\n1634     tr._main_color = None\n1635     res = tr.build_summary_stats_line()\n1636     assert res == ([(\"2 failed\", {\"bold\": True, \"red\": True})], \"red\")\n1637 \n1638     r1.count_towards_summary = False\n1639     tr.stats = {\"failed\": (r1, r2)}\n1640     tr._main_color = None\n1641     res = tr.build_summary_stats_line()\n1642     assert res == ([(\"1 failed\", {\"bold\": True, \"red\": True})], \"red\")\n1643 \n1644 \n1645 class TestClassicOutputStyle:\n1646     \"\"\"Ensure classic output style works as expected (#3883)\"\"\"\n1647 \n1648     @pytest.fixture\n1649     def test_files(self, testdir):\n1650         testdir.makepyfile(\n1651             **{\n1652                 \"test_one.py\": \"def test_one(): pass\",\n1653                 \"test_two.py\": \"def test_two(): assert 0\",\n1654                 \"sub/test_three.py\": \"\"\"\n1655                     def test_three_1(): pass\n1656                     def test_three_2(): assert 0\n1657                     def test_three_3(): pass\n1658                 \"\"\",\n1659             }\n1660         )\n1661 \n1662     def test_normal_verbosity(self, testdir, test_files):\n1663         result = testdir.runpytest(\"-o\", \"console_output_style=classic\")\n1664         result.stdout.fnmatch_lines(\n1665             [\n1666                 \"test_one.py .\",\n1667                 \"test_two.py F\",\n1668                 \"sub{}test_three.py .F.\".format(os.sep),\n1669                 \"*2 failed, 3 passed in*\",\n1670             ]\n1671         )\n1672 \n1673     def test_verbose(self, testdir, test_files):\n1674         result = testdir.runpytest(\"-o\", \"console_output_style=classic\", \"-v\")\n1675         result.stdout.fnmatch_lines(\n1676             [\n1677                 \"test_one.py::test_one PASSED\",\n1678                 \"test_two.py::test_two FAILED\",\n1679                 \"sub{}test_three.py::test_three_1 PASSED\".format(os.sep),\n1680                 \"sub{}test_three.py::test_three_2 FAILED\".format(os.sep),\n1681                 \"sub{}test_three.py::test_three_3 PASSED\".format(os.sep),\n1682                 \"*2 failed, 3 passed in*\",\n1683             ]\n1684         )\n1685 \n1686     def test_quiet(self, testdir, test_files):\n1687         result = testdir.runpytest(\"-o\", \"console_output_style=classic\", \"-q\")\n1688         result.stdout.fnmatch_lines([\".F.F.\", \"*2 failed, 3 passed in*\"])\n1689 \n1690 \n1691 class TestProgressOutputStyle:\n1692     @pytest.fixture\n1693     def many_tests_files(self, testdir):\n1694         testdir.makepyfile(\n1695             test_bar=\"\"\"\n1696                 import pytest\n1697                 @pytest.mark.parametrize('i', range(10))\n1698                 def test_bar(i): pass\n1699             \"\"\",\n1700             test_foo=\"\"\"\n1701                 import pytest\n1702                 @pytest.mark.parametrize('i', range(5))\n1703                 def test_foo(i): pass\n1704             \"\"\",\n1705             test_foobar=\"\"\"\n1706                 import pytest\n1707                 @pytest.mark.parametrize('i', range(5))\n1708                 def test_foobar(i): pass\n1709             \"\"\",\n1710         )\n1711 \n1712     def test_zero_tests_collected(self, testdir):\n1713         \"\"\"Some plugins (testmon for example) might issue pytest_runtest_logreport without any tests being\n1714         actually collected (#2971).\"\"\"\n1715         testdir.makeconftest(\n1716             \"\"\"\n1717         def pytest_collection_modifyitems(items, config):\n1718             from _pytest.runner import CollectReport\n1719             for node_id in ('nodeid1', 'nodeid2'):\n1720                 rep = CollectReport(node_id, 'passed', None, None)\n1721                 rep.when = 'passed'\n1722                 rep.duration = 0.1\n1723                 config.hook.pytest_runtest_logreport(report=rep)\n1724         \"\"\"\n1725         )\n1726         output = testdir.runpytest()\n1727         output.stdout.no_fnmatch_line(\"*ZeroDivisionError*\")\n1728         output.stdout.fnmatch_lines([\"=* 2 passed in *=\"])\n1729 \n1730     def test_normal(self, many_tests_files, testdir):\n1731         output = testdir.runpytest()\n1732         output.stdout.re_match_lines(\n1733             [\n1734                 r\"test_bar.py \\.{10} \\s+ \\[ 50%\\]\",\n1735                 r\"test_foo.py \\.{5} \\s+ \\[ 75%\\]\",\n1736                 r\"test_foobar.py \\.{5} \\s+ \\[100%\\]\",\n1737             ]\n1738         )\n1739 \n1740     def test_colored_progress(self, testdir, monkeypatch, color_mapping):\n1741         monkeypatch.setenv(\"PY_COLORS\", \"1\")\n1742         testdir.makepyfile(\n1743             test_axfail=\"\"\"\n1744                 import pytest\n1745                 @pytest.mark.xfail\n1746                 def test_axfail(): assert 0\n1747             \"\"\",\n1748             test_bar=\"\"\"\n1749                 import pytest\n1750                 @pytest.mark.parametrize('i', range(10))\n1751                 def test_bar(i): pass\n1752             \"\"\",\n1753             test_foo=\"\"\"\n1754                 import pytest\n1755                 import warnings\n1756                 @pytest.mark.parametrize('i', range(5))\n1757                 def test_foo(i):\n1758                     warnings.warn(DeprecationWarning(\"collection\"))\n1759                     pass\n1760             \"\"\",\n1761             test_foobar=\"\"\"\n1762                 import pytest\n1763                 @pytest.mark.parametrize('i', range(5))\n1764                 def test_foobar(i): raise ValueError()\n1765             \"\"\",\n1766         )\n1767         result = testdir.runpytest()\n1768         result.stdout.re_match_lines(\n1769             color_mapping.format_for_rematch(\n1770                 [\n1771                     r\"test_axfail.py {yellow}x{reset}{green} \\s+ \\[  4%\\]{reset}\",\n1772                     r\"test_bar.py ({green}\\.{reset}){{10}}{green} \\s+ \\[ 52%\\]{reset}\",\n1773                     r\"test_foo.py ({green}\\.{reset}){{5}}{yellow} \\s+ \\[ 76%\\]{reset}\",\n1774                     r\"test_foobar.py ({red}F{reset}){{5}}{red} \\s+ \\[100%\\]{reset}\",\n1775                 ]\n1776             )\n1777         )\n1778 \n1779         # Only xfail should have yellow progress indicator.\n1780         result = testdir.runpytest(\"test_axfail.py\")\n1781         result.stdout.re_match_lines(\n1782             color_mapping.format_for_rematch(\n1783                 [\n1784                     r\"test_axfail.py {yellow}x{reset}{yellow} \\s+ \\[100%\\]{reset}\",\n1785                     r\"^{yellow}=+ ({yellow}{bold}|{bold}{yellow})1 xfailed{reset}{yellow} in \",\n1786                 ]\n1787             )\n1788         )\n1789 \n1790     def test_count(self, many_tests_files, testdir):\n1791         testdir.makeini(\n1792             \"\"\"\n1793             [pytest]\n1794             console_output_style = count\n1795         \"\"\"\n1796         )\n1797         output = testdir.runpytest()\n1798         output.stdout.re_match_lines(\n1799             [\n1800                 r\"test_bar.py \\.{10} \\s+ \\[10/20\\]\",\n1801                 r\"test_foo.py \\.{5} \\s+ \\[15/20\\]\",\n1802                 r\"test_foobar.py \\.{5} \\s+ \\[20/20\\]\",\n1803             ]\n1804         )\n1805 \n1806     def test_verbose(self, many_tests_files, testdir):\n1807         output = testdir.runpytest(\"-v\")\n1808         output.stdout.re_match_lines(\n1809             [\n1810                 r\"test_bar.py::test_bar\\[0\\] PASSED \\s+ \\[  5%\\]\",\n1811                 r\"test_foo.py::test_foo\\[4\\] PASSED \\s+ \\[ 75%\\]\",\n1812                 r\"test_foobar.py::test_foobar\\[4\\] PASSED \\s+ \\[100%\\]\",\n1813             ]\n1814         )\n1815 \n1816     def test_verbose_count(self, many_tests_files, testdir):\n1817         testdir.makeini(\n1818             \"\"\"\n1819             [pytest]\n1820             console_output_style = count\n1821         \"\"\"\n1822         )\n1823         output = testdir.runpytest(\"-v\")\n1824         output.stdout.re_match_lines(\n1825             [\n1826                 r\"test_bar.py::test_bar\\[0\\] PASSED \\s+ \\[ 1/20\\]\",\n1827                 r\"test_foo.py::test_foo\\[4\\] PASSED \\s+ \\[15/20\\]\",\n1828                 r\"test_foobar.py::test_foobar\\[4\\] PASSED \\s+ \\[20/20\\]\",\n1829             ]\n1830         )\n1831 \n1832     def test_xdist_normal(self, many_tests_files, testdir, monkeypatch):\n1833         pytest.importorskip(\"xdist\")\n1834         monkeypatch.delenv(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\", raising=False)\n1835         output = testdir.runpytest(\"-n2\")\n1836         output.stdout.re_match_lines([r\"\\.{20} \\s+ \\[100%\\]\"])\n1837 \n1838     def test_xdist_normal_count(self, many_tests_files, testdir, monkeypatch):\n1839         pytest.importorskip(\"xdist\")\n1840         monkeypatch.delenv(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\", raising=False)\n1841         testdir.makeini(\n1842             \"\"\"\n1843             [pytest]\n1844             console_output_style = count\n1845         \"\"\"\n1846         )\n1847         output = testdir.runpytest(\"-n2\")\n1848         output.stdout.re_match_lines([r\"\\.{20} \\s+ \\[20/20\\]\"])\n1849 \n1850     def test_xdist_verbose(self, many_tests_files, testdir, monkeypatch):\n1851         pytest.importorskip(\"xdist\")\n1852         monkeypatch.delenv(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\", raising=False)\n1853         output = testdir.runpytest(\"-n2\", \"-v\")\n1854         output.stdout.re_match_lines_random(\n1855             [\n1856                 r\"\\[gw\\d\\] \\[\\s*\\d+%\\] PASSED test_bar.py::test_bar\\[1\\]\",\n1857                 r\"\\[gw\\d\\] \\[\\s*\\d+%\\] PASSED test_foo.py::test_foo\\[1\\]\",\n1858                 r\"\\[gw\\d\\] \\[\\s*\\d+%\\] PASSED test_foobar.py::test_foobar\\[1\\]\",\n1859             ]\n1860         )\n1861         output.stdout.fnmatch_lines_random(\n1862             [\n1863                 line.translate(TRANS_FNMATCH)\n1864                 for line in [\n1865                     \"test_bar.py::test_bar[0] \",\n1866                     \"test_foo.py::test_foo[0] \",\n1867                     \"test_foobar.py::test_foobar[0] \",\n1868                     \"[gw?] [  5%] PASSED test_*[?] \",\n1869                     \"[gw?] [ 10%] PASSED test_*[?] \",\n1870                     \"[gw?] [ 55%] PASSED test_*[?] \",\n1871                     \"[gw?] [ 60%] PASSED test_*[?] \",\n1872                     \"[gw?] [ 95%] PASSED test_*[?] \",\n1873                     \"[gw?] [100%] PASSED test_*[?] \",\n1874                 ]\n1875             ]\n1876         )\n1877 \n1878     def test_capture_no(self, many_tests_files, testdir):\n1879         output = testdir.runpytest(\"-s\")\n1880         output.stdout.re_match_lines(\n1881             [r\"test_bar.py \\.{10}\", r\"test_foo.py \\.{5}\", r\"test_foobar.py \\.{5}\"]\n1882         )\n1883 \n1884         output = testdir.runpytest(\"--capture=no\")\n1885         output.stdout.no_fnmatch_line(\"*%]*\")\n1886 \n1887 \n1888 class TestProgressWithTeardown:\n1889     \"\"\"Ensure we show the correct percentages for tests that fail during teardown (#3088)\"\"\"\n1890 \n1891     @pytest.fixture\n1892     def contest_with_teardown_fixture(self, testdir):\n1893         testdir.makeconftest(\n1894             \"\"\"\n1895             import pytest\n1896 \n1897             @pytest.fixture\n1898             def fail_teardown():\n1899                 yield\n1900                 assert False\n1901         \"\"\"\n1902         )\n1903 \n1904     @pytest.fixture\n1905     def many_files(self, testdir, contest_with_teardown_fixture):\n1906         testdir.makepyfile(\n1907             test_bar=\"\"\"\n1908                 import pytest\n1909                 @pytest.mark.parametrize('i', range(5))\n1910                 def test_bar(fail_teardown, i):\n1911                     pass\n1912             \"\"\",\n1913             test_foo=\"\"\"\n1914                 import pytest\n1915                 @pytest.mark.parametrize('i', range(15))\n1916                 def test_foo(fail_teardown, i):\n1917                     pass\n1918             \"\"\",\n1919         )\n1920 \n1921     def test_teardown_simple(self, testdir, contest_with_teardown_fixture):\n1922         testdir.makepyfile(\n1923             \"\"\"\n1924             def test_foo(fail_teardown):\n1925                 pass\n1926         \"\"\"\n1927         )\n1928         output = testdir.runpytest()\n1929         output.stdout.re_match_lines([r\"test_teardown_simple.py \\.E\\s+\\[100%\\]\"])\n1930 \n1931     def test_teardown_with_test_also_failing(\n1932         self, testdir, contest_with_teardown_fixture\n1933     ):\n1934         testdir.makepyfile(\n1935             \"\"\"\n1936             def test_foo(fail_teardown):\n1937                 assert 0\n1938         \"\"\"\n1939         )\n1940         output = testdir.runpytest(\"-rfE\")\n1941         output.stdout.re_match_lines(\n1942             [\n1943                 r\"test_teardown_with_test_also_failing.py FE\\s+\\[100%\\]\",\n1944                 \"FAILED test_teardown_with_test_also_failing.py::test_foo - assert 0\",\n1945                 \"ERROR test_teardown_with_test_also_failing.py::test_foo - assert False\",\n1946             ]\n1947         )\n1948 \n1949     def test_teardown_many(self, testdir, many_files):\n1950         output = testdir.runpytest()\n1951         output.stdout.re_match_lines(\n1952             [r\"test_bar.py (\\.E){5}\\s+\\[ 25%\\]\", r\"test_foo.py (\\.E){15}\\s+\\[100%\\]\"]\n1953         )\n1954 \n1955     def test_teardown_many_verbose(\n1956         self, testdir: Testdir, many_files, color_mapping\n1957     ) -> None:\n1958         result = testdir.runpytest(\"-v\")\n1959         result.stdout.fnmatch_lines(\n1960             color_mapping.format_for_fnmatch(\n1961                 [\n1962                     \"test_bar.py::test_bar[0] PASSED  * [  5%]\",\n1963                     \"test_bar.py::test_bar[0] ERROR   * [  5%]\",\n1964                     \"test_bar.py::test_bar[4] PASSED  * [ 25%]\",\n1965                     \"test_foo.py::test_foo[14] PASSED * [100%]\",\n1966                     \"test_foo.py::test_foo[14] ERROR  * [100%]\",\n1967                     \"=* 20 passed, 20 errors in *\",\n1968                 ]\n1969             )\n1970         )\n1971 \n1972     def test_xdist_normal(self, many_files, testdir, monkeypatch):\n1973         pytest.importorskip(\"xdist\")\n1974         monkeypatch.delenv(\"PYTEST_DISABLE_PLUGIN_AUTOLOAD\", raising=False)\n1975         output = testdir.runpytest(\"-n2\")\n1976         output.stdout.re_match_lines([r\"[\\.E]{40} \\s+ \\[100%\\]\"])\n1977 \n1978 \n1979 def test_skip_reasons_folding():\n1980     path = \"xyz\"\n1981     lineno = 3\n1982     message = \"justso\"\n1983     longrepr = (path, lineno, message)\n1984 \n1985     class X:\n1986         pass\n1987 \n1988     ev1 = X()\n1989     ev1.when = \"execute\"\n1990     ev1.skipped = True\n1991     ev1.longrepr = longrepr\n1992 \n1993     ev2 = X()\n1994     ev2.when = \"execute\"\n1995     ev2.longrepr = longrepr\n1996     ev2.skipped = True\n1997 \n1998     # ev3 might be a collection report\n1999     ev3 = X()\n2000     ev3.when = \"collect\"\n2001     ev3.longrepr = longrepr\n2002     ev3.skipped = True\n2003 \n2004     values = _folded_skips(py.path.local(), [ev1, ev2, ev3])\n2005     assert len(values) == 1\n2006     num, fspath, lineno, reason = values[0]\n2007     assert num == 3\n2008     assert fspath == path\n2009     assert lineno == lineno\n2010     assert reason == message\n2011 \n2012 \n2013 def test_line_with_reprcrash(monkeypatch):\n2014     import _pytest.terminal\n2015     from wcwidth import wcswidth\n2016 \n2017     mocked_verbose_word = \"FAILED\"\n2018 \n2019     mocked_pos = \"some::nodeid\"\n2020 \n2021     def mock_get_pos(*args):\n2022         return mocked_pos\n2023 \n2024     monkeypatch.setattr(_pytest.terminal, \"_get_pos\", mock_get_pos)\n2025 \n2026     class config:\n2027         pass\n2028 \n2029     class rep:\n2030         def _get_verbose_word(self, *args):\n2031             return mocked_verbose_word\n2032 \n2033         class longrepr:\n2034             class reprcrash:\n2035                 pass\n2036 \n2037     def check(msg, width, expected):\n2038         __tracebackhide__ = True\n2039         if msg:\n2040             rep.longrepr.reprcrash.message = msg\n2041         actual = _get_line_with_reprcrash_message(config, rep(), width)\n2042 \n2043         assert actual == expected\n2044         if actual != \"{} {}\".format(mocked_verbose_word, mocked_pos):\n2045             assert len(actual) <= width\n2046             assert wcswidth(actual) <= width\n2047 \n2048     # AttributeError with message\n2049     check(None, 80, \"FAILED some::nodeid\")\n2050 \n2051     check(\"msg\", 80, \"FAILED some::nodeid - msg\")\n2052     check(\"msg\", 3, \"FAILED some::nodeid\")\n2053 \n2054     check(\"msg\", 24, \"FAILED some::nodeid\")\n2055     check(\"msg\", 25, \"FAILED some::nodeid - msg\")\n2056 \n2057     check(\"some longer msg\", 24, \"FAILED some::nodeid\")\n2058     check(\"some longer msg\", 25, \"FAILED some::nodeid - ...\")\n2059     check(\"some longer msg\", 26, \"FAILED some::nodeid - s...\")\n2060 \n2061     check(\"some\\nmessage\", 25, \"FAILED some::nodeid - ...\")\n2062     check(\"some\\nmessage\", 26, \"FAILED some::nodeid - some\")\n2063     check(\"some\\nmessage\", 80, \"FAILED some::nodeid - some\")\n2064 \n2065     # Test unicode safety.\n2066     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 25, \"FAILED some::nodeid - ...\")\n2067     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 26, \"FAILED some::nodeid - ...\")\n2068     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 27, \"FAILED some::nodeid - \ud83d\ude04...\")\n2069     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 28, \"FAILED some::nodeid - \ud83d\ude04...\")\n2070     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 29, \"FAILED some::nodeid - \ud83d\ude04\ud83d\ude04...\")\n2071 \n2072     # NOTE: constructed, not sure if this is supported.\n2073     mocked_pos = \"nodeid::\ud83d\ude04::withunicode\"\n2074     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 29, \"FAILED nodeid::\ud83d\ude04::withunicode\")\n2075     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 40, \"FAILED nodeid::\ud83d\ude04::withunicode - \ud83d\ude04\ud83d\ude04...\")\n2076     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 41, \"FAILED nodeid::\ud83d\ude04::withunicode - \ud83d\ude04\ud83d\ude04...\")\n2077     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 42, \"FAILED nodeid::\ud83d\ude04::withunicode - \ud83d\ude04\ud83d\ude04\ud83d\ude04...\")\n2078     check(\"\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\\n2nd line\", 80, \"FAILED nodeid::\ud83d\ude04::withunicode - \ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\ud83d\ude04\")\n2079 \n2080 \n2081 @pytest.mark.parametrize(\n2082     \"seconds, expected\",\n2083     [\n2084         (10.0, \"10.00s\"),\n2085         (10.34, \"10.34s\"),\n2086         (59.99, \"59.99s\"),\n2087         (60.55, \"60.55s (0:01:00)\"),\n2088         (123.55, \"123.55s (0:02:03)\"),\n2089         (60 * 60 + 0.5, \"3600.50s (1:00:00)\"),\n2090     ],\n2091 )\n2092 def test_format_session_duration(seconds, expected):\n2093     from _pytest.terminal import format_session_duration\n2094 \n2095     assert format_session_duration(seconds) == expected\n2096 \n2097 \n2098 def test_collecterror(testdir):\n2099     p1 = testdir.makepyfile(\"raise SyntaxError()\")\n2100     result = testdir.runpytest(\"-ra\", str(p1))\n2101     result.stdout.fnmatch_lines(\n2102         [\n2103             \"collected 0 items / 1 error\",\n2104             \"*= ERRORS =*\",\n2105             \"*_ ERROR collecting test_collecterror.py _*\",\n2106             \"E   SyntaxError: *\",\n2107             \"*= short test summary info =*\",\n2108             \"ERROR test_collecterror.py\",\n2109             \"*! Interrupted: 1 error during collection !*\",\n2110             \"*= 1 error in *\",\n2111         ]\n2112     )\n2113 \n2114 \n2115 def test_via_exec(testdir: Testdir) -> None:\n2116     p1 = testdir.makepyfile(\"exec('def test_via_exec(): pass')\")\n2117     result = testdir.runpytest(str(p1), \"-vv\")\n2118     result.stdout.fnmatch_lines(\n2119         [\"test_via_exec.py::test_via_exec <- <string> PASSED*\", \"*= 1 passed in *\"]\n2120     )\n2121 \n2122 \n2123 class TestCodeHighlight:\n2124     def test_code_highlight_simple(self, testdir: Testdir, color_mapping) -> None:\n2125         testdir.makepyfile(\n2126             \"\"\"\n2127             def test_foo():\n2128                 assert 1 == 10\n2129         \"\"\"\n2130         )\n2131         result = testdir.runpytest(\"--color=yes\")\n2132         color_mapping.requires_ordered_markup(result)\n2133         result.stdout.fnmatch_lines(\n2134             color_mapping.format_for_fnmatch(\n2135                 [\n2136                     \"    {kw}def{hl-reset} {function}test_foo{hl-reset}():\",\n2137                     \">       {kw}assert{hl-reset} {number}1{hl-reset} == {number}10{hl-reset}\",\n2138                     \"{bold}{red}E       assert 1 == 10{reset}\",\n2139                 ]\n2140             )\n2141         )\n2142 \n2143     def test_code_highlight_continuation(self, testdir: Testdir, color_mapping) -> None:\n2144         testdir.makepyfile(\n2145             \"\"\"\n2146             def test_foo():\n2147                 print('''\n2148                 '''); assert 0\n2149         \"\"\"\n2150         )\n2151         result = testdir.runpytest(\"--color=yes\")\n2152         color_mapping.requires_ordered_markup(result)\n2153 \n2154         result.stdout.fnmatch_lines(\n2155             color_mapping.format_for_fnmatch(\n2156                 [\n2157                     \"    {kw}def{hl-reset} {function}test_foo{hl-reset}():\",\n2158                     \"        {print}print{hl-reset}({str}'''{hl-reset}{str}{hl-reset}\",\n2159                     \">   {str}    {hl-reset}{str}'''{hl-reset}); {kw}assert{hl-reset} {number}0{hl-reset}\",\n2160                     \"{bold}{red}E       assert 0{reset}\",\n2161                 ]\n2162             )\n2163         )\n",
        "intended_behavior": "This file needs to be modified to correctly format and display the relative path of skipped tests, ensuring that paths like '../' are prepended for tests outside the invocation directory without affecting other functionalities."
    },
    {
        "rel_file_path": "src/_pytest/runner.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pytest-dev__pytest/setup_pytest-dev__pytest__5.4/src/_pytest/runner.py",
        "start": 1,
        "end": 411,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\" basic collect and runtest protocol implementations \"\"\"\n2 import bdb\n3 import os\n4 import sys\n5 from time import perf_counter  # Intentionally not `import time` to avoid being\n6 from time import time  # affected by tests which monkeypatch `time` (issue #185).\n7 from typing import Callable\n8 from typing import Dict\n9 from typing import List\n10 from typing import Optional\n11 from typing import Tuple\n12 \n13 import attr\n14 \n15 from .reports import CollectErrorRepr\n16 from .reports import CollectReport\n17 from .reports import TestReport\n18 from _pytest._code.code import ExceptionChainRepr\n19 from _pytest._code.code import ExceptionInfo\n20 from _pytest.compat import TYPE_CHECKING\n21 from _pytest.nodes import Collector\n22 from _pytest.nodes import Node\n23 from _pytest.outcomes import Exit\n24 from _pytest.outcomes import Skipped\n25 from _pytest.outcomes import TEST_OUTCOME\n26 \n27 if TYPE_CHECKING:\n28     from typing import Type\n29     from typing_extensions import Literal\n30 \n31 #\n32 # pytest plugin hooks\n33 \n34 \n35 def pytest_addoption(parser):\n36     group = parser.getgroup(\"terminal reporting\", \"reporting\", after=\"general\")\n37     group.addoption(\n38         \"--durations\",\n39         action=\"store\",\n40         type=int,\n41         default=None,\n42         metavar=\"N\",\n43         help=\"show N slowest setup/test durations (N=0 for all).\",\n44     )\n45 \n46 \n47 def pytest_terminal_summary(terminalreporter):\n48     durations = terminalreporter.config.option.durations\n49     verbose = terminalreporter.config.getvalue(\"verbose\")\n50     if durations is None:\n51         return\n52     tr = terminalreporter\n53     dlist = []\n54     for replist in tr.stats.values():\n55         for rep in replist:\n56             if hasattr(rep, \"duration\"):\n57                 dlist.append(rep)\n58     if not dlist:\n59         return\n60     dlist.sort(key=lambda x: x.duration)\n61     dlist.reverse()\n62     if not durations:\n63         tr.write_sep(\"=\", \"slowest durations\")\n64     else:\n65         tr.write_sep(\"=\", \"slowest %s durations\" % durations)\n66         dlist = dlist[:durations]\n67 \n68     for i, rep in enumerate(dlist):\n69         if verbose < 2 and rep.duration < 0.005:\n70             tr.write_line(\"\")\n71             tr.write_line(\n72                 \"(%s durations < 0.005s hidden.  Use -vv to show these durations.)\"\n73                 % (len(dlist) - i)\n74             )\n75             break\n76         tr.write_line(\"{:02.2f}s {:<8} {}\".format(rep.duration, rep.when, rep.nodeid))\n77 \n78 \n79 def pytest_sessionstart(session):\n80     session._setupstate = SetupState()\n81 \n82 \n83 def pytest_sessionfinish(session):\n84     session._setupstate.teardown_all()\n85 \n86 \n87 def pytest_runtest_protocol(item, nextitem):\n88     item.ihook.pytest_runtest_logstart(nodeid=item.nodeid, location=item.location)\n89     runtestprotocol(item, nextitem=nextitem)\n90     item.ihook.pytest_runtest_logfinish(nodeid=item.nodeid, location=item.location)\n91     return True\n92 \n93 \n94 def runtestprotocol(item, log=True, nextitem=None):\n95     hasrequest = hasattr(item, \"_request\")\n96     if hasrequest and not item._request:\n97         item._initrequest()\n98     rep = call_and_report(item, \"setup\", log)\n99     reports = [rep]\n100     if rep.passed:\n101         if item.config.getoption(\"setupshow\", False):\n102             show_test_item(item)\n103         if not item.config.getoption(\"setuponly\", False):\n104             reports.append(call_and_report(item, \"call\", log))\n105     reports.append(call_and_report(item, \"teardown\", log, nextitem=nextitem))\n106     # after all teardown hooks have been called\n107     # want funcargs and request info to go away\n108     if hasrequest:\n109         item._request = False\n110         item.funcargs = None\n111     return reports\n112 \n113 \n114 def show_test_item(item):\n115     \"\"\"Show test function, parameters and the fixtures of the test item.\"\"\"\n116     tw = item.config.get_terminal_writer()\n117     tw.line()\n118     tw.write(\" \" * 8)\n119     tw.write(item.nodeid)\n120     used_fixtures = sorted(getattr(item, \"fixturenames\", []))\n121     if used_fixtures:\n122         tw.write(\" (fixtures used: {})\".format(\", \".join(used_fixtures)))\n123 \n124 \n125 def pytest_runtest_setup(item):\n126     _update_current_test_var(item, \"setup\")\n127     item.session._setupstate.prepare(item)\n128 \n129 \n130 def pytest_runtest_call(item):\n131     _update_current_test_var(item, \"call\")\n132     try:\n133         del sys.last_type\n134         del sys.last_value\n135         del sys.last_traceback\n136     except AttributeError:\n137         pass\n138     try:\n139         item.runtest()\n140     except Exception as e:\n141         # Store trace info to allow postmortem debugging\n142         sys.last_type = type(e)\n143         sys.last_value = e\n144         assert e.__traceback__ is not None\n145         # Skip *this* frame\n146         sys.last_traceback = e.__traceback__.tb_next\n147         raise e\n148 \n149 \n150 def pytest_runtest_teardown(item, nextitem):\n151     _update_current_test_var(item, \"teardown\")\n152     item.session._setupstate.teardown_exact(item, nextitem)\n153     _update_current_test_var(item, None)\n154 \n155 \n156 def _update_current_test_var(item, when):\n157     \"\"\"\n158     Update :envvar:`PYTEST_CURRENT_TEST` to reflect the current item and stage.\n159 \n160     If ``when`` is None, delete ``PYTEST_CURRENT_TEST`` from the environment.\n161     \"\"\"\n162     var_name = \"PYTEST_CURRENT_TEST\"\n163     if when:\n164         value = \"{} ({})\".format(item.nodeid, when)\n165         # don't allow null bytes on environment variables (see #2644, #2957)\n166         value = value.replace(\"\\x00\", \"(null)\")\n167         os.environ[var_name] = value\n168     else:\n169         os.environ.pop(var_name)\n170 \n171 \n172 def pytest_report_teststatus(report):\n173     if report.when in (\"setup\", \"teardown\"):\n174         if report.failed:\n175             #      category, shortletter, verbose-word\n176             return \"error\", \"E\", \"ERROR\"\n177         elif report.skipped:\n178             return \"skipped\", \"s\", \"SKIPPED\"\n179         else:\n180             return \"\", \"\", \"\"\n181 \n182 \n183 #\n184 # Implementation\n185 \n186 \n187 def call_and_report(\n188     item, when: \"Literal['setup', 'call', 'teardown']\", log=True, **kwds\n189 ):\n190     call = call_runtest_hook(item, when, **kwds)\n191     hook = item.ihook\n192     report = hook.pytest_runtest_makereport(item=item, call=call)\n193     if log:\n194         hook.pytest_runtest_logreport(report=report)\n195     if check_interactive_exception(call, report):\n196         hook.pytest_exception_interact(node=item, call=call, report=report)\n197     return report\n198 \n199 \n200 def check_interactive_exception(call, report):\n201     return call.excinfo and not (\n202         hasattr(report, \"wasxfail\")\n203         or call.excinfo.errisinstance(Skipped)\n204         or call.excinfo.errisinstance(bdb.BdbQuit)\n205     )\n206 \n207 \n208 def call_runtest_hook(item, when: \"Literal['setup', 'call', 'teardown']\", **kwds):\n209     if when == \"setup\":\n210         ihook = item.ihook.pytest_runtest_setup\n211     elif when == \"call\":\n212         ihook = item.ihook.pytest_runtest_call\n213     elif when == \"teardown\":\n214         ihook = item.ihook.pytest_runtest_teardown\n215     else:\n216         assert False, \"Unhandled runtest hook case: {}\".format(when)\n217     reraise = (Exit,)  # type: Tuple[Type[BaseException], ...]\n218     if not item.config.getoption(\"usepdb\", False):\n219         reraise += (KeyboardInterrupt,)\n220     return CallInfo.from_call(\n221         lambda: ihook(item=item, **kwds), when=when, reraise=reraise\n222     )\n223 \n224 \n225 @attr.s(repr=False)\n226 class CallInfo:\n227     \"\"\" Result/Exception info a function invocation.\n228 \n229     :param result: The return value of the call, if it didn't raise. Can only be accessed\n230         if excinfo is None.\n231     :param Optional[ExceptionInfo] excinfo: The captured exception of the call, if it raised.\n232     :param float start: The system time when the call started, in seconds since the epoch.\n233     :param float stop: The system time when the call ended, in seconds since the epoch.\n234     :param float duration: The call duration, in seconds.\n235     :param str when: The context of invocation: \"setup\", \"call\", \"teardown\", ...\n236     \"\"\"\n237 \n238     _result = attr.ib()\n239     excinfo = attr.ib(type=Optional[ExceptionInfo])\n240     start = attr.ib(type=float)\n241     stop = attr.ib(type=float)\n242     duration = attr.ib(type=float)\n243     when = attr.ib(type=str)\n244 \n245     @property\n246     def result(self):\n247         if self.excinfo is not None:\n248             raise AttributeError(\"{!r} has no valid result\".format(self))\n249         return self._result\n250 \n251     @classmethod\n252     def from_call(cls, func, when, reraise=None) -> \"CallInfo\":\n253         #: context of invocation: one of \"setup\", \"call\",\n254         #: \"teardown\", \"memocollect\"\n255         excinfo = None\n256         start = time()\n257         precise_start = perf_counter()\n258         try:\n259             result = func()\n260         except:  # noqa\n261             excinfo = ExceptionInfo.from_current()\n262             if reraise is not None and excinfo.errisinstance(reraise):\n263                 raise\n264             result = None\n265         # use the perf counter\n266         precise_stop = perf_counter()\n267         duration = precise_stop - precise_start\n268         stop = time()\n269         return cls(\n270             start=start,\n271             stop=stop,\n272             duration=duration,\n273             when=when,\n274             result=result,\n275             excinfo=excinfo,\n276         )\n277 \n278     def __repr__(self):\n279         if self.excinfo is None:\n280             return \"<CallInfo when={!r} result: {!r}>\".format(self.when, self._result)\n281         return \"<CallInfo when={!r} excinfo={!r}>\".format(self.when, self.excinfo)\n282 \n283 \n284 def pytest_runtest_makereport(item, call):\n285     return TestReport.from_item_and_call(item, call)\n286 \n287 \n288 def pytest_make_collect_report(collector: Collector) -> CollectReport:\n289     call = CallInfo.from_call(lambda: list(collector.collect()), \"collect\")\n290     longrepr = None\n291     if not call.excinfo:\n292         outcome = \"passed\"\n293     else:\n294         skip_exceptions = [Skipped]\n295         unittest = sys.modules.get(\"unittest\")\n296         if unittest is not None:\n297             # Type ignored because unittest is loaded dynamically.\n298             skip_exceptions.append(unittest.SkipTest)  # type: ignore\n299         if call.excinfo.errisinstance(tuple(skip_exceptions)):\n300             outcome = \"skipped\"\n301             r_ = collector._repr_failure_py(call.excinfo, \"line\")\n302             assert isinstance(r_, ExceptionChainRepr), repr(r_)\n303             r = r_.reprcrash\n304             assert r\n305             longrepr = (str(r.path), r.lineno, r.message)\n306         else:\n307             outcome = \"failed\"\n308             errorinfo = collector.repr_failure(call.excinfo)\n309             if not hasattr(errorinfo, \"toterminal\"):\n310                 errorinfo = CollectErrorRepr(errorinfo)\n311             longrepr = errorinfo\n312     rep = CollectReport(\n313         collector.nodeid, outcome, longrepr, getattr(call, \"result\", None)\n314     )\n315     rep.call = call  # type: ignore # see collect_one_node\n316     return rep\n317 \n318 \n319 class SetupState:\n320     \"\"\" shared state for setting up/tearing down test items or collectors. \"\"\"\n321 \n322     def __init__(self):\n323         self.stack = []  # type: List[Node]\n324         self._finalizers = {}  # type: Dict[Node, List[Callable[[], None]]]\n325 \n326     def addfinalizer(self, finalizer, colitem):\n327         \"\"\" attach a finalizer to the given colitem. \"\"\"\n328         assert colitem and not isinstance(colitem, tuple)\n329         assert callable(finalizer)\n330         # assert colitem in self.stack  # some unit tests don't setup stack :/\n331         self._finalizers.setdefault(colitem, []).append(finalizer)\n332 \n333     def _pop_and_teardown(self):\n334         colitem = self.stack.pop()\n335         self._teardown_with_finalization(colitem)\n336 \n337     def _callfinalizers(self, colitem):\n338         finalizers = self._finalizers.pop(colitem, None)\n339         exc = None\n340         while finalizers:\n341             fin = finalizers.pop()\n342             try:\n343                 fin()\n344             except TEST_OUTCOME as e:\n345                 # XXX Only first exception will be seen by user,\n346                 #     ideally all should be reported.\n347                 if exc is None:\n348                     exc = e\n349         if exc:\n350             raise exc\n351 \n352     def _teardown_with_finalization(self, colitem):\n353         self._callfinalizers(colitem)\n354         colitem.teardown()\n355         for colitem in self._finalizers:\n356             assert colitem in self.stack\n357 \n358     def teardown_all(self):\n359         while self.stack:\n360             self._pop_and_teardown()\n361         for key in list(self._finalizers):\n362             self._teardown_with_finalization(key)\n363         assert not self._finalizers\n364 \n365     def teardown_exact(self, item, nextitem):\n366         needed_collectors = nextitem and nextitem.listchain() or []\n367         self._teardown_towards(needed_collectors)\n368 \n369     def _teardown_towards(self, needed_collectors):\n370         exc = None\n371         while self.stack:\n372             if self.stack == needed_collectors[: len(self.stack)]:\n373                 break\n374             try:\n375                 self._pop_and_teardown()\n376             except TEST_OUTCOME as e:\n377                 # XXX Only first exception will be seen by user,\n378                 #     ideally all should be reported.\n379                 if exc is None:\n380                     exc = e\n381         if exc:\n382             raise exc\n383 \n384     def prepare(self, colitem):\n385         \"\"\" setup objects along the collector chain to the test-method\n386             and teardown previously setup objects.\"\"\"\n387         needed_collectors = colitem.listchain()\n388         self._teardown_towards(needed_collectors)\n389 \n390         # check if the last collection node has raised an error\n391         for col in self.stack:\n392             if hasattr(col, \"_prepare_exc\"):\n393                 exc = col._prepare_exc\n394                 raise exc\n395         for col in needed_collectors[len(self.stack) :]:\n396             self.stack.append(col)\n397             try:\n398                 col.setup()\n399             except TEST_OUTCOME as e:\n400                 col._prepare_exc = e\n401                 raise e\n402 \n403 \n404 def collect_one_node(collector):\n405     ihook = collector.ihook\n406     ihook.pytest_collectstart(collector=collector)\n407     rep = ihook.pytest_make_collect_report(collector=collector)\n408     call = rep.__dict__.pop(\"call\", None)\n409     if call and check_interactive_exception(call, rep):\n410         ihook.pytest_exception_interact(node=collector, call=call, report=rep)\n411     return rep\n",
        "intended_behavior": "Review and potentially modify this file to ensure skipped test paths are correctly formatted relative to the invocation directory. This involves accurate calculation and display of paths, considering 'rootdir' and the invocation directory, while preserving existing test reporting and path handling functionalities."
    }
]