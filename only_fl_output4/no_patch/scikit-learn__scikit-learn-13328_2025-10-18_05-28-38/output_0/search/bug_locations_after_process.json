[
    {
        "rel_file_path": "sklearn/linear_model/huber.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/linear_model/huber.py",
        "start": 234,
        "end": 306,
        "class_name": "HuberRegressor",
        "method_name": "fit",
        "code": "234     def fit(self, X, y, sample_weight=None):\n235         \"\"\"Fit the model according to the given training data.\n236 \n237         Parameters\n238         ----------\n239         X : array-like, shape (n_samples, n_features)\n240             Training vector, where n_samples in the number of samples and\n241             n_features is the number of features.\n242 \n243         y : array-like, shape (n_samples,)\n244             Target vector relative to X.\n245 \n246         sample_weight : array-like, shape (n_samples,)\n247             Weight given to each sample.\n248 \n249         Returns\n250         -------\n251         self : object\n252         \"\"\"\n253         X, y = check_X_y(\n254             X, y, copy=False, accept_sparse=['csr'], y_numeric=True)\n255         if sample_weight is not None:\n256             sample_weight = np.array(sample_weight)\n257             check_consistent_length(y, sample_weight)\n258         else:\n259             sample_weight = np.ones_like(y)\n260 \n261         if self.epsilon < 1.0:\n262             raise ValueError(\n263                 \"epsilon should be greater than or equal to 1.0, got %f\"\n264                 % self.epsilon)\n265 \n266         if self.warm_start and hasattr(self, 'coef_'):\n267             parameters = np.concatenate(\n268                 (self.coef_, [self.intercept_, self.scale_]))\n269         else:\n270             if self.fit_intercept:\n271                 parameters = np.zeros(X.shape[1] + 2)\n272             else:\n273                 parameters = np.zeros(X.shape[1] + 1)\n274             # Make sure to initialize the scale parameter to a strictly\n275             # positive value:\n276             parameters[-1] = 1\n277 \n278         # Sigma or the scale factor should be non-negative.\n279         # Setting it to be zero might cause undefined bounds hence we set it\n280         # to a value close to zero.\n281         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))\n282         bounds[-1][0] = np.finfo(np.float64).eps * 10\n283 \n284         parameters, f, dict_ = optimize.fmin_l_bfgs_b(\n285             _huber_loss_and_gradient, parameters,\n286             args=(X, y, self.epsilon, self.alpha, sample_weight),\n287             maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,\n288             iprint=0)\n289         if dict_['warnflag'] == 2:\n290             raise ValueError(\"HuberRegressor convergence failed:\"\n291                              \" l-BFGS-b solver terminated with %s\"\n292                              % dict_['task'].decode('ascii'))\n293         # In scipy <= 1.0.0, nit may exceed maxiter.\n294         # See https://github.com/scipy/scipy/issues/7854.\n295         self.n_iter_ = min(dict_['nit'], self.max_iter)\n296         self.scale_ = parameters[-1]\n297         if self.fit_intercept:\n298             self.intercept_ = parameters[-2]\n299         else:\n300             self.intercept_ = 0.0\n301         self.coef_ = parameters[:X.shape[1]]\n302 \n303         residual = np.abs(\n304             y - safe_sparse_dot(X, self.coef_) - self.intercept_)\n305         self.outliers_ = residual > self.scale_ * self.epsilon\n306         return self\n",
        "intended_behavior": "The method should ensure that all incoming input data `X` is in a supported numerical format before proceeding with fitting the model. If `X` is of boolean dtype, it should be converted to float dtype. This step will preserve the existing functionality for numerical input while extending support for boolean input by converting it to float, thus preventing the TypeError. This behavior aligns with how other linear models in scikit-learn, like `LinearRegression`, implicitly handle boolean predictors."
    },
    {
        "rel_file_path": "sklearn/linear_model/huber.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/linear_model/huber.py",
        "start": 124,
        "end": 306,
        "class_name": "HuberRegressor",
        "method_name": null,
        "code": "124 class HuberRegressor(LinearModel, RegressorMixin, BaseEstimator):\n125     \"\"\"Linear regression model that is robust to outliers.\n126 \n127     The Huber Regressor optimizes the squared loss for the samples where\n128     ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples\n129     where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters\n130     to be optimized. The parameter sigma makes sure that if y is scaled up\n131     or down by a certain factor, one does not need to rescale epsilon to\n132     achieve the same robustness. Note that this does not take into account\n133     the fact that the different features of X may be of different scales.\n134 \n135     This makes sure that the loss function is not heavily influenced by the\n136     outliers while not completely ignoring their effect.\n137 \n138     Read more in the :ref:`User Guide <huber_regression>`\n139 \n140     .. versionadded:: 0.18\n141 \n142     Parameters\n143     ----------\n144     epsilon : float, greater than 1.0, default 1.35\n145         The parameter epsilon controls the number of samples that should be\n146         classified as outliers. The smaller the epsilon, the more robust it is\n147         to outliers.\n148 \n149     max_iter : int, default 100\n150         Maximum number of iterations that scipy.optimize.fmin_l_bfgs_b\n151         should run for.\n152 \n153     alpha : float, default 0.0001\n154         Regularization parameter.\n155 \n156     warm_start : bool, default False\n157         This is useful if the stored attributes of a previously used model\n158         has to be reused. If set to False, then the coefficients will\n159         be rewritten for every call to fit.\n160         See :term:`the Glossary <warm_start>`.\n161 \n162     fit_intercept : bool, default True\n163         Whether or not to fit the intercept. This can be set to False\n164         if the data is already centered around the origin.\n165 \n166     tol : float, default 1e-5\n167         The iteration will stop when\n168         ``max{|proj g_i | i = 1, ..., n}`` <= ``tol``\n169         where pg_i is the i-th component of the projected gradient.\n170 \n171     Attributes\n172     ----------\n173     coef_ : array, shape (n_features,)\n174         Features got by optimizing the Huber loss.\n175 \n176     intercept_ : float\n177         Bias.\n178 \n179     scale_ : float\n180         The value by which ``|y - X'w - c|`` is scaled down.\n181 \n182     n_iter_ : int\n183         Number of iterations that fmin_l_bfgs_b has run for.\n184 \n185         .. versionchanged:: 0.20\n186 \n187             In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n188             ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n189 \n190     outliers_ : array, shape (n_samples,)\n191         A boolean mask which is set to True where the samples are identified\n192         as outliers.\n193 \n194     Examples\n195     --------\n196     >>> import numpy as np\n197     >>> from sklearn.linear_model import HuberRegressor, LinearRegression\n198     >>> from sklearn.datasets import make_regression\n199     >>> rng = np.random.RandomState(0)\n200     >>> X, y, coef = make_regression(\n201     ...     n_samples=200, n_features=2, noise=4.0, coef=True, random_state=0)\n202     >>> X[:4] = rng.uniform(10, 20, (4, 2))\n203     >>> y[:4] = rng.uniform(10, 20, 4)\n204     >>> huber = HuberRegressor().fit(X, y)\n205     >>> huber.score(X, y) # doctest: +ELLIPSIS\n206     -7.284608623514573\n207     >>> huber.predict(X[:1,])\n208     array([806.7200...])\n209     >>> linear = LinearRegression().fit(X, y)\n210     >>> print(\"True coefficients:\", coef)\n211     True coefficients: [20.4923...  34.1698...]\n212     >>> print(\"Huber coefficients:\", huber.coef_)\n213     Huber coefficients: [17.7906... 31.0106...]\n214     >>> print(\"Linear Regression coefficients:\", linear.coef_)\n215     Linear Regression coefficients: [-1.9221...  7.0226...]\n216 \n217     References\n218     ----------\n219     .. [1] Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics\n220            Concomitant scale estimates, pg 172\n221     .. [2] Art B. Owen (2006), A robust hybrid of lasso and ridge regression.\n222            https://statweb.stanford.edu/~owen/reports/hhu.pdf\n223     \"\"\"\n224 \n225     def __init__(self, epsilon=1.35, max_iter=100, alpha=0.0001,\n226                  warm_start=False, fit_intercept=True, tol=1e-05):\n227         self.epsilon = epsilon\n228         self.max_iter = max_iter\n229         self.alpha = alpha\n230         self.warm_start = warm_start\n231         self.fit_intercept = fit_intercept\n232         self.tol = tol\n233 \n234     def fit(self, X, y, sample_weight=None):\n235         \"\"\"Fit the model according to the given training data.\n236 \n237         Parameters\n238         ----------\n239         X : array-like, shape (n_samples, n_features)\n240             Training vector, where n_samples in the number of samples and\n241             n_features is the number of features.\n242 \n243         y : array-like, shape (n_samples,)\n244             Target vector relative to X.\n245 \n246         sample_weight : array-like, shape (n_samples,)\n247             Weight given to each sample.\n248 \n249         Returns\n250         -------\n251         self : object\n252         \"\"\"\n253         X, y = check_X_y(\n254             X, y, copy=False, accept_sparse=['csr'], y_numeric=True)\n255         if sample_weight is not None:\n256             sample_weight = np.array(sample_weight)\n257             check_consistent_length(y, sample_weight)\n258         else:\n259             sample_weight = np.ones_like(y)\n260 \n261         if self.epsilon < 1.0:\n262             raise ValueError(\n263                 \"epsilon should be greater than or equal to 1.0, got %f\"\n264                 % self.epsilon)\n265 \n266         if self.warm_start and hasattr(self, 'coef_'):\n267             parameters = np.concatenate(\n268                 (self.coef_, [self.intercept_, self.scale_]))\n269         else:\n270             if self.fit_intercept:\n271                 parameters = np.zeros(X.shape[1] + 2)\n272             else:\n273                 parameters = np.zeros(X.shape[1] + 1)\n274             # Make sure to initialize the scale parameter to a strictly\n275             # positive value:\n276             parameters[-1] = 1\n277 \n278         # Sigma or the scale factor should be non-negative.\n279         # Setting it to be zero might cause undefined bounds hence we set it\n280         # to a value close to zero.\n281         bounds = np.tile([-np.inf, np.inf], (parameters.shape[0], 1))\n282         bounds[-1][0] = np.finfo(np.float64).eps * 10\n283 \n284         parameters, f, dict_ = optimize.fmin_l_bfgs_b(\n285             _huber_loss_and_gradient, parameters,\n286             args=(X, y, self.epsilon, self.alpha, sample_weight),\n287             maxiter=self.max_iter, pgtol=self.tol, bounds=bounds,\n288             iprint=0)\n289         if dict_['warnflag'] == 2:\n290             raise ValueError(\"HuberRegressor convergence failed:\"\n291                              \" l-BFGS-b solver terminated with %s\"\n292                              % dict_['task'].decode('ascii'))\n293         # In scipy <= 1.0.0, nit may exceed maxiter.\n294         # See https://github.com/scipy/scipy/issues/7854.\n295         self.n_iter_ = min(dict_['nit'], self.max_iter)\n296         self.scale_ = parameters[-1]\n297         if self.fit_intercept:\n298             self.intercept_ = parameters[-2]\n299         else:\n300             self.intercept_ = 0.0\n301         self.coef_ = parameters[:X.shape[1]]\n302 \n303         residual = np.abs(\n304             y - safe_sparse_dot(X, self.coef_) - self.intercept_)\n305         self.outliers_ = residual > self.scale_ * self.epsilon\n306         return self\n",
        "intended_behavior": "This class provides additional context to the issue."
    },
    {
        "rel_file_path": "sklearn/linear_model/huber.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/linear_model/huber.py",
        "start": 16,
        "end": 121,
        "class_name": null,
        "method_name": "_huber_loss_and_gradient",
        "code": "16 def _huber_loss_and_gradient(w, X, y, epsilon, alpha, sample_weight=None):\n17     \"\"\"Returns the Huber loss and the gradient.\n18 \n19     Parameters\n20     ----------\n21     w : ndarray, shape (n_features + 1,) or (n_features + 2,)\n22         Feature vector.\n23         w[:n_features] gives the coefficients\n24         w[-1] gives the scale factor and if the intercept is fit w[-2]\n25         gives the intercept factor.\n26 \n27     X : ndarray, shape (n_samples, n_features)\n28         Input data.\n29 \n30     y : ndarray, shape (n_samples,)\n31         Target vector.\n32 \n33     epsilon : float\n34         Robustness of the Huber estimator.\n35 \n36     alpha : float\n37         Regularization parameter.\n38 \n39     sample_weight : ndarray, shape (n_samples,), optional\n40         Weight assigned to each sample.\n41 \n42     Returns\n43     -------\n44     loss : float\n45         Huber loss.\n46 \n47     gradient : ndarray, shape (len(w))\n48         Returns the derivative of the Huber loss with respect to each\n49         coefficient, intercept and the scale as a vector.\n50     \"\"\"\n51     _, n_features = X.shape\n52     fit_intercept = (n_features + 2 == w.shape[0])\n53     if fit_intercept:\n54         intercept = w[-2]\n55     sigma = w[-1]\n56     w = w[:n_features]\n57     n_samples = np.sum(sample_weight)\n58 \n59     # Calculate the values where |y - X'w -c / sigma| > epsilon\n60     # The values above this threshold are outliers.\n61     linear_loss = y - safe_sparse_dot(X, w)\n62     if fit_intercept:\n63         linear_loss -= intercept\n64     abs_linear_loss = np.abs(linear_loss)\n65     outliers_mask = abs_linear_loss > epsilon * sigma\n66 \n67     # Calculate the linear loss due to the outliers.\n68     # This is equal to (2 * M * |y - X'w -c / sigma| - M**2) * sigma\n69     outliers = abs_linear_loss[outliers_mask]\n70     num_outliers = np.count_nonzero(outliers_mask)\n71     n_non_outliers = X.shape[0] - num_outliers\n72 \n73     # n_sq_outliers includes the weight give to the outliers while\n74     # num_outliers is just the number of outliers.\n75     outliers_sw = sample_weight[outliers_mask]\n76     n_sw_outliers = np.sum(outliers_sw)\n77     outlier_loss = (2. * epsilon * np.sum(outliers_sw * outliers) -\n78                     sigma * n_sw_outliers * epsilon ** 2)\n79 \n80     # Calculate the quadratic loss due to the non-outliers.-\n81     # This is equal to |(y - X'w - c)**2 / sigma**2| * sigma\n82     non_outliers = linear_loss[~outliers_mask]\n83     weighted_non_outliers = sample_weight[~outliers_mask] * non_outliers\n84     weighted_loss = np.dot(weighted_non_outliers.T, non_outliers)\n85     squared_loss = weighted_loss / sigma\n86 \n87     if fit_intercept:\n88         grad = np.zeros(n_features + 2)\n89     else:\n90         grad = np.zeros(n_features + 1)\n91 \n92     # Gradient due to the squared loss.\n93     X_non_outliers = -axis0_safe_slice(X, ~outliers_mask, n_non_outliers)\n94     grad[:n_features] = (\n95         2. / sigma * safe_sparse_dot(weighted_non_outliers, X_non_outliers))\n96 \n97     # Gradient due to the linear loss.\n98     signed_outliers = np.ones_like(outliers)\n99     signed_outliers_mask = linear_loss[outliers_mask] < 0\n100     signed_outliers[signed_outliers_mask] = -1.0\n101     X_outliers = axis0_safe_slice(X, outliers_mask, num_outliers)\n102     sw_outliers = sample_weight[outliers_mask] * signed_outliers\n103     grad[:n_features] -= 2. * epsilon * (\n104         safe_sparse_dot(sw_outliers, X_outliers))\n105 \n106     # Gradient due to the penalty.\n107     grad[:n_features] += alpha * 2. * w\n108 \n109     # Gradient due to sigma.\n110     grad[-1] = n_samples\n111     grad[-1] -= n_sw_outliers * epsilon ** 2\n112     grad[-1] -= squared_loss / sigma\n113 \n114     # Gradient due to the intercept.\n115     if fit_intercept:\n116         grad[-2] = -2. * np.sum(weighted_non_outliers) / sigma\n117         grad[-2] -= 2. * epsilon * np.sum(sw_outliers)\n118 \n119     loss = n_samples * sigma + squared_loss + outlier_loss\n120     loss += alpha * np.dot(w, w)\n121     return loss, grad\n",
        "intended_behavior": "While the direct bug fix might primarily involve changes before this function is called (e.g., within `.fit`), ensuring that this function either checks the input type or is documented to require numerical input could improve clarity and maintainability. However, the primary change would be in how data is prepared before it reaches this function, as described in Location 1. No changes are required directly in this function to resolve the issue, assuming all conversions are handled upstream (in the `.fit` method)."
    }
]