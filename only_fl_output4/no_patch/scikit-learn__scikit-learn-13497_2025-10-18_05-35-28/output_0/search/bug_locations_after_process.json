[
    {
        "rel_file_path": "sklearn/feature_selection/mutual_info_.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/feature_selection/mutual_info_.py",
        "start": 195,
        "end": 290,
        "class_name": null,
        "method_name": "_estimate_mi",
        "code": "195 def _estimate_mi(X, y, discrete_features='auto', discrete_target=False,\n196                  n_neighbors=3, copy=True, random_state=None):\n197     \"\"\"Estimate mutual information between the features and the target.\n198 \n199     Parameters\n200     ----------\n201     X : array_like or sparse matrix, shape (n_samples, n_features)\n202         Feature matrix.\n203 \n204     y : array_like, shape (n_samples,)\n205         Target vector.\n206 \n207     discrete_features : {'auto', bool, array_like}, default 'auto'\n208         If bool, then determines whether to consider all features discrete\n209         or continuous. If array, then it should be either a boolean mask\n210         with shape (n_features,) or array with indices of discrete features.\n211         If 'auto', it is assigned to False for dense `X` and to True for\n212         sparse `X`.\n213 \n214     discrete_target : bool, default False\n215         Whether to consider `y` as a discrete variable.\n216 \n217     n_neighbors : int, default 3\n218         Number of neighbors to use for MI estimation for continuous variables,\n219         see [1]_ and [2]_. Higher values reduce variance of the estimation, but\n220         could introduce a bias.\n221 \n222     copy : bool, default True\n223         Whether to make a copy of the given data. If set to False, the initial\n224         data will be overwritten.\n225 \n226     random_state : int, RandomState instance or None, optional, default None\n227         The seed of the pseudo random number generator for adding small noise\n228         to continuous variables in order to remove repeated values.  If int,\n229         random_state is the seed used by the random number generator; If\n230         RandomState instance, random_state is the random number generator; If\n231         None, the random number generator is the RandomState instance used by\n232         `np.random`.\n233 \n234     Returns\n235     -------\n236     mi : ndarray, shape (n_features,)\n237         Estimated mutual information between each feature and the target.\n238         A negative value will be replaced by 0.\n239 \n240     References\n241     ----------\n242     .. [1] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n243            information\". Phys. Rev. E 69, 2004.\n244     .. [2] B. C. Ross \"Mutual Information between Discrete and Continuous\n245            Data Sets\". PLoS ONE 9(2), 2014.\n246     \"\"\"\n247     X, y = check_X_y(X, y, accept_sparse='csc', y_numeric=not discrete_target)\n248     n_samples, n_features = X.shape\n249 \n250     if discrete_features == 'auto':\n251         discrete_features = issparse(X)\n252 \n253     if isinstance(discrete_features, bool):\n254         discrete_mask = np.empty(n_features, dtype=bool)\n255         discrete_mask.fill(discrete_features)\n256     else:\n257         discrete_features = np.asarray(discrete_features)\n258         if discrete_features.dtype != 'bool':\n259             discrete_mask = np.zeros(n_features, dtype=bool)\n260             discrete_mask[discrete_features] = True\n261         else:\n262             discrete_mask = discrete_features\n263 \n264     continuous_mask = ~discrete_mask\n265     if np.any(continuous_mask) and issparse(X):\n266         raise ValueError(\"Sparse matrix `X` can't have continuous features.\")\n267 \n268     rng = check_random_state(random_state)\n269     if np.any(continuous_mask):\n270         if copy:\n271             X = X.copy()\n272 \n273         if not discrete_target:\n274             X[:, continuous_mask] = scale(X[:, continuous_mask],\n275                                           with_mean=False, copy=False)\n276 \n277         # Add small noise to continuous features as advised in Kraskov et. al.\n278         X = X.astype(float, **_astype_copy_false(X))\n279         means = np.maximum(1, np.mean(np.abs(X[:, continuous_mask]), axis=0))\n280         X[:, continuous_mask] += 1e-10 * means * rng.randn(\n281                 n_samples, np.sum(continuous_mask))\n282 \n283     if not discrete_target:\n284         y = scale(y, with_mean=False)\n285         y += 1e-10 * np.maximum(1, np.mean(np.abs(y))) * rng.randn(n_samples)\n286 \n287     mi = [_compute_mi(x, y, discrete_feature, discrete_target, n_neighbors) for\n288           x, discrete_feature in zip(_iterate_columns(X), discrete_mask)]\n289 \n290     return np.array(mi)\n",
        "intended_behavior": "The handling of the `discrete_features` parameter within `_estimate_mi` needs to be adjusted to ensure that it can correctly identify and process different types of inputs (i.e., 'auto', boolean array, indices array) for `discrete_features`. Specifically, the comparison `discrete_features == 'auto'` should be safeguarded against future numpy versions' compatibility issues, likely by adding an explicit type check before this comparison to ensure that `discrete_features` is being compared as a string when intended. Additionally, for instances where `discrete_features` is an array, the method should intelligibly handle it by accurately creating a mask for discrete features whether it represents a boolean mask or an array of indices, thus facilitating the function's ability to estimate mutual information with both discrete and continuous features accurately. This means refining the subsequent conditions and processing logic after the initial auto check to cater to the nuances of `discrete_features` being an array without relying on behavior that may be deprecated in future versions of dependencies."
    }
]