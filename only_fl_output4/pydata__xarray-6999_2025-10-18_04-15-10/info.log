2025-10-18 04:15:10.961 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 04:15:12.575 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=946, output_tokens=12, cost=0.009820
2025-10-18 04:15:18.420 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1126, output_tokens=267, cost=0.019270
2025-10-18 04:15:19.663 | INFO     | registered non reproducing test 0
2025-10-18 04:15:31.024 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2157, output_tokens=532, cost=0.037530
2025-10-18 04:15:32.216 | INFO     | registered non reproducing test 1
2025-10-18 04:15:44.432 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2419, output_tokens=542, cost=0.040450
2025-10-18 04:15:45.626 | INFO     | registered non reproducing test 2
2025-10-18 04:15:45.626 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 04:15:45.629 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 04:15:51.421 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1244, output_tokens=246, cost=0.019820
2025-10-18 04:15:51.427 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 04:15:55.361 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=600, output_tokens=59, cost=0.007770
2025-10-18 04:15:55.362 | DEBUG    | Extracted a valid json.
2025-10-18 04:15:55.363 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_method(\"rename\")",
        "search_method(\"rename_vars\")",
        "search_method(\"set_index\")",
        "search_class(\"DataArray\")",
        "search_class(\"DataSet\")"
    ],
    "bug_locations": []
}
2025-10-18 04:15:55.497 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 04:15:55.498 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 04:16:14.909 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=8684, output_tokens=837, cost=0.111950
2025-10-18 04:16:14.926 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 04:16:22.279 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=9737, output_tokens=286, cost=0.105950
2025-10-18 04:16:22.286 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 04:16:29.129 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=640, output_tokens=262, cost=0.014260
2025-10-18 04:16:29.130 | DEBUG    | Extracted a valid json.
2025-10-18 04:16:29.130 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/dataarray.py",
            "class": "DataArray",
            "method": "rename",
            "intended_behavior": "The `rename` method should check if the renaming operation converts a coordinate into a dimension coordinate and, if so, automatically generate an appropriate index for this new dimension coordinate. This ensures subsequent operations that rely on this index, such as `sel`, can proceed without raising errors. The preservation of existing functionality is crucial; thus, any change should only augment the current capabilities with additional checks for index management when renaming coordinates to dimension coordinates."
        },
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "rename_vars",
            "intended_behavior": "Similar to the `DataArray.rename` method, the `rename_vars` method in the `Dataset` class should include logic to detect when a variable (coordinate) renaming action results in the variable becoming a dimension coordinate. Upon such an event, it should ensure an index is created or updated for the new dimension coordinate. The method must maintain its current functionality while extending it to manage indexes for newly dimensioned coordinates efficiently."
        }
    ]
}
2025-10-18 04:16:29.171 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>rename</method>
<code>
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)

</code><intended_behavior>The `rename` method should check if the renaming operation converts a coordinate into a dimension coordinate and, if so, automatically generate an appropriate index for this new dimension coordinate. This ensures subsequent operations that rely on this index, such as `sel`, can proceed without raising errors. The preservation of existing functionality is crucial; thus, any change should only augment the current capabilities with additional checks for index management when renaming coordinates to dimension coordinates.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
223 class DataArray(
224     AbstractArray, DataWithCoords, DataArrayArithmetic, DataArrayReductions
225 ):
226     """N-dimensional array with labeled coordinates and dimensions.
227 
228     DataArray provides a wrapper around numpy ndarrays that uses
229     labeled dimensions and coordinates to support metadata aware
230     operations. The API is similar to that for the pandas Series or
231     DataFrame, but DataArray objects can have any number of dimensions,
232     and their contents have fixed data types.
233 
234     Additional features over raw numpy arrays:
235 
236     - Apply operations over dimensions by name: ``x.sum('time')``.
237     - Select or assign values by integer location (like numpy):
238       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
239       ``x.sel(time='2014-01-01')``.
240     - Mathematical operations (e.g., ``x - y``) vectorize across
241       multiple dimensions (known in numpy as "broadcasting") based on
242       dimension names, regardless of their original order.
243     - Keep track of arbitrary metadata in the form of a Python
244       dictionary: ``x.attrs``
245     - Convert to a pandas Series: ``x.to_series()``.
246 
247     Getting items from or doing mathematical operations with a
248     DataArray always returns another DataArray.
249 
250     Parameters
251     ----------
252     data : array_like
253         Values for this array. Must be an ``numpy.ndarray``, ndarray
254         like, or castable to an ``ndarray``. If a self-described xarray
255         or pandas object, attempts are made to use this array's
256         metadata to fill in other unspecified arguments. A view of the
257         array's data is used instead of a copy if possible.
258     coords : sequence or dict of array_like, optional
259         Coordinates (tick labels) to use for indexing along each
260         dimension. The following notations are accepted:
261 
262         - mapping {dimension name: array-like}
263         - sequence of tuples that are valid arguments for
264           ``xarray.Variable()``
265           - (dims, data)
266           - (dims, data, attrs)
267           - (dims, data, attrs, encoding)
268 
269         Additionally, it is possible to define a coord whose name
270         does not match the dimension name, or a coord based on multiple
271         dimensions, with one of the following notations:
272 
273         - mapping {coord name: DataArray}
274         - mapping {coord name: Variable}
275         - mapping {coord name: (dimension name, array-like)}
276         - mapping {coord name: (tuple of dimension names, array-like)}
277 
278     dims : Hashable or sequence of Hashable, optional
279         Name(s) of the data dimension(s). Must be either a Hashable
280         (only for 1D data) or a sequence of Hashables with length equal
281         to the number of dimensions. If this argument is omitted,
282         dimension names are taken from ``coords`` (if possible) and
283         otherwise default to ``['dim_0', ... 'dim_n']``.
284     name : str or None, optional
285         Name of this array.
286     attrs : dict_like or None, optional
287         Attributes to assign to the new instance. By default, an empty
288         attribute dictionary is initialized.
289 
290     Examples
291     --------
292     Create data:
293 
294     >>> np.random.seed(0)
295     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
296     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
297     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
298     >>> time = pd.date_range("2014-09-06", periods=3)
299     >>> reference_time = pd.Timestamp("2014-09-05")
300 
301     Initialize a dataarray with multiple dimensions:
302 
303     >>> da = xr.DataArray(
304     ...     data=temperature,
305     ...     dims=["x", "y", "time"],
306     ...     coords=dict(
307     ...         lon=(["x", "y"], lon),
308     ...         lat=(["x", "y"], lat),
309     ...         time=time,
310     ...         reference_time=reference_time,
311     ...     ),
312     ...     attrs=dict(
313     ...         description="Ambient temperature.",
314     ...         units="degC",
315     ...     ),
316     ... )
317     >>> da
318     <xarray.DataArray (x: 2, y: 2, time: 3)>
319     array([[[29.11241877, 18.20125767, 22.82990387],
320             [32.92714559, 29.94046392,  7.18177696]],
321     <BLANKLINE>
322            [[22.60070734, 13.78914233, 14.17424919],
323             [18.28478802, 16.15234857, 26.63418806]]])
324     Coordinates:
325         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
326         lat             (x, y) float64 42.25 42.21 42.63 42.59
327       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
328         reference_time  datetime64[ns] 2014-09-05
329     Dimensions without coordinates: x, y
330     Attributes:
331         description:  Ambient temperature.
332         units:        degC
333 
334     Find out where the coldest temperature was:
335 
336     >>> da.isel(da.argmin(...))
337     <xarray.DataArray ()>
338     array(7.18177696)
339     Coordinates:
340         lon             float64 -99.32
341         lat             float64 42.21
342         time            datetime64[ns] 2014-09-08
343         reference_time  datetime64[ns] 2014-09-05
344     Attributes:
345         description:  Ambient temperature.
346         units:        degC
347     """
348 
349     _cache: dict[str, Any]
350     _coords: dict[Any, Variable]
351     _close: Callable[[], None] | None
352     _indexes: dict[Hashable, Index]
353     _name: Hashable | None
354     _variable: Variable
355 
356     __slots__ = (
357         "_cache",
358         "_coords",
359         "_close",
360         "_indexes",
361         "_name",
362         "_variable",
363         "__weakref__",
364     )
365 
366     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
367 
368     def __init__(
369         self,
370         data: Any = dtypes.NA,
371         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
372         | Mapping[Any, Any]
373         | None = None,
374         dims: Hashable | Sequence[Hashable] | None = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: dict[Hashable, Index] = None,
379         fastpath: bool = False,
380     ) -> None:
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385             assert indexes is not None
386         else:
387             # TODO: (benbovy - explicit indexes) remove
388             # once it becomes part of the public interface
389             if indexes is not None:
390                 raise ValueError("Providing explicit indexes is not supported yet")
391 
392             # try to fill in arguments from data if they weren't supplied
393             if coords is None:
394 
395                 if isinstance(data, DataArray):
396                     coords = data.coords
397                 elif isinstance(data, pd.Series):
398                     coords = [data.index]
399                 elif isinstance(data, pd.DataFrame):
400                     coords = [data.index, data.columns]
401                 elif isinstance(data, (pd.Index, IndexVariable)):
402                     coords = [data]
403 
404             if dims is None:
405                 dims = getattr(data, "dims", getattr(coords, "dims", None))
406             if name is None:
407                 name = getattr(data, "name", None)
408             if attrs is None and not isinstance(data, PANDAS_TYPES):
409                 attrs = getattr(data, "attrs", None)
410 
411             data = _check_data_shape(data, coords, dims)
412             data = as_compatible_data(data)
413             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
414             variable = Variable(dims, data, attrs, fastpath=True)
415             indexes, coords = _create_indexes_from_coords(coords)
416 
417         # These fully describe a DataArray
418         self._variable = variable
419         assert isinstance(coords, dict)
420         self._coords = coords
421         self._name = name
422 
423         # TODO(shoyer): document this argument, once it becomes part of the
424         # public interface.
425         self._indexes = indexes  # type: ignore[assignment]
426 
427         self._close = None
428 
429     @classmethod
430     def _construct_direct(
431         cls: type[T_DataArray],
432         variable: Variable,
433         coords: dict[Any, Variable],
434         name: Hashable,
435         indexes: dict[Hashable, Index],
436     ) -> T_DataArray:
437         """Shortcut around __init__ for internal use when we want to skip
438         costly validation
439         """
440         obj = object.__new__(cls)
441         obj._variable = variable
442         obj._coords = coords
443         obj._name = name
444         obj._indexes = indexes
445         obj._close = None
446         return obj
447 
448     def _replace(
449         self: T_DataArray,
450         variable: Variable = None,
451         coords=None,
452         name: Hashable | None | Default = _default,
453         indexes=None,
454     ) -> T_DataArray:
455         if variable is None:
456             variable = self.variable
457         if coords is None:
458             coords = self._coords
459         if indexes is None:
460             indexes = self._indexes
461         if name is _default:
462             name = self.name
463         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
464 
465     def _replace_maybe_drop_dims(
466         self: T_DataArray,
467         variable: Variable,
468         name: Hashable | None | Default = _default,
469     ) -> T_DataArray:
470         if variable.dims == self.dims and variable.shape == self.shape:
471             coords = self._coords.copy()
472             indexes = self._indexes
473         elif variable.dims == self.dims:
474             # Shape has changed (e.g. from reduce(..., keepdims=True)
475             new_sizes = dict(zip(self.dims, variable.shape))
476             coords = {
477                 k: v
478                 for k, v in self._coords.items()
479                 if v.shape == tuple(new_sizes[d] for d in v.dims)
480             }
481             indexes = filter_indexes_from_coords(self._indexes, set(coords))
482         else:
483             allowed_dims = set(variable.dims)
484             coords = {
485                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
486             }
487             indexes = filter_indexes_from_coords(self._indexes, set(coords))
488         return self._replace(variable, coords, name, indexes=indexes)
489 
490     def _overwrite_indexes(
491         self: T_DataArray,
492         indexes: Mapping[Any, Index],
493         coords: Mapping[Any, Variable] = None,
494         drop_coords: list[Hashable] = None,
495         rename_dims: Mapping[Any, Any] = None,
496     ) -> T_DataArray:
497         """Maybe replace indexes and their corresponding coordinates."""
498         if not indexes:
499             return self
500 
501         if coords is None:
502             coords = {}
503         if drop_coords is None:
504             drop_coords = []
505 
506         new_variable = self.variable.copy()
507         new_coords = self._coords.copy()
508         new_indexes = dict(self._indexes)
509 
510         for name in indexes:
511             new_coords[name] = coords[name]
512             new_indexes[name] = indexes[name]
513 
514         for name in drop_coords:
515             new_coords.pop(name)
516             new_indexes.pop(name)
517 
518         if rename_dims:
519             new_variable.dims = [rename_dims.get(d, d) for d in new_variable.dims]
520 
521         return self._replace(
522             variable=new_variable, coords=new_coords, indexes=new_indexes
523         )
524 
525     def _to_temp_dataset(self) -> Dataset:
526         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
527 
528     def _from_temp_dataset(
529         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
530     ) -> T_DataArray:
531         variable = dataset._variables.pop(_THIS_ARRAY)
532         coords = dataset._variables
533         indexes = dataset._indexes
534         return self._replace(variable, coords, name, indexes=indexes)
535 
536     def _to_dataset_split(self, dim: Hashable) -> Dataset:
537         """splits dataarray along dimension 'dim'"""
538 
539         def subset(dim, label):
540             array = self.loc[{dim: label}]
541             array.attrs = {}
542             return as_variable(array)
543 
544         variables = {label: subset(dim, label) for label in self.get_index(dim)}
545         variables.update({k: v for k, v in self._coords.items() if k != dim})
546         coord_names = set(self._coords) - {dim}
547         indexes = filter_indexes_from_coords(self._indexes, coord_names)
548         dataset = Dataset._construct_direct(
549             variables, coord_names, indexes=indexes, attrs=self.attrs
550         )
551         return dataset
552 
553     def _to_dataset_whole(
554         self, name: Hashable = None, shallow_copy: bool = True
555     ) -> Dataset:
556         if name is None:
557             name = self.name
558         if name is None:
559             raise ValueError(
560                 "unable to convert unnamed DataArray to a "
561                 "Dataset without providing an explicit name"
562             )
563         if name in self.coords:
564             raise ValueError(
565                 "cannot create a Dataset from a DataArray with "
566                 "the same name as one of its coordinates"
567             )
568         # use private APIs for speed: this is called by _to_temp_dataset(),
569         # which is used in the guts of a lot of operations (e.g., reindex)
570         variables = self._coords.copy()
571         variables[name] = self.variable
572         if shallow_copy:
573             for k in variables:
574                 variables[k] = variables[k].copy(deep=False)
575         indexes = self._indexes
576 
577         coord_names = set(self._coords)
578         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
579 
580     def to_dataset(
581         self,
582         dim: Hashable = None,
583         *,
584         name: Hashable = None,
585         promote_attrs: bool = False,
586     ) -> Dataset:
587         """Convert a DataArray to a Dataset.
588 
589         Parameters
590         ----------
591         dim : Hashable, optional
592             Name of the dimension on this array along which to split this array
593             into separate variables. If not provided, this array is converted
594             into a Dataset of one variable.
595         name : Hashable, optional
596             Name to substitute for this array's name. Only valid if ``dim`` is
597             not provided.
598         promote_attrs : bool, default: False
599             Set to True to shallow copy attrs of DataArray to returned Dataset.
600 
601         Returns
602         -------
603         dataset : Dataset
604         """
605         if dim is not None and dim not in self.dims:
606             raise TypeError(
607                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
608             )
609 
610         if dim is not None:
611             if name is not None:
612                 raise TypeError("cannot supply both dim and name arguments")
613             result = self._to_dataset_split(dim)
614         else:
615             result = self._to_dataset_whole(name)
616 
617         if promote_attrs:
618             result.attrs = dict(self.attrs)
619 
620         return result
621 
622     @property
623     def name(self) -> Hashable | None:
624         """The name of this array."""
625         return self._name
626 
627     @name.setter
628     def name(self, value: Hashable | None) -> None:
629         self._name = value
630 
631     @property
632     def variable(self) -> Variable:
633         """Low level interface to the Variable object for this DataArray."""
634         return self._variable
635 
636     @property
637     def dtype(self) -> np.dtype:
638         return self.variable.dtype
639 
640     @property
641     def shape(self) -> tuple[int, ...]:
642         return self.variable.shape
643 
644     @property
645     def size(self) -> int:
646         return self.variable.size
647 
648     @property
649     def nbytes(self) -> int:
650         """
651         Total bytes consumed by the elements of this DataArray's data.
652 
653         If the backend data array does not include ``nbytes``, estimates
654         the bytes consumed based on the ``size`` and ``dtype``.
655         """
656         return self.variable.nbytes
657 
658     @property
659     def ndim(self) -> int:
660         return self.variable.ndim
661 
662     def __len__(self) -> int:
663         return len(self.variable)
664 
665     @property
666     def data(self) -> Any:
667         """
668         The DataArray's data as an array. The underlying array type
669         (e.g. dask, sparse, pint) is preserved.
670 
671         See Also
672         --------
673         DataArray.to_numpy
674         DataArray.as_numpy
675         DataArray.values
676         """
677         return self.variable.data
678 
679     @data.setter
680     def data(self, value: Any) -> None:
681         self.variable.data = value
682 
683     @property
684     def values(self) -> np.ndarray:
685         """
686         The array's data as a numpy.ndarray.
687 
688         If the array's data is not a numpy.ndarray this will attempt to convert
689         it naively using np.array(), which will raise an error if the array
690         type does not support coercion like this (e.g. cupy).
691         """
692         return self.variable.values
693 
694     @values.setter
695     def values(self, value: Any) -> None:
696         self.variable.values = value
697 
698     def to_numpy(self) -> np.ndarray:
699         """
700         Coerces wrapped data to numpy and returns a numpy.ndarray.
701 
702         See Also
703         --------
704         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
705         Dataset.as_numpy
706         DataArray.values
707         DataArray.data
708         """
709         return self.variable.to_numpy()
710 
711     def as_numpy(self: T_DataArray) -> T_DataArray:
712         """
713         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
714 
715         See Also
716         --------
717         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
718         Dataset.as_numpy : Converts all variables in a Dataset.
719         DataArray.values
720         DataArray.data
721         """
722         coords = {k: v.as_numpy() for k, v in self._coords.items()}
723         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
724 
725     @property
726     def _in_memory(self) -> bool:
727         return self.variable._in_memory
728 
729     def to_index(self) -> pd.Index:
730         """Convert this variable to a pandas.Index. Only possible for 1D
731         arrays.
732         """
733         return self.variable.to_index()
734 
735     @property
736     def dims(self) -> tuple[Hashable, ...]:
737         """Tuple of dimension names associated with this array.
738 
739         Note that the type of this property is inconsistent with
740         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
741         consistently named properties.
742 
743         See Also
744         --------
745         DataArray.sizes
746         Dataset.dims
747         """
748         return self.variable.dims
749 
750     @dims.setter
751     def dims(self, value: Any) -> NoReturn:
752         raise AttributeError(
753             "you cannot assign dims on a DataArray. Use "
754             ".rename() or .swap_dims() instead."
755         )
756 
757     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
758         if utils.is_dict_like(key):
759             return key
760         key = indexing.expanded_indexer(key, self.ndim)
761         return dict(zip(self.dims, key))
762 
763     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
764         from .dataset import _get_virtual_variable
765 
766         try:
767             var = self._coords[key]
768         except KeyError:
769             dim_sizes = dict(zip(self.dims, self.shape))
770             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
771 
772         return self._replace_maybe_drop_dims(var, name=key)
773 
774     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
775         if isinstance(key, str):
776             return self._getitem_coord(key)
777         else:
778             # xarray-style array indexing
779             return self.isel(indexers=self._item_key_to_dict(key))
780 
781     def __setitem__(self, key: Any, value: Any) -> None:
782         if isinstance(key, str):
783             self.coords[key] = value
784         else:
785             # Coordinates in key, value and self[key] should be consistent.
786             # TODO Coordinate consistency in key is checked here, but it
787             # causes unnecessary indexing. It should be optimized.
788             obj = self[key]
789             if isinstance(value, DataArray):
790                 assert_coordinate_consistent(value, obj.coords.variables)
791             # DataArray key -> Variable key
792             key = {
793                 k: v.variable if isinstance(v, DataArray) else v
794                 for k, v in self._item_key_to_dict(key).items()
795             }
796             self.variable[key] = value
797 
798     def __delitem__(self, key: Any) -> None:
799         del self.coords[key]
800 
801     @property
802     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
803         """Places to look-up items for attribute-style access"""
804         yield from self._item_sources
805         yield self.attrs
806 
807     @property
808     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
809         """Places to look-up items for key-completion"""
810         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
811 
812         # virtual coordinates
813         # uses empty dict -- everything here can already be found in self.coords.
814         yield HybridMappingProxy(keys=self.dims, mapping={})
815 
816     def __contains__(self, key: Any) -> bool:
817         return key in self.data
818 
819     @property
820     def loc(self) -> _LocIndexer:
821         """Attribute for location based indexing like pandas."""
822         return _LocIndexer(self)
823 
824     @property
825     # Key type needs to be `Any` because of mypy#4167
826     def attrs(self) -> dict[Any, Any]:
827         """Dictionary storing arbitrary metadata with this array."""
828         return self.variable.attrs
829 
830     @attrs.setter
831     def attrs(self, value: Mapping[Any, Any]) -> None:
832         # Disable type checking to work around mypy bug - see mypy#4167
833         self.variable.attrs = value  # type: ignore[assignment]
834 
835     @property
836     def encoding(self) -> dict[Hashable, Any]:
837         """Dictionary of format-specific settings for how this array should be
838         serialized."""
839         return self.variable.encoding
840 
841     @encoding.setter
842     def encoding(self, value: Mapping[Any, Any]) -> None:
843         self.variable.encoding = value
844 
845     @property
846     def indexes(self) -> Indexes:
847         """Mapping of pandas.Index objects used for label based indexing.
848 
849         Raises an error if this Dataset has indexes that cannot be coerced
850         to pandas.Index objects.
851 
852         See Also
853         --------
854         DataArray.xindexes
855 
856         """
857         return self.xindexes.to_pandas_indexes()
858 
859     @property
860     def xindexes(self) -> Indexes:
861         """Mapping of xarray Index objects used for label based indexing."""
862         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
863 
864     @property
865     def coords(self) -> DataArrayCoordinates:
866         """Dictionary-like container of coordinate arrays."""
867         return DataArrayCoordinates(self)
868 
869     @overload
870     def reset_coords(
871         self: T_DataArray,
872         names: Hashable | Iterable[Hashable] | None = None,
873         drop: Literal[False] = False,
874     ) -> Dataset:
875         ...
876 
877     @overload
878     def reset_coords(
879         self: T_DataArray,
880         names: Hashable | Iterable[Hashable] | None = None,
881         *,
882         drop: Literal[True],
883     ) -> T_DataArray:
884         ...
885 
886     def reset_coords(
887         self: T_DataArray,
888         names: Hashable | Iterable[Hashable] | None = None,
889         drop: bool = False,
890     ) -> T_DataArray | Dataset:
891         """Given names of coordinates, reset them to become variables.
892 
893         Parameters
894         ----------
895         names : Hashable or iterable of Hashable, optional
896             Name(s) of non-index coordinates in this dataset to reset into
897             variables. By default, all non-index coordinates are reset.
898         drop : bool, default: False
899             If True, remove coordinates instead of converting them into
900             variables.
901 
902         Returns
903         -------
904         Dataset, or DataArray if ``drop == True``
905         """
906         if names is None:
907             names = set(self.coords) - set(self._indexes)
908         dataset = self.coords.to_dataset().reset_coords(names, drop)
909         if drop:
910             return self._replace(coords=dataset._variables)
911         if self.name is None:
912             raise ValueError(
913                 "cannot reset_coords with drop=False on an unnamed DataArrray"
914             )
915         dataset[self.name] = self.variable
916         return dataset
917 
918     def __dask_tokenize__(self):
919         from dask.base import normalize_token
920 
921         return normalize_token((type(self), self._variable, self._coords, self._name))
922 
923     def __dask_graph__(self):
924         return self._to_temp_dataset().__dask_graph__()
925 
926     def __dask_keys__(self):
927         return self._to_temp_dataset().__dask_keys__()
928 
929     def __dask_layers__(self):
930         return self._to_temp_dataset().__dask_layers__()
931 
932     @property
933     def __dask_optimize__(self):
934         return self._to_temp_dataset().__dask_optimize__
935 
936     @property
937     def __dask_scheduler__(self):
938         return self._to_temp_dataset().__dask_scheduler__
939 
940     def __dask_postcompute__(self):
941         func, args = self._to_temp_dataset().__dask_postcompute__()
942         return self._dask_finalize, (self.name, func) + args
943 
944     def __dask_postpersist__(self):
945         func, args = self._to_temp_dataset().__dask_postpersist__()
946         return self._dask_finalize, (self.name, func) + args
947 
948     @staticmethod
949     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
950         ds = func(results, *args, **kwargs)
951         variable = ds._variables.pop(_THIS_ARRAY)
952         coords = ds._variables
953         indexes = ds._indexes
954         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
955 
956     def load(self: T_DataArray, **kwargs) -> T_DataArray:
957         """Manually trigger loading of this array's data from disk or a
958         remote source into memory and return this array.
959 
960         Normally, it should not be necessary to call this method in user code,
961         because all xarray functions should either work on deferred data or
962         load data automatically. However, this method can be necessary when
963         working with many file objects on disk.
964 
965         Parameters
966         ----------
967         **kwargs : dict
968             Additional keyword arguments passed on to ``dask.compute``.
969 
970         See Also
971         --------
972         dask.compute
973         """
974         ds = self._to_temp_dataset().load(**kwargs)
975         new = self._from_temp_dataset(ds)
976         self._variable = new._variable
977         self._coords = new._coords
978         return self
979 
980     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
981         """Manually trigger loading of this array's data from disk or a
982         remote source into memory and return a new array. The original is
983         left unaltered.
984 
985         Normally, it should not be necessary to call this method in user code,
986         because all xarray functions should either work on deferred data or
987         load data automatically. However, this method can be necessary when
988         working with many file objects on disk.
989 
990         Parameters
991         ----------
992         **kwargs : dict
993             Additional keyword arguments passed on to ``dask.compute``.
994 
995         See Also
996         --------
997         dask.compute
998         """
999         new = self.copy(deep=False)
1000         return new.load(**kwargs)
1001 
1002     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1003         """Trigger computation in constituent dask arrays
1004 
1005         This keeps them as dask arrays but encourages them to keep data in
1006         memory.  This is particularly useful when on a distributed machine.
1007         When on a single machine consider using ``.compute()`` instead.
1008 
1009         Parameters
1010         ----------
1011         **kwargs : dict
1012             Additional keyword arguments passed on to ``dask.persist``.
1013 
1014         See Also
1015         --------
1016         dask.persist
1017         """
1018         ds = self._to_temp_dataset().persist(**kwargs)
1019         return self._from_temp_dataset(ds)
1020 
1021     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1022         """Returns a copy of this array.
1023 
1024         If `deep=True`, a deep copy is made of the data array.
1025         Otherwise, a shallow copy is made, and the returned data array's
1026         values are a new view of this data array's values.
1027 
1028         Use `data` to create a new object with the same structure as
1029         original but entirely new data.
1030 
1031         Parameters
1032         ----------
1033         deep : bool, optional
1034             Whether the data array and its coordinates are loaded into memory
1035             and copied onto the new object. Default is True.
1036         data : array_like, optional
1037             Data to use in the new object. Must have same shape as original.
1038             When `data` is used, `deep` is ignored for all data variables,
1039             and only used for coords.
1040 
1041         Returns
1042         -------
1043         copy : DataArray
1044             New object with dimensions, attributes, coordinates, name,
1045             encoding, and optionally data copied from original.
1046 
1047         Examples
1048         --------
1049         Shallow versus deep copy
1050 
1051         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1052         >>> array.copy()
1053         <xarray.DataArray (x: 3)>
1054         array([1, 2, 3])
1055         Coordinates:
1056           * x        (x) <U1 'a' 'b' 'c'
1057         >>> array_0 = array.copy(deep=False)
1058         >>> array_0[0] = 7
1059         >>> array_0
1060         <xarray.DataArray (x: 3)>
1061         array([7, 2, 3])
1062         Coordinates:
1063           * x        (x) <U1 'a' 'b' 'c'
1064         >>> array
1065         <xarray.DataArray (x: 3)>
1066         array([7, 2, 3])
1067         Coordinates:
1068           * x        (x) <U1 'a' 'b' 'c'
1069 
1070         Changing the data using the ``data`` argument maintains the
1071         structure of the original object, but with the new data. Original
1072         object is unaffected.
1073 
1074         >>> array.copy(data=[0.1, 0.2, 0.3])
1075         <xarray.DataArray (x: 3)>
1076         array([0.1, 0.2, 0.3])
1077         Coordinates:
1078           * x        (x) <U1 'a' 'b' 'c'
1079         >>> array
1080         <xarray.DataArray (x: 3)>
1081         array([7, 2, 3])
1082         Coordinates:
1083           * x        (x) <U1 'a' 'b' 'c'
1084 
1085         See Also
1086         --------
1087         pandas.DataFrame.copy
1088         """
1089         variable = self.variable.copy(deep=deep, data=data)
1090         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1091 
1092         coords = {}
1093         for k, v in self._coords.items():
1094             if k in index_vars:
1095                 coords[k] = index_vars[k]
1096             else:
1097                 coords[k] = v.copy(deep=deep)
1098 
1099         return self._replace(variable, coords, indexes=indexes)
1100 
1101     def __copy__(self: T_DataArray) -> T_DataArray:
1102         return self.copy(deep=False)
1103 
1104     def __deepcopy__(self: T_DataArray, memo=None) -> T_DataArray:
1105         # memo does nothing but is required for compatibility with
1106         # copy.deepcopy
1107         return self.copy(deep=True)
1108 
1109     # mutable objects should not be Hashable
1110     # https://github.com/python/mypy/issues/4266
1111     __hash__ = None  # type: ignore[assignment]
1112 
1113     @property
1114     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1115         """
1116         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1117         the underlying data is not a dask array.
1118 
1119         See Also
1120         --------
1121         DataArray.chunk
1122         DataArray.chunksizes
1123         xarray.unify_chunks
1124         """
1125         return self.variable.chunks
1126 
1127     @property
1128     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1129         """
1130         Mapping from dimension names to block lengths for this dataarray's data, or None if
1131         the underlying data is not a dask array.
1132         Cannot be modified directly, but can be modified by calling .chunk().
1133 
1134         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1135         instead of a tuple of chunk shapes.
1136 
1137         See Also
1138         --------
1139         DataArray.chunk
1140         DataArray.chunks
1141         xarray.unify_chunks
1142         """
1143         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1144         return get_chunksizes(all_variables)
1145 
1146     def chunk(
1147         self: T_DataArray,
1148         chunks: (
1149             int
1150             | Literal["auto"]
1151             | tuple[int, ...]
1152             | tuple[tuple[int, ...], ...]
1153             | Mapping[Any, None | int | tuple[int, ...]]
1154         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1155         name_prefix: str = "xarray-",
1156         token: str | None = None,
1157         lock: bool = False,
1158         inline_array: bool = False,
1159         **chunks_kwargs: Any,
1160     ) -> T_DataArray:
1161         """Coerce this array's data into a dask arrays with the given chunks.
1162 
1163         If this variable is a non-dask array, it will be converted to dask
1164         array. If it's a dask array, it will be rechunked to the given chunk
1165         sizes.
1166 
1167         If neither chunks is not provided for one or more dimensions, chunk
1168         sizes along that dimension will not be updated; non-dask arrays will be
1169         converted into dask arrays with a single block.
1170 
1171         Parameters
1172         ----------
1173         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1174             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1175             ``{"x": 5, "y": 5}``.
1176         name_prefix : str, optional
1177             Prefix for the name of the new dask array.
1178         token : str, optional
1179             Token uniquely identifying this array.
1180         lock : optional
1181             Passed on to :py:func:`dask.array.from_array`, if the array is not
1182             already as dask array.
1183         inline_array: optional
1184             Passed on to :py:func:`dask.array.from_array`, if the array is not
1185             already as dask array.
1186         **chunks_kwargs : {dim: chunks, ...}, optional
1187             The keyword arguments form of ``chunks``.
1188             One of chunks or chunks_kwargs must be provided.
1189 
1190         Returns
1191         -------
1192         chunked : xarray.DataArray
1193 
1194         See Also
1195         --------
1196         DataArray.chunks
1197         DataArray.chunksizes
1198         xarray.unify_chunks
1199         dask.array.from_array
1200         """
1201         if chunks is None:
1202             warnings.warn(
1203                 "None value for 'chunks' is deprecated. "
1204                 "It will raise an error in the future. Use instead '{}'",
1205                 category=FutureWarning,
1206             )
1207             chunks = {}
1208 
1209         if isinstance(chunks, (float, str, int)):
1210             # ignoring type; unclear why it won't accept a Literal into the value.
1211             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1212         elif isinstance(chunks, (tuple, list)):
1213             chunks = dict(zip(self.dims, chunks))
1214         else:
1215             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1216 
1217         ds = self._to_temp_dataset().chunk(
1218             chunks,
1219             name_prefix=name_prefix,
1220             token=token,
1221             lock=lock,
1222             inline_array=inline_array,
1223         )
1224         return self._from_temp_dataset(ds)
1225 
1226     def isel(
1227         self: T_DataArray,
1228         indexers: Mapping[Any, Any] | None = None,
1229         drop: bool = False,
1230         missing_dims: ErrorOptionsWithWarn = "raise",
1231         **indexers_kwargs: Any,
1232     ) -> T_DataArray:
1233         """Return a new DataArray whose data is given by selecting indexes
1234         along the specified dimension(s).
1235 
1236         Parameters
1237         ----------
1238         indexers : dict, optional
1239             A dict with keys matching dimensions and values given
1240             by integers, slice objects or arrays.
1241             indexer can be a integer, slice, array-like or DataArray.
1242             If DataArrays are passed as indexers, xarray-style indexing will be
1243             carried out. See :ref:`indexing` for the details.
1244             One of indexers or indexers_kwargs must be provided.
1245         drop : bool, default: False
1246             If ``drop=True``, drop coordinates variables indexed by integers
1247             instead of making them scalar.
1248         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1249             What to do if dimensions that should be selected from are not present in the
1250             DataArray:
1251             - "raise": raise an exception
1252             - "warn": raise a warning, and ignore the missing dimensions
1253             - "ignore": ignore the missing dimensions
1254         **indexers_kwargs : {dim: indexer, ...}, optional
1255             The keyword arguments form of ``indexers``.
1256 
1257         Returns
1258         -------
1259         indexed : xarray.DataArray
1260 
1261         See Also
1262         --------
1263         Dataset.isel
1264         DataArray.sel
1265 
1266         Examples
1267         --------
1268         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1269         >>> da
1270         <xarray.DataArray (x: 5, y: 5)>
1271         array([[ 0,  1,  2,  3,  4],
1272                [ 5,  6,  7,  8,  9],
1273                [10, 11, 12, 13, 14],
1274                [15, 16, 17, 18, 19],
1275                [20, 21, 22, 23, 24]])
1276         Dimensions without coordinates: x, y
1277 
1278         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1279         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1280         >>> da = da.isel(x=tgt_x, y=tgt_y)
1281         >>> da
1282         <xarray.DataArray (points: 5)>
1283         array([ 0,  6, 12, 18, 24])
1284         Dimensions without coordinates: points
1285         """
1286 
1287         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1288 
1289         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1290             ds = self._to_temp_dataset()._isel_fancy(
1291                 indexers, drop=drop, missing_dims=missing_dims
1292             )
1293             return self._from_temp_dataset(ds)
1294 
1295         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1296         # lists, or zero or one-dimensional np.ndarray's
1297 
1298         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1299         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1300 
1301         coords = {}
1302         for coord_name, coord_value in self._coords.items():
1303             if coord_name in index_variables:
1304                 coord_value = index_variables[coord_name]
1305             else:
1306                 coord_indexers = {
1307                     k: v for k, v in indexers.items() if k in coord_value.dims
1308                 }
1309                 if coord_indexers:
1310                     coord_value = coord_value.isel(coord_indexers)
1311                     if drop and coord_value.ndim == 0:
1312                         continue
1313             coords[coord_name] = coord_value
1314 
1315         return self._replace(variable=variable, coords=coords, indexes=indexes)
1316 
1317     def sel(
1318         self: T_DataArray,
1319         indexers: Mapping[Any, Any] = None,
1320         method: str = None,
1321         tolerance=None,
1322         drop: bool = False,
1323         **indexers_kwargs: Any,
1324     ) -> T_DataArray:
1325         """Return a new DataArray whose data is given by selecting index
1326         labels along the specified dimension(s).
1327 
1328         In contrast to `DataArray.isel`, indexers for this method should use
1329         labels instead of integers.
1330 
1331         Under the hood, this method is powered by using pandas's powerful Index
1332         objects. This makes label based indexing essentially just as fast as
1333         using integer indexing.
1334 
1335         It also means this method uses pandas's (well documented) logic for
1336         indexing. This means you can use string shortcuts for datetime indexes
1337         (e.g., '2000-01' to select all values in January 2000). It also means
1338         that slices are treated as inclusive of both the start and stop values,
1339         unlike normal Python indexing.
1340 
1341         .. warning::
1342 
1343           Do not try to assign values when using any of the indexing methods
1344           ``isel`` or ``sel``::
1345 
1346             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1347             # DO NOT do this
1348             da.isel(x=[0, 1, 2])[1] = -1
1349 
1350           Assigning values with the chained indexing using ``.sel`` or
1351           ``.isel`` fails silently.
1352 
1353         Parameters
1354         ----------
1355         indexers : dict, optional
1356             A dict with keys matching dimensions and values given
1357             by scalars, slices or arrays of tick labels. For dimensions with
1358             multi-index, the indexer may also be a dict-like object with keys
1359             matching index level names.
1360             If DataArrays are passed as indexers, xarray-style indexing will be
1361             carried out. See :ref:`indexing` for the details.
1362             One of indexers or indexers_kwargs must be provided.
1363         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1364             Method to use for inexact matches:
1365 
1366             - None (default): only exact matches
1367             - pad / ffill: propagate last valid index value forward
1368             - backfill / bfill: propagate next valid index value backward
1369             - nearest: use nearest valid index value
1370 
1371         tolerance : optional
1372             Maximum distance between original and new labels for inexact
1373             matches. The values of the index at the matching locations must
1374             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1375         drop : bool, optional
1376             If ``drop=True``, drop coordinates variables in `indexers` instead
1377             of making them scalar.
1378         **indexers_kwargs : {dim: indexer, ...}, optional
1379             The keyword arguments form of ``indexers``.
1380             One of indexers or indexers_kwargs must be provided.
1381 
1382         Returns
1383         -------
1384         obj : DataArray
1385             A new DataArray with the same contents as this DataArray, except the
1386             data and each dimension is indexed by the appropriate indexers.
1387             If indexer DataArrays have coordinates that do not conflict with
1388             this object, then these coordinates will be attached.
1389             In general, each array's data will be a view of the array's data
1390             in this DataArray, unless vectorized indexing was triggered by using
1391             an array indexer, in which case the data will be a copy.
1392 
1393         See Also
1394         --------
1395         Dataset.sel
1396         DataArray.isel
1397 
1398         Examples
1399         --------
1400         >>> da = xr.DataArray(
1401         ...     np.arange(25).reshape(5, 5),
1402         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1403         ...     dims=("x", "y"),
1404         ... )
1405         >>> da
1406         <xarray.DataArray (x: 5, y: 5)>
1407         array([[ 0,  1,  2,  3,  4],
1408                [ 5,  6,  7,  8,  9],
1409                [10, 11, 12, 13, 14],
1410                [15, 16, 17, 18, 19],
1411                [20, 21, 22, 23, 24]])
1412         Coordinates:
1413           * x        (x) int64 0 1 2 3 4
1414           * y        (y) int64 0 1 2 3 4
1415 
1416         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1417         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1418         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1419         >>> da
1420         <xarray.DataArray (points: 5)>
1421         array([ 0,  6, 12, 18, 24])
1422         Coordinates:
1423             x        (points) int64 0 1 2 3 4
1424             y        (points) int64 0 1 2 3 4
1425         Dimensions without coordinates: points
1426         """
1427         ds = self._to_temp_dataset().sel(
1428             indexers=indexers,
1429             drop=drop,
1430             method=method,
1431             tolerance=tolerance,
1432             **indexers_kwargs,
1433         )
1434         return self._from_temp_dataset(ds)
1435 
1436     def head(
1437         self: T_DataArray,
1438         indexers: Mapping[Any, int] | int | None = None,
1439         **indexers_kwargs: Any,
1440     ) -> T_DataArray:
1441         """Return a new DataArray whose data is given by the the first `n`
1442         values along the specified dimension(s). Default `n` = 5
1443 
1444         See Also
1445         --------
1446         Dataset.head
1447         DataArray.tail
1448         DataArray.thin
1449         """
1450         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1451         return self._from_temp_dataset(ds)
1452 
1453     def tail(
1454         self: T_DataArray,
1455         indexers: Mapping[Any, int] | int | None = None,
1456         **indexers_kwargs: Any,
1457     ) -> T_DataArray:
1458         """Return a new DataArray whose data is given by the the last `n`
1459         values along the specified dimension(s). Default `n` = 5
1460 
1461         See Also
1462         --------
1463         Dataset.tail
1464         DataArray.head
1465         DataArray.thin
1466         """
1467         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1468         return self._from_temp_dataset(ds)
1469 
1470     def thin(
1471         self: T_DataArray,
1472         indexers: Mapping[Any, int] | int | None = None,
1473         **indexers_kwargs: Any,
1474     ) -> T_DataArray:
1475         """Return a new DataArray whose data is given by each `n` value
1476         along the specified dimension(s).
1477 
1478         Examples
1479         --------
1480         >>> x_arr = np.arange(0, 26)
1481         >>> x_arr
1482         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1483                17, 18, 19, 20, 21, 22, 23, 24, 25])
1484         >>> x = xr.DataArray(
1485         ...     np.reshape(x_arr, (2, 13)),
1486         ...     dims=("x", "y"),
1487         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1488         ... )
1489         >>> x
1490         <xarray.DataArray (x: 2, y: 13)>
1491         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1492                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1493         Coordinates:
1494           * x        (x) int64 0 1
1495           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1496 
1497         >>>
1498         >>> x.thin(3)
1499         <xarray.DataArray (x: 1, y: 5)>
1500         array([[ 0,  3,  6,  9, 12]])
1501         Coordinates:
1502           * x        (x) int64 0
1503           * y        (y) int64 0 3 6 9 12
1504         >>> x.thin({"x": 2, "y": 5})
1505         <xarray.DataArray (x: 1, y: 3)>
1506         array([[ 0,  5, 10]])
1507         Coordinates:
1508           * x        (x) int64 0
1509           * y        (y) int64 0 5 10
1510 
1511         See Also
1512         --------
1513         Dataset.thin
1514         DataArray.head
1515         DataArray.tail
1516         """
1517         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1518         return self._from_temp_dataset(ds)
1519 
1520     def broadcast_like(
1521         self: T_DataArray,
1522         other: DataArray | Dataset,
1523         exclude: Iterable[Hashable] | None = None,
1524     ) -> T_DataArray:
1525         """Broadcast this DataArray against another Dataset or DataArray.
1526 
1527         This is equivalent to xr.broadcast(other, self)[1]
1528 
1529         xarray objects are broadcast against each other in arithmetic
1530         operations, so this method is not be necessary for most uses.
1531 
1532         If no change is needed, the input data is returned to the output
1533         without being copied.
1534 
1535         If new coords are added by the broadcast, their values are
1536         NaN filled.
1537 
1538         Parameters
1539         ----------
1540         other : Dataset or DataArray
1541             Object against which to broadcast this array.
1542         exclude : iterable of Hashable, optional
1543             Dimensions that must not be broadcasted
1544 
1545         Returns
1546         -------
1547         new_da : DataArray
1548             The caller broadcasted against ``other``.
1549 
1550         Examples
1551         --------
1552         >>> arr1 = xr.DataArray(
1553         ...     np.random.randn(2, 3),
1554         ...     dims=("x", "y"),
1555         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1556         ... )
1557         >>> arr2 = xr.DataArray(
1558         ...     np.random.randn(3, 2),
1559         ...     dims=("x", "y"),
1560         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1561         ... )
1562         >>> arr1
1563         <xarray.DataArray (x: 2, y: 3)>
1564         array([[ 1.76405235,  0.40015721,  0.97873798],
1565                [ 2.2408932 ,  1.86755799, -0.97727788]])
1566         Coordinates:
1567           * x        (x) <U1 'a' 'b'
1568           * y        (y) <U1 'a' 'b' 'c'
1569         >>> arr2
1570         <xarray.DataArray (x: 3, y: 2)>
1571         array([[ 0.95008842, -0.15135721],
1572                [-0.10321885,  0.4105985 ],
1573                [ 0.14404357,  1.45427351]])
1574         Coordinates:
1575           * x        (x) <U1 'a' 'b' 'c'
1576           * y        (y) <U1 'a' 'b'
1577         >>> arr1.broadcast_like(arr2)
1578         <xarray.DataArray (x: 3, y: 3)>
1579         array([[ 1.76405235,  0.40015721,  0.97873798],
1580                [ 2.2408932 ,  1.86755799, -0.97727788],
1581                [        nan,         nan,         nan]])
1582         Coordinates:
1583           * x        (x) <U1 'a' 'b' 'c'
1584           * y        (y) <U1 'a' 'b' 'c'
1585         """
1586         if exclude is None:
1587             exclude = set()
1588         else:
1589             exclude = set(exclude)
1590         args = align(other, self, join="outer", copy=False, exclude=exclude)
1591 
1592         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1593 
1594         return _broadcast_helper(
1595             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1596         )
1597 
1598     def _reindex_callback(
1599         self: T_DataArray,
1600         aligner: alignment.Aligner,
1601         dim_pos_indexers: dict[Hashable, Any],
1602         variables: dict[Hashable, Variable],
1603         indexes: dict[Hashable, Index],
1604         fill_value: Any,
1605         exclude_dims: frozenset[Hashable],
1606         exclude_vars: frozenset[Hashable],
1607     ) -> T_DataArray:
1608         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1609 
1610         if isinstance(fill_value, dict):
1611             fill_value = fill_value.copy()
1612             sentinel = object()
1613             value = fill_value.pop(self.name, sentinel)
1614             if value is not sentinel:
1615                 fill_value[_THIS_ARRAY] = value
1616 
1617         ds = self._to_temp_dataset()
1618         reindexed = ds._reindex_callback(
1619             aligner,
1620             dim_pos_indexers,
1621             variables,
1622             indexes,
1623             fill_value,
1624             exclude_dims,
1625             exclude_vars,
1626         )
1627         return self._from_temp_dataset(reindexed)
1628 
1629     def reindex_like(
1630         self: T_DataArray,
1631         other: DataArray | Dataset,
1632         method: ReindexMethodOptions = None,
1633         tolerance: int | float | Iterable[int | float] | None = None,
1634         copy: bool = True,
1635         fill_value=dtypes.NA,
1636     ) -> T_DataArray:
1637         """Conform this object onto the indexes of another object, filling in
1638         missing values with ``fill_value``. The default fill value is NaN.
1639 
1640         Parameters
1641         ----------
1642         other : Dataset or DataArray
1643             Object with an 'indexes' attribute giving a mapping from dimension
1644             names to pandas.Index objects, which provides coordinates upon
1645             which to index the variables in this dataset. The indexes on this
1646             other object need not be the same as the indexes on this
1647             dataset. Any mis-matched index values will be filled in with
1648             NaN, and any mis-matched dimension names will simply be ignored.
1649         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1650             Method to use for filling index values from other not found on this
1651             data array:
1652 
1653             - None (default): don't fill gaps
1654             - pad / ffill: propagate last valid index value forward
1655             - backfill / bfill: propagate next valid index value backward
1656             - nearest: use nearest valid index value
1657 
1658         tolerance : optional
1659             Maximum distance between original and new labels for inexact
1660             matches. The values of the index at the matching locations must
1661             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1662             Tolerance may be a scalar value, which applies the same tolerance
1663             to all values, or list-like, which applies variable tolerance per
1664             element. List-like must be the same size as the index and its dtype
1665             must exactly match the index’s type.
1666         copy : bool, default: True
1667             If ``copy=True``, data in the return value is always copied. If
1668             ``copy=False`` and reindexing is unnecessary, or can be performed
1669             with only slice operations, then the output may share memory with
1670             the input. In either case, a new xarray object is always returned.
1671         fill_value : scalar or dict-like, optional
1672             Value to use for newly missing values. If a dict-like, maps
1673             variable names (including coordinates) to fill values. Use this
1674             data array's name to refer to the data array's values.
1675 
1676         Returns
1677         -------
1678         reindexed : DataArray
1679             Another dataset array, with this array's data but coordinates from
1680             the other object.
1681 
1682         See Also
1683         --------
1684         DataArray.reindex
1685         align
1686         """
1687         return alignment.reindex_like(
1688             self,
1689             other=other,
1690             method=method,
1691             tolerance=tolerance,
1692             copy=copy,
1693             fill_value=fill_value,
1694         )
1695 
1696     def reindex(
1697         self: T_DataArray,
1698         indexers: Mapping[Any, Any] = None,
1699         method: ReindexMethodOptions = None,
1700         tolerance: float | Iterable[float] | None = None,
1701         copy: bool = True,
1702         fill_value=dtypes.NA,
1703         **indexers_kwargs: Any,
1704     ) -> T_DataArray:
1705         """Conform this object onto the indexes of another object, filling in
1706         missing values with ``fill_value``. The default fill value is NaN.
1707 
1708         Parameters
1709         ----------
1710         indexers : dict, optional
1711             Dictionary with keys given by dimension names and values given by
1712             arrays of coordinates tick labels. Any mis-matched coordinate
1713             values will be filled in with NaN, and any mis-matched dimension
1714             names will simply be ignored.
1715             One of indexers or indexers_kwargs must be provided.
1716         copy : bool, optional
1717             If ``copy=True``, data in the return value is always copied. If
1718             ``copy=False`` and reindexing is unnecessary, or can be performed
1719             with only slice operations, then the output may share memory with
1720             the input. In either case, a new xarray object is always returned.
1721         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1722             Method to use for filling index values in ``indexers`` not found on
1723             this data array:
1724 
1725             - None (default): don't fill gaps
1726             - pad / ffill: propagate last valid index value forward
1727             - backfill / bfill: propagate next valid index value backward
1728             - nearest: use nearest valid index value
1729 
1730         tolerance : float | Iterable[float] | None, default: None
1731             Maximum distance between original and new labels for inexact
1732             matches. The values of the index at the matching locations must
1733             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1734             Tolerance may be a scalar value, which applies the same tolerance
1735             to all values, or list-like, which applies variable tolerance per
1736             element. List-like must be the same size as the index and its dtype
1737             must exactly match the index’s type.
1738         fill_value : scalar or dict-like, optional
1739             Value to use for newly missing values. If a dict-like, maps
1740             variable names (including coordinates) to fill values. Use this
1741             data array's name to refer to the data array's values.
1742         **indexers_kwargs : {dim: indexer, ...}, optional
1743             The keyword arguments form of ``indexers``.
1744             One of indexers or indexers_kwargs must be provided.
1745 
1746         Returns
1747         -------
1748         reindexed : DataArray
1749             Another dataset array, with this array's data but replaced
1750             coordinates.
1751 
1752         Examples
1753         --------
1754         Reverse latitude:
1755 
1756         >>> da = xr.DataArray(
1757         ...     np.arange(4),
1758         ...     coords=[np.array([90, 89, 88, 87])],
1759         ...     dims="lat",
1760         ... )
1761         >>> da
1762         <xarray.DataArray (lat: 4)>
1763         array([0, 1, 2, 3])
1764         Coordinates:
1765           * lat      (lat) int64 90 89 88 87
1766         >>> da.reindex(lat=da.lat[::-1])
1767         <xarray.DataArray (lat: 4)>
1768         array([3, 2, 1, 0])
1769         Coordinates:
1770           * lat      (lat) int64 87 88 89 90
1771 
1772         See Also
1773         --------
1774         DataArray.reindex_like
1775         align
1776         """
1777         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1778         return alignment.reindex(
1779             self,
1780             indexers=indexers,
1781             method=method,
1782             tolerance=tolerance,
1783             copy=copy,
1784             fill_value=fill_value,
1785         )
1786 
1787     def interp(
1788         self: T_DataArray,
1789         coords: Mapping[Any, Any] | None = None,
1790         method: InterpOptions = "linear",
1791         assume_sorted: bool = False,
1792         kwargs: Mapping[str, Any] | None = None,
1793         **coords_kwargs: Any,
1794     ) -> T_DataArray:
1795         """Interpolate a DataArray onto new coordinates
1796 
1797         Performs univariate or multivariate interpolation of a DataArray onto
1798         new coordinates using scipy's interpolation routines. If interpolating
1799         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
1800         called. When interpolating along multiple existing dimensions, an
1801         attempt is made to decompose the interpolation into multiple
1802         1-dimensional interpolations. If this is possible,
1803         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1804         :py:func:`scipy.interpolate.interpn` is called.
1805 
1806         Parameters
1807         ----------
1808         coords : dict, optional
1809             Mapping from dimension names to the new coordinates.
1810             New coordinate can be a scalar, array-like or DataArray.
1811             If DataArrays are passed as new coordinates, their dimensions are
1812             used for the broadcasting. Missing values are skipped.
1813         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1814             The method used to interpolate. The method should be supported by
1815             the scipy interpolator:
1816 
1817             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
1818               "quadratic", "cubic", "polynomial"}
1819             - ``interpn``: {"linear", "nearest"}
1820 
1821             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1822             also be provided.
1823         assume_sorted : bool, default: False
1824             If False, values of x can be in any order and they are sorted
1825             first. If True, x has to be an array of monotonically increasing
1826             values.
1827         kwargs : dict-like or None, default: None
1828             Additional keyword arguments passed to scipy's interpolator. Valid
1829             options and their behavior depend whether ``interp1d`` or
1830             ``interpn`` is used.
1831         **coords_kwargs : {dim: coordinate, ...}, optional
1832             The keyword arguments form of ``coords``.
1833             One of coords or coords_kwargs must be provided.
1834 
1835         Returns
1836         -------
1837         interpolated : DataArray
1838             New dataarray on the new coordinates.
1839 
1840         Notes
1841         -----
1842         scipy is required.
1843 
1844         See Also
1845         --------
1846         scipy.interpolate.interp1d
1847         scipy.interpolate.interpn
1848 
1849         Examples
1850         --------
1851         >>> da = xr.DataArray(
1852         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1853         ...     dims=("x", "y"),
1854         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1855         ... )
1856         >>> da
1857         <xarray.DataArray (x: 3, y: 4)>
1858         array([[ 1.,  4.,  2.,  9.],
1859                [ 2.,  7.,  6., nan],
1860                [ 6., nan,  5.,  8.]])
1861         Coordinates:
1862           * x        (x) int64 0 1 2
1863           * y        (y) int64 10 12 14 16
1864 
1865         1D linear interpolation (the default):
1866 
1867         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1868         <xarray.DataArray (x: 4, y: 4)>
1869         array([[1.  , 4.  , 2.  ,  nan],
1870                [1.75, 6.25, 5.  ,  nan],
1871                [3.  ,  nan, 5.75,  nan],
1872                [5.  ,  nan, 5.25,  nan]])
1873         Coordinates:
1874           * y        (y) int64 10 12 14 16
1875           * x        (x) float64 0.0 0.75 1.25 1.75
1876 
1877         1D nearest interpolation:
1878 
1879         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1880         <xarray.DataArray (x: 4, y: 4)>
1881         array([[ 1.,  4.,  2.,  9.],
1882                [ 2.,  7.,  6., nan],
1883                [ 2.,  7.,  6., nan],
1884                [ 6., nan,  5.,  8.]])
1885         Coordinates:
1886           * y        (y) int64 10 12 14 16
1887           * x        (x) float64 0.0 0.75 1.25 1.75
1888 
1889         1D linear extrapolation:
1890 
1891         >>> da.interp(
1892         ...     x=[1, 1.5, 2.5, 3.5],
1893         ...     method="linear",
1894         ...     kwargs={"fill_value": "extrapolate"},
1895         ... )
1896         <xarray.DataArray (x: 4, y: 4)>
1897         array([[ 2. ,  7. ,  6. ,  nan],
1898                [ 4. ,  nan,  5.5,  nan],
1899                [ 8. ,  nan,  4.5,  nan],
1900                [12. ,  nan,  3.5,  nan]])
1901         Coordinates:
1902           * y        (y) int64 10 12 14 16
1903           * x        (x) float64 1.0 1.5 2.5 3.5
1904 
1905         2D linear interpolation:
1906 
1907         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1908         <xarray.DataArray (x: 4, y: 3)>
1909         array([[2.5  , 3.   ,   nan],
1910                [4.   , 5.625,   nan],
1911                [  nan,   nan,   nan],
1912                [  nan,   nan,   nan]])
1913         Coordinates:
1914           * x        (x) float64 0.0 0.75 1.25 1.75
1915           * y        (y) int64 11 13 15
1916         """
1917         if self.dtype.kind not in "uifc":
1918             raise TypeError(
1919                 "interp only works for a numeric type array. "
1920                 "Given {}.".format(self.dtype)
1921             )
1922         ds = self._to_temp_dataset().interp(
1923             coords,
1924             method=method,
1925             kwargs=kwargs,
1926             assume_sorted=assume_sorted,
1927             **coords_kwargs,
1928         )
1929         return self._from_temp_dataset(ds)
1930 
1931     def interp_like(
1932         self: T_DataArray,
1933         other: DataArray | Dataset,
1934         method: InterpOptions = "linear",
1935         assume_sorted: bool = False,
1936         kwargs: Mapping[str, Any] | None = None,
1937     ) -> T_DataArray:
1938         """Interpolate this object onto the coordinates of another object,
1939         filling out of range values with NaN.
1940 
1941         If interpolating along a single existing dimension,
1942         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
1943         along multiple existing dimensions, an attempt is made to decompose the
1944         interpolation into multiple 1-dimensional interpolations. If this is
1945         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1946         :py:func:`scipy.interpolate.interpn` is called.
1947 
1948         Parameters
1949         ----------
1950         other : Dataset or DataArray
1951             Object with an 'indexes' attribute giving a mapping from dimension
1952             names to an 1d array-like, which provides coordinates upon
1953             which to index the variables in this dataset. Missing values are skipped.
1954         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1955             The method used to interpolate. The method should be supported by
1956             the scipy interpolator:
1957 
1958             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
1959               "polynomial"} when ``interp1d`` is called.
1960             - {"linear", "nearest"} when ``interpn`` is called.
1961 
1962             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1963             also be provided.
1964         assume_sorted : bool, default: False
1965             If False, values of coordinates that are interpolated over can be
1966             in any order and they are sorted first. If True, interpolated
1967             coordinates are assumed to be an array of monotonically increasing
1968             values.
1969         kwargs : dict, optional
1970             Additional keyword passed to scipy's interpolator.
1971 
1972         Returns
1973         -------
1974         interpolated : DataArray
1975             Another dataarray by interpolating this dataarray's data along the
1976             coordinates of the other object.
1977 
1978         Notes
1979         -----
1980         scipy is required.
1981         If the dataarray has object-type coordinates, reindex is used for these
1982         coordinates instead of the interpolation.
1983 
1984         See Also
1985         --------
1986         DataArray.interp
1987         DataArray.reindex_like
1988         """
1989         if self.dtype.kind not in "uifc":
1990             raise TypeError(
1991                 "interp only works for a numeric type array. "
1992                 "Given {}.".format(self.dtype)
1993             )
1994         ds = self._to_temp_dataset().interp_like(
1995             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1996         )
1997         return self._from_temp_dataset(ds)
1998 
1999     # change type of self and return to T_DataArray once
2000     # https://github.com/python/mypy/issues/12846 is resolved
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)
2044 
2045     def swap_dims(
2046         self: T_DataArray,
2047         dims_dict: Mapping[Any, Hashable] | None = None,
2048         **dims_kwargs,
2049     ) -> T_DataArray:
2050         """Returns a new DataArray with swapped dimensions.
2051 
2052         Parameters
2053         ----------
2054         dims_dict : dict-like
2055             Dictionary whose keys are current dimension names and whose values
2056             are new names.
2057         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2058             The keyword arguments form of ``dims_dict``.
2059             One of dims_dict or dims_kwargs must be provided.
2060 
2061         Returns
2062         -------
2063         swapped : DataArray
2064             DataArray with swapped dimensions.
2065 
2066         Examples
2067         --------
2068         >>> arr = xr.DataArray(
2069         ...     data=[0, 1],
2070         ...     dims="x",
2071         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2072         ... )
2073         >>> arr
2074         <xarray.DataArray (x: 2)>
2075         array([0, 1])
2076         Coordinates:
2077           * x        (x) <U1 'a' 'b'
2078             y        (x) int64 0 1
2079 
2080         >>> arr.swap_dims({"x": "y"})
2081         <xarray.DataArray (y: 2)>
2082         array([0, 1])
2083         Coordinates:
2084             x        (y) <U1 'a' 'b'
2085           * y        (y) int64 0 1
2086 
2087         >>> arr.swap_dims({"x": "z"})
2088         <xarray.DataArray (z: 2)>
2089         array([0, 1])
2090         Coordinates:
2091             x        (z) <U1 'a' 'b'
2092             y        (z) int64 0 1
2093         Dimensions without coordinates: z
2094 
2095         See Also
2096         --------
2097         DataArray.rename
2098         Dataset.swap_dims
2099         """
2100         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2101         ds = self._to_temp_dataset().swap_dims(dims_dict)
2102         return self._from_temp_dataset(ds)
2103 
2104     # change type of self and return to T_DataArray once
2105     # https://github.com/python/mypy/issues/12846 is resolved
2106     def expand_dims(
2107         self,
2108         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2109         axis: None | int | Sequence[int] = None,
2110         **dim_kwargs: Any,
2111     ) -> DataArray:
2112         """Return a new object with an additional axis (or axes) inserted at
2113         the corresponding position in the array shape. The new object is a
2114         view into the underlying array, not a copy.
2115 
2116         If dim is already a scalar coordinate, it will be promoted to a 1D
2117         coordinate consisting of a single value.
2118 
2119         Parameters
2120         ----------
2121         dim : Hashable, sequence of Hashable, dict, or None, optional
2122             Dimensions to include on the new variable.
2123             If provided as str or sequence of str, then dimensions are inserted
2124             with length 1. If provided as a dict, then the keys are the new
2125             dimensions and the values are either integers (giving the length of
2126             the new dimensions) or sequence/ndarray (giving the coordinates of
2127             the new dimensions).
2128         axis : int, sequence of int, or None, default: None
2129             Axis position(s) where new axis is to be inserted (position(s) on
2130             the result array). If a sequence of integers is passed,
2131             multiple axes are inserted. In this case, dim arguments should be
2132             same length list. If axis=None is passed, all the axes will be
2133             inserted to the start of the result array.
2134         **dim_kwargs : int or sequence or ndarray
2135             The keywords are arbitrary dimensions being inserted and the values
2136             are either the lengths of the new dims (if int is given), or their
2137             coordinates. Note, this is an alternative to passing a dict to the
2138             dim kwarg and will only be used if dim is None.
2139 
2140         Returns
2141         -------
2142         expanded : DataArray
2143             This object, but with additional dimension(s).
2144 
2145         See Also
2146         --------
2147         Dataset.expand_dims
2148         """
2149         if isinstance(dim, int):
2150             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2151         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2152             if len(dim) != len(set(dim)):
2153                 raise ValueError("dims should not contain duplicate values.")
2154             dim = dict.fromkeys(dim, 1)
2155         elif dim is not None and not isinstance(dim, Mapping):
2156             dim = {cast(Hashable, dim): 1}
2157 
2158         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2159         ds = self._to_temp_dataset().expand_dims(dim, axis)
2160         return self._from_temp_dataset(ds)
2161 
2162     # change type of self and return to T_DataArray once
2163     # https://github.com/python/mypy/issues/12846 is resolved
2164     def set_index(
2165         self,
2166         indexes: Mapping[Any, Hashable | Sequence[Hashable]] = None,
2167         append: bool = False,
2168         **indexes_kwargs: Hashable | Sequence[Hashable],
2169     ) -> DataArray:
2170         """Set DataArray (multi-)indexes using one or more existing
2171         coordinates.
2172 
2173         Parameters
2174         ----------
2175         indexes : {dim: index, ...}
2176             Mapping from names matching dimensions and values given
2177             by (lists of) the names of existing coordinates or variables to set
2178             as new (multi-)index.
2179         append : bool, default: False
2180             If True, append the supplied index(es) to the existing index(es).
2181             Otherwise replace the existing index(es).
2182         **indexes_kwargs : optional
2183             The keyword arguments form of ``indexes``.
2184             One of indexes or indexes_kwargs must be provided.
2185 
2186         Returns
2187         -------
2188         obj : DataArray
2189             Another DataArray, with this data but replaced coordinates.
2190 
2191         Examples
2192         --------
2193         >>> arr = xr.DataArray(
2194         ...     data=np.ones((2, 3)),
2195         ...     dims=["x", "y"],
2196         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2197         ... )
2198         >>> arr
2199         <xarray.DataArray (x: 2, y: 3)>
2200         array([[1., 1., 1.],
2201                [1., 1., 1.]])
2202         Coordinates:
2203           * x        (x) int64 0 1
2204           * y        (y) int64 0 1 2
2205             a        (x) int64 3 4
2206         >>> arr.set_index(x="a")
2207         <xarray.DataArray (x: 2, y: 3)>
2208         array([[1., 1., 1.],
2209                [1., 1., 1.]])
2210         Coordinates:
2211           * x        (x) int64 3 4
2212           * y        (y) int64 0 1 2
2213 
2214         See Also
2215         --------
2216         DataArray.reset_index
2217         """
2218         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2219         return self._from_temp_dataset(ds)
2220 
2221     # change type of self and return to T_DataArray once
2222     # https://github.com/python/mypy/issues/12846 is resolved
2223     def reset_index(
2224         self,
2225         dims_or_levels: Hashable | Sequence[Hashable],
2226         drop: bool = False,
2227     ) -> DataArray:
2228         """Reset the specified index(es) or multi-index level(s).
2229 
2230         Parameters
2231         ----------
2232         dims_or_levels : Hashable or sequence of Hashable
2233             Name(s) of the dimension(s) and/or multi-index level(s) that will
2234             be reset.
2235         drop : bool, default: False
2236             If True, remove the specified indexes and/or multi-index levels
2237             instead of extracting them as new coordinates (default: False).
2238 
2239         Returns
2240         -------
2241         obj : DataArray
2242             Another dataarray, with this dataarray's data but replaced
2243             coordinates.
2244 
2245         See Also
2246         --------
2247         DataArray.set_index
2248         """
2249         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2250         return self._from_temp_dataset(ds)
2251 
2252     def reorder_levels(
2253         self: T_DataArray,
2254         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2255         **dim_order_kwargs: Sequence[int | Hashable],
2256     ) -> T_DataArray:
2257         """Rearrange index levels using input order.
2258 
2259         Parameters
2260         ----------
2261         dim_order dict-like of Hashable to int or Hashable: optional
2262             Mapping from names matching dimensions and values given
2263             by lists representing new level orders. Every given dimension
2264             must have a multi-index.
2265         **dim_order_kwargs : optional
2266             The keyword arguments form of ``dim_order``.
2267             One of dim_order or dim_order_kwargs must be provided.
2268 
2269         Returns
2270         -------
2271         obj : DataArray
2272             Another dataarray, with this dataarray's data but replaced
2273             coordinates.
2274         """
2275         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2276         return self._from_temp_dataset(ds)
2277 
2278     def stack(
2279         self: T_DataArray,
2280         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2281         create_index: bool | None = True,
2282         index_cls: type[Index] = PandasMultiIndex,
2283         **dimensions_kwargs: Sequence[Hashable],
2284     ) -> T_DataArray:
2285         """
2286         Stack any number of existing dimensions into a single new dimension.
2287 
2288         New dimensions will be added at the end, and the corresponding
2289         coordinate variables will be combined into a MultiIndex.
2290 
2291         Parameters
2292         ----------
2293         dimensions : mapping of Hashable to sequence of Hashable
2294             Mapping of the form `new_name=(dim1, dim2, ...)`.
2295             Names of new dimensions, and the existing dimensions that they
2296             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2297             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2298             all dimensions.
2299         create_index : bool or None, default: True
2300             If True, create a multi-index for each of the stacked dimensions.
2301             If False, don't create any index.
2302             If None, create a multi-index only if exactly one single (1-d) coordinate
2303             index is found for every dimension to stack.
2304         index_cls: class, optional
2305             Can be used to pass a custom multi-index type. Must be an Xarray index that
2306             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2307         **dimensions_kwargs
2308             The keyword arguments form of ``dimensions``.
2309             One of dimensions or dimensions_kwargs must be provided.
2310 
2311         Returns
2312         -------
2313         stacked : DataArray
2314             DataArray with stacked data.
2315 
2316         Examples
2317         --------
2318         >>> arr = xr.DataArray(
2319         ...     np.arange(6).reshape(2, 3),
2320         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2321         ... )
2322         >>> arr
2323         <xarray.DataArray (x: 2, y: 3)>
2324         array([[0, 1, 2],
2325                [3, 4, 5]])
2326         Coordinates:
2327           * x        (x) <U1 'a' 'b'
2328           * y        (y) int64 0 1 2
2329         >>> stacked = arr.stack(z=("x", "y"))
2330         >>> stacked.indexes["z"]
2331         MultiIndex([('a', 0),
2332                     ('a', 1),
2333                     ('a', 2),
2334                     ('b', 0),
2335                     ('b', 1),
2336                     ('b', 2)],
2337                    name='z')
2338 
2339         See Also
2340         --------
2341         DataArray.unstack
2342         """
2343         ds = self._to_temp_dataset().stack(
2344             dimensions,
2345             create_index=create_index,
2346             index_cls=index_cls,
2347             **dimensions_kwargs,
2348         )
2349         return self._from_temp_dataset(ds)
2350 
2351     # change type of self and return to T_DataArray once
2352     # https://github.com/python/mypy/issues/12846 is resolved
2353     def unstack(
2354         self,
2355         dim: Hashable | Sequence[Hashable] | None = None,
2356         fill_value: Any = dtypes.NA,
2357         sparse: bool = False,
2358     ) -> DataArray:
2359         """
2360         Unstack existing dimensions corresponding to MultiIndexes into
2361         multiple new dimensions.
2362 
2363         New dimensions will be added at the end.
2364 
2365         Parameters
2366         ----------
2367         dim : Hashable or sequence of Hashable, optional
2368             Dimension(s) over which to unstack. By default unstacks all
2369             MultiIndexes.
2370         fill_value : scalar or dict-like, default: nan
2371             Value to be filled. If a dict-like, maps variable names to
2372             fill values. Use the data array's name to refer to its
2373             name. If not provided or if the dict-like does not contain
2374             all variables, the dtype's NA value will be used.
2375         sparse : bool, default: False
2376             Use sparse-array if True
2377 
2378         Returns
2379         -------
2380         unstacked : DataArray
2381             Array with unstacked data.
2382 
2383         Examples
2384         --------
2385         >>> arr = xr.DataArray(
2386         ...     np.arange(6).reshape(2, 3),
2387         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2388         ... )
2389         >>> arr
2390         <xarray.DataArray (x: 2, y: 3)>
2391         array([[0, 1, 2],
2392                [3, 4, 5]])
2393         Coordinates:
2394           * x        (x) <U1 'a' 'b'
2395           * y        (y) int64 0 1 2
2396         >>> stacked = arr.stack(z=("x", "y"))
2397         >>> stacked.indexes["z"]
2398         MultiIndex([('a', 0),
2399                     ('a', 1),
2400                     ('a', 2),
2401                     ('b', 0),
2402                     ('b', 1),
2403                     ('b', 2)],
2404                    name='z')
2405         >>> roundtripped = stacked.unstack()
2406         >>> arr.identical(roundtripped)
2407         True
2408 
2409         See Also
2410         --------
2411         DataArray.stack
2412         """
2413         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2414         return self._from_temp_dataset(ds)
2415 
2416     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2417         """Unstack DataArray expanding to Dataset along a given level of a
2418         stacked coordinate.
2419 
2420         This is the inverse operation of Dataset.to_stacked_array.
2421 
2422         Parameters
2423         ----------
2424         dim : Hashable
2425             Name of existing dimension to unstack
2426         level : int or Hashable, default: 0
2427             The MultiIndex level to expand to a dataset along. Can either be
2428             the integer index of the level or its name.
2429 
2430         Returns
2431         -------
2432         unstacked: Dataset
2433 
2434         Examples
2435         --------
2436         >>> arr = xr.DataArray(
2437         ...     np.arange(6).reshape(2, 3),
2438         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2439         ... )
2440         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2441         >>> data
2442         <xarray.Dataset>
2443         Dimensions:  (x: 2, y: 3)
2444         Coordinates:
2445           * x        (x) <U1 'a' 'b'
2446           * y        (y) int64 0 1 2
2447         Data variables:
2448             a        (x, y) int64 0 1 2 3 4 5
2449             b        (x) int64 0 3
2450         >>> stacked = data.to_stacked_array("z", ["x"])
2451         >>> stacked.indexes["z"]
2452         MultiIndex([('a', 0.0),
2453                     ('a', 1.0),
2454                     ('a', 2.0),
2455                     ('b', nan)],
2456                    name='z')
2457         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2458         >>> data.identical(roundtripped)
2459         True
2460 
2461         See Also
2462         --------
2463         Dataset.to_stacked_array
2464         """
2465         idx = self._indexes[dim].to_pandas_index()
2466         if not isinstance(idx, pd.MultiIndex):
2467             raise ValueError(f"'{dim}' is not a stacked coordinate")
2468 
2469         level_number = idx._get_level_number(level)
2470         variables = idx.levels[level_number]
2471         variable_dim = idx.names[level_number]
2472 
2473         # pull variables out of datarray
2474         data_dict = {}
2475         for k in variables:
2476             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2477 
2478         # unstacked dataset
2479         return Dataset(data_dict)
2480 
2481     def transpose(
2482         self: T_DataArray,
2483         *dims: Hashable,
2484         transpose_coords: bool = True,
2485         missing_dims: ErrorOptionsWithWarn = "raise",
2486     ) -> T_DataArray:
2487         """Return a new DataArray object with transposed dimensions.
2488 
2489         Parameters
2490         ----------
2491         *dims : Hashable, optional
2492             By default, reverse the dimensions. Otherwise, reorder the
2493             dimensions to this order.
2494         transpose_coords : bool, default: True
2495             If True, also transpose the coordinates of this DataArray.
2496         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2497             What to do if dimensions that should be selected from are not present in the
2498             DataArray:
2499             - "raise": raise an exception
2500             - "warn": raise a warning, and ignore the missing dimensions
2501             - "ignore": ignore the missing dimensions
2502 
2503         Returns
2504         -------
2505         transposed : DataArray
2506             The returned DataArray's array is transposed.
2507 
2508         Notes
2509         -----
2510         This operation returns a view of this array's data. It is
2511         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2512         -- the data will be fully loaded.
2513 
2514         See Also
2515         --------
2516         numpy.transpose
2517         Dataset.transpose
2518         """
2519         if dims:
2520             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2521         variable = self.variable.transpose(*dims)
2522         if transpose_coords:
2523             coords: dict[Hashable, Variable] = {}
2524             for name, coord in self.coords.items():
2525                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2526                 coords[name] = coord.variable.transpose(*coord_dims)
2527             return self._replace(variable, coords)
2528         else:
2529             return self._replace(variable)
2530 
2531     @property
2532     def T(self: T_DataArray) -> T_DataArray:
2533         return self.transpose()
2534 
2535     # change type of self and return to T_DataArray once
2536     # https://github.com/python/mypy/issues/12846 is resolved
2537     def drop_vars(
2538         self,
2539         names: Hashable | Iterable[Hashable],
2540         *,
2541         errors: ErrorOptions = "raise",
2542     ) -> DataArray:
2543         """Returns an array with dropped variables.
2544 
2545         Parameters
2546         ----------
2547         names : Hashable or iterable of Hashable
2548             Name(s) of variables to drop.
2549         errors : {"raise", "ignore"}, default: "raise"
2550             If 'raise', raises a ValueError error if any of the variable
2551             passed are not in the dataset. If 'ignore', any given names that are in the
2552             DataArray are dropped and no error is raised.
2553 
2554         Returns
2555         -------
2556         dropped : Dataset
2557             New Dataset copied from `self` with variables removed.
2558         """
2559         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2560         return self._from_temp_dataset(ds)
2561 
2562     def drop(
2563         self: T_DataArray,
2564         labels: Mapping[Any, Any] | None = None,
2565         dim: Hashable | None = None,
2566         *,
2567         errors: ErrorOptions = "raise",
2568         **labels_kwargs,
2569     ) -> T_DataArray:
2570         """Backward compatible method based on `drop_vars` and `drop_sel`
2571 
2572         Using either `drop_vars` or `drop_sel` is encouraged
2573 
2574         See Also
2575         --------
2576         DataArray.drop_vars
2577         DataArray.drop_sel
2578         """
2579         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
2580         return self._from_temp_dataset(ds)
2581 
2582     def drop_sel(
2583         self: T_DataArray,
2584         labels: Mapping[Any, Any] | None = None,
2585         *,
2586         errors: ErrorOptions = "raise",
2587         **labels_kwargs,
2588     ) -> T_DataArray:
2589         """Drop index labels from this DataArray.
2590 
2591         Parameters
2592         ----------
2593         labels : mapping of Hashable to Any
2594             Index labels to drop
2595         errors : {"raise", "ignore"}, default: "raise"
2596             If 'raise', raises a ValueError error if
2597             any of the index labels passed are not
2598             in the dataset. If 'ignore', any given labels that are in the
2599             dataset are dropped and no error is raised.
2600         **labels_kwargs : {dim: label, ...}, optional
2601             The keyword arguments form of ``dim`` and ``labels``
2602 
2603         Returns
2604         -------
2605         dropped : DataArray
2606         """
2607         if labels_kwargs or isinstance(labels, dict):
2608             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2609 
2610         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2611         return self._from_temp_dataset(ds)
2612 
2613     def drop_isel(
2614         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
2615     ) -> T_DataArray:
2616         """Drop index positions from this DataArray.
2617 
2618         Parameters
2619         ----------
2620         indexers : mapping of Hashable to Any or None, default: None
2621             Index locations to drop
2622         **indexers_kwargs : {dim: position, ...}, optional
2623             The keyword arguments form of ``dim`` and ``positions``
2624 
2625         Returns
2626         -------
2627         dropped : DataArray
2628 
2629         Raises
2630         ------
2631         IndexError
2632         """
2633         dataset = self._to_temp_dataset()
2634         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2635         return self._from_temp_dataset(dataset)
2636 
2637     def dropna(
2638         self: T_DataArray,
2639         dim: Hashable,
2640         how: Literal["any", "all"] = "any",
2641         thresh: int | None = None,
2642     ) -> T_DataArray:
2643         """Returns a new array with dropped labels for missing values along
2644         the provided dimension.
2645 
2646         Parameters
2647         ----------
2648         dim : Hashable
2649             Dimension along which to drop missing values. Dropping along
2650             multiple dimensions simultaneously is not yet supported.
2651         how : {"any", "all"}, default: "any"
2652             - any : if any NA values are present, drop that label
2653             - all : if all values are NA, drop that label
2654 
2655         thresh : int or None, default: None
2656             If supplied, require this many non-NA values.
2657 
2658         Returns
2659         -------
2660         dropped : DataArray
2661         """
2662         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2663         return self._from_temp_dataset(ds)
2664 
2665     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
2666         """Fill missing values in this object.
2667 
2668         This operation follows the normal broadcasting and alignment rules that
2669         xarray uses for binary arithmetic, except the result is aligned to this
2670         object (``join='left'``) instead of aligned to the intersection of
2671         index coordinates (``join='inner'``).
2672 
2673         Parameters
2674         ----------
2675         value : scalar, ndarray or DataArray
2676             Used to fill all matching missing values in this array. If the
2677             argument is a DataArray, it is first aligned with (reindexed to)
2678             this array.
2679 
2680         Returns
2681         -------
2682         filled : DataArray
2683         """
2684         if utils.is_dict_like(value):
2685             raise TypeError(
2686                 "cannot provide fill value as a dictionary with "
2687                 "fillna on a DataArray"
2688             )
2689         out = ops.fillna(self, value)
2690         return out
2691 
2692     def interpolate_na(
2693         self: T_DataArray,
2694         dim: Hashable | None = None,
2695         method: InterpOptions = "linear",
2696         limit: int | None = None,
2697         use_coordinate: bool | str = True,
2698         max_gap: (
2699             None
2700             | int
2701             | float
2702             | str
2703             | pd.Timedelta
2704             | np.timedelta64
2705             | datetime.timedelta
2706         ) = None,
2707         keep_attrs: bool | None = None,
2708         **kwargs: Any,
2709     ) -> T_DataArray:
2710         """Fill in NaNs by interpolating according to different methods.
2711 
2712         Parameters
2713         ----------
2714         dim : Hashable or None, optional
2715             Specifies the dimension along which to interpolate.
2716         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
2717             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
2718             String indicating which method to use for interpolation:
2719 
2720             - 'linear': linear interpolation. Additional keyword
2721               arguments are passed to :py:func:`numpy.interp`
2722             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2723               are passed to :py:func:`scipy.interpolate.interp1d`. If
2724               ``method='polynomial'``, the ``order`` keyword argument must also be
2725               provided.
2726             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2727               respective :py:class:`scipy.interpolate` classes.
2728 
2729         use_coordinate : bool or str, default: True
2730             Specifies which index to use as the x values in the interpolation
2731             formulated as `y = f(x)`. If False, values are treated as if
2732             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2733             used. If ``use_coordinate`` is a string, it specifies the name of a
2734             coordinate variariable to use as the index.
2735         limit : int or None, default: None
2736             Maximum number of consecutive NaNs to fill. Must be greater than 0
2737             or None for no limit. This filling is done regardless of the size of
2738             the gap in the data. To only interpolate over gaps less than a given length,
2739             see ``max_gap``.
2740         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2741             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2742             Use None for no limit. When interpolating along a datetime64 dimension
2743             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2744 
2745             - a string that is valid input for pandas.to_timedelta
2746             - a :py:class:`numpy.timedelta64` object
2747             - a :py:class:`pandas.Timedelta` object
2748             - a :py:class:`datetime.timedelta` object
2749 
2750             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2751             dimensions has not been implemented yet. Gap length is defined as the difference
2752             between coordinate values at the first data point after a gap and the last value
2753             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2754             between coordinate values at the first (last) valid data point and the first (last) NaN.
2755             For example, consider::
2756 
2757                 <xarray.DataArray (x: 9)>
2758                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2759                 Coordinates:
2760                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2761 
2762             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2763         keep_attrs : bool or None, default: None
2764             If True, the dataarray's attributes (`attrs`) will be copied from
2765             the original object to the new one.  If False, the new
2766             object will be returned without attributes.
2767         **kwargs : dict, optional
2768             parameters passed verbatim to the underlying interpolation function
2769 
2770         Returns
2771         -------
2772         interpolated: DataArray
2773             Filled in DataArray.
2774 
2775         See Also
2776         --------
2777         numpy.interp
2778         scipy.interpolate
2779 
2780         Examples
2781         --------
2782         >>> da = xr.DataArray(
2783         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2784         ... )
2785         >>> da
2786         <xarray.DataArray (x: 5)>
2787         array([nan,  2.,  3., nan,  0.])
2788         Coordinates:
2789           * x        (x) int64 0 1 2 3 4
2790 
2791         >>> da.interpolate_na(dim="x", method="linear")
2792         <xarray.DataArray (x: 5)>
2793         array([nan, 2. , 3. , 1.5, 0. ])
2794         Coordinates:
2795           * x        (x) int64 0 1 2 3 4
2796 
2797         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2798         <xarray.DataArray (x: 5)>
2799         array([1. , 2. , 3. , 1.5, 0. ])
2800         Coordinates:
2801           * x        (x) int64 0 1 2 3 4
2802         """
2803         from .missing import interp_na
2804 
2805         return interp_na(
2806             self,
2807             dim=dim,
2808             method=method,
2809             limit=limit,
2810             use_coordinate=use_coordinate,
2811             max_gap=max_gap,
2812             keep_attrs=keep_attrs,
2813             **kwargs,
2814         )
2815 
2816     def ffill(
2817         self: T_DataArray, dim: Hashable, limit: int | None = None
2818     ) -> T_DataArray:
2819         """Fill NaN values by propagating values forward
2820 
2821         *Requires bottleneck.*
2822 
2823         Parameters
2824         ----------
2825         dim : Hashable
2826             Specifies the dimension along which to propagate values when
2827             filling.
2828         limit : int or None, default: None
2829             The maximum number of consecutive NaN values to forward fill. In
2830             other words, if there is a gap with more than this number of
2831             consecutive NaNs, it will only be partially filled. Must be greater
2832             than 0 or None for no limit. Must be None or greater than or equal
2833             to axis length if filling along chunked axes (dimensions).
2834 
2835         Returns
2836         -------
2837         filled : DataArray
2838         """
2839         from .missing import ffill
2840 
2841         return ffill(self, dim, limit=limit)
2842 
2843     def bfill(
2844         self: T_DataArray, dim: Hashable, limit: int | None = None
2845     ) -> T_DataArray:
2846         """Fill NaN values by propagating values backward
2847 
2848         *Requires bottleneck.*
2849 
2850         Parameters
2851         ----------
2852         dim : str
2853             Specifies the dimension along which to propagate values when
2854             filling.
2855         limit : int or None, default: None
2856             The maximum number of consecutive NaN values to backward fill. In
2857             other words, if there is a gap with more than this number of
2858             consecutive NaNs, it will only be partially filled. Must be greater
2859             than 0 or None for no limit. Must be None or greater than or equal
2860             to axis length if filling along chunked axes (dimensions).
2861 
2862         Returns
2863         -------
2864         filled : DataArray
2865         """
2866         from .missing import bfill
2867 
2868         return bfill(self, dim, limit=limit)
2869 
2870     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
2871         """Combine two DataArray objects, with union of coordinates.
2872 
2873         This operation follows the normal broadcasting and alignment rules of
2874         ``join='outer'``.  Default to non-null values of array calling the
2875         method.  Use np.nan to fill in vacant cells after alignment.
2876 
2877         Parameters
2878         ----------
2879         other : DataArray
2880             Used to fill all matching missing values in this array.
2881 
2882         Returns
2883         -------
2884         DataArray
2885         """
2886         return ops.fillna(self, other, join="outer")
2887 
2888     def reduce(
2889         self: T_DataArray,
2890         func: Callable[..., Any],
2891         dim: None | Hashable | Iterable[Hashable] = None,
2892         *,
2893         axis: None | int | Sequence[int] = None,
2894         keep_attrs: bool | None = None,
2895         keepdims: bool = False,
2896         **kwargs: Any,
2897     ) -> T_DataArray:
2898         """Reduce this array by applying `func` along some dimension(s).
2899 
2900         Parameters
2901         ----------
2902         func : callable
2903             Function which can be called in the form
2904             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2905             np.ndarray over an integer valued axis.
2906         dim : Hashable or Iterable of Hashable, optional
2907             Dimension(s) over which to apply `func`.
2908         axis : int or sequence of int, optional
2909             Axis(es) over which to repeatedly apply `func`. Only one of the
2910             'dim' and 'axis' arguments can be supplied. If neither are
2911             supplied, then the reduction is calculated over the flattened array
2912             (by calling `f(x)` without an axis argument).
2913         keep_attrs : bool or None, optional
2914             If True, the variable's attributes (`attrs`) will be copied from
2915             the original object to the new one.  If False (default), the new
2916             object will be returned without attributes.
2917         keepdims : bool, default: False
2918             If True, the dimensions which are reduced are left in the result
2919             as dimensions of size one. Coordinates that use these dimensions
2920             are removed.
2921         **kwargs : dict
2922             Additional keyword arguments passed on to `func`.
2923 
2924         Returns
2925         -------
2926         reduced : DataArray
2927             DataArray with this object's array replaced with an array with
2928             summarized data and the indicated dimension(s) removed.
2929         """
2930 
2931         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2932         return self._replace_maybe_drop_dims(var)
2933 
2934     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
2935         """Convert this array into a pandas object with the same shape.
2936 
2937         The type of the returned object depends on the number of DataArray
2938         dimensions:
2939 
2940         * 0D -> `xarray.DataArray`
2941         * 1D -> `pandas.Series`
2942         * 2D -> `pandas.DataFrame`
2943 
2944         Only works for arrays with 2 or fewer dimensions.
2945 
2946         The DataArray constructor performs the inverse transformation.
2947 
2948         Returns
2949         -------
2950         result : DataArray | Series | DataFrame
2951             DataArray, pandas Series or pandas DataFrame.
2952         """
2953         # TODO: consolidate the info about pandas constructors and the
2954         # attributes that correspond to their indexes into a separate module?
2955         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2956         try:
2957             constructor = constructors[self.ndim]
2958         except KeyError:
2959             raise ValueError(
2960                 f"Cannot convert arrays with {self.ndim} dimensions into "
2961                 "pandas objects. Requires 2 or fewer dimensions."
2962             )
2963         indexes = [self.get_index(dim) for dim in self.dims]
2964         return constructor(self.values, *indexes)
2965 
2966     def to_dataframe(
2967         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
2968     ) -> pd.DataFrame:
2969         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2970 
2971         The DataFrame is indexed by the Cartesian product of index coordinates
2972         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
2973         included as columns in the DataFrame.
2974 
2975         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
2976         doesn't rely on a MultiIndex to build the DataFrame.
2977 
2978         Parameters
2979         ----------
2980         name: Hashable or None, optional
2981             Name to give to this array (required if unnamed).
2982         dim_order: Sequence of Hashable or None, optional
2983             Hierarchical dimension order for the resulting dataframe.
2984             Array content is transposed to this order and then written out as flat
2985             vectors in contiguous order, so the last dimension in this list
2986             will be contiguous in the resulting DataFrame. This has a major
2987             influence on which operations are efficient on the resulting
2988             dataframe.
2989 
2990             If provided, must include all dimensions of this DataArray. By default,
2991             dimensions are sorted according to the DataArray dimensions order.
2992 
2993         Returns
2994         -------
2995         result: DataFrame
2996             DataArray as a pandas DataFrame.
2997 
2998         See also
2999         --------
3000         DataArray.to_pandas
3001         DataArray.to_series
3002         """
3003         if name is None:
3004             name = self.name
3005         if name is None:
3006             raise ValueError(
3007                 "cannot convert an unnamed DataArray to a "
3008                 "DataFrame: use the ``name`` parameter"
3009             )
3010         if self.ndim == 0:
3011             raise ValueError("cannot convert a scalar to a DataFrame")
3012 
3013         # By using a unique name, we can convert a DataArray into a DataFrame
3014         # even if it shares a name with one of its coordinates.
3015         # I would normally use unique_name = object() but that results in a
3016         # dataframe with columns in the wrong order, for reasons I have not
3017         # been able to debug (possibly a pandas bug?).
3018         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3019         ds = self._to_dataset_whole(name=unique_name)
3020 
3021         if dim_order is None:
3022             ordered_dims = dict(zip(self.dims, self.shape))
3023         else:
3024             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3025 
3026         df = ds._to_dataframe(ordered_dims)
3027         df.columns = [name if c == unique_name else c for c in df.columns]
3028         return df
3029 
3030     def to_series(self) -> pd.Series:
3031         """Convert this array into a pandas.Series.
3032 
3033         The Series is indexed by the Cartesian product of index coordinates
3034         (in the form of a :py:class:`pandas.MultiIndex`).
3035 
3036         Returns
3037         -------
3038         result : Series
3039             DataArray as a pandas Series.
3040 
3041         See also
3042         --------
3043         DataArray.to_pandas
3044         DataArray.to_dataframe
3045         """
3046         index = self.coords.to_index()
3047         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3048 
3049     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3050         """Convert this array into a numpy.ma.MaskedArray
3051 
3052         Parameters
3053         ----------
3054         copy : bool, default: True
3055             If True make a copy of the array in the result. If False,
3056             a MaskedArray view of DataArray.values is returned.
3057 
3058         Returns
3059         -------
3060         result : MaskedArray
3061             Masked where invalid values (nan or inf) occur.
3062         """
3063         values = self.to_numpy()  # only compute lazy arrays once
3064         isnull = pd.isnull(values)
3065         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3066 
3067     # path=None writes to bytes
3068     @overload
3069     def to_netcdf(
3070         self,
3071         path: None = None,
3072         mode: Literal["w", "a"] = "w",
3073         format: T_NetcdfTypes | None = None,
3074         group: str | None = None,
3075         engine: T_NetcdfEngine | None = None,
3076         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3077         unlimited_dims: Iterable[Hashable] | None = None,
3078         compute: bool = True,
3079         invalid_netcdf: bool = False,
3080     ) -> bytes:
3081         ...
3082 
3083     # default return None
3084     @overload
3085     def to_netcdf(
3086         self,
3087         path: str | PathLike,
3088         mode: Literal["w", "a"] = "w",
3089         format: T_NetcdfTypes | None = None,
3090         group: str | None = None,
3091         engine: T_NetcdfEngine | None = None,
3092         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3093         unlimited_dims: Iterable[Hashable] | None = None,
3094         compute: Literal[True] = True,
3095         invalid_netcdf: bool = False,
3096     ) -> None:
3097         ...
3098 
3099     # compute=False returns dask.Delayed
3100     @overload
3101     def to_netcdf(
3102         self,
3103         path: str | PathLike,
3104         mode: Literal["w", "a"] = "w",
3105         format: T_NetcdfTypes | None = None,
3106         group: str | None = None,
3107         engine: T_NetcdfEngine | None = None,
3108         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3109         unlimited_dims: Iterable[Hashable] | None = None,
3110         *,
3111         compute: Literal[False],
3112         invalid_netcdf: bool = False,
3113     ) -> Delayed:
3114         ...
3115 
3116     def to_netcdf(
3117         self,
3118         path: str | PathLike | None = None,
3119         mode: Literal["w", "a"] = "w",
3120         format: T_NetcdfTypes | None = None,
3121         group: str | None = None,
3122         engine: T_NetcdfEngine | None = None,
3123         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3124         unlimited_dims: Iterable[Hashable] | None = None,
3125         compute: bool = True,
3126         invalid_netcdf: bool = False,
3127     ) -> bytes | Delayed | None:
3128         """Write dataset contents to a netCDF file.
3129 
3130         Parameters
3131         ----------
3132         path : str, path-like or None, optional
3133             Path to which to save this dataset. File-like objects are only
3134             supported by the scipy engine. If no path is provided, this
3135             function returns the resulting netCDF file as bytes; in this case,
3136             we need to use scipy, which does not support netCDF version 4 (the
3137             default format becomes NETCDF3_64BIT).
3138         mode : {"w", "a"}, default: "w"
3139             Write ('w') or append ('a') mode. If mode='w', any existing file at
3140             this location will be overwritten. If mode='a', existing variables
3141             will be overwritten.
3142         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3143                   "NETCDF3_CLASSIC"}, optional
3144             File format for the resulting netCDF file:
3145 
3146             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3147               features.
3148             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3149               netCDF 3 compatible API features.
3150             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3151               which fully supports 2+ GB files, but is only compatible with
3152               clients linked against netCDF version 3.6.0 or later.
3153             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3154               handle 2+ GB files very well.
3155 
3156             All formats are supported by the netCDF4-python library.
3157             scipy.io.netcdf only supports the last two formats.
3158 
3159             The default format is NETCDF4 if you are saving a file to disk and
3160             have the netCDF4-python library available. Otherwise, xarray falls
3161             back to using scipy to write netCDF files and defaults to the
3162             NETCDF3_64BIT format (scipy does not support netCDF4).
3163         group : str, optional
3164             Path to the netCDF4 group in the given file to open (only works for
3165             format='NETCDF4'). The group(s) will be created if necessary.
3166         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3167             Engine to use when writing netCDF files. If not provided, the
3168             default engine is chosen based on available dependencies, with a
3169             preference for 'netcdf4' if writing to a file on disk.
3170         encoding : dict, optional
3171             Nested dictionary with variable names as keys and dictionaries of
3172             variable specific encodings as values, e.g.,
3173             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3174             "zlib": True}, ...}``
3175 
3176             The `h5netcdf` engine supports both the NetCDF4-style compression
3177             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3178             ones ``{"compression": "gzip", "compression_opts": 9}``.
3179             This allows using any compression plugin installed in the HDF5
3180             library, e.g. LZF.
3181 
3182         unlimited_dims : iterable of Hashable, optional
3183             Dimension(s) that should be serialized as unlimited dimensions.
3184             By default, no dimensions are treated as unlimited dimensions.
3185             Note that unlimited_dims may also be set via
3186             ``dataset.encoding["unlimited_dims"]``.
3187         compute: bool, default: True
3188             If true compute immediately, otherwise return a
3189             ``dask.delayed.Delayed`` object that can be computed later.
3190         invalid_netcdf: bool, default: False
3191             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3192             hdf5 files which are invalid netcdf as described in
3193             https://github.com/h5netcdf/h5netcdf.
3194 
3195         Returns
3196         -------
3197         store: bytes or Delayed or None
3198             * ``bytes`` if path is None
3199             * ``dask.delayed.Delayed`` if compute is False
3200             * None otherwise
3201 
3202         Notes
3203         -----
3204         Only xarray.Dataset objects can be written to netCDF files, so
3205         the xarray.DataArray is converted to a xarray.Dataset object
3206         containing a single variable. If the DataArray has no name, or if the
3207         name is the same as a coordinate name, then it is given the name
3208         ``"__xarray_dataarray_variable__"``.
3209 
3210         See Also
3211         --------
3212         Dataset.to_netcdf
3213         """
3214         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3215 
3216         if self.name is None:
3217             # If no name is set then use a generic xarray name
3218             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3219         elif self.name in self.coords or self.name in self.dims:
3220             # The name is the same as one of the coords names, which netCDF
3221             # doesn't support, so rename it but keep track of the old name
3222             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3223             dataset.attrs[DATAARRAY_NAME] = self.name
3224         else:
3225             # No problems with the name - so we're fine!
3226             dataset = self.to_dataset()
3227 
3228         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3229             dataset,
3230             path,
3231             mode=mode,
3232             format=format,
3233             group=group,
3234             engine=engine,
3235             encoding=encoding,
3236             unlimited_dims=unlimited_dims,
3237             compute=compute,
3238             multifile=False,
3239             invalid_netcdf=invalid_netcdf,
3240         )
3241 
3242     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
3243         """
3244         Convert this xarray.DataArray into a dictionary following xarray
3245         naming conventions.
3246 
3247         Converts all variables and attributes to native Python objects.
3248         Useful for converting to json. To avoid datetime incompatibility
3249         use decode_times=False kwarg in xarray.open_dataset.
3250 
3251         Parameters
3252         ----------
3253         data : bool, default: True
3254             Whether to include the actual data in the dictionary. When set to
3255             False, returns just the schema.
3256         encoding : bool, default: False
3257             Whether to include the Dataset's encoding in the dictionary.
3258 
3259         Returns
3260         -------
3261         dict: dict
3262 
3263         See Also
3264         --------
3265         DataArray.from_dict
3266         Dataset.to_dict
3267         """
3268         d = self.variable.to_dict(data=data)
3269         d.update({"coords": {}, "name": self.name})
3270         for k in self.coords:
3271             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
3272         if encoding:
3273             d["encoding"] = dict(self.encoding)
3274         return d
3275 
3276     @classmethod
3277     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
3278         """Convert a dictionary into an xarray.DataArray
3279 
3280         Parameters
3281         ----------
3282         d : dict
3283             Mapping with a minimum structure of {"dims": [...], "data": [...]}
3284 
3285         Returns
3286         -------
3287         obj : xarray.DataArray
3288 
3289         See Also
3290         --------
3291         DataArray.to_dict
3292         Dataset.from_dict
3293 
3294         Examples
3295         --------
3296         >>> d = {"dims": "t", "data": [1, 2, 3]}
3297         >>> da = xr.DataArray.from_dict(d)
3298         >>> da
3299         <xarray.DataArray (t: 3)>
3300         array([1, 2, 3])
3301         Dimensions without coordinates: t
3302 
3303         >>> d = {
3304         ...     "coords": {
3305         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
3306         ...     },
3307         ...     "attrs": {"title": "air temperature"},
3308         ...     "dims": "t",
3309         ...     "data": [10, 20, 30],
3310         ...     "name": "a",
3311         ... }
3312         >>> da = xr.DataArray.from_dict(d)
3313         >>> da
3314         <xarray.DataArray 'a' (t: 3)>
3315         array([10, 20, 30])
3316         Coordinates:
3317           * t        (t) int64 0 1 2
3318         Attributes:
3319             title:    air temperature
3320         """
3321         coords = None
3322         if "coords" in d:
3323             try:
3324                 coords = {
3325                     k: (v["dims"], v["data"], v.get("attrs"))
3326                     for k, v in d["coords"].items()
3327                 }
3328             except KeyError as e:
3329                 raise ValueError(
3330                     "cannot convert dict when coords are missing the key "
3331                     "'{dims_data}'".format(dims_data=str(e.args[0]))
3332                 )
3333         try:
3334             data = d["data"]
3335         except KeyError:
3336             raise ValueError("cannot convert dict without the key 'data''")
3337         else:
3338             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
3339 
3340         obj.encoding.update(d.get("encoding", {}))
3341 
3342         return obj
3343 
3344     @classmethod
3345     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
3346         """Convert a pandas.Series into an xarray.DataArray.
3347 
3348         If the series's index is a MultiIndex, it will be expanded into a
3349         tensor product of one-dimensional coordinates (filling in missing
3350         values with NaN). Thus this operation should be the inverse of the
3351         `to_series` method.
3352 
3353         Parameters
3354         ----------
3355         series : Series
3356             Pandas Series object to convert.
3357         sparse : bool, default: False
3358             If sparse=True, creates a sparse array instead of a dense NumPy array.
3359             Requires the pydata/sparse package.
3360 
3361         See Also
3362         --------
3363         DataArray.to_series
3364         Dataset.from_dataframe
3365         """
3366         temp_name = "__temporary_name"
3367         df = pd.DataFrame({temp_name: series})
3368         ds = Dataset.from_dataframe(df, sparse=sparse)
3369         result = cast(DataArray, ds[temp_name])
3370         result.name = series.name
3371         return result
3372 
3373     def to_cdms2(self) -> cdms2_Variable:
3374         """Convert this array into a cdms2.Variable"""
3375         from ..convert import to_cdms2
3376 
3377         return to_cdms2(self)
3378 
3379     @classmethod
3380     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
3381         """Convert a cdms2.Variable into an xarray.DataArray"""
3382         from ..convert import from_cdms2
3383 
3384         return from_cdms2(variable)
3385 
3386     def to_iris(self) -> iris_Cube:
3387         """Convert this array into a iris.cube.Cube"""
3388         from ..convert import to_iris
3389 
3390         return to_iris(self)
3391 
3392     @classmethod
3393     def from_iris(cls, cube: iris_Cube) -> DataArray:
3394         """Convert a iris.cube.Cube into an xarray.DataArray"""
3395         from ..convert import from_iris
3396 
3397         return from_iris(cube)
3398 
3399     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
3400         """Helper function for equals, broadcast_equals, and identical"""
3401 
3402         def compat(x, y):
3403             return getattr(x.variable, compat_str)(y.variable)
3404 
3405         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
3406             self, other
3407         )
3408 
3409     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
3410         """Two DataArrays are broadcast equal if they are equal after
3411         broadcasting them against each other such that they have the same
3412         dimensions.
3413 
3414         Parameters
3415         ----------
3416         other : DataArray
3417             DataArray to compare to.
3418 
3419         Returns
3420         ----------
3421         equal : bool
3422             True if the two DataArrays are broadcast equal.
3423 
3424         See Also
3425         --------
3426         DataArray.equals
3427         DataArray.identical
3428         """
3429         try:
3430             return self._all_compat(other, "broadcast_equals")
3431         except (TypeError, AttributeError):
3432             return False
3433 
3434     def equals(self: T_DataArray, other: T_DataArray) -> bool:
3435         """True if two DataArrays have the same dimensions, coordinates and
3436         values; otherwise False.
3437 
3438         DataArrays can still be equal (like pandas objects) if they have NaN
3439         values in the same locations.
3440 
3441         This method is necessary because `v1 == v2` for ``DataArray``
3442         does element-wise comparisons (like numpy.ndarrays).
3443 
3444         Parameters
3445         ----------
3446         other : DataArray
3447             DataArray to compare to.
3448 
3449         Returns
3450         ----------
3451         equal : bool
3452             True if the two DataArrays are equal.
3453 
3454         See Also
3455         --------
3456         DataArray.broadcast_equals
3457         DataArray.identical
3458         """
3459         try:
3460             return self._all_compat(other, "equals")
3461         except (TypeError, AttributeError):
3462             return False
3463 
3464     def identical(self: T_DataArray, other: T_DataArray) -> bool:
3465         """Like equals, but also checks the array name and attributes, and
3466         attributes on all coordinates.
3467 
3468         Parameters
3469         ----------
3470         other : DataArray
3471             DataArray to compare to.
3472 
3473         Returns
3474         ----------
3475         equal : bool
3476             True if the two DataArrays are identical.
3477 
3478         See Also
3479         --------
3480         DataArray.broadcast_equals
3481         DataArray.equals
3482         """
3483         try:
3484             return self.name == other.name and self._all_compat(other, "identical")
3485         except (TypeError, AttributeError):
3486             return False
3487 
3488     def _result_name(self, other: Any = None) -> Hashable | None:
3489         # use the same naming heuristics as pandas:
3490         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
3491         other_name = getattr(other, "name", _default)
3492         if other_name is _default or other_name == self.name:
3493             return self.name
3494         else:
3495             return None
3496 
3497     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
3498         new_var = self.variable.__array_wrap__(obj, context)
3499         return self._replace(new_var)
3500 
3501     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
3502         return self.dot(obj)
3503 
3504     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3505         # currently somewhat duplicative, as only other DataArrays are
3506         # compatible with matmul
3507         return computation.dot(other, self)
3508 
3509     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
3510         keep_attrs = kwargs.pop("keep_attrs", None)
3511         if keep_attrs is None:
3512             keep_attrs = _get_keep_attrs(default=True)
3513         with warnings.catch_warnings():
3514             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
3515             warnings.filterwarnings(
3516                 "ignore", r"Mean of empty slice", category=RuntimeWarning
3517             )
3518             with np.errstate(all="ignore"):
3519                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
3520             if keep_attrs:
3521                 da.attrs = self.attrs
3522             return da
3523 
3524     def _binary_op(
3525         self: T_DataArray,
3526         other: Any,
3527         f: Callable,
3528         reflexive: bool = False,
3529     ) -> T_DataArray:
3530         from .groupby import GroupBy
3531 
3532         if isinstance(other, (Dataset, GroupBy)):
3533             return NotImplemented
3534         if isinstance(other, DataArray):
3535             align_type = OPTIONS["arithmetic_join"]
3536             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
3537         other_variable = getattr(other, "variable", other)
3538         other_coords = getattr(other, "coords", None)
3539 
3540         variable = (
3541             f(self.variable, other_variable)
3542             if not reflexive
3543             else f(other_variable, self.variable)
3544         )
3545         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
3546         name = self._result_name(other)
3547 
3548         return self._replace(variable, coords, name, indexes=indexes)
3549 
3550     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
3551         from .groupby import GroupBy
3552 
3553         if isinstance(other, GroupBy):
3554             raise TypeError(
3555                 "in-place operations between a DataArray and "
3556                 "a grouped object are not permitted"
3557             )
3558         # n.b. we can't align other to self (with other.reindex_like(self))
3559         # because `other` may be converted into floats, which would cause
3560         # in-place arithmetic to fail unpredictably. Instead, we simply
3561         # don't support automatic alignment with in-place arithmetic.
3562         other_coords = getattr(other, "coords", None)
3563         other_variable = getattr(other, "variable", other)
3564         try:
3565             with self.coords._merge_inplace(other_coords):
3566                 f(self.variable, other_variable)
3567         except MergeError as exc:
3568             raise MergeError(
3569                 "Automatic alignment is not supported for in-place operations.\n"
3570                 "Consider aligning the indices manually or using a not-in-place operation.\n"
3571                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3572             ) from exc
3573         return self
3574 
3575     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
3576         self.attrs = other.attrs
3577 
3578     plot = utils.UncachedAccessor(_PlotMethods)
3579 
3580     def _title_for_slice(self, truncate: int = 50) -> str:
3581         """
3582         If the dataarray has 1 dimensional coordinates or comes from a slice
3583         we can show that info in the title
3584 
3585         Parameters
3586         ----------
3587         truncate : int, default: 50
3588             maximum number of characters for title
3589 
3590         Returns
3591         -------
3592         title : string
3593             Can be used for plot titles
3594 
3595         """
3596         one_dims = []
3597         for dim, coord in self.coords.items():
3598             if coord.size == 1:
3599                 one_dims.append(
3600                     "{dim} = {v}{unit}".format(
3601                         dim=dim,
3602                         v=format_item(coord.values),
3603                         unit=_get_units_from_attrs(coord),
3604                     )
3605                 )
3606 
3607         title = ", ".join(one_dims)
3608         if len(title) > truncate:
3609             title = title[: (truncate - 3)] + "..."
3610 
3611         return title
3612 
3613     def diff(
3614         self: T_DataArray,
3615         dim: Hashable,
3616         n: int = 1,
3617         label: Literal["upper", "lower"] = "upper",
3618     ) -> T_DataArray:
3619         """Calculate the n-th order discrete difference along given axis.
3620 
3621         Parameters
3622         ----------
3623         dim : Hashable
3624             Dimension over which to calculate the finite difference.
3625         n : int, default: 1
3626             The number of times values are differenced.
3627         label : {"upper", "lower"}, default: "upper"
3628             The new coordinate in dimension ``dim`` will have the
3629             values of either the minuend's or subtrahend's coordinate
3630             for values 'upper' and 'lower', respectively.
3631 
3632         Returns
3633         -------
3634         difference : DataArray
3635             The n-th order finite difference of this object.
3636 
3637         Notes
3638         -----
3639         `n` matches numpy's behavior and is different from pandas' first argument named
3640         `periods`.
3641 
3642         Examples
3643         --------
3644         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3645         >>> arr.diff("x")
3646         <xarray.DataArray (x: 3)>
3647         array([0, 1, 0])
3648         Coordinates:
3649           * x        (x) int64 2 3 4
3650         >>> arr.diff("x", 2)
3651         <xarray.DataArray (x: 2)>
3652         array([ 1, -1])
3653         Coordinates:
3654           * x        (x) int64 3 4
3655 
3656         See Also
3657         --------
3658         DataArray.differentiate
3659         """
3660         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3661         return self._from_temp_dataset(ds)
3662 
3663     def shift(
3664         self: T_DataArray,
3665         shifts: Mapping[Any, int] | None = None,
3666         fill_value: Any = dtypes.NA,
3667         **shifts_kwargs: int,
3668     ) -> T_DataArray:
3669         """Shift this DataArray by an offset along one or more dimensions.
3670 
3671         Only the data is moved; coordinates stay in place. This is consistent
3672         with the behavior of ``shift`` in pandas.
3673 
3674         Values shifted from beyond array bounds will appear at one end of
3675         each dimension, which are filled according to `fill_value`. For periodic
3676         offsets instead see `roll`.
3677 
3678         Parameters
3679         ----------
3680         shifts : mapping of Hashable to int or None, optional
3681             Integer offset to shift along each of the given dimensions.
3682             Positive offsets shift to the right; negative offsets shift to the
3683             left.
3684         fill_value : scalar, optional
3685             Value to use for newly missing values
3686         **shifts_kwargs
3687             The keyword arguments form of ``shifts``.
3688             One of shifts or shifts_kwargs must be provided.
3689 
3690         Returns
3691         -------
3692         shifted : DataArray
3693             DataArray with the same coordinates and attributes but shifted
3694             data.
3695 
3696         See Also
3697         --------
3698         roll
3699 
3700         Examples
3701         --------
3702         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3703         >>> arr.shift(x=1)
3704         <xarray.DataArray (x: 3)>
3705         array([nan,  5.,  6.])
3706         Dimensions without coordinates: x
3707         """
3708         variable = self.variable.shift(
3709             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3710         )
3711         return self._replace(variable=variable)
3712 
3713     def roll(
3714         self: T_DataArray,
3715         shifts: Mapping[Hashable, int] | None = None,
3716         roll_coords: bool = False,
3717         **shifts_kwargs: int,
3718     ) -> T_DataArray:
3719         """Roll this array by an offset along one or more dimensions.
3720 
3721         Unlike shift, roll treats the given dimensions as periodic, so will not
3722         create any missing values to be filled.
3723 
3724         Unlike shift, roll may rotate all variables, including coordinates
3725         if specified. The direction of rotation is consistent with
3726         :py:func:`numpy.roll`.
3727 
3728         Parameters
3729         ----------
3730         shifts : mapping of Hashable to int, optional
3731             Integer offset to rotate each of the given dimensions.
3732             Positive offsets roll to the right; negative offsets roll to the
3733             left.
3734         roll_coords : bool, default: False
3735             Indicates whether to roll the coordinates by the offset too.
3736         **shifts_kwargs : {dim: offset, ...}, optional
3737             The keyword arguments form of ``shifts``.
3738             One of shifts or shifts_kwargs must be provided.
3739 
3740         Returns
3741         -------
3742         rolled : DataArray
3743             DataArray with the same attributes but rolled data and coordinates.
3744 
3745         See Also
3746         --------
3747         shift
3748 
3749         Examples
3750         --------
3751         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3752         >>> arr.roll(x=1)
3753         <xarray.DataArray (x: 3)>
3754         array([7, 5, 6])
3755         Dimensions without coordinates: x
3756         """
3757         ds = self._to_temp_dataset().roll(
3758             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3759         )
3760         return self._from_temp_dataset(ds)
3761 
3762     @property
3763     def real(self: T_DataArray) -> T_DataArray:
3764         return self._replace(self.variable.real)
3765 
3766     @property
3767     def imag(self: T_DataArray) -> T_DataArray:
3768         return self._replace(self.variable.imag)
3769 
3770     def dot(
3771         self: T_DataArray,
3772         other: T_DataArray,
3773         dims: str | Iterable[Hashable] | Ellipsis | None = None,
3774     ) -> T_DataArray:
3775         """Perform dot product of two DataArrays along their shared dims.
3776 
3777         Equivalent to taking taking tensordot over all shared dims.
3778 
3779         Parameters
3780         ----------
3781         other : DataArray
3782             The other array with which the dot product is performed.
3783         dims : ..., str or Iterable of Hashable, optional
3784             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3785             If not specified, then all the common dimensions are summed over.
3786 
3787         Returns
3788         -------
3789         result : DataArray
3790             Array resulting from the dot product over all shared dimensions.
3791 
3792         See Also
3793         --------
3794         dot
3795         numpy.tensordot
3796 
3797         Examples
3798         --------
3799         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3800         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3801         >>> dm_vals = np.arange(4)
3802         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3803 
3804         >>> dm.dims
3805         ('z',)
3806 
3807         >>> da.dims
3808         ('x', 'y', 'z')
3809 
3810         >>> dot_result = da.dot(dm)
3811         >>> dot_result.dims
3812         ('x', 'y')
3813 
3814         """
3815         if isinstance(other, Dataset):
3816             raise NotImplementedError(
3817                 "dot products are not yet supported with Dataset objects."
3818             )
3819         if not isinstance(other, DataArray):
3820             raise TypeError("dot only operates on DataArrays.")
3821 
3822         return computation.dot(self, other, dims=dims)
3823 
3824     # change type of self and return to T_DataArray once
3825     # https://github.com/python/mypy/issues/12846 is resolved
3826     def sortby(
3827         self,
3828         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
3829         ascending: bool = True,
3830     ) -> DataArray:
3831         """Sort object by labels or values (along an axis).
3832 
3833         Sorts the dataarray, either along specified dimensions,
3834         or according to values of 1-D dataarrays that share dimension
3835         with calling object.
3836 
3837         If the input variables are dataarrays, then the dataarrays are aligned
3838         (via left-join) to the calling object prior to sorting by cell values.
3839         NaNs are sorted to the end, following Numpy convention.
3840 
3841         If multiple sorts along the same dimension is
3842         given, numpy's lexsort is performed along that dimension:
3843         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
3844         and the FIRST key in the sequence is used as the primary sort key,
3845         followed by the 2nd key, etc.
3846 
3847         Parameters
3848         ----------
3849         variables : Hashable, DataArray, or sequence of Hashable or DataArray
3850             1D DataArray objects or name(s) of 1D variable(s) in
3851             coords whose values are used to sort this array.
3852         ascending : bool, default: True
3853             Whether to sort by ascending or descending order.
3854 
3855         Returns
3856         -------
3857         sorted : DataArray
3858             A new dataarray where all the specified dims are sorted by dim
3859             labels.
3860 
3861         See Also
3862         --------
3863         Dataset.sortby
3864         numpy.sort
3865         pandas.sort_values
3866         pandas.sort_index
3867 
3868         Examples
3869         --------
3870         >>> da = xr.DataArray(
3871         ...     np.random.rand(5),
3872         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3873         ...     dims="time",
3874         ... )
3875         >>> da
3876         <xarray.DataArray (time: 5)>
3877         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3878         Coordinates:
3879           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3880 
3881         >>> da.sortby(da)
3882         <xarray.DataArray (time: 5)>
3883         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3884         Coordinates:
3885           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3886         """
3887         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3888         return self._from_temp_dataset(ds)
3889 
3890     def quantile(
3891         self: T_DataArray,
3892         q: ArrayLike,
3893         dim: str | Iterable[Hashable] | None = None,
3894         method: QUANTILE_METHODS = "linear",
3895         keep_attrs: bool | None = None,
3896         skipna: bool | None = None,
3897         interpolation: QUANTILE_METHODS = None,
3898     ) -> T_DataArray:
3899         """Compute the qth quantile of the data along the specified dimension.
3900 
3901         Returns the qth quantiles(s) of the array elements.
3902 
3903         Parameters
3904         ----------
3905         q : float or array-like of float
3906             Quantile to compute, which must be between 0 and 1 inclusive.
3907         dim : str or Iterable of Hashable, optional
3908             Dimension(s) over which to apply quantile.
3909         method : str, default: "linear"
3910             This optional parameter specifies the interpolation method to use when the
3911             desired quantile lies between two data points. The options sorted by their R
3912             type as summarized in the H&F paper [1]_ are:
3913 
3914                 1. "inverted_cdf" (*)
3915                 2. "averaged_inverted_cdf" (*)
3916                 3. "closest_observation" (*)
3917                 4. "interpolated_inverted_cdf" (*)
3918                 5. "hazen" (*)
3919                 6. "weibull" (*)
3920                 7. "linear"  (default)
3921                 8. "median_unbiased" (*)
3922                 9. "normal_unbiased" (*)
3923 
3924             The first three methods are discontiuous. The following discontinuous
3925             variations of the default "linear" (7.) option are also available:
3926 
3927                 * "lower"
3928                 * "higher"
3929                 * "midpoint"
3930                 * "nearest"
3931 
3932             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
3933             was previously called "interpolation", renamed in accordance with numpy
3934             version 1.22.0.
3935 
3936             (*) These methods require numpy version 1.22 or newer.
3937 
3938         keep_attrs : bool or None, optional
3939             If True, the dataset's attributes (`attrs`) will be copied from
3940             the original object to the new one.  If False (default), the new
3941             object will be returned without attributes.
3942         skipna : bool or None, optional
3943             If True, skip missing values (as marked by NaN). By default, only
3944             skips missing values for float dtypes; other dtypes either do not
3945             have a sentinel missing value (int) or skipna=True has not been
3946             implemented (object, datetime64 or timedelta64).
3947 
3948         Returns
3949         -------
3950         quantiles : DataArray
3951             If `q` is a single quantile, then the result
3952             is a scalar. If multiple percentiles are given, first axis of
3953             the result corresponds to the quantile and a quantile dimension
3954             is added to the return array. The other dimensions are the
3955             dimensions that remain after the reduction of the array.
3956 
3957         See Also
3958         --------
3959         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3960 
3961         Examples
3962         --------
3963         >>> da = xr.DataArray(
3964         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3965         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3966         ...     dims=("x", "y"),
3967         ... )
3968         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3969         <xarray.DataArray ()>
3970         array(0.7)
3971         Coordinates:
3972             quantile  float64 0.0
3973         >>> da.quantile(0, dim="x")
3974         <xarray.DataArray (y: 4)>
3975         array([0.7, 4.2, 2.6, 1.5])
3976         Coordinates:
3977           * y         (y) float64 1.0 1.5 2.0 2.5
3978             quantile  float64 0.0
3979         >>> da.quantile([0, 0.5, 1])
3980         <xarray.DataArray (quantile: 3)>
3981         array([0.7, 3.4, 9.4])
3982         Coordinates:
3983           * quantile  (quantile) float64 0.0 0.5 1.0
3984         >>> da.quantile([0, 0.5, 1], dim="x")
3985         <xarray.DataArray (quantile: 3, y: 4)>
3986         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3987                [3.6 , 5.75, 6.  , 1.7 ],
3988                [6.5 , 7.3 , 9.4 , 1.9 ]])
3989         Coordinates:
3990           * y         (y) float64 1.0 1.5 2.0 2.5
3991           * quantile  (quantile) float64 0.0 0.5 1.0
3992 
3993         References
3994         ----------
3995         .. [1] R. J. Hyndman and Y. Fan,
3996            "Sample quantiles in statistical packages,"
3997            The American Statistician, 50(4), pp. 361-365, 1996
3998         """
3999 
4000         ds = self._to_temp_dataset().quantile(
4001             q,
4002             dim=dim,
4003             keep_attrs=keep_attrs,
4004             method=method,
4005             skipna=skipna,
4006             interpolation=interpolation,
4007         )
4008         return self._from_temp_dataset(ds)
4009 
4010     def rank(
4011         self: T_DataArray,
4012         dim: Hashable,
4013         pct: bool = False,
4014         keep_attrs: bool | None = None,
4015     ) -> T_DataArray:
4016         """Ranks the data.
4017 
4018         Equal values are assigned a rank that is the average of the ranks that
4019         would have been otherwise assigned to all of the values within that
4020         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
4021 
4022         NaNs in the input array are returned as NaNs.
4023 
4024         The `bottleneck` library is required.
4025 
4026         Parameters
4027         ----------
4028         dim : Hashable
4029             Dimension over which to compute rank.
4030         pct : bool, default: False
4031             If True, compute percentage ranks, otherwise compute integer ranks.
4032         keep_attrs : bool or None, optional
4033             If True, the dataset's attributes (`attrs`) will be copied from
4034             the original object to the new one.  If False (default), the new
4035             object will be returned without attributes.
4036 
4037         Returns
4038         -------
4039         ranked : DataArray
4040             DataArray with the same coordinates and dtype 'float64'.
4041 
4042         Examples
4043         --------
4044         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4045         >>> arr.rank("x")
4046         <xarray.DataArray (x: 3)>
4047         array([1., 2., 3.])
4048         Dimensions without coordinates: x
4049         """
4050 
4051         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
4052         return self._from_temp_dataset(ds)
4053 
4054     def differentiate(
4055         self: T_DataArray,
4056         coord: Hashable,
4057         edge_order: Literal[1, 2] = 1,
4058         datetime_unit: DatetimeUnitOptions = None,
4059     ) -> T_DataArray:
4060         """ Differentiate the array with the second order accurate central
4061         differences.
4062 
4063         .. note::
4064             This feature is limited to simple cartesian geometry, i.e. coord
4065             must be one dimensional.
4066 
4067         Parameters
4068         ----------
4069         coord : Hashable
4070             The coordinate to be used to compute the gradient.
4071         edge_order : {1, 2}, default: 1
4072             N-th order accurate differences at the boundaries.
4073         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
4074                          "us", "ns", "ps", "fs", "as", None}, optional
4075             Unit to compute gradient. Only valid for datetime coordinate.
4076 
4077         Returns
4078         -------
4079         differentiated: DataArray
4080 
4081         See also
4082         --------
4083         numpy.gradient: corresponding numpy function
4084 
4085         Examples
4086         --------
4087 
4088         >>> da = xr.DataArray(
4089         ...     np.arange(12).reshape(4, 3),
4090         ...     dims=["x", "y"],
4091         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4092         ... )
4093         >>> da
4094         <xarray.DataArray (x: 4, y: 3)>
4095         array([[ 0,  1,  2],
4096                [ 3,  4,  5],
4097                [ 6,  7,  8],
4098                [ 9, 10, 11]])
4099         Coordinates:
4100           * x        (x) float64 0.0 0.1 1.1 1.2
4101         Dimensions without coordinates: y
4102         >>>
4103         >>> da.differentiate("x")
4104         <xarray.DataArray (x: 4, y: 3)>
4105         array([[30.        , 30.        , 30.        ],
4106                [27.54545455, 27.54545455, 27.54545455],
4107                [27.54545455, 27.54545455, 27.54545455],
4108                [30.        , 30.        , 30.        ]])
4109         Coordinates:
4110           * x        (x) float64 0.0 0.1 1.1 1.2
4111         Dimensions without coordinates: y
4112         """
4113         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
4114         return self._from_temp_dataset(ds)
4115 
4116     # change type of self and return to T_DataArray once
4117     # https://github.com/python/mypy/issues/12846 is resolved
4118     def integrate(
4119         self,
4120         coord: Hashable | Sequence[Hashable] = None,
4121         datetime_unit: DatetimeUnitOptions = None,
4122     ) -> DataArray:
4123         """Integrate along the given coordinate using the trapezoidal rule.
4124 
4125         .. note::
4126             This feature is limited to simple cartesian geometry, i.e. coord
4127             must be one dimensional.
4128 
4129         Parameters
4130         ----------
4131         coord : Hashable, or sequence of Hashable
4132             Coordinate(s) used for the integration.
4133         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4134                         'ps', 'fs', 'as', None}, optional
4135             Specify the unit if a datetime coordinate is used.
4136 
4137         Returns
4138         -------
4139         integrated : DataArray
4140 
4141         See also
4142         --------
4143         Dataset.integrate
4144         numpy.trapz : corresponding numpy function
4145 
4146         Examples
4147         --------
4148 
4149         >>> da = xr.DataArray(
4150         ...     np.arange(12).reshape(4, 3),
4151         ...     dims=["x", "y"],
4152         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4153         ... )
4154         >>> da
4155         <xarray.DataArray (x: 4, y: 3)>
4156         array([[ 0,  1,  2],
4157                [ 3,  4,  5],
4158                [ 6,  7,  8],
4159                [ 9, 10, 11]])
4160         Coordinates:
4161           * x        (x) float64 0.0 0.1 1.1 1.2
4162         Dimensions without coordinates: y
4163         >>>
4164         >>> da.integrate("x")
4165         <xarray.DataArray (y: 3)>
4166         array([5.4, 6.6, 7.8])
4167         Dimensions without coordinates: y
4168         """
4169         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
4170         return self._from_temp_dataset(ds)
4171 
4172     # change type of self and return to T_DataArray once
4173     # https://github.com/python/mypy/issues/12846 is resolved
4174     def cumulative_integrate(
4175         self,
4176         coord: Hashable | Sequence[Hashable] = None,
4177         datetime_unit: DatetimeUnitOptions = None,
4178     ) -> DataArray:
4179         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
4180 
4181         .. note::
4182             This feature is limited to simple cartesian geometry, i.e. coord
4183             must be one dimensional.
4184 
4185             The first entry of the cumulative integral is always 0, in order to keep the
4186             length of the dimension unchanged between input and output.
4187 
4188         Parameters
4189         ----------
4190         coord : Hashable, or sequence of Hashable
4191             Coordinate(s) used for the integration.
4192         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4193                         'ps', 'fs', 'as', None}, optional
4194             Specify the unit if a datetime coordinate is used.
4195 
4196         Returns
4197         -------
4198         integrated : DataArray
4199 
4200         See also
4201         --------
4202         Dataset.cumulative_integrate
4203         scipy.integrate.cumulative_trapezoid : corresponding scipy function
4204 
4205         Examples
4206         --------
4207 
4208         >>> da = xr.DataArray(
4209         ...     np.arange(12).reshape(4, 3),
4210         ...     dims=["x", "y"],
4211         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4212         ... )
4213         >>> da
4214         <xarray.DataArray (x: 4, y: 3)>
4215         array([[ 0,  1,  2],
4216                [ 3,  4,  5],
4217                [ 6,  7,  8],
4218                [ 9, 10, 11]])
4219         Coordinates:
4220           * x        (x) float64 0.0 0.1 1.1 1.2
4221         Dimensions without coordinates: y
4222         >>>
4223         >>> da.cumulative_integrate("x")
4224         <xarray.DataArray (x: 4, y: 3)>
4225         array([[0.  , 0.  , 0.  ],
4226                [0.15, 0.25, 0.35],
4227                [4.65, 5.75, 6.85],
4228                [5.4 , 6.6 , 7.8 ]])
4229         Coordinates:
4230           * x        (x) float64 0.0 0.1 1.1 1.2
4231         Dimensions without coordinates: y
4232         """
4233         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
4234         return self._from_temp_dataset(ds)
4235 
4236     def unify_chunks(self) -> DataArray:
4237         """Unify chunk size along all chunked dimensions of this DataArray.
4238 
4239         Returns
4240         -------
4241         DataArray with consistent chunk sizes for all dask-array variables
4242 
4243         See Also
4244         --------
4245         dask.array.core.unify_chunks
4246         """
4247 
4248         return unify_chunks(self)[0]
4249 
4250     def map_blocks(
4251         self,
4252         func: Callable[..., T_Xarray],
4253         args: Sequence[Any] = (),
4254         kwargs: Mapping[str, Any] | None = None,
4255         template: DataArray | Dataset | None = None,
4256     ) -> T_Xarray:
4257         """
4258         Apply a function to each block of this DataArray.
4259 
4260         .. warning::
4261             This method is experimental and its signature may change.
4262 
4263         Parameters
4264         ----------
4265         func : callable
4266             User-provided function that accepts a DataArray as its first
4267             parameter. The function will receive a subset or 'block' of this DataArray (see below),
4268             corresponding to one chunk along each chunked dimension. ``func`` will be
4269             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
4270 
4271             This function must return either a single DataArray or a single Dataset.
4272 
4273             This function cannot add a new chunked dimension.
4274         args : sequence
4275             Passed to func after unpacking and subsetting any xarray objects by blocks.
4276             xarray objects in args must be aligned with this object, otherwise an error is raised.
4277         kwargs : mapping
4278             Passed verbatim to func after unpacking. xarray objects, if any, will not be
4279             subset to blocks. Passing dask collections in kwargs is not allowed.
4280         template : DataArray or Dataset, optional
4281             xarray object representing the final result after compute is called. If not provided,
4282             the function will be first run on mocked-up data, that looks like this object but
4283             has sizes 0, to determine properties of the returned object such as dtype,
4284             variable names, attributes, new dimensions and new indexes (if any).
4285             ``template`` must be provided if the function changes the size of existing dimensions.
4286             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
4287             ``attrs`` set by ``func`` will be ignored.
4288 
4289         Returns
4290         -------
4291         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
4292         function.
4293 
4294         Notes
4295         -----
4296         This function is designed for when ``func`` needs to manipulate a whole xarray object
4297         subset to each block. Each block is loaded into memory. In the more common case where
4298         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
4299 
4300         If none of the variables in this object is backed by dask arrays, calling this function is
4301         equivalent to calling ``func(obj, *args, **kwargs)``.
4302 
4303         See Also
4304         --------
4305         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
4306         xarray.DataArray.map_blocks
4307 
4308         Examples
4309         --------
4310         Calculate an anomaly from climatology using ``.groupby()``. Using
4311         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
4312         its indices, and its methods like ``.groupby()``.
4313 
4314         >>> def calculate_anomaly(da, groupby_type="time.month"):
4315         ...     gb = da.groupby(groupby_type)
4316         ...     clim = gb.mean(dim="time")
4317         ...     return gb - clim
4318         ...
4319         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
4320         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
4321         >>> np.random.seed(123)
4322         >>> array = xr.DataArray(
4323         ...     np.random.rand(len(time)),
4324         ...     dims=["time"],
4325         ...     coords={"time": time, "month": month},
4326         ... ).chunk()
4327         >>> array.map_blocks(calculate_anomaly, template=array).compute()
4328         <xarray.DataArray (time: 24)>
4329         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
4330                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
4331                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
4332                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
4333                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
4334         Coordinates:
4335           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4336             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
4337 
4338         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
4339         to the function being applied in ``xr.map_blocks()``:
4340 
4341         >>> array.map_blocks(
4342         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
4343         ... )  # doctest: +ELLIPSIS
4344         <xarray.DataArray (time: 24)>
4345         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
4346         Coordinates:
4347           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4348             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
4349         """
4350         from .parallel import map_blocks
4351 
4352         return map_blocks(func, self, args, kwargs, template)
4353 
4354     def polyfit(
4355         self,
4356         dim: Hashable,
4357         deg: int,
4358         skipna: bool | None = None,
4359         rcond: float | None = None,
4360         w: Hashable | Any | None = None,
4361         full: bool = False,
4362         cov: bool | Literal["unscaled"] = False,
4363     ) -> Dataset:
4364         """
4365         Least squares polynomial fit.
4366 
4367         This replicates the behaviour of `numpy.polyfit` but differs by skipping
4368         invalid values when `skipna = True`.
4369 
4370         Parameters
4371         ----------
4372         dim : Hashable
4373             Coordinate along which to fit the polynomials.
4374         deg : int
4375             Degree of the fitting polynomial.
4376         skipna : bool or None, optional
4377             If True, removes all invalid values before fitting each 1D slices of the array.
4378             Default is True if data is stored in a dask.array or if there is any
4379             invalid values, False otherwise.
4380         rcond : float or None, optional
4381             Relative condition number to the fit.
4382         w : Hashable, array-like or None, optional
4383             Weights to apply to the y-coordinate of the sample points.
4384             Can be an array-like object or the name of a coordinate in the dataset.
4385         full : bool, default: False
4386             Whether to return the residuals, matrix rank and singular values in addition
4387             to the coefficients.
4388         cov : bool or "unscaled", default: False
4389             Whether to return to the covariance matrix in addition to the coefficients.
4390             The matrix is not scaled if `cov='unscaled'`.
4391 
4392         Returns
4393         -------
4394         polyfit_results : Dataset
4395             A single dataset which contains:
4396 
4397             polyfit_coefficients
4398                 The coefficients of the best fit.
4399             polyfit_residuals
4400                 The residuals of the least-square computation (only included if `full=True`).
4401                 When the matrix rank is deficient, np.nan is returned.
4402             [dim]_matrix_rank
4403                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4404             [dim]_singular_value
4405                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4406             polyfit_covariance
4407                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
4408 
4409         See Also
4410         --------
4411         numpy.polyfit
4412         numpy.polyval
4413         xarray.polyval
4414         """
4415         return self._to_temp_dataset().polyfit(
4416             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
4417         )
4418 
4419     def pad(
4420         self: T_DataArray,
4421         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
4422         mode: PadModeOptions = "constant",
4423         stat_length: int
4424         | tuple[int, int]
4425         | Mapping[Any, tuple[int, int]]
4426         | None = None,
4427         constant_values: float
4428         | tuple[float, float]
4429         | Mapping[Any, tuple[float, float]]
4430         | None = None,
4431         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
4432         reflect_type: PadReflectOptions = None,
4433         **pad_width_kwargs: Any,
4434     ) -> T_DataArray:
4435         """Pad this array along one or more dimensions.
4436 
4437         .. warning::
4438             This function is experimental and its behaviour is likely to change
4439             especially regarding padding of dimension coordinates (or IndexVariables).
4440 
4441         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
4442         coordinates will be padded with the same mode, otherwise coordinates
4443         are padded using the "constant" mode with fill_value dtypes.NA.
4444 
4445         Parameters
4446         ----------
4447         pad_width : mapping of Hashable to tuple of int
4448             Mapping with the form of {dim: (pad_before, pad_after)}
4449             describing the number of values padded along each dimension.
4450             {dim: pad} is a shortcut for pad_before = pad_after = pad
4451         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
4452             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
4453             How to pad the DataArray (taken from numpy docs):
4454 
4455             - "constant": Pads with a constant value.
4456             - "edge": Pads with the edge values of array.
4457             - "linear_ramp": Pads with the linear ramp between end_value and the
4458               array edge value.
4459             - "maximum": Pads with the maximum value of all or part of the
4460               vector along each axis.
4461             - "mean": Pads with the mean value of all or part of the
4462               vector along each axis.
4463             - "median": Pads with the median value of all or part of the
4464               vector along each axis.
4465             - "minimum": Pads with the minimum value of all or part of the
4466               vector along each axis.
4467             - "reflect": Pads with the reflection of the vector mirrored on
4468               the first and last values of the vector along each axis.
4469             - "symmetric": Pads with the reflection of the vector mirrored
4470               along the edge of the array.
4471             - "wrap": Pads with the wrap of the vector along the axis.
4472               The first values are used to pad the end and the
4473               end values are used to pad the beginning.
4474 
4475         stat_length : int, tuple or mapping of Hashable to tuple, default: None
4476             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
4477             values at edge of each axis used to calculate the statistic value.
4478             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
4479             statistic lengths along each dimension.
4480             ((before, after),) yields same before and after statistic lengths
4481             for each dimension.
4482             (stat_length,) or int is a shortcut for before = after = statistic
4483             length for all axes.
4484             Default is ``None``, to use the entire axis.
4485         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4486             Used in 'constant'.  The values to set the padded values for each
4487             axis.
4488             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4489             pad constants along each dimension.
4490             ``((before, after),)`` yields same before and after constants for each
4491             dimension.
4492             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4493             all dimensions.
4494             Default is 0.
4495         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4496             Used in 'linear_ramp'.  The values used for the ending value of the
4497             linear_ramp and that will form the edge of the padded array.
4498             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4499             end values along each dimension.
4500             ``((before, after),)`` yields same before and after end values for each
4501             axis.
4502             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4503             all axes.
4504             Default is 0.
4505         reflect_type : {"even", "odd", None}, optional
4506             Used in "reflect", and "symmetric". The "even" style is the
4507             default with an unaltered reflection around the edge value. For
4508             the "odd" style, the extended part of the array is created by
4509             subtracting the reflected values from two times the edge value.
4510         **pad_width_kwargs
4511             The keyword arguments form of ``pad_width``.
4512             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
4513 
4514         Returns
4515         -------
4516         padded : DataArray
4517             DataArray with the padded coordinates and data.
4518 
4519         See Also
4520         --------
4521         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
4522 
4523         Notes
4524         -----
4525         For ``mode="constant"`` and ``constant_values=None``, integer types will be
4526         promoted to ``float`` and padded with ``np.nan``.
4527 
4528         Padding coordinates will drop their corresponding index (if any) and will reset default
4529         indexes for dimension coordinates.
4530 
4531         Examples
4532         --------
4533         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
4534         >>> arr.pad(x=(1, 2), constant_values=0)
4535         <xarray.DataArray (x: 6)>
4536         array([0, 5, 6, 7, 0, 0])
4537         Coordinates:
4538           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
4539 
4540         >>> da = xr.DataArray(
4541         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
4542         ...     dims=["x", "y"],
4543         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
4544         ... )
4545         >>> da.pad(x=1)
4546         <xarray.DataArray (x: 4, y: 4)>
4547         array([[nan, nan, nan, nan],
4548                [ 0.,  1.,  2.,  3.],
4549                [10., 11., 12., 13.],
4550                [nan, nan, nan, nan]])
4551         Coordinates:
4552           * x        (x) float64 nan 0.0 1.0 nan
4553           * y        (y) int64 10 20 30 40
4554             z        (x) float64 nan 100.0 200.0 nan
4555 
4556         Careful, ``constant_values`` are coerced to the data type of the array which may
4557         lead to a loss of precision:
4558 
4559         >>> da.pad(x=1, constant_values=1.23456789)
4560         <xarray.DataArray (x: 4, y: 4)>
4561         array([[ 1,  1,  1,  1],
4562                [ 0,  1,  2,  3],
4563                [10, 11, 12, 13],
4564                [ 1,  1,  1,  1]])
4565         Coordinates:
4566           * x        (x) float64 nan 0.0 1.0 nan
4567           * y        (y) int64 10 20 30 40
4568             z        (x) float64 nan 100.0 200.0 nan
4569         """
4570         ds = self._to_temp_dataset().pad(
4571             pad_width=pad_width,
4572             mode=mode,
4573             stat_length=stat_length,
4574             constant_values=constant_values,
4575             end_values=end_values,
4576             reflect_type=reflect_type,
4577             **pad_width_kwargs,
4578         )
4579         return self._from_temp_dataset(ds)
4580 
4581     def idxmin(
4582         self,
4583         dim: Hashable | None = None,
4584         skipna: bool | None = None,
4585         fill_value: Any = dtypes.NA,
4586         keep_attrs: bool | None = None,
4587     ) -> DataArray:
4588         """Return the coordinate label of the minimum value along a dimension.
4589 
4590         Returns a new `DataArray` named after the dimension with the values of
4591         the coordinate labels along that dimension corresponding to minimum
4592         values along that dimension.
4593 
4594         In comparison to :py:meth:`~DataArray.argmin`, this returns the
4595         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
4596 
4597         Parameters
4598         ----------
4599         dim : str, optional
4600             Dimension over which to apply `idxmin`.  This is optional for 1D
4601             arrays, but required for arrays with 2 or more dimensions.
4602         skipna : bool or None, default: None
4603             If True, skip missing values (as marked by NaN). By default, only
4604             skips missing values for ``float``, ``complex``, and ``object``
4605             dtypes; other dtypes either do not have a sentinel missing value
4606             (``int``) or ``skipna=True`` has not been implemented
4607             (``datetime64`` or ``timedelta64``).
4608         fill_value : Any, default: NaN
4609             Value to be filled in case all of the values along a dimension are
4610             null.  By default this is NaN.  The fill value and result are
4611             automatically converted to a compatible dtype if possible.
4612             Ignored if ``skipna`` is False.
4613         keep_attrs : bool or None, optional
4614             If True, the attributes (``attrs``) will be copied from the
4615             original object to the new one. If False, the new object
4616             will be returned without attributes.
4617 
4618         Returns
4619         -------
4620         reduced : DataArray
4621             New `DataArray` object with `idxmin` applied to its data and the
4622             indicated dimension removed.
4623 
4624         See Also
4625         --------
4626         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
4627 
4628         Examples
4629         --------
4630         >>> array = xr.DataArray(
4631         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4632         ... )
4633         >>> array.min()
4634         <xarray.DataArray ()>
4635         array(-2)
4636         >>> array.argmin()
4637         <xarray.DataArray ()>
4638         array(4)
4639         >>> array.idxmin()
4640         <xarray.DataArray 'x' ()>
4641         array('e', dtype='<U1')
4642 
4643         >>> array = xr.DataArray(
4644         ...     [
4645         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4646         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4647         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4648         ...     ],
4649         ...     dims=["y", "x"],
4650         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4651         ... )
4652         >>> array.min(dim="x")
4653         <xarray.DataArray (y: 3)>
4654         array([-2., -4.,  1.])
4655         Coordinates:
4656           * y        (y) int64 -1 0 1
4657         >>> array.argmin(dim="x")
4658         <xarray.DataArray (y: 3)>
4659         array([4, 0, 2])
4660         Coordinates:
4661           * y        (y) int64 -1 0 1
4662         >>> array.idxmin(dim="x")
4663         <xarray.DataArray 'x' (y: 3)>
4664         array([16.,  0.,  4.])
4665         Coordinates:
4666           * y        (y) int64 -1 0 1
4667         """
4668         return computation._calc_idxminmax(
4669             array=self,
4670             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4671             dim=dim,
4672             skipna=skipna,
4673             fill_value=fill_value,
4674             keep_attrs=keep_attrs,
4675         )
4676 
4677     def idxmax(
4678         self,
4679         dim: Hashable = None,
4680         skipna: bool | None = None,
4681         fill_value: Any = dtypes.NA,
4682         keep_attrs: bool | None = None,
4683     ) -> DataArray:
4684         """Return the coordinate label of the maximum value along a dimension.
4685 
4686         Returns a new `DataArray` named after the dimension with the values of
4687         the coordinate labels along that dimension corresponding to maximum
4688         values along that dimension.
4689 
4690         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4691         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4692 
4693         Parameters
4694         ----------
4695         dim : Hashable, optional
4696             Dimension over which to apply `idxmax`.  This is optional for 1D
4697             arrays, but required for arrays with 2 or more dimensions.
4698         skipna : bool or None, default: None
4699             If True, skip missing values (as marked by NaN). By default, only
4700             skips missing values for ``float``, ``complex``, and ``object``
4701             dtypes; other dtypes either do not have a sentinel missing value
4702             (``int``) or ``skipna=True`` has not been implemented
4703             (``datetime64`` or ``timedelta64``).
4704         fill_value : Any, default: NaN
4705             Value to be filled in case all of the values along a dimension are
4706             null.  By default this is NaN.  The fill value and result are
4707             automatically converted to a compatible dtype if possible.
4708             Ignored if ``skipna`` is False.
4709         keep_attrs : bool or None, optional
4710             If True, the attributes (``attrs``) will be copied from the
4711             original object to the new one. If False, the new object
4712             will be returned without attributes.
4713 
4714         Returns
4715         -------
4716         reduced : DataArray
4717             New `DataArray` object with `idxmax` applied to its data and the
4718             indicated dimension removed.
4719 
4720         See Also
4721         --------
4722         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4723 
4724         Examples
4725         --------
4726         >>> array = xr.DataArray(
4727         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4728         ... )
4729         >>> array.max()
4730         <xarray.DataArray ()>
4731         array(2)
4732         >>> array.argmax()
4733         <xarray.DataArray ()>
4734         array(1)
4735         >>> array.idxmax()
4736         <xarray.DataArray 'x' ()>
4737         array('b', dtype='<U1')
4738 
4739         >>> array = xr.DataArray(
4740         ...     [
4741         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4742         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4743         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4744         ...     ],
4745         ...     dims=["y", "x"],
4746         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4747         ... )
4748         >>> array.max(dim="x")
4749         <xarray.DataArray (y: 3)>
4750         array([2., 2., 1.])
4751         Coordinates:
4752           * y        (y) int64 -1 0 1
4753         >>> array.argmax(dim="x")
4754         <xarray.DataArray (y: 3)>
4755         array([0, 2, 2])
4756         Coordinates:
4757           * y        (y) int64 -1 0 1
4758         >>> array.idxmax(dim="x")
4759         <xarray.DataArray 'x' (y: 3)>
4760         array([0., 4., 4.])
4761         Coordinates:
4762           * y        (y) int64 -1 0 1
4763         """
4764         return computation._calc_idxminmax(
4765             array=self,
4766             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4767             dim=dim,
4768             skipna=skipna,
4769             fill_value=fill_value,
4770             keep_attrs=keep_attrs,
4771         )
4772 
4773     # change type of self and return to T_DataArray once
4774     # https://github.com/python/mypy/issues/12846 is resolved
4775     def argmin(
4776         self,
4777         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4778         axis: int | None = None,
4779         keep_attrs: bool | None = None,
4780         skipna: bool | None = None,
4781     ) -> DataArray | dict[Hashable, DataArray]:
4782         """Index or indices of the minimum of the DataArray over one or more dimensions.
4783 
4784         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4785         which can be passed directly to isel(). If a single str is passed to 'dim' then
4786         returns a DataArray with dtype int.
4787 
4788         If there are multiple minima, the indices of the first one found will be
4789         returned.
4790 
4791         Parameters
4792         ----------
4793         dim : Hashable, sequence of Hashable, None or ..., optional
4794             The dimensions over which to find the minimum. By default, finds minimum over
4795             all dimensions - for now returning an int for backward compatibility, but
4796             this is deprecated, in future will return a dict with indices for all
4797             dimensions; to return a dict with all dimensions now, pass '...'.
4798         axis : int or None, optional
4799             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4800             can be supplied.
4801         keep_attrs : bool or None, optional
4802             If True, the attributes (`attrs`) will be copied from the original
4803             object to the new one. If False, the new object will be
4804             returned without attributes.
4805         skipna : bool or None, optional
4806             If True, skip missing values (as marked by NaN). By default, only
4807             skips missing values for float dtypes; other dtypes either do not
4808             have a sentinel missing value (int) or skipna=True has not been
4809             implemented (object, datetime64 or timedelta64).
4810 
4811         Returns
4812         -------
4813         result : DataArray or dict of DataArray
4814 
4815         See Also
4816         --------
4817         Variable.argmin, DataArray.idxmin
4818 
4819         Examples
4820         --------
4821         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4822         >>> array.min()
4823         <xarray.DataArray ()>
4824         array(-1)
4825         >>> array.argmin()
4826         <xarray.DataArray ()>
4827         array(2)
4828         >>> array.argmin(...)
4829         {'x': <xarray.DataArray ()>
4830         array(2)}
4831         >>> array.isel(array.argmin(...))
4832         <xarray.DataArray ()>
4833         array(-1)
4834 
4835         >>> array = xr.DataArray(
4836         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4837         ...     dims=("x", "y", "z"),
4838         ... )
4839         >>> array.min(dim="x")
4840         <xarray.DataArray (y: 3, z: 3)>
4841         array([[ 1,  2,  1],
4842                [ 2, -5,  1],
4843                [ 2,  1,  1]])
4844         Dimensions without coordinates: y, z
4845         >>> array.argmin(dim="x")
4846         <xarray.DataArray (y: 3, z: 3)>
4847         array([[1, 0, 0],
4848                [1, 1, 1],
4849                [0, 0, 1]])
4850         Dimensions without coordinates: y, z
4851         >>> array.argmin(dim=["x"])
4852         {'x': <xarray.DataArray (y: 3, z: 3)>
4853         array([[1, 0, 0],
4854                [1, 1, 1],
4855                [0, 0, 1]])
4856         Dimensions without coordinates: y, z}
4857         >>> array.min(dim=("x", "z"))
4858         <xarray.DataArray (y: 3)>
4859         array([ 1, -5,  1])
4860         Dimensions without coordinates: y
4861         >>> array.argmin(dim=["x", "z"])
4862         {'x': <xarray.DataArray (y: 3)>
4863         array([0, 1, 0])
4864         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4865         array([2, 1, 1])
4866         Dimensions without coordinates: y}
4867         >>> array.isel(array.argmin(dim=["x", "z"]))
4868         <xarray.DataArray (y: 3)>
4869         array([ 1, -5,  1])
4870         Dimensions without coordinates: y
4871         """
4872         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4873         if isinstance(result, dict):
4874             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4875         else:
4876             return self._replace_maybe_drop_dims(result)
4877 
4878     # change type of self and return to T_DataArray once
4879     # https://github.com/python/mypy/issues/12846 is resolved
4880     def argmax(
4881         self,
4882         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4883         axis: int | None = None,
4884         keep_attrs: bool | None = None,
4885         skipna: bool | None = None,
4886     ) -> DataArray | dict[Hashable, DataArray]:
4887         """Index or indices of the maximum of the DataArray over one or more dimensions.
4888 
4889         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4890         which can be passed directly to isel(). If a single str is passed to 'dim' then
4891         returns a DataArray with dtype int.
4892 
4893         If there are multiple maxima, the indices of the first one found will be
4894         returned.
4895 
4896         Parameters
4897         ----------
4898         dim : Hashable, sequence of Hashable, None or ..., optional
4899             The dimensions over which to find the maximum. By default, finds maximum over
4900             all dimensions - for now returning an int for backward compatibility, but
4901             this is deprecated, in future will return a dict with indices for all
4902             dimensions; to return a dict with all dimensions now, pass '...'.
4903         axis : int or None, optional
4904             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4905             can be supplied.
4906         keep_attrs : bool or None, optional
4907             If True, the attributes (`attrs`) will be copied from the original
4908             object to the new one. If False, the new object will be
4909             returned without attributes.
4910         skipna : bool or None, optional
4911             If True, skip missing values (as marked by NaN). By default, only
4912             skips missing values for float dtypes; other dtypes either do not
4913             have a sentinel missing value (int) or skipna=True has not been
4914             implemented (object, datetime64 or timedelta64).
4915 
4916         Returns
4917         -------
4918         result : DataArray or dict of DataArray
4919 
4920         See Also
4921         --------
4922         Variable.argmax, DataArray.idxmax
4923 
4924         Examples
4925         --------
4926         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4927         >>> array.max()
4928         <xarray.DataArray ()>
4929         array(3)
4930         >>> array.argmax()
4931         <xarray.DataArray ()>
4932         array(3)
4933         >>> array.argmax(...)
4934         {'x': <xarray.DataArray ()>
4935         array(3)}
4936         >>> array.isel(array.argmax(...))
4937         <xarray.DataArray ()>
4938         array(3)
4939 
4940         >>> array = xr.DataArray(
4941         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4942         ...     dims=("x", "y", "z"),
4943         ... )
4944         >>> array.max(dim="x")
4945         <xarray.DataArray (y: 3, z: 3)>
4946         array([[3, 3, 2],
4947                [3, 5, 2],
4948                [2, 3, 3]])
4949         Dimensions without coordinates: y, z
4950         >>> array.argmax(dim="x")
4951         <xarray.DataArray (y: 3, z: 3)>
4952         array([[0, 1, 1],
4953                [0, 1, 0],
4954                [0, 1, 0]])
4955         Dimensions without coordinates: y, z
4956         >>> array.argmax(dim=["x"])
4957         {'x': <xarray.DataArray (y: 3, z: 3)>
4958         array([[0, 1, 1],
4959                [0, 1, 0],
4960                [0, 1, 0]])
4961         Dimensions without coordinates: y, z}
4962         >>> array.max(dim=("x", "z"))
4963         <xarray.DataArray (y: 3)>
4964         array([3, 5, 3])
4965         Dimensions without coordinates: y
4966         >>> array.argmax(dim=["x", "z"])
4967         {'x': <xarray.DataArray (y: 3)>
4968         array([0, 1, 0])
4969         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4970         array([0, 1, 2])
4971         Dimensions without coordinates: y}
4972         >>> array.isel(array.argmax(dim=["x", "z"]))
4973         <xarray.DataArray (y: 3)>
4974         array([3, 5, 3])
4975         Dimensions without coordinates: y
4976         """
4977         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4978         if isinstance(result, dict):
4979             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4980         else:
4981             return self._replace_maybe_drop_dims(result)
4982 
4983     def query(
4984         self,
4985         queries: Mapping[Any, Any] | None = None,
4986         parser: QueryParserOptions = "pandas",
4987         engine: QueryEngineOptions = None,
4988         missing_dims: ErrorOptionsWithWarn = "raise",
4989         **queries_kwargs: Any,
4990     ) -> DataArray:
4991         """Return a new data array indexed along the specified
4992         dimension(s), where the indexers are given as strings containing
4993         Python expressions to be evaluated against the values in the array.
4994 
4995         Parameters
4996         ----------
4997         queries : dict-like or None, optional
4998             A dict-like with keys matching dimensions and values given by strings
4999             containing Python expressions to be evaluated against the data variables
5000             in the dataset. The expressions will be evaluated using the pandas
5001             eval() function, and can contain any valid Python expressions but cannot
5002             contain any Python statements.
5003         parser : {"pandas", "python"}, default: "pandas"
5004             The parser to use to construct the syntax tree from the expression.
5005             The default of 'pandas' parses code slightly different than standard
5006             Python. Alternatively, you can parse an expression using the 'python'
5007             parser to retain strict Python semantics.
5008         engine : {"python", "numexpr", None}, default: None
5009             The engine used to evaluate the expression. Supported engines are:
5010 
5011             - None: tries to use numexpr, falls back to python
5012             - "numexpr": evaluates expressions using numexpr
5013             - "python": performs operations as if you had eval’d in top level python
5014 
5015         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5016             What to do if dimensions that should be selected from are not present in the
5017             DataArray:
5018 
5019             - "raise": raise an exception
5020             - "warn": raise a warning, and ignore the missing dimensions
5021             - "ignore": ignore the missing dimensions
5022 
5023         **queries_kwargs : {dim: query, ...}, optional
5024             The keyword arguments form of ``queries``.
5025             One of queries or queries_kwargs must be provided.
5026 
5027         Returns
5028         -------
5029         obj : DataArray
5030             A new DataArray with the same contents as this dataset, indexed by
5031             the results of the appropriate queries.
5032 
5033         See Also
5034         --------
5035         DataArray.isel
5036         Dataset.query
5037         pandas.eval
5038 
5039         Examples
5040         --------
5041         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
5042         >>> da
5043         <xarray.DataArray 'a' (x: 5)>
5044         array([0, 1, 2, 3, 4])
5045         Dimensions without coordinates: x
5046         >>> da.query(x="a > 2")
5047         <xarray.DataArray 'a' (x: 2)>
5048         array([3, 4])
5049         Dimensions without coordinates: x
5050         """
5051 
5052         ds = self._to_dataset_whole(shallow_copy=True)
5053         ds = ds.query(
5054             queries=queries,
5055             parser=parser,
5056             engine=engine,
5057             missing_dims=missing_dims,
5058             **queries_kwargs,
5059         )
5060         return ds[self.name]
5061 
5062     def curvefit(
5063         self,
5064         coords: str | DataArray | Iterable[str | DataArray],
5065         func: Callable[..., Any],
5066         reduce_dims: Hashable | Iterable[Hashable] | None = None,
5067         skipna: bool = True,
5068         p0: dict[str, Any] | None = None,
5069         bounds: dict[str, Any] | None = None,
5070         param_names: Sequence[str] | None = None,
5071         kwargs: dict[str, Any] | None = None,
5072     ) -> Dataset:
5073         """
5074         Curve fitting optimization for arbitrary functions.
5075 
5076         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
5077 
5078         Parameters
5079         ----------
5080         coords : Hashable, DataArray, or sequence of DataArray or Hashable
5081             Independent coordinate(s) over which to perform the curve fitting. Must share
5082             at least one dimension with the calling object. When fitting multi-dimensional
5083             functions, supply `coords` as a sequence in the same order as arguments in
5084             `func`. To fit along existing dimensions of the calling object, `coords` can
5085             also be specified as a str or sequence of strs.
5086         func : callable
5087             User specified function in the form `f(x, *params)` which returns a numpy
5088             array of length `len(x)`. `params` are the fittable parameters which are optimized
5089             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
5090             coordinates, e.g. `f((x0, x1), *params)`.
5091         reduce_dims : Hashable or sequence of Hashable
5092             Additional dimension(s) over which to aggregate while fitting. For example,
5093             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
5094             aggregate all lat and lon points and fit the specified function along the
5095             time dimension.
5096         skipna : bool, default: True
5097             Whether to skip missing values when fitting. Default is True.
5098         p0 : dict-like or None, optional
5099             Optional dictionary of parameter names to initial guesses passed to the
5100             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
5101             be assigned initial values following the default scipy behavior.
5102         bounds : dict-like or None, optional
5103             Optional dictionary of parameter names to bounding values passed to the
5104             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
5105             will be unbounded following the default scipy behavior.
5106         param_names : sequence of Hashable or None, optional
5107             Sequence of names for the fittable parameters of `func`. If not supplied,
5108             this will be automatically determined by arguments of `func`. `param_names`
5109             should be manually supplied when fitting a function that takes a variable
5110             number of parameters.
5111         **kwargs : optional
5112             Additional keyword arguments to passed to scipy curve_fit.
5113 
5114         Returns
5115         -------
5116         curvefit_results : Dataset
5117             A single dataset which contains:
5118 
5119             [var]_curvefit_coefficients
5120                 The coefficients of the best fit.
5121             [var]_curvefit_covariance
5122                 The covariance matrix of the coefficient estimates.
5123 
5124         See Also
5125         --------
5126         DataArray.polyfit
5127         scipy.optimize.curve_fit
5128         """
5129         return self._to_temp_dataset().curvefit(
5130             coords,
5131             func,
5132             reduce_dims=reduce_dims,
5133             skipna=skipna,
5134             p0=p0,
5135             bounds=bounds,
5136             param_names=param_names,
5137             kwargs=kwargs,
5138         )
5139 
5140     def drop_duplicates(
5141         self: T_DataArray,
5142         dim: Hashable | Iterable[Hashable],
5143         keep: Literal["first", "last", False] = "first",
5144     ) -> T_DataArray:
5145         """Returns a new DataArray with duplicate dimension values removed.
5146 
5147         Parameters
5148         ----------
5149         dim : dimension label or labels
5150             Pass `...` to drop duplicates along all dimensions.
5151         keep : {"first", "last", False}, default: "first"
5152             Determines which duplicates (if any) to keep.
5153 
5154             - ``"first"`` : Drop duplicates except for the first occurrence.
5155             - ``"last"`` : Drop duplicates except for the last occurrence.
5156             - False : Drop all duplicates.
5157 
5158         Returns
5159         -------
5160         DataArray
5161 
5162         See Also
5163         --------
5164         Dataset.drop_duplicates
5165         """
5166         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
5167         return self._from_temp_dataset(deduplicated)
5168 
5169     def convert_calendar(
5170         self,
5171         calendar: str,
5172         dim: str = "time",
5173         align_on: str | None = None,
5174         missing: Any | None = None,
5175         use_cftime: bool | None = None,
5176     ) -> DataArray:
5177         """Convert the DataArray to another calendar.
5178 
5179         Only converts the individual timestamps, does not modify any data except
5180         in dropping invalid/surplus dates or inserting missing dates.
5181 
5182         If the source and target calendars are either no_leap, all_leap or a
5183         standard type, only the type of the time array is modified.
5184         When converting to a leap year from a non-leap year, the 29th of February
5185         is removed from the array. In the other direction the 29th of February
5186         will be missing in the output, unless `missing` is specified,
5187         in which case that value is inserted.
5188 
5189         For conversions involving `360_day` calendars, see Notes.
5190 
5191         This method is safe to use with sub-daily data as it doesn't touch the
5192         time part of the timestamps.
5193 
5194         Parameters
5195         ---------
5196         calendar : str
5197             The target calendar name.
5198         dim : str
5199             Name of the time coordinate.
5200         align_on : {None, 'date', 'year'}
5201             Must be specified when either source or target is a `360_day` calendar,
5202            ignored otherwise. See Notes.
5203         missing : Optional[any]
5204             By default, i.e. if the value is None, this method will simply attempt
5205             to convert the dates in the source calendar to the same dates in the
5206             target calendar, and drop any of those that are not possible to
5207             represent.  If a value is provided, a new time coordinate will be
5208             created in the target calendar with the same frequency as the original
5209             time coordinate; for any dates that are not present in the source, the
5210             data will be filled with this value.  Note that using this mode requires
5211             that the source data have an inferable frequency; for more information
5212             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
5213             target calendar combinations, this could result in many missing values, see notes.
5214         use_cftime : boolean, optional
5215             Whether to use cftime objects in the output, only used if `calendar`
5216             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
5217             If True, the new time axis uses cftime objects.
5218             If None (default), it uses :py:class:`numpy.datetime64` values if the
5219             date range permits it, and :py:class:`cftime.datetime` objects if not.
5220             If False, it uses :py:class:`numpy.datetime64`  or fails.
5221 
5222         Returns
5223         -------
5224         DataArray
5225             Copy of the dataarray with the time coordinate converted to the
5226             target calendar. If 'missing' was None (default), invalid dates in
5227             the new calendar are dropped, but missing dates are not inserted.
5228             If `missing` was given, the new data is reindexed to have a time axis
5229             with the same frequency as the source, but in the new calendar; any
5230             missing datapoints are filled with `missing`.
5231 
5232         Notes
5233         -----
5234         Passing a value to `missing` is only usable if the source's time coordinate as an
5235         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
5236         if the target coordinate, generated from this frequency, has dates equivalent to the
5237         source. It is usually **not** appropriate to use this mode with:
5238 
5239         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
5240         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
5241             or 'mH' where 24 % m != 0).
5242 
5243         If one of the source or target calendars is `"360_day"`, `align_on` must
5244         be specified and two options are offered.
5245 
5246         - "year"
5247             The dates are translated according to their relative position in the year,
5248             ignoring their original month and day information, meaning that the
5249             missing/surplus days are added/removed at regular intervals.
5250 
5251             From a `360_day` to a standard calendar, the output will be missing the
5252             following dates (day of year in parentheses):
5253 
5254             To a leap year:
5255                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
5256                 September 31st (275) and November 30th (335).
5257             To a non-leap year:
5258                 February 6th (36), April 19th (109), July 2nd (183),
5259                 September 12th (255), November 25th (329).
5260 
5261             From a standard calendar to a `"360_day"`, the following dates in the
5262             source array will be dropped:
5263 
5264             From a leap year:
5265                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
5266                 September 31st (275), December 1st (336)
5267             From a non-leap year:
5268                 February 6th (37), April 20th (110), July 2nd (183),
5269                 September 13th (256), November 25th (329)
5270 
5271             This option is best used on daily and subdaily data.
5272 
5273         - "date"
5274             The month/day information is conserved and invalid dates are dropped
5275             from the output. This means that when converting from a `"360_day"` to a
5276             standard calendar, all 31st (Jan, March, May, July, August, October and
5277             December) will be missing as there is no equivalent dates in the
5278             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
5279             will be dropped as there are no equivalent dates in a standard calendar.
5280 
5281             This option is best used with data on a frequency coarser than daily.
5282         """
5283         return convert_calendar(
5284             self,
5285             calendar,
5286             dim=dim,
5287             align_on=align_on,
5288             missing=missing,
5289             use_cftime=use_cftime,
5290         )
5291 
5292     def interp_calendar(
5293         self,
5294         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
5295         dim: str = "time",
5296     ) -> DataArray:
5297         """Interpolates the DataArray to another calendar based on decimal year measure.
5298 
5299         Each timestamp in `source` and `target` are first converted to their decimal
5300         year equivalent then `source` is interpolated on the target coordinate.
5301         The decimal year of a timestamp is its year plus its sub-year component
5302         converted to the fraction of its year. For example "2000-03-01 12:00" is
5303         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
5304 
5305         This method should only be used when the time (HH:MM:SS) information of
5306         time coordinate is not important.
5307 
5308         Parameters
5309         ----------
5310         target: DataArray or DatetimeIndex or CFTimeIndex
5311             The target time coordinate of a valid dtype
5312             (np.datetime64 or cftime objects)
5313         dim : str
5314             The time coordinate name.
5315 
5316         Return
5317         ------
5318         DataArray
5319             The source interpolated on the decimal years of target,
5320         """
5321         return interp_calendar(self, target, dim=dim)
5322 
5323     def groupby(
5324         self,
5325         group: Hashable | DataArray | IndexVariable,
5326         squeeze: bool = True,
5327         restore_coord_dims: bool = False,
5328     ) -> DataArrayGroupBy:
5329         """Returns a DataArrayGroupBy object for performing grouped operations.
5330 
5331         Parameters
5332         ----------
5333         group : Hashable, DataArray or IndexVariable
5334             Array whose unique values should be used to group this array. If a
5335             Hashable, must be the name of a coordinate contained in this dataarray.
5336         squeeze : bool, default: True
5337             If "group" is a dimension of any arrays in this dataset, `squeeze`
5338             controls whether the subarrays have a dimension of length 1 along
5339             that dimension or if the dimension is squeezed out.
5340         restore_coord_dims : bool, default: False
5341             If True, also restore the dimension order of multi-dimensional
5342             coordinates.
5343 
5344         Returns
5345         -------
5346         grouped : DataArrayGroupBy
5347             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5348             iterated over in the form of `(unique_value, grouped_array)` pairs.
5349 
5350         Examples
5351         --------
5352         Calculate daily anomalies for daily data:
5353 
5354         >>> da = xr.DataArray(
5355         ...     np.linspace(0, 1826, num=1827),
5356         ...     coords=[pd.date_range("1/1/2000", "31/12/2004", freq="D")],
5357         ...     dims="time",
5358         ... )
5359         >>> da
5360         <xarray.DataArray (time: 1827)>
5361         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
5362                1.826e+03])
5363         Coordinates:
5364           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5365         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
5366         <xarray.DataArray (time: 1827)>
5367         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
5368         Coordinates:
5369           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5370             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
5371 
5372         See Also
5373         --------
5374         DataArray.groupby_bins
5375         Dataset.groupby
5376         core.groupby.DataArrayGroupBy
5377         pandas.DataFrame.groupby
5378         """
5379         from .groupby import DataArrayGroupBy
5380 
5381         # While we don't generally check the type of every arg, passing
5382         # multiple dimensions as multiple arguments is common enough, and the
5383         # consequences hidden enough (strings evaluate as true) to warrant
5384         # checking here.
5385         # A future version could make squeeze kwarg only, but would face
5386         # backward-compat issues.
5387         if not isinstance(squeeze, bool):
5388             raise TypeError(
5389                 f"`squeeze` must be True or False, but {squeeze} was supplied"
5390             )
5391 
5392         return DataArrayGroupBy(
5393             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
5394         )
5395 
5396     def groupby_bins(
5397         self,
5398         group: Hashable | DataArray | IndexVariable,
5399         bins: ArrayLike,
5400         right: bool = True,
5401         labels: ArrayLike | Literal[False] | None = None,
5402         precision: int = 3,
5403         include_lowest: bool = False,
5404         squeeze: bool = True,
5405         restore_coord_dims: bool = False,
5406     ) -> DataArrayGroupBy:
5407         """Returns a DataArrayGroupBy object for performing grouped operations.
5408 
5409         Rather than using all unique values of `group`, the values are discretized
5410         first by applying `pandas.cut` [1]_ to `group`.
5411 
5412         Parameters
5413         ----------
5414         group : Hashable, DataArray or IndexVariable
5415             Array whose binned values should be used to group this array. If a
5416             Hashable, must be the name of a coordinate contained in this dataarray.
5417         bins : int or array-like
5418             If bins is an int, it defines the number of equal-width bins in the
5419             range of x. However, in this case, the range of x is extended by .1%
5420             on each side to include the min or max values of x. If bins is a
5421             sequence it defines the bin edges allowing for non-uniform bin
5422             width. No extension of the range of x is done in this case.
5423         right : bool, default: True
5424             Indicates whether the bins include the rightmost edge or not. If
5425             right == True (the default), then the bins [1,2,3,4] indicate
5426             (1,2], (2,3], (3,4].
5427         labels : array-like, False or None, default: None
5428             Used as labels for the resulting bins. Must be of the same length as
5429             the resulting bins. If False, string bin labels are assigned by
5430             `pandas.cut`.
5431         precision : int, default: 3
5432             The precision at which to store and display the bins labels.
5433         include_lowest : bool, default: False
5434             Whether the first interval should be left-inclusive or not.
5435         squeeze : bool, default: True
5436             If "group" is a dimension of any arrays in this dataset, `squeeze`
5437             controls whether the subarrays have a dimension of length 1 along
5438             that dimension or if the dimension is squeezed out.
5439         restore_coord_dims : bool, default: False
5440             If True, also restore the dimension order of multi-dimensional
5441             coordinates.
5442 
5443         Returns
5444         -------
5445         grouped : DataArrayGroupBy
5446             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5447             iterated over in the form of `(unique_value, grouped_array)` pairs.
5448             The name of the group has the added suffix `_bins` in order to
5449             distinguish it from the original variable.
5450 
5451         See Also
5452         --------
5453         DataArray.groupby
5454         Dataset.groupby_bins
5455         core.groupby.DataArrayGroupBy
5456         pandas.DataFrame.groupby
5457 
5458         References
5459         ----------
5460         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
5461         """
5462         from .groupby import DataArrayGroupBy
5463 
5464         return DataArrayGroupBy(
5465             self,
5466             group,
5467             squeeze=squeeze,
5468             bins=bins,
5469             restore_coord_dims=restore_coord_dims,
5470             cut_kwargs={
5471                 "right": right,
5472                 "labels": labels,
5473                 "precision": precision,
5474                 "include_lowest": include_lowest,
5475             },
5476         )
5477 
5478     def weighted(self, weights: DataArray) -> DataArrayWeighted:
5479         """
5480         Weighted DataArray operations.
5481 
5482         Parameters
5483         ----------
5484         weights : DataArray
5485             An array of weights associated with the values in this Dataset.
5486             Each value in the data contributes to the reduction operation
5487             according to its associated weight.
5488 
5489         Notes
5490         -----
5491         ``weights`` must be a DataArray and cannot contain missing values.
5492         Missing values can be replaced by ``weights.fillna(0)``.
5493 
5494         Returns
5495         -------
5496         core.weighted.DataArrayWeighted
5497 
5498         See Also
5499         --------
5500         Dataset.weighted
5501         """
5502         from .weighted import DataArrayWeighted
5503 
5504         return DataArrayWeighted(self, weights)
5505 
5506     def rolling(
5507         self,
5508         dim: Mapping[Any, int] | None = None,
5509         min_periods: int | None = None,
5510         center: bool | Mapping[Any, bool] = False,
5511         **window_kwargs: int,
5512     ) -> DataArrayRolling:
5513         """
5514         Rolling window object for DataArrays.
5515 
5516         Parameters
5517         ----------
5518         dim : dict, optional
5519             Mapping from the dimension name to create the rolling iterator
5520             along (e.g. `time`) to its moving window size.
5521         min_periods : int or None, default: None
5522             Minimum number of observations in window required to have a value
5523             (otherwise result is NA). The default, None, is equivalent to
5524             setting min_periods equal to the size of the window.
5525         center : bool or Mapping to int, default: False
5526             Set the labels at the center of the window.
5527         **window_kwargs : optional
5528             The keyword arguments form of ``dim``.
5529             One of dim or window_kwargs must be provided.
5530 
5531         Returns
5532         -------
5533         core.rolling.DataArrayRolling
5534 
5535         Examples
5536         --------
5537         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
5538 
5539         >>> da = xr.DataArray(
5540         ...     np.linspace(0, 11, num=12),
5541         ...     coords=[
5542         ...         pd.date_range(
5543         ...             "1999-12-15",
5544         ...             periods=12,
5545         ...             freq=pd.DateOffset(months=1),
5546         ...         )
5547         ...     ],
5548         ...     dims="time",
5549         ... )
5550         >>> da
5551         <xarray.DataArray (time: 12)>
5552         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5553         Coordinates:
5554           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5555         >>> da.rolling(time=3, center=True).mean()
5556         <xarray.DataArray (time: 12)>
5557         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
5558         Coordinates:
5559           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5560 
5561         Remove the NaNs using ``dropna()``:
5562 
5563         >>> da.rolling(time=3, center=True).mean().dropna("time")
5564         <xarray.DataArray (time: 10)>
5565         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
5566         Coordinates:
5567           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
5568 
5569         See Also
5570         --------
5571         core.rolling.DataArrayRolling
5572         Dataset.rolling
5573         """
5574         from .rolling import DataArrayRolling
5575 
5576         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
5577         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
5578 
5579     def coarsen(
5580         self,
5581         dim: Mapping[Any, int] | None = None,
5582         boundary: CoarsenBoundaryOptions = "exact",
5583         side: SideOptions | Mapping[Any, SideOptions] = "left",
5584         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
5585         **window_kwargs: int,
5586     ) -> DataArrayCoarsen:
5587         """
5588         Coarsen object for DataArrays.
5589 
5590         Parameters
5591         ----------
5592         dim : mapping of hashable to int, optional
5593             Mapping from the dimension name to the window size.
5594         boundary : {"exact", "trim", "pad"}, default: "exact"
5595             If 'exact', a ValueError will be raised if dimension size is not a
5596             multiple of the window size. If 'trim', the excess entries are
5597             dropped. If 'pad', NA will be padded.
5598         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
5599         coord_func : str or mapping of hashable to str, default: "mean"
5600             function (name) that is applied to the coordinates,
5601             or a mapping from coordinate name to function (name).
5602 
5603         Returns
5604         -------
5605         core.rolling.DataArrayCoarsen
5606 
5607         Examples
5608         --------
5609         Coarsen the long time series by averaging over every four days.
5610 
5611         >>> da = xr.DataArray(
5612         ...     np.linspace(0, 364, num=364),
5613         ...     dims="time",
5614         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
5615         ... )
5616         >>> da  # +doctest: ELLIPSIS
5617         <xarray.DataArray (time: 364)>
5618         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
5619                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
5620                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
5621         ...
5622                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
5623                360.99173554, 361.99449036, 362.99724518, 364.        ])
5624         Coordinates:
5625           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
5626         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
5627         <xarray.DataArray (time: 121)>
5628         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
5629                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
5630                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
5631         ...
5632                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
5633                361.99449036])
5634         Coordinates:
5635           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
5636         >>>
5637 
5638         See Also
5639         --------
5640         core.rolling.DataArrayCoarsen
5641         Dataset.coarsen
5642         """
5643         from .rolling import DataArrayCoarsen
5644 
5645         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
5646         return DataArrayCoarsen(
5647             self,
5648             dim,
5649             boundary=boundary,
5650             side=side,
5651             coord_func=coord_func,
5652         )
5653 
5654     def resample(
5655         self,
5656         indexer: Mapping[Any, str] | None = None,
5657         skipna: bool | None = None,
5658         closed: SideOptions | None = None,
5659         label: SideOptions | None = None,
5660         base: int = 0,
5661         keep_attrs: bool | None = None,
5662         loffset: datetime.timedelta | str | None = None,
5663         restore_coord_dims: bool | None = None,
5664         **indexer_kwargs: str,
5665     ) -> DataArrayResample:
5666         """Returns a Resample object for performing resampling operations.
5667 
5668         Handles both downsampling and upsampling. The resampled
5669         dimension must be a datetime-like coordinate. If any intervals
5670         contain no values from the original object, they will be given
5671         the value ``NaN``.
5672 
5673         Parameters
5674         ----------
5675         indexer : Mapping of Hashable to str, optional
5676             Mapping from the dimension name to resample frequency [1]_. The
5677             dimension must be datetime-like.
5678         skipna : bool, optional
5679             Whether to skip missing values when aggregating in downsampling.
5680         closed : {"left", "right"}, optional
5681             Side of each interval to treat as closed.
5682         label : {"left", "right"}, optional
5683             Side of each interval to use for labeling.
5684         base : int, default = 0
5685             For frequencies that evenly subdivide 1 day, the "origin" of the
5686             aggregated intervals. For example, for "24H" frequency, base could
5687             range from 0 through 23.
5688         loffset : timedelta or str, optional
5689             Offset used to adjust the resampled time labels. Some pandas date
5690             offset strings are supported.
5691         restore_coord_dims : bool, optional
5692             If True, also restore the dimension order of multi-dimensional
5693             coordinates.
5694         **indexer_kwargs : str
5695             The keyword arguments form of ``indexer``.
5696             One of indexer or indexer_kwargs must be provided.
5697 
5698         Returns
5699         -------
5700         resampled : core.resample.DataArrayResample
5701             This object resampled.
5702 
5703         Examples
5704         --------
5705         Downsample monthly time-series data to seasonal data:
5706 
5707         >>> da = xr.DataArray(
5708         ...     np.linspace(0, 11, num=12),
5709         ...     coords=[
5710         ...         pd.date_range(
5711         ...             "1999-12-15",
5712         ...             periods=12,
5713         ...             freq=pd.DateOffset(months=1),
5714         ...         )
5715         ...     ],
5716         ...     dims="time",
5717         ... )
5718         >>> da
5719         <xarray.DataArray (time: 12)>
5720         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5721         Coordinates:
5722           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5723         >>> da.resample(time="QS-DEC").mean()
5724         <xarray.DataArray (time: 4)>
5725         array([ 1.,  4.,  7., 10.])
5726         Coordinates:
5727           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
5728 
5729         Upsample monthly time-series data to daily data:
5730 
5731         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
5732         <xarray.DataArray (time: 337)>
5733         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
5734                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
5735                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
5736         ...
5737                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
5738                10.96774194, 11.        ])
5739         Coordinates:
5740           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5741 
5742         Limit scope of upsampling method
5743 
5744         >>> da.resample(time="1D").nearest(tolerance="1D")
5745         <xarray.DataArray (time: 337)>
5746         array([ 0.,  0., nan, ..., nan, 11., 11.])
5747         Coordinates:
5748           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5749 
5750         See Also
5751         --------
5752         Dataset.resample
5753         pandas.Series.resample
5754         pandas.DataFrame.resample
5755 
5756         References
5757         ----------
5758         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
5759         """
5760         from .resample import DataArrayResample
5761 
5762         return self._resample(
5763             resample_cls=DataArrayResample,
5764             indexer=indexer,
5765             skipna=skipna,
5766             closed=closed,
5767             label=label,
5768             base=base,
5769             keep_attrs=keep_attrs,
5770             loffset=loffset,
5771             restore_coord_dims=restore_coord_dims,
5772             **indexer_kwargs,
5773         )
5774 
5775     # this needs to be at the end, or mypy will confuse with `str`
5776     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
5777     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>rename_vars</method>
<code>
3651     def rename_vars(
3652         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3653     ) -> T_Dataset:
3654         """Returns a new object with renamed variables including coordinates
3655 
3656         Parameters
3657         ----------
3658         name_dict : dict-like, optional
3659             Dictionary whose keys are current variable or coordinate names and
3660             whose values are the desired names.
3661         **names : optional
3662             Keyword form of ``name_dict``.
3663             One of name_dict or names must be provided.
3664 
3665         Returns
3666         -------
3667         renamed : Dataset
3668             Dataset with renamed variables including coordinates
3669 
3670         See Also
3671         --------
3672         Dataset.swap_dims
3673         Dataset.rename
3674         Dataset.rename_dims
3675         DataArray.rename
3676         """
3677         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3678         for k in name_dict:
3679             if k not in self:
3680                 raise ValueError(
3681                     f"cannot rename {k!r} because it is not a "
3682                     "variable or coordinate in this dataset"
3683                 )
3684         variables, coord_names, dims, indexes = self._rename_all(
3685             name_dict=name_dict, dims_dict={}
3686         )
3687         return self._replace(variables, coord_names, dims=dims, indexes=indexes)

</code><intended_behavior>Similar to the `DataArray.rename` method, the `rename_vars` method in the `Dataset` class should include logic to detect when a variable (coordinate) renaming action results in the variable becoming a dimension coordinate. Upon such an event, it should ensure an index is created or updated for the new dimension coordinate. The method must maintain its current functionality while extending it to manage indexes for newly dimensioned coordinates efficiently.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the index’s type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the index’s type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Returns a new object with renamed variables, coordinates and dimensions.
3569 
3570         Parameters
3571         ----------
3572         name_dict : dict-like, optional
3573             Dictionary whose keys are current variable, coordinate or dimension names and
3574             whose values are the desired names.
3575         **names : optional
3576             Keyword form of ``name_dict``.
3577             One of name_dict or names must be provided.
3578 
3579         Returns
3580         -------
3581         renamed : Dataset
3582             Dataset with renamed variables, coordinates and dimensions.
3583 
3584         See Also
3585         --------
3586         Dataset.swap_dims
3587         Dataset.rename_vars
3588         Dataset.rename_dims
3589         DataArray.rename
3590         """
3591         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3592         for k in name_dict.keys():
3593             if k not in self and k not in self.dims:
3594                 raise ValueError(
3595                     f"cannot rename {k!r} because it is not a "
3596                     "variable or dimension in this dataset"
3597                 )
3598 
3599         variables, coord_names, dims, indexes = self._rename_all(
3600             name_dict=name_dict, dims_dict=name_dict
3601         )
3602         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3603 
3604     def rename_dims(
3605         self: T_Dataset,
3606         dims_dict: Mapping[Any, Hashable] | None = None,
3607         **dims: Hashable,
3608     ) -> T_Dataset:
3609         """Returns a new object with renamed dimensions only.
3610 
3611         Parameters
3612         ----------
3613         dims_dict : dict-like, optional
3614             Dictionary whose keys are current dimension names and
3615             whose values are the desired names. The desired names must
3616             not be the name of an existing dimension or Variable in the Dataset.
3617         **dims : optional
3618             Keyword form of ``dims_dict``.
3619             One of dims_dict or dims must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename
3630         Dataset.rename_vars
3631         DataArray.rename
3632         """
3633         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3634         for k, v in dims_dict.items():
3635             if k not in self.dims:
3636                 raise ValueError(
3637                     f"cannot rename {k!r} because it is not a "
3638                     "dimension in this dataset"
3639                 )
3640             if v in self.dims or v in self:
3641                 raise ValueError(
3642                     f"Cannot rename {k} to {v} because {v} already exists. "
3643                     "Try using swap_dims instead."
3644                 )
3645 
3646         variables, coord_names, sizes, indexes = self._rename_all(
3647             name_dict={}, dims_dict=dims_dict
3648         )
3649         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3650 
3651     def rename_vars(
3652         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3653     ) -> T_Dataset:
3654         """Returns a new object with renamed variables including coordinates
3655 
3656         Parameters
3657         ----------
3658         name_dict : dict-like, optional
3659             Dictionary whose keys are current variable or coordinate names and
3660             whose values are the desired names.
3661         **names : optional
3662             Keyword form of ``name_dict``.
3663             One of name_dict or names must be provided.
3664 
3665         Returns
3666         -------
3667         renamed : Dataset
3668             Dataset with renamed variables including coordinates
3669 
3670         See Also
3671         --------
3672         Dataset.swap_dims
3673         Dataset.rename
3674         Dataset.rename_dims
3675         DataArray.rename
3676         """
3677         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3678         for k in name_dict:
3679             if k not in self:
3680                 raise ValueError(
3681                     f"cannot rename {k!r} because it is not a "
3682                     "variable or coordinate in this dataset"
3683                 )
3684         variables, coord_names, dims, indexes = self._rename_all(
3685             name_dict=name_dict, dims_dict={}
3686         )
3687         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3688 
3689     def swap_dims(
3690         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3691     ) -> T_Dataset:
3692         """Returns a new object with swapped dimensions.
3693 
3694         Parameters
3695         ----------
3696         dims_dict : dict-like
3697             Dictionary whose keys are current dimension names and whose values
3698             are new names.
3699         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3700             The keyword arguments form of ``dims_dict``.
3701             One of dims_dict or dims_kwargs must be provided.
3702 
3703         Returns
3704         -------
3705         swapped : Dataset
3706             Dataset with swapped dimensions.
3707 
3708         Examples
3709         --------
3710         >>> ds = xr.Dataset(
3711         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3712         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3713         ... )
3714         >>> ds
3715         <xarray.Dataset>
3716         Dimensions:  (x: 2)
3717         Coordinates:
3718           * x        (x) <U1 'a' 'b'
3719             y        (x) int64 0 1
3720         Data variables:
3721             a        (x) int64 5 7
3722             b        (x) float64 0.1 2.4
3723 
3724         >>> ds.swap_dims({"x": "y"})
3725         <xarray.Dataset>
3726         Dimensions:  (y: 2)
3727         Coordinates:
3728             x        (y) <U1 'a' 'b'
3729           * y        (y) int64 0 1
3730         Data variables:
3731             a        (y) int64 5 7
3732             b        (y) float64 0.1 2.4
3733 
3734         >>> ds.swap_dims({"x": "z"})
3735         <xarray.Dataset>
3736         Dimensions:  (z: 2)
3737         Coordinates:
3738             x        (z) <U1 'a' 'b'
3739             y        (z) int64 0 1
3740         Dimensions without coordinates: z
3741         Data variables:
3742             a        (z) int64 5 7
3743             b        (z) float64 0.1 2.4
3744 
3745         See Also
3746         --------
3747         Dataset.rename
3748         DataArray.swap_dims
3749         """
3750         # TODO: deprecate this method in favor of a (less confusing)
3751         # rename_dims() method that only renames dimensions.
3752 
3753         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3754         for k, v in dims_dict.items():
3755             if k not in self.dims:
3756                 raise ValueError(
3757                     f"cannot swap from dimension {k!r} because it is "
3758                     "not an existing dimension"
3759                 )
3760             if v in self.variables and self.variables[v].dims != (k,):
3761                 raise ValueError(
3762                     f"replacement dimension {v!r} is not a 1D "
3763                     f"variable along the old dimension {k!r}"
3764                 )
3765 
3766         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3767 
3768         coord_names = self._coord_names.copy()
3769         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3770 
3771         variables: dict[Hashable, Variable] = {}
3772         indexes: dict[Hashable, Index] = {}
3773         for k, v in self.variables.items():
3774             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3775             var: Variable
3776             if k in result_dims:
3777                 var = v.to_index_variable()
3778                 var.dims = dims
3779                 if k in self._indexes:
3780                     indexes[k] = self._indexes[k]
3781                     variables[k] = var
3782                 else:
3783                     index, index_vars = create_default_index_implicit(var)
3784                     indexes.update({name: index for name in index_vars})
3785                     variables.update(index_vars)
3786                     coord_names.update(index_vars)
3787             else:
3788                 var = v.to_base_variable()
3789                 var.dims = dims
3790                 variables[k] = var
3791 
3792         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3793 
3794     # change type of self and return to T_Dataset once
3795     # https://github.com/python/mypy/issues/12846 is resolved
3796     def expand_dims(
3797         self,
3798         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3799         axis: None | int | Sequence[int] = None,
3800         **dim_kwargs: Any,
3801     ) -> Dataset:
3802         """Return a new object with an additional axis (or axes) inserted at
3803         the corresponding position in the array shape.  The new object is a
3804         view into the underlying array, not a copy.
3805 
3806         If dim is already a scalar coordinate, it will be promoted to a 1D
3807         coordinate consisting of a single value.
3808 
3809         Parameters
3810         ----------
3811         dim : hashable, sequence of hashable, mapping, or None
3812             Dimensions to include on the new variable. If provided as hashable
3813             or sequence of hashable, then dimensions are inserted with length
3814             1. If provided as a mapping, then the keys are the new dimensions
3815             and the values are either integers (giving the length of the new
3816             dimensions) or array-like (giving the coordinates of the new
3817             dimensions).
3818         axis : int, sequence of int, or None, default: None
3819             Axis position(s) where new axis is to be inserted (position(s) on
3820             the result array). If a sequence of integers is passed,
3821             multiple axes are inserted. In this case, dim arguments should be
3822             same length list. If axis=None is passed, all the axes will be
3823             inserted to the start of the result array.
3824         **dim_kwargs : int or sequence or ndarray
3825             The keywords are arbitrary dimensions being inserted and the values
3826             are either the lengths of the new dims (if int is given), or their
3827             coordinates. Note, this is an alternative to passing a dict to the
3828             dim kwarg and will only be used if dim is None.
3829 
3830         Returns
3831         -------
3832         expanded : Dataset
3833             This object, but with additional dimension(s).
3834 
3835         See Also
3836         --------
3837         DataArray.expand_dims
3838         """
3839         if dim is None:
3840             pass
3841         elif isinstance(dim, Mapping):
3842             # We're later going to modify dim in place; don't tamper with
3843             # the input
3844             dim = dict(dim)
3845         elif isinstance(dim, int):
3846             raise TypeError(
3847                 "dim should be hashable or sequence of hashables or mapping"
3848             )
3849         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3850             dim = {dim: 1}
3851         elif isinstance(dim, Sequence):
3852             if len(dim) != len(set(dim)):
3853                 raise ValueError("dims should not contain duplicate values.")
3854             dim = {d: 1 for d in dim}
3855 
3856         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3857         assert isinstance(dim, MutableMapping)
3858 
3859         if axis is None:
3860             axis = list(range(len(dim)))
3861         elif not isinstance(axis, Sequence):
3862             axis = [axis]
3863 
3864         if len(dim) != len(axis):
3865             raise ValueError("lengths of dim and axis should be identical.")
3866         for d in dim:
3867             if d in self.dims:
3868                 raise ValueError(f"Dimension {d} already exists.")
3869             if d in self._variables and not utils.is_scalar(self._variables[d]):
3870                 raise ValueError(
3871                     "{dim} already exists as coordinate or"
3872                     " variable name.".format(dim=d)
3873                 )
3874 
3875         variables: dict[Hashable, Variable] = {}
3876         indexes: dict[Hashable, Index] = dict(self._indexes)
3877         coord_names = self._coord_names.copy()
3878         # If dim is a dict, then ensure that the values are either integers
3879         # or iterables.
3880         for k, v in dim.items():
3881             if hasattr(v, "__iter__"):
3882                 # If the value for the new dimension is an iterable, then
3883                 # save the coordinates to the variables dict, and set the
3884                 # value within the dim dict to the length of the iterable
3885                 # for later use.
3886                 index = PandasIndex(v, k)
3887                 indexes[k] = index
3888                 variables.update(index.create_variables())
3889                 coord_names.add(k)
3890                 dim[k] = variables[k].size
3891             elif isinstance(v, int):
3892                 pass  # Do nothing if the dimensions value is just an int
3893             else:
3894                 raise TypeError(
3895                     "The value of new dimension {k} must be "
3896                     "an iterable or an int".format(k=k)
3897                 )
3898 
3899         for k, v in self._variables.items():
3900             if k not in dim:
3901                 if k in coord_names:  # Do not change coordinates
3902                     variables[k] = v
3903                 else:
3904                     result_ndim = len(v.dims) + len(axis)
3905                     for a in axis:
3906                         if a < -result_ndim or result_ndim - 1 < a:
3907                             raise IndexError(
3908                                 f"Axis {a} of variable {k} is out of bounds of the "
3909                                 f"expanded dimension size {result_ndim}"
3910                             )
3911 
3912                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3913                     if len(axis_pos) != len(set(axis_pos)):
3914                         raise ValueError("axis should not contain duplicate values")
3915                     # We need to sort them to make sure `axis` equals to the
3916                     # axis positions of the result array.
3917                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3918 
3919                     all_dims = list(zip(v.dims, v.shape))
3920                     for d, c in zip_axis_dim:
3921                         all_dims.insert(d, c)
3922                     variables[k] = v.set_dims(dict(all_dims))
3923             else:
3924                 if k not in variables:
3925                     # If dims includes a label of a non-dimension coordinate,
3926                     # it will be promoted to a 1D coordinate with a single value.
3927                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3928                     indexes[k] = index
3929                     variables.update(index_vars)
3930 
3931         return self._replace_with_new_dims(
3932             variables, coord_names=coord_names, indexes=indexes
3933         )
3934 
3935     # change type of self and return to T_Dataset once
3936     # https://github.com/python/mypy/issues/12846 is resolved
3937     def set_index(
3938         self,
3939         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3940         append: bool = False,
3941         **indexes_kwargs: Hashable | Sequence[Hashable],
3942     ) -> Dataset:
3943         """Set Dataset (multi-)indexes using one or more existing coordinates
3944         or variables.
3945 
3946         Parameters
3947         ----------
3948         indexes : {dim: index, ...}
3949             Mapping from names matching dimensions and values given
3950             by (lists of) the names of existing coordinates or variables to set
3951             as new (multi-)index.
3952         append : bool, default: False
3953             If True, append the supplied index(es) to the existing index(es).
3954             Otherwise replace the existing index(es) (default).
3955         **indexes_kwargs : optional
3956             The keyword arguments form of ``indexes``.
3957             One of indexes or indexes_kwargs must be provided.
3958 
3959         Returns
3960         -------
3961         obj : Dataset
3962             Another dataset, with this dataset's data but replaced coordinates.
3963 
3964         Examples
3965         --------
3966         >>> arr = xr.DataArray(
3967         ...     data=np.ones((2, 3)),
3968         ...     dims=["x", "y"],
3969         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3970         ... )
3971         >>> ds = xr.Dataset({"v": arr})
3972         >>> ds
3973         <xarray.Dataset>
3974         Dimensions:  (x: 2, y: 3)
3975         Coordinates:
3976           * x        (x) int64 0 1
3977           * y        (y) int64 0 1 2
3978             a        (x) int64 3 4
3979         Data variables:
3980             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3981         >>> ds.set_index(x="a")
3982         <xarray.Dataset>
3983         Dimensions:  (x: 2, y: 3)
3984         Coordinates:
3985           * x        (x) int64 3 4
3986           * y        (y) int64 0 1 2
3987         Data variables:
3988             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3989 
3990         See Also
3991         --------
3992         Dataset.reset_index
3993         Dataset.swap_dims
3994         """
3995         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3996 
3997         new_indexes: dict[Hashable, Index] = {}
3998         new_variables: dict[Hashable, IndexVariable] = {}
3999         maybe_drop_indexes: list[Hashable] = []
4000         drop_variables: list[Hashable] = []
4001         replace_dims: dict[Hashable, Hashable] = {}
4002 
4003         for dim, _var_names in dim_coords.items():
4004             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4005                 var_names = [_var_names]
4006             else:
4007                 var_names = list(_var_names)
4008 
4009             invalid_vars = set(var_names) - set(self._variables)
4010             if invalid_vars:
4011                 raise ValueError(
4012                     ", ".join([str(v) for v in invalid_vars])
4013                     + " variable(s) do not exist"
4014                 )
4015 
4016             current_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4017 
4018             # drop any pre-existing index involved
4019             maybe_drop_indexes += list(current_coord_names) + var_names
4020             for k in var_names:
4021                 maybe_drop_indexes += list(
4022                     self.xindexes.get_all_coords(k, errors="ignore")
4023                 )
4024 
4025             drop_variables += var_names
4026 
4027             if len(var_names) == 1 and (not append or dim not in self._indexes):
4028                 var_name = var_names[0]
4029                 var = self._variables[var_name]
4030                 if var.dims != (dim,):
4031                     raise ValueError(
4032                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4033                         f"variable {var_name!r} that has dimensions {var.dims}"
4034                     )
4035                 idx = PandasIndex.from_variables({dim: var})
4036                 idx_vars = idx.create_variables({var_name: var})
4037             else:
4038                 if append:
4039                     current_variables = {
4040                         k: self._variables[k] for k in current_coord_names
4041                     }
4042                 else:
4043                     current_variables = {}
4044                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4045                     dim,
4046                     current_variables,
4047                     {k: self._variables[k] for k in var_names},
4048                 )
4049                 for n in idx.index.names:
4050                     replace_dims[n] = dim
4051 
4052             new_indexes.update({k: idx for k in idx_vars})
4053             new_variables.update(idx_vars)
4054 
4055         indexes_: dict[Any, Index] = {
4056             k: v for k, v in self._indexes.items() if k not in maybe_drop_indexes
4057         }
4058         indexes_.update(new_indexes)
4059 
4060         variables = {
4061             k: v for k, v in self._variables.items() if k not in drop_variables
4062         }
4063         variables.update(new_variables)
4064 
4065         # update dimensions if necessary, GH: 3512
4066         for k, v in variables.items():
4067             if any(d in replace_dims for d in v.dims):
4068                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4069                 variables[k] = v._replace(dims=new_dims)
4070 
4071         coord_names = self._coord_names - set(drop_variables) | set(new_variables)
4072 
4073         return self._replace_with_new_dims(
4074             variables, coord_names=coord_names, indexes=indexes_
4075         )
4076 
4077     def reset_index(
4078         self: T_Dataset,
4079         dims_or_levels: Hashable | Sequence[Hashable],
4080         drop: bool = False,
4081     ) -> T_Dataset:
4082         """Reset the specified index(es) or multi-index level(s).
4083 
4084         Parameters
4085         ----------
4086         dims_or_levels : Hashable or Sequence of Hashable
4087             Name(s) of the dimension(s) and/or multi-index level(s) that will
4088             be reset.
4089         drop : bool, default: False
4090             If True, remove the specified indexes and/or multi-index levels
4091             instead of extracting them as new coordinates (default: False).
4092 
4093         Returns
4094         -------
4095         obj : Dataset
4096             Another dataset, with this dataset's data but replaced coordinates.
4097 
4098         See Also
4099         --------
4100         Dataset.set_index
4101         """
4102         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4103             dims_or_levels = [dims_or_levels]
4104 
4105         invalid_coords = set(dims_or_levels) - set(self._indexes)
4106         if invalid_coords:
4107             raise ValueError(
4108                 f"{tuple(invalid_coords)} are not coordinates with an index"
4109             )
4110 
4111         drop_indexes: list[Hashable] = []
4112         drop_variables: list[Hashable] = []
4113         replaced_indexes: list[PandasMultiIndex] = []
4114         new_indexes: dict[Hashable, Index] = {}
4115         new_variables: dict[Hashable, IndexVariable] = {}
4116 
4117         for name in dims_or_levels:
4118             index = self._indexes[name]
4119             drop_indexes += list(self.xindexes.get_all_coords(name))
4120 
4121             if isinstance(index, PandasMultiIndex) and name not in self.dims:
4122                 # special case for pd.MultiIndex (name is an index level):
4123                 # replace by a new index with dropped level(s) instead of just drop the index
4124                 if index not in replaced_indexes:
4125                     level_names = index.index.names
4126                     level_vars = {
4127                         k: self._variables[k]
4128                         for k in level_names
4129                         if k not in dims_or_levels
4130                     }
4131                     if level_vars:
4132                         idx = index.keep_levels(level_vars)
4133                         idx_vars = idx.create_variables(level_vars)
4134                         new_indexes.update({k: idx for k in idx_vars})
4135                         new_variables.update(idx_vars)
4136                 replaced_indexes.append(index)
4137 
4138             if drop:
4139                 drop_variables.append(name)
4140 
4141         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4142         indexes.update(new_indexes)
4143 
4144         variables = {
4145             k: v for k, v in self._variables.items() if k not in drop_variables
4146         }
4147         variables.update(new_variables)
4148 
4149         coord_names = set(new_variables) | self._coord_names
4150 
4151         return self._replace(variables, coord_names=coord_names, indexes=indexes)
4152 
4153     def reorder_levels(
4154         self: T_Dataset,
4155         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4156         **dim_order_kwargs: Sequence[int | Hashable],
4157     ) -> T_Dataset:
4158         """Rearrange index levels using input order.
4159 
4160         Parameters
4161         ----------
4162         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4163             Mapping from names matching dimensions and values given
4164             by lists representing new level orders. Every given dimension
4165             must have a multi-index.
4166         **dim_order_kwargs : Sequence of int or Hashable, optional
4167             The keyword arguments form of ``dim_order``.
4168             One of dim_order or dim_order_kwargs must be provided.
4169 
4170         Returns
4171         -------
4172         obj : Dataset
4173             Another dataset, with this dataset's data but replaced
4174             coordinates.
4175         """
4176         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4177         variables = self._variables.copy()
4178         indexes = dict(self._indexes)
4179         new_indexes: dict[Hashable, Index] = {}
4180         new_variables: dict[Hashable, IndexVariable] = {}
4181 
4182         for dim, order in dim_order.items():
4183             index = self._indexes[dim]
4184 
4185             if not isinstance(index, PandasMultiIndex):
4186                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4187 
4188             level_vars = {k: self._variables[k] for k in order}
4189             idx = index.reorder_levels(level_vars)
4190             idx_vars = idx.create_variables(level_vars)
4191             new_indexes.update({k: idx for k in idx_vars})
4192             new_variables.update(idx_vars)
4193 
4194         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4195         indexes.update(new_indexes)
4196 
4197         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4198         variables.update(new_variables)
4199 
4200         return self._replace(variables, indexes=indexes)
4201 
4202     def _get_stack_index(
4203         self,
4204         dim,
4205         multi=False,
4206         create_index=False,
4207     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4208         """Used by stack and unstack to get one pandas (multi-)index among
4209         the indexed coordinates along dimension `dim`.
4210 
4211         If exactly one index is found, return it with its corresponding
4212         coordinate variables(s), otherwise return None and an empty dict.
4213 
4214         If `create_index=True`, create a new index if none is found or raise
4215         an error if multiple indexes are found.
4216 
4217         """
4218         stack_index: Index | None = None
4219         stack_coords: dict[Hashable, Variable] = {}
4220 
4221         for name, index in self._indexes.items():
4222             var = self._variables[name]
4223             if (
4224                 var.ndim == 1
4225                 and var.dims[0] == dim
4226                 and (
4227                     # stack: must be a single coordinate index
4228                     not multi
4229                     and not self.xindexes.is_multi(name)
4230                     # unstack: must be an index that implements .unstack
4231                     or multi
4232                     and type(index).unstack is not Index.unstack
4233                 )
4234             ):
4235                 if stack_index is not None and index is not stack_index:
4236                     # more than one index found, stop
4237                     if create_index:
4238                         raise ValueError(
4239                             f"cannot stack dimension {dim!r} with `create_index=True` "
4240                             "and with more than one index found along that dimension"
4241                         )
4242                     return None, {}
4243                 stack_index = index
4244                 stack_coords[name] = var
4245 
4246         if create_index and stack_index is None:
4247             if dim in self._variables:
4248                 var = self._variables[dim]
4249             else:
4250                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4251             # dummy index (only `stack_coords` will be used to construct the multi-index)
4252             stack_index = PandasIndex([0], dim)
4253             stack_coords = {dim: var}
4254 
4255         return stack_index, stack_coords
4256 
4257     def _stack_once(
4258         self: T_Dataset,
4259         dims: Sequence[Hashable | Ellipsis],
4260         new_dim: Hashable,
4261         index_cls: type[Index],
4262         create_index: bool | None = True,
4263     ) -> T_Dataset:
4264         if dims == ...:
4265             raise ValueError("Please use [...] for dims, rather than just ...")
4266         if ... in dims:
4267             dims = list(infix_dims(dims, self.dims))
4268 
4269         new_variables: dict[Hashable, Variable] = {}
4270         stacked_var_names: list[Hashable] = []
4271         drop_indexes: list[Hashable] = []
4272 
4273         for name, var in self.variables.items():
4274             if any(d in var.dims for d in dims):
4275                 add_dims = [d for d in dims if d not in var.dims]
4276                 vdims = list(var.dims) + add_dims
4277                 shape = [self.dims[d] for d in vdims]
4278                 exp_var = var.set_dims(vdims, shape)
4279                 stacked_var = exp_var.stack(**{new_dim: dims})
4280                 new_variables[name] = stacked_var
4281                 stacked_var_names.append(name)
4282             else:
4283                 new_variables[name] = var.copy(deep=False)
4284 
4285         # drop indexes of stacked coordinates (if any)
4286         for name in stacked_var_names:
4287             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4288 
4289         new_indexes = {}
4290         new_coord_names = set(self._coord_names)
4291         if create_index or create_index is None:
4292             product_vars: dict[Any, Variable] = {}
4293             for dim in dims:
4294                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4295                 if idx is not None:
4296                     product_vars.update(idx_vars)
4297 
4298             if len(product_vars) == len(dims):
4299                 idx = index_cls.stack(product_vars, new_dim)
4300                 new_indexes[new_dim] = idx
4301                 new_indexes.update({k: idx for k in product_vars})
4302                 idx_vars = idx.create_variables(product_vars)
4303                 # keep consistent multi-index coordinate order
4304                 for k in idx_vars:
4305                     new_variables.pop(k, None)
4306                 new_variables.update(idx_vars)
4307                 new_coord_names.update(idx_vars)
4308 
4309         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4310         indexes.update(new_indexes)
4311 
4312         return self._replace_with_new_dims(
4313             new_variables, coord_names=new_coord_names, indexes=indexes
4314         )
4315 
4316     def stack(
4317         self: T_Dataset,
4318         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4319         create_index: bool | None = True,
4320         index_cls: type[Index] = PandasMultiIndex,
4321         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4322     ) -> T_Dataset:
4323         """
4324         Stack any number of existing dimensions into a single new dimension.
4325 
4326         New dimensions will be added at the end, and by default the corresponding
4327         coordinate variables will be combined into a MultiIndex.
4328 
4329         Parameters
4330         ----------
4331         dimensions : mapping of hashable to sequence of hashable
4332             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4333             dimensions, and the existing dimensions that they replace. An
4334             ellipsis (`...`) will be replaced by all unlisted dimensions.
4335             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4336             all dimensions.
4337         create_index : bool or None, default: True
4338 
4339             - True: create a multi-index for each of the stacked dimensions.
4340             - False: don't create any index.
4341             - None. create a multi-index only if exactly one single (1-d) coordinate
4342               index is found for every dimension to stack.
4343 
4344         index_cls: Index-class, default: PandasMultiIndex
4345             Can be used to pass a custom multi-index type (must be an Xarray index that
4346             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4347         **dimensions_kwargs
4348             The keyword arguments form of ``dimensions``.
4349             One of dimensions or dimensions_kwargs must be provided.
4350 
4351         Returns
4352         -------
4353         stacked : Dataset
4354             Dataset with stacked data.
4355 
4356         See Also
4357         --------
4358         Dataset.unstack
4359         """
4360         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4361         result = self
4362         for new_dim, dims in dimensions.items():
4363             result = result._stack_once(dims, new_dim, index_cls, create_index)
4364         return result
4365 
4366     def to_stacked_array(
4367         self,
4368         new_dim: Hashable,
4369         sample_dims: Collection[Hashable],
4370         variable_dim: Hashable = "variable",
4371         name: Hashable | None = None,
4372     ) -> DataArray:
4373         """Combine variables of differing dimensionality into a DataArray
4374         without broadcasting.
4375 
4376         This method is similar to Dataset.to_array but does not broadcast the
4377         variables.
4378 
4379         Parameters
4380         ----------
4381         new_dim : hashable
4382             Name of the new stacked coordinate
4383         sample_dims : Collection of hashables
4384             List of dimensions that **will not** be stacked. Each array in the
4385             dataset must share these dimensions. For machine learning
4386             applications, these define the dimensions over which samples are
4387             drawn.
4388         variable_dim : hashable, default: "variable"
4389             Name of the level in the stacked coordinate which corresponds to
4390             the variables.
4391         name : hashable, optional
4392             Name of the new data array.
4393 
4394         Returns
4395         -------
4396         stacked : DataArray
4397             DataArray with the specified dimensions and data variables
4398             stacked together. The stacked coordinate is named ``new_dim``
4399             and represented by a MultiIndex object with a level containing the
4400             data variable names. The name of this level is controlled using
4401             the ``variable_dim`` argument.
4402 
4403         See Also
4404         --------
4405         Dataset.to_array
4406         Dataset.stack
4407         DataArray.to_unstacked_dataset
4408 
4409         Examples
4410         --------
4411         >>> data = xr.Dataset(
4412         ...     data_vars={
4413         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4414         ...         "b": ("x", [6, 7]),
4415         ...     },
4416         ...     coords={"y": ["u", "v", "w"]},
4417         ... )
4418 
4419         >>> data
4420         <xarray.Dataset>
4421         Dimensions:  (x: 2, y: 3)
4422         Coordinates:
4423           * y        (y) <U1 'u' 'v' 'w'
4424         Dimensions without coordinates: x
4425         Data variables:
4426             a        (x, y) int64 0 1 2 3 4 5
4427             b        (x) int64 6 7
4428 
4429         >>> data.to_stacked_array("z", sample_dims=["x"])
4430         <xarray.DataArray 'a' (x: 2, z: 4)>
4431         array([[0, 1, 2, 6],
4432                [3, 4, 5, 7]])
4433         Coordinates:
4434           * z         (z) object MultiIndex
4435           * variable  (z) object 'a' 'a' 'a' 'b'
4436           * y         (z) object 'u' 'v' 'w' nan
4437         Dimensions without coordinates: x
4438 
4439         """
4440         from .concat import concat
4441 
4442         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4443 
4444         for variable in self:
4445             dims = self[variable].dims
4446             dims_include_sample_dims = set(sample_dims) <= set(dims)
4447             if not dims_include_sample_dims:
4448                 raise ValueError(
4449                     "All variables in the dataset must contain the "
4450                     "dimensions {}.".format(dims)
4451                 )
4452 
4453         def ensure_stackable(val):
4454             assign_coords = {variable_dim: val.name}
4455             for dim in stacking_dims:
4456                 if dim not in val.dims:
4457                     assign_coords[dim] = None
4458 
4459             expand_dims = set(stacking_dims).difference(set(val.dims))
4460             expand_dims.add(variable_dim)
4461             # must be list for .expand_dims
4462             expand_dims = list(expand_dims)
4463 
4464             return (
4465                 val.assign_coords(**assign_coords)
4466                 .expand_dims(expand_dims)
4467                 .stack({new_dim: (variable_dim,) + stacking_dims})
4468             )
4469 
4470         # concatenate the arrays
4471         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4472         data_array = concat(stackable_vars, dim=new_dim)
4473 
4474         if name is not None:
4475             data_array.name = name
4476 
4477         return data_array
4478 
4479     def _unstack_once(
4480         self: T_Dataset,
4481         dim: Hashable,
4482         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4483         fill_value,
4484         sparse: bool = False,
4485     ) -> T_Dataset:
4486         index, index_vars = index_and_vars
4487         variables: dict[Hashable, Variable] = {}
4488         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4489 
4490         new_indexes, clean_index = index.unstack()
4491         indexes.update(new_indexes)
4492 
4493         for name, idx in new_indexes.items():
4494             variables.update(idx.create_variables(index_vars))
4495 
4496         for name, var in self.variables.items():
4497             if name not in index_vars:
4498                 if dim in var.dims:
4499                     if isinstance(fill_value, Mapping):
4500                         fill_value_ = fill_value[name]
4501                     else:
4502                         fill_value_ = fill_value
4503 
4504                     variables[name] = var._unstack_once(
4505                         index=clean_index,
4506                         dim=dim,
4507                         fill_value=fill_value_,
4508                         sparse=sparse,
4509                     )
4510                 else:
4511                     variables[name] = var
4512 
4513         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4514 
4515         return self._replace_with_new_dims(
4516             variables, coord_names=coord_names, indexes=indexes
4517         )
4518 
4519     def _unstack_full_reindex(
4520         self: T_Dataset,
4521         dim: Hashable,
4522         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4523         fill_value,
4524         sparse: bool,
4525     ) -> T_Dataset:
4526         index, index_vars = index_and_vars
4527         variables: dict[Hashable, Variable] = {}
4528         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4529 
4530         new_indexes, clean_index = index.unstack()
4531         indexes.update(new_indexes)
4532 
4533         new_index_variables = {}
4534         for name, idx in new_indexes.items():
4535             new_index_variables.update(idx.create_variables(index_vars))
4536 
4537         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4538         variables.update(new_index_variables)
4539 
4540         # take a shortcut in case the MultiIndex was not modified.
4541         full_idx = pd.MultiIndex.from_product(
4542             clean_index.levels, names=clean_index.names
4543         )
4544         if clean_index.equals(full_idx):
4545             obj = self
4546         else:
4547             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4548             xr_full_idx = PandasMultiIndex(full_idx, dim)
4549             indexers = Indexes(
4550                 {k: xr_full_idx for k in index_vars},
4551                 xr_full_idx.create_variables(index_vars),
4552             )
4553             obj = self._reindex(
4554                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4555             )
4556 
4557         for name, var in obj.variables.items():
4558             if name not in index_vars:
4559                 if dim in var.dims:
4560                     variables[name] = var.unstack({dim: new_dim_sizes})
4561                 else:
4562                     variables[name] = var
4563 
4564         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4565 
4566         return self._replace_with_new_dims(
4567             variables, coord_names=coord_names, indexes=indexes
4568         )
4569 
4570     def unstack(
4571         self: T_Dataset,
4572         dim: Hashable | Iterable[Hashable] | None = None,
4573         fill_value: Any = xrdtypes.NA,
4574         sparse: bool = False,
4575     ) -> T_Dataset:
4576         """
4577         Unstack existing dimensions corresponding to MultiIndexes into
4578         multiple new dimensions.
4579 
4580         New dimensions will be added at the end.
4581 
4582         Parameters
4583         ----------
4584         dim : hashable or iterable of hashable, optional
4585             Dimension(s) over which to unstack. By default unstacks all
4586             MultiIndexes.
4587         fill_value : scalar or dict-like, default: nan
4588             value to be filled. If a dict-like, maps variable names to
4589             fill values. If not provided or if the dict-like does not
4590             contain all variables, the dtype's NA value will be used.
4591         sparse : bool, default: False
4592             use sparse-array if True
4593 
4594         Returns
4595         -------
4596         unstacked : Dataset
4597             Dataset with unstacked data.
4598 
4599         See Also
4600         --------
4601         Dataset.stack
4602         """
4603 
4604         if dim is None:
4605             dims = list(self.dims)
4606         else:
4607             if isinstance(dim, str) or not isinstance(dim, Iterable):
4608                 dims = [dim]
4609             else:
4610                 dims = list(dim)
4611 
4612             missing_dims = [d for d in dims if d not in self.dims]
4613             if missing_dims:
4614                 raise ValueError(
4615                     f"Dataset does not contain the dimensions: {missing_dims}"
4616                 )
4617 
4618         # each specified dimension must have exactly one multi-index
4619         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4620         for d in dims:
4621             idx, idx_vars = self._get_stack_index(d, multi=True)
4622             if idx is not None:
4623                 stacked_indexes[d] = idx, idx_vars
4624 
4625         if dim is None:
4626             dims = list(stacked_indexes)
4627         else:
4628             non_multi_dims = set(dims) - set(stacked_indexes)
4629             if non_multi_dims:
4630                 raise ValueError(
4631                     "cannot unstack dimensions that do not "
4632                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4633                 )
4634 
4635         result = self.copy(deep=False)
4636 
4637         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4638         # so we can't just access self.variables[v].data for every variable.
4639         # We only check the non-index variables.
4640         # https://github.com/pydata/xarray/issues/5902
4641         nonindexes = [
4642             self.variables[k] for k in set(self.variables) - set(self._indexes)
4643         ]
4644         # Notes for each of these cases:
4645         # 1. Dask arrays don't support assignment by index, which the fast unstack
4646         #    function requires.
4647         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4648         # 2. Sparse doesn't currently support (though we could special-case it)
4649         #    https://github.com/pydata/sparse/issues/422
4650         # 3. pint requires checking if it's a NumPy array until
4651         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4652         #    Once that is resolved, explicitly exclude pint arrays.
4653         #    pint doesn't implement `np.full_like` in a way that's
4654         #    currently compatible.
4655         needs_full_reindex = any(
4656             is_duck_dask_array(v.data)
4657             or isinstance(v.data, sparse_array_type)
4658             or not isinstance(v.data, np.ndarray)
4659             for v in nonindexes
4660         )
4661 
4662         for dim in dims:
4663             if needs_full_reindex:
4664                 result = result._unstack_full_reindex(
4665                     dim, stacked_indexes[dim], fill_value, sparse
4666                 )
4667             else:
4668                 result = result._unstack_once(
4669                     dim, stacked_indexes[dim], fill_value, sparse
4670                 )
4671         return result
4672 
4673     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4674         """Update this dataset's variables with those from another dataset.
4675 
4676         Just like :py:meth:`dict.update` this is a in-place operation.
4677         For a non-inplace version, see :py:meth:`Dataset.merge`.
4678 
4679         Parameters
4680         ----------
4681         other : Dataset or mapping
4682             Variables with which to update this dataset. One of:
4683 
4684             - Dataset
4685             - mapping {var name: DataArray}
4686             - mapping {var name: Variable}
4687             - mapping {var name: (dimension name, array-like)}
4688             - mapping {var name: (tuple of dimension names, array-like)}
4689 
4690         Returns
4691         -------
4692         updated : Dataset
4693             Updated dataset. Note that since the update is in-place this is the input
4694             dataset.
4695 
4696             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4697 
4698         Raises
4699         ------
4700         ValueError
4701             If any dimensions would have inconsistent sizes in the updated
4702             dataset.
4703 
4704         See Also
4705         --------
4706         Dataset.assign
4707         Dataset.merge
4708         """
4709         merge_result = dataset_update_method(self, other)
4710         return self._replace(inplace=True, **merge_result._asdict())
4711 
4712     def merge(
4713         self: T_Dataset,
4714         other: CoercibleMapping | DataArray,
4715         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4716         compat: CompatOptions = "no_conflicts",
4717         join: JoinOptions = "outer",
4718         fill_value: Any = xrdtypes.NA,
4719         combine_attrs: CombineAttrsOptions = "override",
4720     ) -> T_Dataset:
4721         """Merge the arrays of two datasets into a single dataset.
4722 
4723         This method generally does not allow for overriding data, with the
4724         exception of attributes, which are ignored on the second dataset.
4725         Variables with the same name are checked for conflicts via the equals
4726         or identical methods.
4727 
4728         Parameters
4729         ----------
4730         other : Dataset or mapping
4731             Dataset or variables to merge with this dataset.
4732         overwrite_vars : hashable or iterable of hashable, optional
4733             If provided, update variables of these name(s) without checking for
4734             conflicts in this dataset.
4735         compat : {"identical", "equals", "broadcast_equals", \
4736                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4737             String indicating how to compare variables of the same name for
4738             potential conflicts:
4739 
4740             - 'identical': all values, dimensions and attributes must be the
4741               same.
4742             - 'equals': all values and dimensions must be the same.
4743             - 'broadcast_equals': all values must be equal when variables are
4744               broadcast against each other to ensure common dimensions.
4745             - 'no_conflicts': only values which are not null in both datasets
4746               must be equal. The returned dataset then contains the combination
4747               of all non-null values.
4748             - 'override': skip comparing and pick variable from first dataset
4749             - 'minimal': drop conflicting coordinates
4750 
4751         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4752                default: "outer"
4753             Method for joining ``self`` and ``other`` along shared dimensions:
4754 
4755             - 'outer': use the union of the indexes
4756             - 'inner': use the intersection of the indexes
4757             - 'left': use indexes from ``self``
4758             - 'right': use indexes from ``other``
4759             - 'exact': error instead of aligning non-equal indexes
4760             - 'override': use indexes from ``self`` that are the same size
4761               as those of ``other`` in that dimension
4762 
4763         fill_value : scalar or dict-like, optional
4764             Value to use for newly missing values. If a dict-like, maps
4765             variable names (including coordinates) to fill values.
4766         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4767                          "override"} or callable, default: "override"
4768             A callable or a string indicating how to combine attrs of the objects being
4769             merged:
4770 
4771             - "drop": empty attrs on returned Dataset.
4772             - "identical": all attrs must be the same on every object.
4773             - "no_conflicts": attrs from all objects are combined, any that have
4774               the same name must also have the same value.
4775             - "drop_conflicts": attrs from all objects are combined, any that have
4776               the same name but different values are dropped.
4777             - "override": skip comparing and copy attrs from the first dataset to
4778               the result.
4779 
4780             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4781             as its only parameters.
4782 
4783         Returns
4784         -------
4785         merged : Dataset
4786             Merged dataset.
4787 
4788         Raises
4789         ------
4790         MergeError
4791             If any variables conflict (see ``compat``).
4792 
4793         See Also
4794         --------
4795         Dataset.update
4796         """
4797         from .dataarray import DataArray
4798 
4799         other = other.to_dataset() if isinstance(other, DataArray) else other
4800         merge_result = dataset_merge_method(
4801             self,
4802             other,
4803             overwrite_vars=overwrite_vars,
4804             compat=compat,
4805             join=join,
4806             fill_value=fill_value,
4807             combine_attrs=combine_attrs,
4808         )
4809         return self._replace(**merge_result._asdict())
4810 
4811     def _assert_all_in_dataset(
4812         self, names: Iterable[Hashable], virtual_okay: bool = False
4813     ) -> None:
4814         bad_names = set(names) - set(self._variables)
4815         if virtual_okay:
4816             bad_names -= self.virtual_variables
4817         if bad_names:
4818             raise ValueError(
4819                 "One or more of the specified variables "
4820                 "cannot be found in this dataset"
4821             )
4822 
4823     def drop_vars(
4824         self: T_Dataset,
4825         names: Hashable | Iterable[Hashable],
4826         *,
4827         errors: ErrorOptions = "raise",
4828     ) -> T_Dataset:
4829         """Drop variables from this dataset.
4830 
4831         Parameters
4832         ----------
4833         names : hashable or iterable of hashable
4834             Name(s) of variables to drop.
4835         errors : {"raise", "ignore"}, default: "raise"
4836             If 'raise', raises a ValueError error if any of the variable
4837             passed are not in the dataset. If 'ignore', any given names that are in the
4838             dataset are dropped and no error is raised.
4839 
4840         Returns
4841         -------
4842         dropped : Dataset
4843 
4844         """
4845         # the Iterable check is required for mypy
4846         if is_scalar(names) or not isinstance(names, Iterable):
4847             names = {names}
4848         else:
4849             names = set(names)
4850         if errors == "raise":
4851             self._assert_all_in_dataset(names)
4852 
4853         # GH6505
4854         other_names = set()
4855         for var in names:
4856             maybe_midx = self._indexes.get(var, None)
4857             if isinstance(maybe_midx, PandasMultiIndex):
4858                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
4859                 idx_other_names = idx_coord_names - set(names)
4860                 other_names.update(idx_other_names)
4861         if other_names:
4862             names |= set(other_names)
4863             warnings.warn(
4864                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
4865                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
4866                 DeprecationWarning,
4867                 stacklevel=2,
4868             )
4869 
4870         assert_no_index_corrupted(self.xindexes, names)
4871 
4872         variables = {k: v for k, v in self._variables.items() if k not in names}
4873         coord_names = {k for k in self._coord_names if k in variables}
4874         indexes = {k: v for k, v in self._indexes.items() if k not in names}
4875         return self._replace_with_new_dims(
4876             variables, coord_names=coord_names, indexes=indexes
4877         )
4878 
4879     def drop(
4880         self: T_Dataset,
4881         labels=None,
4882         dim=None,
4883         *,
4884         errors: ErrorOptions = "raise",
4885         **labels_kwargs,
4886     ) -> T_Dataset:
4887         """Backward compatible method based on `drop_vars` and `drop_sel`
4888 
4889         Using either `drop_vars` or `drop_sel` is encouraged
4890 
4891         See Also
4892         --------
4893         Dataset.drop_vars
4894         Dataset.drop_sel
4895         """
4896         if errors not in ["raise", "ignore"]:
4897             raise ValueError('errors must be either "raise" or "ignore"')
4898 
4899         if is_dict_like(labels) and not isinstance(labels, dict):
4900             warnings.warn(
4901                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
4902                 FutureWarning,
4903                 stacklevel=2,
4904             )
4905             return self.drop_vars(labels, errors=errors)
4906 
4907         if labels_kwargs or isinstance(labels, dict):
4908             if dim is not None:
4909                 raise ValueError("cannot specify dim and dict-like arguments.")
4910             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
4911 
4912         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
4913             warnings.warn(
4914                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
4915                 PendingDeprecationWarning,
4916                 stacklevel=2,
4917             )
4918             return self.drop_vars(labels, errors=errors)
4919         if dim is not None:
4920             warnings.warn(
4921                 "dropping labels using list-like labels is deprecated; using "
4922                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
4923                 DeprecationWarning,
4924                 stacklevel=2,
4925             )
4926             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
4927 
4928         warnings.warn(
4929             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
4930             PendingDeprecationWarning,
4931             stacklevel=2,
4932         )
4933         return self.drop_sel(labels, errors=errors)
4934 
4935     def drop_sel(
4936         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
4937     ) -> T_Dataset:
4938         """Drop index labels from this dataset.
4939 
4940         Parameters
4941         ----------
4942         labels : mapping of hashable to Any
4943             Index labels to drop
4944         errors : {"raise", "ignore"}, default: "raise"
4945             If 'raise', raises a ValueError error if
4946             any of the index labels passed are not
4947             in the dataset. If 'ignore', any given labels that are in the
4948             dataset are dropped and no error is raised.
4949         **labels_kwargs : {dim: label, ...}, optional
4950             The keyword arguments form of ``dim`` and ``labels``
4951 
4952         Returns
4953         -------
4954         dropped : Dataset
4955 
4956         Examples
4957         --------
4958         >>> data = np.arange(6).reshape(2, 3)
4959         >>> labels = ["a", "b", "c"]
4960         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4961         >>> ds
4962         <xarray.Dataset>
4963         Dimensions:  (x: 2, y: 3)
4964         Coordinates:
4965           * y        (y) <U1 'a' 'b' 'c'
4966         Dimensions without coordinates: x
4967         Data variables:
4968             A        (x, y) int64 0 1 2 3 4 5
4969         >>> ds.drop_sel(y=["a", "c"])
4970         <xarray.Dataset>
4971         Dimensions:  (x: 2, y: 1)
4972         Coordinates:
4973           * y        (y) <U1 'b'
4974         Dimensions without coordinates: x
4975         Data variables:
4976             A        (x, y) int64 1 4
4977         >>> ds.drop_sel(y="b")
4978         <xarray.Dataset>
4979         Dimensions:  (x: 2, y: 2)
4980         Coordinates:
4981           * y        (y) <U1 'a' 'c'
4982         Dimensions without coordinates: x
4983         Data variables:
4984             A        (x, y) int64 0 2 3 5
4985         """
4986         if errors not in ["raise", "ignore"]:
4987             raise ValueError('errors must be either "raise" or "ignore"')
4988 
4989         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
4990 
4991         ds = self
4992         for dim, labels_for_dim in labels.items():
4993             # Don't cast to set, as it would harm performance when labels
4994             # is a large numpy array
4995             if utils.is_scalar(labels_for_dim):
4996                 labels_for_dim = [labels_for_dim]
4997             labels_for_dim = np.asarray(labels_for_dim)
4998             try:
4999                 index = self.get_index(dim)
5000             except KeyError:
5001                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5002             new_index = index.drop(labels_for_dim, errors=errors)
5003             ds = ds.loc[{dim: new_index}]
5004         return ds
5005 
5006     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5007         """Drop index positions from this Dataset.
5008 
5009         Parameters
5010         ----------
5011         indexers : mapping of hashable to Any
5012             Index locations to drop
5013         **indexers_kwargs : {dim: position, ...}, optional
5014             The keyword arguments form of ``dim`` and ``positions``
5015 
5016         Returns
5017         -------
5018         dropped : Dataset
5019 
5020         Raises
5021         ------
5022         IndexError
5023 
5024         Examples
5025         --------
5026         >>> data = np.arange(6).reshape(2, 3)
5027         >>> labels = ["a", "b", "c"]
5028         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5029         >>> ds
5030         <xarray.Dataset>
5031         Dimensions:  (x: 2, y: 3)
5032         Coordinates:
5033           * y        (y) <U1 'a' 'b' 'c'
5034         Dimensions without coordinates: x
5035         Data variables:
5036             A        (x, y) int64 0 1 2 3 4 5
5037         >>> ds.drop_isel(y=[0, 2])
5038         <xarray.Dataset>
5039         Dimensions:  (x: 2, y: 1)
5040         Coordinates:
5041           * y        (y) <U1 'b'
5042         Dimensions without coordinates: x
5043         Data variables:
5044             A        (x, y) int64 1 4
5045         >>> ds.drop_isel(y=1)
5046         <xarray.Dataset>
5047         Dimensions:  (x: 2, y: 2)
5048         Coordinates:
5049           * y        (y) <U1 'a' 'c'
5050         Dimensions without coordinates: x
5051         Data variables:
5052             A        (x, y) int64 0 2 3 5
5053         """
5054 
5055         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5056 
5057         ds = self
5058         dimension_index = {}
5059         for dim, pos_for_dim in indexers.items():
5060             # Don't cast to set, as it would harm performance when labels
5061             # is a large numpy array
5062             if utils.is_scalar(pos_for_dim):
5063                 pos_for_dim = [pos_for_dim]
5064             pos_for_dim = np.asarray(pos_for_dim)
5065             index = self.get_index(dim)
5066             new_index = index.delete(pos_for_dim)
5067             dimension_index[dim] = new_index
5068         ds = ds.loc[dimension_index]
5069         return ds
5070 
5071     def drop_dims(
5072         self: T_Dataset,
5073         drop_dims: Hashable | Iterable[Hashable],
5074         *,
5075         errors: ErrorOptions = "raise",
5076     ) -> T_Dataset:
5077         """Drop dimensions and associated variables from this dataset.
5078 
5079         Parameters
5080         ----------
5081         drop_dims : hashable or iterable of hashable
5082             Dimension or dimensions to drop.
5083         errors : {"raise", "ignore"}, default: "raise"
5084             If 'raise', raises a ValueError error if any of the
5085             dimensions passed are not in the dataset. If 'ignore', any given
5086             dimensions that are in the dataset are dropped and no error is raised.
5087 
5088         Returns
5089         -------
5090         obj : Dataset
5091             The dataset without the given dimensions (or any variables
5092             containing those dimensions).
5093         """
5094         if errors not in ["raise", "ignore"]:
5095             raise ValueError('errors must be either "raise" or "ignore"')
5096 
5097         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5098             drop_dims = {drop_dims}
5099         else:
5100             drop_dims = set(drop_dims)
5101 
5102         if errors == "raise":
5103             missing_dims = drop_dims - set(self.dims)
5104             if missing_dims:
5105                 raise ValueError(
5106                     f"Dataset does not contain the dimensions: {missing_dims}"
5107                 )
5108 
5109         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5110         return self.drop_vars(drop_vars)
5111 
5112     def transpose(
5113         self: T_Dataset,
5114         *dims: Hashable,
5115         missing_dims: ErrorOptionsWithWarn = "raise",
5116     ) -> T_Dataset:
5117         """Return a new Dataset object with all array dimensions transposed.
5118 
5119         Although the order of dimensions on each array will change, the dataset
5120         dimensions themselves will remain in fixed (sorted) order.
5121 
5122         Parameters
5123         ----------
5124         *dims : hashable, optional
5125             By default, reverse the dimensions on each array. Otherwise,
5126             reorder the dimensions to this order.
5127         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5128             What to do if dimensions that should be selected from are not present in the
5129             Dataset:
5130             - "raise": raise an exception
5131             - "warn": raise a warning, and ignore the missing dimensions
5132             - "ignore": ignore the missing dimensions
5133 
5134         Returns
5135         -------
5136         transposed : Dataset
5137             Each array in the dataset (including) coordinates will be
5138             transposed to the given order.
5139 
5140         Notes
5141         -----
5142         This operation returns a view of each array's data. It is
5143         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5144         -- the data will be fully loaded into memory.
5145 
5146         See Also
5147         --------
5148         numpy.transpose
5149         DataArray.transpose
5150         """
5151         # Use infix_dims to check once for missing dimensions
5152         if len(dims) != 0:
5153             _ = list(infix_dims(dims, self.dims, missing_dims))
5154 
5155         ds = self.copy()
5156         for name, var in self._variables.items():
5157             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5158             ds._variables[name] = var.transpose(*var_dims)
5159         return ds
5160 
5161     def dropna(
5162         self: T_Dataset,
5163         dim: Hashable,
5164         how: Literal["any", "all"] = "any",
5165         thresh: int | None = None,
5166         subset: Iterable[Hashable] | None = None,
5167     ) -> T_Dataset:
5168         """Returns a new dataset with dropped labels for missing values along
5169         the provided dimension.
5170 
5171         Parameters
5172         ----------
5173         dim : hashable
5174             Dimension along which to drop missing values. Dropping along
5175             multiple dimensions simultaneously is not yet supported.
5176         how : {"any", "all"}, default: "any"
5177             - any : if any NA values are present, drop that label
5178             - all : if all values are NA, drop that label
5179 
5180         thresh : int or None, optional
5181             If supplied, require this many non-NA values.
5182         subset : iterable of hashable or None, optional
5183             Which variables to check for missing values. By default, all
5184             variables in the dataset are checked.
5185 
5186         Returns
5187         -------
5188         Dataset
5189         """
5190         # TODO: consider supporting multiple dimensions? Or not, given that
5191         # there are some ugly edge cases, e.g., pandas's dropna differs
5192         # depending on the order of the supplied axes.
5193 
5194         if dim not in self.dims:
5195             raise ValueError(f"{dim} must be a single dataset dimension")
5196 
5197         if subset is None:
5198             subset = iter(self.data_vars)
5199 
5200         count = np.zeros(self.dims[dim], dtype=np.int64)
5201         size = np.int_(0)  # for type checking
5202 
5203         for k in subset:
5204             array = self._variables[k]
5205             if dim in array.dims:
5206                 dims = [d for d in array.dims if d != dim]
5207                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5208                 size += math.prod([self.dims[d] for d in dims])
5209 
5210         if thresh is not None:
5211             mask = count >= thresh
5212         elif how == "any":
5213             mask = count == size
5214         elif how == "all":
5215             mask = count > 0
5216         elif how is not None:
5217             raise ValueError(f"invalid how option: {how}")
5218         else:
5219             raise TypeError("must specify how or thresh")
5220 
5221         return self.isel({dim: mask})
5222 
5223     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5224         """Fill missing values in this object.
5225 
5226         This operation follows the normal broadcasting and alignment rules that
5227         xarray uses for binary arithmetic, except the result is aligned to this
5228         object (``join='left'``) instead of aligned to the intersection of
5229         index coordinates (``join='inner'``).
5230 
5231         Parameters
5232         ----------
5233         value : scalar, ndarray, DataArray, dict or Dataset
5234             Used to fill all matching missing values in this dataset's data
5235             variables. Scalars, ndarrays or DataArrays arguments are used to
5236             fill all data with aligned coordinates (for DataArrays).
5237             Dictionaries or datasets match data variables and then align
5238             coordinates if necessary.
5239 
5240         Returns
5241         -------
5242         Dataset
5243 
5244         Examples
5245         --------
5246         >>> ds = xr.Dataset(
5247         ...     {
5248         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5249         ...         "B": ("x", [3, 4, np.nan, 1]),
5250         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5251         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5252         ...     },
5253         ...     coords={"x": [0, 1, 2, 3]},
5254         ... )
5255         >>> ds
5256         <xarray.Dataset>
5257         Dimensions:  (x: 4)
5258         Coordinates:
5259           * x        (x) int64 0 1 2 3
5260         Data variables:
5261             A        (x) float64 nan 2.0 nan 0.0
5262             B        (x) float64 3.0 4.0 nan 1.0
5263             C        (x) float64 nan nan nan 5.0
5264             D        (x) float64 nan 3.0 nan 4.0
5265 
5266         Replace all `NaN` values with 0s.
5267 
5268         >>> ds.fillna(0)
5269         <xarray.Dataset>
5270         Dimensions:  (x: 4)
5271         Coordinates:
5272           * x        (x) int64 0 1 2 3
5273         Data variables:
5274             A        (x) float64 0.0 2.0 0.0 0.0
5275             B        (x) float64 3.0 4.0 0.0 1.0
5276             C        (x) float64 0.0 0.0 0.0 5.0
5277             D        (x) float64 0.0 3.0 0.0 4.0
5278 
5279         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
5280 
5281         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5282         >>> ds.fillna(value=values)
5283         <xarray.Dataset>
5284         Dimensions:  (x: 4)
5285         Coordinates:
5286           * x        (x) int64 0 1 2 3
5287         Data variables:
5288             A        (x) float64 0.0 2.0 0.0 0.0
5289             B        (x) float64 3.0 4.0 1.0 1.0
5290             C        (x) float64 2.0 2.0 2.0 5.0
5291             D        (x) float64 3.0 3.0 3.0 4.0
5292         """
5293         if utils.is_dict_like(value):
5294             value_keys = getattr(value, "data_vars", value).keys()
5295             if not set(value_keys) <= set(self.data_vars.keys()):
5296                 raise ValueError(
5297                     "all variables in the argument to `fillna` "
5298                     "must be contained in the original dataset"
5299                 )
5300         out = ops.fillna(self, value)
5301         return out
5302 
5303     def interpolate_na(
5304         self: T_Dataset,
5305         dim: Hashable | None = None,
5306         method: InterpOptions = "linear",
5307         limit: int = None,
5308         use_coordinate: bool | Hashable = True,
5309         max_gap: (
5310             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5311         ) = None,
5312         **kwargs: Any,
5313     ) -> T_Dataset:
5314         """Fill in NaNs by interpolating according to different methods.
5315 
5316         Parameters
5317         ----------
5318         dim : Hashable or None, optional
5319             Specifies the dimension along which to interpolate.
5320         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5321             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5322             String indicating which method to use for interpolation:
5323 
5324             - 'linear': linear interpolation. Additional keyword
5325               arguments are passed to :py:func:`numpy.interp`
5326             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5327               are passed to :py:func:`scipy.interpolate.interp1d`. If
5328               ``method='polynomial'``, the ``order`` keyword argument must also be
5329               provided.
5330             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5331               respective :py:class:`scipy.interpolate` classes.
5332 
5333         use_coordinate : bool or Hashable, default: True
5334             Specifies which index to use as the x values in the interpolation
5335             formulated as `y = f(x)`. If False, values are treated as if
5336             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5337             used. If ``use_coordinate`` is a string, it specifies the name of a
5338             coordinate variariable to use as the index.
5339         limit : int, default: None
5340             Maximum number of consecutive NaNs to fill. Must be greater than 0
5341             or None for no limit. This filling is done regardless of the size of
5342             the gap in the data. To only interpolate over gaps less than a given length,
5343             see ``max_gap``.
5344         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5345             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5346             Use None for no limit. When interpolating along a datetime64 dimension
5347             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5348 
5349             - a string that is valid input for pandas.to_timedelta
5350             - a :py:class:`numpy.timedelta64` object
5351             - a :py:class:`pandas.Timedelta` object
5352             - a :py:class:`datetime.timedelta` object
5353 
5354             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5355             dimensions has not been implemented yet. Gap length is defined as the difference
5356             between coordinate values at the first data point after a gap and the last value
5357             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5358             between coordinate values at the first (last) valid data point and the first (last) NaN.
5359             For example, consider::
5360 
5361                 <xarray.DataArray (x: 9)>
5362                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5363                 Coordinates:
5364                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5365 
5366             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5367         **kwargs : dict, optional
5368             parameters passed verbatim to the underlying interpolation function
5369 
5370         Returns
5371         -------
5372         interpolated: Dataset
5373             Filled in Dataset.
5374 
5375         See Also
5376         --------
5377         numpy.interp
5378         scipy.interpolate
5379 
5380         Examples
5381         --------
5382         >>> ds = xr.Dataset(
5383         ...     {
5384         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5385         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5386         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5387         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5388         ...     },
5389         ...     coords={"x": [0, 1, 2, 3, 4]},
5390         ... )
5391         >>> ds
5392         <xarray.Dataset>
5393         Dimensions:  (x: 5)
5394         Coordinates:
5395           * x        (x) int64 0 1 2 3 4
5396         Data variables:
5397             A        (x) float64 nan 2.0 3.0 nan 0.0
5398             B        (x) float64 3.0 4.0 nan 1.0 7.0
5399             C        (x) float64 nan nan nan 5.0 0.0
5400             D        (x) float64 nan 3.0 nan -1.0 4.0
5401 
5402         >>> ds.interpolate_na(dim="x", method="linear")
5403         <xarray.Dataset>
5404         Dimensions:  (x: 5)
5405         Coordinates:
5406           * x        (x) int64 0 1 2 3 4
5407         Data variables:
5408             A        (x) float64 nan 2.0 3.0 1.5 0.0
5409             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5410             C        (x) float64 nan nan nan 5.0 0.0
5411             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5412 
5413         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5414         <xarray.Dataset>
5415         Dimensions:  (x: 5)
5416         Coordinates:
5417           * x        (x) int64 0 1 2 3 4
5418         Data variables:
5419             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5420             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5421             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5422             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5423         """
5424         from .missing import _apply_over_vars_with_dim, interp_na
5425 
5426         new = _apply_over_vars_with_dim(
5427             interp_na,
5428             self,
5429             dim=dim,
5430             method=method,
5431             limit=limit,
5432             use_coordinate=use_coordinate,
5433             max_gap=max_gap,
5434             **kwargs,
5435         )
5436         return new
5437 
5438     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5439         """Fill NaN values by propagating values forward
5440 
5441         *Requires bottleneck.*
5442 
5443         Parameters
5444         ----------
5445         dim : Hashable
5446             Specifies the dimension along which to propagate values when
5447             filling.
5448         limit : int or None, optional
5449             The maximum number of consecutive NaN values to forward fill. In
5450             other words, if there is a gap with more than this number of
5451             consecutive NaNs, it will only be partially filled. Must be greater
5452             than 0 or None for no limit. Must be None or greater than or equal
5453             to axis length if filling along chunked axes (dimensions).
5454 
5455         Returns
5456         -------
5457         Dataset
5458         """
5459         from .missing import _apply_over_vars_with_dim, ffill
5460 
5461         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5462         return new
5463 
5464     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5465         """Fill NaN values by propagating values backward
5466 
5467         *Requires bottleneck.*
5468 
5469         Parameters
5470         ----------
5471         dim : Hashable
5472             Specifies the dimension along which to propagate values when
5473             filling.
5474         limit : int or None, optional
5475             The maximum number of consecutive NaN values to backward fill. In
5476             other words, if there is a gap with more than this number of
5477             consecutive NaNs, it will only be partially filled. Must be greater
5478             than 0 or None for no limit. Must be None or greater than or equal
5479             to axis length if filling along chunked axes (dimensions).
5480 
5481         Returns
5482         -------
5483         Dataset
5484         """
5485         from .missing import _apply_over_vars_with_dim, bfill
5486 
5487         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5488         return new
5489 
5490     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5491         """Combine two Datasets, default to data_vars of self.
5492 
5493         The new coordinates follow the normal broadcasting and alignment rules
5494         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5495         filled with np.nan.
5496 
5497         Parameters
5498         ----------
5499         other : Dataset
5500             Used to fill all matching missing values in this array.
5501 
5502         Returns
5503         -------
5504         Dataset
5505         """
5506         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5507         return out
5508 
5509     def reduce(
5510         self: T_Dataset,
5511         func: Callable,
5512         dim: Hashable | Iterable[Hashable] = None,
5513         *,
5514         keep_attrs: bool | None = None,
5515         keepdims: bool = False,
5516         numeric_only: bool = False,
5517         **kwargs: Any,
5518     ) -> T_Dataset:
5519         """Reduce this dataset by applying `func` along some dimension(s).
5520 
5521         Parameters
5522         ----------
5523         func : callable
5524             Function which can be called in the form
5525             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5526             np.ndarray over an integer valued axis.
5527         dim : str or sequence of str, optional
5528             Dimension(s) over which to apply `func`.  By default `func` is
5529             applied over all dimensions.
5530         keep_attrs : bool or None, optional
5531             If True, the dataset's attributes (`attrs`) will be copied from
5532             the original object to the new one.  If False (default), the new
5533             object will be returned without attributes.
5534         keepdims : bool, default: False
5535             If True, the dimensions which are reduced are left in the result
5536             as dimensions of size one. Coordinates that use these dimensions
5537             are removed.
5538         numeric_only : bool, default: False
5539             If True, only apply ``func`` to variables with a numeric dtype.
5540         **kwargs : Any
5541             Additional keyword arguments passed on to ``func``.
5542 
5543         Returns
5544         -------
5545         reduced : Dataset
5546             Dataset with this object's DataArrays replaced with new DataArrays
5547             of summarized data and the indicated dimension(s) removed.
5548         """
5549         if kwargs.get("axis", None) is not None:
5550             raise ValueError(
5551                 "passing 'axis' to Dataset reduce methods is ambiguous."
5552                 " Please use 'dim' instead."
5553             )
5554 
5555         if dim is None or dim is ...:
5556             dims = set(self.dims)
5557         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5558             dims = {dim}
5559         else:
5560             dims = set(dim)
5561 
5562         missing_dimensions = [d for d in dims if d not in self.dims]
5563         if missing_dimensions:
5564             raise ValueError(
5565                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5566             )
5567 
5568         if keep_attrs is None:
5569             keep_attrs = _get_keep_attrs(default=False)
5570 
5571         variables: dict[Hashable, Variable] = {}
5572         for name, var in self._variables.items():
5573             reduce_dims = [d for d in var.dims if d in dims]
5574             if name in self.coords:
5575                 if not reduce_dims:
5576                     variables[name] = var
5577             else:
5578                 if (
5579                     # Some reduction functions (e.g. std, var) need to run on variables
5580                     # that don't have the reduce dims: PR5393
5581                     not reduce_dims
5582                     or not numeric_only
5583                     or np.issubdtype(var.dtype, np.number)
5584                     or (var.dtype == np.bool_)
5585                 ):
5586                     reduce_maybe_single: Hashable | None | list[Hashable]
5587                     if len(reduce_dims) == 1:
5588                         # unpack dimensions for the benefit of functions
5589                         # like np.argmin which can't handle tuple arguments
5590                         (reduce_maybe_single,) = reduce_dims
5591                     elif len(reduce_dims) == var.ndim:
5592                         # prefer to aggregate over axis=None rather than
5593                         # axis=(0, 1) if they will be equivalent, because
5594                         # the former is often more efficient
5595                         reduce_maybe_single = None
5596                     else:
5597                         reduce_maybe_single = reduce_dims
5598                     variables[name] = var.reduce(
5599                         func,
5600                         dim=reduce_maybe_single,
5601                         keep_attrs=keep_attrs,
5602                         keepdims=keepdims,
5603                         **kwargs,
5604                     )
5605 
5606         coord_names = {k for k in self.coords if k in variables}
5607         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5608         attrs = self.attrs if keep_attrs else None
5609         return self._replace_with_new_dims(
5610             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5611         )
5612 
5613     def map(
5614         self: T_Dataset,
5615         func: Callable,
5616         keep_attrs: bool | None = None,
5617         args: Iterable[Any] = (),
5618         **kwargs: Any,
5619     ) -> T_Dataset:
5620         """Apply a function to each data variable in this dataset
5621 
5622         Parameters
5623         ----------
5624         func : callable
5625             Function which can be called in the form `func(x, *args, **kwargs)`
5626             to transform each DataArray `x` in this dataset into another
5627             DataArray.
5628         keep_attrs : bool or None, optional
5629             If True, both the dataset's and variables' attributes (`attrs`) will be
5630             copied from the original objects to the new ones. If False, the new dataset
5631             and variables will be returned without copying the attributes.
5632         args : iterable, optional
5633             Positional arguments passed on to `func`.
5634         **kwargs : Any
5635             Keyword arguments passed on to `func`.
5636 
5637         Returns
5638         -------
5639         applied : Dataset
5640             Resulting dataset from applying ``func`` to each data variable.
5641 
5642         Examples
5643         --------
5644         >>> da = xr.DataArray(np.random.randn(2, 3))
5645         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5646         >>> ds
5647         <xarray.Dataset>
5648         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5649         Dimensions without coordinates: dim_0, dim_1, x
5650         Data variables:
5651             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5652             bar      (x) int64 -1 2
5653         >>> ds.map(np.fabs)
5654         <xarray.Dataset>
5655         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5656         Dimensions without coordinates: dim_0, dim_1, x
5657         Data variables:
5658             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5659             bar      (x) float64 1.0 2.0
5660         """
5661         if keep_attrs is None:
5662             keep_attrs = _get_keep_attrs(default=False)
5663         variables = {
5664             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5665             for k, v in self.data_vars.items()
5666         }
5667         if keep_attrs:
5668             for k, v in variables.items():
5669                 v._copy_attrs_from(self.data_vars[k])
5670         attrs = self.attrs if keep_attrs else None
5671         return type(self)(variables, attrs=attrs)
5672 
5673     def apply(
5674         self: T_Dataset,
5675         func: Callable,
5676         keep_attrs: bool | None = None,
5677         args: Iterable[Any] = (),
5678         **kwargs: Any,
5679     ) -> T_Dataset:
5680         """
5681         Backward compatible implementation of ``map``
5682 
5683         See Also
5684         --------
5685         Dataset.map
5686         """
5687         warnings.warn(
5688             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5689             PendingDeprecationWarning,
5690             stacklevel=2,
5691         )
5692         return self.map(func, keep_attrs, args, **kwargs)
5693 
5694     def assign(
5695         self: T_Dataset,
5696         variables: Mapping[Any, Any] | None = None,
5697         **variables_kwargs: Any,
5698     ) -> T_Dataset:
5699         """Assign new data variables to a Dataset, returning a new object
5700         with all the original variables in addition to the new ones.
5701 
5702         Parameters
5703         ----------
5704         variables : mapping of hashable to Any
5705             Mapping from variables names to the new values. If the new values
5706             are callable, they are computed on the Dataset and assigned to new
5707             data variables. If the values are not callable, (e.g. a DataArray,
5708             scalar, or array), they are simply assigned.
5709         **variables_kwargs
5710             The keyword arguments form of ``variables``.
5711             One of variables or variables_kwargs must be provided.
5712 
5713         Returns
5714         -------
5715         ds : Dataset
5716             A new Dataset with the new variables in addition to all the
5717             existing variables.
5718 
5719         Notes
5720         -----
5721         Since ``kwargs`` is a dictionary, the order of your arguments may not
5722         be preserved, and so the order of the new variables is not well
5723         defined. Assigning multiple variables within the same ``assign`` is
5724         possible, but you cannot reference other variables created within the
5725         same ``assign`` call.
5726 
5727         See Also
5728         --------
5729         pandas.DataFrame.assign
5730 
5731         Examples
5732         --------
5733         >>> x = xr.Dataset(
5734         ...     {
5735         ...         "temperature_c": (
5736         ...             ("lat", "lon"),
5737         ...             20 * np.random.rand(4).reshape(2, 2),
5738         ...         ),
5739         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5740         ...     },
5741         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5742         ... )
5743         >>> x
5744         <xarray.Dataset>
5745         Dimensions:        (lat: 2, lon: 2)
5746         Coordinates:
5747           * lat            (lat) int64 10 20
5748           * lon            (lon) int64 150 160
5749         Data variables:
5750             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
5751             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
5752 
5753         Where the value is a callable, evaluated on dataset:
5754 
5755         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
5756         <xarray.Dataset>
5757         Dimensions:        (lat: 2, lon: 2)
5758         Coordinates:
5759           * lat            (lat) int64 10 20
5760           * lon            (lon) int64 150 160
5761         Data variables:
5762             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
5763             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
5764             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
5765 
5766         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
5767 
5768         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
5769         <xarray.Dataset>
5770         Dimensions:        (lat: 2, lon: 2)
5771         Coordinates:
5772           * lat            (lat) int64 10 20
5773           * lon            (lon) int64 150 160
5774         Data variables:
5775             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
5776             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
5777             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
5778 
5779         """
5780         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
5781         data = self.copy()
5782         # do all calculations first...
5783         results: CoercibleMapping = data._calc_assign_results(variables)
5784         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
5785         # ... and then assign
5786         data.update(results)
5787         return data
5788 
5789     def to_array(
5790         self, dim: Hashable = "variable", name: Hashable | None = None
5791     ) -> DataArray:
5792         """Convert this dataset into an xarray.DataArray
5793 
5794         The data variables of this dataset will be broadcast against each other
5795         and stacked along the first axis of the new array. All coordinates of
5796         this dataset will remain coordinates.
5797 
5798         Parameters
5799         ----------
5800         dim : Hashable, default: "variable"
5801             Name of the new dimension.
5802         name : Hashable or None, optional
5803             Name of the new data array.
5804 
5805         Returns
5806         -------
5807         array : xarray.DataArray
5808         """
5809         from .dataarray import DataArray
5810 
5811         data_vars = [self.variables[k] for k in self.data_vars]
5812         broadcast_vars = broadcast_variables(*data_vars)
5813         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
5814 
5815         dims = (dim,) + broadcast_vars[0].dims
5816         variable = Variable(dims, data, self.attrs, fastpath=True)
5817 
5818         coords = {k: v.variable for k, v in self.coords.items()}
5819         indexes = filter_indexes_from_coords(self._indexes, set(coords))
5820         new_dim_index = PandasIndex(list(self.data_vars), dim)
5821         indexes[dim] = new_dim_index
5822         coords.update(new_dim_index.create_variables())
5823 
5824         return DataArray._construct_direct(variable, coords, name, indexes)
5825 
5826     def _normalize_dim_order(
5827         self, dim_order: Sequence[Hashable] | None = None
5828     ) -> dict[Hashable, int]:
5829         """
5830         Check the validity of the provided dimensions if any and return the mapping
5831         between dimension name and their size.
5832 
5833         Parameters
5834         ----------
5835         dim_order: Sequence of Hashable or None, optional
5836             Dimension order to validate (default to the alphabetical order if None).
5837 
5838         Returns
5839         -------
5840         result : dict[Hashable, int]
5841             Validated dimensions mapping.
5842 
5843         """
5844         if dim_order is None:
5845             dim_order = list(self.dims)
5846         elif set(dim_order) != set(self.dims):
5847             raise ValueError(
5848                 "dim_order {} does not match the set of dimensions of this "
5849                 "Dataset: {}".format(dim_order, list(self.dims))
5850             )
5851 
5852         ordered_dims = {k: self.dims[k] for k in dim_order}
5853 
5854         return ordered_dims
5855 
5856     def to_pandas(self) -> pd.Series | pd.DataFrame:
5857         """Convert this dataset into a pandas object without changing the number of dimensions.
5858 
5859         The type of the returned object depends on the number of Dataset
5860         dimensions:
5861 
5862         * 0D -> `pandas.Series`
5863         * 1D -> `pandas.DataFrame`
5864 
5865         Only works for Datasets with 1 or fewer dimensions.
5866         """
5867         if len(self.dims) == 0:
5868             return pd.Series({k: v.item() for k, v in self.items()})
5869         if len(self.dims) == 1:
5870             return self.to_dataframe()
5871         raise ValueError(
5872             "cannot convert Datasets with %s dimensions into "
5873             "pandas objects without changing the number of dimensions. "
5874             "Please use Dataset.to_dataframe() instead." % len(self.dims)
5875         )
5876 
5877     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
5878         columns = [k for k in self.variables if k not in self.dims]
5879         data = [
5880             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
5881             for k in columns
5882         ]
5883         index = self.coords.to_index([*ordered_dims])
5884         return pd.DataFrame(dict(zip(columns, data)), index=index)
5885 
5886     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
5887         """Convert this dataset into a pandas.DataFrame.
5888 
5889         Non-index variables in this dataset form the columns of the
5890         DataFrame. The DataFrame is indexed by the Cartesian product of
5891         this dataset's indices.
5892 
5893         Parameters
5894         ----------
5895         dim_order: Sequence of Hashable or None, optional
5896             Hierarchical dimension order for the resulting dataframe. All
5897             arrays are transposed to this order and then written out as flat
5898             vectors in contiguous order, so the last dimension in this list
5899             will be contiguous in the resulting DataFrame. This has a major
5900             influence on which operations are efficient on the resulting
5901             dataframe.
5902 
5903             If provided, must include all dimensions of this dataset. By
5904             default, dimensions are sorted alphabetically.
5905 
5906         Returns
5907         -------
5908         result : DataFrame
5909             Dataset as a pandas DataFrame.
5910 
5911         """
5912 
5913         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
5914 
5915         return self._to_dataframe(ordered_dims=ordered_dims)
5916 
5917     def _set_sparse_data_from_dataframe(
5918         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
5919     ) -> None:
5920         from sparse import COO
5921 
5922         if isinstance(idx, pd.MultiIndex):
5923             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
5924             is_sorted = idx.is_monotonic_increasing
5925             shape = tuple(lev.size for lev in idx.levels)
5926         else:
5927             coords = np.arange(idx.size).reshape(1, -1)
5928             is_sorted = True
5929             shape = (idx.size,)
5930 
5931         for name, values in arrays:
5932             # In virtually all real use cases, the sparse array will now have
5933             # missing values and needs a fill_value. For consistency, don't
5934             # special case the rare exceptions (e.g., dtype=int without a
5935             # MultiIndex).
5936             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
5937             values = np.asarray(values, dtype=dtype)
5938 
5939             data = COO(
5940                 coords,
5941                 values,
5942                 shape,
5943                 has_duplicates=False,
5944                 sorted=is_sorted,
5945                 fill_value=fill_value,
5946             )
5947             self[name] = (dims, data)
5948 
5949     def _set_numpy_data_from_dataframe(
5950         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
5951     ) -> None:
5952         if not isinstance(idx, pd.MultiIndex):
5953             for name, values in arrays:
5954                 self[name] = (dims, values)
5955             return
5956 
5957         # NB: similar, more general logic, now exists in
5958         # variable.unstack_once; we could consider combining them at some
5959         # point.
5960 
5961         shape = tuple(lev.size for lev in idx.levels)
5962         indexer = tuple(idx.codes)
5963 
5964         # We already verified that the MultiIndex has all unique values, so
5965         # there are missing values if and only if the size of output arrays is
5966         # larger that the index.
5967         missing_values = math.prod(shape) > idx.shape[0]
5968 
5969         for name, values in arrays:
5970             # NumPy indexing is much faster than using DataFrame.reindex() to
5971             # fill in missing values:
5972             # https://stackoverflow.com/a/35049899/809705
5973             if missing_values:
5974                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
5975                 data = np.full(shape, fill_value, dtype)
5976             else:
5977                 # If there are no missing values, keep the existing dtype
5978                 # instead of promoting to support NA, e.g., keep integer
5979                 # columns as integers.
5980                 # TODO: consider removing this special case, which doesn't
5981                 # exist for sparse=True.
5982                 data = np.zeros(shape, values.dtype)
5983             data[indexer] = values
5984             self[name] = (dims, data)
5985 
5986     @classmethod
5987     def from_dataframe(
5988         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
5989     ) -> T_Dataset:
5990         """Convert a pandas.DataFrame into an xarray.Dataset
5991 
5992         Each column will be converted into an independent variable in the
5993         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
5994         into a tensor product of one-dimensional indices (filling in missing
5995         values with NaN). This method will produce a Dataset very similar to
5996         that on which the 'to_dataframe' method was called, except with
5997         possibly redundant dimensions (since all dataset variables will have
5998         the same dimensionality)
5999 
6000         Parameters
6001         ----------
6002         dataframe : DataFrame
6003             DataFrame from which to copy data and indices.
6004         sparse : bool, default: False
6005             If true, create a sparse arrays instead of dense numpy arrays. This
6006             can potentially save a large amount of memory if the DataFrame has
6007             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6008 
6009         Returns
6010         -------
6011         New Dataset.
6012 
6013         See Also
6014         --------
6015         xarray.DataArray.from_series
6016         pandas.DataFrame.to_xarray
6017         """
6018         # TODO: Add an option to remove dimensions along which the variables
6019         # are constant, to enable consistent serialization to/from a dataframe,
6020         # even if some variables have different dimensionality.
6021 
6022         if not dataframe.columns.is_unique:
6023             raise ValueError("cannot convert DataFrame with non-unique columns")
6024 
6025         idx = remove_unused_levels_categories(dataframe.index)
6026 
6027         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6028             raise ValueError(
6029                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6030             )
6031 
6032         # Cast to a NumPy array first, in case the Series is a pandas Extension
6033         # array (which doesn't have a valid NumPy dtype)
6034         # TODO: allow users to control how this casting happens, e.g., by
6035         # forwarding arguments to pandas.Series.to_numpy?
6036         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6037 
6038         indexes: dict[Hashable, Index] = {}
6039         index_vars: dict[Hashable, Variable] = {}
6040 
6041         if isinstance(idx, pd.MultiIndex):
6042             dims = tuple(
6043                 name if name is not None else "level_%i" % n
6044                 for n, name in enumerate(idx.names)
6045             )
6046             for dim, lev in zip(dims, idx.levels):
6047                 xr_idx = PandasIndex(lev, dim)
6048                 indexes[dim] = xr_idx
6049                 index_vars.update(xr_idx.create_variables())
6050         else:
6051             index_name = idx.name if idx.name is not None else "index"
6052             dims = (index_name,)
6053             xr_idx = PandasIndex(idx, index_name)
6054             indexes[index_name] = xr_idx
6055             index_vars.update(xr_idx.create_variables())
6056 
6057         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6058 
6059         if sparse:
6060             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6061         else:
6062             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6063         return obj
6064 
6065     def to_dask_dataframe(
6066         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6067     ) -> DaskDataFrame:
6068         """
6069         Convert this dataset into a dask.dataframe.DataFrame.
6070 
6071         The dimensions, coordinates and data variables in this dataset form
6072         the columns of the DataFrame.
6073 
6074         Parameters
6075         ----------
6076         dim_order : list, optional
6077             Hierarchical dimension order for the resulting dataframe. All
6078             arrays are transposed to this order and then written out as flat
6079             vectors in contiguous order, so the last dimension in this list
6080             will be contiguous in the resulting DataFrame. This has a major
6081             influence on which operations are efficient on the resulting dask
6082             dataframe.
6083 
6084             If provided, must include all dimensions of this dataset. By
6085             default, dimensions are sorted alphabetically.
6086         set_index : bool, default: False
6087             If set_index=True, the dask DataFrame is indexed by this dataset's
6088             coordinate. Since dask DataFrames do not support multi-indexes,
6089             set_index only works if the dataset only contains one dimension.
6090 
6091         Returns
6092         -------
6093         dask.dataframe.DataFrame
6094         """
6095 
6096         import dask.array as da
6097         import dask.dataframe as dd
6098 
6099         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6100 
6101         columns = list(ordered_dims)
6102         columns.extend(k for k in self.coords if k not in self.dims)
6103         columns.extend(self.data_vars)
6104 
6105         series_list = []
6106         for name in columns:
6107             try:
6108                 var = self.variables[name]
6109             except KeyError:
6110                 # dimension without a matching coordinate
6111                 size = self.dims[name]
6112                 data = da.arange(size, chunks=size, dtype=np.int64)
6113                 var = Variable((name,), data)
6114 
6115             # IndexVariable objects have a dummy .chunk() method
6116             if isinstance(var, IndexVariable):
6117                 var = var.to_base_variable()
6118 
6119             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6120             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6121             series_list.append(series)
6122 
6123         df = dd.concat(series_list, axis=1)
6124 
6125         if set_index:
6126             dim_order = [*ordered_dims]
6127 
6128             if len(dim_order) == 1:
6129                 (dim,) = dim_order
6130                 df = df.set_index(dim)
6131             else:
6132                 # triggers an error about multi-indexes, even if only one
6133                 # dimension is passed
6134                 df = df.set_index(dim_order)
6135 
6136         return df
6137 
6138     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6139         """
6140         Convert this dataset to a dictionary following xarray naming
6141         conventions.
6142 
6143         Converts all variables and attributes to native Python objects
6144         Useful for converting to json. To avoid datetime incompatibility
6145         use decode_times=False kwarg in xarrray.open_dataset.
6146 
6147         Parameters
6148         ----------
6149         data : bool, default: True
6150             Whether to include the actual data in the dictionary. When set to
6151             False, returns just the schema.
6152         encoding : bool, default: False
6153             Whether to include the Dataset's encoding in the dictionary.
6154 
6155         Returns
6156         -------
6157         d : dict
6158             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6159             "encoding".
6160 
6161         See Also
6162         --------
6163         Dataset.from_dict
6164         DataArray.to_dict
6165         """
6166         d: dict = {
6167             "coords": {},
6168             "attrs": decode_numpy_dict_values(self.attrs),
6169             "dims": dict(self.dims),
6170             "data_vars": {},
6171         }
6172         for k in self.coords:
6173             d["coords"].update(
6174                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6175             )
6176         for k in self.data_vars:
6177             d["data_vars"].update(
6178                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6179             )
6180         if encoding:
6181             d["encoding"] = dict(self.encoding)
6182         return d
6183 
6184     @classmethod
6185     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6186         """Convert a dictionary into an xarray.Dataset.
6187 
6188         Parameters
6189         ----------
6190         d : dict-like
6191             Mapping with a minimum structure of
6192                 ``{"var_0": {"dims": [..], "data": [..]}, \
6193                             ...}``
6194 
6195         Returns
6196         -------
6197         obj : Dataset
6198 
6199         See also
6200         --------
6201         Dataset.to_dict
6202         DataArray.from_dict
6203 
6204         Examples
6205         --------
6206         >>> d = {
6207         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6208         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6209         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6210         ... }
6211         >>> ds = xr.Dataset.from_dict(d)
6212         >>> ds
6213         <xarray.Dataset>
6214         Dimensions:  (t: 3)
6215         Coordinates:
6216           * t        (t) int64 0 1 2
6217         Data variables:
6218             a        (t) <U1 'a' 'b' 'c'
6219             b        (t) int64 10 20 30
6220 
6221         >>> d = {
6222         ...     "coords": {
6223         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6224         ...     },
6225         ...     "attrs": {"title": "air temperature"},
6226         ...     "dims": "t",
6227         ...     "data_vars": {
6228         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6229         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6230         ...     },
6231         ... }
6232         >>> ds = xr.Dataset.from_dict(d)
6233         >>> ds
6234         <xarray.Dataset>
6235         Dimensions:  (t: 3)
6236         Coordinates:
6237           * t        (t) int64 0 1 2
6238         Data variables:
6239             a        (t) int64 10 20 30
6240             b        (t) <U1 'a' 'b' 'c'
6241         Attributes:
6242             title:    air temperature
6243 
6244         """
6245 
6246         variables: Iterable[tuple[Hashable, Any]]
6247         if not {"coords", "data_vars"}.issubset(set(d)):
6248             variables = d.items()
6249         else:
6250             import itertools
6251 
6252             variables = itertools.chain(
6253                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6254             )
6255         try:
6256             variable_dict = {
6257                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6258             }
6259         except KeyError as e:
6260             raise ValueError(
6261                 "cannot convert dict without the key "
6262                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6263             )
6264         obj = cls(variable_dict)
6265 
6266         # what if coords aren't dims?
6267         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6268         obj = obj.set_coords(coords)
6269 
6270         obj.attrs.update(d.get("attrs", {}))
6271         obj.encoding.update(d.get("encoding", {}))
6272 
6273         return obj
6274 
6275     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6276         variables = {}
6277         keep_attrs = kwargs.pop("keep_attrs", None)
6278         if keep_attrs is None:
6279             keep_attrs = _get_keep_attrs(default=True)
6280         for k, v in self._variables.items():
6281             if k in self._coord_names:
6282                 variables[k] = v
6283             else:
6284                 variables[k] = f(v, *args, **kwargs)
6285                 if keep_attrs:
6286                     variables[k].attrs = v._attrs
6287         attrs = self._attrs if keep_attrs else None
6288         return self._replace_with_new_dims(variables, attrs=attrs)
6289 
6290     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6291         from .dataarray import DataArray
6292         from .groupby import GroupBy
6293 
6294         if isinstance(other, GroupBy):
6295             return NotImplemented
6296         align_type = OPTIONS["arithmetic_join"] if join is None else join
6297         if isinstance(other, (DataArray, Dataset)):
6298             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6299         g = f if not reflexive else lambda x, y: f(y, x)
6300         ds = self._calculate_binary_op(g, other, join=align_type)
6301         return ds
6302 
6303     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6304         from .dataarray import DataArray
6305         from .groupby import GroupBy
6306 
6307         if isinstance(other, GroupBy):
6308             raise TypeError(
6309                 "in-place operations between a Dataset and "
6310                 "a grouped object are not permitted"
6311             )
6312         # we don't actually modify arrays in-place with in-place Dataset
6313         # arithmetic -- this lets us automatically align things
6314         if isinstance(other, (DataArray, Dataset)):
6315             other = other.reindex_like(self, copy=False)
6316         g = ops.inplace_to_noninplace_op(f)
6317         ds = self._calculate_binary_op(g, other, inplace=True)
6318         self._replace_with_new_dims(
6319             ds._variables,
6320             ds._coord_names,
6321             attrs=ds._attrs,
6322             indexes=ds._indexes,
6323             inplace=True,
6324         )
6325         return self
6326 
6327     def _calculate_binary_op(
6328         self, f, other, join="inner", inplace: bool = False
6329     ) -> Dataset:
6330         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6331             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6332                 raise ValueError(
6333                     "datasets must have the same data variables "
6334                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6335                 )
6336 
6337             dest_vars = {}
6338 
6339             for k in lhs_data_vars:
6340                 if k in rhs_data_vars:
6341                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6342                 elif join in ["left", "outer"]:
6343                     dest_vars[k] = f(lhs_vars[k], np.nan)
6344             for k in rhs_data_vars:
6345                 if k not in dest_vars and join in ["right", "outer"]:
6346                     dest_vars[k] = f(rhs_vars[k], np.nan)
6347             return dest_vars
6348 
6349         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6350             # can't use our shortcut of doing the binary operation with
6351             # Variable objects, so apply over our data vars instead.
6352             new_data_vars = apply_over_both(
6353                 self.data_vars, other, self.data_vars, other
6354             )
6355             return type(self)(new_data_vars)
6356 
6357         other_coords: Coordinates | None = getattr(other, "coords", None)
6358         ds = self.coords.merge(other_coords)
6359 
6360         if isinstance(other, Dataset):
6361             new_vars = apply_over_both(
6362                 self.data_vars, other.data_vars, self.variables, other.variables
6363             )
6364         else:
6365             other_variable = getattr(other, "variable", other)
6366             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6367         ds._variables.update(new_vars)
6368         ds._dims = calculate_dimensions(ds._variables)
6369         return ds
6370 
6371     def _copy_attrs_from(self, other):
6372         self.attrs = other.attrs
6373         for v in other.variables:
6374             if v in self.variables:
6375                 self.variables[v].attrs = other.variables[v].attrs
6376 
6377     def diff(
6378         self: T_Dataset,
6379         dim: Hashable,
6380         n: int = 1,
6381         label: Literal["upper", "lower"] = "upper",
6382     ) -> T_Dataset:
6383         """Calculate the n-th order discrete difference along given axis.
6384 
6385         Parameters
6386         ----------
6387         dim : Hashable
6388             Dimension over which to calculate the finite difference.
6389         n : int, default: 1
6390             The number of times values are differenced.
6391         label : {"upper", "lower"}, default: "upper"
6392             The new coordinate in dimension ``dim`` will have the
6393             values of either the minuend's or subtrahend's coordinate
6394             for values 'upper' and 'lower', respectively.
6395 
6396         Returns
6397         -------
6398         difference : Dataset
6399             The n-th order finite difference of this object.
6400 
6401         Notes
6402         -----
6403         `n` matches numpy's behavior and is different from pandas' first argument named
6404         `periods`.
6405 
6406         Examples
6407         --------
6408         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6409         >>> ds.diff("x")
6410         <xarray.Dataset>
6411         Dimensions:  (x: 3)
6412         Dimensions without coordinates: x
6413         Data variables:
6414             foo      (x) int64 0 1 0
6415         >>> ds.diff("x", 2)
6416         <xarray.Dataset>
6417         Dimensions:  (x: 2)
6418         Dimensions without coordinates: x
6419         Data variables:
6420             foo      (x) int64 1 -1
6421 
6422         See Also
6423         --------
6424         Dataset.differentiate
6425         """
6426         if n == 0:
6427             return self
6428         if n < 0:
6429             raise ValueError(f"order `n` must be non-negative but got {n}")
6430 
6431         # prepare slices
6432         slice_start = {dim: slice(None, -1)}
6433         slice_end = {dim: slice(1, None)}
6434 
6435         # prepare new coordinate
6436         if label == "upper":
6437             slice_new = slice_end
6438         elif label == "lower":
6439             slice_new = slice_start
6440         else:
6441             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6442 
6443         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6444         variables = {}
6445 
6446         for name, var in self.variables.items():
6447             if name in index_vars:
6448                 variables[name] = index_vars[name]
6449             elif dim in var.dims:
6450                 if name in self.data_vars:
6451                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6452                 else:
6453                     variables[name] = var.isel(slice_new)
6454             else:
6455                 variables[name] = var
6456 
6457         difference = self._replace_with_new_dims(variables, indexes=indexes)
6458 
6459         if n > 1:
6460             return difference.diff(dim, n - 1)
6461         else:
6462             return difference
6463 
6464     def shift(
6465         self: T_Dataset,
6466         shifts: Mapping[Any, int] | None = None,
6467         fill_value: Any = xrdtypes.NA,
6468         **shifts_kwargs: int,
6469     ) -> T_Dataset:
6470 
6471         """Shift this dataset by an offset along one or more dimensions.
6472 
6473         Only data variables are moved; coordinates stay in place. This is
6474         consistent with the behavior of ``shift`` in pandas.
6475 
6476         Values shifted from beyond array bounds will appear at one end of
6477         each dimension, which are filled according to `fill_value`. For periodic
6478         offsets instead see `roll`.
6479 
6480         Parameters
6481         ----------
6482         shifts : mapping of hashable to int
6483             Integer offset to shift along each of the given dimensions.
6484             Positive offsets shift to the right; negative offsets shift to the
6485             left.
6486         fill_value : scalar or dict-like, optional
6487             Value to use for newly missing values. If a dict-like, maps
6488             variable names (including coordinates) to fill values.
6489         **shifts_kwargs
6490             The keyword arguments form of ``shifts``.
6491             One of shifts or shifts_kwargs must be provided.
6492 
6493         Returns
6494         -------
6495         shifted : Dataset
6496             Dataset with the same coordinates and attributes but shifted data
6497             variables.
6498 
6499         See Also
6500         --------
6501         roll
6502 
6503         Examples
6504         --------
6505         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6506         >>> ds.shift(x=2)
6507         <xarray.Dataset>
6508         Dimensions:  (x: 5)
6509         Dimensions without coordinates: x
6510         Data variables:
6511             foo      (x) object nan nan 'a' 'b' 'c'
6512         """
6513         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6514         invalid = [k for k in shifts if k not in self.dims]
6515         if invalid:
6516             raise ValueError(f"dimensions {invalid!r} do not exist")
6517 
6518         variables = {}
6519         for name, var in self.variables.items():
6520             if name in self.data_vars:
6521                 fill_value_ = (
6522                     fill_value.get(name, xrdtypes.NA)
6523                     if isinstance(fill_value, dict)
6524                     else fill_value
6525                 )
6526 
6527                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6528                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6529             else:
6530                 variables[name] = var
6531 
6532         return self._replace(variables)
6533 
6534     def roll(
6535         self: T_Dataset,
6536         shifts: Mapping[Any, int] | None = None,
6537         roll_coords: bool = False,
6538         **shifts_kwargs: int,
6539     ) -> T_Dataset:
6540         """Roll this dataset by an offset along one or more dimensions.
6541 
6542         Unlike shift, roll treats the given dimensions as periodic, so will not
6543         create any missing values to be filled.
6544 
6545         Also unlike shift, roll may rotate all variables, including coordinates
6546         if specified. The direction of rotation is consistent with
6547         :py:func:`numpy.roll`.
6548 
6549         Parameters
6550         ----------
6551         shifts : mapping of hashable to int, optional
6552             A dict with keys matching dimensions and values given
6553             by integers to rotate each of the given dimensions. Positive
6554             offsets roll to the right; negative offsets roll to the left.
6555         roll_coords : bool, default: False
6556             Indicates whether to roll the coordinates by the offset too.
6557         **shifts_kwargs : {dim: offset, ...}, optional
6558             The keyword arguments form of ``shifts``.
6559             One of shifts or shifts_kwargs must be provided.
6560 
6561         Returns
6562         -------
6563         rolled : Dataset
6564             Dataset with the same attributes but rolled data and coordinates.
6565 
6566         See Also
6567         --------
6568         shift
6569 
6570         Examples
6571         --------
6572         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6573         >>> ds.roll(x=2)
6574         <xarray.Dataset>
6575         Dimensions:  (x: 5)
6576         Coordinates:
6577           * x        (x) int64 0 1 2 3 4
6578         Data variables:
6579             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6580 
6581         >>> ds.roll(x=2, roll_coords=True)
6582         <xarray.Dataset>
6583         Dimensions:  (x: 5)
6584         Coordinates:
6585           * x        (x) int64 3 4 0 1 2
6586         Data variables:
6587             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6588 
6589         """
6590         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6591         invalid = [k for k in shifts if k not in self.dims]
6592         if invalid:
6593             raise ValueError(f"dimensions {invalid!r} do not exist")
6594 
6595         unrolled_vars: tuple[Hashable, ...]
6596 
6597         if roll_coords:
6598             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6599             unrolled_vars = ()
6600         else:
6601             indexes = dict(self._indexes)
6602             index_vars = dict(self.xindexes.variables)
6603             unrolled_vars = tuple(self.coords)
6604 
6605         variables = {}
6606         for k, var in self.variables.items():
6607             if k in index_vars:
6608                 variables[k] = index_vars[k]
6609             elif k not in unrolled_vars:
6610                 variables[k] = var.roll(
6611                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6612                 )
6613             else:
6614                 variables[k] = var
6615 
6616         return self._replace(variables, indexes=indexes)
6617 
6618     def sortby(
6619         self: T_Dataset,
6620         variables: Hashable | DataArray | list[Hashable | DataArray],
6621         ascending: bool = True,
6622     ) -> T_Dataset:
6623         """
6624         Sort object by labels or values (along an axis).
6625 
6626         Sorts the dataset, either along specified dimensions,
6627         or according to values of 1-D dataarrays that share dimension
6628         with calling object.
6629 
6630         If the input variables are dataarrays, then the dataarrays are aligned
6631         (via left-join) to the calling object prior to sorting by cell values.
6632         NaNs are sorted to the end, following Numpy convention.
6633 
6634         If multiple sorts along the same dimension is
6635         given, numpy's lexsort is performed along that dimension:
6636         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6637         and the FIRST key in the sequence is used as the primary sort key,
6638         followed by the 2nd key, etc.
6639 
6640         Parameters
6641         ----------
6642         variables : Hashable, DataArray, or list of hashable or DataArray
6643             1D DataArray objects or name(s) of 1D variable(s) in
6644             coords/data_vars whose values are used to sort the dataset.
6645         ascending : bool, default: True
6646             Whether to sort by ascending or descending order.
6647 
6648         Returns
6649         -------
6650         sorted : Dataset
6651             A new dataset where all the specified dims are sorted by dim
6652             labels.
6653 
6654         See Also
6655         --------
6656         DataArray.sortby
6657         numpy.sort
6658         pandas.sort_values
6659         pandas.sort_index
6660 
6661         Examples
6662         --------
6663         >>> ds = xr.Dataset(
6664         ...     {
6665         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6666         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6667         ...     },
6668         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6669         ... )
6670         >>> ds.sortby("x")
6671         <xarray.Dataset>
6672         Dimensions:  (x: 2, y: 2)
6673         Coordinates:
6674           * x        (x) <U1 'a' 'b'
6675           * y        (y) int64 1 0
6676         Data variables:
6677             A        (x, y) int64 3 4 1 2
6678             B        (x, y) int64 7 8 5 6
6679         """
6680         from .dataarray import DataArray
6681 
6682         if not isinstance(variables, list):
6683             variables = [variables]
6684         else:
6685             variables = variables
6686         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6687         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6688         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6689         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6690         vars_by_dim = defaultdict(list)
6691         for data_array in aligned_other_vars:
6692             if data_array.ndim != 1:
6693                 raise ValueError("Input DataArray is not 1-D.")
6694             (key,) = data_array.dims
6695             vars_by_dim[key].append(data_array)
6696 
6697         indices = {}
6698         for key, arrays in vars_by_dim.items():
6699             order = np.lexsort(tuple(reversed(arrays)))
6700             indices[key] = order if ascending else order[::-1]
6701         return aligned_self.isel(indices)
6702 
6703     def quantile(
6704         self: T_Dataset,
6705         q: ArrayLike,
6706         dim: str | Iterable[Hashable] | None = None,
6707         method: QUANTILE_METHODS = "linear",
6708         numeric_only: bool = False,
6709         keep_attrs: bool = None,
6710         skipna: bool = None,
6711         interpolation: QUANTILE_METHODS = None,
6712     ) -> T_Dataset:
6713         """Compute the qth quantile of the data along the specified dimension.
6714 
6715         Returns the qth quantiles(s) of the array elements for each variable
6716         in the Dataset.
6717 
6718         Parameters
6719         ----------
6720         q : float or array-like of float
6721             Quantile to compute, which must be between 0 and 1 inclusive.
6722         dim : str or Iterable of Hashable, optional
6723             Dimension(s) over which to apply quantile.
6724         method : str, default: "linear"
6725             This optional parameter specifies the interpolation method to use when the
6726             desired quantile lies between two data points. The options sorted by their R
6727             type as summarized in the H&F paper [1]_ are:
6728 
6729                 1. "inverted_cdf" (*)
6730                 2. "averaged_inverted_cdf" (*)
6731                 3. "closest_observation" (*)
6732                 4. "interpolated_inverted_cdf" (*)
6733                 5. "hazen" (*)
6734                 6. "weibull" (*)
6735                 7. "linear"  (default)
6736                 8. "median_unbiased" (*)
6737                 9. "normal_unbiased" (*)
6738 
6739             The first three methods are discontiuous.  The following discontinuous
6740             variations of the default "linear" (7.) option are also available:
6741 
6742                 * "lower"
6743                 * "higher"
6744                 * "midpoint"
6745                 * "nearest"
6746 
6747             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
6748             was previously called "interpolation", renamed in accordance with numpy
6749             version 1.22.0.
6750 
6751             (*) These methods require numpy version 1.22 or newer.
6752 
6753         keep_attrs : bool, optional
6754             If True, the dataset's attributes (`attrs`) will be copied from
6755             the original object to the new one.  If False (default), the new
6756             object will be returned without attributes.
6757         numeric_only : bool, optional
6758             If True, only apply ``func`` to variables with a numeric dtype.
6759         skipna : bool, optional
6760             If True, skip missing values (as marked by NaN). By default, only
6761             skips missing values for float dtypes; other dtypes either do not
6762             have a sentinel missing value (int) or skipna=True has not been
6763             implemented (object, datetime64 or timedelta64).
6764 
6765         Returns
6766         -------
6767         quantiles : Dataset
6768             If `q` is a single quantile, then the result is a scalar for each
6769             variable in data_vars. If multiple percentiles are given, first
6770             axis of the result corresponds to the quantile and a quantile
6771             dimension is added to the return Dataset. The other dimensions are
6772             the dimensions that remain after the reduction of the array.
6773 
6774         See Also
6775         --------
6776         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
6777 
6778         Examples
6779         --------
6780         >>> ds = xr.Dataset(
6781         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
6782         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
6783         ... )
6784         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
6785         <xarray.Dataset>
6786         Dimensions:   ()
6787         Coordinates:
6788             quantile  float64 0.0
6789         Data variables:
6790             a         float64 0.7
6791         >>> ds.quantile(0, dim="x")
6792         <xarray.Dataset>
6793         Dimensions:   (y: 4)
6794         Coordinates:
6795           * y         (y) float64 1.0 1.5 2.0 2.5
6796             quantile  float64 0.0
6797         Data variables:
6798             a         (y) float64 0.7 4.2 2.6 1.5
6799         >>> ds.quantile([0, 0.5, 1])
6800         <xarray.Dataset>
6801         Dimensions:   (quantile: 3)
6802         Coordinates:
6803           * quantile  (quantile) float64 0.0 0.5 1.0
6804         Data variables:
6805             a         (quantile) float64 0.7 3.4 9.4
6806         >>> ds.quantile([0, 0.5, 1], dim="x")
6807         <xarray.Dataset>
6808         Dimensions:   (quantile: 3, y: 4)
6809         Coordinates:
6810           * y         (y) float64 1.0 1.5 2.0 2.5
6811           * quantile  (quantile) float64 0.0 0.5 1.0
6812         Data variables:
6813             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
6814 
6815         References
6816         ----------
6817         .. [1] R. J. Hyndman and Y. Fan,
6818            "Sample quantiles in statistical packages,"
6819            The American Statistician, 50(4), pp. 361-365, 1996
6820         """
6821 
6822         # interpolation renamed to method in version 0.21.0
6823         # check here and in variable to avoid repeated warnings
6824         if interpolation is not None:
6825             warnings.warn(
6826                 "The `interpolation` argument to quantile was renamed to `method`.",
6827                 FutureWarning,
6828             )
6829 
6830             if method != "linear":
6831                 raise TypeError("Cannot pass interpolation and method keywords!")
6832 
6833             method = interpolation
6834 
6835         dims: set[Hashable]
6836         if isinstance(dim, str):
6837             dims = {dim}
6838         elif dim is None or dim is ...:
6839             dims = set(self.dims)
6840         else:
6841             dims = set(dim)
6842 
6843         _assert_empty(
6844             tuple(d for d in dims if d not in self.dims),
6845             "Dataset does not contain the dimensions: %s",
6846         )
6847 
6848         q = np.asarray(q, dtype=np.float64)
6849 
6850         variables = {}
6851         for name, var in self.variables.items():
6852             reduce_dims = [d for d in var.dims if d in dims]
6853             if reduce_dims or not var.dims:
6854                 if name not in self.coords:
6855                     if (
6856                         not numeric_only
6857                         or np.issubdtype(var.dtype, np.number)
6858                         or var.dtype == np.bool_
6859                     ):
6860                         variables[name] = var.quantile(
6861                             q,
6862                             dim=reduce_dims,
6863                             method=method,
6864                             keep_attrs=keep_attrs,
6865                             skipna=skipna,
6866                         )
6867 
6868             else:
6869                 variables[name] = var
6870 
6871         # construct the new dataset
6872         coord_names = {k for k in self.coords if k in variables}
6873         indexes = {k: v for k, v in self._indexes.items() if k in variables}
6874         if keep_attrs is None:
6875             keep_attrs = _get_keep_attrs(default=False)
6876         attrs = self.attrs if keep_attrs else None
6877         new = self._replace_with_new_dims(
6878             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
6879         )
6880         return new.assign_coords(quantile=q)
6881 
6882     def rank(
6883         self: T_Dataset,
6884         dim: Hashable,
6885         pct: bool = False,
6886         keep_attrs: bool | None = None,
6887     ) -> T_Dataset:
6888         """Ranks the data.
6889 
6890         Equal values are assigned a rank that is the average of the ranks that
6891         would have been otherwise assigned to all of the values within
6892         that set.
6893         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
6894 
6895         NaNs in the input array are returned as NaNs.
6896 
6897         The `bottleneck` library is required.
6898 
6899         Parameters
6900         ----------
6901         dim : Hashable
6902             Dimension over which to compute rank.
6903         pct : bool, default: False
6904             If True, compute percentage ranks, otherwise compute integer ranks.
6905         keep_attrs : bool or None, optional
6906             If True, the dataset's attributes (`attrs`) will be copied from
6907             the original object to the new one.  If False, the new
6908             object will be returned without attributes.
6909 
6910         Returns
6911         -------
6912         ranked : Dataset
6913             Variables that do not depend on `dim` are dropped.
6914         """
6915         if not OPTIONS["use_bottleneck"]:
6916             raise RuntimeError(
6917                 "rank requires bottleneck to be enabled."
6918                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
6919             )
6920 
6921         if dim not in self.dims:
6922             raise ValueError(f"Dataset does not contain the dimension: {dim}")
6923 
6924         variables = {}
6925         for name, var in self.variables.items():
6926             if name in self.data_vars:
6927                 if dim in var.dims:
6928                     variables[name] = var.rank(dim, pct=pct)
6929             else:
6930                 variables[name] = var
6931 
6932         coord_names = set(self.coords)
6933         if keep_attrs is None:
6934             keep_attrs = _get_keep_attrs(default=False)
6935         attrs = self.attrs if keep_attrs else None
6936         return self._replace(variables, coord_names, attrs=attrs)
6937 
6938     def differentiate(
6939         self: T_Dataset,
6940         coord: Hashable,
6941         edge_order: Literal[1, 2] = 1,
6942         datetime_unit: DatetimeUnitOptions | None = None,
6943     ) -> T_Dataset:
6944         """ Differentiate with the second order accurate central
6945         differences.
6946 
6947         .. note::
6948             This feature is limited to simple cartesian geometry, i.e. coord
6949             must be one dimensional.
6950 
6951         Parameters
6952         ----------
6953         coord : Hashable
6954             The coordinate to be used to compute the gradient.
6955         edge_order : {1, 2}, default: 1
6956             N-th order accurate differences at the boundaries.
6957         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
6958             "us", "ns", "ps", "fs", "as", None}, default: None
6959             Unit to compute gradient. Only valid for datetime coordinate.
6960 
6961         Returns
6962         -------
6963         differentiated: Dataset
6964 
6965         See also
6966         --------
6967         numpy.gradient: corresponding numpy function
6968         """
6969         from .variable import Variable
6970 
6971         if coord not in self.variables and coord not in self.dims:
6972             raise ValueError(f"Coordinate {coord} does not exist.")
6973 
6974         coord_var = self[coord].variable
6975         if coord_var.ndim != 1:
6976             raise ValueError(
6977                 "Coordinate {} must be 1 dimensional but is {}"
6978                 " dimensional".format(coord, coord_var.ndim)
6979             )
6980 
6981         dim = coord_var.dims[0]
6982         if _contains_datetime_like_objects(coord_var):
6983             if coord_var.dtype.kind in "mM" and datetime_unit is None:
6984                 datetime_unit = cast(
6985                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
6986                 )
6987             elif datetime_unit is None:
6988                 datetime_unit = "s"  # Default to seconds for cftime objects
6989             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
6990 
6991         variables = {}
6992         for k, v in self.variables.items():
6993             if k in self.data_vars and dim in v.dims and k not in self.coords:
6994                 if _contains_datetime_like_objects(v):
6995                     v = v._to_numeric(datetime_unit=datetime_unit)
6996                 grad = duck_array_ops.gradient(
6997                     v.data,
6998                     coord_var.data,
6999                     edge_order=edge_order,
7000                     axis=v.get_axis_num(dim),
7001                 )
7002                 variables[k] = Variable(v.dims, grad)
7003             else:
7004                 variables[k] = v
7005         return self._replace(variables)
7006 
7007     def integrate(
7008         self: T_Dataset,
7009         coord: Hashable | Sequence[Hashable],
7010         datetime_unit: DatetimeUnitOptions = None,
7011     ) -> T_Dataset:
7012         """Integrate along the given coordinate using the trapezoidal rule.
7013 
7014         .. note::
7015             This feature is limited to simple cartesian geometry, i.e. coord
7016             must be one dimensional.
7017 
7018         Parameters
7019         ----------
7020         coord : hashable, or sequence of hashable
7021             Coordinate(s) used for the integration.
7022         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7023                         'ps', 'fs', 'as', None}, optional
7024             Specify the unit if datetime coordinate is used.
7025 
7026         Returns
7027         -------
7028         integrated : Dataset
7029 
7030         See also
7031         --------
7032         DataArray.integrate
7033         numpy.trapz : corresponding numpy function
7034 
7035         Examples
7036         --------
7037         >>> ds = xr.Dataset(
7038         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7039         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7040         ... )
7041         >>> ds
7042         <xarray.Dataset>
7043         Dimensions:  (x: 4)
7044         Coordinates:
7045           * x        (x) int64 0 1 2 3
7046             y        (x) int64 1 7 3 5
7047         Data variables:
7048             a        (x) int64 5 5 6 6
7049             b        (x) int64 1 2 1 0
7050         >>> ds.integrate("x")
7051         <xarray.Dataset>
7052         Dimensions:  ()
7053         Data variables:
7054             a        float64 16.5
7055             b        float64 3.5
7056         >>> ds.integrate("y")
7057         <xarray.Dataset>
7058         Dimensions:  ()
7059         Data variables:
7060             a        float64 20.0
7061             b        float64 4.0
7062         """
7063         if not isinstance(coord, (list, tuple)):
7064             coord = (coord,)
7065         result = self
7066         for c in coord:
7067             result = result._integrate_one(c, datetime_unit=datetime_unit)
7068         return result
7069 
7070     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7071         from .variable import Variable
7072 
7073         if coord not in self.variables and coord not in self.dims:
7074             raise ValueError(f"Coordinate {coord} does not exist.")
7075 
7076         coord_var = self[coord].variable
7077         if coord_var.ndim != 1:
7078             raise ValueError(
7079                 "Coordinate {} must be 1 dimensional but is {}"
7080                 " dimensional".format(coord, coord_var.ndim)
7081             )
7082 
7083         dim = coord_var.dims[0]
7084         if _contains_datetime_like_objects(coord_var):
7085             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7086                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7087             elif datetime_unit is None:
7088                 datetime_unit = "s"  # Default to seconds for cftime objects
7089             coord_var = coord_var._replace(
7090                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7091             )
7092 
7093         variables = {}
7094         coord_names = set()
7095         for k, v in self.variables.items():
7096             if k in self.coords:
7097                 if dim not in v.dims or cumulative:
7098                     variables[k] = v
7099                     coord_names.add(k)
7100             else:
7101                 if k in self.data_vars and dim in v.dims:
7102                     if _contains_datetime_like_objects(v):
7103                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7104                     if cumulative:
7105                         integ = duck_array_ops.cumulative_trapezoid(
7106                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7107                         )
7108                         v_dims = v.dims
7109                     else:
7110                         integ = duck_array_ops.trapz(
7111                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7112                         )
7113                         v_dims = list(v.dims)
7114                         v_dims.remove(dim)
7115                     variables[k] = Variable(v_dims, integ)
7116                 else:
7117                     variables[k] = v
7118         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7119         return self._replace_with_new_dims(
7120             variables, coord_names=coord_names, indexes=indexes
7121         )
7122 
7123     def cumulative_integrate(
7124         self: T_Dataset,
7125         coord: Hashable | Sequence[Hashable],
7126         datetime_unit: DatetimeUnitOptions = None,
7127     ) -> T_Dataset:
7128         """Integrate along the given coordinate using the trapezoidal rule.
7129 
7130         .. note::
7131             This feature is limited to simple cartesian geometry, i.e. coord
7132             must be one dimensional.
7133 
7134             The first entry of the cumulative integral of each variable is always 0, in
7135             order to keep the length of the dimension unchanged between input and
7136             output.
7137 
7138         Parameters
7139         ----------
7140         coord : hashable, or sequence of hashable
7141             Coordinate(s) used for the integration.
7142         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7143                         'ps', 'fs', 'as', None}, optional
7144             Specify the unit if datetime coordinate is used.
7145 
7146         Returns
7147         -------
7148         integrated : Dataset
7149 
7150         See also
7151         --------
7152         DataArray.cumulative_integrate
7153         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7154 
7155         Examples
7156         --------
7157         >>> ds = xr.Dataset(
7158         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7159         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7160         ... )
7161         >>> ds
7162         <xarray.Dataset>
7163         Dimensions:  (x: 4)
7164         Coordinates:
7165           * x        (x) int64 0 1 2 3
7166             y        (x) int64 1 7 3 5
7167         Data variables:
7168             a        (x) int64 5 5 6 6
7169             b        (x) int64 1 2 1 0
7170         >>> ds.cumulative_integrate("x")
7171         <xarray.Dataset>
7172         Dimensions:  (x: 4)
7173         Coordinates:
7174           * x        (x) int64 0 1 2 3
7175             y        (x) int64 1 7 3 5
7176         Data variables:
7177             a        (x) float64 0.0 5.0 10.5 16.5
7178             b        (x) float64 0.0 1.5 3.0 3.5
7179         >>> ds.cumulative_integrate("y")
7180         <xarray.Dataset>
7181         Dimensions:  (x: 4)
7182         Coordinates:
7183           * x        (x) int64 0 1 2 3
7184             y        (x) int64 1 7 3 5
7185         Data variables:
7186             a        (x) float64 0.0 30.0 8.0 20.0
7187             b        (x) float64 0.0 9.0 3.0 4.0
7188         """
7189         if not isinstance(coord, (list, tuple)):
7190             coord = (coord,)
7191         result = self
7192         for c in coord:
7193             result = result._integrate_one(
7194                 c, datetime_unit=datetime_unit, cumulative=True
7195             )
7196         return result
7197 
7198     @property
7199     def real(self: T_Dataset) -> T_Dataset:
7200         return self.map(lambda x: x.real, keep_attrs=True)
7201 
7202     @property
7203     def imag(self: T_Dataset) -> T_Dataset:
7204         return self.map(lambda x: x.imag, keep_attrs=True)
7205 
7206     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7207 
7208     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7209         """Returns a ``Dataset`` with variables that match specific conditions.
7210 
7211         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7212         containing only the variables for which all the filter tests pass.
7213         These tests are either ``key=value`` for which the attribute ``key``
7214         has the exact value ``value`` or the callable passed into
7215         ``key=callable`` returns True. The callable will be passed a single
7216         value, either the value of the attribute ``key`` or ``None`` if the
7217         DataArray does not have an attribute with the name ``key``.
7218 
7219         Parameters
7220         ----------
7221         **kwargs
7222             key : str
7223                 Attribute name.
7224             value : callable or obj
7225                 If value is a callable, it should return a boolean in the form
7226                 of bool = func(attr) where attr is da.attrs[key].
7227                 Otherwise, value will be compared to the each
7228                 DataArray's attrs[key].
7229 
7230         Returns
7231         -------
7232         new : Dataset
7233             New dataset with variables filtered by attribute.
7234 
7235         Examples
7236         --------
7237         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7238         >>> precip = 10 * np.random.rand(2, 2, 3)
7239         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7240         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7241         >>> dims = ["x", "y", "time"]
7242         >>> temp_attr = dict(standard_name="air_potential_temperature")
7243         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7244 
7245         >>> ds = xr.Dataset(
7246         ...     dict(
7247         ...         temperature=(dims, temp, temp_attr),
7248         ...         precipitation=(dims, precip, precip_attr),
7249         ...     ),
7250         ...     coords=dict(
7251         ...         lon=(["x", "y"], lon),
7252         ...         lat=(["x", "y"], lat),
7253         ...         time=pd.date_range("2014-09-06", periods=3),
7254         ...         reference_time=pd.Timestamp("2014-09-05"),
7255         ...     ),
7256         ... )
7257 
7258         Get variables matching a specific standard_name:
7259 
7260         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7261         <xarray.Dataset>
7262         Dimensions:         (x: 2, y: 2, time: 3)
7263         Coordinates:
7264             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7265             lat             (x, y) float64 42.25 42.21 42.63 42.59
7266           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7267             reference_time  datetime64[ns] 2014-09-05
7268         Dimensions without coordinates: x, y
7269         Data variables:
7270             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7271 
7272         Get all variables that have a standard_name attribute:
7273 
7274         >>> standard_name = lambda v: v is not None
7275         >>> ds.filter_by_attrs(standard_name=standard_name)
7276         <xarray.Dataset>
7277         Dimensions:         (x: 2, y: 2, time: 3)
7278         Coordinates:
7279             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7280             lat             (x, y) float64 42.25 42.21 42.63 42.59
7281           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7282             reference_time  datetime64[ns] 2014-09-05
7283         Dimensions without coordinates: x, y
7284         Data variables:
7285             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7286             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7287 
7288         """
7289         selection = []
7290         for var_name, variable in self.variables.items():
7291             has_value_flag = False
7292             for attr_name, pattern in kwargs.items():
7293                 attr_value = variable.attrs.get(attr_name)
7294                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7295                     has_value_flag = True
7296                 else:
7297                     has_value_flag = False
7298                     break
7299             if has_value_flag is True:
7300                 selection.append(var_name)
7301         return self[selection]
7302 
7303     def unify_chunks(self: T_Dataset) -> T_Dataset:
7304         """Unify chunk size along all chunked dimensions of this Dataset.
7305 
7306         Returns
7307         -------
7308         Dataset with consistent chunk sizes for all dask-array variables
7309 
7310         See Also
7311         --------
7312         dask.array.core.unify_chunks
7313         """
7314 
7315         return unify_chunks(self)[0]
7316 
7317     def map_blocks(
7318         self,
7319         func: Callable[..., T_Xarray],
7320         args: Sequence[Any] = (),
7321         kwargs: Mapping[str, Any] | None = None,
7322         template: DataArray | Dataset | None = None,
7323     ) -> T_Xarray:
7324         """
7325         Apply a function to each block of this Dataset.
7326 
7327         .. warning::
7328             This method is experimental and its signature may change.
7329 
7330         Parameters
7331         ----------
7332         func : callable
7333             User-provided function that accepts a Dataset as its first
7334             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7335             corresponding to one chunk along each chunked dimension. ``func`` will be
7336             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7337 
7338             This function must return either a single DataArray or a single Dataset.
7339 
7340             This function cannot add a new chunked dimension.
7341         args : sequence
7342             Passed to func after unpacking and subsetting any xarray objects by blocks.
7343             xarray objects in args must be aligned with obj, otherwise an error is raised.
7344         kwargs : Mapping or None
7345             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7346             subset to blocks. Passing dask collections in kwargs is not allowed.
7347         template : DataArray, Dataset or None, optional
7348             xarray object representing the final result after compute is called. If not provided,
7349             the function will be first run on mocked-up data, that looks like this object but
7350             has sizes 0, to determine properties of the returned object such as dtype,
7351             variable names, attributes, new dimensions and new indexes (if any).
7352             ``template`` must be provided if the function changes the size of existing dimensions.
7353             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7354             ``attrs`` set by ``func`` will be ignored.
7355 
7356         Returns
7357         -------
7358         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7359         function.
7360 
7361         Notes
7362         -----
7363         This function is designed for when ``func`` needs to manipulate a whole xarray object
7364         subset to each block. Each block is loaded into memory. In the more common case where
7365         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7366 
7367         If none of the variables in this object is backed by dask arrays, calling this function is
7368         equivalent to calling ``func(obj, *args, **kwargs)``.
7369 
7370         See Also
7371         --------
7372         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7373         xarray.DataArray.map_blocks
7374 
7375         Examples
7376         --------
7377         Calculate an anomaly from climatology using ``.groupby()``. Using
7378         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7379         its indices, and its methods like ``.groupby()``.
7380 
7381         >>> def calculate_anomaly(da, groupby_type="time.month"):
7382         ...     gb = da.groupby(groupby_type)
7383         ...     clim = gb.mean(dim="time")
7384         ...     return gb - clim
7385         ...
7386         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7387         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7388         >>> np.random.seed(123)
7389         >>> array = xr.DataArray(
7390         ...     np.random.rand(len(time)),
7391         ...     dims=["time"],
7392         ...     coords={"time": time, "month": month},
7393         ... ).chunk()
7394         >>> ds = xr.Dataset({"a": array})
7395         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7396         <xarray.Dataset>
7397         Dimensions:  (time: 24)
7398         Coordinates:
7399           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7400             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7401         Data variables:
7402             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7403 
7404         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7405         to the function being applied in ``xr.map_blocks()``:
7406 
7407         >>> ds.map_blocks(
7408         ...     calculate_anomaly,
7409         ...     kwargs={"groupby_type": "time.year"},
7410         ...     template=ds,
7411         ... )
7412         <xarray.Dataset>
7413         Dimensions:  (time: 24)
7414         Coordinates:
7415           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7416             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7417         Data variables:
7418             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7419         """
7420         from .parallel import map_blocks
7421 
7422         return map_blocks(func, self, args, kwargs, template)
7423 
7424     def polyfit(
7425         self: T_Dataset,
7426         dim: Hashable,
7427         deg: int,
7428         skipna: bool | None = None,
7429         rcond: float | None = None,
7430         w: Hashable | Any = None,
7431         full: bool = False,
7432         cov: bool | Literal["unscaled"] = False,
7433     ) -> T_Dataset:
7434         """
7435         Least squares polynomial fit.
7436 
7437         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7438         invalid values when `skipna = True`.
7439 
7440         Parameters
7441         ----------
7442         dim : hashable
7443             Coordinate along which to fit the polynomials.
7444         deg : int
7445             Degree of the fitting polynomial.
7446         skipna : bool or None, optional
7447             If True, removes all invalid values before fitting each 1D slices of the array.
7448             Default is True if data is stored in a dask.array or if there is any
7449             invalid values, False otherwise.
7450         rcond : float or None, optional
7451             Relative condition number to the fit.
7452         w : hashable or Any, optional
7453             Weights to apply to the y-coordinate of the sample points.
7454             Can be an array-like object or the name of a coordinate in the dataset.
7455         full : bool, default: False
7456             Whether to return the residuals, matrix rank and singular values in addition
7457             to the coefficients.
7458         cov : bool or "unscaled", default: False
7459             Whether to return to the covariance matrix in addition to the coefficients.
7460             The matrix is not scaled if `cov='unscaled'`.
7461 
7462         Returns
7463         -------
7464         polyfit_results : Dataset
7465             A single dataset which contains (for each "var" in the input dataset):
7466 
7467             [var]_polyfit_coefficients
7468                 The coefficients of the best fit for each variable in this dataset.
7469             [var]_polyfit_residuals
7470                 The residuals of the least-square computation for each variable (only included if `full=True`)
7471                 When the matrix rank is deficient, np.nan is returned.
7472             [dim]_matrix_rank
7473                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7474                 The rank is computed ignoring the NaN values that might be skipped.
7475             [dim]_singular_values
7476                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7477             [var]_polyfit_covariance
7478                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7479 
7480         Warns
7481         -----
7482         RankWarning
7483             The rank of the coefficient matrix in the least-squares fit is deficient.
7484             The warning is not raised with in-memory (not dask) data and `full=True`.
7485 
7486         See Also
7487         --------
7488         numpy.polyfit
7489         numpy.polyval
7490         xarray.polyval
7491         """
7492         from .dataarray import DataArray
7493 
7494         variables = {}
7495         skipna_da = skipna
7496 
7497         x = get_clean_interp_index(self, dim, strict=False)
7498         xname = f"{self[dim].name}_"
7499         order = int(deg) + 1
7500         lhs = np.vander(x, order)
7501 
7502         if rcond is None:
7503             rcond = (
7504                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7505             )
7506 
7507         # Weights:
7508         if w is not None:
7509             if isinstance(w, Hashable):
7510                 w = self.coords[w]
7511             w = np.asarray(w)
7512             if w.ndim != 1:
7513                 raise TypeError("Expected a 1-d array for weights.")
7514             if w.shape[0] != lhs.shape[0]:
7515                 raise TypeError(f"Expected w and {dim} to have the same length")
7516             lhs *= w[:, np.newaxis]
7517 
7518         # Scaling
7519         scale = np.sqrt((lhs * lhs).sum(axis=0))
7520         lhs /= scale
7521 
7522         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7523 
7524         rank = np.linalg.matrix_rank(lhs)
7525 
7526         if full:
7527             rank = DataArray(rank, name=xname + "matrix_rank")
7528             variables[rank.name] = rank
7529             _sing = np.linalg.svd(lhs, compute_uv=False)
7530             sing = DataArray(
7531                 _sing,
7532                 dims=(degree_dim,),
7533                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7534                 name=xname + "singular_values",
7535             )
7536             variables[sing.name] = sing
7537 
7538         for name, da in self.data_vars.items():
7539             if dim not in da.dims:
7540                 continue
7541 
7542             if is_duck_dask_array(da.data) and (
7543                 rank != order or full or skipna is None
7544             ):
7545                 # Current algorithm with dask and skipna=False neither supports
7546                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7547                 skipna_da = True
7548             elif skipna is None:
7549                 skipna_da = bool(np.any(da.isnull()))
7550 
7551             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7552             stacked_coords: dict[Hashable, DataArray] = {}
7553             if dims_to_stack:
7554                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7555                 rhs = da.transpose(dim, *dims_to_stack).stack(
7556                     {stacked_dim: dims_to_stack}
7557                 )
7558                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7559                 scale_da = scale[:, np.newaxis]
7560             else:
7561                 rhs = da
7562                 scale_da = scale
7563 
7564             if w is not None:
7565                 rhs *= w[:, np.newaxis]
7566 
7567             with warnings.catch_warnings():
7568                 if full:  # Copy np.polyfit behavior
7569                     warnings.simplefilter("ignore", np.RankWarning)
7570                 else:  # Raise only once per variable
7571                     warnings.simplefilter("once", np.RankWarning)
7572 
7573                 coeffs, residuals = duck_array_ops.least_squares(
7574                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7575                 )
7576 
7577             if isinstance(name, str):
7578                 name = f"{name}_"
7579             else:
7580                 # Thus a ReprObject => polyfit was called on a DataArray
7581                 name = ""
7582 
7583             coeffs = DataArray(
7584                 coeffs / scale_da,
7585                 dims=[degree_dim] + list(stacked_coords.keys()),
7586                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7587                 name=name + "polyfit_coefficients",
7588             )
7589             if dims_to_stack:
7590                 coeffs = coeffs.unstack(stacked_dim)
7591             variables[coeffs.name] = coeffs
7592 
7593             if full or (cov is True):
7594                 residuals = DataArray(
7595                     residuals if dims_to_stack else residuals.squeeze(),
7596                     dims=list(stacked_coords.keys()),
7597                     coords=stacked_coords,
7598                     name=name + "polyfit_residuals",
7599                 )
7600                 if dims_to_stack:
7601                     residuals = residuals.unstack(stacked_dim)
7602                 variables[residuals.name] = residuals
7603 
7604             if cov:
7605                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7606                 Vbase /= np.outer(scale, scale)
7607                 if cov == "unscaled":
7608                     fac = 1
7609                 else:
7610                     if x.shape[0] <= order:
7611                         raise ValueError(
7612                             "The number of data points must exceed order to scale the covariance matrix."
7613                         )
7614                     fac = residuals / (x.shape[0] - order)
7615                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7616                 variables[name + "polyfit_covariance"] = covariance
7617 
7618         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7619 
7620     def pad(
7621         self: T_Dataset,
7622         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7623         mode: PadModeOptions = "constant",
7624         stat_length: int
7625         | tuple[int, int]
7626         | Mapping[Any, tuple[int, int]]
7627         | None = None,
7628         constant_values: (
7629             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7630         ) = None,
7631         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7632         reflect_type: PadReflectOptions = None,
7633         **pad_width_kwargs: Any,
7634     ) -> T_Dataset:
7635         """Pad this dataset along one or more dimensions.
7636 
7637         .. warning::
7638             This function is experimental and its behaviour is likely to change
7639             especially regarding padding of dimension coordinates (or IndexVariables).
7640 
7641         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7642         coordinates will be padded with the same mode, otherwise coordinates
7643         are padded using the "constant" mode with fill_value dtypes.NA.
7644 
7645         Parameters
7646         ----------
7647         pad_width : mapping of hashable to tuple of int
7648             Mapping with the form of {dim: (pad_before, pad_after)}
7649             describing the number of values padded along each dimension.
7650             {dim: pad} is a shortcut for pad_before = pad_after = pad
7651         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7652             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7653             How to pad the DataArray (taken from numpy docs):
7654 
7655             - "constant": Pads with a constant value.
7656             - "edge": Pads with the edge values of array.
7657             - "linear_ramp": Pads with the linear ramp between end_value and the
7658               array edge value.
7659             - "maximum": Pads with the maximum value of all or part of the
7660               vector along each axis.
7661             - "mean": Pads with the mean value of all or part of the
7662               vector along each axis.
7663             - "median": Pads with the median value of all or part of the
7664               vector along each axis.
7665             - "minimum": Pads with the minimum value of all or part of the
7666               vector along each axis.
7667             - "reflect": Pads with the reflection of the vector mirrored on
7668               the first and last values of the vector along each axis.
7669             - "symmetric": Pads with the reflection of the vector mirrored
7670               along the edge of the array.
7671             - "wrap": Pads with the wrap of the vector along the axis.
7672               The first values are used to pad the end and the
7673               end values are used to pad the beginning.
7674 
7675         stat_length : int, tuple or mapping of hashable to tuple, default: None
7676             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7677             values at edge of each axis used to calculate the statistic value.
7678             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7679             statistic lengths along each dimension.
7680             ((before, after),) yields same before and after statistic lengths
7681             for each dimension.
7682             (stat_length,) or int is a shortcut for before = after = statistic
7683             length for all axes.
7684             Default is ``None``, to use the entire axis.
7685         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7686             Used in 'constant'.  The values to set the padded values for each
7687             axis.
7688             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7689             pad constants along each dimension.
7690             ``((before, after),)`` yields same before and after constants for each
7691             dimension.
7692             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7693             all dimensions.
7694             Default is 0.
7695         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7696             Used in 'linear_ramp'.  The values used for the ending value of the
7697             linear_ramp and that will form the edge of the padded array.
7698             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7699             end values along each dimension.
7700             ``((before, after),)`` yields same before and after end values for each
7701             axis.
7702             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7703             all axes.
7704             Default is 0.
7705         reflect_type : {"even", "odd", None}, optional
7706             Used in "reflect", and "symmetric".  The "even" style is the
7707             default with an unaltered reflection around the edge value.  For
7708             the "odd" style, the extended part of the array is created by
7709             subtracting the reflected values from two times the edge value.
7710         **pad_width_kwargs
7711             The keyword arguments form of ``pad_width``.
7712             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7713 
7714         Returns
7715         -------
7716         padded : Dataset
7717             Dataset with the padded coordinates and data.
7718 
7719         See Also
7720         --------
7721         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7722 
7723         Notes
7724         -----
7725         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7726         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7727         specify ``constant_values=np.nan``
7728 
7729         Padding coordinates will drop their corresponding index (if any) and will reset default
7730         indexes for dimension coordinates.
7731 
7732         Examples
7733         --------
7734         >>> ds = xr.Dataset({"foo": ("x", range(5))})
7735         >>> ds.pad(x=(1, 2))
7736         <xarray.Dataset>
7737         Dimensions:  (x: 8)
7738         Dimensions without coordinates: x
7739         Data variables:
7740             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
7741         """
7742         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
7743 
7744         if mode in ("edge", "reflect", "symmetric", "wrap"):
7745             coord_pad_mode = mode
7746             coord_pad_options = {
7747                 "stat_length": stat_length,
7748                 "constant_values": constant_values,
7749                 "end_values": end_values,
7750                 "reflect_type": reflect_type,
7751             }
7752         else:
7753             coord_pad_mode = "constant"
7754             coord_pad_options = {}
7755 
7756         variables = {}
7757 
7758         # keep indexes that won't be affected by pad and drop all other indexes
7759         xindexes = self.xindexes
7760         pad_dims = set(pad_width)
7761         indexes = {}
7762         for k, idx in xindexes.items():
7763             if not pad_dims.intersection(xindexes.get_all_dims(k)):
7764                 indexes[k] = idx
7765 
7766         for name, var in self.variables.items():
7767             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
7768             if not var_pad_width:
7769                 variables[name] = var
7770             elif name in self.data_vars:
7771                 variables[name] = var.pad(
7772                     pad_width=var_pad_width,
7773                     mode=mode,
7774                     stat_length=stat_length,
7775                     constant_values=constant_values,
7776                     end_values=end_values,
7777                     reflect_type=reflect_type,
7778                 )
7779             else:
7780                 variables[name] = var.pad(
7781                     pad_width=var_pad_width,
7782                     mode=coord_pad_mode,
7783                     **coord_pad_options,  # type: ignore[arg-type]
7784                 )
7785                 # reset default index of dimension coordinates
7786                 if (name,) == var.dims:
7787                     dim_var = {name: variables[name]}
7788                     index = PandasIndex.from_variables(dim_var)
7789                     index_vars = index.create_variables(dim_var)
7790                     indexes[name] = index
7791                     variables[name] = index_vars[name]
7792 
7793         return self._replace_with_new_dims(variables, indexes=indexes)
7794 
7795     def idxmin(
7796         self: T_Dataset,
7797         dim: Hashable | None = None,
7798         skipna: bool | None = None,
7799         fill_value: Any = xrdtypes.NA,
7800         keep_attrs: bool | None = None,
7801     ) -> T_Dataset:
7802         """Return the coordinate label of the minimum value along a dimension.
7803 
7804         Returns a new `Dataset` named after the dimension with the values of
7805         the coordinate labels along that dimension corresponding to minimum
7806         values along that dimension.
7807 
7808         In comparison to :py:meth:`~Dataset.argmin`, this returns the
7809         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
7810 
7811         Parameters
7812         ----------
7813         dim : Hashable, optional
7814             Dimension over which to apply `idxmin`.  This is optional for 1D
7815             variables, but required for variables with 2 or more dimensions.
7816         skipna : bool or None, optional
7817             If True, skip missing values (as marked by NaN). By default, only
7818             skips missing values for ``float``, ``complex``, and ``object``
7819             dtypes; other dtypes either do not have a sentinel missing value
7820             (``int``) or ``skipna=True`` has not been implemented
7821             (``datetime64`` or ``timedelta64``).
7822         fill_value : Any, default: NaN
7823             Value to be filled in case all of the values along a dimension are
7824             null.  By default this is NaN.  The fill value and result are
7825             automatically converted to a compatible dtype if possible.
7826             Ignored if ``skipna`` is False.
7827         keep_attrs : bool or None, optional
7828             If True, the attributes (``attrs``) will be copied from the
7829             original object to the new one. If False, the new object
7830             will be returned without attributes.
7831 
7832         Returns
7833         -------
7834         reduced : Dataset
7835             New `Dataset` object with `idxmin` applied to its data and the
7836             indicated dimension removed.
7837 
7838         See Also
7839         --------
7840         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
7841 
7842         Examples
7843         --------
7844         >>> array1 = xr.DataArray(
7845         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
7846         ... )
7847         >>> array2 = xr.DataArray(
7848         ...     [
7849         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
7850         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
7851         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
7852         ...     ],
7853         ...     dims=["y", "x"],
7854         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
7855         ... )
7856         >>> ds = xr.Dataset({"int": array1, "float": array2})
7857         >>> ds.min(dim="x")
7858         <xarray.Dataset>
7859         Dimensions:  (y: 3)
7860         Coordinates:
7861           * y        (y) int64 -1 0 1
7862         Data variables:
7863             int      int64 -2
7864             float    (y) float64 -2.0 -4.0 1.0
7865         >>> ds.argmin(dim="x")
7866         <xarray.Dataset>
7867         Dimensions:  (y: 3)
7868         Coordinates:
7869           * y        (y) int64 -1 0 1
7870         Data variables:
7871             int      int64 4
7872             float    (y) int64 4 0 2
7873         >>> ds.idxmin(dim="x")
7874         <xarray.Dataset>
7875         Dimensions:  (y: 3)
7876         Coordinates:
7877           * y        (y) int64 -1 0 1
7878         Data variables:
7879             int      <U1 'e'
7880             float    (y) object 'e' 'a' 'c'
7881         """
7882         return self.map(
7883             methodcaller(
7884                 "idxmin",
7885                 dim=dim,
7886                 skipna=skipna,
7887                 fill_value=fill_value,
7888                 keep_attrs=keep_attrs,
7889             )
7890         )
7891 
7892     def idxmax(
7893         self: T_Dataset,
7894         dim: Hashable | None = None,
7895         skipna: bool | None = None,
7896         fill_value: Any = xrdtypes.NA,
7897         keep_attrs: bool | None = None,
7898     ) -> T_Dataset:
7899         """Return the coordinate label of the maximum value along a dimension.
7900 
7901         Returns a new `Dataset` named after the dimension with the values of
7902         the coordinate labels along that dimension corresponding to maximum
7903         values along that dimension.
7904 
7905         In comparison to :py:meth:`~Dataset.argmax`, this returns the
7906         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
7907 
7908         Parameters
7909         ----------
7910         dim : str, optional
7911             Dimension over which to apply `idxmax`.  This is optional for 1D
7912             variables, but required for variables with 2 or more dimensions.
7913         skipna : bool or None, optional
7914             If True, skip missing values (as marked by NaN). By default, only
7915             skips missing values for ``float``, ``complex``, and ``object``
7916             dtypes; other dtypes either do not have a sentinel missing value
7917             (``int``) or ``skipna=True`` has not been implemented
7918             (``datetime64`` or ``timedelta64``).
7919         fill_value : Any, default: NaN
7920             Value to be filled in case all of the values along a dimension are
7921             null.  By default this is NaN.  The fill value and result are
7922             automatically converted to a compatible dtype if possible.
7923             Ignored if ``skipna`` is False.
7924         keep_attrs : bool or None, optional
7925             If True, the attributes (``attrs``) will be copied from the
7926             original object to the new one. If False, the new object
7927             will be returned without attributes.
7928 
7929         Returns
7930         -------
7931         reduced : Dataset
7932             New `Dataset` object with `idxmax` applied to its data and the
7933             indicated dimension removed.
7934 
7935         See Also
7936         --------
7937         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
7938 
7939         Examples
7940         --------
7941         >>> array1 = xr.DataArray(
7942         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
7943         ... )
7944         >>> array2 = xr.DataArray(
7945         ...     [
7946         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
7947         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
7948         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
7949         ...     ],
7950         ...     dims=["y", "x"],
7951         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
7952         ... )
7953         >>> ds = xr.Dataset({"int": array1, "float": array2})
7954         >>> ds.max(dim="x")
7955         <xarray.Dataset>
7956         Dimensions:  (y: 3)
7957         Coordinates:
7958           * y        (y) int64 -1 0 1
7959         Data variables:
7960             int      int64 2
7961             float    (y) float64 2.0 2.0 1.0
7962         >>> ds.argmax(dim="x")
7963         <xarray.Dataset>
7964         Dimensions:  (y: 3)
7965         Coordinates:
7966           * y        (y) int64 -1 0 1
7967         Data variables:
7968             int      int64 1
7969             float    (y) int64 0 2 2
7970         >>> ds.idxmax(dim="x")
7971         <xarray.Dataset>
7972         Dimensions:  (y: 3)
7973         Coordinates:
7974           * y        (y) int64 -1 0 1
7975         Data variables:
7976             int      <U1 'b'
7977             float    (y) object 'a' 'c' 'c'
7978         """
7979         return self.map(
7980             methodcaller(
7981                 "idxmax",
7982                 dim=dim,
7983                 skipna=skipna,
7984                 fill_value=fill_value,
7985                 keep_attrs=keep_attrs,
7986             )
7987         )
7988 
7989     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
7990         """Indices of the minima of the member variables.
7991 
7992         If there are multiple minima, the indices of the first one found will be
7993         returned.
7994 
7995         Parameters
7996         ----------
7997         dim : Hashable, optional
7998             The dimension over which to find the minimum. By default, finds minimum over
7999             all dimensions - for now returning an int for backward compatibility, but
8000             this is deprecated, in future will be an error, since DataArray.argmin will
8001             return a dict with indices for all dimensions, which does not make sense for
8002             a Dataset.
8003         keep_attrs : bool, optional
8004             If True, the attributes (`attrs`) will be copied from the original
8005             object to the new one.  If False (default), the new object will be
8006             returned without attributes.
8007         skipna : bool, optional
8008             If True, skip missing values (as marked by NaN). By default, only
8009             skips missing values for float dtypes; other dtypes either do not
8010             have a sentinel missing value (int) or skipna=True has not been
8011             implemented (object, datetime64 or timedelta64).
8012 
8013         Returns
8014         -------
8015         result : Dataset
8016 
8017         See Also
8018         --------
8019         DataArray.argmin
8020         """
8021         if dim is None:
8022             warnings.warn(
8023                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8024                 "dim changes to return a dict of indices of each dimension, for "
8025                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8026                 "since we don't return a dict of Datasets.",
8027                 DeprecationWarning,
8028                 stacklevel=2,
8029             )
8030         if (
8031             dim is None
8032             or (not isinstance(dim, Sequence) and dim is not ...)
8033             or isinstance(dim, str)
8034         ):
8035             # Return int index if single dimension is passed, and is not part of a
8036             # sequence
8037             argmin_func = getattr(duck_array_ops, "argmin")
8038             return self.reduce(argmin_func, dim=dim, **kwargs)
8039         else:
8040             raise ValueError(
8041                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8042                 "dicts cannot be contained in a Dataset, so cannot call "
8043                 "Dataset.argmin() with a sequence or ... for dim"
8044             )
8045 
8046     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8047         """Indices of the maxima of the member variables.
8048 
8049         If there are multiple maxima, the indices of the first one found will be
8050         returned.
8051 
8052         Parameters
8053         ----------
8054         dim : str, optional
8055             The dimension over which to find the maximum. By default, finds maximum over
8056             all dimensions - for now returning an int for backward compatibility, but
8057             this is deprecated, in future will be an error, since DataArray.argmax will
8058             return a dict with indices for all dimensions, which does not make sense for
8059             a Dataset.
8060         keep_attrs : bool, optional
8061             If True, the attributes (`attrs`) will be copied from the original
8062             object to the new one.  If False (default), the new object will be
8063             returned without attributes.
8064         skipna : bool, optional
8065             If True, skip missing values (as marked by NaN). By default, only
8066             skips missing values for float dtypes; other dtypes either do not
8067             have a sentinel missing value (int) or skipna=True has not been
8068             implemented (object, datetime64 or timedelta64).
8069 
8070         Returns
8071         -------
8072         result : Dataset
8073 
8074         See Also
8075         --------
8076         DataArray.argmax
8077 
8078         """
8079         if dim is None:
8080             warnings.warn(
8081                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8082                 "dim changes to return a dict of indices of each dimension, for "
8083                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8084                 "since we don't return a dict of Datasets.",
8085                 DeprecationWarning,
8086                 stacklevel=2,
8087             )
8088         if (
8089             dim is None
8090             or (not isinstance(dim, Sequence) and dim is not ...)
8091             or isinstance(dim, str)
8092         ):
8093             # Return int index if single dimension is passed, and is not part of a
8094             # sequence
8095             argmax_func = getattr(duck_array_ops, "argmax")
8096             return self.reduce(argmax_func, dim=dim, **kwargs)
8097         else:
8098             raise ValueError(
8099                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8100                 "dicts cannot be contained in a Dataset, so cannot call "
8101                 "Dataset.argmin() with a sequence or ... for dim"
8102             )
8103 
8104     def query(
8105         self: T_Dataset,
8106         queries: Mapping[Any, Any] | None = None,
8107         parser: QueryParserOptions = "pandas",
8108         engine: QueryEngineOptions = None,
8109         missing_dims: ErrorOptionsWithWarn = "raise",
8110         **queries_kwargs: Any,
8111     ) -> T_Dataset:
8112         """Return a new dataset with each array indexed along the specified
8113         dimension(s), where the indexers are given as strings containing
8114         Python expressions to be evaluated against the data variables in the
8115         dataset.
8116 
8117         Parameters
8118         ----------
8119         queries : dict-like, optional
8120             A dict-like with keys matching dimensions and values given by strings
8121             containing Python expressions to be evaluated against the data variables
8122             in the dataset. The expressions will be evaluated using the pandas
8123             eval() function, and can contain any valid Python expressions but cannot
8124             contain any Python statements.
8125         parser : {"pandas", "python"}, default: "pandas"
8126             The parser to use to construct the syntax tree from the expression.
8127             The default of 'pandas' parses code slightly different than standard
8128             Python. Alternatively, you can parse an expression using the 'python'
8129             parser to retain strict Python semantics.
8130         engine : {"python", "numexpr", None}, default: None
8131             The engine used to evaluate the expression. Supported engines are:
8132 
8133             - None: tries to use numexpr, falls back to python
8134             - "numexpr": evaluates expressions using numexpr
8135             - "python": performs operations as if you had eval’d in top level python
8136 
8137         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8138             What to do if dimensions that should be selected from are not present in the
8139             Dataset:
8140 
8141             - "raise": raise an exception
8142             - "warn": raise a warning, and ignore the missing dimensions
8143             - "ignore": ignore the missing dimensions
8144 
8145         **queries_kwargs : {dim: query, ...}, optional
8146             The keyword arguments form of ``queries``.
8147             One of queries or queries_kwargs must be provided.
8148 
8149         Returns
8150         -------
8151         obj : Dataset
8152             A new Dataset with the same contents as this dataset, except each
8153             array and dimension is indexed by the results of the appropriate
8154             queries.
8155 
8156         See Also
8157         --------
8158         Dataset.isel
8159         pandas.eval
8160 
8161         Examples
8162         --------
8163         >>> a = np.arange(0, 5, 1)
8164         >>> b = np.linspace(0, 1, 5)
8165         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8166         >>> ds
8167         <xarray.Dataset>
8168         Dimensions:  (x: 5)
8169         Dimensions without coordinates: x
8170         Data variables:
8171             a        (x) int64 0 1 2 3 4
8172             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8173         >>> ds.query(x="a > 2")
8174         <xarray.Dataset>
8175         Dimensions:  (x: 2)
8176         Dimensions without coordinates: x
8177         Data variables:
8178             a        (x) int64 3 4
8179             b        (x) float64 0.75 1.0
8180         """
8181 
8182         # allow queries to be given either as a dict or as kwargs
8183         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8184 
8185         # check queries
8186         for dim, expr in queries.items():
8187             if not isinstance(expr, str):
8188                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8189                 raise ValueError(msg)
8190 
8191         # evaluate the queries to create the indexers
8192         indexers = {
8193             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8194             for dim, expr in queries.items()
8195         }
8196 
8197         # apply the selection
8198         return self.isel(indexers, missing_dims=missing_dims)
8199 
8200     def curvefit(
8201         self: T_Dataset,
8202         coords: str | DataArray | Iterable[str | DataArray],
8203         func: Callable[..., Any],
8204         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8205         skipna: bool = True,
8206         p0: dict[str, Any] | None = None,
8207         bounds: dict[str, Any] | None = None,
8208         param_names: Sequence[str] | None = None,
8209         kwargs: dict[str, Any] | None = None,
8210     ) -> T_Dataset:
8211         """
8212         Curve fitting optimization for arbitrary functions.
8213 
8214         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8215 
8216         Parameters
8217         ----------
8218         coords : hashable, DataArray, or sequence of hashable or DataArray
8219             Independent coordinate(s) over which to perform the curve fitting. Must share
8220             at least one dimension with the calling object. When fitting multi-dimensional
8221             functions, supply `coords` as a sequence in the same order as arguments in
8222             `func`. To fit along existing dimensions of the calling object, `coords` can
8223             also be specified as a str or sequence of strs.
8224         func : callable
8225             User specified function in the form `f(x, *params)` which returns a numpy
8226             array of length `len(x)`. `params` are the fittable parameters which are optimized
8227             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8228             coordinates, e.g. `f((x0, x1), *params)`.
8229         reduce_dims : hashable or sequence of hashable
8230             Additional dimension(s) over which to aggregate while fitting. For example,
8231             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8232             aggregate all lat and lon points and fit the specified function along the
8233             time dimension.
8234         skipna : bool, default: True
8235             Whether to skip missing values when fitting. Default is True.
8236         p0 : dict-like, optional
8237             Optional dictionary of parameter names to initial guesses passed to the
8238             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8239             be assigned initial values following the default scipy behavior.
8240         bounds : dict-like, optional
8241             Optional dictionary of parameter names to bounding values passed to the
8242             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8243             will be unbounded following the default scipy behavior.
8244         param_names : sequence of hashable, optional
8245             Sequence of names for the fittable parameters of `func`. If not supplied,
8246             this will be automatically determined by arguments of `func`. `param_names`
8247             should be manually supplied when fitting a function that takes a variable
8248             number of parameters.
8249         **kwargs : optional
8250             Additional keyword arguments to passed to scipy curve_fit.
8251 
8252         Returns
8253         -------
8254         curvefit_results : Dataset
8255             A single dataset which contains:
8256 
8257             [var]_curvefit_coefficients
8258                 The coefficients of the best fit.
8259             [var]_curvefit_covariance
8260                 The covariance matrix of the coefficient estimates.
8261 
8262         See Also
8263         --------
8264         Dataset.polyfit
8265         scipy.optimize.curve_fit
8266         """
8267         from scipy.optimize import curve_fit
8268 
8269         from .alignment import broadcast
8270         from .computation import apply_ufunc
8271         from .dataarray import _THIS_ARRAY, DataArray
8272 
8273         if p0 is None:
8274             p0 = {}
8275         if bounds is None:
8276             bounds = {}
8277         if kwargs is None:
8278             kwargs = {}
8279 
8280         if not reduce_dims:
8281             reduce_dims_ = []
8282         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8283             reduce_dims_ = [reduce_dims]
8284         else:
8285             reduce_dims_ = list(reduce_dims)
8286 
8287         if (
8288             isinstance(coords, str)
8289             or isinstance(coords, DataArray)
8290             or not isinstance(coords, Iterable)
8291         ):
8292             coords = [coords]
8293         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8294 
8295         # Determine whether any coords are dims on self
8296         for coord in coords_:
8297             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8298         reduce_dims_ = list(set(reduce_dims_))
8299         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8300         if not reduce_dims_:
8301             raise ValueError(
8302                 "No arguments to `coords` were identified as a dimension on the calling "
8303                 "object, and no dims were supplied to `reduce_dims`. This would result "
8304                 "in fitting on scalar data."
8305             )
8306 
8307         # Broadcast all coords with each other
8308         coords_ = broadcast(*coords_)
8309         coords_ = [
8310             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8311         ]
8312 
8313         params, func_args = _get_func_args(func, param_names)
8314         param_defaults, bounds_defaults = _initialize_curvefit_params(
8315             params, p0, bounds, func_args
8316         )
8317         n_params = len(params)
8318         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8319         kwargs.setdefault(
8320             "bounds",
8321             [
8322                 [bounds_defaults[p][0] for p in params],
8323                 [bounds_defaults[p][1] for p in params],
8324             ],
8325         )
8326 
8327         def _wrapper(Y, *coords_, **kwargs):
8328             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8329             x = np.vstack([c.ravel() for c in coords_])
8330             y = Y.ravel()
8331             if skipna:
8332                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8333                 x = x[:, mask]
8334                 y = y[mask]
8335                 if not len(y):
8336                     popt = np.full([n_params], np.nan)
8337                     pcov = np.full([n_params, n_params], np.nan)
8338                     return popt, pcov
8339             x = np.squeeze(x)
8340             popt, pcov = curve_fit(func, x, y, **kwargs)
8341             return popt, pcov
8342 
8343         result = type(self)()
8344         for name, da in self.data_vars.items():
8345             if name is _THIS_ARRAY:
8346                 name = ""
8347             else:
8348                 name = f"{str(name)}_"
8349 
8350             popt, pcov = apply_ufunc(
8351                 _wrapper,
8352                 da,
8353                 *coords_,
8354                 vectorize=True,
8355                 dask="parallelized",
8356                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8357                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8358                 dask_gufunc_kwargs={
8359                     "output_sizes": {
8360                         "param": n_params,
8361                         "cov_i": n_params,
8362                         "cov_j": n_params,
8363                     },
8364                 },
8365                 output_dtypes=(np.float64, np.float64),
8366                 exclude_dims=set(reduce_dims_),
8367                 kwargs=kwargs,
8368             )
8369             result[name + "curvefit_coefficients"] = popt
8370             result[name + "curvefit_covariance"] = pcov
8371 
8372         result = result.assign_coords(
8373             {"param": params, "cov_i": params, "cov_j": params}
8374         )
8375         result.attrs = self.attrs.copy()
8376 
8377         return result
8378 
8379     def drop_duplicates(
8380         self: T_Dataset,
8381         dim: Hashable | Iterable[Hashable],
8382         keep: Literal["first", "last", False] = "first",
8383     ) -> T_Dataset:
8384         """Returns a new Dataset with duplicate dimension values removed.
8385 
8386         Parameters
8387         ----------
8388         dim : dimension label or labels
8389             Pass `...` to drop duplicates along all dimensions.
8390         keep : {"first", "last", False}, default: "first"
8391             Determines which duplicates (if any) to keep.
8392             - ``"first"`` : Drop duplicates except for the first occurrence.
8393             - ``"last"`` : Drop duplicates except for the last occurrence.
8394             - False : Drop all duplicates.
8395 
8396         Returns
8397         -------
8398         Dataset
8399 
8400         See Also
8401         --------
8402         DataArray.drop_duplicates
8403         """
8404         if isinstance(dim, str):
8405             dims: Iterable = (dim,)
8406         elif dim is ...:
8407             dims = self.dims
8408         elif not isinstance(dim, Iterable):
8409             dims = [dim]
8410         else:
8411             dims = dim
8412 
8413         missing_dims = set(dims) - set(self.dims)
8414         if missing_dims:
8415             raise ValueError(f"'{missing_dims}' not found in dimensions")
8416 
8417         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8418         return self.isel(indexes)
8419 
8420     def convert_calendar(
8421         self: T_Dataset,
8422         calendar: CFCalendar,
8423         dim: Hashable = "time",
8424         align_on: Literal["date", "year", None] = None,
8425         missing: Any | None = None,
8426         use_cftime: bool | None = None,
8427     ) -> T_Dataset:
8428         """Convert the Dataset to another calendar.
8429 
8430         Only converts the individual timestamps, does not modify any data except
8431         in dropping invalid/surplus dates or inserting missing dates.
8432 
8433         If the source and target calendars are either no_leap, all_leap or a
8434         standard type, only the type of the time array is modified.
8435         When converting to a leap year from a non-leap year, the 29th of February
8436         is removed from the array. In the other direction the 29th of February
8437         will be missing in the output, unless `missing` is specified,
8438         in which case that value is inserted.
8439 
8440         For conversions involving `360_day` calendars, see Notes.
8441 
8442         This method is safe to use with sub-daily data as it doesn't touch the
8443         time part of the timestamps.
8444 
8445         Parameters
8446         ---------
8447         calendar : str
8448             The target calendar name.
8449         dim : Hashable, default: "time"
8450             Name of the time coordinate.
8451         align_on : {None, 'date', 'year'}, optional
8452             Must be specified when either source or target is a `360_day` calendar,
8453             ignored otherwise. See Notes.
8454         missing : Any or None, optional
8455             By default, i.e. if the value is None, this method will simply attempt
8456             to convert the dates in the source calendar to the same dates in the
8457             target calendar, and drop any of those that are not possible to
8458             represent.  If a value is provided, a new time coordinate will be
8459             created in the target calendar with the same frequency as the original
8460             time coordinate; for any dates that are not present in the source, the
8461             data will be filled with this value.  Note that using this mode requires
8462             that the source data have an inferable frequency; for more information
8463             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8464             target calendar combinations, this could result in many missing values, see notes.
8465         use_cftime : bool or None, optional
8466             Whether to use cftime objects in the output, only used if `calendar`
8467             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8468             If True, the new time axis uses cftime objects.
8469             If None (default), it uses :py:class:`numpy.datetime64` values if the
8470             date range permits it, and :py:class:`cftime.datetime` objects if not.
8471             If False, it uses :py:class:`numpy.datetime64`  or fails.
8472 
8473         Returns
8474         -------
8475         Dataset
8476             Copy of the dataarray with the time coordinate converted to the
8477             target calendar. If 'missing' was None (default), invalid dates in
8478             the new calendar are dropped, but missing dates are not inserted.
8479             If `missing` was given, the new data is reindexed to have a time axis
8480             with the same frequency as the source, but in the new calendar; any
8481             missing datapoints are filled with `missing`.
8482 
8483         Notes
8484         -----
8485         Passing a value to `missing` is only usable if the source's time coordinate as an
8486         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8487         if the target coordinate, generated from this frequency, has dates equivalent to the
8488         source. It is usually **not** appropriate to use this mode with:
8489 
8490         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8491         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8492             or 'mH' where 24 % m != 0).
8493 
8494         If one of the source or target calendars is `"360_day"`, `align_on` must
8495         be specified and two options are offered.
8496 
8497         - "year"
8498             The dates are translated according to their relative position in the year,
8499             ignoring their original month and day information, meaning that the
8500             missing/surplus days are added/removed at regular intervals.
8501 
8502             From a `360_day` to a standard calendar, the output will be missing the
8503             following dates (day of year in parentheses):
8504 
8505             To a leap year:
8506                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8507                 September 31st (275) and November 30th (335).
8508             To a non-leap year:
8509                 February 6th (36), April 19th (109), July 2nd (183),
8510                 September 12th (255), November 25th (329).
8511 
8512             From a standard calendar to a `"360_day"`, the following dates in the
8513             source array will be dropped:
8514 
8515             From a leap year:
8516                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8517                 September 31st (275), December 1st (336)
8518             From a non-leap year:
8519                 February 6th (37), April 20th (110), July 2nd (183),
8520                 September 13th (256), November 25th (329)
8521 
8522             This option is best used on daily and subdaily data.
8523 
8524         - "date"
8525             The month/day information is conserved and invalid dates are dropped
8526             from the output. This means that when converting from a `"360_day"` to a
8527             standard calendar, all 31st (Jan, March, May, July, August, October and
8528             December) will be missing as there is no equivalent dates in the
8529             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8530             will be dropped as there are no equivalent dates in a standard calendar.
8531 
8532             This option is best used with data on a frequency coarser than daily.
8533         """
8534         return convert_calendar(
8535             self,
8536             calendar,
8537             dim=dim,
8538             align_on=align_on,
8539             missing=missing,
8540             use_cftime=use_cftime,
8541         )
8542 
8543     def interp_calendar(
8544         self: T_Dataset,
8545         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8546         dim: Hashable = "time",
8547     ) -> T_Dataset:
8548         """Interpolates the Dataset to another calendar based on decimal year measure.
8549 
8550         Each timestamp in `source` and `target` are first converted to their decimal
8551         year equivalent then `source` is interpolated on the target coordinate.
8552         The decimal year of a timestamp is its year plus its sub-year component
8553         converted to the fraction of its year. For example "2000-03-01 12:00" is
8554         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8555 
8556         This method should only be used when the time (HH:MM:SS) information of
8557         time coordinate is not important.
8558 
8559         Parameters
8560         ----------
8561         target: DataArray or DatetimeIndex or CFTimeIndex
8562             The target time coordinate of a valid dtype
8563             (np.datetime64 or cftime objects)
8564         dim : Hashable, default: "time"
8565             The time coordinate name.
8566 
8567         Return
8568         ------
8569         DataArray
8570             The source interpolated on the decimal years of target,
8571         """
8572         return interp_calendar(self, target, dim=dim)
8573 
8574     def groupby(
8575         self,
8576         group: Hashable | DataArray | IndexVariable,
8577         squeeze: bool = True,
8578         restore_coord_dims: bool = False,
8579     ) -> DatasetGroupBy:
8580         """Returns a DatasetGroupBy object for performing grouped operations.
8581 
8582         Parameters
8583         ----------
8584         group : Hashable, DataArray or IndexVariable
8585             Array whose unique values should be used to group this array. If a
8586             string, must be the name of a variable contained in this dataset.
8587         squeeze : bool, default: True
8588             If "group" is a dimension of any arrays in this dataset, `squeeze`
8589             controls whether the subarrays have a dimension of length 1 along
8590             that dimension or if the dimension is squeezed out.
8591         restore_coord_dims : bool, default: False
8592             If True, also restore the dimension order of multi-dimensional
8593             coordinates.
8594 
8595         Returns
8596         -------
8597         grouped : DatasetGroupBy
8598             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8599             iterated over in the form of `(unique_value, grouped_array)` pairs.
8600 
8601         See Also
8602         --------
8603         Dataset.groupby_bins
8604         DataArray.groupby
8605         core.groupby.DatasetGroupBy
8606         pandas.DataFrame.groupby
8607         """
8608         from .groupby import DatasetGroupBy
8609 
8610         # While we don't generally check the type of every arg, passing
8611         # multiple dimensions as multiple arguments is common enough, and the
8612         # consequences hidden enough (strings evaluate as true) to warrant
8613         # checking here.
8614         # A future version could make squeeze kwarg only, but would face
8615         # backward-compat issues.
8616         if not isinstance(squeeze, bool):
8617             raise TypeError(
8618                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8619             )
8620 
8621         return DatasetGroupBy(
8622             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8623         )
8624 
8625     def groupby_bins(
8626         self,
8627         group: Hashable | DataArray | IndexVariable,
8628         bins: ArrayLike,
8629         right: bool = True,
8630         labels: ArrayLike | None = None,
8631         precision: int = 3,
8632         include_lowest: bool = False,
8633         squeeze: bool = True,
8634         restore_coord_dims: bool = False,
8635     ) -> DatasetGroupBy:
8636         """Returns a DatasetGroupBy object for performing grouped operations.
8637 
8638         Rather than using all unique values of `group`, the values are discretized
8639         first by applying `pandas.cut` [1]_ to `group`.
8640 
8641         Parameters
8642         ----------
8643         group : Hashable, DataArray or IndexVariable
8644             Array whose binned values should be used to group this array. If a
8645             string, must be the name of a variable contained in this dataset.
8646         bins : int or array-like
8647             If bins is an int, it defines the number of equal-width bins in the
8648             range of x. However, in this case, the range of x is extended by .1%
8649             on each side to include the min or max values of x. If bins is a
8650             sequence it defines the bin edges allowing for non-uniform bin
8651             width. No extension of the range of x is done in this case.
8652         right : bool, default: True
8653             Indicates whether the bins include the rightmost edge or not. If
8654             right == True (the default), then the bins [1,2,3,4] indicate
8655             (1,2], (2,3], (3,4].
8656         labels : array-like or bool, default: None
8657             Used as labels for the resulting bins. Must be of the same length as
8658             the resulting bins. If False, string bin labels are assigned by
8659             `pandas.cut`.
8660         precision : int, default: 3
8661             The precision at which to store and display the bins labels.
8662         include_lowest : bool, default: False
8663             Whether the first interval should be left-inclusive or not.
8664         squeeze : bool, default: True
8665             If "group" is a dimension of any arrays in this dataset, `squeeze`
8666             controls whether the subarrays have a dimension of length 1 along
8667             that dimension or if the dimension is squeezed out.
8668         restore_coord_dims : bool, default: False
8669             If True, also restore the dimension order of multi-dimensional
8670             coordinates.
8671 
8672         Returns
8673         -------
8674         grouped : DatasetGroupBy
8675             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8676             iterated over in the form of `(unique_value, grouped_array)` pairs.
8677             The name of the group has the added suffix `_bins` in order to
8678             distinguish it from the original variable.
8679 
8680         See Also
8681         --------
8682         Dataset.groupby
8683         DataArray.groupby_bins
8684         core.groupby.DatasetGroupBy
8685         pandas.DataFrame.groupby
8686 
8687         References
8688         ----------
8689         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8690         """
8691         from .groupby import DatasetGroupBy
8692 
8693         return DatasetGroupBy(
8694             self,
8695             group,
8696             squeeze=squeeze,
8697             bins=bins,
8698             restore_coord_dims=restore_coord_dims,
8699             cut_kwargs={
8700                 "right": right,
8701                 "labels": labels,
8702                 "precision": precision,
8703                 "include_lowest": include_lowest,
8704             },
8705         )
8706 
8707     def weighted(self, weights: DataArray) -> DatasetWeighted:
8708         """
8709         Weighted Dataset operations.
8710 
8711         Parameters
8712         ----------
8713         weights : DataArray
8714             An array of weights associated with the values in this Dataset.
8715             Each value in the data contributes to the reduction operation
8716             according to its associated weight.
8717 
8718         Notes
8719         -----
8720         ``weights`` must be a DataArray and cannot contain missing values.
8721         Missing values can be replaced by ``weights.fillna(0)``.
8722 
8723         Returns
8724         -------
8725         core.weighted.DatasetWeighted
8726 
8727         See Also
8728         --------
8729         DataArray.weighted
8730         """
8731         from .weighted import DatasetWeighted
8732 
8733         return DatasetWeighted(self, weights)
8734 
8735     def rolling(
8736         self,
8737         dim: Mapping[Any, int] | None = None,
8738         min_periods: int | None = None,
8739         center: bool | Mapping[Any, bool] = False,
8740         **window_kwargs: int,
8741     ) -> DatasetRolling:
8742         """
8743         Rolling window object for Datasets.
8744 
8745         Parameters
8746         ----------
8747         dim : dict, optional
8748             Mapping from the dimension name to create the rolling iterator
8749             along (e.g. `time`) to its moving window size.
8750         min_periods : int or None, default: None
8751             Minimum number of observations in window required to have a value
8752             (otherwise result is NA). The default, None, is equivalent to
8753             setting min_periods equal to the size of the window.
8754         center : bool or Mapping to int, default: False
8755             Set the labels at the center of the window.
8756         **window_kwargs : optional
8757             The keyword arguments form of ``dim``.
8758             One of dim or window_kwargs must be provided.
8759 
8760         Returns
8761         -------
8762         core.rolling.DatasetRolling
8763 
8764         See Also
8765         --------
8766         core.rolling.DatasetRolling
8767         DataArray.rolling
8768         """
8769         from .rolling import DatasetRolling
8770 
8771         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
8772         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
8773 
8774     def coarsen(
8775         self,
8776         dim: Mapping[Any, int] | None = None,
8777         boundary: CoarsenBoundaryOptions = "exact",
8778         side: SideOptions | Mapping[Any, SideOptions] = "left",
8779         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
8780         **window_kwargs: int,
8781     ) -> DatasetCoarsen:
8782         """
8783         Coarsen object for Datasets.
8784 
8785         Parameters
8786         ----------
8787         dim : mapping of hashable to int, optional
8788             Mapping from the dimension name to the window size.
8789         boundary : {"exact", "trim", "pad"}, default: "exact"
8790             If 'exact', a ValueError will be raised if dimension size is not a
8791             multiple of the window size. If 'trim', the excess entries are
8792             dropped. If 'pad', NA will be padded.
8793         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
8794         coord_func : str or mapping of hashable to str, default: "mean"
8795             function (name) that is applied to the coordinates,
8796             or a mapping from coordinate name to function (name).
8797 
8798         Returns
8799         -------
8800         core.rolling.DatasetCoarsen
8801 
8802         See Also
8803         --------
8804         core.rolling.DatasetCoarsen
8805         DataArray.coarsen
8806         """
8807         from .rolling import DatasetCoarsen
8808 
8809         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
8810         return DatasetCoarsen(
8811             self,
8812             dim,
8813             boundary=boundary,
8814             side=side,
8815             coord_func=coord_func,
8816         )
8817 
8818     def resample(
8819         self,
8820         indexer: Mapping[Any, str] | None = None,
8821         skipna: bool | None = None,
8822         closed: SideOptions | None = None,
8823         label: SideOptions | None = None,
8824         base: int = 0,
8825         keep_attrs: bool | None = None,
8826         loffset: datetime.timedelta | str | None = None,
8827         restore_coord_dims: bool | None = None,
8828         **indexer_kwargs: str,
8829     ) -> DatasetResample:
8830         """Returns a Resample object for performing resampling operations.
8831 
8832         Handles both downsampling and upsampling. The resampled
8833         dimension must be a datetime-like coordinate. If any intervals
8834         contain no values from the original object, they will be given
8835         the value ``NaN``.
8836 
8837         Parameters
8838         ----------
8839         indexer : Mapping of Hashable to str, optional
8840             Mapping from the dimension name to resample frequency [1]_. The
8841             dimension must be datetime-like.
8842         skipna : bool, optional
8843             Whether to skip missing values when aggregating in downsampling.
8844         closed : {"left", "right"}, optional
8845             Side of each interval to treat as closed.
8846         label : {"left", "right"}, optional
8847             Side of each interval to use for labeling.
8848         base : int, default = 0
8849             For frequencies that evenly subdivide 1 day, the "origin" of the
8850             aggregated intervals. For example, for "24H" frequency, base could
8851             range from 0 through 23.
8852         loffset : timedelta or str, optional
8853             Offset used to adjust the resampled time labels. Some pandas date
8854             offset strings are supported.
8855         restore_coord_dims : bool, optional
8856             If True, also restore the dimension order of multi-dimensional
8857             coordinates.
8858         **indexer_kwargs : str
8859             The keyword arguments form of ``indexer``.
8860             One of indexer or indexer_kwargs must be provided.
8861 
8862         Returns
8863         -------
8864         resampled : core.resample.DataArrayResample
8865             This object resampled.
8866 
8867         See Also
8868         --------
8869         DataArray.resample
8870         pandas.Series.resample
8871         pandas.DataFrame.resample
8872 
8873         References
8874         ----------
8875         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
8876         """
8877         from .resample import DatasetResample
8878 
8879         return self._resample(
8880             resample_cls=DatasetResample,
8881             indexer=indexer,
8882             skipna=skipna,
8883             closed=closed,
8884             label=label,
8885             base=base,
8886             keep_attrs=keep_attrs,
8887             loffset=loffset,
8888             restore_coord_dims=restore_coord_dims,
8889             **indexer_kwargs,
8890         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 04:16:29.174 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>rename</method>
<code>
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)

</code><intended_behavior>The `rename` method should check if the renaming operation converts a coordinate into a dimension coordinate and, if so, automatically generate an appropriate index for this new dimension coordinate. This ensures subsequent operations that rely on this index, such as `sel`, can proceed without raising errors. The preservation of existing functionality is crucial; thus, any change should only augment the current capabilities with additional checks for index management when renaming coordinates to dimension coordinates.</intended_behavior>, <file>xarray/core/dataarray.py</file>
<class>DataArray</class>
<method>None</method>
<code>
223 class DataArray(
224     AbstractArray, DataWithCoords, DataArrayArithmetic, DataArrayReductions
225 ):
226     """N-dimensional array with labeled coordinates and dimensions.
227 
228     DataArray provides a wrapper around numpy ndarrays that uses
229     labeled dimensions and coordinates to support metadata aware
230     operations. The API is similar to that for the pandas Series or
231     DataFrame, but DataArray objects can have any number of dimensions,
232     and their contents have fixed data types.
233 
234     Additional features over raw numpy arrays:
235 
236     - Apply operations over dimensions by name: ``x.sum('time')``.
237     - Select or assign values by integer location (like numpy):
238       ``x[:10]`` or by label (like pandas): ``x.loc['2014-01-01']`` or
239       ``x.sel(time='2014-01-01')``.
240     - Mathematical operations (e.g., ``x - y``) vectorize across
241       multiple dimensions (known in numpy as "broadcasting") based on
242       dimension names, regardless of their original order.
243     - Keep track of arbitrary metadata in the form of a Python
244       dictionary: ``x.attrs``
245     - Convert to a pandas Series: ``x.to_series()``.
246 
247     Getting items from or doing mathematical operations with a
248     DataArray always returns another DataArray.
249 
250     Parameters
251     ----------
252     data : array_like
253         Values for this array. Must be an ``numpy.ndarray``, ndarray
254         like, or castable to an ``ndarray``. If a self-described xarray
255         or pandas object, attempts are made to use this array's
256         metadata to fill in other unspecified arguments. A view of the
257         array's data is used instead of a copy if possible.
258     coords : sequence or dict of array_like, optional
259         Coordinates (tick labels) to use for indexing along each
260         dimension. The following notations are accepted:
261 
262         - mapping {dimension name: array-like}
263         - sequence of tuples that are valid arguments for
264           ``xarray.Variable()``
265           - (dims, data)
266           - (dims, data, attrs)
267           - (dims, data, attrs, encoding)
268 
269         Additionally, it is possible to define a coord whose name
270         does not match the dimension name, or a coord based on multiple
271         dimensions, with one of the following notations:
272 
273         - mapping {coord name: DataArray}
274         - mapping {coord name: Variable}
275         - mapping {coord name: (dimension name, array-like)}
276         - mapping {coord name: (tuple of dimension names, array-like)}
277 
278     dims : Hashable or sequence of Hashable, optional
279         Name(s) of the data dimension(s). Must be either a Hashable
280         (only for 1D data) or a sequence of Hashables with length equal
281         to the number of dimensions. If this argument is omitted,
282         dimension names are taken from ``coords`` (if possible) and
283         otherwise default to ``['dim_0', ... 'dim_n']``.
284     name : str or None, optional
285         Name of this array.
286     attrs : dict_like or None, optional
287         Attributes to assign to the new instance. By default, an empty
288         attribute dictionary is initialized.
289 
290     Examples
291     --------
292     Create data:
293 
294     >>> np.random.seed(0)
295     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
296     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
297     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
298     >>> time = pd.date_range("2014-09-06", periods=3)
299     >>> reference_time = pd.Timestamp("2014-09-05")
300 
301     Initialize a dataarray with multiple dimensions:
302 
303     >>> da = xr.DataArray(
304     ...     data=temperature,
305     ...     dims=["x", "y", "time"],
306     ...     coords=dict(
307     ...         lon=(["x", "y"], lon),
308     ...         lat=(["x", "y"], lat),
309     ...         time=time,
310     ...         reference_time=reference_time,
311     ...     ),
312     ...     attrs=dict(
313     ...         description="Ambient temperature.",
314     ...         units="degC",
315     ...     ),
316     ... )
317     >>> da
318     <xarray.DataArray (x: 2, y: 2, time: 3)>
319     array([[[29.11241877, 18.20125767, 22.82990387],
320             [32.92714559, 29.94046392,  7.18177696]],
321     <BLANKLINE>
322            [[22.60070734, 13.78914233, 14.17424919],
323             [18.28478802, 16.15234857, 26.63418806]]])
324     Coordinates:
325         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
326         lat             (x, y) float64 42.25 42.21 42.63 42.59
327       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
328         reference_time  datetime64[ns] 2014-09-05
329     Dimensions without coordinates: x, y
330     Attributes:
331         description:  Ambient temperature.
332         units:        degC
333 
334     Find out where the coldest temperature was:
335 
336     >>> da.isel(da.argmin(...))
337     <xarray.DataArray ()>
338     array(7.18177696)
339     Coordinates:
340         lon             float64 -99.32
341         lat             float64 42.21
342         time            datetime64[ns] 2014-09-08
343         reference_time  datetime64[ns] 2014-09-05
344     Attributes:
345         description:  Ambient temperature.
346         units:        degC
347     """
348 
349     _cache: dict[str, Any]
350     _coords: dict[Any, Variable]
351     _close: Callable[[], None] | None
352     _indexes: dict[Hashable, Index]
353     _name: Hashable | None
354     _variable: Variable
355 
356     __slots__ = (
357         "_cache",
358         "_coords",
359         "_close",
360         "_indexes",
361         "_name",
362         "_variable",
363         "__weakref__",
364     )
365 
366     dt = utils.UncachedAccessor(CombinedDatetimelikeAccessor["DataArray"])
367 
368     def __init__(
369         self,
370         data: Any = dtypes.NA,
371         coords: Sequence[Sequence[Any] | pd.Index | DataArray]
372         | Mapping[Any, Any]
373         | None = None,
374         dims: Hashable | Sequence[Hashable] | None = None,
375         name: Hashable = None,
376         attrs: Mapping = None,
377         # internal parameters
378         indexes: dict[Hashable, Index] = None,
379         fastpath: bool = False,
380     ) -> None:
381         if fastpath:
382             variable = data
383             assert dims is None
384             assert attrs is None
385             assert indexes is not None
386         else:
387             # TODO: (benbovy - explicit indexes) remove
388             # once it becomes part of the public interface
389             if indexes is not None:
390                 raise ValueError("Providing explicit indexes is not supported yet")
391 
392             # try to fill in arguments from data if they weren't supplied
393             if coords is None:
394 
395                 if isinstance(data, DataArray):
396                     coords = data.coords
397                 elif isinstance(data, pd.Series):
398                     coords = [data.index]
399                 elif isinstance(data, pd.DataFrame):
400                     coords = [data.index, data.columns]
401                 elif isinstance(data, (pd.Index, IndexVariable)):
402                     coords = [data]
403 
404             if dims is None:
405                 dims = getattr(data, "dims", getattr(coords, "dims", None))
406             if name is None:
407                 name = getattr(data, "name", None)
408             if attrs is None and not isinstance(data, PANDAS_TYPES):
409                 attrs = getattr(data, "attrs", None)
410 
411             data = _check_data_shape(data, coords, dims)
412             data = as_compatible_data(data)
413             coords, dims = _infer_coords_and_dims(data.shape, coords, dims)
414             variable = Variable(dims, data, attrs, fastpath=True)
415             indexes, coords = _create_indexes_from_coords(coords)
416 
417         # These fully describe a DataArray
418         self._variable = variable
419         assert isinstance(coords, dict)
420         self._coords = coords
421         self._name = name
422 
423         # TODO(shoyer): document this argument, once it becomes part of the
424         # public interface.
425         self._indexes = indexes  # type: ignore[assignment]
426 
427         self._close = None
428 
429     @classmethod
430     def _construct_direct(
431         cls: type[T_DataArray],
432         variable: Variable,
433         coords: dict[Any, Variable],
434         name: Hashable,
435         indexes: dict[Hashable, Index],
436     ) -> T_DataArray:
437         """Shortcut around __init__ for internal use when we want to skip
438         costly validation
439         """
440         obj = object.__new__(cls)
441         obj._variable = variable
442         obj._coords = coords
443         obj._name = name
444         obj._indexes = indexes
445         obj._close = None
446         return obj
447 
448     def _replace(
449         self: T_DataArray,
450         variable: Variable = None,
451         coords=None,
452         name: Hashable | None | Default = _default,
453         indexes=None,
454     ) -> T_DataArray:
455         if variable is None:
456             variable = self.variable
457         if coords is None:
458             coords = self._coords
459         if indexes is None:
460             indexes = self._indexes
461         if name is _default:
462             name = self.name
463         return type(self)(variable, coords, name=name, indexes=indexes, fastpath=True)
464 
465     def _replace_maybe_drop_dims(
466         self: T_DataArray,
467         variable: Variable,
468         name: Hashable | None | Default = _default,
469     ) -> T_DataArray:
470         if variable.dims == self.dims and variable.shape == self.shape:
471             coords = self._coords.copy()
472             indexes = self._indexes
473         elif variable.dims == self.dims:
474             # Shape has changed (e.g. from reduce(..., keepdims=True)
475             new_sizes = dict(zip(self.dims, variable.shape))
476             coords = {
477                 k: v
478                 for k, v in self._coords.items()
479                 if v.shape == tuple(new_sizes[d] for d in v.dims)
480             }
481             indexes = filter_indexes_from_coords(self._indexes, set(coords))
482         else:
483             allowed_dims = set(variable.dims)
484             coords = {
485                 k: v for k, v in self._coords.items() if set(v.dims) <= allowed_dims
486             }
487             indexes = filter_indexes_from_coords(self._indexes, set(coords))
488         return self._replace(variable, coords, name, indexes=indexes)
489 
490     def _overwrite_indexes(
491         self: T_DataArray,
492         indexes: Mapping[Any, Index],
493         coords: Mapping[Any, Variable] = None,
494         drop_coords: list[Hashable] = None,
495         rename_dims: Mapping[Any, Any] = None,
496     ) -> T_DataArray:
497         """Maybe replace indexes and their corresponding coordinates."""
498         if not indexes:
499             return self
500 
501         if coords is None:
502             coords = {}
503         if drop_coords is None:
504             drop_coords = []
505 
506         new_variable = self.variable.copy()
507         new_coords = self._coords.copy()
508         new_indexes = dict(self._indexes)
509 
510         for name in indexes:
511             new_coords[name] = coords[name]
512             new_indexes[name] = indexes[name]
513 
514         for name in drop_coords:
515             new_coords.pop(name)
516             new_indexes.pop(name)
517 
518         if rename_dims:
519             new_variable.dims = [rename_dims.get(d, d) for d in new_variable.dims]
520 
521         return self._replace(
522             variable=new_variable, coords=new_coords, indexes=new_indexes
523         )
524 
525     def _to_temp_dataset(self) -> Dataset:
526         return self._to_dataset_whole(name=_THIS_ARRAY, shallow_copy=False)
527 
528     def _from_temp_dataset(
529         self: T_DataArray, dataset: Dataset, name: Hashable | None | Default = _default
530     ) -> T_DataArray:
531         variable = dataset._variables.pop(_THIS_ARRAY)
532         coords = dataset._variables
533         indexes = dataset._indexes
534         return self._replace(variable, coords, name, indexes=indexes)
535 
536     def _to_dataset_split(self, dim: Hashable) -> Dataset:
537         """splits dataarray along dimension 'dim'"""
538 
539         def subset(dim, label):
540             array = self.loc[{dim: label}]
541             array.attrs = {}
542             return as_variable(array)
543 
544         variables = {label: subset(dim, label) for label in self.get_index(dim)}
545         variables.update({k: v for k, v in self._coords.items() if k != dim})
546         coord_names = set(self._coords) - {dim}
547         indexes = filter_indexes_from_coords(self._indexes, coord_names)
548         dataset = Dataset._construct_direct(
549             variables, coord_names, indexes=indexes, attrs=self.attrs
550         )
551         return dataset
552 
553     def _to_dataset_whole(
554         self, name: Hashable = None, shallow_copy: bool = True
555     ) -> Dataset:
556         if name is None:
557             name = self.name
558         if name is None:
559             raise ValueError(
560                 "unable to convert unnamed DataArray to a "
561                 "Dataset without providing an explicit name"
562             )
563         if name in self.coords:
564             raise ValueError(
565                 "cannot create a Dataset from a DataArray with "
566                 "the same name as one of its coordinates"
567             )
568         # use private APIs for speed: this is called by _to_temp_dataset(),
569         # which is used in the guts of a lot of operations (e.g., reindex)
570         variables = self._coords.copy()
571         variables[name] = self.variable
572         if shallow_copy:
573             for k in variables:
574                 variables[k] = variables[k].copy(deep=False)
575         indexes = self._indexes
576 
577         coord_names = set(self._coords)
578         return Dataset._construct_direct(variables, coord_names, indexes=indexes)
579 
580     def to_dataset(
581         self,
582         dim: Hashable = None,
583         *,
584         name: Hashable = None,
585         promote_attrs: bool = False,
586     ) -> Dataset:
587         """Convert a DataArray to a Dataset.
588 
589         Parameters
590         ----------
591         dim : Hashable, optional
592             Name of the dimension on this array along which to split this array
593             into separate variables. If not provided, this array is converted
594             into a Dataset of one variable.
595         name : Hashable, optional
596             Name to substitute for this array's name. Only valid if ``dim`` is
597             not provided.
598         promote_attrs : bool, default: False
599             Set to True to shallow copy attrs of DataArray to returned Dataset.
600 
601         Returns
602         -------
603         dataset : Dataset
604         """
605         if dim is not None and dim not in self.dims:
606             raise TypeError(
607                 f"{dim} is not a dim. If supplying a ``name``, pass as a kwarg."
608             )
609 
610         if dim is not None:
611             if name is not None:
612                 raise TypeError("cannot supply both dim and name arguments")
613             result = self._to_dataset_split(dim)
614         else:
615             result = self._to_dataset_whole(name)
616 
617         if promote_attrs:
618             result.attrs = dict(self.attrs)
619 
620         return result
621 
622     @property
623     def name(self) -> Hashable | None:
624         """The name of this array."""
625         return self._name
626 
627     @name.setter
628     def name(self, value: Hashable | None) -> None:
629         self._name = value
630 
631     @property
632     def variable(self) -> Variable:
633         """Low level interface to the Variable object for this DataArray."""
634         return self._variable
635 
636     @property
637     def dtype(self) -> np.dtype:
638         return self.variable.dtype
639 
640     @property
641     def shape(self) -> tuple[int, ...]:
642         return self.variable.shape
643 
644     @property
645     def size(self) -> int:
646         return self.variable.size
647 
648     @property
649     def nbytes(self) -> int:
650         """
651         Total bytes consumed by the elements of this DataArray's data.
652 
653         If the backend data array does not include ``nbytes``, estimates
654         the bytes consumed based on the ``size`` and ``dtype``.
655         """
656         return self.variable.nbytes
657 
658     @property
659     def ndim(self) -> int:
660         return self.variable.ndim
661 
662     def __len__(self) -> int:
663         return len(self.variable)
664 
665     @property
666     def data(self) -> Any:
667         """
668         The DataArray's data as an array. The underlying array type
669         (e.g. dask, sparse, pint) is preserved.
670 
671         See Also
672         --------
673         DataArray.to_numpy
674         DataArray.as_numpy
675         DataArray.values
676         """
677         return self.variable.data
678 
679     @data.setter
680     def data(self, value: Any) -> None:
681         self.variable.data = value
682 
683     @property
684     def values(self) -> np.ndarray:
685         """
686         The array's data as a numpy.ndarray.
687 
688         If the array's data is not a numpy.ndarray this will attempt to convert
689         it naively using np.array(), which will raise an error if the array
690         type does not support coercion like this (e.g. cupy).
691         """
692         return self.variable.values
693 
694     @values.setter
695     def values(self, value: Any) -> None:
696         self.variable.values = value
697 
698     def to_numpy(self) -> np.ndarray:
699         """
700         Coerces wrapped data to numpy and returns a numpy.ndarray.
701 
702         See Also
703         --------
704         DataArray.as_numpy : Same but returns the surrounding DataArray instead.
705         Dataset.as_numpy
706         DataArray.values
707         DataArray.data
708         """
709         return self.variable.to_numpy()
710 
711     def as_numpy(self: T_DataArray) -> T_DataArray:
712         """
713         Coerces wrapped data and coordinates into numpy arrays, returning a DataArray.
714 
715         See Also
716         --------
717         DataArray.to_numpy : Same but returns only the data as a numpy.ndarray object.
718         Dataset.as_numpy : Converts all variables in a Dataset.
719         DataArray.values
720         DataArray.data
721         """
722         coords = {k: v.as_numpy() for k, v in self._coords.items()}
723         return self._replace(self.variable.as_numpy(), coords, indexes=self._indexes)
724 
725     @property
726     def _in_memory(self) -> bool:
727         return self.variable._in_memory
728 
729     def to_index(self) -> pd.Index:
730         """Convert this variable to a pandas.Index. Only possible for 1D
731         arrays.
732         """
733         return self.variable.to_index()
734 
735     @property
736     def dims(self) -> tuple[Hashable, ...]:
737         """Tuple of dimension names associated with this array.
738 
739         Note that the type of this property is inconsistent with
740         `Dataset.dims`.  See `Dataset.sizes` and `DataArray.sizes` for
741         consistently named properties.
742 
743         See Also
744         --------
745         DataArray.sizes
746         Dataset.dims
747         """
748         return self.variable.dims
749 
750     @dims.setter
751     def dims(self, value: Any) -> NoReturn:
752         raise AttributeError(
753             "you cannot assign dims on a DataArray. Use "
754             ".rename() or .swap_dims() instead."
755         )
756 
757     def _item_key_to_dict(self, key: Any) -> Mapping[Hashable, Any]:
758         if utils.is_dict_like(key):
759             return key
760         key = indexing.expanded_indexer(key, self.ndim)
761         return dict(zip(self.dims, key))
762 
763     def _getitem_coord(self: T_DataArray, key: Any) -> T_DataArray:
764         from .dataset import _get_virtual_variable
765 
766         try:
767             var = self._coords[key]
768         except KeyError:
769             dim_sizes = dict(zip(self.dims, self.shape))
770             _, key, var = _get_virtual_variable(self._coords, key, dim_sizes)
771 
772         return self._replace_maybe_drop_dims(var, name=key)
773 
774     def __getitem__(self: T_DataArray, key: Any) -> T_DataArray:
775         if isinstance(key, str):
776             return self._getitem_coord(key)
777         else:
778             # xarray-style array indexing
779             return self.isel(indexers=self._item_key_to_dict(key))
780 
781     def __setitem__(self, key: Any, value: Any) -> None:
782         if isinstance(key, str):
783             self.coords[key] = value
784         else:
785             # Coordinates in key, value and self[key] should be consistent.
786             # TODO Coordinate consistency in key is checked here, but it
787             # causes unnecessary indexing. It should be optimized.
788             obj = self[key]
789             if isinstance(value, DataArray):
790                 assert_coordinate_consistent(value, obj.coords.variables)
791             # DataArray key -> Variable key
792             key = {
793                 k: v.variable if isinstance(v, DataArray) else v
794                 for k, v in self._item_key_to_dict(key).items()
795             }
796             self.variable[key] = value
797 
798     def __delitem__(self, key: Any) -> None:
799         del self.coords[key]
800 
801     @property
802     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
803         """Places to look-up items for attribute-style access"""
804         yield from self._item_sources
805         yield self.attrs
806 
807     @property
808     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
809         """Places to look-up items for key-completion"""
810         yield HybridMappingProxy(keys=self._coords, mapping=self.coords)
811 
812         # virtual coordinates
813         # uses empty dict -- everything here can already be found in self.coords.
814         yield HybridMappingProxy(keys=self.dims, mapping={})
815 
816     def __contains__(self, key: Any) -> bool:
817         return key in self.data
818 
819     @property
820     def loc(self) -> _LocIndexer:
821         """Attribute for location based indexing like pandas."""
822         return _LocIndexer(self)
823 
824     @property
825     # Key type needs to be `Any` because of mypy#4167
826     def attrs(self) -> dict[Any, Any]:
827         """Dictionary storing arbitrary metadata with this array."""
828         return self.variable.attrs
829 
830     @attrs.setter
831     def attrs(self, value: Mapping[Any, Any]) -> None:
832         # Disable type checking to work around mypy bug - see mypy#4167
833         self.variable.attrs = value  # type: ignore[assignment]
834 
835     @property
836     def encoding(self) -> dict[Hashable, Any]:
837         """Dictionary of format-specific settings for how this array should be
838         serialized."""
839         return self.variable.encoding
840 
841     @encoding.setter
842     def encoding(self, value: Mapping[Any, Any]) -> None:
843         self.variable.encoding = value
844 
845     @property
846     def indexes(self) -> Indexes:
847         """Mapping of pandas.Index objects used for label based indexing.
848 
849         Raises an error if this Dataset has indexes that cannot be coerced
850         to pandas.Index objects.
851 
852         See Also
853         --------
854         DataArray.xindexes
855 
856         """
857         return self.xindexes.to_pandas_indexes()
858 
859     @property
860     def xindexes(self) -> Indexes:
861         """Mapping of xarray Index objects used for label based indexing."""
862         return Indexes(self._indexes, {k: self._coords[k] for k in self._indexes})
863 
864     @property
865     def coords(self) -> DataArrayCoordinates:
866         """Dictionary-like container of coordinate arrays."""
867         return DataArrayCoordinates(self)
868 
869     @overload
870     def reset_coords(
871         self: T_DataArray,
872         names: Hashable | Iterable[Hashable] | None = None,
873         drop: Literal[False] = False,
874     ) -> Dataset:
875         ...
876 
877     @overload
878     def reset_coords(
879         self: T_DataArray,
880         names: Hashable | Iterable[Hashable] | None = None,
881         *,
882         drop: Literal[True],
883     ) -> T_DataArray:
884         ...
885 
886     def reset_coords(
887         self: T_DataArray,
888         names: Hashable | Iterable[Hashable] | None = None,
889         drop: bool = False,
890     ) -> T_DataArray | Dataset:
891         """Given names of coordinates, reset them to become variables.
892 
893         Parameters
894         ----------
895         names : Hashable or iterable of Hashable, optional
896             Name(s) of non-index coordinates in this dataset to reset into
897             variables. By default, all non-index coordinates are reset.
898         drop : bool, default: False
899             If True, remove coordinates instead of converting them into
900             variables.
901 
902         Returns
903         -------
904         Dataset, or DataArray if ``drop == True``
905         """
906         if names is None:
907             names = set(self.coords) - set(self._indexes)
908         dataset = self.coords.to_dataset().reset_coords(names, drop)
909         if drop:
910             return self._replace(coords=dataset._variables)
911         if self.name is None:
912             raise ValueError(
913                 "cannot reset_coords with drop=False on an unnamed DataArrray"
914             )
915         dataset[self.name] = self.variable
916         return dataset
917 
918     def __dask_tokenize__(self):
919         from dask.base import normalize_token
920 
921         return normalize_token((type(self), self._variable, self._coords, self._name))
922 
923     def __dask_graph__(self):
924         return self._to_temp_dataset().__dask_graph__()
925 
926     def __dask_keys__(self):
927         return self._to_temp_dataset().__dask_keys__()
928 
929     def __dask_layers__(self):
930         return self._to_temp_dataset().__dask_layers__()
931 
932     @property
933     def __dask_optimize__(self):
934         return self._to_temp_dataset().__dask_optimize__
935 
936     @property
937     def __dask_scheduler__(self):
938         return self._to_temp_dataset().__dask_scheduler__
939 
940     def __dask_postcompute__(self):
941         func, args = self._to_temp_dataset().__dask_postcompute__()
942         return self._dask_finalize, (self.name, func) + args
943 
944     def __dask_postpersist__(self):
945         func, args = self._to_temp_dataset().__dask_postpersist__()
946         return self._dask_finalize, (self.name, func) + args
947 
948     @staticmethod
949     def _dask_finalize(results, name, func, *args, **kwargs) -> DataArray:
950         ds = func(results, *args, **kwargs)
951         variable = ds._variables.pop(_THIS_ARRAY)
952         coords = ds._variables
953         indexes = ds._indexes
954         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
955 
956     def load(self: T_DataArray, **kwargs) -> T_DataArray:
957         """Manually trigger loading of this array's data from disk or a
958         remote source into memory and return this array.
959 
960         Normally, it should not be necessary to call this method in user code,
961         because all xarray functions should either work on deferred data or
962         load data automatically. However, this method can be necessary when
963         working with many file objects on disk.
964 
965         Parameters
966         ----------
967         **kwargs : dict
968             Additional keyword arguments passed on to ``dask.compute``.
969 
970         See Also
971         --------
972         dask.compute
973         """
974         ds = self._to_temp_dataset().load(**kwargs)
975         new = self._from_temp_dataset(ds)
976         self._variable = new._variable
977         self._coords = new._coords
978         return self
979 
980     def compute(self: T_DataArray, **kwargs) -> T_DataArray:
981         """Manually trigger loading of this array's data from disk or a
982         remote source into memory and return a new array. The original is
983         left unaltered.
984 
985         Normally, it should not be necessary to call this method in user code,
986         because all xarray functions should either work on deferred data or
987         load data automatically. However, this method can be necessary when
988         working with many file objects on disk.
989 
990         Parameters
991         ----------
992         **kwargs : dict
993             Additional keyword arguments passed on to ``dask.compute``.
994 
995         See Also
996         --------
997         dask.compute
998         """
999         new = self.copy(deep=False)
1000         return new.load(**kwargs)
1001 
1002     def persist(self: T_DataArray, **kwargs) -> T_DataArray:
1003         """Trigger computation in constituent dask arrays
1004 
1005         This keeps them as dask arrays but encourages them to keep data in
1006         memory.  This is particularly useful when on a distributed machine.
1007         When on a single machine consider using ``.compute()`` instead.
1008 
1009         Parameters
1010         ----------
1011         **kwargs : dict
1012             Additional keyword arguments passed on to ``dask.persist``.
1013 
1014         See Also
1015         --------
1016         dask.persist
1017         """
1018         ds = self._to_temp_dataset().persist(**kwargs)
1019         return self._from_temp_dataset(ds)
1020 
1021     def copy(self: T_DataArray, deep: bool = True, data: Any = None) -> T_DataArray:
1022         """Returns a copy of this array.
1023 
1024         If `deep=True`, a deep copy is made of the data array.
1025         Otherwise, a shallow copy is made, and the returned data array's
1026         values are a new view of this data array's values.
1027 
1028         Use `data` to create a new object with the same structure as
1029         original but entirely new data.
1030 
1031         Parameters
1032         ----------
1033         deep : bool, optional
1034             Whether the data array and its coordinates are loaded into memory
1035             and copied onto the new object. Default is True.
1036         data : array_like, optional
1037             Data to use in the new object. Must have same shape as original.
1038             When `data` is used, `deep` is ignored for all data variables,
1039             and only used for coords.
1040 
1041         Returns
1042         -------
1043         copy : DataArray
1044             New object with dimensions, attributes, coordinates, name,
1045             encoding, and optionally data copied from original.
1046 
1047         Examples
1048         --------
1049         Shallow versus deep copy
1050 
1051         >>> array = xr.DataArray([1, 2, 3], dims="x", coords={"x": ["a", "b", "c"]})
1052         >>> array.copy()
1053         <xarray.DataArray (x: 3)>
1054         array([1, 2, 3])
1055         Coordinates:
1056           * x        (x) <U1 'a' 'b' 'c'
1057         >>> array_0 = array.copy(deep=False)
1058         >>> array_0[0] = 7
1059         >>> array_0
1060         <xarray.DataArray (x: 3)>
1061         array([7, 2, 3])
1062         Coordinates:
1063           * x        (x) <U1 'a' 'b' 'c'
1064         >>> array
1065         <xarray.DataArray (x: 3)>
1066         array([7, 2, 3])
1067         Coordinates:
1068           * x        (x) <U1 'a' 'b' 'c'
1069 
1070         Changing the data using the ``data`` argument maintains the
1071         structure of the original object, but with the new data. Original
1072         object is unaffected.
1073 
1074         >>> array.copy(data=[0.1, 0.2, 0.3])
1075         <xarray.DataArray (x: 3)>
1076         array([0.1, 0.2, 0.3])
1077         Coordinates:
1078           * x        (x) <U1 'a' 'b' 'c'
1079         >>> array
1080         <xarray.DataArray (x: 3)>
1081         array([7, 2, 3])
1082         Coordinates:
1083           * x        (x) <U1 'a' 'b' 'c'
1084 
1085         See Also
1086         --------
1087         pandas.DataFrame.copy
1088         """
1089         variable = self.variable.copy(deep=deep, data=data)
1090         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1091 
1092         coords = {}
1093         for k, v in self._coords.items():
1094             if k in index_vars:
1095                 coords[k] = index_vars[k]
1096             else:
1097                 coords[k] = v.copy(deep=deep)
1098 
1099         return self._replace(variable, coords, indexes=indexes)
1100 
1101     def __copy__(self: T_DataArray) -> T_DataArray:
1102         return self.copy(deep=False)
1103 
1104     def __deepcopy__(self: T_DataArray, memo=None) -> T_DataArray:
1105         # memo does nothing but is required for compatibility with
1106         # copy.deepcopy
1107         return self.copy(deep=True)
1108 
1109     # mutable objects should not be Hashable
1110     # https://github.com/python/mypy/issues/4266
1111     __hash__ = None  # type: ignore[assignment]
1112 
1113     @property
1114     def chunks(self) -> tuple[tuple[int, ...], ...] | None:
1115         """
1116         Tuple of block lengths for this dataarray's data, in order of dimensions, or None if
1117         the underlying data is not a dask array.
1118 
1119         See Also
1120         --------
1121         DataArray.chunk
1122         DataArray.chunksizes
1123         xarray.unify_chunks
1124         """
1125         return self.variable.chunks
1126 
1127     @property
1128     def chunksizes(self) -> Mapping[Any, tuple[int, ...]]:
1129         """
1130         Mapping from dimension names to block lengths for this dataarray's data, or None if
1131         the underlying data is not a dask array.
1132         Cannot be modified directly, but can be modified by calling .chunk().
1133 
1134         Differs from DataArray.chunks because it returns a mapping of dimensions to chunk shapes
1135         instead of a tuple of chunk shapes.
1136 
1137         See Also
1138         --------
1139         DataArray.chunk
1140         DataArray.chunks
1141         xarray.unify_chunks
1142         """
1143         all_variables = [self.variable] + [c.variable for c in self.coords.values()]
1144         return get_chunksizes(all_variables)
1145 
1146     def chunk(
1147         self: T_DataArray,
1148         chunks: (
1149             int
1150             | Literal["auto"]
1151             | tuple[int, ...]
1152             | tuple[tuple[int, ...], ...]
1153             | Mapping[Any, None | int | tuple[int, ...]]
1154         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1155         name_prefix: str = "xarray-",
1156         token: str | None = None,
1157         lock: bool = False,
1158         inline_array: bool = False,
1159         **chunks_kwargs: Any,
1160     ) -> T_DataArray:
1161         """Coerce this array's data into a dask arrays with the given chunks.
1162 
1163         If this variable is a non-dask array, it will be converted to dask
1164         array. If it's a dask array, it will be rechunked to the given chunk
1165         sizes.
1166 
1167         If neither chunks is not provided for one or more dimensions, chunk
1168         sizes along that dimension will not be updated; non-dask arrays will be
1169         converted into dask arrays with a single block.
1170 
1171         Parameters
1172         ----------
1173         chunks : int, "auto", tuple of int or mapping of Hashable to int, optional
1174             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, ``(5, 5)`` or
1175             ``{"x": 5, "y": 5}``.
1176         name_prefix : str, optional
1177             Prefix for the name of the new dask array.
1178         token : str, optional
1179             Token uniquely identifying this array.
1180         lock : optional
1181             Passed on to :py:func:`dask.array.from_array`, if the array is not
1182             already as dask array.
1183         inline_array: optional
1184             Passed on to :py:func:`dask.array.from_array`, if the array is not
1185             already as dask array.
1186         **chunks_kwargs : {dim: chunks, ...}, optional
1187             The keyword arguments form of ``chunks``.
1188             One of chunks or chunks_kwargs must be provided.
1189 
1190         Returns
1191         -------
1192         chunked : xarray.DataArray
1193 
1194         See Also
1195         --------
1196         DataArray.chunks
1197         DataArray.chunksizes
1198         xarray.unify_chunks
1199         dask.array.from_array
1200         """
1201         if chunks is None:
1202             warnings.warn(
1203                 "None value for 'chunks' is deprecated. "
1204                 "It will raise an error in the future. Use instead '{}'",
1205                 category=FutureWarning,
1206             )
1207             chunks = {}
1208 
1209         if isinstance(chunks, (float, str, int)):
1210             # ignoring type; unclear why it won't accept a Literal into the value.
1211             chunks = dict.fromkeys(self.dims, chunks)  # type: ignore
1212         elif isinstance(chunks, (tuple, list)):
1213             chunks = dict(zip(self.dims, chunks))
1214         else:
1215             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
1216 
1217         ds = self._to_temp_dataset().chunk(
1218             chunks,
1219             name_prefix=name_prefix,
1220             token=token,
1221             lock=lock,
1222             inline_array=inline_array,
1223         )
1224         return self._from_temp_dataset(ds)
1225 
1226     def isel(
1227         self: T_DataArray,
1228         indexers: Mapping[Any, Any] | None = None,
1229         drop: bool = False,
1230         missing_dims: ErrorOptionsWithWarn = "raise",
1231         **indexers_kwargs: Any,
1232     ) -> T_DataArray:
1233         """Return a new DataArray whose data is given by selecting indexes
1234         along the specified dimension(s).
1235 
1236         Parameters
1237         ----------
1238         indexers : dict, optional
1239             A dict with keys matching dimensions and values given
1240             by integers, slice objects or arrays.
1241             indexer can be a integer, slice, array-like or DataArray.
1242             If DataArrays are passed as indexers, xarray-style indexing will be
1243             carried out. See :ref:`indexing` for the details.
1244             One of indexers or indexers_kwargs must be provided.
1245         drop : bool, default: False
1246             If ``drop=True``, drop coordinates variables indexed by integers
1247             instead of making them scalar.
1248         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
1249             What to do if dimensions that should be selected from are not present in the
1250             DataArray:
1251             - "raise": raise an exception
1252             - "warn": raise a warning, and ignore the missing dimensions
1253             - "ignore": ignore the missing dimensions
1254         **indexers_kwargs : {dim: indexer, ...}, optional
1255             The keyword arguments form of ``indexers``.
1256 
1257         Returns
1258         -------
1259         indexed : xarray.DataArray
1260 
1261         See Also
1262         --------
1263         Dataset.isel
1264         DataArray.sel
1265 
1266         Examples
1267         --------
1268         >>> da = xr.DataArray(np.arange(25).reshape(5, 5), dims=("x", "y"))
1269         >>> da
1270         <xarray.DataArray (x: 5, y: 5)>
1271         array([[ 0,  1,  2,  3,  4],
1272                [ 5,  6,  7,  8,  9],
1273                [10, 11, 12, 13, 14],
1274                [15, 16, 17, 18, 19],
1275                [20, 21, 22, 23, 24]])
1276         Dimensions without coordinates: x, y
1277 
1278         >>> tgt_x = xr.DataArray(np.arange(0, 5), dims="points")
1279         >>> tgt_y = xr.DataArray(np.arange(0, 5), dims="points")
1280         >>> da = da.isel(x=tgt_x, y=tgt_y)
1281         >>> da
1282         <xarray.DataArray (points: 5)>
1283         array([ 0,  6, 12, 18, 24])
1284         Dimensions without coordinates: points
1285         """
1286 
1287         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
1288 
1289         if any(is_fancy_indexer(idx) for idx in indexers.values()):
1290             ds = self._to_temp_dataset()._isel_fancy(
1291                 indexers, drop=drop, missing_dims=missing_dims
1292             )
1293             return self._from_temp_dataset(ds)
1294 
1295         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
1296         # lists, or zero or one-dimensional np.ndarray's
1297 
1298         variable = self._variable.isel(indexers, missing_dims=missing_dims)
1299         indexes, index_variables = isel_indexes(self.xindexes, indexers)
1300 
1301         coords = {}
1302         for coord_name, coord_value in self._coords.items():
1303             if coord_name in index_variables:
1304                 coord_value = index_variables[coord_name]
1305             else:
1306                 coord_indexers = {
1307                     k: v for k, v in indexers.items() if k in coord_value.dims
1308                 }
1309                 if coord_indexers:
1310                     coord_value = coord_value.isel(coord_indexers)
1311                     if drop and coord_value.ndim == 0:
1312                         continue
1313             coords[coord_name] = coord_value
1314 
1315         return self._replace(variable=variable, coords=coords, indexes=indexes)
1316 
1317     def sel(
1318         self: T_DataArray,
1319         indexers: Mapping[Any, Any] = None,
1320         method: str = None,
1321         tolerance=None,
1322         drop: bool = False,
1323         **indexers_kwargs: Any,
1324     ) -> T_DataArray:
1325         """Return a new DataArray whose data is given by selecting index
1326         labels along the specified dimension(s).
1327 
1328         In contrast to `DataArray.isel`, indexers for this method should use
1329         labels instead of integers.
1330 
1331         Under the hood, this method is powered by using pandas's powerful Index
1332         objects. This makes label based indexing essentially just as fast as
1333         using integer indexing.
1334 
1335         It also means this method uses pandas's (well documented) logic for
1336         indexing. This means you can use string shortcuts for datetime indexes
1337         (e.g., '2000-01' to select all values in January 2000). It also means
1338         that slices are treated as inclusive of both the start and stop values,
1339         unlike normal Python indexing.
1340 
1341         .. warning::
1342 
1343           Do not try to assign values when using any of the indexing methods
1344           ``isel`` or ``sel``::
1345 
1346             da = xr.DataArray([0, 1, 2, 3], dims=['x'])
1347             # DO NOT do this
1348             da.isel(x=[0, 1, 2])[1] = -1
1349 
1350           Assigning values with the chained indexing using ``.sel`` or
1351           ``.isel`` fails silently.
1352 
1353         Parameters
1354         ----------
1355         indexers : dict, optional
1356             A dict with keys matching dimensions and values given
1357             by scalars, slices or arrays of tick labels. For dimensions with
1358             multi-index, the indexer may also be a dict-like object with keys
1359             matching index level names.
1360             If DataArrays are passed as indexers, xarray-style indexing will be
1361             carried out. See :ref:`indexing` for the details.
1362             One of indexers or indexers_kwargs must be provided.
1363         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1364             Method to use for inexact matches:
1365 
1366             - None (default): only exact matches
1367             - pad / ffill: propagate last valid index value forward
1368             - backfill / bfill: propagate next valid index value backward
1369             - nearest: use nearest valid index value
1370 
1371         tolerance : optional
1372             Maximum distance between original and new labels for inexact
1373             matches. The values of the index at the matching locations must
1374             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1375         drop : bool, optional
1376             If ``drop=True``, drop coordinates variables in `indexers` instead
1377             of making them scalar.
1378         **indexers_kwargs : {dim: indexer, ...}, optional
1379             The keyword arguments form of ``indexers``.
1380             One of indexers or indexers_kwargs must be provided.
1381 
1382         Returns
1383         -------
1384         obj : DataArray
1385             A new DataArray with the same contents as this DataArray, except the
1386             data and each dimension is indexed by the appropriate indexers.
1387             If indexer DataArrays have coordinates that do not conflict with
1388             this object, then these coordinates will be attached.
1389             In general, each array's data will be a view of the array's data
1390             in this DataArray, unless vectorized indexing was triggered by using
1391             an array indexer, in which case the data will be a copy.
1392 
1393         See Also
1394         --------
1395         Dataset.sel
1396         DataArray.isel
1397 
1398         Examples
1399         --------
1400         >>> da = xr.DataArray(
1401         ...     np.arange(25).reshape(5, 5),
1402         ...     coords={"x": np.arange(5), "y": np.arange(5)},
1403         ...     dims=("x", "y"),
1404         ... )
1405         >>> da
1406         <xarray.DataArray (x: 5, y: 5)>
1407         array([[ 0,  1,  2,  3,  4],
1408                [ 5,  6,  7,  8,  9],
1409                [10, 11, 12, 13, 14],
1410                [15, 16, 17, 18, 19],
1411                [20, 21, 22, 23, 24]])
1412         Coordinates:
1413           * x        (x) int64 0 1 2 3 4
1414           * y        (y) int64 0 1 2 3 4
1415 
1416         >>> tgt_x = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1417         >>> tgt_y = xr.DataArray(np.linspace(0, 4, num=5), dims="points")
1418         >>> da = da.sel(x=tgt_x, y=tgt_y, method="nearest")
1419         >>> da
1420         <xarray.DataArray (points: 5)>
1421         array([ 0,  6, 12, 18, 24])
1422         Coordinates:
1423             x        (points) int64 0 1 2 3 4
1424             y        (points) int64 0 1 2 3 4
1425         Dimensions without coordinates: points
1426         """
1427         ds = self._to_temp_dataset().sel(
1428             indexers=indexers,
1429             drop=drop,
1430             method=method,
1431             tolerance=tolerance,
1432             **indexers_kwargs,
1433         )
1434         return self._from_temp_dataset(ds)
1435 
1436     def head(
1437         self: T_DataArray,
1438         indexers: Mapping[Any, int] | int | None = None,
1439         **indexers_kwargs: Any,
1440     ) -> T_DataArray:
1441         """Return a new DataArray whose data is given by the the first `n`
1442         values along the specified dimension(s). Default `n` = 5
1443 
1444         See Also
1445         --------
1446         Dataset.head
1447         DataArray.tail
1448         DataArray.thin
1449         """
1450         ds = self._to_temp_dataset().head(indexers, **indexers_kwargs)
1451         return self._from_temp_dataset(ds)
1452 
1453     def tail(
1454         self: T_DataArray,
1455         indexers: Mapping[Any, int] | int | None = None,
1456         **indexers_kwargs: Any,
1457     ) -> T_DataArray:
1458         """Return a new DataArray whose data is given by the the last `n`
1459         values along the specified dimension(s). Default `n` = 5
1460 
1461         See Also
1462         --------
1463         Dataset.tail
1464         DataArray.head
1465         DataArray.thin
1466         """
1467         ds = self._to_temp_dataset().tail(indexers, **indexers_kwargs)
1468         return self._from_temp_dataset(ds)
1469 
1470     def thin(
1471         self: T_DataArray,
1472         indexers: Mapping[Any, int] | int | None = None,
1473         **indexers_kwargs: Any,
1474     ) -> T_DataArray:
1475         """Return a new DataArray whose data is given by each `n` value
1476         along the specified dimension(s).
1477 
1478         Examples
1479         --------
1480         >>> x_arr = np.arange(0, 26)
1481         >>> x_arr
1482         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
1483                17, 18, 19, 20, 21, 22, 23, 24, 25])
1484         >>> x = xr.DataArray(
1485         ...     np.reshape(x_arr, (2, 13)),
1486         ...     dims=("x", "y"),
1487         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
1488         ... )
1489         >>> x
1490         <xarray.DataArray (x: 2, y: 13)>
1491         array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12],
1492                [13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]])
1493         Coordinates:
1494           * x        (x) int64 0 1
1495           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
1496 
1497         >>>
1498         >>> x.thin(3)
1499         <xarray.DataArray (x: 1, y: 5)>
1500         array([[ 0,  3,  6,  9, 12]])
1501         Coordinates:
1502           * x        (x) int64 0
1503           * y        (y) int64 0 3 6 9 12
1504         >>> x.thin({"x": 2, "y": 5})
1505         <xarray.DataArray (x: 1, y: 3)>
1506         array([[ 0,  5, 10]])
1507         Coordinates:
1508           * x        (x) int64 0
1509           * y        (y) int64 0 5 10
1510 
1511         See Also
1512         --------
1513         Dataset.thin
1514         DataArray.head
1515         DataArray.tail
1516         """
1517         ds = self._to_temp_dataset().thin(indexers, **indexers_kwargs)
1518         return self._from_temp_dataset(ds)
1519 
1520     def broadcast_like(
1521         self: T_DataArray,
1522         other: DataArray | Dataset,
1523         exclude: Iterable[Hashable] | None = None,
1524     ) -> T_DataArray:
1525         """Broadcast this DataArray against another Dataset or DataArray.
1526 
1527         This is equivalent to xr.broadcast(other, self)[1]
1528 
1529         xarray objects are broadcast against each other in arithmetic
1530         operations, so this method is not be necessary for most uses.
1531 
1532         If no change is needed, the input data is returned to the output
1533         without being copied.
1534 
1535         If new coords are added by the broadcast, their values are
1536         NaN filled.
1537 
1538         Parameters
1539         ----------
1540         other : Dataset or DataArray
1541             Object against which to broadcast this array.
1542         exclude : iterable of Hashable, optional
1543             Dimensions that must not be broadcasted
1544 
1545         Returns
1546         -------
1547         new_da : DataArray
1548             The caller broadcasted against ``other``.
1549 
1550         Examples
1551         --------
1552         >>> arr1 = xr.DataArray(
1553         ...     np.random.randn(2, 3),
1554         ...     dims=("x", "y"),
1555         ...     coords={"x": ["a", "b"], "y": ["a", "b", "c"]},
1556         ... )
1557         >>> arr2 = xr.DataArray(
1558         ...     np.random.randn(3, 2),
1559         ...     dims=("x", "y"),
1560         ...     coords={"x": ["a", "b", "c"], "y": ["a", "b"]},
1561         ... )
1562         >>> arr1
1563         <xarray.DataArray (x: 2, y: 3)>
1564         array([[ 1.76405235,  0.40015721,  0.97873798],
1565                [ 2.2408932 ,  1.86755799, -0.97727788]])
1566         Coordinates:
1567           * x        (x) <U1 'a' 'b'
1568           * y        (y) <U1 'a' 'b' 'c'
1569         >>> arr2
1570         <xarray.DataArray (x: 3, y: 2)>
1571         array([[ 0.95008842, -0.15135721],
1572                [-0.10321885,  0.4105985 ],
1573                [ 0.14404357,  1.45427351]])
1574         Coordinates:
1575           * x        (x) <U1 'a' 'b' 'c'
1576           * y        (y) <U1 'a' 'b'
1577         >>> arr1.broadcast_like(arr2)
1578         <xarray.DataArray (x: 3, y: 3)>
1579         array([[ 1.76405235,  0.40015721,  0.97873798],
1580                [ 2.2408932 ,  1.86755799, -0.97727788],
1581                [        nan,         nan,         nan]])
1582         Coordinates:
1583           * x        (x) <U1 'a' 'b' 'c'
1584           * y        (y) <U1 'a' 'b' 'c'
1585         """
1586         if exclude is None:
1587             exclude = set()
1588         else:
1589             exclude = set(exclude)
1590         args = align(other, self, join="outer", copy=False, exclude=exclude)
1591 
1592         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
1593 
1594         return _broadcast_helper(
1595             cast("T_DataArray", args[1]), exclude, dims_map, common_coords
1596         )
1597 
1598     def _reindex_callback(
1599         self: T_DataArray,
1600         aligner: alignment.Aligner,
1601         dim_pos_indexers: dict[Hashable, Any],
1602         variables: dict[Hashable, Variable],
1603         indexes: dict[Hashable, Index],
1604         fill_value: Any,
1605         exclude_dims: frozenset[Hashable],
1606         exclude_vars: frozenset[Hashable],
1607     ) -> T_DataArray:
1608         """Callback called from ``Aligner`` to create a new reindexed DataArray."""
1609 
1610         if isinstance(fill_value, dict):
1611             fill_value = fill_value.copy()
1612             sentinel = object()
1613             value = fill_value.pop(self.name, sentinel)
1614             if value is not sentinel:
1615                 fill_value[_THIS_ARRAY] = value
1616 
1617         ds = self._to_temp_dataset()
1618         reindexed = ds._reindex_callback(
1619             aligner,
1620             dim_pos_indexers,
1621             variables,
1622             indexes,
1623             fill_value,
1624             exclude_dims,
1625             exclude_vars,
1626         )
1627         return self._from_temp_dataset(reindexed)
1628 
1629     def reindex_like(
1630         self: T_DataArray,
1631         other: DataArray | Dataset,
1632         method: ReindexMethodOptions = None,
1633         tolerance: int | float | Iterable[int | float] | None = None,
1634         copy: bool = True,
1635         fill_value=dtypes.NA,
1636     ) -> T_DataArray:
1637         """Conform this object onto the indexes of another object, filling in
1638         missing values with ``fill_value``. The default fill value is NaN.
1639 
1640         Parameters
1641         ----------
1642         other : Dataset or DataArray
1643             Object with an 'indexes' attribute giving a mapping from dimension
1644             names to pandas.Index objects, which provides coordinates upon
1645             which to index the variables in this dataset. The indexes on this
1646             other object need not be the same as the indexes on this
1647             dataset. Any mis-matched index values will be filled in with
1648             NaN, and any mis-matched dimension names will simply be ignored.
1649         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
1650             Method to use for filling index values from other not found on this
1651             data array:
1652 
1653             - None (default): don't fill gaps
1654             - pad / ffill: propagate last valid index value forward
1655             - backfill / bfill: propagate next valid index value backward
1656             - nearest: use nearest valid index value
1657 
1658         tolerance : optional
1659             Maximum distance between original and new labels for inexact
1660             matches. The values of the index at the matching locations must
1661             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1662             Tolerance may be a scalar value, which applies the same tolerance
1663             to all values, or list-like, which applies variable tolerance per
1664             element. List-like must be the same size as the index and its dtype
1665             must exactly match the index’s type.
1666         copy : bool, default: True
1667             If ``copy=True``, data in the return value is always copied. If
1668             ``copy=False`` and reindexing is unnecessary, or can be performed
1669             with only slice operations, then the output may share memory with
1670             the input. In either case, a new xarray object is always returned.
1671         fill_value : scalar or dict-like, optional
1672             Value to use for newly missing values. If a dict-like, maps
1673             variable names (including coordinates) to fill values. Use this
1674             data array's name to refer to the data array's values.
1675 
1676         Returns
1677         -------
1678         reindexed : DataArray
1679             Another dataset array, with this array's data but coordinates from
1680             the other object.
1681 
1682         See Also
1683         --------
1684         DataArray.reindex
1685         align
1686         """
1687         return alignment.reindex_like(
1688             self,
1689             other=other,
1690             method=method,
1691             tolerance=tolerance,
1692             copy=copy,
1693             fill_value=fill_value,
1694         )
1695 
1696     def reindex(
1697         self: T_DataArray,
1698         indexers: Mapping[Any, Any] = None,
1699         method: ReindexMethodOptions = None,
1700         tolerance: float | Iterable[float] | None = None,
1701         copy: bool = True,
1702         fill_value=dtypes.NA,
1703         **indexers_kwargs: Any,
1704     ) -> T_DataArray:
1705         """Conform this object onto the indexes of another object, filling in
1706         missing values with ``fill_value``. The default fill value is NaN.
1707 
1708         Parameters
1709         ----------
1710         indexers : dict, optional
1711             Dictionary with keys given by dimension names and values given by
1712             arrays of coordinates tick labels. Any mis-matched coordinate
1713             values will be filled in with NaN, and any mis-matched dimension
1714             names will simply be ignored.
1715             One of indexers or indexers_kwargs must be provided.
1716         copy : bool, optional
1717             If ``copy=True``, data in the return value is always copied. If
1718             ``copy=False`` and reindexing is unnecessary, or can be performed
1719             with only slice operations, then the output may share memory with
1720             the input. In either case, a new xarray object is always returned.
1721         method : {None, 'nearest', 'pad'/'ffill', 'backfill'/'bfill'}, optional
1722             Method to use for filling index values in ``indexers`` not found on
1723             this data array:
1724 
1725             - None (default): don't fill gaps
1726             - pad / ffill: propagate last valid index value forward
1727             - backfill / bfill: propagate next valid index value backward
1728             - nearest: use nearest valid index value
1729 
1730         tolerance : float | Iterable[float] | None, default: None
1731             Maximum distance between original and new labels for inexact
1732             matches. The values of the index at the matching locations must
1733             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
1734             Tolerance may be a scalar value, which applies the same tolerance
1735             to all values, or list-like, which applies variable tolerance per
1736             element. List-like must be the same size as the index and its dtype
1737             must exactly match the index’s type.
1738         fill_value : scalar or dict-like, optional
1739             Value to use for newly missing values. If a dict-like, maps
1740             variable names (including coordinates) to fill values. Use this
1741             data array's name to refer to the data array's values.
1742         **indexers_kwargs : {dim: indexer, ...}, optional
1743             The keyword arguments form of ``indexers``.
1744             One of indexers or indexers_kwargs must be provided.
1745 
1746         Returns
1747         -------
1748         reindexed : DataArray
1749             Another dataset array, with this array's data but replaced
1750             coordinates.
1751 
1752         Examples
1753         --------
1754         Reverse latitude:
1755 
1756         >>> da = xr.DataArray(
1757         ...     np.arange(4),
1758         ...     coords=[np.array([90, 89, 88, 87])],
1759         ...     dims="lat",
1760         ... )
1761         >>> da
1762         <xarray.DataArray (lat: 4)>
1763         array([0, 1, 2, 3])
1764         Coordinates:
1765           * lat      (lat) int64 90 89 88 87
1766         >>> da.reindex(lat=da.lat[::-1])
1767         <xarray.DataArray (lat: 4)>
1768         array([3, 2, 1, 0])
1769         Coordinates:
1770           * lat      (lat) int64 87 88 89 90
1771 
1772         See Also
1773         --------
1774         DataArray.reindex_like
1775         align
1776         """
1777         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
1778         return alignment.reindex(
1779             self,
1780             indexers=indexers,
1781             method=method,
1782             tolerance=tolerance,
1783             copy=copy,
1784             fill_value=fill_value,
1785         )
1786 
1787     def interp(
1788         self: T_DataArray,
1789         coords: Mapping[Any, Any] | None = None,
1790         method: InterpOptions = "linear",
1791         assume_sorted: bool = False,
1792         kwargs: Mapping[str, Any] | None = None,
1793         **coords_kwargs: Any,
1794     ) -> T_DataArray:
1795         """Interpolate a DataArray onto new coordinates
1796 
1797         Performs univariate or multivariate interpolation of a DataArray onto
1798         new coordinates using scipy's interpolation routines. If interpolating
1799         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
1800         called. When interpolating along multiple existing dimensions, an
1801         attempt is made to decompose the interpolation into multiple
1802         1-dimensional interpolations. If this is possible,
1803         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1804         :py:func:`scipy.interpolate.interpn` is called.
1805 
1806         Parameters
1807         ----------
1808         coords : dict, optional
1809             Mapping from dimension names to the new coordinates.
1810             New coordinate can be a scalar, array-like or DataArray.
1811             If DataArrays are passed as new coordinates, their dimensions are
1812             used for the broadcasting. Missing values are skipped.
1813         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1814             The method used to interpolate. The method should be supported by
1815             the scipy interpolator:
1816 
1817             - ``interp1d``: {"linear", "nearest", "zero", "slinear",
1818               "quadratic", "cubic", "polynomial"}
1819             - ``interpn``: {"linear", "nearest"}
1820 
1821             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1822             also be provided.
1823         assume_sorted : bool, default: False
1824             If False, values of x can be in any order and they are sorted
1825             first. If True, x has to be an array of monotonically increasing
1826             values.
1827         kwargs : dict-like or None, default: None
1828             Additional keyword arguments passed to scipy's interpolator. Valid
1829             options and their behavior depend whether ``interp1d`` or
1830             ``interpn`` is used.
1831         **coords_kwargs : {dim: coordinate, ...}, optional
1832             The keyword arguments form of ``coords``.
1833             One of coords or coords_kwargs must be provided.
1834 
1835         Returns
1836         -------
1837         interpolated : DataArray
1838             New dataarray on the new coordinates.
1839 
1840         Notes
1841         -----
1842         scipy is required.
1843 
1844         See Also
1845         --------
1846         scipy.interpolate.interp1d
1847         scipy.interpolate.interpn
1848 
1849         Examples
1850         --------
1851         >>> da = xr.DataArray(
1852         ...     data=[[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
1853         ...     dims=("x", "y"),
1854         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
1855         ... )
1856         >>> da
1857         <xarray.DataArray (x: 3, y: 4)>
1858         array([[ 1.,  4.,  2.,  9.],
1859                [ 2.,  7.,  6., nan],
1860                [ 6., nan,  5.,  8.]])
1861         Coordinates:
1862           * x        (x) int64 0 1 2
1863           * y        (y) int64 10 12 14 16
1864 
1865         1D linear interpolation (the default):
1866 
1867         >>> da.interp(x=[0, 0.75, 1.25, 1.75])
1868         <xarray.DataArray (x: 4, y: 4)>
1869         array([[1.  , 4.  , 2.  ,  nan],
1870                [1.75, 6.25, 5.  ,  nan],
1871                [3.  ,  nan, 5.75,  nan],
1872                [5.  ,  nan, 5.25,  nan]])
1873         Coordinates:
1874           * y        (y) int64 10 12 14 16
1875           * x        (x) float64 0.0 0.75 1.25 1.75
1876 
1877         1D nearest interpolation:
1878 
1879         >>> da.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
1880         <xarray.DataArray (x: 4, y: 4)>
1881         array([[ 1.,  4.,  2.,  9.],
1882                [ 2.,  7.,  6., nan],
1883                [ 2.,  7.,  6., nan],
1884                [ 6., nan,  5.,  8.]])
1885         Coordinates:
1886           * y        (y) int64 10 12 14 16
1887           * x        (x) float64 0.0 0.75 1.25 1.75
1888 
1889         1D linear extrapolation:
1890 
1891         >>> da.interp(
1892         ...     x=[1, 1.5, 2.5, 3.5],
1893         ...     method="linear",
1894         ...     kwargs={"fill_value": "extrapolate"},
1895         ... )
1896         <xarray.DataArray (x: 4, y: 4)>
1897         array([[ 2. ,  7. ,  6. ,  nan],
1898                [ 4. ,  nan,  5.5,  nan],
1899                [ 8. ,  nan,  4.5,  nan],
1900                [12. ,  nan,  3.5,  nan]])
1901         Coordinates:
1902           * y        (y) int64 10 12 14 16
1903           * x        (x) float64 1.0 1.5 2.5 3.5
1904 
1905         2D linear interpolation:
1906 
1907         >>> da.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
1908         <xarray.DataArray (x: 4, y: 3)>
1909         array([[2.5  , 3.   ,   nan],
1910                [4.   , 5.625,   nan],
1911                [  nan,   nan,   nan],
1912                [  nan,   nan,   nan]])
1913         Coordinates:
1914           * x        (x) float64 0.0 0.75 1.25 1.75
1915           * y        (y) int64 11 13 15
1916         """
1917         if self.dtype.kind not in "uifc":
1918             raise TypeError(
1919                 "interp only works for a numeric type array. "
1920                 "Given {}.".format(self.dtype)
1921             )
1922         ds = self._to_temp_dataset().interp(
1923             coords,
1924             method=method,
1925             kwargs=kwargs,
1926             assume_sorted=assume_sorted,
1927             **coords_kwargs,
1928         )
1929         return self._from_temp_dataset(ds)
1930 
1931     def interp_like(
1932         self: T_DataArray,
1933         other: DataArray | Dataset,
1934         method: InterpOptions = "linear",
1935         assume_sorted: bool = False,
1936         kwargs: Mapping[str, Any] | None = None,
1937     ) -> T_DataArray:
1938         """Interpolate this object onto the coordinates of another object,
1939         filling out of range values with NaN.
1940 
1941         If interpolating along a single existing dimension,
1942         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
1943         along multiple existing dimensions, an attempt is made to decompose the
1944         interpolation into multiple 1-dimensional interpolations. If this is
1945         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
1946         :py:func:`scipy.interpolate.interpn` is called.
1947 
1948         Parameters
1949         ----------
1950         other : Dataset or DataArray
1951             Object with an 'indexes' attribute giving a mapping from dimension
1952             names to an 1d array-like, which provides coordinates upon
1953             which to index the variables in this dataset. Missing values are skipped.
1954         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial"}, default: "linear"
1955             The method used to interpolate. The method should be supported by
1956             the scipy interpolator:
1957 
1958             - {"linear", "nearest", "zero", "slinear", "quadratic", "cubic",
1959               "polynomial"} when ``interp1d`` is called.
1960             - {"linear", "nearest"} when ``interpn`` is called.
1961 
1962             If ``"polynomial"`` is passed, the ``order`` keyword argument must
1963             also be provided.
1964         assume_sorted : bool, default: False
1965             If False, values of coordinates that are interpolated over can be
1966             in any order and they are sorted first. If True, interpolated
1967             coordinates are assumed to be an array of monotonically increasing
1968             values.
1969         kwargs : dict, optional
1970             Additional keyword passed to scipy's interpolator.
1971 
1972         Returns
1973         -------
1974         interpolated : DataArray
1975             Another dataarray by interpolating this dataarray's data along the
1976             coordinates of the other object.
1977 
1978         Notes
1979         -----
1980         scipy is required.
1981         If the dataarray has object-type coordinates, reindex is used for these
1982         coordinates instead of the interpolation.
1983 
1984         See Also
1985         --------
1986         DataArray.interp
1987         DataArray.reindex_like
1988         """
1989         if self.dtype.kind not in "uifc":
1990             raise TypeError(
1991                 "interp only works for a numeric type array. "
1992                 "Given {}.".format(self.dtype)
1993             )
1994         ds = self._to_temp_dataset().interp_like(
1995             other, method=method, kwargs=kwargs, assume_sorted=assume_sorted
1996         )
1997         return self._from_temp_dataset(ds)
1998 
1999     # change type of self and return to T_DataArray once
2000     # https://github.com/python/mypy/issues/12846 is resolved
2001     def rename(
2002         self,
2003         new_name_or_name_dict: Hashable | Mapping[Any, Hashable] | None = None,
2004         **names: Hashable,
2005     ) -> DataArray:
2006         """Returns a new DataArray with renamed coordinates, dimensions or a new name.
2007 
2008         Parameters
2009         ----------
2010         new_name_or_name_dict : str or dict-like, optional
2011             If the argument is dict-like, it used as a mapping from old
2012             names to new names for coordinates or dimensions. Otherwise,
2013             use the argument as the new name for this array.
2014         **names : Hashable, optional
2015             The keyword arguments form of a mapping from old names to
2016             new names for coordinates or dimensions.
2017             One of new_name_or_name_dict or names must be provided.
2018 
2019         Returns
2020         -------
2021         renamed : DataArray
2022             Renamed array or array with renamed coordinates.
2023 
2024         See Also
2025         --------
2026         Dataset.rename
2027         DataArray.swap_dims
2028         """
2029         if new_name_or_name_dict is None and not names:
2030             # change name to None?
2031             return self._replace(name=None)
2032         if utils.is_dict_like(new_name_or_name_dict) or new_name_or_name_dict is None:
2033             # change dims/coords
2034             name_dict = either_dict_or_kwargs(new_name_or_name_dict, names, "rename")
2035             dataset = self._to_temp_dataset().rename(name_dict)
2036             return self._from_temp_dataset(dataset)
2037         if utils.hashable(new_name_or_name_dict) and names:
2038             # change name + dims/coords
2039             dataset = self._to_temp_dataset().rename(names)
2040             dataarray = self._from_temp_dataset(dataset)
2041             return dataarray._replace(name=new_name_or_name_dict)
2042         # only change name
2043         return self._replace(name=new_name_or_name_dict)
2044 
2045     def swap_dims(
2046         self: T_DataArray,
2047         dims_dict: Mapping[Any, Hashable] | None = None,
2048         **dims_kwargs,
2049     ) -> T_DataArray:
2050         """Returns a new DataArray with swapped dimensions.
2051 
2052         Parameters
2053         ----------
2054         dims_dict : dict-like
2055             Dictionary whose keys are current dimension names and whose values
2056             are new names.
2057         **dims_kwargs : {existing_dim: new_dim, ...}, optional
2058             The keyword arguments form of ``dims_dict``.
2059             One of dims_dict or dims_kwargs must be provided.
2060 
2061         Returns
2062         -------
2063         swapped : DataArray
2064             DataArray with swapped dimensions.
2065 
2066         Examples
2067         --------
2068         >>> arr = xr.DataArray(
2069         ...     data=[0, 1],
2070         ...     dims="x",
2071         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
2072         ... )
2073         >>> arr
2074         <xarray.DataArray (x: 2)>
2075         array([0, 1])
2076         Coordinates:
2077           * x        (x) <U1 'a' 'b'
2078             y        (x) int64 0 1
2079 
2080         >>> arr.swap_dims({"x": "y"})
2081         <xarray.DataArray (y: 2)>
2082         array([0, 1])
2083         Coordinates:
2084             x        (y) <U1 'a' 'b'
2085           * y        (y) int64 0 1
2086 
2087         >>> arr.swap_dims({"x": "z"})
2088         <xarray.DataArray (z: 2)>
2089         array([0, 1])
2090         Coordinates:
2091             x        (z) <U1 'a' 'b'
2092             y        (z) int64 0 1
2093         Dimensions without coordinates: z
2094 
2095         See Also
2096         --------
2097         DataArray.rename
2098         Dataset.swap_dims
2099         """
2100         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
2101         ds = self._to_temp_dataset().swap_dims(dims_dict)
2102         return self._from_temp_dataset(ds)
2103 
2104     # change type of self and return to T_DataArray once
2105     # https://github.com/python/mypy/issues/12846 is resolved
2106     def expand_dims(
2107         self,
2108         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
2109         axis: None | int | Sequence[int] = None,
2110         **dim_kwargs: Any,
2111     ) -> DataArray:
2112         """Return a new object with an additional axis (or axes) inserted at
2113         the corresponding position in the array shape. The new object is a
2114         view into the underlying array, not a copy.
2115 
2116         If dim is already a scalar coordinate, it will be promoted to a 1D
2117         coordinate consisting of a single value.
2118 
2119         Parameters
2120         ----------
2121         dim : Hashable, sequence of Hashable, dict, or None, optional
2122             Dimensions to include on the new variable.
2123             If provided as str or sequence of str, then dimensions are inserted
2124             with length 1. If provided as a dict, then the keys are the new
2125             dimensions and the values are either integers (giving the length of
2126             the new dimensions) or sequence/ndarray (giving the coordinates of
2127             the new dimensions).
2128         axis : int, sequence of int, or None, default: None
2129             Axis position(s) where new axis is to be inserted (position(s) on
2130             the result array). If a sequence of integers is passed,
2131             multiple axes are inserted. In this case, dim arguments should be
2132             same length list. If axis=None is passed, all the axes will be
2133             inserted to the start of the result array.
2134         **dim_kwargs : int or sequence or ndarray
2135             The keywords are arbitrary dimensions being inserted and the values
2136             are either the lengths of the new dims (if int is given), or their
2137             coordinates. Note, this is an alternative to passing a dict to the
2138             dim kwarg and will only be used if dim is None.
2139 
2140         Returns
2141         -------
2142         expanded : DataArray
2143             This object, but with additional dimension(s).
2144 
2145         See Also
2146         --------
2147         Dataset.expand_dims
2148         """
2149         if isinstance(dim, int):
2150             raise TypeError("dim should be Hashable or sequence/mapping of Hashables")
2151         elif isinstance(dim, Sequence) and not isinstance(dim, str):
2152             if len(dim) != len(set(dim)):
2153                 raise ValueError("dims should not contain duplicate values.")
2154             dim = dict.fromkeys(dim, 1)
2155         elif dim is not None and not isinstance(dim, Mapping):
2156             dim = {cast(Hashable, dim): 1}
2157 
2158         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
2159         ds = self._to_temp_dataset().expand_dims(dim, axis)
2160         return self._from_temp_dataset(ds)
2161 
2162     # change type of self and return to T_DataArray once
2163     # https://github.com/python/mypy/issues/12846 is resolved
2164     def set_index(
2165         self,
2166         indexes: Mapping[Any, Hashable | Sequence[Hashable]] = None,
2167         append: bool = False,
2168         **indexes_kwargs: Hashable | Sequence[Hashable],
2169     ) -> DataArray:
2170         """Set DataArray (multi-)indexes using one or more existing
2171         coordinates.
2172 
2173         Parameters
2174         ----------
2175         indexes : {dim: index, ...}
2176             Mapping from names matching dimensions and values given
2177             by (lists of) the names of existing coordinates or variables to set
2178             as new (multi-)index.
2179         append : bool, default: False
2180             If True, append the supplied index(es) to the existing index(es).
2181             Otherwise replace the existing index(es).
2182         **indexes_kwargs : optional
2183             The keyword arguments form of ``indexes``.
2184             One of indexes or indexes_kwargs must be provided.
2185 
2186         Returns
2187         -------
2188         obj : DataArray
2189             Another DataArray, with this data but replaced coordinates.
2190 
2191         Examples
2192         --------
2193         >>> arr = xr.DataArray(
2194         ...     data=np.ones((2, 3)),
2195         ...     dims=["x", "y"],
2196         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
2197         ... )
2198         >>> arr
2199         <xarray.DataArray (x: 2, y: 3)>
2200         array([[1., 1., 1.],
2201                [1., 1., 1.]])
2202         Coordinates:
2203           * x        (x) int64 0 1
2204           * y        (y) int64 0 1 2
2205             a        (x) int64 3 4
2206         >>> arr.set_index(x="a")
2207         <xarray.DataArray (x: 2, y: 3)>
2208         array([[1., 1., 1.],
2209                [1., 1., 1.]])
2210         Coordinates:
2211           * x        (x) int64 3 4
2212           * y        (y) int64 0 1 2
2213 
2214         See Also
2215         --------
2216         DataArray.reset_index
2217         """
2218         ds = self._to_temp_dataset().set_index(indexes, append=append, **indexes_kwargs)
2219         return self._from_temp_dataset(ds)
2220 
2221     # change type of self and return to T_DataArray once
2222     # https://github.com/python/mypy/issues/12846 is resolved
2223     def reset_index(
2224         self,
2225         dims_or_levels: Hashable | Sequence[Hashable],
2226         drop: bool = False,
2227     ) -> DataArray:
2228         """Reset the specified index(es) or multi-index level(s).
2229 
2230         Parameters
2231         ----------
2232         dims_or_levels : Hashable or sequence of Hashable
2233             Name(s) of the dimension(s) and/or multi-index level(s) that will
2234             be reset.
2235         drop : bool, default: False
2236             If True, remove the specified indexes and/or multi-index levels
2237             instead of extracting them as new coordinates (default: False).
2238 
2239         Returns
2240         -------
2241         obj : DataArray
2242             Another dataarray, with this dataarray's data but replaced
2243             coordinates.
2244 
2245         See Also
2246         --------
2247         DataArray.set_index
2248         """
2249         ds = self._to_temp_dataset().reset_index(dims_or_levels, drop=drop)
2250         return self._from_temp_dataset(ds)
2251 
2252     def reorder_levels(
2253         self: T_DataArray,
2254         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
2255         **dim_order_kwargs: Sequence[int | Hashable],
2256     ) -> T_DataArray:
2257         """Rearrange index levels using input order.
2258 
2259         Parameters
2260         ----------
2261         dim_order dict-like of Hashable to int or Hashable: optional
2262             Mapping from names matching dimensions and values given
2263             by lists representing new level orders. Every given dimension
2264             must have a multi-index.
2265         **dim_order_kwargs : optional
2266             The keyword arguments form of ``dim_order``.
2267             One of dim_order or dim_order_kwargs must be provided.
2268 
2269         Returns
2270         -------
2271         obj : DataArray
2272             Another dataarray, with this dataarray's data but replaced
2273             coordinates.
2274         """
2275         ds = self._to_temp_dataset().reorder_levels(dim_order, **dim_order_kwargs)
2276         return self._from_temp_dataset(ds)
2277 
2278     def stack(
2279         self: T_DataArray,
2280         dimensions: Mapping[Any, Sequence[Hashable]] | None = None,
2281         create_index: bool | None = True,
2282         index_cls: type[Index] = PandasMultiIndex,
2283         **dimensions_kwargs: Sequence[Hashable],
2284     ) -> T_DataArray:
2285         """
2286         Stack any number of existing dimensions into a single new dimension.
2287 
2288         New dimensions will be added at the end, and the corresponding
2289         coordinate variables will be combined into a MultiIndex.
2290 
2291         Parameters
2292         ----------
2293         dimensions : mapping of Hashable to sequence of Hashable
2294             Mapping of the form `new_name=(dim1, dim2, ...)`.
2295             Names of new dimensions, and the existing dimensions that they
2296             replace. An ellipsis (`...`) will be replaced by all unlisted dimensions.
2297             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
2298             all dimensions.
2299         create_index : bool or None, default: True
2300             If True, create a multi-index for each of the stacked dimensions.
2301             If False, don't create any index.
2302             If None, create a multi-index only if exactly one single (1-d) coordinate
2303             index is found for every dimension to stack.
2304         index_cls: class, optional
2305             Can be used to pass a custom multi-index type. Must be an Xarray index that
2306             implements `.stack()`. By default, a pandas multi-index wrapper is used.
2307         **dimensions_kwargs
2308             The keyword arguments form of ``dimensions``.
2309             One of dimensions or dimensions_kwargs must be provided.
2310 
2311         Returns
2312         -------
2313         stacked : DataArray
2314             DataArray with stacked data.
2315 
2316         Examples
2317         --------
2318         >>> arr = xr.DataArray(
2319         ...     np.arange(6).reshape(2, 3),
2320         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2321         ... )
2322         >>> arr
2323         <xarray.DataArray (x: 2, y: 3)>
2324         array([[0, 1, 2],
2325                [3, 4, 5]])
2326         Coordinates:
2327           * x        (x) <U1 'a' 'b'
2328           * y        (y) int64 0 1 2
2329         >>> stacked = arr.stack(z=("x", "y"))
2330         >>> stacked.indexes["z"]
2331         MultiIndex([('a', 0),
2332                     ('a', 1),
2333                     ('a', 2),
2334                     ('b', 0),
2335                     ('b', 1),
2336                     ('b', 2)],
2337                    name='z')
2338 
2339         See Also
2340         --------
2341         DataArray.unstack
2342         """
2343         ds = self._to_temp_dataset().stack(
2344             dimensions,
2345             create_index=create_index,
2346             index_cls=index_cls,
2347             **dimensions_kwargs,
2348         )
2349         return self._from_temp_dataset(ds)
2350 
2351     # change type of self and return to T_DataArray once
2352     # https://github.com/python/mypy/issues/12846 is resolved
2353     def unstack(
2354         self,
2355         dim: Hashable | Sequence[Hashable] | None = None,
2356         fill_value: Any = dtypes.NA,
2357         sparse: bool = False,
2358     ) -> DataArray:
2359         """
2360         Unstack existing dimensions corresponding to MultiIndexes into
2361         multiple new dimensions.
2362 
2363         New dimensions will be added at the end.
2364 
2365         Parameters
2366         ----------
2367         dim : Hashable or sequence of Hashable, optional
2368             Dimension(s) over which to unstack. By default unstacks all
2369             MultiIndexes.
2370         fill_value : scalar or dict-like, default: nan
2371             Value to be filled. If a dict-like, maps variable names to
2372             fill values. Use the data array's name to refer to its
2373             name. If not provided or if the dict-like does not contain
2374             all variables, the dtype's NA value will be used.
2375         sparse : bool, default: False
2376             Use sparse-array if True
2377 
2378         Returns
2379         -------
2380         unstacked : DataArray
2381             Array with unstacked data.
2382 
2383         Examples
2384         --------
2385         >>> arr = xr.DataArray(
2386         ...     np.arange(6).reshape(2, 3),
2387         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2388         ... )
2389         >>> arr
2390         <xarray.DataArray (x: 2, y: 3)>
2391         array([[0, 1, 2],
2392                [3, 4, 5]])
2393         Coordinates:
2394           * x        (x) <U1 'a' 'b'
2395           * y        (y) int64 0 1 2
2396         >>> stacked = arr.stack(z=("x", "y"))
2397         >>> stacked.indexes["z"]
2398         MultiIndex([('a', 0),
2399                     ('a', 1),
2400                     ('a', 2),
2401                     ('b', 0),
2402                     ('b', 1),
2403                     ('b', 2)],
2404                    name='z')
2405         >>> roundtripped = stacked.unstack()
2406         >>> arr.identical(roundtripped)
2407         True
2408 
2409         See Also
2410         --------
2411         DataArray.stack
2412         """
2413         ds = self._to_temp_dataset().unstack(dim, fill_value, sparse)
2414         return self._from_temp_dataset(ds)
2415 
2416     def to_unstacked_dataset(self, dim: Hashable, level: int | Hashable = 0) -> Dataset:
2417         """Unstack DataArray expanding to Dataset along a given level of a
2418         stacked coordinate.
2419 
2420         This is the inverse operation of Dataset.to_stacked_array.
2421 
2422         Parameters
2423         ----------
2424         dim : Hashable
2425             Name of existing dimension to unstack
2426         level : int or Hashable, default: 0
2427             The MultiIndex level to expand to a dataset along. Can either be
2428             the integer index of the level or its name.
2429 
2430         Returns
2431         -------
2432         unstacked: Dataset
2433 
2434         Examples
2435         --------
2436         >>> arr = xr.DataArray(
2437         ...     np.arange(6).reshape(2, 3),
2438         ...     coords=[("x", ["a", "b"]), ("y", [0, 1, 2])],
2439         ... )
2440         >>> data = xr.Dataset({"a": arr, "b": arr.isel(y=0)})
2441         >>> data
2442         <xarray.Dataset>
2443         Dimensions:  (x: 2, y: 3)
2444         Coordinates:
2445           * x        (x) <U1 'a' 'b'
2446           * y        (y) int64 0 1 2
2447         Data variables:
2448             a        (x, y) int64 0 1 2 3 4 5
2449             b        (x) int64 0 3
2450         >>> stacked = data.to_stacked_array("z", ["x"])
2451         >>> stacked.indexes["z"]
2452         MultiIndex([('a', 0.0),
2453                     ('a', 1.0),
2454                     ('a', 2.0),
2455                     ('b', nan)],
2456                    name='z')
2457         >>> roundtripped = stacked.to_unstacked_dataset(dim="z")
2458         >>> data.identical(roundtripped)
2459         True
2460 
2461         See Also
2462         --------
2463         Dataset.to_stacked_array
2464         """
2465         idx = self._indexes[dim].to_pandas_index()
2466         if not isinstance(idx, pd.MultiIndex):
2467             raise ValueError(f"'{dim}' is not a stacked coordinate")
2468 
2469         level_number = idx._get_level_number(level)
2470         variables = idx.levels[level_number]
2471         variable_dim = idx.names[level_number]
2472 
2473         # pull variables out of datarray
2474         data_dict = {}
2475         for k in variables:
2476             data_dict[k] = self.sel({variable_dim: k}, drop=True).squeeze(drop=True)
2477 
2478         # unstacked dataset
2479         return Dataset(data_dict)
2480 
2481     def transpose(
2482         self: T_DataArray,
2483         *dims: Hashable,
2484         transpose_coords: bool = True,
2485         missing_dims: ErrorOptionsWithWarn = "raise",
2486     ) -> T_DataArray:
2487         """Return a new DataArray object with transposed dimensions.
2488 
2489         Parameters
2490         ----------
2491         *dims : Hashable, optional
2492             By default, reverse the dimensions. Otherwise, reorder the
2493             dimensions to this order.
2494         transpose_coords : bool, default: True
2495             If True, also transpose the coordinates of this DataArray.
2496         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2497             What to do if dimensions that should be selected from are not present in the
2498             DataArray:
2499             - "raise": raise an exception
2500             - "warn": raise a warning, and ignore the missing dimensions
2501             - "ignore": ignore the missing dimensions
2502 
2503         Returns
2504         -------
2505         transposed : DataArray
2506             The returned DataArray's array is transposed.
2507 
2508         Notes
2509         -----
2510         This operation returns a view of this array's data. It is
2511         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
2512         -- the data will be fully loaded.
2513 
2514         See Also
2515         --------
2516         numpy.transpose
2517         Dataset.transpose
2518         """
2519         if dims:
2520             dims = tuple(utils.infix_dims(dims, self.dims, missing_dims))
2521         variable = self.variable.transpose(*dims)
2522         if transpose_coords:
2523             coords: dict[Hashable, Variable] = {}
2524             for name, coord in self.coords.items():
2525                 coord_dims = tuple(dim for dim in dims if dim in coord.dims)
2526                 coords[name] = coord.variable.transpose(*coord_dims)
2527             return self._replace(variable, coords)
2528         else:
2529             return self._replace(variable)
2530 
2531     @property
2532     def T(self: T_DataArray) -> T_DataArray:
2533         return self.transpose()
2534 
2535     # change type of self and return to T_DataArray once
2536     # https://github.com/python/mypy/issues/12846 is resolved
2537     def drop_vars(
2538         self,
2539         names: Hashable | Iterable[Hashable],
2540         *,
2541         errors: ErrorOptions = "raise",
2542     ) -> DataArray:
2543         """Returns an array with dropped variables.
2544 
2545         Parameters
2546         ----------
2547         names : Hashable or iterable of Hashable
2548             Name(s) of variables to drop.
2549         errors : {"raise", "ignore"}, default: "raise"
2550             If 'raise', raises a ValueError error if any of the variable
2551             passed are not in the dataset. If 'ignore', any given names that are in the
2552             DataArray are dropped and no error is raised.
2553 
2554         Returns
2555         -------
2556         dropped : Dataset
2557             New Dataset copied from `self` with variables removed.
2558         """
2559         ds = self._to_temp_dataset().drop_vars(names, errors=errors)
2560         return self._from_temp_dataset(ds)
2561 
2562     def drop(
2563         self: T_DataArray,
2564         labels: Mapping[Any, Any] | None = None,
2565         dim: Hashable | None = None,
2566         *,
2567         errors: ErrorOptions = "raise",
2568         **labels_kwargs,
2569     ) -> T_DataArray:
2570         """Backward compatible method based on `drop_vars` and `drop_sel`
2571 
2572         Using either `drop_vars` or `drop_sel` is encouraged
2573 
2574         See Also
2575         --------
2576         DataArray.drop_vars
2577         DataArray.drop_sel
2578         """
2579         ds = self._to_temp_dataset().drop(labels, dim, errors=errors, **labels_kwargs)
2580         return self._from_temp_dataset(ds)
2581 
2582     def drop_sel(
2583         self: T_DataArray,
2584         labels: Mapping[Any, Any] | None = None,
2585         *,
2586         errors: ErrorOptions = "raise",
2587         **labels_kwargs,
2588     ) -> T_DataArray:
2589         """Drop index labels from this DataArray.
2590 
2591         Parameters
2592         ----------
2593         labels : mapping of Hashable to Any
2594             Index labels to drop
2595         errors : {"raise", "ignore"}, default: "raise"
2596             If 'raise', raises a ValueError error if
2597             any of the index labels passed are not
2598             in the dataset. If 'ignore', any given labels that are in the
2599             dataset are dropped and no error is raised.
2600         **labels_kwargs : {dim: label, ...}, optional
2601             The keyword arguments form of ``dim`` and ``labels``
2602 
2603         Returns
2604         -------
2605         dropped : DataArray
2606         """
2607         if labels_kwargs or isinstance(labels, dict):
2608             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
2609 
2610         ds = self._to_temp_dataset().drop_sel(labels, errors=errors)
2611         return self._from_temp_dataset(ds)
2612 
2613     def drop_isel(
2614         self: T_DataArray, indexers: Mapping[Any, Any] | None = None, **indexers_kwargs
2615     ) -> T_DataArray:
2616         """Drop index positions from this DataArray.
2617 
2618         Parameters
2619         ----------
2620         indexers : mapping of Hashable to Any or None, default: None
2621             Index locations to drop
2622         **indexers_kwargs : {dim: position, ...}, optional
2623             The keyword arguments form of ``dim`` and ``positions``
2624 
2625         Returns
2626         -------
2627         dropped : DataArray
2628 
2629         Raises
2630         ------
2631         IndexError
2632         """
2633         dataset = self._to_temp_dataset()
2634         dataset = dataset.drop_isel(indexers=indexers, **indexers_kwargs)
2635         return self._from_temp_dataset(dataset)
2636 
2637     def dropna(
2638         self: T_DataArray,
2639         dim: Hashable,
2640         how: Literal["any", "all"] = "any",
2641         thresh: int | None = None,
2642     ) -> T_DataArray:
2643         """Returns a new array with dropped labels for missing values along
2644         the provided dimension.
2645 
2646         Parameters
2647         ----------
2648         dim : Hashable
2649             Dimension along which to drop missing values. Dropping along
2650             multiple dimensions simultaneously is not yet supported.
2651         how : {"any", "all"}, default: "any"
2652             - any : if any NA values are present, drop that label
2653             - all : if all values are NA, drop that label
2654 
2655         thresh : int or None, default: None
2656             If supplied, require this many non-NA values.
2657 
2658         Returns
2659         -------
2660         dropped : DataArray
2661         """
2662         ds = self._to_temp_dataset().dropna(dim, how=how, thresh=thresh)
2663         return self._from_temp_dataset(ds)
2664 
2665     def fillna(self: T_DataArray, value: Any) -> T_DataArray:
2666         """Fill missing values in this object.
2667 
2668         This operation follows the normal broadcasting and alignment rules that
2669         xarray uses for binary arithmetic, except the result is aligned to this
2670         object (``join='left'``) instead of aligned to the intersection of
2671         index coordinates (``join='inner'``).
2672 
2673         Parameters
2674         ----------
2675         value : scalar, ndarray or DataArray
2676             Used to fill all matching missing values in this array. If the
2677             argument is a DataArray, it is first aligned with (reindexed to)
2678             this array.
2679 
2680         Returns
2681         -------
2682         filled : DataArray
2683         """
2684         if utils.is_dict_like(value):
2685             raise TypeError(
2686                 "cannot provide fill value as a dictionary with "
2687                 "fillna on a DataArray"
2688             )
2689         out = ops.fillna(self, value)
2690         return out
2691 
2692     def interpolate_na(
2693         self: T_DataArray,
2694         dim: Hashable | None = None,
2695         method: InterpOptions = "linear",
2696         limit: int | None = None,
2697         use_coordinate: bool | str = True,
2698         max_gap: (
2699             None
2700             | int
2701             | float
2702             | str
2703             | pd.Timedelta
2704             | np.timedelta64
2705             | datetime.timedelta
2706         ) = None,
2707         keep_attrs: bool | None = None,
2708         **kwargs: Any,
2709     ) -> T_DataArray:
2710         """Fill in NaNs by interpolating according to different methods.
2711 
2712         Parameters
2713         ----------
2714         dim : Hashable or None, optional
2715             Specifies the dimension along which to interpolate.
2716         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
2717             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
2718             String indicating which method to use for interpolation:
2719 
2720             - 'linear': linear interpolation. Additional keyword
2721               arguments are passed to :py:func:`numpy.interp`
2722             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
2723               are passed to :py:func:`scipy.interpolate.interp1d`. If
2724               ``method='polynomial'``, the ``order`` keyword argument must also be
2725               provided.
2726             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
2727               respective :py:class:`scipy.interpolate` classes.
2728 
2729         use_coordinate : bool or str, default: True
2730             Specifies which index to use as the x values in the interpolation
2731             formulated as `y = f(x)`. If False, values are treated as if
2732             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
2733             used. If ``use_coordinate`` is a string, it specifies the name of a
2734             coordinate variariable to use as the index.
2735         limit : int or None, default: None
2736             Maximum number of consecutive NaNs to fill. Must be greater than 0
2737             or None for no limit. This filling is done regardless of the size of
2738             the gap in the data. To only interpolate over gaps less than a given length,
2739             see ``max_gap``.
2740         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
2741             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
2742             Use None for no limit. When interpolating along a datetime64 dimension
2743             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
2744 
2745             - a string that is valid input for pandas.to_timedelta
2746             - a :py:class:`numpy.timedelta64` object
2747             - a :py:class:`pandas.Timedelta` object
2748             - a :py:class:`datetime.timedelta` object
2749 
2750             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
2751             dimensions has not been implemented yet. Gap length is defined as the difference
2752             between coordinate values at the first data point after a gap and the last value
2753             before a gap. For gaps at the beginning (end), gap length is defined as the difference
2754             between coordinate values at the first (last) valid data point and the first (last) NaN.
2755             For example, consider::
2756 
2757                 <xarray.DataArray (x: 9)>
2758                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
2759                 Coordinates:
2760                   * x        (x) int64 0 1 2 3 4 5 6 7 8
2761 
2762             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
2763         keep_attrs : bool or None, default: None
2764             If True, the dataarray's attributes (`attrs`) will be copied from
2765             the original object to the new one.  If False, the new
2766             object will be returned without attributes.
2767         **kwargs : dict, optional
2768             parameters passed verbatim to the underlying interpolation function
2769 
2770         Returns
2771         -------
2772         interpolated: DataArray
2773             Filled in DataArray.
2774 
2775         See Also
2776         --------
2777         numpy.interp
2778         scipy.interpolate
2779 
2780         Examples
2781         --------
2782         >>> da = xr.DataArray(
2783         ...     [np.nan, 2, 3, np.nan, 0], dims="x", coords={"x": [0, 1, 2, 3, 4]}
2784         ... )
2785         >>> da
2786         <xarray.DataArray (x: 5)>
2787         array([nan,  2.,  3., nan,  0.])
2788         Coordinates:
2789           * x        (x) int64 0 1 2 3 4
2790 
2791         >>> da.interpolate_na(dim="x", method="linear")
2792         <xarray.DataArray (x: 5)>
2793         array([nan, 2. , 3. , 1.5, 0. ])
2794         Coordinates:
2795           * x        (x) int64 0 1 2 3 4
2796 
2797         >>> da.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
2798         <xarray.DataArray (x: 5)>
2799         array([1. , 2. , 3. , 1.5, 0. ])
2800         Coordinates:
2801           * x        (x) int64 0 1 2 3 4
2802         """
2803         from .missing import interp_na
2804 
2805         return interp_na(
2806             self,
2807             dim=dim,
2808             method=method,
2809             limit=limit,
2810             use_coordinate=use_coordinate,
2811             max_gap=max_gap,
2812             keep_attrs=keep_attrs,
2813             **kwargs,
2814         )
2815 
2816     def ffill(
2817         self: T_DataArray, dim: Hashable, limit: int | None = None
2818     ) -> T_DataArray:
2819         """Fill NaN values by propagating values forward
2820 
2821         *Requires bottleneck.*
2822 
2823         Parameters
2824         ----------
2825         dim : Hashable
2826             Specifies the dimension along which to propagate values when
2827             filling.
2828         limit : int or None, default: None
2829             The maximum number of consecutive NaN values to forward fill. In
2830             other words, if there is a gap with more than this number of
2831             consecutive NaNs, it will only be partially filled. Must be greater
2832             than 0 or None for no limit. Must be None or greater than or equal
2833             to axis length if filling along chunked axes (dimensions).
2834 
2835         Returns
2836         -------
2837         filled : DataArray
2838         """
2839         from .missing import ffill
2840 
2841         return ffill(self, dim, limit=limit)
2842 
2843     def bfill(
2844         self: T_DataArray, dim: Hashable, limit: int | None = None
2845     ) -> T_DataArray:
2846         """Fill NaN values by propagating values backward
2847 
2848         *Requires bottleneck.*
2849 
2850         Parameters
2851         ----------
2852         dim : str
2853             Specifies the dimension along which to propagate values when
2854             filling.
2855         limit : int or None, default: None
2856             The maximum number of consecutive NaN values to backward fill. In
2857             other words, if there is a gap with more than this number of
2858             consecutive NaNs, it will only be partially filled. Must be greater
2859             than 0 or None for no limit. Must be None or greater than or equal
2860             to axis length if filling along chunked axes (dimensions).
2861 
2862         Returns
2863         -------
2864         filled : DataArray
2865         """
2866         from .missing import bfill
2867 
2868         return bfill(self, dim, limit=limit)
2869 
2870     def combine_first(self: T_DataArray, other: T_DataArray) -> T_DataArray:
2871         """Combine two DataArray objects, with union of coordinates.
2872 
2873         This operation follows the normal broadcasting and alignment rules of
2874         ``join='outer'``.  Default to non-null values of array calling the
2875         method.  Use np.nan to fill in vacant cells after alignment.
2876 
2877         Parameters
2878         ----------
2879         other : DataArray
2880             Used to fill all matching missing values in this array.
2881 
2882         Returns
2883         -------
2884         DataArray
2885         """
2886         return ops.fillna(self, other, join="outer")
2887 
2888     def reduce(
2889         self: T_DataArray,
2890         func: Callable[..., Any],
2891         dim: None | Hashable | Iterable[Hashable] = None,
2892         *,
2893         axis: None | int | Sequence[int] = None,
2894         keep_attrs: bool | None = None,
2895         keepdims: bool = False,
2896         **kwargs: Any,
2897     ) -> T_DataArray:
2898         """Reduce this array by applying `func` along some dimension(s).
2899 
2900         Parameters
2901         ----------
2902         func : callable
2903             Function which can be called in the form
2904             `f(x, axis=axis, **kwargs)` to return the result of reducing an
2905             np.ndarray over an integer valued axis.
2906         dim : Hashable or Iterable of Hashable, optional
2907             Dimension(s) over which to apply `func`.
2908         axis : int or sequence of int, optional
2909             Axis(es) over which to repeatedly apply `func`. Only one of the
2910             'dim' and 'axis' arguments can be supplied. If neither are
2911             supplied, then the reduction is calculated over the flattened array
2912             (by calling `f(x)` without an axis argument).
2913         keep_attrs : bool or None, optional
2914             If True, the variable's attributes (`attrs`) will be copied from
2915             the original object to the new one.  If False (default), the new
2916             object will be returned without attributes.
2917         keepdims : bool, default: False
2918             If True, the dimensions which are reduced are left in the result
2919             as dimensions of size one. Coordinates that use these dimensions
2920             are removed.
2921         **kwargs : dict
2922             Additional keyword arguments passed on to `func`.
2923 
2924         Returns
2925         -------
2926         reduced : DataArray
2927             DataArray with this object's array replaced with an array with
2928             summarized data and the indicated dimension(s) removed.
2929         """
2930 
2931         var = self.variable.reduce(func, dim, axis, keep_attrs, keepdims, **kwargs)
2932         return self._replace_maybe_drop_dims(var)
2933 
2934     def to_pandas(self) -> DataArray | pd.Series | pd.DataFrame:
2935         """Convert this array into a pandas object with the same shape.
2936 
2937         The type of the returned object depends on the number of DataArray
2938         dimensions:
2939 
2940         * 0D -> `xarray.DataArray`
2941         * 1D -> `pandas.Series`
2942         * 2D -> `pandas.DataFrame`
2943 
2944         Only works for arrays with 2 or fewer dimensions.
2945 
2946         The DataArray constructor performs the inverse transformation.
2947 
2948         Returns
2949         -------
2950         result : DataArray | Series | DataFrame
2951             DataArray, pandas Series or pandas DataFrame.
2952         """
2953         # TODO: consolidate the info about pandas constructors and the
2954         # attributes that correspond to their indexes into a separate module?
2955         constructors = {0: lambda x: x, 1: pd.Series, 2: pd.DataFrame}
2956         try:
2957             constructor = constructors[self.ndim]
2958         except KeyError:
2959             raise ValueError(
2960                 f"Cannot convert arrays with {self.ndim} dimensions into "
2961                 "pandas objects. Requires 2 or fewer dimensions."
2962             )
2963         indexes = [self.get_index(dim) for dim in self.dims]
2964         return constructor(self.values, *indexes)
2965 
2966     def to_dataframe(
2967         self, name: Hashable | None = None, dim_order: Sequence[Hashable] | None = None
2968     ) -> pd.DataFrame:
2969         """Convert this array and its coordinates into a tidy pandas.DataFrame.
2970 
2971         The DataFrame is indexed by the Cartesian product of index coordinates
2972         (in the form of a :py:class:`pandas.MultiIndex`). Other coordinates are
2973         included as columns in the DataFrame.
2974 
2975         For 1D and 2D DataArrays, see also :py:func:`DataArray.to_pandas` which
2976         doesn't rely on a MultiIndex to build the DataFrame.
2977 
2978         Parameters
2979         ----------
2980         name: Hashable or None, optional
2981             Name to give to this array (required if unnamed).
2982         dim_order: Sequence of Hashable or None, optional
2983             Hierarchical dimension order for the resulting dataframe.
2984             Array content is transposed to this order and then written out as flat
2985             vectors in contiguous order, so the last dimension in this list
2986             will be contiguous in the resulting DataFrame. This has a major
2987             influence on which operations are efficient on the resulting
2988             dataframe.
2989 
2990             If provided, must include all dimensions of this DataArray. By default,
2991             dimensions are sorted according to the DataArray dimensions order.
2992 
2993         Returns
2994         -------
2995         result: DataFrame
2996             DataArray as a pandas DataFrame.
2997 
2998         See also
2999         --------
3000         DataArray.to_pandas
3001         DataArray.to_series
3002         """
3003         if name is None:
3004             name = self.name
3005         if name is None:
3006             raise ValueError(
3007                 "cannot convert an unnamed DataArray to a "
3008                 "DataFrame: use the ``name`` parameter"
3009             )
3010         if self.ndim == 0:
3011             raise ValueError("cannot convert a scalar to a DataFrame")
3012 
3013         # By using a unique name, we can convert a DataArray into a DataFrame
3014         # even if it shares a name with one of its coordinates.
3015         # I would normally use unique_name = object() but that results in a
3016         # dataframe with columns in the wrong order, for reasons I have not
3017         # been able to debug (possibly a pandas bug?).
3018         unique_name = "__unique_name_identifier_z98xfz98xugfg73ho__"
3019         ds = self._to_dataset_whole(name=unique_name)
3020 
3021         if dim_order is None:
3022             ordered_dims = dict(zip(self.dims, self.shape))
3023         else:
3024             ordered_dims = ds._normalize_dim_order(dim_order=dim_order)
3025 
3026         df = ds._to_dataframe(ordered_dims)
3027         df.columns = [name if c == unique_name else c for c in df.columns]
3028         return df
3029 
3030     def to_series(self) -> pd.Series:
3031         """Convert this array into a pandas.Series.
3032 
3033         The Series is indexed by the Cartesian product of index coordinates
3034         (in the form of a :py:class:`pandas.MultiIndex`).
3035 
3036         Returns
3037         -------
3038         result : Series
3039             DataArray as a pandas Series.
3040 
3041         See also
3042         --------
3043         DataArray.to_pandas
3044         DataArray.to_dataframe
3045         """
3046         index = self.coords.to_index()
3047         return pd.Series(self.values.reshape(-1), index=index, name=self.name)
3048 
3049     def to_masked_array(self, copy: bool = True) -> np.ma.MaskedArray:
3050         """Convert this array into a numpy.ma.MaskedArray
3051 
3052         Parameters
3053         ----------
3054         copy : bool, default: True
3055             If True make a copy of the array in the result. If False,
3056             a MaskedArray view of DataArray.values is returned.
3057 
3058         Returns
3059         -------
3060         result : MaskedArray
3061             Masked where invalid values (nan or inf) occur.
3062         """
3063         values = self.to_numpy()  # only compute lazy arrays once
3064         isnull = pd.isnull(values)
3065         return np.ma.MaskedArray(data=values, mask=isnull, copy=copy)
3066 
3067     # path=None writes to bytes
3068     @overload
3069     def to_netcdf(
3070         self,
3071         path: None = None,
3072         mode: Literal["w", "a"] = "w",
3073         format: T_NetcdfTypes | None = None,
3074         group: str | None = None,
3075         engine: T_NetcdfEngine | None = None,
3076         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3077         unlimited_dims: Iterable[Hashable] | None = None,
3078         compute: bool = True,
3079         invalid_netcdf: bool = False,
3080     ) -> bytes:
3081         ...
3082 
3083     # default return None
3084     @overload
3085     def to_netcdf(
3086         self,
3087         path: str | PathLike,
3088         mode: Literal["w", "a"] = "w",
3089         format: T_NetcdfTypes | None = None,
3090         group: str | None = None,
3091         engine: T_NetcdfEngine | None = None,
3092         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3093         unlimited_dims: Iterable[Hashable] | None = None,
3094         compute: Literal[True] = True,
3095         invalid_netcdf: bool = False,
3096     ) -> None:
3097         ...
3098 
3099     # compute=False returns dask.Delayed
3100     @overload
3101     def to_netcdf(
3102         self,
3103         path: str | PathLike,
3104         mode: Literal["w", "a"] = "w",
3105         format: T_NetcdfTypes | None = None,
3106         group: str | None = None,
3107         engine: T_NetcdfEngine | None = None,
3108         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3109         unlimited_dims: Iterable[Hashable] | None = None,
3110         *,
3111         compute: Literal[False],
3112         invalid_netcdf: bool = False,
3113     ) -> Delayed:
3114         ...
3115 
3116     def to_netcdf(
3117         self,
3118         path: str | PathLike | None = None,
3119         mode: Literal["w", "a"] = "w",
3120         format: T_NetcdfTypes | None = None,
3121         group: str | None = None,
3122         engine: T_NetcdfEngine | None = None,
3123         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
3124         unlimited_dims: Iterable[Hashable] | None = None,
3125         compute: bool = True,
3126         invalid_netcdf: bool = False,
3127     ) -> bytes | Delayed | None:
3128         """Write dataset contents to a netCDF file.
3129 
3130         Parameters
3131         ----------
3132         path : str, path-like or None, optional
3133             Path to which to save this dataset. File-like objects are only
3134             supported by the scipy engine. If no path is provided, this
3135             function returns the resulting netCDF file as bytes; in this case,
3136             we need to use scipy, which does not support netCDF version 4 (the
3137             default format becomes NETCDF3_64BIT).
3138         mode : {"w", "a"}, default: "w"
3139             Write ('w') or append ('a') mode. If mode='w', any existing file at
3140             this location will be overwritten. If mode='a', existing variables
3141             will be overwritten.
3142         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
3143                   "NETCDF3_CLASSIC"}, optional
3144             File format for the resulting netCDF file:
3145 
3146             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
3147               features.
3148             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
3149               netCDF 3 compatible API features.
3150             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
3151               which fully supports 2+ GB files, but is only compatible with
3152               clients linked against netCDF version 3.6.0 or later.
3153             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
3154               handle 2+ GB files very well.
3155 
3156             All formats are supported by the netCDF4-python library.
3157             scipy.io.netcdf only supports the last two formats.
3158 
3159             The default format is NETCDF4 if you are saving a file to disk and
3160             have the netCDF4-python library available. Otherwise, xarray falls
3161             back to using scipy to write netCDF files and defaults to the
3162             NETCDF3_64BIT format (scipy does not support netCDF4).
3163         group : str, optional
3164             Path to the netCDF4 group in the given file to open (only works for
3165             format='NETCDF4'). The group(s) will be created if necessary.
3166         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
3167             Engine to use when writing netCDF files. If not provided, the
3168             default engine is chosen based on available dependencies, with a
3169             preference for 'netcdf4' if writing to a file on disk.
3170         encoding : dict, optional
3171             Nested dictionary with variable names as keys and dictionaries of
3172             variable specific encodings as values, e.g.,
3173             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
3174             "zlib": True}, ...}``
3175 
3176             The `h5netcdf` engine supports both the NetCDF4-style compression
3177             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
3178             ones ``{"compression": "gzip", "compression_opts": 9}``.
3179             This allows using any compression plugin installed in the HDF5
3180             library, e.g. LZF.
3181 
3182         unlimited_dims : iterable of Hashable, optional
3183             Dimension(s) that should be serialized as unlimited dimensions.
3184             By default, no dimensions are treated as unlimited dimensions.
3185             Note that unlimited_dims may also be set via
3186             ``dataset.encoding["unlimited_dims"]``.
3187         compute: bool, default: True
3188             If true compute immediately, otherwise return a
3189             ``dask.delayed.Delayed`` object that can be computed later.
3190         invalid_netcdf: bool, default: False
3191             Only valid along with ``engine="h5netcdf"``. If True, allow writing
3192             hdf5 files which are invalid netcdf as described in
3193             https://github.com/h5netcdf/h5netcdf.
3194 
3195         Returns
3196         -------
3197         store: bytes or Delayed or None
3198             * ``bytes`` if path is None
3199             * ``dask.delayed.Delayed`` if compute is False
3200             * None otherwise
3201 
3202         Notes
3203         -----
3204         Only xarray.Dataset objects can be written to netCDF files, so
3205         the xarray.DataArray is converted to a xarray.Dataset object
3206         containing a single variable. If the DataArray has no name, or if the
3207         name is the same as a coordinate name, then it is given the name
3208         ``"__xarray_dataarray_variable__"``.
3209 
3210         See Also
3211         --------
3212         Dataset.to_netcdf
3213         """
3214         from ..backends.api import DATAARRAY_NAME, DATAARRAY_VARIABLE, to_netcdf
3215 
3216         if self.name is None:
3217             # If no name is set then use a generic xarray name
3218             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3219         elif self.name in self.coords or self.name in self.dims:
3220             # The name is the same as one of the coords names, which netCDF
3221             # doesn't support, so rename it but keep track of the old name
3222             dataset = self.to_dataset(name=DATAARRAY_VARIABLE)
3223             dataset.attrs[DATAARRAY_NAME] = self.name
3224         else:
3225             # No problems with the name - so we're fine!
3226             dataset = self.to_dataset()
3227 
3228         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
3229             dataset,
3230             path,
3231             mode=mode,
3232             format=format,
3233             group=group,
3234             engine=engine,
3235             encoding=encoding,
3236             unlimited_dims=unlimited_dims,
3237             compute=compute,
3238             multifile=False,
3239             invalid_netcdf=invalid_netcdf,
3240         )
3241 
3242     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
3243         """
3244         Convert this xarray.DataArray into a dictionary following xarray
3245         naming conventions.
3246 
3247         Converts all variables and attributes to native Python objects.
3248         Useful for converting to json. To avoid datetime incompatibility
3249         use decode_times=False kwarg in xarray.open_dataset.
3250 
3251         Parameters
3252         ----------
3253         data : bool, default: True
3254             Whether to include the actual data in the dictionary. When set to
3255             False, returns just the schema.
3256         encoding : bool, default: False
3257             Whether to include the Dataset's encoding in the dictionary.
3258 
3259         Returns
3260         -------
3261         dict: dict
3262 
3263         See Also
3264         --------
3265         DataArray.from_dict
3266         Dataset.to_dict
3267         """
3268         d = self.variable.to_dict(data=data)
3269         d.update({"coords": {}, "name": self.name})
3270         for k in self.coords:
3271             d["coords"][k] = self.coords[k].variable.to_dict(data=data)
3272         if encoding:
3273             d["encoding"] = dict(self.encoding)
3274         return d
3275 
3276     @classmethod
3277     def from_dict(cls: type[T_DataArray], d: Mapping[str, Any]) -> T_DataArray:
3278         """Convert a dictionary into an xarray.DataArray
3279 
3280         Parameters
3281         ----------
3282         d : dict
3283             Mapping with a minimum structure of {"dims": [...], "data": [...]}
3284 
3285         Returns
3286         -------
3287         obj : xarray.DataArray
3288 
3289         See Also
3290         --------
3291         DataArray.to_dict
3292         Dataset.from_dict
3293 
3294         Examples
3295         --------
3296         >>> d = {"dims": "t", "data": [1, 2, 3]}
3297         >>> da = xr.DataArray.from_dict(d)
3298         >>> da
3299         <xarray.DataArray (t: 3)>
3300         array([1, 2, 3])
3301         Dimensions without coordinates: t
3302 
3303         >>> d = {
3304         ...     "coords": {
3305         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
3306         ...     },
3307         ...     "attrs": {"title": "air temperature"},
3308         ...     "dims": "t",
3309         ...     "data": [10, 20, 30],
3310         ...     "name": "a",
3311         ... }
3312         >>> da = xr.DataArray.from_dict(d)
3313         >>> da
3314         <xarray.DataArray 'a' (t: 3)>
3315         array([10, 20, 30])
3316         Coordinates:
3317           * t        (t) int64 0 1 2
3318         Attributes:
3319             title:    air temperature
3320         """
3321         coords = None
3322         if "coords" in d:
3323             try:
3324                 coords = {
3325                     k: (v["dims"], v["data"], v.get("attrs"))
3326                     for k, v in d["coords"].items()
3327                 }
3328             except KeyError as e:
3329                 raise ValueError(
3330                     "cannot convert dict when coords are missing the key "
3331                     "'{dims_data}'".format(dims_data=str(e.args[0]))
3332                 )
3333         try:
3334             data = d["data"]
3335         except KeyError:
3336             raise ValueError("cannot convert dict without the key 'data''")
3337         else:
3338             obj = cls(data, coords, d.get("dims"), d.get("name"), d.get("attrs"))
3339 
3340         obj.encoding.update(d.get("encoding", {}))
3341 
3342         return obj
3343 
3344     @classmethod
3345     def from_series(cls, series: pd.Series, sparse: bool = False) -> DataArray:
3346         """Convert a pandas.Series into an xarray.DataArray.
3347 
3348         If the series's index is a MultiIndex, it will be expanded into a
3349         tensor product of one-dimensional coordinates (filling in missing
3350         values with NaN). Thus this operation should be the inverse of the
3351         `to_series` method.
3352 
3353         Parameters
3354         ----------
3355         series : Series
3356             Pandas Series object to convert.
3357         sparse : bool, default: False
3358             If sparse=True, creates a sparse array instead of a dense NumPy array.
3359             Requires the pydata/sparse package.
3360 
3361         See Also
3362         --------
3363         DataArray.to_series
3364         Dataset.from_dataframe
3365         """
3366         temp_name = "__temporary_name"
3367         df = pd.DataFrame({temp_name: series})
3368         ds = Dataset.from_dataframe(df, sparse=sparse)
3369         result = cast(DataArray, ds[temp_name])
3370         result.name = series.name
3371         return result
3372 
3373     def to_cdms2(self) -> cdms2_Variable:
3374         """Convert this array into a cdms2.Variable"""
3375         from ..convert import to_cdms2
3376 
3377         return to_cdms2(self)
3378 
3379     @classmethod
3380     def from_cdms2(cls, variable: cdms2_Variable) -> DataArray:
3381         """Convert a cdms2.Variable into an xarray.DataArray"""
3382         from ..convert import from_cdms2
3383 
3384         return from_cdms2(variable)
3385 
3386     def to_iris(self) -> iris_Cube:
3387         """Convert this array into a iris.cube.Cube"""
3388         from ..convert import to_iris
3389 
3390         return to_iris(self)
3391 
3392     @classmethod
3393     def from_iris(cls, cube: iris_Cube) -> DataArray:
3394         """Convert a iris.cube.Cube into an xarray.DataArray"""
3395         from ..convert import from_iris
3396 
3397         return from_iris(cube)
3398 
3399     def _all_compat(self: T_DataArray, other: T_DataArray, compat_str: str) -> bool:
3400         """Helper function for equals, broadcast_equals, and identical"""
3401 
3402         def compat(x, y):
3403             return getattr(x.variable, compat_str)(y.variable)
3404 
3405         return utils.dict_equiv(self.coords, other.coords, compat=compat) and compat(
3406             self, other
3407         )
3408 
3409     def broadcast_equals(self: T_DataArray, other: T_DataArray) -> bool:
3410         """Two DataArrays are broadcast equal if they are equal after
3411         broadcasting them against each other such that they have the same
3412         dimensions.
3413 
3414         Parameters
3415         ----------
3416         other : DataArray
3417             DataArray to compare to.
3418 
3419         Returns
3420         ----------
3421         equal : bool
3422             True if the two DataArrays are broadcast equal.
3423 
3424         See Also
3425         --------
3426         DataArray.equals
3427         DataArray.identical
3428         """
3429         try:
3430             return self._all_compat(other, "broadcast_equals")
3431         except (TypeError, AttributeError):
3432             return False
3433 
3434     def equals(self: T_DataArray, other: T_DataArray) -> bool:
3435         """True if two DataArrays have the same dimensions, coordinates and
3436         values; otherwise False.
3437 
3438         DataArrays can still be equal (like pandas objects) if they have NaN
3439         values in the same locations.
3440 
3441         This method is necessary because `v1 == v2` for ``DataArray``
3442         does element-wise comparisons (like numpy.ndarrays).
3443 
3444         Parameters
3445         ----------
3446         other : DataArray
3447             DataArray to compare to.
3448 
3449         Returns
3450         ----------
3451         equal : bool
3452             True if the two DataArrays are equal.
3453 
3454         See Also
3455         --------
3456         DataArray.broadcast_equals
3457         DataArray.identical
3458         """
3459         try:
3460             return self._all_compat(other, "equals")
3461         except (TypeError, AttributeError):
3462             return False
3463 
3464     def identical(self: T_DataArray, other: T_DataArray) -> bool:
3465         """Like equals, but also checks the array name and attributes, and
3466         attributes on all coordinates.
3467 
3468         Parameters
3469         ----------
3470         other : DataArray
3471             DataArray to compare to.
3472 
3473         Returns
3474         ----------
3475         equal : bool
3476             True if the two DataArrays are identical.
3477 
3478         See Also
3479         --------
3480         DataArray.broadcast_equals
3481         DataArray.equals
3482         """
3483         try:
3484             return self.name == other.name and self._all_compat(other, "identical")
3485         except (TypeError, AttributeError):
3486             return False
3487 
3488     def _result_name(self, other: Any = None) -> Hashable | None:
3489         # use the same naming heuristics as pandas:
3490         # https://github.com/ContinuumIO/blaze/issues/458#issuecomment-51936356
3491         other_name = getattr(other, "name", _default)
3492         if other_name is _default or other_name == self.name:
3493             return self.name
3494         else:
3495             return None
3496 
3497     def __array_wrap__(self: T_DataArray, obj, context=None) -> T_DataArray:
3498         new_var = self.variable.__array_wrap__(obj, context)
3499         return self._replace(new_var)
3500 
3501     def __matmul__(self: T_DataArray, obj: T_DataArray) -> T_DataArray:
3502         return self.dot(obj)
3503 
3504     def __rmatmul__(self: T_DataArray, other: T_DataArray) -> T_DataArray:
3505         # currently somewhat duplicative, as only other DataArrays are
3506         # compatible with matmul
3507         return computation.dot(other, self)
3508 
3509     def _unary_op(self: T_DataArray, f: Callable, *args, **kwargs) -> T_DataArray:
3510         keep_attrs = kwargs.pop("keep_attrs", None)
3511         if keep_attrs is None:
3512             keep_attrs = _get_keep_attrs(default=True)
3513         with warnings.catch_warnings():
3514             warnings.filterwarnings("ignore", r"All-NaN (slice|axis) encountered")
3515             warnings.filterwarnings(
3516                 "ignore", r"Mean of empty slice", category=RuntimeWarning
3517             )
3518             with np.errstate(all="ignore"):
3519                 da = self.__array_wrap__(f(self.variable.data, *args, **kwargs))
3520             if keep_attrs:
3521                 da.attrs = self.attrs
3522             return da
3523 
3524     def _binary_op(
3525         self: T_DataArray,
3526         other: Any,
3527         f: Callable,
3528         reflexive: bool = False,
3529     ) -> T_DataArray:
3530         from .groupby import GroupBy
3531 
3532         if isinstance(other, (Dataset, GroupBy)):
3533             return NotImplemented
3534         if isinstance(other, DataArray):
3535             align_type = OPTIONS["arithmetic_join"]
3536             self, other = align(self, other, join=align_type, copy=False)  # type: ignore
3537         other_variable = getattr(other, "variable", other)
3538         other_coords = getattr(other, "coords", None)
3539 
3540         variable = (
3541             f(self.variable, other_variable)
3542             if not reflexive
3543             else f(other_variable, self.variable)
3544         )
3545         coords, indexes = self.coords._merge_raw(other_coords, reflexive)
3546         name = self._result_name(other)
3547 
3548         return self._replace(variable, coords, name, indexes=indexes)
3549 
3550     def _inplace_binary_op(self: T_DataArray, other: Any, f: Callable) -> T_DataArray:
3551         from .groupby import GroupBy
3552 
3553         if isinstance(other, GroupBy):
3554             raise TypeError(
3555                 "in-place operations between a DataArray and "
3556                 "a grouped object are not permitted"
3557             )
3558         # n.b. we can't align other to self (with other.reindex_like(self))
3559         # because `other` may be converted into floats, which would cause
3560         # in-place arithmetic to fail unpredictably. Instead, we simply
3561         # don't support automatic alignment with in-place arithmetic.
3562         other_coords = getattr(other, "coords", None)
3563         other_variable = getattr(other, "variable", other)
3564         try:
3565             with self.coords._merge_inplace(other_coords):
3566                 f(self.variable, other_variable)
3567         except MergeError as exc:
3568             raise MergeError(
3569                 "Automatic alignment is not supported for in-place operations.\n"
3570                 "Consider aligning the indices manually or using a not-in-place operation.\n"
3571                 "See https://github.com/pydata/xarray/issues/3910 for more explanations."
3572             ) from exc
3573         return self
3574 
3575     def _copy_attrs_from(self, other: DataArray | Dataset | Variable) -> None:
3576         self.attrs = other.attrs
3577 
3578     plot = utils.UncachedAccessor(_PlotMethods)
3579 
3580     def _title_for_slice(self, truncate: int = 50) -> str:
3581         """
3582         If the dataarray has 1 dimensional coordinates or comes from a slice
3583         we can show that info in the title
3584 
3585         Parameters
3586         ----------
3587         truncate : int, default: 50
3588             maximum number of characters for title
3589 
3590         Returns
3591         -------
3592         title : string
3593             Can be used for plot titles
3594 
3595         """
3596         one_dims = []
3597         for dim, coord in self.coords.items():
3598             if coord.size == 1:
3599                 one_dims.append(
3600                     "{dim} = {v}{unit}".format(
3601                         dim=dim,
3602                         v=format_item(coord.values),
3603                         unit=_get_units_from_attrs(coord),
3604                     )
3605                 )
3606 
3607         title = ", ".join(one_dims)
3608         if len(title) > truncate:
3609             title = title[: (truncate - 3)] + "..."
3610 
3611         return title
3612 
3613     def diff(
3614         self: T_DataArray,
3615         dim: Hashable,
3616         n: int = 1,
3617         label: Literal["upper", "lower"] = "upper",
3618     ) -> T_DataArray:
3619         """Calculate the n-th order discrete difference along given axis.
3620 
3621         Parameters
3622         ----------
3623         dim : Hashable
3624             Dimension over which to calculate the finite difference.
3625         n : int, default: 1
3626             The number of times values are differenced.
3627         label : {"upper", "lower"}, default: "upper"
3628             The new coordinate in dimension ``dim`` will have the
3629             values of either the minuend's or subtrahend's coordinate
3630             for values 'upper' and 'lower', respectively.
3631 
3632         Returns
3633         -------
3634         difference : DataArray
3635             The n-th order finite difference of this object.
3636 
3637         Notes
3638         -----
3639         `n` matches numpy's behavior and is different from pandas' first argument named
3640         `periods`.
3641 
3642         Examples
3643         --------
3644         >>> arr = xr.DataArray([5, 5, 6, 6], [[1, 2, 3, 4]], ["x"])
3645         >>> arr.diff("x")
3646         <xarray.DataArray (x: 3)>
3647         array([0, 1, 0])
3648         Coordinates:
3649           * x        (x) int64 2 3 4
3650         >>> arr.diff("x", 2)
3651         <xarray.DataArray (x: 2)>
3652         array([ 1, -1])
3653         Coordinates:
3654           * x        (x) int64 3 4
3655 
3656         See Also
3657         --------
3658         DataArray.differentiate
3659         """
3660         ds = self._to_temp_dataset().diff(n=n, dim=dim, label=label)
3661         return self._from_temp_dataset(ds)
3662 
3663     def shift(
3664         self: T_DataArray,
3665         shifts: Mapping[Any, int] | None = None,
3666         fill_value: Any = dtypes.NA,
3667         **shifts_kwargs: int,
3668     ) -> T_DataArray:
3669         """Shift this DataArray by an offset along one or more dimensions.
3670 
3671         Only the data is moved; coordinates stay in place. This is consistent
3672         with the behavior of ``shift`` in pandas.
3673 
3674         Values shifted from beyond array bounds will appear at one end of
3675         each dimension, which are filled according to `fill_value`. For periodic
3676         offsets instead see `roll`.
3677 
3678         Parameters
3679         ----------
3680         shifts : mapping of Hashable to int or None, optional
3681             Integer offset to shift along each of the given dimensions.
3682             Positive offsets shift to the right; negative offsets shift to the
3683             left.
3684         fill_value : scalar, optional
3685             Value to use for newly missing values
3686         **shifts_kwargs
3687             The keyword arguments form of ``shifts``.
3688             One of shifts or shifts_kwargs must be provided.
3689 
3690         Returns
3691         -------
3692         shifted : DataArray
3693             DataArray with the same coordinates and attributes but shifted
3694             data.
3695 
3696         See Also
3697         --------
3698         roll
3699 
3700         Examples
3701         --------
3702         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3703         >>> arr.shift(x=1)
3704         <xarray.DataArray (x: 3)>
3705         array([nan,  5.,  6.])
3706         Dimensions without coordinates: x
3707         """
3708         variable = self.variable.shift(
3709             shifts=shifts, fill_value=fill_value, **shifts_kwargs
3710         )
3711         return self._replace(variable=variable)
3712 
3713     def roll(
3714         self: T_DataArray,
3715         shifts: Mapping[Hashable, int] | None = None,
3716         roll_coords: bool = False,
3717         **shifts_kwargs: int,
3718     ) -> T_DataArray:
3719         """Roll this array by an offset along one or more dimensions.
3720 
3721         Unlike shift, roll treats the given dimensions as periodic, so will not
3722         create any missing values to be filled.
3723 
3724         Unlike shift, roll may rotate all variables, including coordinates
3725         if specified. The direction of rotation is consistent with
3726         :py:func:`numpy.roll`.
3727 
3728         Parameters
3729         ----------
3730         shifts : mapping of Hashable to int, optional
3731             Integer offset to rotate each of the given dimensions.
3732             Positive offsets roll to the right; negative offsets roll to the
3733             left.
3734         roll_coords : bool, default: False
3735             Indicates whether to roll the coordinates by the offset too.
3736         **shifts_kwargs : {dim: offset, ...}, optional
3737             The keyword arguments form of ``shifts``.
3738             One of shifts or shifts_kwargs must be provided.
3739 
3740         Returns
3741         -------
3742         rolled : DataArray
3743             DataArray with the same attributes but rolled data and coordinates.
3744 
3745         See Also
3746         --------
3747         shift
3748 
3749         Examples
3750         --------
3751         >>> arr = xr.DataArray([5, 6, 7], dims="x")
3752         >>> arr.roll(x=1)
3753         <xarray.DataArray (x: 3)>
3754         array([7, 5, 6])
3755         Dimensions without coordinates: x
3756         """
3757         ds = self._to_temp_dataset().roll(
3758             shifts=shifts, roll_coords=roll_coords, **shifts_kwargs
3759         )
3760         return self._from_temp_dataset(ds)
3761 
3762     @property
3763     def real(self: T_DataArray) -> T_DataArray:
3764         return self._replace(self.variable.real)
3765 
3766     @property
3767     def imag(self: T_DataArray) -> T_DataArray:
3768         return self._replace(self.variable.imag)
3769 
3770     def dot(
3771         self: T_DataArray,
3772         other: T_DataArray,
3773         dims: str | Iterable[Hashable] | Ellipsis | None = None,
3774     ) -> T_DataArray:
3775         """Perform dot product of two DataArrays along their shared dims.
3776 
3777         Equivalent to taking taking tensordot over all shared dims.
3778 
3779         Parameters
3780         ----------
3781         other : DataArray
3782             The other array with which the dot product is performed.
3783         dims : ..., str or Iterable of Hashable, optional
3784             Which dimensions to sum over. Ellipsis (`...`) sums over all dimensions.
3785             If not specified, then all the common dimensions are summed over.
3786 
3787         Returns
3788         -------
3789         result : DataArray
3790             Array resulting from the dot product over all shared dimensions.
3791 
3792         See Also
3793         --------
3794         dot
3795         numpy.tensordot
3796 
3797         Examples
3798         --------
3799         >>> da_vals = np.arange(6 * 5 * 4).reshape((6, 5, 4))
3800         >>> da = xr.DataArray(da_vals, dims=["x", "y", "z"])
3801         >>> dm_vals = np.arange(4)
3802         >>> dm = xr.DataArray(dm_vals, dims=["z"])
3803 
3804         >>> dm.dims
3805         ('z',)
3806 
3807         >>> da.dims
3808         ('x', 'y', 'z')
3809 
3810         >>> dot_result = da.dot(dm)
3811         >>> dot_result.dims
3812         ('x', 'y')
3813 
3814         """
3815         if isinstance(other, Dataset):
3816             raise NotImplementedError(
3817                 "dot products are not yet supported with Dataset objects."
3818             )
3819         if not isinstance(other, DataArray):
3820             raise TypeError("dot only operates on DataArrays.")
3821 
3822         return computation.dot(self, other, dims=dims)
3823 
3824     # change type of self and return to T_DataArray once
3825     # https://github.com/python/mypy/issues/12846 is resolved
3826     def sortby(
3827         self,
3828         variables: Hashable | DataArray | Sequence[Hashable | DataArray],
3829         ascending: bool = True,
3830     ) -> DataArray:
3831         """Sort object by labels or values (along an axis).
3832 
3833         Sorts the dataarray, either along specified dimensions,
3834         or according to values of 1-D dataarrays that share dimension
3835         with calling object.
3836 
3837         If the input variables are dataarrays, then the dataarrays are aligned
3838         (via left-join) to the calling object prior to sorting by cell values.
3839         NaNs are sorted to the end, following Numpy convention.
3840 
3841         If multiple sorts along the same dimension is
3842         given, numpy's lexsort is performed along that dimension:
3843         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
3844         and the FIRST key in the sequence is used as the primary sort key,
3845         followed by the 2nd key, etc.
3846 
3847         Parameters
3848         ----------
3849         variables : Hashable, DataArray, or sequence of Hashable or DataArray
3850             1D DataArray objects or name(s) of 1D variable(s) in
3851             coords whose values are used to sort this array.
3852         ascending : bool, default: True
3853             Whether to sort by ascending or descending order.
3854 
3855         Returns
3856         -------
3857         sorted : DataArray
3858             A new dataarray where all the specified dims are sorted by dim
3859             labels.
3860 
3861         See Also
3862         --------
3863         Dataset.sortby
3864         numpy.sort
3865         pandas.sort_values
3866         pandas.sort_index
3867 
3868         Examples
3869         --------
3870         >>> da = xr.DataArray(
3871         ...     np.random.rand(5),
3872         ...     coords=[pd.date_range("1/1/2000", periods=5)],
3873         ...     dims="time",
3874         ... )
3875         >>> da
3876         <xarray.DataArray (time: 5)>
3877         array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ])
3878         Coordinates:
3879           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2000-01-05
3880 
3881         >>> da.sortby(da)
3882         <xarray.DataArray (time: 5)>
3883         array([0.4236548 , 0.54488318, 0.5488135 , 0.60276338, 0.71518937])
3884         Coordinates:
3885           * time     (time) datetime64[ns] 2000-01-05 2000-01-04 ... 2000-01-02
3886         """
3887         ds = self._to_temp_dataset().sortby(variables, ascending=ascending)
3888         return self._from_temp_dataset(ds)
3889 
3890     def quantile(
3891         self: T_DataArray,
3892         q: ArrayLike,
3893         dim: str | Iterable[Hashable] | None = None,
3894         method: QUANTILE_METHODS = "linear",
3895         keep_attrs: bool | None = None,
3896         skipna: bool | None = None,
3897         interpolation: QUANTILE_METHODS = None,
3898     ) -> T_DataArray:
3899         """Compute the qth quantile of the data along the specified dimension.
3900 
3901         Returns the qth quantiles(s) of the array elements.
3902 
3903         Parameters
3904         ----------
3905         q : float or array-like of float
3906             Quantile to compute, which must be between 0 and 1 inclusive.
3907         dim : str or Iterable of Hashable, optional
3908             Dimension(s) over which to apply quantile.
3909         method : str, default: "linear"
3910             This optional parameter specifies the interpolation method to use when the
3911             desired quantile lies between two data points. The options sorted by their R
3912             type as summarized in the H&F paper [1]_ are:
3913 
3914                 1. "inverted_cdf" (*)
3915                 2. "averaged_inverted_cdf" (*)
3916                 3. "closest_observation" (*)
3917                 4. "interpolated_inverted_cdf" (*)
3918                 5. "hazen" (*)
3919                 6. "weibull" (*)
3920                 7. "linear"  (default)
3921                 8. "median_unbiased" (*)
3922                 9. "normal_unbiased" (*)
3923 
3924             The first three methods are discontiuous. The following discontinuous
3925             variations of the default "linear" (7.) option are also available:
3926 
3927                 * "lower"
3928                 * "higher"
3929                 * "midpoint"
3930                 * "nearest"
3931 
3932             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
3933             was previously called "interpolation", renamed in accordance with numpy
3934             version 1.22.0.
3935 
3936             (*) These methods require numpy version 1.22 or newer.
3937 
3938         keep_attrs : bool or None, optional
3939             If True, the dataset's attributes (`attrs`) will be copied from
3940             the original object to the new one.  If False (default), the new
3941             object will be returned without attributes.
3942         skipna : bool or None, optional
3943             If True, skip missing values (as marked by NaN). By default, only
3944             skips missing values for float dtypes; other dtypes either do not
3945             have a sentinel missing value (int) or skipna=True has not been
3946             implemented (object, datetime64 or timedelta64).
3947 
3948         Returns
3949         -------
3950         quantiles : DataArray
3951             If `q` is a single quantile, then the result
3952             is a scalar. If multiple percentiles are given, first axis of
3953             the result corresponds to the quantile and a quantile dimension
3954             is added to the return array. The other dimensions are the
3955             dimensions that remain after the reduction of the array.
3956 
3957         See Also
3958         --------
3959         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, Dataset.quantile
3960 
3961         Examples
3962         --------
3963         >>> da = xr.DataArray(
3964         ...     data=[[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]],
3965         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
3966         ...     dims=("x", "y"),
3967         ... )
3968         >>> da.quantile(0)  # or da.quantile(0, dim=...)
3969         <xarray.DataArray ()>
3970         array(0.7)
3971         Coordinates:
3972             quantile  float64 0.0
3973         >>> da.quantile(0, dim="x")
3974         <xarray.DataArray (y: 4)>
3975         array([0.7, 4.2, 2.6, 1.5])
3976         Coordinates:
3977           * y         (y) float64 1.0 1.5 2.0 2.5
3978             quantile  float64 0.0
3979         >>> da.quantile([0, 0.5, 1])
3980         <xarray.DataArray (quantile: 3)>
3981         array([0.7, 3.4, 9.4])
3982         Coordinates:
3983           * quantile  (quantile) float64 0.0 0.5 1.0
3984         >>> da.quantile([0, 0.5, 1], dim="x")
3985         <xarray.DataArray (quantile: 3, y: 4)>
3986         array([[0.7 , 4.2 , 2.6 , 1.5 ],
3987                [3.6 , 5.75, 6.  , 1.7 ],
3988                [6.5 , 7.3 , 9.4 , 1.9 ]])
3989         Coordinates:
3990           * y         (y) float64 1.0 1.5 2.0 2.5
3991           * quantile  (quantile) float64 0.0 0.5 1.0
3992 
3993         References
3994         ----------
3995         .. [1] R. J. Hyndman and Y. Fan,
3996            "Sample quantiles in statistical packages,"
3997            The American Statistician, 50(4), pp. 361-365, 1996
3998         """
3999 
4000         ds = self._to_temp_dataset().quantile(
4001             q,
4002             dim=dim,
4003             keep_attrs=keep_attrs,
4004             method=method,
4005             skipna=skipna,
4006             interpolation=interpolation,
4007         )
4008         return self._from_temp_dataset(ds)
4009 
4010     def rank(
4011         self: T_DataArray,
4012         dim: Hashable,
4013         pct: bool = False,
4014         keep_attrs: bool | None = None,
4015     ) -> T_DataArray:
4016         """Ranks the data.
4017 
4018         Equal values are assigned a rank that is the average of the ranks that
4019         would have been otherwise assigned to all of the values within that
4020         set.  Ranks begin at 1, not 0. If pct, computes percentage ranks.
4021 
4022         NaNs in the input array are returned as NaNs.
4023 
4024         The `bottleneck` library is required.
4025 
4026         Parameters
4027         ----------
4028         dim : Hashable
4029             Dimension over which to compute rank.
4030         pct : bool, default: False
4031             If True, compute percentage ranks, otherwise compute integer ranks.
4032         keep_attrs : bool or None, optional
4033             If True, the dataset's attributes (`attrs`) will be copied from
4034             the original object to the new one.  If False (default), the new
4035             object will be returned without attributes.
4036 
4037         Returns
4038         -------
4039         ranked : DataArray
4040             DataArray with the same coordinates and dtype 'float64'.
4041 
4042         Examples
4043         --------
4044         >>> arr = xr.DataArray([5, 6, 7], dims="x")
4045         >>> arr.rank("x")
4046         <xarray.DataArray (x: 3)>
4047         array([1., 2., 3.])
4048         Dimensions without coordinates: x
4049         """
4050 
4051         ds = self._to_temp_dataset().rank(dim, pct=pct, keep_attrs=keep_attrs)
4052         return self._from_temp_dataset(ds)
4053 
4054     def differentiate(
4055         self: T_DataArray,
4056         coord: Hashable,
4057         edge_order: Literal[1, 2] = 1,
4058         datetime_unit: DatetimeUnitOptions = None,
4059     ) -> T_DataArray:
4060         """ Differentiate the array with the second order accurate central
4061         differences.
4062 
4063         .. note::
4064             This feature is limited to simple cartesian geometry, i.e. coord
4065             must be one dimensional.
4066 
4067         Parameters
4068         ----------
4069         coord : Hashable
4070             The coordinate to be used to compute the gradient.
4071         edge_order : {1, 2}, default: 1
4072             N-th order accurate differences at the boundaries.
4073         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", \
4074                          "us", "ns", "ps", "fs", "as", None}, optional
4075             Unit to compute gradient. Only valid for datetime coordinate.
4076 
4077         Returns
4078         -------
4079         differentiated: DataArray
4080 
4081         See also
4082         --------
4083         numpy.gradient: corresponding numpy function
4084 
4085         Examples
4086         --------
4087 
4088         >>> da = xr.DataArray(
4089         ...     np.arange(12).reshape(4, 3),
4090         ...     dims=["x", "y"],
4091         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4092         ... )
4093         >>> da
4094         <xarray.DataArray (x: 4, y: 3)>
4095         array([[ 0,  1,  2],
4096                [ 3,  4,  5],
4097                [ 6,  7,  8],
4098                [ 9, 10, 11]])
4099         Coordinates:
4100           * x        (x) float64 0.0 0.1 1.1 1.2
4101         Dimensions without coordinates: y
4102         >>>
4103         >>> da.differentiate("x")
4104         <xarray.DataArray (x: 4, y: 3)>
4105         array([[30.        , 30.        , 30.        ],
4106                [27.54545455, 27.54545455, 27.54545455],
4107                [27.54545455, 27.54545455, 27.54545455],
4108                [30.        , 30.        , 30.        ]])
4109         Coordinates:
4110           * x        (x) float64 0.0 0.1 1.1 1.2
4111         Dimensions without coordinates: y
4112         """
4113         ds = self._to_temp_dataset().differentiate(coord, edge_order, datetime_unit)
4114         return self._from_temp_dataset(ds)
4115 
4116     # change type of self and return to T_DataArray once
4117     # https://github.com/python/mypy/issues/12846 is resolved
4118     def integrate(
4119         self,
4120         coord: Hashable | Sequence[Hashable] = None,
4121         datetime_unit: DatetimeUnitOptions = None,
4122     ) -> DataArray:
4123         """Integrate along the given coordinate using the trapezoidal rule.
4124 
4125         .. note::
4126             This feature is limited to simple cartesian geometry, i.e. coord
4127             must be one dimensional.
4128 
4129         Parameters
4130         ----------
4131         coord : Hashable, or sequence of Hashable
4132             Coordinate(s) used for the integration.
4133         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4134                         'ps', 'fs', 'as', None}, optional
4135             Specify the unit if a datetime coordinate is used.
4136 
4137         Returns
4138         -------
4139         integrated : DataArray
4140 
4141         See also
4142         --------
4143         Dataset.integrate
4144         numpy.trapz : corresponding numpy function
4145 
4146         Examples
4147         --------
4148 
4149         >>> da = xr.DataArray(
4150         ...     np.arange(12).reshape(4, 3),
4151         ...     dims=["x", "y"],
4152         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4153         ... )
4154         >>> da
4155         <xarray.DataArray (x: 4, y: 3)>
4156         array([[ 0,  1,  2],
4157                [ 3,  4,  5],
4158                [ 6,  7,  8],
4159                [ 9, 10, 11]])
4160         Coordinates:
4161           * x        (x) float64 0.0 0.1 1.1 1.2
4162         Dimensions without coordinates: y
4163         >>>
4164         >>> da.integrate("x")
4165         <xarray.DataArray (y: 3)>
4166         array([5.4, 6.6, 7.8])
4167         Dimensions without coordinates: y
4168         """
4169         ds = self._to_temp_dataset().integrate(coord, datetime_unit)
4170         return self._from_temp_dataset(ds)
4171 
4172     # change type of self and return to T_DataArray once
4173     # https://github.com/python/mypy/issues/12846 is resolved
4174     def cumulative_integrate(
4175         self,
4176         coord: Hashable | Sequence[Hashable] = None,
4177         datetime_unit: DatetimeUnitOptions = None,
4178     ) -> DataArray:
4179         """Integrate cumulatively along the given coordinate using the trapezoidal rule.
4180 
4181         .. note::
4182             This feature is limited to simple cartesian geometry, i.e. coord
4183             must be one dimensional.
4184 
4185             The first entry of the cumulative integral is always 0, in order to keep the
4186             length of the dimension unchanged between input and output.
4187 
4188         Parameters
4189         ----------
4190         coord : Hashable, or sequence of Hashable
4191             Coordinate(s) used for the integration.
4192         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
4193                         'ps', 'fs', 'as', None}, optional
4194             Specify the unit if a datetime coordinate is used.
4195 
4196         Returns
4197         -------
4198         integrated : DataArray
4199 
4200         See also
4201         --------
4202         Dataset.cumulative_integrate
4203         scipy.integrate.cumulative_trapezoid : corresponding scipy function
4204 
4205         Examples
4206         --------
4207 
4208         >>> da = xr.DataArray(
4209         ...     np.arange(12).reshape(4, 3),
4210         ...     dims=["x", "y"],
4211         ...     coords={"x": [0, 0.1, 1.1, 1.2]},
4212         ... )
4213         >>> da
4214         <xarray.DataArray (x: 4, y: 3)>
4215         array([[ 0,  1,  2],
4216                [ 3,  4,  5],
4217                [ 6,  7,  8],
4218                [ 9, 10, 11]])
4219         Coordinates:
4220           * x        (x) float64 0.0 0.1 1.1 1.2
4221         Dimensions without coordinates: y
4222         >>>
4223         >>> da.cumulative_integrate("x")
4224         <xarray.DataArray (x: 4, y: 3)>
4225         array([[0.  , 0.  , 0.  ],
4226                [0.15, 0.25, 0.35],
4227                [4.65, 5.75, 6.85],
4228                [5.4 , 6.6 , 7.8 ]])
4229         Coordinates:
4230           * x        (x) float64 0.0 0.1 1.1 1.2
4231         Dimensions without coordinates: y
4232         """
4233         ds = self._to_temp_dataset().cumulative_integrate(coord, datetime_unit)
4234         return self._from_temp_dataset(ds)
4235 
4236     def unify_chunks(self) -> DataArray:
4237         """Unify chunk size along all chunked dimensions of this DataArray.
4238 
4239         Returns
4240         -------
4241         DataArray with consistent chunk sizes for all dask-array variables
4242 
4243         See Also
4244         --------
4245         dask.array.core.unify_chunks
4246         """
4247 
4248         return unify_chunks(self)[0]
4249 
4250     def map_blocks(
4251         self,
4252         func: Callable[..., T_Xarray],
4253         args: Sequence[Any] = (),
4254         kwargs: Mapping[str, Any] | None = None,
4255         template: DataArray | Dataset | None = None,
4256     ) -> T_Xarray:
4257         """
4258         Apply a function to each block of this DataArray.
4259 
4260         .. warning::
4261             This method is experimental and its signature may change.
4262 
4263         Parameters
4264         ----------
4265         func : callable
4266             User-provided function that accepts a DataArray as its first
4267             parameter. The function will receive a subset or 'block' of this DataArray (see below),
4268             corresponding to one chunk along each chunked dimension. ``func`` will be
4269             executed as ``func(subset_dataarray, *subset_args, **kwargs)``.
4270 
4271             This function must return either a single DataArray or a single Dataset.
4272 
4273             This function cannot add a new chunked dimension.
4274         args : sequence
4275             Passed to func after unpacking and subsetting any xarray objects by blocks.
4276             xarray objects in args must be aligned with this object, otherwise an error is raised.
4277         kwargs : mapping
4278             Passed verbatim to func after unpacking. xarray objects, if any, will not be
4279             subset to blocks. Passing dask collections in kwargs is not allowed.
4280         template : DataArray or Dataset, optional
4281             xarray object representing the final result after compute is called. If not provided,
4282             the function will be first run on mocked-up data, that looks like this object but
4283             has sizes 0, to determine properties of the returned object such as dtype,
4284             variable names, attributes, new dimensions and new indexes (if any).
4285             ``template`` must be provided if the function changes the size of existing dimensions.
4286             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
4287             ``attrs`` set by ``func`` will be ignored.
4288 
4289         Returns
4290         -------
4291         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
4292         function.
4293 
4294         Notes
4295         -----
4296         This function is designed for when ``func`` needs to manipulate a whole xarray object
4297         subset to each block. Each block is loaded into memory. In the more common case where
4298         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
4299 
4300         If none of the variables in this object is backed by dask arrays, calling this function is
4301         equivalent to calling ``func(obj, *args, **kwargs)``.
4302 
4303         See Also
4304         --------
4305         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
4306         xarray.DataArray.map_blocks
4307 
4308         Examples
4309         --------
4310         Calculate an anomaly from climatology using ``.groupby()``. Using
4311         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
4312         its indices, and its methods like ``.groupby()``.
4313 
4314         >>> def calculate_anomaly(da, groupby_type="time.month"):
4315         ...     gb = da.groupby(groupby_type)
4316         ...     clim = gb.mean(dim="time")
4317         ...     return gb - clim
4318         ...
4319         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
4320         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
4321         >>> np.random.seed(123)
4322         >>> array = xr.DataArray(
4323         ...     np.random.rand(len(time)),
4324         ...     dims=["time"],
4325         ...     coords={"time": time, "month": month},
4326         ... ).chunk()
4327         >>> array.map_blocks(calculate_anomaly, template=array).compute()
4328         <xarray.DataArray (time: 24)>
4329         array([ 0.12894847,  0.11323072, -0.0855964 , -0.09334032,  0.26848862,
4330                 0.12382735,  0.22460641,  0.07650108, -0.07673453, -0.22865714,
4331                -0.19063865,  0.0590131 , -0.12894847, -0.11323072,  0.0855964 ,
4332                 0.09334032, -0.26848862, -0.12382735, -0.22460641, -0.07650108,
4333                 0.07673453,  0.22865714,  0.19063865, -0.0590131 ])
4334         Coordinates:
4335           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4336             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
4337 
4338         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
4339         to the function being applied in ``xr.map_blocks()``:
4340 
4341         >>> array.map_blocks(
4342         ...     calculate_anomaly, kwargs={"groupby_type": "time.year"}, template=array
4343         ... )  # doctest: +ELLIPSIS
4344         <xarray.DataArray (time: 24)>
4345         dask.array<<this-array>-calculate_anomaly, shape=(24,), dtype=float64, chunksize=(24,), chunktype=numpy.ndarray>
4346         Coordinates:
4347           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
4348             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
4349         """
4350         from .parallel import map_blocks
4351 
4352         return map_blocks(func, self, args, kwargs, template)
4353 
4354     def polyfit(
4355         self,
4356         dim: Hashable,
4357         deg: int,
4358         skipna: bool | None = None,
4359         rcond: float | None = None,
4360         w: Hashable | Any | None = None,
4361         full: bool = False,
4362         cov: bool | Literal["unscaled"] = False,
4363     ) -> Dataset:
4364         """
4365         Least squares polynomial fit.
4366 
4367         This replicates the behaviour of `numpy.polyfit` but differs by skipping
4368         invalid values when `skipna = True`.
4369 
4370         Parameters
4371         ----------
4372         dim : Hashable
4373             Coordinate along which to fit the polynomials.
4374         deg : int
4375             Degree of the fitting polynomial.
4376         skipna : bool or None, optional
4377             If True, removes all invalid values before fitting each 1D slices of the array.
4378             Default is True if data is stored in a dask.array or if there is any
4379             invalid values, False otherwise.
4380         rcond : float or None, optional
4381             Relative condition number to the fit.
4382         w : Hashable, array-like or None, optional
4383             Weights to apply to the y-coordinate of the sample points.
4384             Can be an array-like object or the name of a coordinate in the dataset.
4385         full : bool, default: False
4386             Whether to return the residuals, matrix rank and singular values in addition
4387             to the coefficients.
4388         cov : bool or "unscaled", default: False
4389             Whether to return to the covariance matrix in addition to the coefficients.
4390             The matrix is not scaled if `cov='unscaled'`.
4391 
4392         Returns
4393         -------
4394         polyfit_results : Dataset
4395             A single dataset which contains:
4396 
4397             polyfit_coefficients
4398                 The coefficients of the best fit.
4399             polyfit_residuals
4400                 The residuals of the least-square computation (only included if `full=True`).
4401                 When the matrix rank is deficient, np.nan is returned.
4402             [dim]_matrix_rank
4403                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4404             [dim]_singular_value
4405                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
4406             polyfit_covariance
4407                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
4408 
4409         See Also
4410         --------
4411         numpy.polyfit
4412         numpy.polyval
4413         xarray.polyval
4414         """
4415         return self._to_temp_dataset().polyfit(
4416             dim, deg, skipna=skipna, rcond=rcond, w=w, full=full, cov=cov
4417         )
4418 
4419     def pad(
4420         self: T_DataArray,
4421         pad_width: Mapping[Any, int | tuple[int, int]] | None = None,
4422         mode: PadModeOptions = "constant",
4423         stat_length: int
4424         | tuple[int, int]
4425         | Mapping[Any, tuple[int, int]]
4426         | None = None,
4427         constant_values: float
4428         | tuple[float, float]
4429         | Mapping[Any, tuple[float, float]]
4430         | None = None,
4431         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
4432         reflect_type: PadReflectOptions = None,
4433         **pad_width_kwargs: Any,
4434     ) -> T_DataArray:
4435         """Pad this array along one or more dimensions.
4436 
4437         .. warning::
4438             This function is experimental and its behaviour is likely to change
4439             especially regarding padding of dimension coordinates (or IndexVariables).
4440 
4441         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
4442         coordinates will be padded with the same mode, otherwise coordinates
4443         are padded using the "constant" mode with fill_value dtypes.NA.
4444 
4445         Parameters
4446         ----------
4447         pad_width : mapping of Hashable to tuple of int
4448             Mapping with the form of {dim: (pad_before, pad_after)}
4449             describing the number of values padded along each dimension.
4450             {dim: pad} is a shortcut for pad_before = pad_after = pad
4451         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
4452             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
4453             How to pad the DataArray (taken from numpy docs):
4454 
4455             - "constant": Pads with a constant value.
4456             - "edge": Pads with the edge values of array.
4457             - "linear_ramp": Pads with the linear ramp between end_value and the
4458               array edge value.
4459             - "maximum": Pads with the maximum value of all or part of the
4460               vector along each axis.
4461             - "mean": Pads with the mean value of all or part of the
4462               vector along each axis.
4463             - "median": Pads with the median value of all or part of the
4464               vector along each axis.
4465             - "minimum": Pads with the minimum value of all or part of the
4466               vector along each axis.
4467             - "reflect": Pads with the reflection of the vector mirrored on
4468               the first and last values of the vector along each axis.
4469             - "symmetric": Pads with the reflection of the vector mirrored
4470               along the edge of the array.
4471             - "wrap": Pads with the wrap of the vector along the axis.
4472               The first values are used to pad the end and the
4473               end values are used to pad the beginning.
4474 
4475         stat_length : int, tuple or mapping of Hashable to tuple, default: None
4476             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
4477             values at edge of each axis used to calculate the statistic value.
4478             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
4479             statistic lengths along each dimension.
4480             ((before, after),) yields same before and after statistic lengths
4481             for each dimension.
4482             (stat_length,) or int is a shortcut for before = after = statistic
4483             length for all axes.
4484             Default is ``None``, to use the entire axis.
4485         constant_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4486             Used in 'constant'.  The values to set the padded values for each
4487             axis.
4488             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4489             pad constants along each dimension.
4490             ``((before, after),)`` yields same before and after constants for each
4491             dimension.
4492             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4493             all dimensions.
4494             Default is 0.
4495         end_values : scalar, tuple or mapping of Hashable to tuple, default: 0
4496             Used in 'linear_ramp'.  The values used for the ending value of the
4497             linear_ramp and that will form the edge of the padded array.
4498             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
4499             end values along each dimension.
4500             ``((before, after),)`` yields same before and after end values for each
4501             axis.
4502             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
4503             all axes.
4504             Default is 0.
4505         reflect_type : {"even", "odd", None}, optional
4506             Used in "reflect", and "symmetric". The "even" style is the
4507             default with an unaltered reflection around the edge value. For
4508             the "odd" style, the extended part of the array is created by
4509             subtracting the reflected values from two times the edge value.
4510         **pad_width_kwargs
4511             The keyword arguments form of ``pad_width``.
4512             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
4513 
4514         Returns
4515         -------
4516         padded : DataArray
4517             DataArray with the padded coordinates and data.
4518 
4519         See Also
4520         --------
4521         DataArray.shift, DataArray.roll, DataArray.bfill, DataArray.ffill, numpy.pad, dask.array.pad
4522 
4523         Notes
4524         -----
4525         For ``mode="constant"`` and ``constant_values=None``, integer types will be
4526         promoted to ``float`` and padded with ``np.nan``.
4527 
4528         Padding coordinates will drop their corresponding index (if any) and will reset default
4529         indexes for dimension coordinates.
4530 
4531         Examples
4532         --------
4533         >>> arr = xr.DataArray([5, 6, 7], coords=[("x", [0, 1, 2])])
4534         >>> arr.pad(x=(1, 2), constant_values=0)
4535         <xarray.DataArray (x: 6)>
4536         array([0, 5, 6, 7, 0, 0])
4537         Coordinates:
4538           * x        (x) float64 nan 0.0 1.0 2.0 nan nan
4539 
4540         >>> da = xr.DataArray(
4541         ...     [[0, 1, 2, 3], [10, 11, 12, 13]],
4542         ...     dims=["x", "y"],
4543         ...     coords={"x": [0, 1], "y": [10, 20, 30, 40], "z": ("x", [100, 200])},
4544         ... )
4545         >>> da.pad(x=1)
4546         <xarray.DataArray (x: 4, y: 4)>
4547         array([[nan, nan, nan, nan],
4548                [ 0.,  1.,  2.,  3.],
4549                [10., 11., 12., 13.],
4550                [nan, nan, nan, nan]])
4551         Coordinates:
4552           * x        (x) float64 nan 0.0 1.0 nan
4553           * y        (y) int64 10 20 30 40
4554             z        (x) float64 nan 100.0 200.0 nan
4555 
4556         Careful, ``constant_values`` are coerced to the data type of the array which may
4557         lead to a loss of precision:
4558 
4559         >>> da.pad(x=1, constant_values=1.23456789)
4560         <xarray.DataArray (x: 4, y: 4)>
4561         array([[ 1,  1,  1,  1],
4562                [ 0,  1,  2,  3],
4563                [10, 11, 12, 13],
4564                [ 1,  1,  1,  1]])
4565         Coordinates:
4566           * x        (x) float64 nan 0.0 1.0 nan
4567           * y        (y) int64 10 20 30 40
4568             z        (x) float64 nan 100.0 200.0 nan
4569         """
4570         ds = self._to_temp_dataset().pad(
4571             pad_width=pad_width,
4572             mode=mode,
4573             stat_length=stat_length,
4574             constant_values=constant_values,
4575             end_values=end_values,
4576             reflect_type=reflect_type,
4577             **pad_width_kwargs,
4578         )
4579         return self._from_temp_dataset(ds)
4580 
4581     def idxmin(
4582         self,
4583         dim: Hashable | None = None,
4584         skipna: bool | None = None,
4585         fill_value: Any = dtypes.NA,
4586         keep_attrs: bool | None = None,
4587     ) -> DataArray:
4588         """Return the coordinate label of the minimum value along a dimension.
4589 
4590         Returns a new `DataArray` named after the dimension with the values of
4591         the coordinate labels along that dimension corresponding to minimum
4592         values along that dimension.
4593 
4594         In comparison to :py:meth:`~DataArray.argmin`, this returns the
4595         coordinate label while :py:meth:`~DataArray.argmin` returns the index.
4596 
4597         Parameters
4598         ----------
4599         dim : str, optional
4600             Dimension over which to apply `idxmin`.  This is optional for 1D
4601             arrays, but required for arrays with 2 or more dimensions.
4602         skipna : bool or None, default: None
4603             If True, skip missing values (as marked by NaN). By default, only
4604             skips missing values for ``float``, ``complex``, and ``object``
4605             dtypes; other dtypes either do not have a sentinel missing value
4606             (``int``) or ``skipna=True`` has not been implemented
4607             (``datetime64`` or ``timedelta64``).
4608         fill_value : Any, default: NaN
4609             Value to be filled in case all of the values along a dimension are
4610             null.  By default this is NaN.  The fill value and result are
4611             automatically converted to a compatible dtype if possible.
4612             Ignored if ``skipna`` is False.
4613         keep_attrs : bool or None, optional
4614             If True, the attributes (``attrs``) will be copied from the
4615             original object to the new one. If False, the new object
4616             will be returned without attributes.
4617 
4618         Returns
4619         -------
4620         reduced : DataArray
4621             New `DataArray` object with `idxmin` applied to its data and the
4622             indicated dimension removed.
4623 
4624         See Also
4625         --------
4626         Dataset.idxmin, DataArray.idxmax, DataArray.min, DataArray.argmin
4627 
4628         Examples
4629         --------
4630         >>> array = xr.DataArray(
4631         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4632         ... )
4633         >>> array.min()
4634         <xarray.DataArray ()>
4635         array(-2)
4636         >>> array.argmin()
4637         <xarray.DataArray ()>
4638         array(4)
4639         >>> array.idxmin()
4640         <xarray.DataArray 'x' ()>
4641         array('e', dtype='<U1')
4642 
4643         >>> array = xr.DataArray(
4644         ...     [
4645         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4646         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4647         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4648         ...     ],
4649         ...     dims=["y", "x"],
4650         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4651         ... )
4652         >>> array.min(dim="x")
4653         <xarray.DataArray (y: 3)>
4654         array([-2., -4.,  1.])
4655         Coordinates:
4656           * y        (y) int64 -1 0 1
4657         >>> array.argmin(dim="x")
4658         <xarray.DataArray (y: 3)>
4659         array([4, 0, 2])
4660         Coordinates:
4661           * y        (y) int64 -1 0 1
4662         >>> array.idxmin(dim="x")
4663         <xarray.DataArray 'x' (y: 3)>
4664         array([16.,  0.,  4.])
4665         Coordinates:
4666           * y        (y) int64 -1 0 1
4667         """
4668         return computation._calc_idxminmax(
4669             array=self,
4670             func=lambda x, *args, **kwargs: x.argmin(*args, **kwargs),
4671             dim=dim,
4672             skipna=skipna,
4673             fill_value=fill_value,
4674             keep_attrs=keep_attrs,
4675         )
4676 
4677     def idxmax(
4678         self,
4679         dim: Hashable = None,
4680         skipna: bool | None = None,
4681         fill_value: Any = dtypes.NA,
4682         keep_attrs: bool | None = None,
4683     ) -> DataArray:
4684         """Return the coordinate label of the maximum value along a dimension.
4685 
4686         Returns a new `DataArray` named after the dimension with the values of
4687         the coordinate labels along that dimension corresponding to maximum
4688         values along that dimension.
4689 
4690         In comparison to :py:meth:`~DataArray.argmax`, this returns the
4691         coordinate label while :py:meth:`~DataArray.argmax` returns the index.
4692 
4693         Parameters
4694         ----------
4695         dim : Hashable, optional
4696             Dimension over which to apply `idxmax`.  This is optional for 1D
4697             arrays, but required for arrays with 2 or more dimensions.
4698         skipna : bool or None, default: None
4699             If True, skip missing values (as marked by NaN). By default, only
4700             skips missing values for ``float``, ``complex``, and ``object``
4701             dtypes; other dtypes either do not have a sentinel missing value
4702             (``int``) or ``skipna=True`` has not been implemented
4703             (``datetime64`` or ``timedelta64``).
4704         fill_value : Any, default: NaN
4705             Value to be filled in case all of the values along a dimension are
4706             null.  By default this is NaN.  The fill value and result are
4707             automatically converted to a compatible dtype if possible.
4708             Ignored if ``skipna`` is False.
4709         keep_attrs : bool or None, optional
4710             If True, the attributes (``attrs``) will be copied from the
4711             original object to the new one. If False, the new object
4712             will be returned without attributes.
4713 
4714         Returns
4715         -------
4716         reduced : DataArray
4717             New `DataArray` object with `idxmax` applied to its data and the
4718             indicated dimension removed.
4719 
4720         See Also
4721         --------
4722         Dataset.idxmax, DataArray.idxmin, DataArray.max, DataArray.argmax
4723 
4724         Examples
4725         --------
4726         >>> array = xr.DataArray(
4727         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
4728         ... )
4729         >>> array.max()
4730         <xarray.DataArray ()>
4731         array(2)
4732         >>> array.argmax()
4733         <xarray.DataArray ()>
4734         array(1)
4735         >>> array.idxmax()
4736         <xarray.DataArray 'x' ()>
4737         array('b', dtype='<U1')
4738 
4739         >>> array = xr.DataArray(
4740         ...     [
4741         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
4742         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
4743         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
4744         ...     ],
4745         ...     dims=["y", "x"],
4746         ...     coords={"y": [-1, 0, 1], "x": np.arange(5.0) ** 2},
4747         ... )
4748         >>> array.max(dim="x")
4749         <xarray.DataArray (y: 3)>
4750         array([2., 2., 1.])
4751         Coordinates:
4752           * y        (y) int64 -1 0 1
4753         >>> array.argmax(dim="x")
4754         <xarray.DataArray (y: 3)>
4755         array([0, 2, 2])
4756         Coordinates:
4757           * y        (y) int64 -1 0 1
4758         >>> array.idxmax(dim="x")
4759         <xarray.DataArray 'x' (y: 3)>
4760         array([0., 4., 4.])
4761         Coordinates:
4762           * y        (y) int64 -1 0 1
4763         """
4764         return computation._calc_idxminmax(
4765             array=self,
4766             func=lambda x, *args, **kwargs: x.argmax(*args, **kwargs),
4767             dim=dim,
4768             skipna=skipna,
4769             fill_value=fill_value,
4770             keep_attrs=keep_attrs,
4771         )
4772 
4773     # change type of self and return to T_DataArray once
4774     # https://github.com/python/mypy/issues/12846 is resolved
4775     def argmin(
4776         self,
4777         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4778         axis: int | None = None,
4779         keep_attrs: bool | None = None,
4780         skipna: bool | None = None,
4781     ) -> DataArray | dict[Hashable, DataArray]:
4782         """Index or indices of the minimum of the DataArray over one or more dimensions.
4783 
4784         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4785         which can be passed directly to isel(). If a single str is passed to 'dim' then
4786         returns a DataArray with dtype int.
4787 
4788         If there are multiple minima, the indices of the first one found will be
4789         returned.
4790 
4791         Parameters
4792         ----------
4793         dim : Hashable, sequence of Hashable, None or ..., optional
4794             The dimensions over which to find the minimum. By default, finds minimum over
4795             all dimensions - for now returning an int for backward compatibility, but
4796             this is deprecated, in future will return a dict with indices for all
4797             dimensions; to return a dict with all dimensions now, pass '...'.
4798         axis : int or None, optional
4799             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
4800             can be supplied.
4801         keep_attrs : bool or None, optional
4802             If True, the attributes (`attrs`) will be copied from the original
4803             object to the new one. If False, the new object will be
4804             returned without attributes.
4805         skipna : bool or None, optional
4806             If True, skip missing values (as marked by NaN). By default, only
4807             skips missing values for float dtypes; other dtypes either do not
4808             have a sentinel missing value (int) or skipna=True has not been
4809             implemented (object, datetime64 or timedelta64).
4810 
4811         Returns
4812         -------
4813         result : DataArray or dict of DataArray
4814 
4815         See Also
4816         --------
4817         Variable.argmin, DataArray.idxmin
4818 
4819         Examples
4820         --------
4821         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4822         >>> array.min()
4823         <xarray.DataArray ()>
4824         array(-1)
4825         >>> array.argmin()
4826         <xarray.DataArray ()>
4827         array(2)
4828         >>> array.argmin(...)
4829         {'x': <xarray.DataArray ()>
4830         array(2)}
4831         >>> array.isel(array.argmin(...))
4832         <xarray.DataArray ()>
4833         array(-1)
4834 
4835         >>> array = xr.DataArray(
4836         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, -5, 1], [2, 3, 1]]],
4837         ...     dims=("x", "y", "z"),
4838         ... )
4839         >>> array.min(dim="x")
4840         <xarray.DataArray (y: 3, z: 3)>
4841         array([[ 1,  2,  1],
4842                [ 2, -5,  1],
4843                [ 2,  1,  1]])
4844         Dimensions without coordinates: y, z
4845         >>> array.argmin(dim="x")
4846         <xarray.DataArray (y: 3, z: 3)>
4847         array([[1, 0, 0],
4848                [1, 1, 1],
4849                [0, 0, 1]])
4850         Dimensions without coordinates: y, z
4851         >>> array.argmin(dim=["x"])
4852         {'x': <xarray.DataArray (y: 3, z: 3)>
4853         array([[1, 0, 0],
4854                [1, 1, 1],
4855                [0, 0, 1]])
4856         Dimensions without coordinates: y, z}
4857         >>> array.min(dim=("x", "z"))
4858         <xarray.DataArray (y: 3)>
4859         array([ 1, -5,  1])
4860         Dimensions without coordinates: y
4861         >>> array.argmin(dim=["x", "z"])
4862         {'x': <xarray.DataArray (y: 3)>
4863         array([0, 1, 0])
4864         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4865         array([2, 1, 1])
4866         Dimensions without coordinates: y}
4867         >>> array.isel(array.argmin(dim=["x", "z"]))
4868         <xarray.DataArray (y: 3)>
4869         array([ 1, -5,  1])
4870         Dimensions without coordinates: y
4871         """
4872         result = self.variable.argmin(dim, axis, keep_attrs, skipna)
4873         if isinstance(result, dict):
4874             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4875         else:
4876             return self._replace_maybe_drop_dims(result)
4877 
4878     # change type of self and return to T_DataArray once
4879     # https://github.com/python/mypy/issues/12846 is resolved
4880     def argmax(
4881         self,
4882         dim: Hashable | Sequence[Hashable] | Ellipsis | None = None,
4883         axis: int | None = None,
4884         keep_attrs: bool | None = None,
4885         skipna: bool | None = None,
4886     ) -> DataArray | dict[Hashable, DataArray]:
4887         """Index or indices of the maximum of the DataArray over one or more dimensions.
4888 
4889         If a sequence is passed to 'dim', then result returned as dict of DataArrays,
4890         which can be passed directly to isel(). If a single str is passed to 'dim' then
4891         returns a DataArray with dtype int.
4892 
4893         If there are multiple maxima, the indices of the first one found will be
4894         returned.
4895 
4896         Parameters
4897         ----------
4898         dim : Hashable, sequence of Hashable, None or ..., optional
4899             The dimensions over which to find the maximum. By default, finds maximum over
4900             all dimensions - for now returning an int for backward compatibility, but
4901             this is deprecated, in future will return a dict with indices for all
4902             dimensions; to return a dict with all dimensions now, pass '...'.
4903         axis : int or None, optional
4904             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
4905             can be supplied.
4906         keep_attrs : bool or None, optional
4907             If True, the attributes (`attrs`) will be copied from the original
4908             object to the new one. If False, the new object will be
4909             returned without attributes.
4910         skipna : bool or None, optional
4911             If True, skip missing values (as marked by NaN). By default, only
4912             skips missing values for float dtypes; other dtypes either do not
4913             have a sentinel missing value (int) or skipna=True has not been
4914             implemented (object, datetime64 or timedelta64).
4915 
4916         Returns
4917         -------
4918         result : DataArray or dict of DataArray
4919 
4920         See Also
4921         --------
4922         Variable.argmax, DataArray.idxmax
4923 
4924         Examples
4925         --------
4926         >>> array = xr.DataArray([0, 2, -1, 3], dims="x")
4927         >>> array.max()
4928         <xarray.DataArray ()>
4929         array(3)
4930         >>> array.argmax()
4931         <xarray.DataArray ()>
4932         array(3)
4933         >>> array.argmax(...)
4934         {'x': <xarray.DataArray ()>
4935         array(3)}
4936         >>> array.isel(array.argmax(...))
4937         <xarray.DataArray ()>
4938         array(3)
4939 
4940         >>> array = xr.DataArray(
4941         ...     [[[3, 2, 1], [3, 1, 2], [2, 1, 3]], [[1, 3, 2], [2, 5, 1], [2, 3, 1]]],
4942         ...     dims=("x", "y", "z"),
4943         ... )
4944         >>> array.max(dim="x")
4945         <xarray.DataArray (y: 3, z: 3)>
4946         array([[3, 3, 2],
4947                [3, 5, 2],
4948                [2, 3, 3]])
4949         Dimensions without coordinates: y, z
4950         >>> array.argmax(dim="x")
4951         <xarray.DataArray (y: 3, z: 3)>
4952         array([[0, 1, 1],
4953                [0, 1, 0],
4954                [0, 1, 0]])
4955         Dimensions without coordinates: y, z
4956         >>> array.argmax(dim=["x"])
4957         {'x': <xarray.DataArray (y: 3, z: 3)>
4958         array([[0, 1, 1],
4959                [0, 1, 0],
4960                [0, 1, 0]])
4961         Dimensions without coordinates: y, z}
4962         >>> array.max(dim=("x", "z"))
4963         <xarray.DataArray (y: 3)>
4964         array([3, 5, 3])
4965         Dimensions without coordinates: y
4966         >>> array.argmax(dim=["x", "z"])
4967         {'x': <xarray.DataArray (y: 3)>
4968         array([0, 1, 0])
4969         Dimensions without coordinates: y, 'z': <xarray.DataArray (y: 3)>
4970         array([0, 1, 2])
4971         Dimensions without coordinates: y}
4972         >>> array.isel(array.argmax(dim=["x", "z"]))
4973         <xarray.DataArray (y: 3)>
4974         array([3, 5, 3])
4975         Dimensions without coordinates: y
4976         """
4977         result = self.variable.argmax(dim, axis, keep_attrs, skipna)
4978         if isinstance(result, dict):
4979             return {k: self._replace_maybe_drop_dims(v) for k, v in result.items()}
4980         else:
4981             return self._replace_maybe_drop_dims(result)
4982 
4983     def query(
4984         self,
4985         queries: Mapping[Any, Any] | None = None,
4986         parser: QueryParserOptions = "pandas",
4987         engine: QueryEngineOptions = None,
4988         missing_dims: ErrorOptionsWithWarn = "raise",
4989         **queries_kwargs: Any,
4990     ) -> DataArray:
4991         """Return a new data array indexed along the specified
4992         dimension(s), where the indexers are given as strings containing
4993         Python expressions to be evaluated against the values in the array.
4994 
4995         Parameters
4996         ----------
4997         queries : dict-like or None, optional
4998             A dict-like with keys matching dimensions and values given by strings
4999             containing Python expressions to be evaluated against the data variables
5000             in the dataset. The expressions will be evaluated using the pandas
5001             eval() function, and can contain any valid Python expressions but cannot
5002             contain any Python statements.
5003         parser : {"pandas", "python"}, default: "pandas"
5004             The parser to use to construct the syntax tree from the expression.
5005             The default of 'pandas' parses code slightly different than standard
5006             Python. Alternatively, you can parse an expression using the 'python'
5007             parser to retain strict Python semantics.
5008         engine : {"python", "numexpr", None}, default: None
5009             The engine used to evaluate the expression. Supported engines are:
5010 
5011             - None: tries to use numexpr, falls back to python
5012             - "numexpr": evaluates expressions using numexpr
5013             - "python": performs operations as if you had eval’d in top level python
5014 
5015         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5016             What to do if dimensions that should be selected from are not present in the
5017             DataArray:
5018 
5019             - "raise": raise an exception
5020             - "warn": raise a warning, and ignore the missing dimensions
5021             - "ignore": ignore the missing dimensions
5022 
5023         **queries_kwargs : {dim: query, ...}, optional
5024             The keyword arguments form of ``queries``.
5025             One of queries or queries_kwargs must be provided.
5026 
5027         Returns
5028         -------
5029         obj : DataArray
5030             A new DataArray with the same contents as this dataset, indexed by
5031             the results of the appropriate queries.
5032 
5033         See Also
5034         --------
5035         DataArray.isel
5036         Dataset.query
5037         pandas.eval
5038 
5039         Examples
5040         --------
5041         >>> da = xr.DataArray(np.arange(0, 5, 1), dims="x", name="a")
5042         >>> da
5043         <xarray.DataArray 'a' (x: 5)>
5044         array([0, 1, 2, 3, 4])
5045         Dimensions without coordinates: x
5046         >>> da.query(x="a > 2")
5047         <xarray.DataArray 'a' (x: 2)>
5048         array([3, 4])
5049         Dimensions without coordinates: x
5050         """
5051 
5052         ds = self._to_dataset_whole(shallow_copy=True)
5053         ds = ds.query(
5054             queries=queries,
5055             parser=parser,
5056             engine=engine,
5057             missing_dims=missing_dims,
5058             **queries_kwargs,
5059         )
5060         return ds[self.name]
5061 
5062     def curvefit(
5063         self,
5064         coords: str | DataArray | Iterable[str | DataArray],
5065         func: Callable[..., Any],
5066         reduce_dims: Hashable | Iterable[Hashable] | None = None,
5067         skipna: bool = True,
5068         p0: dict[str, Any] | None = None,
5069         bounds: dict[str, Any] | None = None,
5070         param_names: Sequence[str] | None = None,
5071         kwargs: dict[str, Any] | None = None,
5072     ) -> Dataset:
5073         """
5074         Curve fitting optimization for arbitrary functions.
5075 
5076         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
5077 
5078         Parameters
5079         ----------
5080         coords : Hashable, DataArray, or sequence of DataArray or Hashable
5081             Independent coordinate(s) over which to perform the curve fitting. Must share
5082             at least one dimension with the calling object. When fitting multi-dimensional
5083             functions, supply `coords` as a sequence in the same order as arguments in
5084             `func`. To fit along existing dimensions of the calling object, `coords` can
5085             also be specified as a str or sequence of strs.
5086         func : callable
5087             User specified function in the form `f(x, *params)` which returns a numpy
5088             array of length `len(x)`. `params` are the fittable parameters which are optimized
5089             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
5090             coordinates, e.g. `f((x0, x1), *params)`.
5091         reduce_dims : Hashable or sequence of Hashable
5092             Additional dimension(s) over which to aggregate while fitting. For example,
5093             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
5094             aggregate all lat and lon points and fit the specified function along the
5095             time dimension.
5096         skipna : bool, default: True
5097             Whether to skip missing values when fitting. Default is True.
5098         p0 : dict-like or None, optional
5099             Optional dictionary of parameter names to initial guesses passed to the
5100             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
5101             be assigned initial values following the default scipy behavior.
5102         bounds : dict-like or None, optional
5103             Optional dictionary of parameter names to bounding values passed to the
5104             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
5105             will be unbounded following the default scipy behavior.
5106         param_names : sequence of Hashable or None, optional
5107             Sequence of names for the fittable parameters of `func`. If not supplied,
5108             this will be automatically determined by arguments of `func`. `param_names`
5109             should be manually supplied when fitting a function that takes a variable
5110             number of parameters.
5111         **kwargs : optional
5112             Additional keyword arguments to passed to scipy curve_fit.
5113 
5114         Returns
5115         -------
5116         curvefit_results : Dataset
5117             A single dataset which contains:
5118 
5119             [var]_curvefit_coefficients
5120                 The coefficients of the best fit.
5121             [var]_curvefit_covariance
5122                 The covariance matrix of the coefficient estimates.
5123 
5124         See Also
5125         --------
5126         DataArray.polyfit
5127         scipy.optimize.curve_fit
5128         """
5129         return self._to_temp_dataset().curvefit(
5130             coords,
5131             func,
5132             reduce_dims=reduce_dims,
5133             skipna=skipna,
5134             p0=p0,
5135             bounds=bounds,
5136             param_names=param_names,
5137             kwargs=kwargs,
5138         )
5139 
5140     def drop_duplicates(
5141         self: T_DataArray,
5142         dim: Hashable | Iterable[Hashable],
5143         keep: Literal["first", "last", False] = "first",
5144     ) -> T_DataArray:
5145         """Returns a new DataArray with duplicate dimension values removed.
5146 
5147         Parameters
5148         ----------
5149         dim : dimension label or labels
5150             Pass `...` to drop duplicates along all dimensions.
5151         keep : {"first", "last", False}, default: "first"
5152             Determines which duplicates (if any) to keep.
5153 
5154             - ``"first"`` : Drop duplicates except for the first occurrence.
5155             - ``"last"`` : Drop duplicates except for the last occurrence.
5156             - False : Drop all duplicates.
5157 
5158         Returns
5159         -------
5160         DataArray
5161 
5162         See Also
5163         --------
5164         Dataset.drop_duplicates
5165         """
5166         deduplicated = self._to_temp_dataset().drop_duplicates(dim, keep=keep)
5167         return self._from_temp_dataset(deduplicated)
5168 
5169     def convert_calendar(
5170         self,
5171         calendar: str,
5172         dim: str = "time",
5173         align_on: str | None = None,
5174         missing: Any | None = None,
5175         use_cftime: bool | None = None,
5176     ) -> DataArray:
5177         """Convert the DataArray to another calendar.
5178 
5179         Only converts the individual timestamps, does not modify any data except
5180         in dropping invalid/surplus dates or inserting missing dates.
5181 
5182         If the source and target calendars are either no_leap, all_leap or a
5183         standard type, only the type of the time array is modified.
5184         When converting to a leap year from a non-leap year, the 29th of February
5185         is removed from the array. In the other direction the 29th of February
5186         will be missing in the output, unless `missing` is specified,
5187         in which case that value is inserted.
5188 
5189         For conversions involving `360_day` calendars, see Notes.
5190 
5191         This method is safe to use with sub-daily data as it doesn't touch the
5192         time part of the timestamps.
5193 
5194         Parameters
5195         ---------
5196         calendar : str
5197             The target calendar name.
5198         dim : str
5199             Name of the time coordinate.
5200         align_on : {None, 'date', 'year'}
5201             Must be specified when either source or target is a `360_day` calendar,
5202            ignored otherwise. See Notes.
5203         missing : Optional[any]
5204             By default, i.e. if the value is None, this method will simply attempt
5205             to convert the dates in the source calendar to the same dates in the
5206             target calendar, and drop any of those that are not possible to
5207             represent.  If a value is provided, a new time coordinate will be
5208             created in the target calendar with the same frequency as the original
5209             time coordinate; for any dates that are not present in the source, the
5210             data will be filled with this value.  Note that using this mode requires
5211             that the source data have an inferable frequency; for more information
5212             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
5213             target calendar combinations, this could result in many missing values, see notes.
5214         use_cftime : boolean, optional
5215             Whether to use cftime objects in the output, only used if `calendar`
5216             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
5217             If True, the new time axis uses cftime objects.
5218             If None (default), it uses :py:class:`numpy.datetime64` values if the
5219             date range permits it, and :py:class:`cftime.datetime` objects if not.
5220             If False, it uses :py:class:`numpy.datetime64`  or fails.
5221 
5222         Returns
5223         -------
5224         DataArray
5225             Copy of the dataarray with the time coordinate converted to the
5226             target calendar. If 'missing' was None (default), invalid dates in
5227             the new calendar are dropped, but missing dates are not inserted.
5228             If `missing` was given, the new data is reindexed to have a time axis
5229             with the same frequency as the source, but in the new calendar; any
5230             missing datapoints are filled with `missing`.
5231 
5232         Notes
5233         -----
5234         Passing a value to `missing` is only usable if the source's time coordinate as an
5235         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
5236         if the target coordinate, generated from this frequency, has dates equivalent to the
5237         source. It is usually **not** appropriate to use this mode with:
5238 
5239         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
5240         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
5241             or 'mH' where 24 % m != 0).
5242 
5243         If one of the source or target calendars is `"360_day"`, `align_on` must
5244         be specified and two options are offered.
5245 
5246         - "year"
5247             The dates are translated according to their relative position in the year,
5248             ignoring their original month and day information, meaning that the
5249             missing/surplus days are added/removed at regular intervals.
5250 
5251             From a `360_day` to a standard calendar, the output will be missing the
5252             following dates (day of year in parentheses):
5253 
5254             To a leap year:
5255                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
5256                 September 31st (275) and November 30th (335).
5257             To a non-leap year:
5258                 February 6th (36), April 19th (109), July 2nd (183),
5259                 September 12th (255), November 25th (329).
5260 
5261             From a standard calendar to a `"360_day"`, the following dates in the
5262             source array will be dropped:
5263 
5264             From a leap year:
5265                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
5266                 September 31st (275), December 1st (336)
5267             From a non-leap year:
5268                 February 6th (37), April 20th (110), July 2nd (183),
5269                 September 13th (256), November 25th (329)
5270 
5271             This option is best used on daily and subdaily data.
5272 
5273         - "date"
5274             The month/day information is conserved and invalid dates are dropped
5275             from the output. This means that when converting from a `"360_day"` to a
5276             standard calendar, all 31st (Jan, March, May, July, August, October and
5277             December) will be missing as there is no equivalent dates in the
5278             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
5279             will be dropped as there are no equivalent dates in a standard calendar.
5280 
5281             This option is best used with data on a frequency coarser than daily.
5282         """
5283         return convert_calendar(
5284             self,
5285             calendar,
5286             dim=dim,
5287             align_on=align_on,
5288             missing=missing,
5289             use_cftime=use_cftime,
5290         )
5291 
5292     def interp_calendar(
5293         self,
5294         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
5295         dim: str = "time",
5296     ) -> DataArray:
5297         """Interpolates the DataArray to another calendar based on decimal year measure.
5298 
5299         Each timestamp in `source` and `target` are first converted to their decimal
5300         year equivalent then `source` is interpolated on the target coordinate.
5301         The decimal year of a timestamp is its year plus its sub-year component
5302         converted to the fraction of its year. For example "2000-03-01 12:00" is
5303         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
5304 
5305         This method should only be used when the time (HH:MM:SS) information of
5306         time coordinate is not important.
5307 
5308         Parameters
5309         ----------
5310         target: DataArray or DatetimeIndex or CFTimeIndex
5311             The target time coordinate of a valid dtype
5312             (np.datetime64 or cftime objects)
5313         dim : str
5314             The time coordinate name.
5315 
5316         Return
5317         ------
5318         DataArray
5319             The source interpolated on the decimal years of target,
5320         """
5321         return interp_calendar(self, target, dim=dim)
5322 
5323     def groupby(
5324         self,
5325         group: Hashable | DataArray | IndexVariable,
5326         squeeze: bool = True,
5327         restore_coord_dims: bool = False,
5328     ) -> DataArrayGroupBy:
5329         """Returns a DataArrayGroupBy object for performing grouped operations.
5330 
5331         Parameters
5332         ----------
5333         group : Hashable, DataArray or IndexVariable
5334             Array whose unique values should be used to group this array. If a
5335             Hashable, must be the name of a coordinate contained in this dataarray.
5336         squeeze : bool, default: True
5337             If "group" is a dimension of any arrays in this dataset, `squeeze`
5338             controls whether the subarrays have a dimension of length 1 along
5339             that dimension or if the dimension is squeezed out.
5340         restore_coord_dims : bool, default: False
5341             If True, also restore the dimension order of multi-dimensional
5342             coordinates.
5343 
5344         Returns
5345         -------
5346         grouped : DataArrayGroupBy
5347             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5348             iterated over in the form of `(unique_value, grouped_array)` pairs.
5349 
5350         Examples
5351         --------
5352         Calculate daily anomalies for daily data:
5353 
5354         >>> da = xr.DataArray(
5355         ...     np.linspace(0, 1826, num=1827),
5356         ...     coords=[pd.date_range("1/1/2000", "31/12/2004", freq="D")],
5357         ...     dims="time",
5358         ... )
5359         >>> da
5360         <xarray.DataArray (time: 1827)>
5361         array([0.000e+00, 1.000e+00, 2.000e+00, ..., 1.824e+03, 1.825e+03,
5362                1.826e+03])
5363         Coordinates:
5364           * time     (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5365         >>> da.groupby("time.dayofyear") - da.groupby("time.dayofyear").mean("time")
5366         <xarray.DataArray (time: 1827)>
5367         array([-730.8, -730.8, -730.8, ...,  730.2,  730.2,  730.5])
5368         Coordinates:
5369           * time       (time) datetime64[ns] 2000-01-01 2000-01-02 ... 2004-12-31
5370             dayofyear  (time) int64 1 2 3 4 5 6 7 8 ... 359 360 361 362 363 364 365 366
5371 
5372         See Also
5373         --------
5374         DataArray.groupby_bins
5375         Dataset.groupby
5376         core.groupby.DataArrayGroupBy
5377         pandas.DataFrame.groupby
5378         """
5379         from .groupby import DataArrayGroupBy
5380 
5381         # While we don't generally check the type of every arg, passing
5382         # multiple dimensions as multiple arguments is common enough, and the
5383         # consequences hidden enough (strings evaluate as true) to warrant
5384         # checking here.
5385         # A future version could make squeeze kwarg only, but would face
5386         # backward-compat issues.
5387         if not isinstance(squeeze, bool):
5388             raise TypeError(
5389                 f"`squeeze` must be True or False, but {squeeze} was supplied"
5390             )
5391 
5392         return DataArrayGroupBy(
5393             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
5394         )
5395 
5396     def groupby_bins(
5397         self,
5398         group: Hashable | DataArray | IndexVariable,
5399         bins: ArrayLike,
5400         right: bool = True,
5401         labels: ArrayLike | Literal[False] | None = None,
5402         precision: int = 3,
5403         include_lowest: bool = False,
5404         squeeze: bool = True,
5405         restore_coord_dims: bool = False,
5406     ) -> DataArrayGroupBy:
5407         """Returns a DataArrayGroupBy object for performing grouped operations.
5408 
5409         Rather than using all unique values of `group`, the values are discretized
5410         first by applying `pandas.cut` [1]_ to `group`.
5411 
5412         Parameters
5413         ----------
5414         group : Hashable, DataArray or IndexVariable
5415             Array whose binned values should be used to group this array. If a
5416             Hashable, must be the name of a coordinate contained in this dataarray.
5417         bins : int or array-like
5418             If bins is an int, it defines the number of equal-width bins in the
5419             range of x. However, in this case, the range of x is extended by .1%
5420             on each side to include the min or max values of x. If bins is a
5421             sequence it defines the bin edges allowing for non-uniform bin
5422             width. No extension of the range of x is done in this case.
5423         right : bool, default: True
5424             Indicates whether the bins include the rightmost edge or not. If
5425             right == True (the default), then the bins [1,2,3,4] indicate
5426             (1,2], (2,3], (3,4].
5427         labels : array-like, False or None, default: None
5428             Used as labels for the resulting bins. Must be of the same length as
5429             the resulting bins. If False, string bin labels are assigned by
5430             `pandas.cut`.
5431         precision : int, default: 3
5432             The precision at which to store and display the bins labels.
5433         include_lowest : bool, default: False
5434             Whether the first interval should be left-inclusive or not.
5435         squeeze : bool, default: True
5436             If "group" is a dimension of any arrays in this dataset, `squeeze`
5437             controls whether the subarrays have a dimension of length 1 along
5438             that dimension or if the dimension is squeezed out.
5439         restore_coord_dims : bool, default: False
5440             If True, also restore the dimension order of multi-dimensional
5441             coordinates.
5442 
5443         Returns
5444         -------
5445         grouped : DataArrayGroupBy
5446             A `DataArrayGroupBy` object patterned after `pandas.GroupBy` that can be
5447             iterated over in the form of `(unique_value, grouped_array)` pairs.
5448             The name of the group has the added suffix `_bins` in order to
5449             distinguish it from the original variable.
5450 
5451         See Also
5452         --------
5453         DataArray.groupby
5454         Dataset.groupby_bins
5455         core.groupby.DataArrayGroupBy
5456         pandas.DataFrame.groupby
5457 
5458         References
5459         ----------
5460         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
5461         """
5462         from .groupby import DataArrayGroupBy
5463 
5464         return DataArrayGroupBy(
5465             self,
5466             group,
5467             squeeze=squeeze,
5468             bins=bins,
5469             restore_coord_dims=restore_coord_dims,
5470             cut_kwargs={
5471                 "right": right,
5472                 "labels": labels,
5473                 "precision": precision,
5474                 "include_lowest": include_lowest,
5475             },
5476         )
5477 
5478     def weighted(self, weights: DataArray) -> DataArrayWeighted:
5479         """
5480         Weighted DataArray operations.
5481 
5482         Parameters
5483         ----------
5484         weights : DataArray
5485             An array of weights associated with the values in this Dataset.
5486             Each value in the data contributes to the reduction operation
5487             according to its associated weight.
5488 
5489         Notes
5490         -----
5491         ``weights`` must be a DataArray and cannot contain missing values.
5492         Missing values can be replaced by ``weights.fillna(0)``.
5493 
5494         Returns
5495         -------
5496         core.weighted.DataArrayWeighted
5497 
5498         See Also
5499         --------
5500         Dataset.weighted
5501         """
5502         from .weighted import DataArrayWeighted
5503 
5504         return DataArrayWeighted(self, weights)
5505 
5506     def rolling(
5507         self,
5508         dim: Mapping[Any, int] | None = None,
5509         min_periods: int | None = None,
5510         center: bool | Mapping[Any, bool] = False,
5511         **window_kwargs: int,
5512     ) -> DataArrayRolling:
5513         """
5514         Rolling window object for DataArrays.
5515 
5516         Parameters
5517         ----------
5518         dim : dict, optional
5519             Mapping from the dimension name to create the rolling iterator
5520             along (e.g. `time`) to its moving window size.
5521         min_periods : int or None, default: None
5522             Minimum number of observations in window required to have a value
5523             (otherwise result is NA). The default, None, is equivalent to
5524             setting min_periods equal to the size of the window.
5525         center : bool or Mapping to int, default: False
5526             Set the labels at the center of the window.
5527         **window_kwargs : optional
5528             The keyword arguments form of ``dim``.
5529             One of dim or window_kwargs must be provided.
5530 
5531         Returns
5532         -------
5533         core.rolling.DataArrayRolling
5534 
5535         Examples
5536         --------
5537         Create rolling seasonal average of monthly data e.g. DJF, JFM, ..., SON:
5538 
5539         >>> da = xr.DataArray(
5540         ...     np.linspace(0, 11, num=12),
5541         ...     coords=[
5542         ...         pd.date_range(
5543         ...             "1999-12-15",
5544         ...             periods=12,
5545         ...             freq=pd.DateOffset(months=1),
5546         ...         )
5547         ...     ],
5548         ...     dims="time",
5549         ... )
5550         >>> da
5551         <xarray.DataArray (time: 12)>
5552         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5553         Coordinates:
5554           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5555         >>> da.rolling(time=3, center=True).mean()
5556         <xarray.DataArray (time: 12)>
5557         array([nan,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., nan])
5558         Coordinates:
5559           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5560 
5561         Remove the NaNs using ``dropna()``:
5562 
5563         >>> da.rolling(time=3, center=True).mean().dropna("time")
5564         <xarray.DataArray (time: 10)>
5565         array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])
5566         Coordinates:
5567           * time     (time) datetime64[ns] 2000-01-15 2000-02-15 ... 2000-10-15
5568 
5569         See Also
5570         --------
5571         core.rolling.DataArrayRolling
5572         Dataset.rolling
5573         """
5574         from .rolling import DataArrayRolling
5575 
5576         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
5577         return DataArrayRolling(self, dim, min_periods=min_periods, center=center)
5578 
5579     def coarsen(
5580         self,
5581         dim: Mapping[Any, int] | None = None,
5582         boundary: CoarsenBoundaryOptions = "exact",
5583         side: SideOptions | Mapping[Any, SideOptions] = "left",
5584         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
5585         **window_kwargs: int,
5586     ) -> DataArrayCoarsen:
5587         """
5588         Coarsen object for DataArrays.
5589 
5590         Parameters
5591         ----------
5592         dim : mapping of hashable to int, optional
5593             Mapping from the dimension name to the window size.
5594         boundary : {"exact", "trim", "pad"}, default: "exact"
5595             If 'exact', a ValueError will be raised if dimension size is not a
5596             multiple of the window size. If 'trim', the excess entries are
5597             dropped. If 'pad', NA will be padded.
5598         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
5599         coord_func : str or mapping of hashable to str, default: "mean"
5600             function (name) that is applied to the coordinates,
5601             or a mapping from coordinate name to function (name).
5602 
5603         Returns
5604         -------
5605         core.rolling.DataArrayCoarsen
5606 
5607         Examples
5608         --------
5609         Coarsen the long time series by averaging over every four days.
5610 
5611         >>> da = xr.DataArray(
5612         ...     np.linspace(0, 364, num=364),
5613         ...     dims="time",
5614         ...     coords={"time": pd.date_range("1999-12-15", periods=364)},
5615         ... )
5616         >>> da  # +doctest: ELLIPSIS
5617         <xarray.DataArray (time: 364)>
5618         array([  0.        ,   1.00275482,   2.00550964,   3.00826446,
5619                  4.01101928,   5.0137741 ,   6.01652893,   7.01928375,
5620                  8.02203857,   9.02479339,  10.02754821,  11.03030303,
5621         ...
5622                356.98071625, 357.98347107, 358.9862259 , 359.98898072,
5623                360.99173554, 361.99449036, 362.99724518, 364.        ])
5624         Coordinates:
5625           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-12-12
5626         >>> da.coarsen(time=3, boundary="trim").mean()  # +doctest: ELLIPSIS
5627         <xarray.DataArray (time: 121)>
5628         array([  1.00275482,   4.01101928,   7.01928375,  10.02754821,
5629                 13.03581267,  16.04407713,  19.0523416 ,  22.06060606,
5630                 25.06887052,  28.07713499,  31.08539945,  34.09366391,
5631         ...
5632                349.96143251, 352.96969697, 355.97796143, 358.9862259 ,
5633                361.99449036])
5634         Coordinates:
5635           * time     (time) datetime64[ns] 1999-12-16 1999-12-19 ... 2000-12-10
5636         >>>
5637 
5638         See Also
5639         --------
5640         core.rolling.DataArrayCoarsen
5641         Dataset.coarsen
5642         """
5643         from .rolling import DataArrayCoarsen
5644 
5645         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
5646         return DataArrayCoarsen(
5647             self,
5648             dim,
5649             boundary=boundary,
5650             side=side,
5651             coord_func=coord_func,
5652         )
5653 
5654     def resample(
5655         self,
5656         indexer: Mapping[Any, str] | None = None,
5657         skipna: bool | None = None,
5658         closed: SideOptions | None = None,
5659         label: SideOptions | None = None,
5660         base: int = 0,
5661         keep_attrs: bool | None = None,
5662         loffset: datetime.timedelta | str | None = None,
5663         restore_coord_dims: bool | None = None,
5664         **indexer_kwargs: str,
5665     ) -> DataArrayResample:
5666         """Returns a Resample object for performing resampling operations.
5667 
5668         Handles both downsampling and upsampling. The resampled
5669         dimension must be a datetime-like coordinate. If any intervals
5670         contain no values from the original object, they will be given
5671         the value ``NaN``.
5672 
5673         Parameters
5674         ----------
5675         indexer : Mapping of Hashable to str, optional
5676             Mapping from the dimension name to resample frequency [1]_. The
5677             dimension must be datetime-like.
5678         skipna : bool, optional
5679             Whether to skip missing values when aggregating in downsampling.
5680         closed : {"left", "right"}, optional
5681             Side of each interval to treat as closed.
5682         label : {"left", "right"}, optional
5683             Side of each interval to use for labeling.
5684         base : int, default = 0
5685             For frequencies that evenly subdivide 1 day, the "origin" of the
5686             aggregated intervals. For example, for "24H" frequency, base could
5687             range from 0 through 23.
5688         loffset : timedelta or str, optional
5689             Offset used to adjust the resampled time labels. Some pandas date
5690             offset strings are supported.
5691         restore_coord_dims : bool, optional
5692             If True, also restore the dimension order of multi-dimensional
5693             coordinates.
5694         **indexer_kwargs : str
5695             The keyword arguments form of ``indexer``.
5696             One of indexer or indexer_kwargs must be provided.
5697 
5698         Returns
5699         -------
5700         resampled : core.resample.DataArrayResample
5701             This object resampled.
5702 
5703         Examples
5704         --------
5705         Downsample monthly time-series data to seasonal data:
5706 
5707         >>> da = xr.DataArray(
5708         ...     np.linspace(0, 11, num=12),
5709         ...     coords=[
5710         ...         pd.date_range(
5711         ...             "1999-12-15",
5712         ...             periods=12,
5713         ...             freq=pd.DateOffset(months=1),
5714         ...         )
5715         ...     ],
5716         ...     dims="time",
5717         ... )
5718         >>> da
5719         <xarray.DataArray (time: 12)>
5720         array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.])
5721         Coordinates:
5722           * time     (time) datetime64[ns] 1999-12-15 2000-01-15 ... 2000-11-15
5723         >>> da.resample(time="QS-DEC").mean()
5724         <xarray.DataArray (time: 4)>
5725         array([ 1.,  4.,  7., 10.])
5726         Coordinates:
5727           * time     (time) datetime64[ns] 1999-12-01 2000-03-01 2000-06-01 2000-09-01
5728 
5729         Upsample monthly time-series data to daily data:
5730 
5731         >>> da.resample(time="1D").interpolate("linear")  # +doctest: ELLIPSIS
5732         <xarray.DataArray (time: 337)>
5733         array([ 0.        ,  0.03225806,  0.06451613,  0.09677419,  0.12903226,
5734                 0.16129032,  0.19354839,  0.22580645,  0.25806452,  0.29032258,
5735                 0.32258065,  0.35483871,  0.38709677,  0.41935484,  0.4516129 ,
5736         ...
5737                10.80645161, 10.83870968, 10.87096774, 10.90322581, 10.93548387,
5738                10.96774194, 11.        ])
5739         Coordinates:
5740           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5741 
5742         Limit scope of upsampling method
5743 
5744         >>> da.resample(time="1D").nearest(tolerance="1D")
5745         <xarray.DataArray (time: 337)>
5746         array([ 0.,  0., nan, ..., nan, 11., 11.])
5747         Coordinates:
5748           * time     (time) datetime64[ns] 1999-12-15 1999-12-16 ... 2000-11-15
5749 
5750         See Also
5751         --------
5752         Dataset.resample
5753         pandas.Series.resample
5754         pandas.DataFrame.resample
5755 
5756         References
5757         ----------
5758         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
5759         """
5760         from .resample import DataArrayResample
5761 
5762         return self._resample(
5763             resample_cls=DataArrayResample,
5764             indexer=indexer,
5765             skipna=skipna,
5766             closed=closed,
5767             label=label,
5768             base=base,
5769             keep_attrs=keep_attrs,
5770             loffset=loffset,
5771             restore_coord_dims=restore_coord_dims,
5772             **indexer_kwargs,
5773         )
5774 
5775     # this needs to be at the end, or mypy will confuse with `str`
5776     # https://mypy.readthedocs.io/en/latest/common_issues.html#dealing-with-conflicting-names
5777     str = utils.UncachedAccessor(StringAccessor["DataArray"])

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>rename_vars</method>
<code>
3651     def rename_vars(
3652         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3653     ) -> T_Dataset:
3654         """Returns a new object with renamed variables including coordinates
3655 
3656         Parameters
3657         ----------
3658         name_dict : dict-like, optional
3659             Dictionary whose keys are current variable or coordinate names and
3660             whose values are the desired names.
3661         **names : optional
3662             Keyword form of ``name_dict``.
3663             One of name_dict or names must be provided.
3664 
3665         Returns
3666         -------
3667         renamed : Dataset
3668             Dataset with renamed variables including coordinates
3669 
3670         See Also
3671         --------
3672         Dataset.swap_dims
3673         Dataset.rename
3674         Dataset.rename_dims
3675         DataArray.rename
3676         """
3677         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3678         for k in name_dict:
3679             if k not in self:
3680                 raise ValueError(
3681                     f"cannot rename {k!r} because it is not a "
3682                     "variable or coordinate in this dataset"
3683                 )
3684         variables, coord_names, dims, indexes = self._rename_all(
3685             name_dict=name_dict, dims_dict={}
3686         )
3687         return self._replace(variables, coord_names, dims=dims, indexes=indexes)

</code><intended_behavior>Similar to the `DataArray.rename` method, the `rename_vars` method in the `Dataset` class should include logic to detect when a variable (coordinate) renaming action results in the variable becoming a dimension coordinate. Upon such an event, it should ensure an index is created or updated for the new dimension coordinate. The method must maintain its current functionality while extending it to manage indexes for newly dimensioned coordinates efficiently.</intended_behavior>, <file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
430 class Dataset(
431     DataWithCoords, DatasetReductions, DatasetArithmetic, Mapping[Hashable, "DataArray"]
432 ):
433     """A multi-dimensional, in memory, array database.
434 
435     A dataset resembles an in-memory representation of a NetCDF file,
436     and consists of variables, coordinates and attributes which
437     together form a self describing dataset.
438 
439     Dataset implements the mapping interface with keys given by variable
440     names and values given by DataArray objects for each variable name.
441 
442     One dimensional variables with name equal to their dimension are
443     index coordinates used for label based indexing.
444 
445     To load data from a file or file-like object, use the `open_dataset`
446     function.
447 
448     Parameters
449     ----------
450     data_vars : dict-like, optional
451         A mapping from variable names to :py:class:`~xarray.DataArray`
452         objects, :py:class:`~xarray.Variable` objects or to tuples of
453         the form ``(dims, data[, attrs])`` which can be used as
454         arguments to create a new ``Variable``. Each dimension must
455         have the same length in all variables in which it appears.
456 
457         The following notations are accepted:
458 
459         - mapping {var name: DataArray}
460         - mapping {var name: Variable}
461         - mapping {var name: (dimension name, array-like)}
462         - mapping {var name: (tuple of dimension names, array-like)}
463         - mapping {dimension name: array-like}
464           (it will be automatically moved to coords, see below)
465 
466         Each dimension must have the same length in all variables in
467         which it appears.
468     coords : dict-like, optional
469         Another mapping in similar form as the `data_vars` argument,
470         except the each item is saved on the dataset as a "coordinate".
471         These variables have an associated meaning: they describe
472         constant/fixed/independent quantities, unlike the
473         varying/measured/dependent quantities that belong in
474         `variables`. Coordinates values may be given by 1-dimensional
475         arrays or scalars, in which case `dims` do not need to be
476         supplied: 1D arrays will be assumed to give index values along
477         the dimension with the same name.
478 
479         The following notations are accepted:
480 
481         - mapping {coord name: DataArray}
482         - mapping {coord name: Variable}
483         - mapping {coord name: (dimension name, array-like)}
484         - mapping {coord name: (tuple of dimension names, array-like)}
485         - mapping {dimension name: array-like}
486           (the dimension name is implicitly set to be the same as the
487           coord name)
488 
489         The last notation implies that the coord name is the same as
490         the dimension name.
491 
492     attrs : dict-like, optional
493         Global attributes to save on this dataset.
494 
495     Examples
496     --------
497     Create data:
498 
499     >>> np.random.seed(0)
500     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
501     >>> precipitation = 10 * np.random.rand(2, 2, 3)
502     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
503     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
504     >>> time = pd.date_range("2014-09-06", periods=3)
505     >>> reference_time = pd.Timestamp("2014-09-05")
506 
507     Initialize a dataset with multiple dimensions:
508 
509     >>> ds = xr.Dataset(
510     ...     data_vars=dict(
511     ...         temperature=(["x", "y", "time"], temperature),
512     ...         precipitation=(["x", "y", "time"], precipitation),
513     ...     ),
514     ...     coords=dict(
515     ...         lon=(["x", "y"], lon),
516     ...         lat=(["x", "y"], lat),
517     ...         time=time,
518     ...         reference_time=reference_time,
519     ...     ),
520     ...     attrs=dict(description="Weather related data."),
521     ... )
522     >>> ds
523     <xarray.Dataset>
524     Dimensions:         (x: 2, y: 2, time: 3)
525     Coordinates:
526         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
527         lat             (x, y) float64 42.25 42.21 42.63 42.59
528       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
529         reference_time  datetime64[ns] 2014-09-05
530     Dimensions without coordinates: x, y
531     Data variables:
532         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
533         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
534     Attributes:
535         description:  Weather related data.
536 
537     Find out where the coldest temperature was and what values the
538     other variables had:
539 
540     >>> ds.isel(ds.temperature.argmin(...))
541     <xarray.Dataset>
542     Dimensions:         ()
543     Coordinates:
544         lon             float64 -99.32
545         lat             float64 42.21
546         time            datetime64[ns] 2014-09-08
547         reference_time  datetime64[ns] 2014-09-05
548     Data variables:
549         temperature     float64 7.182
550         precipitation   float64 8.326
551     Attributes:
552         description:  Weather related data.
553     """
554 
555     _attrs: dict[Hashable, Any] | None
556     _cache: dict[str, Any]
557     _coord_names: set[Hashable]
558     _dims: dict[Hashable, int]
559     _encoding: dict[Hashable, Any] | None
560     _close: Callable[[], None] | None
561     _indexes: dict[Hashable, Index]
562     _variables: dict[Hashable, Variable]
563 
564     __slots__ = (
565         "_attrs",
566         "_cache",
567         "_coord_names",
568         "_dims",
569         "_encoding",
570         "_close",
571         "_indexes",
572         "_variables",
573         "__weakref__",
574     )
575 
576     def __init__(
577         self,
578         # could make a VariableArgs to use more generally, and refine these
579         # categories
580         data_vars: Mapping[Any, Any] | None = None,
581         coords: Mapping[Any, Any] | None = None,
582         attrs: Mapping[Any, Any] | None = None,
583     ) -> None:
584         # TODO(shoyer): expose indexes as a public argument in __init__
585 
586         if data_vars is None:
587             data_vars = {}
588         if coords is None:
589             coords = {}
590 
591         both_data_and_coords = set(data_vars) & set(coords)
592         if both_data_and_coords:
593             raise ValueError(
594                 f"variables {both_data_and_coords!r} are found in both data_vars and coords"
595             )
596 
597         if isinstance(coords, Dataset):
598             coords = coords.variables
599 
600         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
601             data_vars, coords, compat="broadcast_equals"
602         )
603 
604         self._attrs = dict(attrs) if attrs is not None else None
605         self._close = None
606         self._encoding = None
607         self._variables = variables
608         self._coord_names = coord_names
609         self._dims = dims
610         self._indexes = indexes
611 
612     @classmethod
613     def load_store(cls: type[T_Dataset], store, decoder=None) -> T_Dataset:
614         """Create a new dataset from the contents of a backends.*DataStore
615         object
616         """
617         variables, attributes = store.load()
618         if decoder:
619             variables, attributes = decoder(variables, attributes)
620         obj = cls(variables, attrs=attributes)
621         obj.set_close(store.close)
622         return obj
623 
624     @property
625     def variables(self) -> Frozen[Hashable, Variable]:
626         """Low level interface to Dataset contents as dict of Variable objects.
627 
628         This ordered dictionary is frozen to prevent mutation that could
629         violate Dataset invariants. It contains all variable objects
630         constituting the Dataset, including both data variables and
631         coordinates.
632         """
633         return Frozen(self._variables)
634 
635     @property
636     def attrs(self) -> dict[Hashable, Any]:
637         """Dictionary of global attributes on this dataset"""
638         if self._attrs is None:
639             self._attrs = {}
640         return self._attrs
641 
642     @attrs.setter
643     def attrs(self, value: Mapping[Any, Any]) -> None:
644         self._attrs = dict(value)
645 
646     @property
647     def encoding(self) -> dict[Hashable, Any]:
648         """Dictionary of global encoding attributes on this dataset"""
649         if self._encoding is None:
650             self._encoding = {}
651         return self._encoding
652 
653     @encoding.setter
654     def encoding(self, value: Mapping[Any, Any]) -> None:
655         self._encoding = dict(value)
656 
657     @property
658     def dims(self) -> Frozen[Hashable, int]:
659         """Mapping from dimension names to lengths.
660 
661         Cannot be modified directly, but is updated when adding new variables.
662 
663         Note that type of this object differs from `DataArray.dims`.
664         See `Dataset.sizes` and `DataArray.sizes` for consistently named
665         properties.
666 
667         See Also
668         --------
669         Dataset.sizes
670         DataArray.dims
671         """
672         return Frozen(self._dims)
673 
674     @property
675     def sizes(self) -> Frozen[Hashable, int]:
676         """Mapping from dimension names to lengths.
677 
678         Cannot be modified directly, but is updated when adding new variables.
679 
680         This is an alias for `Dataset.dims` provided for the benefit of
681         consistency with `DataArray.sizes`.
682 
683         See Also
684         --------
685         DataArray.sizes
686         """
687         return self.dims
688 
689     @property
690     def dtypes(self) -> Frozen[Hashable, np.dtype]:
691         """Mapping from data variable names to dtypes.
692 
693         Cannot be modified directly, but is updated when adding new variables.
694 
695         See Also
696         --------
697         DataArray.dtype
698         """
699         return Frozen(
700             {
701                 n: v.dtype
702                 for n, v in self._variables.items()
703                 if n not in self._coord_names
704             }
705         )
706 
707     def load(self: T_Dataset, **kwargs) -> T_Dataset:
708         """Manually trigger loading and/or computation of this dataset's data
709         from disk or a remote source into memory and return this dataset.
710         Unlike compute, the original dataset is modified and returned.
711 
712         Normally, it should not be necessary to call this method in user code,
713         because all xarray functions should either work on deferred data or
714         load data automatically. However, this method can be necessary when
715         working with many file objects on disk.
716 
717         Parameters
718         ----------
719         **kwargs : dict
720             Additional keyword arguments passed on to ``dask.compute``.
721 
722         See Also
723         --------
724         dask.compute
725         """
726         # access .data to coerce everything to numpy or dask arrays
727         lazy_data = {
728             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
729         }
730         if lazy_data:
731             import dask.array as da
732 
733             # evaluate all the dask arrays simultaneously
734             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
735 
736             for k, data in zip(lazy_data, evaluated_data):
737                 self.variables[k].data = data
738 
739         # load everything else sequentially
740         for k, v in self.variables.items():
741             if k not in lazy_data:
742                 v.load()
743 
744         return self
745 
746     def __dask_tokenize__(self):
747         from dask.base import normalize_token
748 
749         return normalize_token(
750             (type(self), self._variables, self._coord_names, self._attrs)
751         )
752 
753     def __dask_graph__(self):
754         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
755         graphs = {k: v for k, v in graphs.items() if v is not None}
756         if not graphs:
757             return None
758         else:
759             try:
760                 from dask.highlevelgraph import HighLevelGraph
761 
762                 return HighLevelGraph.merge(*graphs.values())
763             except ImportError:
764                 from dask import sharedict
765 
766                 return sharedict.merge(*graphs.values())
767 
768     def __dask_keys__(self):
769         import dask
770 
771         return [
772             v.__dask_keys__()
773             for v in self.variables.values()
774             if dask.is_dask_collection(v)
775         ]
776 
777     def __dask_layers__(self):
778         import dask
779 
780         return sum(
781             (
782                 v.__dask_layers__()
783                 for v in self.variables.values()
784                 if dask.is_dask_collection(v)
785             ),
786             (),
787         )
788 
789     @property
790     def __dask_optimize__(self):
791         import dask.array as da
792 
793         return da.Array.__dask_optimize__
794 
795     @property
796     def __dask_scheduler__(self):
797         import dask.array as da
798 
799         return da.Array.__dask_scheduler__
800 
801     def __dask_postcompute__(self):
802         return self._dask_postcompute, ()
803 
804     def __dask_postpersist__(self):
805         return self._dask_postpersist, ()
806 
807     def _dask_postcompute(self: T_Dataset, results: Iterable[Variable]) -> T_Dataset:
808         import dask
809 
810         variables = {}
811         results_iter = iter(results)
812 
813         for k, v in self._variables.items():
814             if dask.is_dask_collection(v):
815                 rebuild, args = v.__dask_postcompute__()
816                 v = rebuild(next(results_iter), *args)
817             variables[k] = v
818 
819         return type(self)._construct_direct(
820             variables,
821             self._coord_names,
822             self._dims,
823             self._attrs,
824             self._indexes,
825             self._encoding,
826             self._close,
827         )
828 
829     def _dask_postpersist(
830         self: T_Dataset, dsk: Mapping, *, rename: Mapping[str, str] = None
831     ) -> T_Dataset:
832         from dask import is_dask_collection
833         from dask.highlevelgraph import HighLevelGraph
834         from dask.optimization import cull
835 
836         variables = {}
837 
838         for k, v in self._variables.items():
839             if not is_dask_collection(v):
840                 variables[k] = v
841                 continue
842 
843             if isinstance(dsk, HighLevelGraph):
844                 # dask >= 2021.3
845                 # __dask_postpersist__() was called by dask.highlevelgraph.
846                 # Don't use dsk.cull(), as we need to prevent partial layers:
847                 # https://github.com/dask/dask/issues/7137
848                 layers = v.__dask_layers__()
849                 if rename:
850                     layers = [rename.get(k, k) for k in layers]
851                 dsk2 = dsk.cull_layers(layers)
852             elif rename:  # pragma: nocover
853                 # At the moment of writing, this is only for forward compatibility.
854                 # replace_name_in_key requires dask >= 2021.3.
855                 from dask.base import flatten, replace_name_in_key
856 
857                 keys = [
858                     replace_name_in_key(k, rename) for k in flatten(v.__dask_keys__())
859                 ]
860                 dsk2, _ = cull(dsk, keys)
861             else:
862                 # __dask_postpersist__() was called by dask.optimize or dask.persist
863                 dsk2, _ = cull(dsk, v.__dask_keys__())
864 
865             rebuild, args = v.__dask_postpersist__()
866             # rename was added in dask 2021.3
867             kwargs = {"rename": rename} if rename else {}
868             variables[k] = rebuild(dsk2, *args, **kwargs)
869 
870         return type(self)._construct_direct(
871             variables,
872             self._coord_names,
873             self._dims,
874             self._attrs,
875             self._indexes,
876             self._encoding,
877             self._close,
878         )
879 
880     def compute(self: T_Dataset, **kwargs) -> T_Dataset:
881         """Manually trigger loading and/or computation of this dataset's data
882         from disk or a remote source into memory and return a new dataset.
883         Unlike load, the original dataset is left unaltered.
884 
885         Normally, it should not be necessary to call this method in user code,
886         because all xarray functions should either work on deferred data or
887         load data automatically. However, this method can be necessary when
888         working with many file objects on disk.
889 
890         Parameters
891         ----------
892         **kwargs : dict
893             Additional keyword arguments passed on to ``dask.compute``.
894 
895         See Also
896         --------
897         dask.compute
898         """
899         new = self.copy(deep=False)
900         return new.load(**kwargs)
901 
902     def _persist_inplace(self: T_Dataset, **kwargs) -> T_Dataset:
903         """Persist all Dask arrays in memory"""
904         # access .data to coerce everything to numpy or dask arrays
905         lazy_data = {
906             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
907         }
908         if lazy_data:
909             import dask
910 
911             # evaluate all the dask arrays simultaneously
912             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
913 
914             for k, data in zip(lazy_data, evaluated_data):
915                 self.variables[k].data = data
916 
917         return self
918 
919     def persist(self: T_Dataset, **kwargs) -> T_Dataset:
920         """Trigger computation, keeping data as dask arrays
921 
922         This operation can be used to trigger computation on underlying dask
923         arrays, similar to ``.compute()`` or ``.load()``.  However this
924         operation keeps the data as dask arrays. This is particularly useful
925         when using the dask.distributed scheduler and you want to load a large
926         amount of data into distributed memory.
927 
928         Parameters
929         ----------
930         **kwargs : dict
931             Additional keyword arguments passed on to ``dask.persist``.
932 
933         See Also
934         --------
935         dask.persist
936         """
937         new = self.copy(deep=False)
938         return new._persist_inplace(**kwargs)
939 
940     @classmethod
941     def _construct_direct(
942         cls: type[T_Dataset],
943         variables: dict[Any, Variable],
944         coord_names: set[Hashable],
945         dims: dict[Any, int] | None = None,
946         attrs: dict | None = None,
947         indexes: dict[Any, Index] | None = None,
948         encoding: dict | None = None,
949         close: Callable[[], None] | None = None,
950     ) -> T_Dataset:
951         """Shortcut around __init__ for internal use when we want to skip
952         costly validation
953         """
954         if dims is None:
955             dims = calculate_dimensions(variables)
956         if indexes is None:
957             indexes = {}
958         obj = object.__new__(cls)
959         obj._variables = variables
960         obj._coord_names = coord_names
961         obj._dims = dims
962         obj._indexes = indexes
963         obj._attrs = attrs
964         obj._close = close
965         obj._encoding = encoding
966         return obj
967 
968     def _replace(
969         self: T_Dataset,
970         variables: dict[Hashable, Variable] = None,
971         coord_names: set[Hashable] | None = None,
972         dims: dict[Any, int] | None = None,
973         attrs: dict[Hashable, Any] | None | Default = _default,
974         indexes: dict[Hashable, Index] | None = None,
975         encoding: dict | None | Default = _default,
976         inplace: bool = False,
977     ) -> T_Dataset:
978         """Fastpath constructor for internal use.
979 
980         Returns an object with optionally with replaced attributes.
981 
982         Explicitly passed arguments are *not* copied when placed on the new
983         dataset. It is up to the caller to ensure that they have the right type
984         and are not used elsewhere.
985         """
986         if inplace:
987             if variables is not None:
988                 self._variables = variables
989             if coord_names is not None:
990                 self._coord_names = coord_names
991             if dims is not None:
992                 self._dims = dims
993             if attrs is not _default:
994                 self._attrs = attrs
995             if indexes is not None:
996                 self._indexes = indexes
997             if encoding is not _default:
998                 self._encoding = encoding
999             obj = self
1000         else:
1001             if variables is None:
1002                 variables = self._variables.copy()
1003             if coord_names is None:
1004                 coord_names = self._coord_names.copy()
1005             if dims is None:
1006                 dims = self._dims.copy()
1007             if attrs is _default:
1008                 attrs = copy.copy(self._attrs)
1009             if indexes is None:
1010                 indexes = self._indexes.copy()
1011             if encoding is _default:
1012                 encoding = copy.copy(self._encoding)
1013             obj = self._construct_direct(
1014                 variables, coord_names, dims, attrs, indexes, encoding
1015             )
1016         return obj
1017 
1018     def _replace_with_new_dims(
1019         self: T_Dataset,
1020         variables: dict[Hashable, Variable],
1021         coord_names: set | None = None,
1022         attrs: dict[Hashable, Any] | None | Default = _default,
1023         indexes: dict[Hashable, Index] | None = None,
1024         inplace: bool = False,
1025     ) -> T_Dataset:
1026         """Replace variables with recalculated dimensions."""
1027         dims = calculate_dimensions(variables)
1028         return self._replace(
1029             variables, coord_names, dims, attrs, indexes, inplace=inplace
1030         )
1031 
1032     def _replace_vars_and_dims(
1033         self: T_Dataset,
1034         variables: dict[Hashable, Variable],
1035         coord_names: set | None = None,
1036         dims: dict[Hashable, int] | None = None,
1037         attrs: dict[Hashable, Any] | None | Default = _default,
1038         inplace: bool = False,
1039     ) -> T_Dataset:
1040         """Deprecated version of _replace_with_new_dims().
1041 
1042         Unlike _replace_with_new_dims(), this method always recalculates
1043         indexes from variables.
1044         """
1045         if dims is None:
1046             dims = calculate_dimensions(variables)
1047         return self._replace(
1048             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1049         )
1050 
1051     def _overwrite_indexes(
1052         self: T_Dataset,
1053         indexes: Mapping[Hashable, Index],
1054         variables: Mapping[Hashable, Variable] | None = None,
1055         drop_variables: list[Hashable] | None = None,
1056         drop_indexes: list[Hashable] | None = None,
1057         rename_dims: Mapping[Hashable, Hashable] | None = None,
1058     ) -> T_Dataset:
1059         """Maybe replace indexes.
1060 
1061         This function may do a lot more depending on index query
1062         results.
1063 
1064         """
1065         if not indexes:
1066             return self
1067 
1068         if variables is None:
1069             variables = {}
1070         if drop_variables is None:
1071             drop_variables = []
1072         if drop_indexes is None:
1073             drop_indexes = []
1074 
1075         new_variables = self._variables.copy()
1076         new_coord_names = self._coord_names.copy()
1077         new_indexes = dict(self._indexes)
1078 
1079         index_variables = {}
1080         no_index_variables = {}
1081         for name, var in variables.items():
1082             old_var = self._variables.get(name)
1083             if old_var is not None:
1084                 var.attrs.update(old_var.attrs)
1085                 var.encoding.update(old_var.encoding)
1086             if name in indexes:
1087                 index_variables[name] = var
1088             else:
1089                 no_index_variables[name] = var
1090 
1091         for name in indexes:
1092             new_indexes[name] = indexes[name]
1093 
1094         for name, var in index_variables.items():
1095             new_coord_names.add(name)
1096             new_variables[name] = var
1097 
1098         # append no-index variables at the end
1099         for k in no_index_variables:
1100             new_variables.pop(k)
1101         new_variables.update(no_index_variables)
1102 
1103         for name in drop_indexes:
1104             new_indexes.pop(name)
1105 
1106         for name in drop_variables:
1107             new_variables.pop(name)
1108             new_indexes.pop(name, None)
1109             new_coord_names.remove(name)
1110 
1111         replaced = self._replace(
1112             variables=new_variables, coord_names=new_coord_names, indexes=new_indexes
1113         )
1114 
1115         if rename_dims:
1116             # skip rename indexes: they should already have the right name(s)
1117             dims = replaced._rename_dims(rename_dims)
1118             new_variables, new_coord_names = replaced._rename_vars({}, rename_dims)
1119             return replaced._replace(
1120                 variables=new_variables, coord_names=new_coord_names, dims=dims
1121             )
1122         else:
1123             return replaced
1124 
1125     def copy(
1126         self: T_Dataset, deep: bool = False, data: Mapping | None = None
1127     ) -> T_Dataset:
1128         """Returns a copy of this dataset.
1129 
1130         If `deep=True`, a deep copy is made of each of the component variables.
1131         Otherwise, a shallow copy of each of the component variable is made, so
1132         that the underlying memory region of the new dataset is the same as in
1133         the original dataset.
1134 
1135         Use `data` to create a new object with the same structure as
1136         original but entirely new data.
1137 
1138         Parameters
1139         ----------
1140         deep : bool, default: False
1141             Whether each component variable is loaded into memory and copied onto
1142             the new object. Default is False.
1143         data : dict-like or None, optional
1144             Data to use in the new object. Each item in `data` must have same
1145             shape as corresponding data variable in original. When `data` is
1146             used, `deep` is ignored for the data variables and only used for
1147             coords.
1148 
1149         Returns
1150         -------
1151         object : Dataset
1152             New object with dimensions, attributes, coordinates, name, encoding,
1153             and optionally data copied from original.
1154 
1155         Examples
1156         --------
1157         Shallow copy versus deep copy
1158 
1159         >>> da = xr.DataArray(np.random.randn(2, 3))
1160         >>> ds = xr.Dataset(
1161         ...     {"foo": da, "bar": ("x", [-1, 2])},
1162         ...     coords={"x": ["one", "two"]},
1163         ... )
1164         >>> ds.copy()
1165         <xarray.Dataset>
1166         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1167         Coordinates:
1168           * x        (x) <U3 'one' 'two'
1169         Dimensions without coordinates: dim_0, dim_1
1170         Data variables:
1171             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1172             bar      (x) int64 -1 2
1173 
1174         >>> ds_0 = ds.copy(deep=False)
1175         >>> ds_0["foo"][0, 0] = 7
1176         >>> ds_0
1177         <xarray.Dataset>
1178         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1179         Coordinates:
1180           * x        (x) <U3 'one' 'two'
1181         Dimensions without coordinates: dim_0, dim_1
1182         Data variables:
1183             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1184             bar      (x) int64 -1 2
1185 
1186         >>> ds
1187         <xarray.Dataset>
1188         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1189         Coordinates:
1190           * x        (x) <U3 'one' 'two'
1191         Dimensions without coordinates: dim_0, dim_1
1192         Data variables:
1193             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1194             bar      (x) int64 -1 2
1195 
1196         Changing the data using the ``data`` argument maintains the
1197         structure of the original object, but with the new data. Original
1198         object is unaffected.
1199 
1200         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1201         <xarray.Dataset>
1202         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1203         Coordinates:
1204           * x        (x) <U3 'one' 'two'
1205         Dimensions without coordinates: dim_0, dim_1
1206         Data variables:
1207             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1208             bar      (x) <U1 'a' 'b'
1209 
1210         >>> ds
1211         <xarray.Dataset>
1212         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1213         Coordinates:
1214           * x        (x) <U3 'one' 'two'
1215         Dimensions without coordinates: dim_0, dim_1
1216         Data variables:
1217             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1218             bar      (x) int64 -1 2
1219 
1220         See Also
1221         --------
1222         pandas.DataFrame.copy
1223         """
1224         if data is None:
1225             data = {}
1226         elif not utils.is_dict_like(data):
1227             raise ValueError("Data must be dict-like")
1228 
1229         if data:
1230             var_keys = set(self.data_vars.keys())
1231             data_keys = set(data.keys())
1232             keys_not_in_vars = data_keys - var_keys
1233             if keys_not_in_vars:
1234                 raise ValueError(
1235                     "Data must only contain variables in original "
1236                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1237                 )
1238             keys_missing_from_data = var_keys - data_keys
1239             if keys_missing_from_data:
1240                 raise ValueError(
1241                     "Data must contain all variables in original "
1242                     "dataset. Data is missing {}".format(keys_missing_from_data)
1243                 )
1244 
1245         indexes, index_vars = self.xindexes.copy_indexes(deep=deep)
1246 
1247         variables = {}
1248         for k, v in self._variables.items():
1249             if k in index_vars:
1250                 variables[k] = index_vars[k]
1251             else:
1252                 variables[k] = v.copy(deep=deep, data=data.get(k))
1253 
1254         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1255 
1256         return self._replace(variables, indexes=indexes, attrs=attrs)
1257 
1258     def as_numpy(self: T_Dataset) -> T_Dataset:
1259         """
1260         Coerces wrapped data and coordinates into numpy arrays, returning a Dataset.
1261 
1262         See also
1263         --------
1264         DataArray.as_numpy
1265         DataArray.to_numpy : Returns only the data as a numpy.ndarray object.
1266         """
1267         numpy_variables = {k: v.as_numpy() for k, v in self.variables.items()}
1268         return self._replace(variables=numpy_variables)
1269 
1270     def _copy_listed(self: T_Dataset, names: Iterable[Hashable]) -> T_Dataset:
1271         """Create a new Dataset with the listed variables from this dataset and
1272         the all relevant coordinates. Skips all validation.
1273         """
1274         variables: dict[Hashable, Variable] = {}
1275         coord_names = set()
1276         indexes: dict[Hashable, Index] = {}
1277 
1278         for name in names:
1279             try:
1280                 variables[name] = self._variables[name]
1281             except KeyError:
1282                 ref_name, var_name, var = _get_virtual_variable(
1283                     self._variables, name, self.dims
1284                 )
1285                 variables[var_name] = var
1286                 if ref_name in self._coord_names or ref_name in self.dims:
1287                     coord_names.add(var_name)
1288                 if (var_name,) == var.dims:
1289                     index, index_vars = create_default_index_implicit(var, names)
1290                     indexes.update({k: index for k in index_vars})
1291                     variables.update(index_vars)
1292                     coord_names.update(index_vars)
1293 
1294         needed_dims: OrderedSet[Hashable] = OrderedSet()
1295         for v in variables.values():
1296             needed_dims.update(v.dims)
1297 
1298         dims = {k: self.dims[k] for k in needed_dims}
1299 
1300         # preserves ordering of coordinates
1301         for k in self._variables:
1302             if k not in self._coord_names:
1303                 continue
1304 
1305             if set(self.variables[k].dims) <= needed_dims:
1306                 variables[k] = self._variables[k]
1307                 coord_names.add(k)
1308 
1309         indexes.update(filter_indexes_from_coords(self._indexes, coord_names))
1310 
1311         return self._replace(variables, coord_names, dims, indexes=indexes)
1312 
1313     def _construct_dataarray(self, name: Hashable) -> DataArray:
1314         """Construct a DataArray by indexing this dataset"""
1315         from .dataarray import DataArray
1316 
1317         try:
1318             variable = self._variables[name]
1319         except KeyError:
1320             _, name, variable = _get_virtual_variable(self._variables, name, self.dims)
1321 
1322         needed_dims = set(variable.dims)
1323 
1324         coords: dict[Hashable, Variable] = {}
1325         # preserve ordering
1326         for k in self._variables:
1327             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1328                 coords[k] = self.variables[k]
1329 
1330         indexes = filter_indexes_from_coords(self._indexes, set(coords))
1331 
1332         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1333 
1334     def __copy__(self: T_Dataset) -> T_Dataset:
1335         return self.copy(deep=False)
1336 
1337     def __deepcopy__(self: T_Dataset, memo=None) -> T_Dataset:
1338         # memo does nothing but is required for compatibility with
1339         # copy.deepcopy
1340         return self.copy(deep=True)
1341 
1342     @property
1343     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1344         """Places to look-up items for attribute-style access"""
1345         yield from self._item_sources
1346         yield self.attrs
1347 
1348     @property
1349     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1350         """Places to look-up items for key-completion"""
1351         yield self.data_vars
1352         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1353 
1354         # virtual coordinates
1355         yield HybridMappingProxy(keys=self.dims, mapping=self)
1356 
1357     def __contains__(self, key: object) -> bool:
1358         """The 'in' operator will return true or false depending on whether
1359         'key' is an array in the dataset or not.
1360         """
1361         return key in self._variables
1362 
1363     def __len__(self) -> int:
1364         return len(self.data_vars)
1365 
1366     def __bool__(self) -> bool:
1367         return bool(self.data_vars)
1368 
1369     def __iter__(self) -> Iterator[Hashable]:
1370         return iter(self.data_vars)
1371 
1372     def __array__(self, dtype=None):
1373         raise TypeError(
1374             "cannot directly convert an xarray.Dataset into a "
1375             "numpy array. Instead, create an xarray.DataArray "
1376             "first, either with indexing on the Dataset or by "
1377             "invoking the `to_array()` method."
1378         )
1379 
1380     @property
1381     def nbytes(self) -> int:
1382         """
1383         Total bytes consumed by the data arrays of all variables in this dataset.
1384 
1385         If the backend array for any variable does not include ``nbytes``, estimates
1386         the total bytes for that array based on the ``size`` and ``dtype``.
1387         """
1388         return sum(v.nbytes for v in self.variables.values())
1389 
1390     @property
1391     def loc(self: T_Dataset) -> _LocIndexer[T_Dataset]:
1392         """Attribute for location based indexing. Only supports __getitem__,
1393         and only when the key is a dict of the form {dim: labels}.
1394         """
1395         return _LocIndexer(self)
1396 
1397     @overload
1398     def __getitem__(self, key: Hashable) -> DataArray:
1399         ...
1400 
1401     # Mapping is Iterable
1402     @overload
1403     def __getitem__(self: T_Dataset, key: Iterable[Hashable]) -> T_Dataset:
1404         ...
1405 
1406     def __getitem__(
1407         self: T_Dataset, key: Mapping[Any, Any] | Hashable | Iterable[Hashable]
1408     ) -> T_Dataset | DataArray:
1409         """Access variables or coordinates of this dataset as a
1410         :py:class:`~xarray.DataArray` or a subset of variables or a indexed dataset.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**key)
1416         if utils.hashable(key):
1417             return self._construct_dataarray(key)
1418         if utils.iterable_of_hashable(key):
1419             return self._copy_listed(key)
1420         raise ValueError(f"Unsupported key-type {type(key)}")
1421 
1422     def __setitem__(
1423         self, key: Hashable | Iterable[Hashable] | Mapping, value: Any
1424     ) -> None:
1425         """Add an array to this dataset.
1426         Multiple arrays can be added at the same time, in which case each of
1427         the following operations is applied to the respective value.
1428 
1429         If key is dict-like, update all variables in the dataset
1430         one by one with the given value at the given location.
1431         If the given value is also a dataset, select corresponding variables
1432         in the given value and in the dataset to be changed.
1433 
1434         If value is a `
1435         from .dataarray import DataArray`, call its `select_vars()` method, rename it
1436         to `key` and merge the contents of the resulting dataset into this
1437         dataset.
1438 
1439         If value is a `Variable` object (or tuple of form
1440         ``(dims, data[, attrs])``), add it to this dataset as a new
1441         variable.
1442         """
1443         from .dataarray import DataArray
1444 
1445         if utils.is_dict_like(key):
1446             # check for consistency and convert value to dataset
1447             value = self._setitem_check(key, value)
1448             # loop over dataset variables and set new values
1449             processed = []
1450             for name, var in self.items():
1451                 try:
1452                     var[key] = value[name]
1453                     processed.append(name)
1454                 except Exception as e:
1455                     if processed:
1456                         raise RuntimeError(
1457                             "An error occurred while setting values of the"
1458                             f" variable '{name}'. The following variables have"
1459                             f" been successfully updated:\n{processed}"
1460                         ) from e
1461                     else:
1462                         raise e
1463 
1464         elif utils.hashable(key):
1465             if isinstance(value, Dataset):
1466                 raise TypeError(
1467                     "Cannot assign a Dataset to a single key - only a DataArray or Variable "
1468                     "object can be stored under a single key."
1469                 )
1470             self.update({key: value})
1471 
1472         elif utils.iterable_of_hashable(key):
1473             keylist = list(key)
1474             if len(keylist) == 0:
1475                 raise ValueError("Empty list of variables to be set")
1476             if len(keylist) == 1:
1477                 self.update({keylist[0]: value})
1478             else:
1479                 if len(keylist) != len(value):
1480                     raise ValueError(
1481                         f"Different lengths of variables to be set "
1482                         f"({len(keylist)}) and data used as input for "
1483                         f"setting ({len(value)})"
1484                     )
1485                 if isinstance(value, Dataset):
1486                     self.update(dict(zip(keylist, value.data_vars.values())))
1487                 elif isinstance(value, DataArray):
1488                     raise ValueError("Cannot assign single DataArray to multiple keys")
1489                 else:
1490                     self.update(dict(zip(keylist, value)))
1491 
1492         else:
1493             raise ValueError(f"Unsupported key-type {type(key)}")
1494 
1495     def _setitem_check(self, key, value):
1496         """Consistency check for __setitem__
1497 
1498         When assigning values to a subset of a Dataset, do consistency check beforehand
1499         to avoid leaving the dataset in a partially updated state when an error occurs.
1500         """
1501         from .alignment import align
1502         from .dataarray import DataArray
1503 
1504         if isinstance(value, Dataset):
1505             missing_vars = [
1506                 name for name in value.data_vars if name not in self.data_vars
1507             ]
1508             if missing_vars:
1509                 raise ValueError(
1510                     f"Variables {missing_vars} in new values"
1511                     f" not available in original dataset:\n{self}"
1512                 )
1513         elif not any([isinstance(value, t) for t in [DataArray, Number, str]]):
1514             raise TypeError(
1515                 "Dataset assignment only accepts DataArrays, Datasets, and scalars."
1516             )
1517 
1518         new_value = Dataset()
1519         for name, var in self.items():
1520             # test indexing
1521             try:
1522                 var_k = var[key]
1523             except Exception as e:
1524                 raise ValueError(
1525                     f"Variable '{name}': indexer {key} not available"
1526                 ) from e
1527 
1528             if isinstance(value, Dataset):
1529                 val = value[name]
1530             else:
1531                 val = value
1532 
1533             if isinstance(val, DataArray):
1534                 # check consistency of dimensions
1535                 for dim in val.dims:
1536                     if dim not in var_k.dims:
1537                         raise KeyError(
1538                             f"Variable '{name}': dimension '{dim}' appears in new values "
1539                             f"but not in the indexed original data"
1540                         )
1541                 dims = tuple(dim for dim in var_k.dims if dim in val.dims)
1542                 if dims != val.dims:
1543                     raise ValueError(
1544                         f"Variable '{name}': dimension order differs between"
1545                         f" original and new data:\n{dims}\nvs.\n{val.dims}"
1546                     )
1547             else:
1548                 val = np.array(val)
1549 
1550             # type conversion
1551             new_value[name] = val.astype(var_k.dtype, copy=False)
1552 
1553         # check consistency of dimension sizes and dimension coordinates
1554         if isinstance(value, DataArray) or isinstance(value, Dataset):
1555             align(self[key], value, join="exact", copy=False)
1556 
1557         return new_value
1558 
1559     def __delitem__(self, key: Hashable) -> None:
1560         """Remove a variable from this dataset."""
1561         assert_no_index_corrupted(self.xindexes, {key})
1562 
1563         if key in self._indexes:
1564             del self._indexes[key]
1565         del self._variables[key]
1566         self._coord_names.discard(key)
1567         self._dims = calculate_dimensions(self._variables)
1568 
1569     # mutable objects should not be hashable
1570     # https://github.com/python/mypy/issues/4266
1571     __hash__ = None  # type: ignore[assignment]
1572 
1573     def _all_compat(self, other: Dataset, compat_str: str) -> bool:
1574         """Helper function for equals and identical"""
1575 
1576         # some stores (e.g., scipy) do not seem to preserve order, so don't
1577         # require matching order for equality
1578         def compat(x: Variable, y: Variable) -> bool:
1579             return getattr(x, compat_str)(y)
1580 
1581         return self._coord_names == other._coord_names and utils.dict_equiv(
1582             self._variables, other._variables, compat=compat
1583         )
1584 
1585     def broadcast_equals(self, other: Dataset) -> bool:
1586         """Two Datasets are broadcast equal if they are equal after
1587         broadcasting all variables against each other.
1588 
1589         For example, variables that are scalar in one dataset but non-scalar in
1590         the other dataset can still be broadcast equal if the the non-scalar
1591         variable is a constant.
1592 
1593         See Also
1594         --------
1595         Dataset.equals
1596         Dataset.identical
1597         """
1598         try:
1599             return self._all_compat(other, "broadcast_equals")
1600         except (TypeError, AttributeError):
1601             return False
1602 
1603     def equals(self, other: Dataset) -> bool:
1604         """Two Datasets are equal if they have matching variables and
1605         coordinates, all of which are equal.
1606 
1607         Datasets can still be equal (like pandas objects) if they have NaN
1608         values in the same locations.
1609 
1610         This method is necessary because `v1 == v2` for ``Dataset``
1611         does element-wise comparisons (like numpy.ndarrays).
1612 
1613         See Also
1614         --------
1615         Dataset.broadcast_equals
1616         Dataset.identical
1617         """
1618         try:
1619             return self._all_compat(other, "equals")
1620         except (TypeError, AttributeError):
1621             return False
1622 
1623     def identical(self, other: Dataset) -> bool:
1624         """Like equals, but also checks all dataset attributes and the
1625         attributes on all variables and coordinates.
1626 
1627         See Also
1628         --------
1629         Dataset.broadcast_equals
1630         Dataset.equals
1631         """
1632         try:
1633             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1634                 other, "identical"
1635             )
1636         except (TypeError, AttributeError):
1637             return False
1638 
1639     @property
1640     def indexes(self) -> Indexes[pd.Index]:
1641         """Mapping of pandas.Index objects used for label based indexing.
1642 
1643         Raises an error if this Dataset has indexes that cannot be coerced
1644         to pandas.Index objects.
1645 
1646         See Also
1647         --------
1648         Dataset.xindexes
1649 
1650         """
1651         return self.xindexes.to_pandas_indexes()
1652 
1653     @property
1654     def xindexes(self) -> Indexes[Index]:
1655         """Mapping of xarray Index objects used for label based indexing."""
1656         return Indexes(self._indexes, {k: self._variables[k] for k in self._indexes})
1657 
1658     @property
1659     def coords(self) -> DatasetCoordinates:
1660         """Dictionary of xarray.DataArray objects corresponding to coordinate
1661         variables
1662         """
1663         return DatasetCoordinates(self)
1664 
1665     @property
1666     def data_vars(self) -> DataVariables:
1667         """Dictionary of DataArray objects corresponding to data variables"""
1668         return DataVariables(self)
1669 
1670     def set_coords(self: T_Dataset, names: Hashable | Iterable[Hashable]) -> T_Dataset:
1671         """Given names of one or more variables, set them as coordinates
1672 
1673         Parameters
1674         ----------
1675         names : hashable or iterable of hashable
1676             Name(s) of variables in this dataset to convert into coordinates.
1677 
1678         Returns
1679         -------
1680         Dataset
1681 
1682         See Also
1683         --------
1684         Dataset.swap_dims
1685         """
1686         # TODO: allow inserting new coordinates with this method, like
1687         # DataFrame.set_index?
1688         # nb. check in self._variables, not self.data_vars to insure that the
1689         # operation is idempotent
1690         if isinstance(names, str) or not isinstance(names, Iterable):
1691             names = [names]
1692         else:
1693             names = list(names)
1694         self._assert_all_in_dataset(names)
1695         obj = self.copy()
1696         obj._coord_names.update(names)
1697         return obj
1698 
1699     def reset_coords(
1700         self: T_Dataset,
1701         names: Hashable | Iterable[Hashable] | None = None,
1702         drop: bool = False,
1703     ) -> T_Dataset:
1704         """Given names of coordinates, reset them to become variables
1705 
1706         Parameters
1707         ----------
1708         names : hashable or iterable of hashable, optional
1709             Name(s) of non-index coordinates in this dataset to reset into
1710             variables. By default, all non-index coordinates are reset.
1711         drop : bool, default: False
1712             If True, remove coordinates instead of converting them into
1713             variables.
1714 
1715         Returns
1716         -------
1717         Dataset
1718         """
1719         if names is None:
1720             names = self._coord_names - set(self._indexes)
1721         else:
1722             if isinstance(names, str) or not isinstance(names, Iterable):
1723                 names = [names]
1724             else:
1725                 names = list(names)
1726             self._assert_all_in_dataset(names)
1727             bad_coords = set(names) & set(self._indexes)
1728             if bad_coords:
1729                 raise ValueError(
1730                     f"cannot remove index coordinates with reset_coords: {bad_coords}"
1731                 )
1732         obj = self.copy()
1733         obj._coord_names.difference_update(names)
1734         if drop:
1735             for name in names:
1736                 del obj._variables[name]
1737         return obj
1738 
1739     def dump_to_store(self, store: AbstractDataStore, **kwargs) -> None:
1740         """Store dataset contents to a backends.*DataStore object."""
1741         from ..backends.api import dump_to_store
1742 
1743         # TODO: rename and/or cleanup this method to make it more consistent
1744         # with to_netcdf()
1745         dump_to_store(self, store, **kwargs)
1746 
1747     # path=None writes to bytes
1748     @overload
1749     def to_netcdf(
1750         self,
1751         path: None = None,
1752         mode: Literal["w", "a"] = "w",
1753         format: T_NetcdfTypes | None = None,
1754         group: str | None = None,
1755         engine: T_NetcdfEngine | None = None,
1756         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1757         unlimited_dims: Iterable[Hashable] | None = None,
1758         compute: bool = True,
1759         invalid_netcdf: bool = False,
1760     ) -> bytes:
1761         ...
1762 
1763     # default return None
1764     @overload
1765     def to_netcdf(
1766         self,
1767         path: str | PathLike,
1768         mode: Literal["w", "a"] = "w",
1769         format: T_NetcdfTypes | None = None,
1770         group: str | None = None,
1771         engine: T_NetcdfEngine | None = None,
1772         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1773         unlimited_dims: Iterable[Hashable] | None = None,
1774         compute: Literal[True] = True,
1775         invalid_netcdf: bool = False,
1776     ) -> None:
1777         ...
1778 
1779     # compute=False returns dask.Delayed
1780     @overload
1781     def to_netcdf(
1782         self,
1783         path: str | PathLike,
1784         mode: Literal["w", "a"] = "w",
1785         format: T_NetcdfTypes | None = None,
1786         group: str | None = None,
1787         engine: T_NetcdfEngine | None = None,
1788         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1789         unlimited_dims: Iterable[Hashable] | None = None,
1790         *,
1791         compute: Literal[False],
1792         invalid_netcdf: bool = False,
1793     ) -> Delayed:
1794         ...
1795 
1796     def to_netcdf(
1797         self,
1798         path: str | PathLike | None = None,
1799         mode: Literal["w", "a"] = "w",
1800         format: T_NetcdfTypes | None = None,
1801         group: str | None = None,
1802         engine: T_NetcdfEngine | None = None,
1803         encoding: Mapping[Hashable, Mapping[str, Any]] | None = None,
1804         unlimited_dims: Iterable[Hashable] | None = None,
1805         compute: bool = True,
1806         invalid_netcdf: bool = False,
1807     ) -> bytes | Delayed | None:
1808         """Write dataset contents to a netCDF file.
1809 
1810         Parameters
1811         ----------
1812         path : str, path-like or file-like, optional
1813             Path to which to save this dataset. File-like objects are only
1814             supported by the scipy engine. If no path is provided, this
1815             function returns the resulting netCDF file as bytes; in this case,
1816             we need to use scipy, which does not support netCDF version 4 (the
1817             default format becomes NETCDF3_64BIT).
1818         mode : {"w", "a"}, default: "w"
1819             Write ('w') or append ('a') mode. If mode='w', any existing file at
1820             this location will be overwritten. If mode='a', existing variables
1821             will be overwritten.
1822         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1823                   "NETCDF3_CLASSIC"}, optional
1824             File format for the resulting netCDF file:
1825 
1826             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1827               features.
1828             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1829               netCDF 3 compatible API features.
1830             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1831               which fully supports 2+ GB files, but is only compatible with
1832               clients linked against netCDF version 3.6.0 or later.
1833             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1834               handle 2+ GB files very well.
1835 
1836             All formats are supported by the netCDF4-python library.
1837             scipy.io.netcdf only supports the last two formats.
1838 
1839             The default format is NETCDF4 if you are saving a file to disk and
1840             have the netCDF4-python library available. Otherwise, xarray falls
1841             back to using scipy to write netCDF files and defaults to the
1842             NETCDF3_64BIT format (scipy does not support netCDF4).
1843         group : str, optional
1844             Path to the netCDF4 group in the given file to open (only works for
1845             format='NETCDF4'). The group(s) will be created if necessary.
1846         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1847             Engine to use when writing netCDF files. If not provided, the
1848             default engine is chosen based on available dependencies, with a
1849             preference for 'netcdf4' if writing to a file on disk.
1850         encoding : dict, optional
1851             Nested dictionary with variable names as keys and dictionaries of
1852             variable specific encodings as values, e.g.,
1853             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1854             "zlib": True}, ...}``
1855 
1856             The `h5netcdf` engine supports both the NetCDF4-style compression
1857             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1858             ones ``{"compression": "gzip", "compression_opts": 9}``.
1859             This allows using any compression plugin installed in the HDF5
1860             library, e.g. LZF.
1861 
1862         unlimited_dims : iterable of hashable, optional
1863             Dimension(s) that should be serialized as unlimited dimensions.
1864             By default, no dimensions are treated as unlimited dimensions.
1865             Note that unlimited_dims may also be set via
1866             ``dataset.encoding["unlimited_dims"]``.
1867         compute: bool, default: True
1868             If true compute immediately, otherwise return a
1869             ``dask.delayed.Delayed`` object that can be computed later.
1870         invalid_netcdf: bool, default: False
1871             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1872             hdf5 files which are invalid netcdf as described in
1873             https://github.com/h5netcdf/h5netcdf.
1874 
1875         Returns
1876         -------
1877             * ``bytes`` if path is None
1878             * ``dask.delayed.Delayed`` if compute is False
1879             * None otherwise
1880 
1881         See Also
1882         --------
1883         DataArray.to_netcdf
1884         """
1885         if encoding is None:
1886             encoding = {}
1887         from ..backends.api import to_netcdf
1888 
1889         return to_netcdf(  # type: ignore  # mypy cannot resolve the overloads:(
1890             self,
1891             path,
1892             mode=mode,
1893             format=format,
1894             group=group,
1895             engine=engine,
1896             encoding=encoding,
1897             unlimited_dims=unlimited_dims,
1898             compute=compute,
1899             multifile=False,
1900             invalid_netcdf=invalid_netcdf,
1901         )
1902 
1903     # compute=True (default) returns ZarrStore
1904     @overload
1905     def to_zarr(
1906         self,
1907         store: MutableMapping | str | PathLike[str] | None = None,
1908         chunk_store: MutableMapping | str | PathLike | None = None,
1909         mode: Literal["w", "w-", "a", "r+", None] = None,
1910         synchronizer=None,
1911         group: str | None = None,
1912         encoding: Mapping | None = None,
1913         compute: Literal[True] = True,
1914         consolidated: bool | None = None,
1915         append_dim: Hashable | None = None,
1916         region: Mapping[str, slice] | None = None,
1917         safe_chunks: bool = True,
1918         storage_options: dict[str, str] | None = None,
1919     ) -> ZarrStore:
1920         ...
1921 
1922     # compute=False returns dask.Delayed
1923     @overload
1924     def to_zarr(
1925         self,
1926         store: MutableMapping | str | PathLike[str] | None = None,
1927         chunk_store: MutableMapping | str | PathLike | None = None,
1928         mode: Literal["w", "w-", "a", "r+", None] = None,
1929         synchronizer=None,
1930         group: str | None = None,
1931         encoding: Mapping | None = None,
1932         *,
1933         compute: Literal[False],
1934         consolidated: bool | None = None,
1935         append_dim: Hashable | None = None,
1936         region: Mapping[str, slice] | None = None,
1937         safe_chunks: bool = True,
1938         storage_options: dict[str, str] | None = None,
1939     ) -> Delayed:
1940         ...
1941 
1942     def to_zarr(
1943         self,
1944         store: MutableMapping | str | PathLike[str] | None = None,
1945         chunk_store: MutableMapping | str | PathLike | None = None,
1946         mode: Literal["w", "w-", "a", "r+", None] = None,
1947         synchronizer=None,
1948         group: str | None = None,
1949         encoding: Mapping | None = None,
1950         compute: bool = True,
1951         consolidated: bool | None = None,
1952         append_dim: Hashable | None = None,
1953         region: Mapping[str, slice] | None = None,
1954         safe_chunks: bool = True,
1955         storage_options: dict[str, str] | None = None,
1956     ) -> ZarrStore | Delayed:
1957         """Write dataset contents to a zarr group.
1958 
1959         Zarr chunks are determined in the following way:
1960 
1961         - From the ``chunks`` attribute in each variable's ``encoding``
1962           (can be set via `Dataset.chunk`).
1963         - If the variable is a Dask array, from the dask chunks
1964         - If neither Dask chunks nor encoding chunks are present, chunks will
1965           be determined automatically by Zarr
1966         - If both Dask chunks and encoding chunks are present, encoding chunks
1967           will be used, provided that there is a many-to-one relationship between
1968           encoding chunks and dask chunks (i.e. Dask chunks are bigger than and
1969           evenly divide encoding chunks); otherwise raise a ``ValueError``.
1970           This restriction ensures that no synchronization / locks are required
1971           when writing. To disable this restriction, use ``safe_chunks=False``.
1972 
1973         Parameters
1974         ----------
1975         store : MutableMapping, str or path-like, optional
1976             Store or path to directory in local or remote file system.
1977         chunk_store : MutableMapping, str or path-like, optional
1978             Store or path to directory in local or remote file system only for Zarr
1979             array chunks. Requires zarr-python v2.4.0 or later.
1980         mode : {"w", "w-", "a", "r+", None}, optional
1981             Persistence mode: "w" means create (overwrite if exists);
1982             "w-" means create (fail if exists);
1983             "a" means override existing variables (create if does not exist);
1984             "r+" means modify existing array *values* only (raise an error if
1985             any metadata or shapes would change).
1986             The default mode is "a" if ``append_dim`` is set. Otherwise, it is
1987             "r+" if ``region`` is set and ``w-`` otherwise.
1988         synchronizer : object, optional
1989             Zarr array synchronizer.
1990         group : str, optional
1991             Group path. (a.k.a. `path` in zarr terminology.)
1992         encoding : dict, optional
1993             Nested dictionary with variable names as keys and dictionaries of
1994             variable specific encodings as values, e.g.,
1995             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1996         compute : bool, optional
1997             If True write array data immediately, otherwise return a
1998             ``dask.delayed.Delayed`` object that can be computed to write
1999             array data later. Metadata is always updated eagerly.
2000         consolidated : bool, optional
2001             If True, apply zarr's `consolidate_metadata` function to the store
2002             after writing metadata and read existing stores with consolidated
2003             metadata; if False, do not. The default (`consolidated=None`) means
2004             write consolidated metadata and attempt to read consolidated
2005             metadata for existing stores (falling back to non-consolidated).
2006         append_dim : hashable, optional
2007             If set, the dimension along which the data will be appended. All
2008             other dimensions on overridden variables must remain the same size.
2009         region : dict, optional
2010             Optional mapping from dimension names to integer slices along
2011             dataset dimensions to indicate the region of existing zarr array(s)
2012             in which to write this dataset's data. For example,
2013             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
2014             that values should be written to the region ``0:1000`` along ``x``
2015             and ``10000:11000`` along ``y``.
2016 
2017             Two restrictions apply to the use of ``region``:
2018 
2019             - If ``region`` is set, _all_ variables in a dataset must have at
2020               least one dimension in common with the region. Other variables
2021               should be written in a separate call to ``to_zarr()``.
2022             - Dimensions cannot be included in both ``region`` and
2023               ``append_dim`` at the same time. To create empty arrays to fill
2024               in with ``region``, use a separate call to ``to_zarr()`` with
2025               ``compute=False``. See "Appending to existing Zarr stores" in
2026               the reference documentation for full details.
2027         safe_chunks : bool, optional
2028             If True, only allow writes to when there is a many-to-one relationship
2029             between Zarr chunks (specified in encoding) and Dask chunks.
2030             Set False to override this restriction; however, data may become corrupted
2031             if Zarr arrays are written in parallel. This option may be useful in combination
2032             with ``compute=False`` to initialize a Zarr from an existing
2033             Dataset with arbitrary chunk structure.
2034         storage_options : dict, optional
2035             Any additional parameters for the storage backend (ignored for local
2036             paths).
2037 
2038         Returns
2039         -------
2040             * ``dask.delayed.Delayed`` if compute is False
2041             * ZarrStore otherwise
2042 
2043         References
2044         ----------
2045         https://zarr.readthedocs.io/
2046 
2047         Notes
2048         -----
2049         Zarr chunking behavior:
2050             If chunks are found in the encoding argument or attribute
2051             corresponding to any DataArray, those chunks are used.
2052             If a DataArray is a dask array, it is written with those chunks.
2053             If not other chunks are found, Zarr uses its own heuristics to
2054             choose automatic chunk sizes.
2055 
2056         encoding:
2057             The encoding attribute (if exists) of the DataArray(s) will be
2058             used. Override any existing encodings by providing the ``encoding`` kwarg.
2059 
2060         See Also
2061         --------
2062         :ref:`io.zarr`
2063             The I/O user guide, with more details and examples.
2064         """
2065         from ..backends.api import to_zarr
2066 
2067         return to_zarr(  # type: ignore
2068             self,
2069             store=store,
2070             chunk_store=chunk_store,
2071             storage_options=storage_options,
2072             mode=mode,
2073             synchronizer=synchronizer,
2074             group=group,
2075             encoding=encoding,
2076             compute=compute,
2077             consolidated=consolidated,
2078             append_dim=append_dim,
2079             region=region,
2080             safe_chunks=safe_chunks,
2081         )
2082 
2083     def __repr__(self) -> str:
2084         return formatting.dataset_repr(self)
2085 
2086     def _repr_html_(self) -> str:
2087         if OPTIONS["display_style"] == "text":
2088             return f"<pre>{escape(repr(self))}</pre>"
2089         return formatting_html.dataset_repr(self)
2090 
2091     def info(self, buf: IO | None = None) -> None:
2092         """
2093         Concise summary of a Dataset variables and attributes.
2094 
2095         Parameters
2096         ----------
2097         buf : file-like, default: sys.stdout
2098             writable buffer
2099 
2100         See Also
2101         --------
2102         pandas.DataFrame.assign
2103         ncdump : netCDF's ncdump
2104         """
2105         if buf is None:  # pragma: no cover
2106             buf = sys.stdout
2107 
2108         lines = []
2109         lines.append("xarray.Dataset {")
2110         lines.append("dimensions:")
2111         for name, size in self.dims.items():
2112             lines.append(f"\t{name} = {size} ;")
2113         lines.append("\nvariables:")
2114         for name, da in self.variables.items():
2115             dims = ", ".join(map(str, da.dims))
2116             lines.append(f"\t{da.dtype} {name}({dims}) ;")
2117             for k, v in da.attrs.items():
2118                 lines.append(f"\t\t{name}:{k} = {v} ;")
2119         lines.append("\n// global attributes:")
2120         for k, v in self.attrs.items():
2121             lines.append(f"\t:{k} = {v} ;")
2122         lines.append("}")
2123 
2124         buf.write("\n".join(lines))
2125 
2126     @property
2127     def chunks(self) -> Mapping[Hashable, tuple[int, ...]]:
2128         """
2129         Mapping from dimension names to block lengths for this dataset's data, or None if
2130         the underlying data is not a dask array.
2131         Cannot be modified directly, but can be modified by calling .chunk().
2132 
2133         Same as Dataset.chunksizes, but maintained for backwards compatibility.
2134 
2135         See Also
2136         --------
2137         Dataset.chunk
2138         Dataset.chunksizes
2139         xarray.unify_chunks
2140         """
2141         return get_chunksizes(self.variables.values())
2142 
2143     @property
2144     def chunksizes(self) -> Mapping[Hashable, tuple[int, ...]]:
2145         """
2146         Mapping from dimension names to block lengths for this dataset's data, or None if
2147         the underlying data is not a dask array.
2148         Cannot be modified directly, but can be modified by calling .chunk().
2149 
2150         Same as Dataset.chunks.
2151 
2152         See Also
2153         --------
2154         Dataset.chunk
2155         Dataset.chunks
2156         xarray.unify_chunks
2157         """
2158         return get_chunksizes(self.variables.values())
2159 
2160     def chunk(
2161         self: T_Dataset,
2162         chunks: (
2163             int | Literal["auto"] | Mapping[Any, None | int | str | tuple[int, ...]]
2164         ) = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
2165         name_prefix: str = "xarray-",
2166         token: str | None = None,
2167         lock: bool = False,
2168         inline_array: bool = False,
2169         **chunks_kwargs: Any,
2170     ) -> T_Dataset:
2171         """Coerce all arrays in this dataset into dask arrays with the given
2172         chunks.
2173 
2174         Non-dask arrays in this dataset will be converted to dask arrays. Dask
2175         arrays will be rechunked to the given chunk sizes.
2176 
2177         If neither chunks is not provided for one or more dimensions, chunk
2178         sizes along that dimension will not be updated; non-dask arrays will be
2179         converted into dask arrays with a single block.
2180 
2181         Parameters
2182         ----------
2183         chunks : int, tuple of int, "auto" or mapping of hashable to int, optional
2184             Chunk sizes along each dimension, e.g., ``5``, ``"auto"``, or
2185             ``{"x": 5, "y": 5}``.
2186         name_prefix : str, default: "xarray-"
2187             Prefix for the name of any new dask arrays.
2188         token : str, optional
2189             Token uniquely identifying this dataset.
2190         lock : bool, default: False
2191             Passed on to :py:func:`dask.array.from_array`, if the array is not
2192             already as dask array.
2193         inline_array: bool, default: False
2194             Passed on to :py:func:`dask.array.from_array`, if the array is not
2195             already as dask array.
2196         **chunks_kwargs : {dim: chunks, ...}, optional
2197             The keyword arguments form of ``chunks``.
2198             One of chunks or chunks_kwargs must be provided
2199 
2200         Returns
2201         -------
2202         chunked : xarray.Dataset
2203 
2204         See Also
2205         --------
2206         Dataset.chunks
2207         Dataset.chunksizes
2208         xarray.unify_chunks
2209         dask.array.from_array
2210         """
2211         if chunks is None and chunks_kwargs is None:
2212             warnings.warn(
2213                 "None value for 'chunks' is deprecated. "
2214                 "It will raise an error in the future. Use instead '{}'",
2215                 category=FutureWarning,
2216             )
2217             chunks = {}
2218 
2219         if isinstance(chunks, (Number, str, int)):
2220             chunks = dict.fromkeys(self.dims, chunks)
2221         else:
2222             chunks = either_dict_or_kwargs(chunks, chunks_kwargs, "chunk")
2223 
2224         bad_dims = chunks.keys() - self.dims.keys()
2225         if bad_dims:
2226             raise ValueError(
2227                 f"some chunks keys are not dimensions on this object: {bad_dims}"
2228             )
2229 
2230         variables = {
2231             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
2232             for k, v in self.variables.items()
2233         }
2234         return self._replace(variables)
2235 
2236     def _validate_indexers(
2237         self, indexers: Mapping[Any, Any], missing_dims: ErrorOptionsWithWarn = "raise"
2238     ) -> Iterator[tuple[Hashable, int | slice | np.ndarray | Variable]]:
2239         """Here we make sure
2240         + indexer has a valid keys
2241         + indexer is in a valid data type
2242         + string indexers are cast to the appropriate date type if the
2243           associated index is a DatetimeIndex or CFTimeIndex
2244         """
2245         from ..coding.cftimeindex import CFTimeIndex
2246         from .dataarray import DataArray
2247 
2248         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2249 
2250         # all indexers should be int, slice, np.ndarrays, or Variable
2251         for k, v in indexers.items():
2252             if isinstance(v, (int, slice, Variable)):
2253                 yield k, v
2254             elif isinstance(v, DataArray):
2255                 yield k, v.variable
2256             elif isinstance(v, tuple):
2257                 yield k, as_variable(v)
2258             elif isinstance(v, Dataset):
2259                 raise TypeError("cannot use a Dataset as an indexer")
2260             elif isinstance(v, Sequence) and len(v) == 0:
2261                 yield k, np.empty((0,), dtype="int64")
2262             else:
2263                 v = np.asarray(v)
2264 
2265                 if v.dtype.kind in "US":
2266                     index = self._indexes[k].to_pandas_index()
2267                     if isinstance(index, pd.DatetimeIndex):
2268                         v = v.astype("datetime64[ns]")
2269                     elif isinstance(index, CFTimeIndex):
2270                         v = _parse_array_of_cftime_strings(v, index.date_type)
2271 
2272                 if v.ndim > 1:
2273                     raise IndexError(
2274                         "Unlabeled multi-dimensional array cannot be "
2275                         "used for indexing: {}".format(k)
2276                     )
2277                 yield k, v
2278 
2279     def _validate_interp_indexers(
2280         self, indexers: Mapping[Any, Any]
2281     ) -> Iterator[tuple[Hashable, Variable]]:
2282         """Variant of _validate_indexers to be used for interpolation"""
2283         for k, v in self._validate_indexers(indexers):
2284             if isinstance(v, Variable):
2285                 if v.ndim == 1:
2286                     yield k, v.to_index_variable()
2287                 else:
2288                     yield k, v
2289             elif isinstance(v, int):
2290                 yield k, Variable((), v, attrs=self.coords[k].attrs)
2291             elif isinstance(v, np.ndarray):
2292                 if v.ndim == 0:
2293                     yield k, Variable((), v, attrs=self.coords[k].attrs)
2294                 elif v.ndim == 1:
2295                     yield k, IndexVariable((k,), v, attrs=self.coords[k].attrs)
2296                 else:
2297                     raise AssertionError()  # Already tested by _validate_indexers
2298             else:
2299                 raise TypeError(type(v))
2300 
2301     def _get_indexers_coords_and_indexes(self, indexers):
2302         """Extract coordinates and indexes from indexers.
2303 
2304         Only coordinate with a name different from any of self.variables will
2305         be attached.
2306         """
2307         from .dataarray import DataArray
2308 
2309         coords_list = []
2310         for k, v in indexers.items():
2311             if isinstance(v, DataArray):
2312                 if v.dtype.kind == "b":
2313                     if v.ndim != 1:  # we only support 1-d boolean array
2314                         raise ValueError(
2315                             "{:d}d-boolean array is used for indexing along "
2316                             "dimension {!r}, but only 1d boolean arrays are "
2317                             "supported.".format(v.ndim, k)
2318                         )
2319                     # Make sure in case of boolean DataArray, its
2320                     # coordinate also should be indexed.
2321                     v_coords = v[v.values.nonzero()[0]].coords
2322                 else:
2323                     v_coords = v.coords
2324                 coords_list.append(v_coords)
2325 
2326         # we don't need to call align() explicitly or check indexes for
2327         # alignment, because merge_variables already checks for exact alignment
2328         # between dimension coordinates
2329         coords, indexes = merge_coordinates_without_align(coords_list)
2330         assert_coordinate_consistent(self, coords)
2331 
2332         # silently drop the conflicted variables.
2333         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2334         attached_indexes = {
2335             k: v for k, v in indexes.items() if k not in self._variables
2336         }
2337         return attached_coords, attached_indexes
2338 
2339     def isel(
2340         self: T_Dataset,
2341         indexers: Mapping[Any, Any] | None = None,
2342         drop: bool = False,
2343         missing_dims: ErrorOptionsWithWarn = "raise",
2344         **indexers_kwargs: Any,
2345     ) -> T_Dataset:
2346         """Returns a new dataset with each array indexed along the specified
2347         dimension(s).
2348 
2349         This method selects values from each array using its `__getitem__`
2350         method, except this method does not require knowing the order of
2351         each array's dimensions.
2352 
2353         Parameters
2354         ----------
2355         indexers : dict, optional
2356             A dict with keys matching dimensions and values given
2357             by integers, slice objects or arrays.
2358             indexer can be a integer, slice, array-like or DataArray.
2359             If DataArrays are passed as indexers, xarray-style indexing will be
2360             carried out. See :ref:`indexing` for the details.
2361             One of indexers or indexers_kwargs must be provided.
2362         drop : bool, default: False
2363             If ``drop=True``, drop coordinates variables indexed by integers
2364             instead of making them scalar.
2365         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2366             What to do if dimensions that should be selected from are not present in the
2367             Dataset:
2368             - "raise": raise an exception
2369             - "warn": raise a warning, and ignore the missing dimensions
2370             - "ignore": ignore the missing dimensions
2371 
2372         **indexers_kwargs : {dim: indexer, ...}, optional
2373             The keyword arguments form of ``indexers``.
2374             One of indexers or indexers_kwargs must be provided.
2375 
2376         Returns
2377         -------
2378         obj : Dataset
2379             A new Dataset with the same contents as this dataset, except each
2380             array and dimension is indexed by the appropriate indexers.
2381             If indexer DataArrays have coordinates that do not conflict with
2382             this object, then these coordinates will be attached.
2383             In general, each array's data will be a view of the array's data
2384             in this dataset, unless vectorized indexing was triggered by using
2385             an array indexer, in which case the data will be a copy.
2386 
2387         See Also
2388         --------
2389         Dataset.sel
2390         DataArray.isel
2391         """
2392         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2393         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2394             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2395 
2396         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2397         # lists, or zero or one-dimensional np.ndarray's
2398         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2399 
2400         variables = {}
2401         dims: dict[Hashable, int] = {}
2402         coord_names = self._coord_names.copy()
2403 
2404         indexes, index_variables = isel_indexes(self.xindexes, indexers)
2405 
2406         for name, var in self._variables.items():
2407             # preserve variable order
2408             if name in index_variables:
2409                 var = index_variables[name]
2410             else:
2411                 var_indexers = {k: v for k, v in indexers.items() if k in var.dims}
2412                 if var_indexers:
2413                     var = var.isel(var_indexers)
2414                     if drop and var.ndim == 0 and name in coord_names:
2415                         coord_names.remove(name)
2416                         continue
2417             variables[name] = var
2418             dims.update(zip(var.dims, var.shape))
2419 
2420         return self._construct_direct(
2421             variables=variables,
2422             coord_names=coord_names,
2423             dims=dims,
2424             attrs=self._attrs,
2425             indexes=indexes,
2426             encoding=self._encoding,
2427             close=self._close,
2428         )
2429 
2430     def _isel_fancy(
2431         self: T_Dataset,
2432         indexers: Mapping[Any, Any],
2433         *,
2434         drop: bool,
2435         missing_dims: ErrorOptionsWithWarn = "raise",
2436     ) -> T_Dataset:
2437         valid_indexers = dict(self._validate_indexers(indexers, missing_dims))
2438 
2439         variables: dict[Hashable, Variable] = {}
2440         indexes, index_variables = isel_indexes(self.xindexes, valid_indexers)
2441 
2442         for name, var in self.variables.items():
2443             if name in index_variables:
2444                 new_var = index_variables[name]
2445             else:
2446                 var_indexers = {
2447                     k: v for k, v in valid_indexers.items() if k in var.dims
2448                 }
2449                 if var_indexers:
2450                     new_var = var.isel(indexers=var_indexers)
2451                     # drop scalar coordinates
2452                     # https://github.com/pydata/xarray/issues/6554
2453                     if name in self.coords and drop and new_var.ndim == 0:
2454                         continue
2455                 else:
2456                     new_var = var.copy(deep=False)
2457                 if name not in indexes:
2458                     new_var = new_var.to_base_variable()
2459             variables[name] = new_var
2460 
2461         coord_names = self._coord_names & variables.keys()
2462         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2463 
2464         # Extract coordinates from indexers
2465         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2466         variables.update(coord_vars)
2467         indexes.update(new_indexes)
2468         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2469         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2470 
2471     def sel(
2472         self: T_Dataset,
2473         indexers: Mapping[Any, Any] = None,
2474         method: str = None,
2475         tolerance: int | float | Iterable[int | float] | None = None,
2476         drop: bool = False,
2477         **indexers_kwargs: Any,
2478     ) -> T_Dataset:
2479         """Returns a new dataset with each array indexed by tick labels
2480         along the specified dimension(s).
2481 
2482         In contrast to `Dataset.isel`, indexers for this method should use
2483         labels instead of integers.
2484 
2485         Under the hood, this method is powered by using pandas's powerful Index
2486         objects. This makes label based indexing essentially just as fast as
2487         using integer indexing.
2488 
2489         It also means this method uses pandas's (well documented) logic for
2490         indexing. This means you can use string shortcuts for datetime indexes
2491         (e.g., '2000-01' to select all values in January 2000). It also means
2492         that slices are treated as inclusive of both the start and stop values,
2493         unlike normal Python indexing.
2494 
2495         Parameters
2496         ----------
2497         indexers : dict, optional
2498             A dict with keys matching dimensions and values given
2499             by scalars, slices or arrays of tick labels. For dimensions with
2500             multi-index, the indexer may also be a dict-like object with keys
2501             matching index level names.
2502             If DataArrays are passed as indexers, xarray-style indexing will be
2503             carried out. See :ref:`indexing` for the details.
2504             One of indexers or indexers_kwargs must be provided.
2505         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2506             Method to use for inexact matches:
2507 
2508             * None (default): only exact matches
2509             * pad / ffill: propagate last valid index value forward
2510             * backfill / bfill: propagate next valid index value backward
2511             * nearest: use nearest valid index value
2512         tolerance : optional
2513             Maximum distance between original and new labels for inexact
2514             matches. The values of the index at the matching locations must
2515             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2516         drop : bool, optional
2517             If ``drop=True``, drop coordinates variables in `indexers` instead
2518             of making them scalar.
2519         **indexers_kwargs : {dim: indexer, ...}, optional
2520             The keyword arguments form of ``indexers``.
2521             One of indexers or indexers_kwargs must be provided.
2522 
2523         Returns
2524         -------
2525         obj : Dataset
2526             A new Dataset with the same contents as this dataset, except each
2527             variable and dimension is indexed by the appropriate indexers.
2528             If indexer DataArrays have coordinates that do not conflict with
2529             this object, then these coordinates will be attached.
2530             In general, each array's data will be a view of the array's data
2531             in this dataset, unless vectorized indexing was triggered by using
2532             an array indexer, in which case the data will be a copy.
2533 
2534         See Also
2535         --------
2536         Dataset.isel
2537         DataArray.sel
2538         """
2539         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2540         query_results = map_index_queries(
2541             self, indexers=indexers, method=method, tolerance=tolerance
2542         )
2543 
2544         if drop:
2545             no_scalar_variables = {}
2546             for k, v in query_results.variables.items():
2547                 if v.dims:
2548                     no_scalar_variables[k] = v
2549                 else:
2550                     if k in self._coord_names:
2551                         query_results.drop_coords.append(k)
2552             query_results.variables = no_scalar_variables
2553 
2554         result = self.isel(indexers=query_results.dim_indexers, drop=drop)
2555         return result._overwrite_indexes(*query_results.as_tuple()[1:])
2556 
2557     def head(
2558         self: T_Dataset,
2559         indexers: Mapping[Any, int] | int | None = None,
2560         **indexers_kwargs: Any,
2561     ) -> T_Dataset:
2562         """Returns a new dataset with the first `n` values of each array
2563         for the specified dimension(s).
2564 
2565         Parameters
2566         ----------
2567         indexers : dict or int, default: 5
2568             A dict with keys matching dimensions and integer values `n`
2569             or a single integer `n` applied over all dimensions.
2570             One of indexers or indexers_kwargs must be provided.
2571         **indexers_kwargs : {dim: n, ...}, optional
2572             The keyword arguments form of ``indexers``.
2573             One of indexers or indexers_kwargs must be provided.
2574 
2575         See Also
2576         --------
2577         Dataset.tail
2578         Dataset.thin
2579         DataArray.head
2580         """
2581         if not indexers_kwargs:
2582             if indexers is None:
2583                 indexers = 5
2584             if not isinstance(indexers, int) and not is_dict_like(indexers):
2585                 raise TypeError("indexers must be either dict-like or a single integer")
2586         if isinstance(indexers, int):
2587             indexers = {dim: indexers for dim in self.dims}
2588         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2589         for k, v in indexers.items():
2590             if not isinstance(v, int):
2591                 raise TypeError(
2592                     "expected integer type indexer for "
2593                     f"dimension {k!r}, found {type(v)!r}"
2594                 )
2595             elif v < 0:
2596                 raise ValueError(
2597                     "expected positive integer as indexer "
2598                     f"for dimension {k!r}, found {v}"
2599                 )
2600         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2601         return self.isel(indexers_slices)
2602 
2603     def tail(
2604         self: T_Dataset,
2605         indexers: Mapping[Any, int] | int | None = None,
2606         **indexers_kwargs: Any,
2607     ) -> T_Dataset:
2608         """Returns a new dataset with the last `n` values of each array
2609         for the specified dimension(s).
2610 
2611         Parameters
2612         ----------
2613         indexers : dict or int, default: 5
2614             A dict with keys matching dimensions and integer values `n`
2615             or a single integer `n` applied over all dimensions.
2616             One of indexers or indexers_kwargs must be provided.
2617         **indexers_kwargs : {dim: n, ...}, optional
2618             The keyword arguments form of ``indexers``.
2619             One of indexers or indexers_kwargs must be provided.
2620 
2621         See Also
2622         --------
2623         Dataset.head
2624         Dataset.thin
2625         DataArray.tail
2626         """
2627         if not indexers_kwargs:
2628             if indexers is None:
2629                 indexers = 5
2630             if not isinstance(indexers, int) and not is_dict_like(indexers):
2631                 raise TypeError("indexers must be either dict-like or a single integer")
2632         if isinstance(indexers, int):
2633             indexers = {dim: indexers for dim in self.dims}
2634         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2635         for k, v in indexers.items():
2636             if not isinstance(v, int):
2637                 raise TypeError(
2638                     "expected integer type indexer for "
2639                     f"dimension {k!r}, found {type(v)!r}"
2640                 )
2641             elif v < 0:
2642                 raise ValueError(
2643                     "expected positive integer as indexer "
2644                     f"for dimension {k!r}, found {v}"
2645                 )
2646         indexers_slices = {
2647             k: slice(-val, None) if val != 0 else slice(val)
2648             for k, val in indexers.items()
2649         }
2650         return self.isel(indexers_slices)
2651 
2652     def thin(
2653         self: T_Dataset,
2654         indexers: Mapping[Any, int] | int | None = None,
2655         **indexers_kwargs: Any,
2656     ) -> T_Dataset:
2657         """Returns a new dataset with each array indexed along every `n`-th
2658         value for the specified dimension(s)
2659 
2660         Parameters
2661         ----------
2662         indexers : dict or int
2663             A dict with keys matching dimensions and integer values `n`
2664             or a single integer `n` applied over all dimensions.
2665             One of indexers or indexers_kwargs must be provided.
2666         **indexers_kwargs : {dim: n, ...}, optional
2667             The keyword arguments form of ``indexers``.
2668             One of indexers or indexers_kwargs must be provided.
2669 
2670         Examples
2671         --------
2672         >>> x_arr = np.arange(0, 26)
2673         >>> x_arr
2674         array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,
2675                17, 18, 19, 20, 21, 22, 23, 24, 25])
2676         >>> x = xr.DataArray(
2677         ...     np.reshape(x_arr, (2, 13)),
2678         ...     dims=("x", "y"),
2679         ...     coords={"x": [0, 1], "y": np.arange(0, 13)},
2680         ... )
2681         >>> x_ds = xr.Dataset({"foo": x})
2682         >>> x_ds
2683         <xarray.Dataset>
2684         Dimensions:  (x: 2, y: 13)
2685         Coordinates:
2686           * x        (x) int64 0 1
2687           * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11 12
2688         Data variables:
2689             foo      (x, y) int64 0 1 2 3 4 5 6 7 8 9 ... 16 17 18 19 20 21 22 23 24 25
2690 
2691         >>> x_ds.thin(3)
2692         <xarray.Dataset>
2693         Dimensions:  (x: 1, y: 5)
2694         Coordinates:
2695           * x        (x) int64 0
2696           * y        (y) int64 0 3 6 9 12
2697         Data variables:
2698             foo      (x, y) int64 0 3 6 9 12
2699         >>> x.thin({"x": 2, "y": 5})
2700         <xarray.DataArray (x: 1, y: 3)>
2701         array([[ 0,  5, 10]])
2702         Coordinates:
2703           * x        (x) int64 0
2704           * y        (y) int64 0 5 10
2705 
2706         See Also
2707         --------
2708         Dataset.head
2709         Dataset.tail
2710         DataArray.thin
2711         """
2712         if (
2713             not indexers_kwargs
2714             and not isinstance(indexers, int)
2715             and not is_dict_like(indexers)
2716         ):
2717             raise TypeError("indexers must be either dict-like or a single integer")
2718         if isinstance(indexers, int):
2719             indexers = {dim: indexers for dim in self.dims}
2720         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2721         for k, v in indexers.items():
2722             if not isinstance(v, int):
2723                 raise TypeError(
2724                     "expected integer type indexer for "
2725                     f"dimension {k!r}, found {type(v)!r}"
2726                 )
2727             elif v < 0:
2728                 raise ValueError(
2729                     "expected positive integer as indexer "
2730                     f"for dimension {k!r}, found {v}"
2731                 )
2732             elif v == 0:
2733                 raise ValueError("step cannot be zero")
2734         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2735         return self.isel(indexers_slices)
2736 
2737     def broadcast_like(
2738         self: T_Dataset, other: Dataset | DataArray, exclude: Iterable[Hashable] = None
2739     ) -> T_Dataset:
2740         """Broadcast this DataArray against another Dataset or DataArray.
2741         This is equivalent to xr.broadcast(other, self)[1]
2742 
2743         Parameters
2744         ----------
2745         other : Dataset or DataArray
2746             Object against which to broadcast this array.
2747         exclude : iterable of hashable, optional
2748             Dimensions that must not be broadcasted
2749 
2750         """
2751         if exclude is None:
2752             exclude = set()
2753         else:
2754             exclude = set(exclude)
2755         args = align(other, self, join="outer", copy=False, exclude=exclude)
2756 
2757         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2758 
2759         return _broadcast_helper(
2760             cast("T_Dataset", args[1]), exclude, dims_map, common_coords
2761         )
2762 
2763     def _reindex_callback(
2764         self,
2765         aligner: alignment.Aligner,
2766         dim_pos_indexers: dict[Hashable, Any],
2767         variables: dict[Hashable, Variable],
2768         indexes: dict[Hashable, Index],
2769         fill_value: Any,
2770         exclude_dims: frozenset[Hashable],
2771         exclude_vars: frozenset[Hashable],
2772     ) -> Dataset:
2773         """Callback called from ``Aligner`` to create a new reindexed Dataset."""
2774 
2775         new_variables = variables.copy()
2776         new_indexes = indexes.copy()
2777 
2778         # re-assign variable metadata
2779         for name, new_var in new_variables.items():
2780             var = self._variables.get(name)
2781             if var is not None:
2782                 new_var.attrs = var.attrs
2783                 new_var.encoding = var.encoding
2784 
2785         # pass through indexes from excluded dimensions
2786         # no extra check needed for multi-coordinate indexes, potential conflicts
2787         # should already have been detected when aligning the indexes
2788         for name, idx in self._indexes.items():
2789             var = self._variables[name]
2790             if set(var.dims) <= exclude_dims:
2791                 new_indexes[name] = idx
2792                 new_variables[name] = var
2793 
2794         if not dim_pos_indexers:
2795             # fast path for no reindexing necessary
2796             if set(new_indexes) - set(self._indexes):
2797                 # this only adds new indexes and their coordinate variables
2798                 reindexed = self._overwrite_indexes(new_indexes, new_variables)
2799             else:
2800                 reindexed = self.copy(deep=aligner.copy)
2801         else:
2802             to_reindex = {
2803                 k: v
2804                 for k, v in self.variables.items()
2805                 if k not in variables and k not in exclude_vars
2806             }
2807             reindexed_vars = alignment.reindex_variables(
2808                 to_reindex,
2809                 dim_pos_indexers,
2810                 copy=aligner.copy,
2811                 fill_value=fill_value,
2812                 sparse=aligner.sparse,
2813             )
2814             new_variables.update(reindexed_vars)
2815             new_coord_names = self._coord_names | set(new_indexes)
2816             reindexed = self._replace_with_new_dims(
2817                 new_variables, new_coord_names, indexes=new_indexes
2818             )
2819 
2820         return reindexed
2821 
2822     def reindex_like(
2823         self: T_Dataset,
2824         other: Dataset | DataArray,
2825         method: ReindexMethodOptions = None,
2826         tolerance: int | float | Iterable[int | float] | None = None,
2827         copy: bool = True,
2828         fill_value: Any = xrdtypes.NA,
2829     ) -> T_Dataset:
2830         """Conform this object onto the indexes of another object, filling in
2831         missing values with ``fill_value``. The default fill value is NaN.
2832 
2833         Parameters
2834         ----------
2835         other : Dataset or DataArray
2836             Object with an 'indexes' attribute giving a mapping from dimension
2837             names to pandas.Index objects, which provides coordinates upon
2838             which to index the variables in this dataset. The indexes on this
2839             other object need not be the same as the indexes on this
2840             dataset. Any mis-matched index values will be filled in with
2841             NaN, and any mis-matched dimension names will simply be ignored.
2842         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2843             Method to use for filling index values from other not found in this
2844             dataset:
2845 
2846             - None (default): don't fill gaps
2847             - "pad" / "ffill": propagate last valid index value forward
2848             - "backfill" / "bfill": propagate next valid index value backward
2849             - "nearest": use nearest valid index value
2850 
2851         tolerance : optional
2852             Maximum distance between original and new labels for inexact
2853             matches. The values of the index at the matching locations must
2854             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2855             Tolerance may be a scalar value, which applies the same tolerance
2856             to all values, or list-like, which applies variable tolerance per
2857             element. List-like must be the same size as the index and its dtype
2858             must exactly match the index’s type.
2859         copy : bool, default: True
2860             If ``copy=True``, data in the return value is always copied. If
2861             ``copy=False`` and reindexing is unnecessary, or can be performed
2862             with only slice operations, then the output may share memory with
2863             the input. In either case, a new xarray object is always returned.
2864         fill_value : scalar or dict-like, optional
2865             Value to use for newly missing values. If a dict-like maps
2866             variable names to fill values.
2867 
2868         Returns
2869         -------
2870         reindexed : Dataset
2871             Another dataset, with this dataset's data but coordinates from the
2872             other object.
2873 
2874         See Also
2875         --------
2876         Dataset.reindex
2877         align
2878         """
2879         return alignment.reindex_like(
2880             self,
2881             other=other,
2882             method=method,
2883             tolerance=tolerance,
2884             copy=copy,
2885             fill_value=fill_value,
2886         )
2887 
2888     def reindex(
2889         self: T_Dataset,
2890         indexers: Mapping[Any, Any] | None = None,
2891         method: ReindexMethodOptions = None,
2892         tolerance: int | float | Iterable[int | float] | None = None,
2893         copy: bool = True,
2894         fill_value: Any = xrdtypes.NA,
2895         **indexers_kwargs: Any,
2896     ) -> T_Dataset:
2897         """Conform this object onto a new set of indexes, filling in
2898         missing values with ``fill_value``. The default fill value is NaN.
2899 
2900         Parameters
2901         ----------
2902         indexers : dict, optional
2903             Dictionary with keys given by dimension names and values given by
2904             arrays of coordinates tick labels. Any mis-matched coordinate
2905             values will be filled in with NaN, and any mis-matched dimension
2906             names will simply be ignored.
2907             One of indexers or indexers_kwargs must be provided.
2908         method : {None, "nearest", "pad", "ffill", "backfill", "bfill", None}, optional
2909             Method to use for filling index values in ``indexers`` not found in
2910             this dataset:
2911 
2912             - None (default): don't fill gaps
2913             - "pad" / "ffill": propagate last valid index value forward
2914             - "backfill" / "bfill": propagate next valid index value backward
2915             - "nearest": use nearest valid index value
2916 
2917         tolerance : optional
2918             Maximum distance between original and new labels for inexact
2919             matches. The values of the index at the matching locations must
2920             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2921             Tolerance may be a scalar value, which applies the same tolerance
2922             to all values, or list-like, which applies variable tolerance per
2923             element. List-like must be the same size as the index and its dtype
2924             must exactly match the index’s type.
2925         copy : bool, default: True
2926             If ``copy=True``, data in the return value is always copied. If
2927             ``copy=False`` and reindexing is unnecessary, or can be performed
2928             with only slice operations, then the output may share memory with
2929             the input. In either case, a new xarray object is always returned.
2930         fill_value : scalar or dict-like, optional
2931             Value to use for newly missing values. If a dict-like,
2932             maps variable names (including coordinates) to fill values.
2933         sparse : bool, default: False
2934             use sparse-array.
2935         **indexers_kwargs : {dim: indexer, ...}, optional
2936             Keyword arguments in the same form as ``indexers``.
2937             One of indexers or indexers_kwargs must be provided.
2938 
2939         Returns
2940         -------
2941         reindexed : Dataset
2942             Another dataset, with this dataset's data but replaced coordinates.
2943 
2944         See Also
2945         --------
2946         Dataset.reindex_like
2947         align
2948         pandas.Index.get_indexer
2949 
2950         Examples
2951         --------
2952         Create a dataset with some fictional data.
2953 
2954         >>> x = xr.Dataset(
2955         ...     {
2956         ...         "temperature": ("station", 20 * np.random.rand(4)),
2957         ...         "pressure": ("station", 500 * np.random.rand(4)),
2958         ...     },
2959         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2960         ... )
2961         >>> x
2962         <xarray.Dataset>
2963         Dimensions:      (station: 4)
2964         Coordinates:
2965           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2966         Data variables:
2967             temperature  (station) float64 10.98 14.3 12.06 10.9
2968             pressure     (station) float64 211.8 322.9 218.8 445.9
2969         >>> x.indexes
2970         Indexes:
2971         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2972 
2973         Create a new index and reindex the dataset. By default values in the new index that
2974         do not have corresponding records in the dataset are assigned `NaN`.
2975 
2976         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2977         >>> x.reindex({"station": new_index})
2978         <xarray.Dataset>
2979         Dimensions:      (station: 4)
2980         Coordinates:
2981           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2982         Data variables:
2983             temperature  (station) float64 10.98 nan 12.06 nan
2984             pressure     (station) float64 211.8 nan 218.8 nan
2985 
2986         We can fill in the missing values by passing a value to the keyword `fill_value`.
2987 
2988         >>> x.reindex({"station": new_index}, fill_value=0)
2989         <xarray.Dataset>
2990         Dimensions:      (station: 4)
2991         Coordinates:
2992           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2993         Data variables:
2994             temperature  (station) float64 10.98 0.0 12.06 0.0
2995             pressure     (station) float64 211.8 0.0 218.8 0.0
2996 
2997         We can also use different fill values for each variable.
2998 
2999         >>> x.reindex(
3000         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
3001         ... )
3002         <xarray.Dataset>
3003         Dimensions:      (station: 4)
3004         Coordinates:
3005           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
3006         Data variables:
3007             temperature  (station) float64 10.98 0.0 12.06 0.0
3008             pressure     (station) float64 211.8 100.0 218.8 100.0
3009 
3010         Because the index is not monotonically increasing or decreasing, we cannot use arguments
3011         to the keyword method to fill the `NaN` values.
3012 
3013         >>> x.reindex({"station": new_index}, method="nearest")
3014         Traceback (most recent call last):
3015         ...
3016             raise ValueError('index must be monotonic increasing or decreasing')
3017         ValueError: index must be monotonic increasing or decreasing
3018 
3019         To further illustrate the filling functionality in reindex, we will create a
3020         dataset with a monotonically increasing index (for example, a sequence of dates).
3021 
3022         >>> x2 = xr.Dataset(
3023         ...     {
3024         ...         "temperature": (
3025         ...             "time",
3026         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
3027         ...         ),
3028         ...         "pressure": ("time", 500 * np.random.rand(6)),
3029         ...     },
3030         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
3031         ... )
3032         >>> x2
3033         <xarray.Dataset>
3034         Dimensions:      (time: 6)
3035         Coordinates:
3036           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
3037         Data variables:
3038             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
3039             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
3040 
3041         Suppose we decide to expand the dataset to cover a wider date range.
3042 
3043         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
3044         >>> x2.reindex({"time": time_index2})
3045         <xarray.Dataset>
3046         Dimensions:      (time: 10)
3047         Coordinates:
3048           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3049         Data variables:
3050             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
3051             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
3052 
3053         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
3054         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
3055 
3056         For example, to back-propagate the last valid value to fill the `NaN` values,
3057         pass `bfill` as an argument to the `method` keyword.
3058 
3059         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
3060         >>> x3
3061         <xarray.Dataset>
3062         Dimensions:      (time: 10)
3063         Coordinates:
3064           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
3065         Data variables:
3066             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
3067             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
3068 
3069         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
3070         will not be filled by any of the value propagation schemes.
3071 
3072         >>> x2.where(x2.temperature.isnull(), drop=True)
3073         <xarray.Dataset>
3074         Dimensions:      (time: 1)
3075         Coordinates:
3076           * time         (time) datetime64[ns] 2019-01-03
3077         Data variables:
3078             temperature  (time) float64 nan
3079             pressure     (time) float64 395.9
3080         >>> x3.where(x3.temperature.isnull(), drop=True)
3081         <xarray.Dataset>
3082         Dimensions:      (time: 2)
3083         Coordinates:
3084           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
3085         Data variables:
3086             temperature  (time) float64 nan nan
3087             pressure     (time) float64 395.9 nan
3088 
3089         This is because filling while reindexing does not look at dataset values, but only compares
3090         the original and desired indexes. If you do want to fill in the `NaN` values present in the
3091         original dataset, use the :py:meth:`~Dataset.fillna()` method.
3092 
3093         """
3094         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3095         return alignment.reindex(
3096             self,
3097             indexers=indexers,
3098             method=method,
3099             tolerance=tolerance,
3100             copy=copy,
3101             fill_value=fill_value,
3102         )
3103 
3104     def _reindex(
3105         self: T_Dataset,
3106         indexers: Mapping[Any, Any] = None,
3107         method: str = None,
3108         tolerance: int | float | Iterable[int | float] | None = None,
3109         copy: bool = True,
3110         fill_value: Any = xrdtypes.NA,
3111         sparse: bool = False,
3112         **indexers_kwargs: Any,
3113     ) -> T_Dataset:
3114         """
3115         Same as reindex but supports sparse option.
3116         """
3117         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
3118         return alignment.reindex(
3119             self,
3120             indexers=indexers,
3121             method=method,
3122             tolerance=tolerance,
3123             copy=copy,
3124             fill_value=fill_value,
3125             sparse=sparse,
3126         )
3127 
3128     def interp(
3129         self: T_Dataset,
3130         coords: Mapping[Any, Any] | None = None,
3131         method: InterpOptions = "linear",
3132         assume_sorted: bool = False,
3133         kwargs: Mapping[str, Any] = None,
3134         method_non_numeric: str = "nearest",
3135         **coords_kwargs: Any,
3136     ) -> T_Dataset:
3137         """Interpolate a Dataset onto new coordinates
3138 
3139         Performs univariate or multivariate interpolation of a Dataset onto
3140         new coordinates using scipy's interpolation routines. If interpolating
3141         along an existing dimension, :py:class:`scipy.interpolate.interp1d` is
3142         called.  When interpolating along multiple existing dimensions, an
3143         attempt is made to decompose the interpolation into multiple
3144         1-dimensional interpolations. If this is possible,
3145         :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3146         :py:func:`scipy.interpolate.interpn` is called.
3147 
3148         Parameters
3149         ----------
3150         coords : dict, optional
3151             Mapping from dimension names to the new coordinates.
3152             New coordinate can be a scalar, array-like or DataArray.
3153             If DataArrays are passed as new coordinates, their dimensions are
3154             used for the broadcasting. Missing values are skipped.
3155         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3156             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3157             String indicating which method to use for interpolation:
3158 
3159             - 'linear': linear interpolation. Additional keyword
3160               arguments are passed to :py:func:`numpy.interp`
3161             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3162               are passed to :py:func:`scipy.interpolate.interp1d`. If
3163               ``method='polynomial'``, the ``order`` keyword argument must also be
3164               provided.
3165             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3166               respective :py:class:`scipy.interpolate` classes.
3167 
3168         assume_sorted : bool, default: False
3169             If False, values of coordinates that are interpolated over can be
3170             in any order and they are sorted first. If True, interpolated
3171             coordinates are assumed to be an array of monotonically increasing
3172             values.
3173         kwargs : dict, optional
3174             Additional keyword arguments passed to scipy's interpolator. Valid
3175             options and their behavior depend whether ``interp1d`` or
3176             ``interpn`` is used.
3177         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3178             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3179             ``"nearest"`` is used by default.
3180         **coords_kwargs : {dim: coordinate, ...}, optional
3181             The keyword arguments form of ``coords``.
3182             One of coords or coords_kwargs must be provided.
3183 
3184         Returns
3185         -------
3186         interpolated : Dataset
3187             New dataset on the new coordinates.
3188 
3189         Notes
3190         -----
3191         scipy is required.
3192 
3193         See Also
3194         --------
3195         scipy.interpolate.interp1d
3196         scipy.interpolate.interpn
3197 
3198         Examples
3199         --------
3200         >>> ds = xr.Dataset(
3201         ...     data_vars={
3202         ...         "a": ("x", [5, 7, 4]),
3203         ...         "b": (
3204         ...             ("x", "y"),
3205         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
3206         ...         ),
3207         ...     },
3208         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
3209         ... )
3210         >>> ds
3211         <xarray.Dataset>
3212         Dimensions:  (x: 3, y: 4)
3213         Coordinates:
3214           * x        (x) int64 0 1 2
3215           * y        (y) int64 10 12 14 16
3216         Data variables:
3217             a        (x) int64 5 7 4
3218             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
3219 
3220         1D interpolation with the default method (linear):
3221 
3222         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
3223         <xarray.Dataset>
3224         Dimensions:  (x: 4, y: 4)
3225         Coordinates:
3226           * y        (y) int64 10 12 14 16
3227           * x        (x) float64 0.0 0.75 1.25 1.75
3228         Data variables:
3229             a        (x) float64 5.0 6.5 6.25 4.75
3230             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
3231 
3232         1D interpolation with a different method:
3233 
3234         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
3235         <xarray.Dataset>
3236         Dimensions:  (x: 4, y: 4)
3237         Coordinates:
3238           * y        (y) int64 10 12 14 16
3239           * x        (x) float64 0.0 0.75 1.25 1.75
3240         Data variables:
3241             a        (x) float64 5.0 7.0 7.0 4.0
3242             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
3243 
3244         1D extrapolation:
3245 
3246         >>> ds.interp(
3247         ...     x=[1, 1.5, 2.5, 3.5],
3248         ...     method="linear",
3249         ...     kwargs={"fill_value": "extrapolate"},
3250         ... )
3251         <xarray.Dataset>
3252         Dimensions:  (x: 4, y: 4)
3253         Coordinates:
3254           * y        (y) int64 10 12 14 16
3255           * x        (x) float64 1.0 1.5 2.5 3.5
3256         Data variables:
3257             a        (x) float64 7.0 5.5 2.5 -0.5
3258             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
3259 
3260         2D interpolation:
3261 
3262         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
3263         <xarray.Dataset>
3264         Dimensions:  (x: 4, y: 3)
3265         Coordinates:
3266           * x        (x) float64 0.0 0.75 1.25 1.75
3267           * y        (y) int64 11 13 15
3268         Data variables:
3269             a        (x) float64 5.0 6.5 6.25 4.75
3270             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
3271         """
3272         from . import missing
3273 
3274         if kwargs is None:
3275             kwargs = {}
3276 
3277         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
3278         indexers = dict(self._validate_interp_indexers(coords))
3279 
3280         if coords:
3281             # This avoids broadcasting over coordinates that are both in
3282             # the original array AND in the indexing array. It essentially
3283             # forces interpolation along the shared coordinates.
3284             sdims = (
3285                 set(self.dims)
3286                 .intersection(*[set(nx.dims) for nx in indexers.values()])
3287                 .difference(coords.keys())
3288             )
3289             indexers.update({d: self.variables[d] for d in sdims})
3290 
3291         obj = self if assume_sorted else self.sortby([k for k in coords])
3292 
3293         def maybe_variable(obj, k):
3294             # workaround to get variable for dimension without coordinate.
3295             try:
3296                 return obj._variables[k]
3297             except KeyError:
3298                 return as_variable((k, range(obj.dims[k])))
3299 
3300         def _validate_interp_indexer(x, new_x):
3301             # In the case of datetimes, the restrictions placed on indexers
3302             # used with interp are stronger than those which are placed on
3303             # isel, so we need an additional check after _validate_indexers.
3304             if _contains_datetime_like_objects(
3305                 x
3306             ) and not _contains_datetime_like_objects(new_x):
3307                 raise TypeError(
3308                     "When interpolating over a datetime-like "
3309                     "coordinate, the coordinates to "
3310                     "interpolate to must be either datetime "
3311                     "strings or datetimes. "
3312                     "Instead got\n{}".format(new_x)
3313                 )
3314             return x, new_x
3315 
3316         validated_indexers = {
3317             k: _validate_interp_indexer(maybe_variable(obj, k), v)
3318             for k, v in indexers.items()
3319         }
3320 
3321         # optimization: subset to coordinate range of the target index
3322         if method in ["linear", "nearest"]:
3323             for k, v in validated_indexers.items():
3324                 obj, newidx = missing._localize(obj, {k: v})
3325                 validated_indexers[k] = newidx[k]
3326 
3327         # optimization: create dask coordinate arrays once per Dataset
3328         # rather than once per Variable when dask.array.unify_chunks is called later
3329         # GH4739
3330         if obj.__dask_graph__():
3331             dask_indexers = {
3332                 k: (index.to_base_variable().chunk(), dest.to_base_variable().chunk())
3333                 for k, (index, dest) in validated_indexers.items()
3334             }
3335 
3336         variables: dict[Hashable, Variable] = {}
3337         reindex: bool = False
3338         for name, var in obj._variables.items():
3339             if name in indexers:
3340                 continue
3341 
3342             if is_duck_dask_array(var.data):
3343                 use_indexers = dask_indexers
3344             else:
3345                 use_indexers = validated_indexers
3346 
3347             dtype_kind = var.dtype.kind
3348             if dtype_kind in "uifc":
3349                 # For normal number types do the interpolation:
3350                 var_indexers = {k: v for k, v in use_indexers.items() if k in var.dims}
3351                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
3352             elif dtype_kind in "ObU" and (use_indexers.keys() & var.dims):
3353                 # For types that we do not understand do stepwise
3354                 # interpolation to avoid modifying the elements.
3355                 # reindex the variable instead because it supports
3356                 # booleans and objects and retains the dtype but inside
3357                 # this loop there might be some duplicate code that slows it
3358                 # down, therefore collect these signals and run it later:
3359                 reindex = True
3360             elif all(d not in indexers for d in var.dims):
3361                 # For anything else we can only keep variables if they
3362                 # are not dependent on any coords that are being
3363                 # interpolated along:
3364                 variables[name] = var
3365 
3366         if reindex:
3367             reindex_indexers = {
3368                 k: v for k, (_, v) in validated_indexers.items() if v.dims == (k,)
3369             }
3370             reindexed = alignment.reindex(
3371                 obj,
3372                 indexers=reindex_indexers,
3373                 method=method_non_numeric,
3374                 exclude_vars=variables.keys(),
3375             )
3376             indexes = dict(reindexed._indexes)
3377             variables.update(reindexed.variables)
3378         else:
3379             # Get the indexes that are not being interpolated along
3380             indexes = {k: v for k, v in obj._indexes.items() if k not in indexers}
3381 
3382         # Get the coords that also exist in the variables:
3383         coord_names = obj._coord_names & variables.keys()
3384         selected = self._replace_with_new_dims(
3385             variables.copy(), coord_names, indexes=indexes
3386         )
3387 
3388         # Attach indexer as coordinate
3389         for k, v in indexers.items():
3390             assert isinstance(v, Variable)
3391             if v.dims == (k,):
3392                 index = PandasIndex(v, k, coord_dtype=v.dtype)
3393                 index_vars = index.create_variables({k: v})
3394                 indexes[k] = index
3395                 variables.update(index_vars)
3396             else:
3397                 variables[k] = v
3398 
3399         # Extract coordinates from indexers
3400         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
3401         variables.update(coord_vars)
3402         indexes.update(new_indexes)
3403 
3404         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
3405         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3406 
3407     def interp_like(
3408         self,
3409         other: Dataset | DataArray,
3410         method: InterpOptions = "linear",
3411         assume_sorted: bool = False,
3412         kwargs: Mapping[str, Any] | None = None,
3413         method_non_numeric: str = "nearest",
3414     ) -> Dataset:
3415         """Interpolate this object onto the coordinates of another object,
3416         filling the out of range values with NaN.
3417 
3418         If interpolating along a single existing dimension,
3419         :py:class:`scipy.interpolate.interp1d` is called. When interpolating
3420         along multiple existing dimensions, an attempt is made to decompose the
3421         interpolation into multiple 1-dimensional interpolations. If this is
3422         possible, :py:class:`scipy.interpolate.interp1d` is called. Otherwise,
3423         :py:func:`scipy.interpolate.interpn` is called.
3424 
3425         Parameters
3426         ----------
3427         other : Dataset or DataArray
3428             Object with an 'indexes' attribute giving a mapping from dimension
3429             names to an 1d array-like, which provides coordinates upon
3430             which to index the variables in this dataset. Missing values are skipped.
3431         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
3432             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
3433             String indicating which method to use for interpolation:
3434 
3435             - 'linear': linear interpolation. Additional keyword
3436               arguments are passed to :py:func:`numpy.interp`
3437             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
3438               are passed to :py:func:`scipy.interpolate.interp1d`. If
3439               ``method='polynomial'``, the ``order`` keyword argument must also be
3440               provided.
3441             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
3442               respective :py:class:`scipy.interpolate` classes.
3443 
3444         assume_sorted : bool, default: False
3445             If False, values of coordinates that are interpolated over can be
3446             in any order and they are sorted first. If True, interpolated
3447             coordinates are assumed to be an array of monotonically increasing
3448             values.
3449         kwargs : dict, optional
3450             Additional keyword passed to scipy's interpolator.
3451         method_non_numeric : {"nearest", "pad", "ffill", "backfill", "bfill"}, optional
3452             Method for non-numeric types. Passed on to :py:meth:`Dataset.reindex`.
3453             ``"nearest"`` is used by default.
3454 
3455         Returns
3456         -------
3457         interpolated : Dataset
3458             Another dataset by interpolating this dataset's data along the
3459             coordinates of the other object.
3460 
3461         Notes
3462         -----
3463         scipy is required.
3464         If the dataset has object-type coordinates, reindex is used for these
3465         coordinates instead of the interpolation.
3466 
3467         See Also
3468         --------
3469         Dataset.interp
3470         Dataset.reindex_like
3471         """
3472         if kwargs is None:
3473             kwargs = {}
3474 
3475         # pick only dimension coordinates with a single index
3476         coords = {}
3477         other_indexes = other.xindexes
3478         for dim in self.dims:
3479             other_dim_coords = other_indexes.get_all_coords(dim, errors="ignore")
3480             if len(other_dim_coords) == 1:
3481                 coords[dim] = other_dim_coords[dim]
3482 
3483         numeric_coords: dict[Hashable, pd.Index] = {}
3484         object_coords: dict[Hashable, pd.Index] = {}
3485         for k, v in coords.items():
3486             if v.dtype.kind in "uifcMm":
3487                 numeric_coords[k] = v
3488             else:
3489                 object_coords[k] = v
3490 
3491         ds = self
3492         if object_coords:
3493             # We do not support interpolation along object coordinate.
3494             # reindex instead.
3495             ds = self.reindex(object_coords)
3496         return ds.interp(
3497             coords=numeric_coords,
3498             method=method,
3499             assume_sorted=assume_sorted,
3500             kwargs=kwargs,
3501             method_non_numeric=method_non_numeric,
3502         )
3503 
3504     # Helper methods for rename()
3505     def _rename_vars(
3506         self, name_dict, dims_dict
3507     ) -> tuple[dict[Hashable, Variable], set[Hashable]]:
3508         variables = {}
3509         coord_names = set()
3510         for k, v in self.variables.items():
3511             var = v.copy(deep=False)
3512             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3513             name = name_dict.get(k, k)
3514             if name in variables:
3515                 raise ValueError(f"the new name {name!r} conflicts")
3516             variables[name] = var
3517             if k in self._coord_names:
3518                 coord_names.add(name)
3519         return variables, coord_names
3520 
3521     def _rename_dims(self, name_dict: Mapping[Any, Hashable]) -> dict[Hashable, int]:
3522         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3523 
3524     def _rename_indexes(
3525         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3526     ) -> tuple[dict[Hashable, Index], dict[Hashable, Variable]]:
3527         if not self._indexes:
3528             return {}, {}
3529 
3530         indexes = {}
3531         variables = {}
3532 
3533         for index, coord_names in self.xindexes.group_by_index():
3534             new_index = index.rename(name_dict, dims_dict)
3535             new_coord_names = [name_dict.get(k, k) for k in coord_names]
3536             indexes.update({k: new_index for k in new_coord_names})
3537             new_index_vars = new_index.create_variables(
3538                 {
3539                     new: self._variables[old]
3540                     for old, new in zip(coord_names, new_coord_names)
3541                 }
3542             )
3543             variables.update(new_index_vars)
3544 
3545         return indexes, variables
3546 
3547     def _rename_all(
3548         self, name_dict: Mapping[Any, Hashable], dims_dict: Mapping[Any, Hashable]
3549     ) -> tuple[
3550         dict[Hashable, Variable],
3551         set[Hashable],
3552         dict[Hashable, int],
3553         dict[Hashable, Index],
3554     ]:
3555         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3556         dims = self._rename_dims(dims_dict)
3557 
3558         indexes, index_vars = self._rename_indexes(name_dict, dims_dict)
3559         variables = {k: index_vars.get(k, v) for k, v in variables.items()}
3560 
3561         return variables, coord_names, dims, indexes
3562 
3563     def rename(
3564         self: T_Dataset,
3565         name_dict: Mapping[Any, Hashable] | None = None,
3566         **names: Hashable,
3567     ) -> T_Dataset:
3568         """Returns a new object with renamed variables, coordinates and dimensions.
3569 
3570         Parameters
3571         ----------
3572         name_dict : dict-like, optional
3573             Dictionary whose keys are current variable, coordinate or dimension names and
3574             whose values are the desired names.
3575         **names : optional
3576             Keyword form of ``name_dict``.
3577             One of name_dict or names must be provided.
3578 
3579         Returns
3580         -------
3581         renamed : Dataset
3582             Dataset with renamed variables, coordinates and dimensions.
3583 
3584         See Also
3585         --------
3586         Dataset.swap_dims
3587         Dataset.rename_vars
3588         Dataset.rename_dims
3589         DataArray.rename
3590         """
3591         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3592         for k in name_dict.keys():
3593             if k not in self and k not in self.dims:
3594                 raise ValueError(
3595                     f"cannot rename {k!r} because it is not a "
3596                     "variable or dimension in this dataset"
3597                 )
3598 
3599         variables, coord_names, dims, indexes = self._rename_all(
3600             name_dict=name_dict, dims_dict=name_dict
3601         )
3602         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3603 
3604     def rename_dims(
3605         self: T_Dataset,
3606         dims_dict: Mapping[Any, Hashable] | None = None,
3607         **dims: Hashable,
3608     ) -> T_Dataset:
3609         """Returns a new object with renamed dimensions only.
3610 
3611         Parameters
3612         ----------
3613         dims_dict : dict-like, optional
3614             Dictionary whose keys are current dimension names and
3615             whose values are the desired names. The desired names must
3616             not be the name of an existing dimension or Variable in the Dataset.
3617         **dims : optional
3618             Keyword form of ``dims_dict``.
3619             One of dims_dict or dims must be provided.
3620 
3621         Returns
3622         -------
3623         renamed : Dataset
3624             Dataset with renamed dimensions.
3625 
3626         See Also
3627         --------
3628         Dataset.swap_dims
3629         Dataset.rename
3630         Dataset.rename_vars
3631         DataArray.rename
3632         """
3633         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3634         for k, v in dims_dict.items():
3635             if k not in self.dims:
3636                 raise ValueError(
3637                     f"cannot rename {k!r} because it is not a "
3638                     "dimension in this dataset"
3639                 )
3640             if v in self.dims or v in self:
3641                 raise ValueError(
3642                     f"Cannot rename {k} to {v} because {v} already exists. "
3643                     "Try using swap_dims instead."
3644                 )
3645 
3646         variables, coord_names, sizes, indexes = self._rename_all(
3647             name_dict={}, dims_dict=dims_dict
3648         )
3649         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3650 
3651     def rename_vars(
3652         self: T_Dataset, name_dict: Mapping[Any, Hashable] = None, **names: Hashable
3653     ) -> T_Dataset:
3654         """Returns a new object with renamed variables including coordinates
3655 
3656         Parameters
3657         ----------
3658         name_dict : dict-like, optional
3659             Dictionary whose keys are current variable or coordinate names and
3660             whose values are the desired names.
3661         **names : optional
3662             Keyword form of ``name_dict``.
3663             One of name_dict or names must be provided.
3664 
3665         Returns
3666         -------
3667         renamed : Dataset
3668             Dataset with renamed variables including coordinates
3669 
3670         See Also
3671         --------
3672         Dataset.swap_dims
3673         Dataset.rename
3674         Dataset.rename_dims
3675         DataArray.rename
3676         """
3677         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3678         for k in name_dict:
3679             if k not in self:
3680                 raise ValueError(
3681                     f"cannot rename {k!r} because it is not a "
3682                     "variable or coordinate in this dataset"
3683                 )
3684         variables, coord_names, dims, indexes = self._rename_all(
3685             name_dict=name_dict, dims_dict={}
3686         )
3687         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3688 
3689     def swap_dims(
3690         self: T_Dataset, dims_dict: Mapping[Any, Hashable] = None, **dims_kwargs
3691     ) -> T_Dataset:
3692         """Returns a new object with swapped dimensions.
3693 
3694         Parameters
3695         ----------
3696         dims_dict : dict-like
3697             Dictionary whose keys are current dimension names and whose values
3698             are new names.
3699         **dims_kwargs : {existing_dim: new_dim, ...}, optional
3700             The keyword arguments form of ``dims_dict``.
3701             One of dims_dict or dims_kwargs must be provided.
3702 
3703         Returns
3704         -------
3705         swapped : Dataset
3706             Dataset with swapped dimensions.
3707 
3708         Examples
3709         --------
3710         >>> ds = xr.Dataset(
3711         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3712         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3713         ... )
3714         >>> ds
3715         <xarray.Dataset>
3716         Dimensions:  (x: 2)
3717         Coordinates:
3718           * x        (x) <U1 'a' 'b'
3719             y        (x) int64 0 1
3720         Data variables:
3721             a        (x) int64 5 7
3722             b        (x) float64 0.1 2.4
3723 
3724         >>> ds.swap_dims({"x": "y"})
3725         <xarray.Dataset>
3726         Dimensions:  (y: 2)
3727         Coordinates:
3728             x        (y) <U1 'a' 'b'
3729           * y        (y) int64 0 1
3730         Data variables:
3731             a        (y) int64 5 7
3732             b        (y) float64 0.1 2.4
3733 
3734         >>> ds.swap_dims({"x": "z"})
3735         <xarray.Dataset>
3736         Dimensions:  (z: 2)
3737         Coordinates:
3738             x        (z) <U1 'a' 'b'
3739             y        (z) int64 0 1
3740         Dimensions without coordinates: z
3741         Data variables:
3742             a        (z) int64 5 7
3743             b        (z) float64 0.1 2.4
3744 
3745         See Also
3746         --------
3747         Dataset.rename
3748         DataArray.swap_dims
3749         """
3750         # TODO: deprecate this method in favor of a (less confusing)
3751         # rename_dims() method that only renames dimensions.
3752 
3753         dims_dict = either_dict_or_kwargs(dims_dict, dims_kwargs, "swap_dims")
3754         for k, v in dims_dict.items():
3755             if k not in self.dims:
3756                 raise ValueError(
3757                     f"cannot swap from dimension {k!r} because it is "
3758                     "not an existing dimension"
3759                 )
3760             if v in self.variables and self.variables[v].dims != (k,):
3761                 raise ValueError(
3762                     f"replacement dimension {v!r} is not a 1D "
3763                     f"variable along the old dimension {k!r}"
3764                 )
3765 
3766         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3767 
3768         coord_names = self._coord_names.copy()
3769         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3770 
3771         variables: dict[Hashable, Variable] = {}
3772         indexes: dict[Hashable, Index] = {}
3773         for k, v in self.variables.items():
3774             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3775             var: Variable
3776             if k in result_dims:
3777                 var = v.to_index_variable()
3778                 var.dims = dims
3779                 if k in self._indexes:
3780                     indexes[k] = self._indexes[k]
3781                     variables[k] = var
3782                 else:
3783                     index, index_vars = create_default_index_implicit(var)
3784                     indexes.update({name: index for name in index_vars})
3785                     variables.update(index_vars)
3786                     coord_names.update(index_vars)
3787             else:
3788                 var = v.to_base_variable()
3789                 var.dims = dims
3790                 variables[k] = var
3791 
3792         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3793 
3794     # change type of self and return to T_Dataset once
3795     # https://github.com/python/mypy/issues/12846 is resolved
3796     def expand_dims(
3797         self,
3798         dim: None | Hashable | Sequence[Hashable] | Mapping[Any, Any] = None,
3799         axis: None | int | Sequence[int] = None,
3800         **dim_kwargs: Any,
3801     ) -> Dataset:
3802         """Return a new object with an additional axis (or axes) inserted at
3803         the corresponding position in the array shape.  The new object is a
3804         view into the underlying array, not a copy.
3805 
3806         If dim is already a scalar coordinate, it will be promoted to a 1D
3807         coordinate consisting of a single value.
3808 
3809         Parameters
3810         ----------
3811         dim : hashable, sequence of hashable, mapping, or None
3812             Dimensions to include on the new variable. If provided as hashable
3813             or sequence of hashable, then dimensions are inserted with length
3814             1. If provided as a mapping, then the keys are the new dimensions
3815             and the values are either integers (giving the length of the new
3816             dimensions) or array-like (giving the coordinates of the new
3817             dimensions).
3818         axis : int, sequence of int, or None, default: None
3819             Axis position(s) where new axis is to be inserted (position(s) on
3820             the result array). If a sequence of integers is passed,
3821             multiple axes are inserted. In this case, dim arguments should be
3822             same length list. If axis=None is passed, all the axes will be
3823             inserted to the start of the result array.
3824         **dim_kwargs : int or sequence or ndarray
3825             The keywords are arbitrary dimensions being inserted and the values
3826             are either the lengths of the new dims (if int is given), or their
3827             coordinates. Note, this is an alternative to passing a dict to the
3828             dim kwarg and will only be used if dim is None.
3829 
3830         Returns
3831         -------
3832         expanded : Dataset
3833             This object, but with additional dimension(s).
3834 
3835         See Also
3836         --------
3837         DataArray.expand_dims
3838         """
3839         if dim is None:
3840             pass
3841         elif isinstance(dim, Mapping):
3842             # We're later going to modify dim in place; don't tamper with
3843             # the input
3844             dim = dict(dim)
3845         elif isinstance(dim, int):
3846             raise TypeError(
3847                 "dim should be hashable or sequence of hashables or mapping"
3848             )
3849         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3850             dim = {dim: 1}
3851         elif isinstance(dim, Sequence):
3852             if len(dim) != len(set(dim)):
3853                 raise ValueError("dims should not contain duplicate values.")
3854             dim = {d: 1 for d in dim}
3855 
3856         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3857         assert isinstance(dim, MutableMapping)
3858 
3859         if axis is None:
3860             axis = list(range(len(dim)))
3861         elif not isinstance(axis, Sequence):
3862             axis = [axis]
3863 
3864         if len(dim) != len(axis):
3865             raise ValueError("lengths of dim and axis should be identical.")
3866         for d in dim:
3867             if d in self.dims:
3868                 raise ValueError(f"Dimension {d} already exists.")
3869             if d in self._variables and not utils.is_scalar(self._variables[d]):
3870                 raise ValueError(
3871                     "{dim} already exists as coordinate or"
3872                     " variable name.".format(dim=d)
3873                 )
3874 
3875         variables: dict[Hashable, Variable] = {}
3876         indexes: dict[Hashable, Index] = dict(self._indexes)
3877         coord_names = self._coord_names.copy()
3878         # If dim is a dict, then ensure that the values are either integers
3879         # or iterables.
3880         for k, v in dim.items():
3881             if hasattr(v, "__iter__"):
3882                 # If the value for the new dimension is an iterable, then
3883                 # save the coordinates to the variables dict, and set the
3884                 # value within the dim dict to the length of the iterable
3885                 # for later use.
3886                 index = PandasIndex(v, k)
3887                 indexes[k] = index
3888                 variables.update(index.create_variables())
3889                 coord_names.add(k)
3890                 dim[k] = variables[k].size
3891             elif isinstance(v, int):
3892                 pass  # Do nothing if the dimensions value is just an int
3893             else:
3894                 raise TypeError(
3895                     "The value of new dimension {k} must be "
3896                     "an iterable or an int".format(k=k)
3897                 )
3898 
3899         for k, v in self._variables.items():
3900             if k not in dim:
3901                 if k in coord_names:  # Do not change coordinates
3902                     variables[k] = v
3903                 else:
3904                     result_ndim = len(v.dims) + len(axis)
3905                     for a in axis:
3906                         if a < -result_ndim or result_ndim - 1 < a:
3907                             raise IndexError(
3908                                 f"Axis {a} of variable {k} is out of bounds of the "
3909                                 f"expanded dimension size {result_ndim}"
3910                             )
3911 
3912                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3913                     if len(axis_pos) != len(set(axis_pos)):
3914                         raise ValueError("axis should not contain duplicate values")
3915                     # We need to sort them to make sure `axis` equals to the
3916                     # axis positions of the result array.
3917                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3918 
3919                     all_dims = list(zip(v.dims, v.shape))
3920                     for d, c in zip_axis_dim:
3921                         all_dims.insert(d, c)
3922                     variables[k] = v.set_dims(dict(all_dims))
3923             else:
3924                 if k not in variables:
3925                     # If dims includes a label of a non-dimension coordinate,
3926                     # it will be promoted to a 1D coordinate with a single value.
3927                     index, index_vars = create_default_index_implicit(v.set_dims(k))
3928                     indexes[k] = index
3929                     variables.update(index_vars)
3930 
3931         return self._replace_with_new_dims(
3932             variables, coord_names=coord_names, indexes=indexes
3933         )
3934 
3935     # change type of self and return to T_Dataset once
3936     # https://github.com/python/mypy/issues/12846 is resolved
3937     def set_index(
3938         self,
3939         indexes: Mapping[Any, Hashable | Sequence[Hashable]] | None = None,
3940         append: bool = False,
3941         **indexes_kwargs: Hashable | Sequence[Hashable],
3942     ) -> Dataset:
3943         """Set Dataset (multi-)indexes using one or more existing coordinates
3944         or variables.
3945 
3946         Parameters
3947         ----------
3948         indexes : {dim: index, ...}
3949             Mapping from names matching dimensions and values given
3950             by (lists of) the names of existing coordinates or variables to set
3951             as new (multi-)index.
3952         append : bool, default: False
3953             If True, append the supplied index(es) to the existing index(es).
3954             Otherwise replace the existing index(es) (default).
3955         **indexes_kwargs : optional
3956             The keyword arguments form of ``indexes``.
3957             One of indexes or indexes_kwargs must be provided.
3958 
3959         Returns
3960         -------
3961         obj : Dataset
3962             Another dataset, with this dataset's data but replaced coordinates.
3963 
3964         Examples
3965         --------
3966         >>> arr = xr.DataArray(
3967         ...     data=np.ones((2, 3)),
3968         ...     dims=["x", "y"],
3969         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3970         ... )
3971         >>> ds = xr.Dataset({"v": arr})
3972         >>> ds
3973         <xarray.Dataset>
3974         Dimensions:  (x: 2, y: 3)
3975         Coordinates:
3976           * x        (x) int64 0 1
3977           * y        (y) int64 0 1 2
3978             a        (x) int64 3 4
3979         Data variables:
3980             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3981         >>> ds.set_index(x="a")
3982         <xarray.Dataset>
3983         Dimensions:  (x: 2, y: 3)
3984         Coordinates:
3985           * x        (x) int64 3 4
3986           * y        (y) int64 0 1 2
3987         Data variables:
3988             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3989 
3990         See Also
3991         --------
3992         Dataset.reset_index
3993         Dataset.swap_dims
3994         """
3995         dim_coords = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3996 
3997         new_indexes: dict[Hashable, Index] = {}
3998         new_variables: dict[Hashable, IndexVariable] = {}
3999         maybe_drop_indexes: list[Hashable] = []
4000         drop_variables: list[Hashable] = []
4001         replace_dims: dict[Hashable, Hashable] = {}
4002 
4003         for dim, _var_names in dim_coords.items():
4004             if isinstance(_var_names, str) or not isinstance(_var_names, Sequence):
4005                 var_names = [_var_names]
4006             else:
4007                 var_names = list(_var_names)
4008 
4009             invalid_vars = set(var_names) - set(self._variables)
4010             if invalid_vars:
4011                 raise ValueError(
4012                     ", ".join([str(v) for v in invalid_vars])
4013                     + " variable(s) do not exist"
4014                 )
4015 
4016             current_coord_names = self.xindexes.get_all_coords(dim, errors="ignore")
4017 
4018             # drop any pre-existing index involved
4019             maybe_drop_indexes += list(current_coord_names) + var_names
4020             for k in var_names:
4021                 maybe_drop_indexes += list(
4022                     self.xindexes.get_all_coords(k, errors="ignore")
4023                 )
4024 
4025             drop_variables += var_names
4026 
4027             if len(var_names) == 1 and (not append or dim not in self._indexes):
4028                 var_name = var_names[0]
4029                 var = self._variables[var_name]
4030                 if var.dims != (dim,):
4031                     raise ValueError(
4032                         f"dimension mismatch: try setting an index for dimension {dim!r} with "
4033                         f"variable {var_name!r} that has dimensions {var.dims}"
4034                     )
4035                 idx = PandasIndex.from_variables({dim: var})
4036                 idx_vars = idx.create_variables({var_name: var})
4037             else:
4038                 if append:
4039                     current_variables = {
4040                         k: self._variables[k] for k in current_coord_names
4041                     }
4042                 else:
4043                     current_variables = {}
4044                 idx, idx_vars = PandasMultiIndex.from_variables_maybe_expand(
4045                     dim,
4046                     current_variables,
4047                     {k: self._variables[k] for k in var_names},
4048                 )
4049                 for n in idx.index.names:
4050                     replace_dims[n] = dim
4051 
4052             new_indexes.update({k: idx for k in idx_vars})
4053             new_variables.update(idx_vars)
4054 
4055         indexes_: dict[Any, Index] = {
4056             k: v for k, v in self._indexes.items() if k not in maybe_drop_indexes
4057         }
4058         indexes_.update(new_indexes)
4059 
4060         variables = {
4061             k: v for k, v in self._variables.items() if k not in drop_variables
4062         }
4063         variables.update(new_variables)
4064 
4065         # update dimensions if necessary, GH: 3512
4066         for k, v in variables.items():
4067             if any(d in replace_dims for d in v.dims):
4068                 new_dims = [replace_dims.get(d, d) for d in v.dims]
4069                 variables[k] = v._replace(dims=new_dims)
4070 
4071         coord_names = self._coord_names - set(drop_variables) | set(new_variables)
4072 
4073         return self._replace_with_new_dims(
4074             variables, coord_names=coord_names, indexes=indexes_
4075         )
4076 
4077     def reset_index(
4078         self: T_Dataset,
4079         dims_or_levels: Hashable | Sequence[Hashable],
4080         drop: bool = False,
4081     ) -> T_Dataset:
4082         """Reset the specified index(es) or multi-index level(s).
4083 
4084         Parameters
4085         ----------
4086         dims_or_levels : Hashable or Sequence of Hashable
4087             Name(s) of the dimension(s) and/or multi-index level(s) that will
4088             be reset.
4089         drop : bool, default: False
4090             If True, remove the specified indexes and/or multi-index levels
4091             instead of extracting them as new coordinates (default: False).
4092 
4093         Returns
4094         -------
4095         obj : Dataset
4096             Another dataset, with this dataset's data but replaced coordinates.
4097 
4098         See Also
4099         --------
4100         Dataset.set_index
4101         """
4102         if isinstance(dims_or_levels, str) or not isinstance(dims_or_levels, Sequence):
4103             dims_or_levels = [dims_or_levels]
4104 
4105         invalid_coords = set(dims_or_levels) - set(self._indexes)
4106         if invalid_coords:
4107             raise ValueError(
4108                 f"{tuple(invalid_coords)} are not coordinates with an index"
4109             )
4110 
4111         drop_indexes: list[Hashable] = []
4112         drop_variables: list[Hashable] = []
4113         replaced_indexes: list[PandasMultiIndex] = []
4114         new_indexes: dict[Hashable, Index] = {}
4115         new_variables: dict[Hashable, IndexVariable] = {}
4116 
4117         for name in dims_or_levels:
4118             index = self._indexes[name]
4119             drop_indexes += list(self.xindexes.get_all_coords(name))
4120 
4121             if isinstance(index, PandasMultiIndex) and name not in self.dims:
4122                 # special case for pd.MultiIndex (name is an index level):
4123                 # replace by a new index with dropped level(s) instead of just drop the index
4124                 if index not in replaced_indexes:
4125                     level_names = index.index.names
4126                     level_vars = {
4127                         k: self._variables[k]
4128                         for k in level_names
4129                         if k not in dims_or_levels
4130                     }
4131                     if level_vars:
4132                         idx = index.keep_levels(level_vars)
4133                         idx_vars = idx.create_variables(level_vars)
4134                         new_indexes.update({k: idx for k in idx_vars})
4135                         new_variables.update(idx_vars)
4136                 replaced_indexes.append(index)
4137 
4138             if drop:
4139                 drop_variables.append(name)
4140 
4141         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4142         indexes.update(new_indexes)
4143 
4144         variables = {
4145             k: v for k, v in self._variables.items() if k not in drop_variables
4146         }
4147         variables.update(new_variables)
4148 
4149         coord_names = set(new_variables) | self._coord_names
4150 
4151         return self._replace(variables, coord_names=coord_names, indexes=indexes)
4152 
4153     def reorder_levels(
4154         self: T_Dataset,
4155         dim_order: Mapping[Any, Sequence[int | Hashable]] | None = None,
4156         **dim_order_kwargs: Sequence[int | Hashable],
4157     ) -> T_Dataset:
4158         """Rearrange index levels using input order.
4159 
4160         Parameters
4161         ----------
4162         dim_order : dict-like of Hashable to Sequence of int or Hashable, optional
4163             Mapping from names matching dimensions and values given
4164             by lists representing new level orders. Every given dimension
4165             must have a multi-index.
4166         **dim_order_kwargs : Sequence of int or Hashable, optional
4167             The keyword arguments form of ``dim_order``.
4168             One of dim_order or dim_order_kwargs must be provided.
4169 
4170         Returns
4171         -------
4172         obj : Dataset
4173             Another dataset, with this dataset's data but replaced
4174             coordinates.
4175         """
4176         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
4177         variables = self._variables.copy()
4178         indexes = dict(self._indexes)
4179         new_indexes: dict[Hashable, Index] = {}
4180         new_variables: dict[Hashable, IndexVariable] = {}
4181 
4182         for dim, order in dim_order.items():
4183             index = self._indexes[dim]
4184 
4185             if not isinstance(index, PandasMultiIndex):
4186                 raise ValueError(f"coordinate {dim} has no MultiIndex")
4187 
4188             level_vars = {k: self._variables[k] for k in order}
4189             idx = index.reorder_levels(level_vars)
4190             idx_vars = idx.create_variables(level_vars)
4191             new_indexes.update({k: idx for k in idx_vars})
4192             new_variables.update(idx_vars)
4193 
4194         indexes = {k: v for k, v in self._indexes.items() if k not in new_indexes}
4195         indexes.update(new_indexes)
4196 
4197         variables = {k: v for k, v in self._variables.items() if k not in new_variables}
4198         variables.update(new_variables)
4199 
4200         return self._replace(variables, indexes=indexes)
4201 
4202     def _get_stack_index(
4203         self,
4204         dim,
4205         multi=False,
4206         create_index=False,
4207     ) -> tuple[Index | None, dict[Hashable, Variable]]:
4208         """Used by stack and unstack to get one pandas (multi-)index among
4209         the indexed coordinates along dimension `dim`.
4210 
4211         If exactly one index is found, return it with its corresponding
4212         coordinate variables(s), otherwise return None and an empty dict.
4213 
4214         If `create_index=True`, create a new index if none is found or raise
4215         an error if multiple indexes are found.
4216 
4217         """
4218         stack_index: Index | None = None
4219         stack_coords: dict[Hashable, Variable] = {}
4220 
4221         for name, index in self._indexes.items():
4222             var = self._variables[name]
4223             if (
4224                 var.ndim == 1
4225                 and var.dims[0] == dim
4226                 and (
4227                     # stack: must be a single coordinate index
4228                     not multi
4229                     and not self.xindexes.is_multi(name)
4230                     # unstack: must be an index that implements .unstack
4231                     or multi
4232                     and type(index).unstack is not Index.unstack
4233                 )
4234             ):
4235                 if stack_index is not None and index is not stack_index:
4236                     # more than one index found, stop
4237                     if create_index:
4238                         raise ValueError(
4239                             f"cannot stack dimension {dim!r} with `create_index=True` "
4240                             "and with more than one index found along that dimension"
4241                         )
4242                     return None, {}
4243                 stack_index = index
4244                 stack_coords[name] = var
4245 
4246         if create_index and stack_index is None:
4247             if dim in self._variables:
4248                 var = self._variables[dim]
4249             else:
4250                 _, _, var = _get_virtual_variable(self._variables, dim, self.dims)
4251             # dummy index (only `stack_coords` will be used to construct the multi-index)
4252             stack_index = PandasIndex([0], dim)
4253             stack_coords = {dim: var}
4254 
4255         return stack_index, stack_coords
4256 
4257     def _stack_once(
4258         self: T_Dataset,
4259         dims: Sequence[Hashable | Ellipsis],
4260         new_dim: Hashable,
4261         index_cls: type[Index],
4262         create_index: bool | None = True,
4263     ) -> T_Dataset:
4264         if dims == ...:
4265             raise ValueError("Please use [...] for dims, rather than just ...")
4266         if ... in dims:
4267             dims = list(infix_dims(dims, self.dims))
4268 
4269         new_variables: dict[Hashable, Variable] = {}
4270         stacked_var_names: list[Hashable] = []
4271         drop_indexes: list[Hashable] = []
4272 
4273         for name, var in self.variables.items():
4274             if any(d in var.dims for d in dims):
4275                 add_dims = [d for d in dims if d not in var.dims]
4276                 vdims = list(var.dims) + add_dims
4277                 shape = [self.dims[d] for d in vdims]
4278                 exp_var = var.set_dims(vdims, shape)
4279                 stacked_var = exp_var.stack(**{new_dim: dims})
4280                 new_variables[name] = stacked_var
4281                 stacked_var_names.append(name)
4282             else:
4283                 new_variables[name] = var.copy(deep=False)
4284 
4285         # drop indexes of stacked coordinates (if any)
4286         for name in stacked_var_names:
4287             drop_indexes += list(self.xindexes.get_all_coords(name, errors="ignore"))
4288 
4289         new_indexes = {}
4290         new_coord_names = set(self._coord_names)
4291         if create_index or create_index is None:
4292             product_vars: dict[Any, Variable] = {}
4293             for dim in dims:
4294                 idx, idx_vars = self._get_stack_index(dim, create_index=create_index)
4295                 if idx is not None:
4296                     product_vars.update(idx_vars)
4297 
4298             if len(product_vars) == len(dims):
4299                 idx = index_cls.stack(product_vars, new_dim)
4300                 new_indexes[new_dim] = idx
4301                 new_indexes.update({k: idx for k in product_vars})
4302                 idx_vars = idx.create_variables(product_vars)
4303                 # keep consistent multi-index coordinate order
4304                 for k in idx_vars:
4305                     new_variables.pop(k, None)
4306                 new_variables.update(idx_vars)
4307                 new_coord_names.update(idx_vars)
4308 
4309         indexes = {k: v for k, v in self._indexes.items() if k not in drop_indexes}
4310         indexes.update(new_indexes)
4311 
4312         return self._replace_with_new_dims(
4313             new_variables, coord_names=new_coord_names, indexes=indexes
4314         )
4315 
4316     def stack(
4317         self: T_Dataset,
4318         dimensions: Mapping[Any, Sequence[Hashable | Ellipsis]] | None = None,
4319         create_index: bool | None = True,
4320         index_cls: type[Index] = PandasMultiIndex,
4321         **dimensions_kwargs: Sequence[Hashable | Ellipsis],
4322     ) -> T_Dataset:
4323         """
4324         Stack any number of existing dimensions into a single new dimension.
4325 
4326         New dimensions will be added at the end, and by default the corresponding
4327         coordinate variables will be combined into a MultiIndex.
4328 
4329         Parameters
4330         ----------
4331         dimensions : mapping of hashable to sequence of hashable
4332             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
4333             dimensions, and the existing dimensions that they replace. An
4334             ellipsis (`...`) will be replaced by all unlisted dimensions.
4335             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
4336             all dimensions.
4337         create_index : bool or None, default: True
4338 
4339             - True: create a multi-index for each of the stacked dimensions.
4340             - False: don't create any index.
4341             - None. create a multi-index only if exactly one single (1-d) coordinate
4342               index is found for every dimension to stack.
4343 
4344         index_cls: Index-class, default: PandasMultiIndex
4345             Can be used to pass a custom multi-index type (must be an Xarray index that
4346             implements `.stack()`). By default, a pandas multi-index wrapper is used.
4347         **dimensions_kwargs
4348             The keyword arguments form of ``dimensions``.
4349             One of dimensions or dimensions_kwargs must be provided.
4350 
4351         Returns
4352         -------
4353         stacked : Dataset
4354             Dataset with stacked data.
4355 
4356         See Also
4357         --------
4358         Dataset.unstack
4359         """
4360         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
4361         result = self
4362         for new_dim, dims in dimensions.items():
4363             result = result._stack_once(dims, new_dim, index_cls, create_index)
4364         return result
4365 
4366     def to_stacked_array(
4367         self,
4368         new_dim: Hashable,
4369         sample_dims: Collection[Hashable],
4370         variable_dim: Hashable = "variable",
4371         name: Hashable | None = None,
4372     ) -> DataArray:
4373         """Combine variables of differing dimensionality into a DataArray
4374         without broadcasting.
4375 
4376         This method is similar to Dataset.to_array but does not broadcast the
4377         variables.
4378 
4379         Parameters
4380         ----------
4381         new_dim : hashable
4382             Name of the new stacked coordinate
4383         sample_dims : Collection of hashables
4384             List of dimensions that **will not** be stacked. Each array in the
4385             dataset must share these dimensions. For machine learning
4386             applications, these define the dimensions over which samples are
4387             drawn.
4388         variable_dim : hashable, default: "variable"
4389             Name of the level in the stacked coordinate which corresponds to
4390             the variables.
4391         name : hashable, optional
4392             Name of the new data array.
4393 
4394         Returns
4395         -------
4396         stacked : DataArray
4397             DataArray with the specified dimensions and data variables
4398             stacked together. The stacked coordinate is named ``new_dim``
4399             and represented by a MultiIndex object with a level containing the
4400             data variable names. The name of this level is controlled using
4401             the ``variable_dim`` argument.
4402 
4403         See Also
4404         --------
4405         Dataset.to_array
4406         Dataset.stack
4407         DataArray.to_unstacked_dataset
4408 
4409         Examples
4410         --------
4411         >>> data = xr.Dataset(
4412         ...     data_vars={
4413         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
4414         ...         "b": ("x", [6, 7]),
4415         ...     },
4416         ...     coords={"y": ["u", "v", "w"]},
4417         ... )
4418 
4419         >>> data
4420         <xarray.Dataset>
4421         Dimensions:  (x: 2, y: 3)
4422         Coordinates:
4423           * y        (y) <U1 'u' 'v' 'w'
4424         Dimensions without coordinates: x
4425         Data variables:
4426             a        (x, y) int64 0 1 2 3 4 5
4427             b        (x) int64 6 7
4428 
4429         >>> data.to_stacked_array("z", sample_dims=["x"])
4430         <xarray.DataArray 'a' (x: 2, z: 4)>
4431         array([[0, 1, 2, 6],
4432                [3, 4, 5, 7]])
4433         Coordinates:
4434           * z         (z) object MultiIndex
4435           * variable  (z) object 'a' 'a' 'a' 'b'
4436           * y         (z) object 'u' 'v' 'w' nan
4437         Dimensions without coordinates: x
4438 
4439         """
4440         from .concat import concat
4441 
4442         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
4443 
4444         for variable in self:
4445             dims = self[variable].dims
4446             dims_include_sample_dims = set(sample_dims) <= set(dims)
4447             if not dims_include_sample_dims:
4448                 raise ValueError(
4449                     "All variables in the dataset must contain the "
4450                     "dimensions {}.".format(dims)
4451                 )
4452 
4453         def ensure_stackable(val):
4454             assign_coords = {variable_dim: val.name}
4455             for dim in stacking_dims:
4456                 if dim not in val.dims:
4457                     assign_coords[dim] = None
4458 
4459             expand_dims = set(stacking_dims).difference(set(val.dims))
4460             expand_dims.add(variable_dim)
4461             # must be list for .expand_dims
4462             expand_dims = list(expand_dims)
4463 
4464             return (
4465                 val.assign_coords(**assign_coords)
4466                 .expand_dims(expand_dims)
4467                 .stack({new_dim: (variable_dim,) + stacking_dims})
4468             )
4469 
4470         # concatenate the arrays
4471         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
4472         data_array = concat(stackable_vars, dim=new_dim)
4473 
4474         if name is not None:
4475             data_array.name = name
4476 
4477         return data_array
4478 
4479     def _unstack_once(
4480         self: T_Dataset,
4481         dim: Hashable,
4482         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4483         fill_value,
4484         sparse: bool = False,
4485     ) -> T_Dataset:
4486         index, index_vars = index_and_vars
4487         variables: dict[Hashable, Variable] = {}
4488         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4489 
4490         new_indexes, clean_index = index.unstack()
4491         indexes.update(new_indexes)
4492 
4493         for name, idx in new_indexes.items():
4494             variables.update(idx.create_variables(index_vars))
4495 
4496         for name, var in self.variables.items():
4497             if name not in index_vars:
4498                 if dim in var.dims:
4499                     if isinstance(fill_value, Mapping):
4500                         fill_value_ = fill_value[name]
4501                     else:
4502                         fill_value_ = fill_value
4503 
4504                     variables[name] = var._unstack_once(
4505                         index=clean_index,
4506                         dim=dim,
4507                         fill_value=fill_value_,
4508                         sparse=sparse,
4509                     )
4510                 else:
4511                     variables[name] = var
4512 
4513         coord_names = set(self._coord_names) - {dim} | set(new_indexes)
4514 
4515         return self._replace_with_new_dims(
4516             variables, coord_names=coord_names, indexes=indexes
4517         )
4518 
4519     def _unstack_full_reindex(
4520         self: T_Dataset,
4521         dim: Hashable,
4522         index_and_vars: tuple[Index, dict[Hashable, Variable]],
4523         fill_value,
4524         sparse: bool,
4525     ) -> T_Dataset:
4526         index, index_vars = index_and_vars
4527         variables: dict[Hashable, Variable] = {}
4528         indexes = {k: v for k, v in self._indexes.items() if k != dim}
4529 
4530         new_indexes, clean_index = index.unstack()
4531         indexes.update(new_indexes)
4532 
4533         new_index_variables = {}
4534         for name, idx in new_indexes.items():
4535             new_index_variables.update(idx.create_variables(index_vars))
4536 
4537         new_dim_sizes = {k: v.size for k, v in new_index_variables.items()}
4538         variables.update(new_index_variables)
4539 
4540         # take a shortcut in case the MultiIndex was not modified.
4541         full_idx = pd.MultiIndex.from_product(
4542             clean_index.levels, names=clean_index.names
4543         )
4544         if clean_index.equals(full_idx):
4545             obj = self
4546         else:
4547             # TODO: we may depreciate implicit re-indexing with a pandas.MultiIndex
4548             xr_full_idx = PandasMultiIndex(full_idx, dim)
4549             indexers = Indexes(
4550                 {k: xr_full_idx for k in index_vars},
4551                 xr_full_idx.create_variables(index_vars),
4552             )
4553             obj = self._reindex(
4554                 indexers, copy=False, fill_value=fill_value, sparse=sparse
4555             )
4556 
4557         for name, var in obj.variables.items():
4558             if name not in index_vars:
4559                 if dim in var.dims:
4560                     variables[name] = var.unstack({dim: new_dim_sizes})
4561                 else:
4562                     variables[name] = var
4563 
4564         coord_names = set(self._coord_names) - {dim} | set(new_dim_sizes)
4565 
4566         return self._replace_with_new_dims(
4567             variables, coord_names=coord_names, indexes=indexes
4568         )
4569 
4570     def unstack(
4571         self: T_Dataset,
4572         dim: Hashable | Iterable[Hashable] | None = None,
4573         fill_value: Any = xrdtypes.NA,
4574         sparse: bool = False,
4575     ) -> T_Dataset:
4576         """
4577         Unstack existing dimensions corresponding to MultiIndexes into
4578         multiple new dimensions.
4579 
4580         New dimensions will be added at the end.
4581 
4582         Parameters
4583         ----------
4584         dim : hashable or iterable of hashable, optional
4585             Dimension(s) over which to unstack. By default unstacks all
4586             MultiIndexes.
4587         fill_value : scalar or dict-like, default: nan
4588             value to be filled. If a dict-like, maps variable names to
4589             fill values. If not provided or if the dict-like does not
4590             contain all variables, the dtype's NA value will be used.
4591         sparse : bool, default: False
4592             use sparse-array if True
4593 
4594         Returns
4595         -------
4596         unstacked : Dataset
4597             Dataset with unstacked data.
4598 
4599         See Also
4600         --------
4601         Dataset.stack
4602         """
4603 
4604         if dim is None:
4605             dims = list(self.dims)
4606         else:
4607             if isinstance(dim, str) or not isinstance(dim, Iterable):
4608                 dims = [dim]
4609             else:
4610                 dims = list(dim)
4611 
4612             missing_dims = [d for d in dims if d not in self.dims]
4613             if missing_dims:
4614                 raise ValueError(
4615                     f"Dataset does not contain the dimensions: {missing_dims}"
4616                 )
4617 
4618         # each specified dimension must have exactly one multi-index
4619         stacked_indexes: dict[Any, tuple[Index, dict[Hashable, Variable]]] = {}
4620         for d in dims:
4621             idx, idx_vars = self._get_stack_index(d, multi=True)
4622             if idx is not None:
4623                 stacked_indexes[d] = idx, idx_vars
4624 
4625         if dim is None:
4626             dims = list(stacked_indexes)
4627         else:
4628             non_multi_dims = set(dims) - set(stacked_indexes)
4629             if non_multi_dims:
4630                 raise ValueError(
4631                     "cannot unstack dimensions that do not "
4632                     f"have exactly one multi-index: {tuple(non_multi_dims)}"
4633                 )
4634 
4635         result = self.copy(deep=False)
4636 
4637         # we want to avoid allocating an object-dtype ndarray for a MultiIndex,
4638         # so we can't just access self.variables[v].data for every variable.
4639         # We only check the non-index variables.
4640         # https://github.com/pydata/xarray/issues/5902
4641         nonindexes = [
4642             self.variables[k] for k in set(self.variables) - set(self._indexes)
4643         ]
4644         # Notes for each of these cases:
4645         # 1. Dask arrays don't support assignment by index, which the fast unstack
4646         #    function requires.
4647         #    https://github.com/pydata/xarray/pull/4746#issuecomment-753282125
4648         # 2. Sparse doesn't currently support (though we could special-case it)
4649         #    https://github.com/pydata/sparse/issues/422
4650         # 3. pint requires checking if it's a NumPy array until
4651         #    https://github.com/pydata/xarray/pull/4751 is resolved,
4652         #    Once that is resolved, explicitly exclude pint arrays.
4653         #    pint doesn't implement `np.full_like` in a way that's
4654         #    currently compatible.
4655         needs_full_reindex = any(
4656             is_duck_dask_array(v.data)
4657             or isinstance(v.data, sparse_array_type)
4658             or not isinstance(v.data, np.ndarray)
4659             for v in nonindexes
4660         )
4661 
4662         for dim in dims:
4663             if needs_full_reindex:
4664                 result = result._unstack_full_reindex(
4665                     dim, stacked_indexes[dim], fill_value, sparse
4666                 )
4667             else:
4668                 result = result._unstack_once(
4669                     dim, stacked_indexes[dim], fill_value, sparse
4670                 )
4671         return result
4672 
4673     def update(self: T_Dataset, other: CoercibleMapping) -> T_Dataset:
4674         """Update this dataset's variables with those from another dataset.
4675 
4676         Just like :py:meth:`dict.update` this is a in-place operation.
4677         For a non-inplace version, see :py:meth:`Dataset.merge`.
4678 
4679         Parameters
4680         ----------
4681         other : Dataset or mapping
4682             Variables with which to update this dataset. One of:
4683 
4684             - Dataset
4685             - mapping {var name: DataArray}
4686             - mapping {var name: Variable}
4687             - mapping {var name: (dimension name, array-like)}
4688             - mapping {var name: (tuple of dimension names, array-like)}
4689 
4690         Returns
4691         -------
4692         updated : Dataset
4693             Updated dataset. Note that since the update is in-place this is the input
4694             dataset.
4695 
4696             It is deprecated since version 0.17 and scheduled to be removed in 0.21.
4697 
4698         Raises
4699         ------
4700         ValueError
4701             If any dimensions would have inconsistent sizes in the updated
4702             dataset.
4703 
4704         See Also
4705         --------
4706         Dataset.assign
4707         Dataset.merge
4708         """
4709         merge_result = dataset_update_method(self, other)
4710         return self._replace(inplace=True, **merge_result._asdict())
4711 
4712     def merge(
4713         self: T_Dataset,
4714         other: CoercibleMapping | DataArray,
4715         overwrite_vars: Hashable | Iterable[Hashable] = frozenset(),
4716         compat: CompatOptions = "no_conflicts",
4717         join: JoinOptions = "outer",
4718         fill_value: Any = xrdtypes.NA,
4719         combine_attrs: CombineAttrsOptions = "override",
4720     ) -> T_Dataset:
4721         """Merge the arrays of two datasets into a single dataset.
4722 
4723         This method generally does not allow for overriding data, with the
4724         exception of attributes, which are ignored on the second dataset.
4725         Variables with the same name are checked for conflicts via the equals
4726         or identical methods.
4727 
4728         Parameters
4729         ----------
4730         other : Dataset or mapping
4731             Dataset or variables to merge with this dataset.
4732         overwrite_vars : hashable or iterable of hashable, optional
4733             If provided, update variables of these name(s) without checking for
4734             conflicts in this dataset.
4735         compat : {"identical", "equals", "broadcast_equals", \
4736                   "no_conflicts", "override", "minimal"}, default: "no_conflicts"
4737             String indicating how to compare variables of the same name for
4738             potential conflicts:
4739 
4740             - 'identical': all values, dimensions and attributes must be the
4741               same.
4742             - 'equals': all values and dimensions must be the same.
4743             - 'broadcast_equals': all values must be equal when variables are
4744               broadcast against each other to ensure common dimensions.
4745             - 'no_conflicts': only values which are not null in both datasets
4746               must be equal. The returned dataset then contains the combination
4747               of all non-null values.
4748             - 'override': skip comparing and pick variable from first dataset
4749             - 'minimal': drop conflicting coordinates
4750 
4751         join : {"outer", "inner", "left", "right", "exact", "override"}, \
4752                default: "outer"
4753             Method for joining ``self`` and ``other`` along shared dimensions:
4754 
4755             - 'outer': use the union of the indexes
4756             - 'inner': use the intersection of the indexes
4757             - 'left': use indexes from ``self``
4758             - 'right': use indexes from ``other``
4759             - 'exact': error instead of aligning non-equal indexes
4760             - 'override': use indexes from ``self`` that are the same size
4761               as those of ``other`` in that dimension
4762 
4763         fill_value : scalar or dict-like, optional
4764             Value to use for newly missing values. If a dict-like, maps
4765             variable names (including coordinates) to fill values.
4766         combine_attrs : {"drop", "identical", "no_conflicts", "drop_conflicts", \
4767                          "override"} or callable, default: "override"
4768             A callable or a string indicating how to combine attrs of the objects being
4769             merged:
4770 
4771             - "drop": empty attrs on returned Dataset.
4772             - "identical": all attrs must be the same on every object.
4773             - "no_conflicts": attrs from all objects are combined, any that have
4774               the same name must also have the same value.
4775             - "drop_conflicts": attrs from all objects are combined, any that have
4776               the same name but different values are dropped.
4777             - "override": skip comparing and copy attrs from the first dataset to
4778               the result.
4779 
4780             If a callable, it must expect a sequence of ``attrs`` dicts and a context object
4781             as its only parameters.
4782 
4783         Returns
4784         -------
4785         merged : Dataset
4786             Merged dataset.
4787 
4788         Raises
4789         ------
4790         MergeError
4791             If any variables conflict (see ``compat``).
4792 
4793         See Also
4794         --------
4795         Dataset.update
4796         """
4797         from .dataarray import DataArray
4798 
4799         other = other.to_dataset() if isinstance(other, DataArray) else other
4800         merge_result = dataset_merge_method(
4801             self,
4802             other,
4803             overwrite_vars=overwrite_vars,
4804             compat=compat,
4805             join=join,
4806             fill_value=fill_value,
4807             combine_attrs=combine_attrs,
4808         )
4809         return self._replace(**merge_result._asdict())
4810 
4811     def _assert_all_in_dataset(
4812         self, names: Iterable[Hashable], virtual_okay: bool = False
4813     ) -> None:
4814         bad_names = set(names) - set(self._variables)
4815         if virtual_okay:
4816             bad_names -= self.virtual_variables
4817         if bad_names:
4818             raise ValueError(
4819                 "One or more of the specified variables "
4820                 "cannot be found in this dataset"
4821             )
4822 
4823     def drop_vars(
4824         self: T_Dataset,
4825         names: Hashable | Iterable[Hashable],
4826         *,
4827         errors: ErrorOptions = "raise",
4828     ) -> T_Dataset:
4829         """Drop variables from this dataset.
4830 
4831         Parameters
4832         ----------
4833         names : hashable or iterable of hashable
4834             Name(s) of variables to drop.
4835         errors : {"raise", "ignore"}, default: "raise"
4836             If 'raise', raises a ValueError error if any of the variable
4837             passed are not in the dataset. If 'ignore', any given names that are in the
4838             dataset are dropped and no error is raised.
4839 
4840         Returns
4841         -------
4842         dropped : Dataset
4843 
4844         """
4845         # the Iterable check is required for mypy
4846         if is_scalar(names) or not isinstance(names, Iterable):
4847             names = {names}
4848         else:
4849             names = set(names)
4850         if errors == "raise":
4851             self._assert_all_in_dataset(names)
4852 
4853         # GH6505
4854         other_names = set()
4855         for var in names:
4856             maybe_midx = self._indexes.get(var, None)
4857             if isinstance(maybe_midx, PandasMultiIndex):
4858                 idx_coord_names = set(maybe_midx.index.names + [maybe_midx.dim])
4859                 idx_other_names = idx_coord_names - set(names)
4860                 other_names.update(idx_other_names)
4861         if other_names:
4862             names |= set(other_names)
4863             warnings.warn(
4864                 f"Deleting a single level of a MultiIndex is deprecated. Previously, this deleted all levels of a MultiIndex. "
4865                 f"Please also drop the following variables: {other_names!r} to avoid an error in the future.",
4866                 DeprecationWarning,
4867                 stacklevel=2,
4868             )
4869 
4870         assert_no_index_corrupted(self.xindexes, names)
4871 
4872         variables = {k: v for k, v in self._variables.items() if k not in names}
4873         coord_names = {k for k in self._coord_names if k in variables}
4874         indexes = {k: v for k, v in self._indexes.items() if k not in names}
4875         return self._replace_with_new_dims(
4876             variables, coord_names=coord_names, indexes=indexes
4877         )
4878 
4879     def drop(
4880         self: T_Dataset,
4881         labels=None,
4882         dim=None,
4883         *,
4884         errors: ErrorOptions = "raise",
4885         **labels_kwargs,
4886     ) -> T_Dataset:
4887         """Backward compatible method based on `drop_vars` and `drop_sel`
4888 
4889         Using either `drop_vars` or `drop_sel` is encouraged
4890 
4891         See Also
4892         --------
4893         Dataset.drop_vars
4894         Dataset.drop_sel
4895         """
4896         if errors not in ["raise", "ignore"]:
4897             raise ValueError('errors must be either "raise" or "ignore"')
4898 
4899         if is_dict_like(labels) and not isinstance(labels, dict):
4900             warnings.warn(
4901                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
4902                 FutureWarning,
4903                 stacklevel=2,
4904             )
4905             return self.drop_vars(labels, errors=errors)
4906 
4907         if labels_kwargs or isinstance(labels, dict):
4908             if dim is not None:
4909                 raise ValueError("cannot specify dim and dict-like arguments.")
4910             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
4911 
4912         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
4913             warnings.warn(
4914                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
4915                 PendingDeprecationWarning,
4916                 stacklevel=2,
4917             )
4918             return self.drop_vars(labels, errors=errors)
4919         if dim is not None:
4920             warnings.warn(
4921                 "dropping labels using list-like labels is deprecated; using "
4922                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
4923                 DeprecationWarning,
4924                 stacklevel=2,
4925             )
4926             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
4927 
4928         warnings.warn(
4929             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
4930             PendingDeprecationWarning,
4931             stacklevel=2,
4932         )
4933         return self.drop_sel(labels, errors=errors)
4934 
4935     def drop_sel(
4936         self: T_Dataset, labels=None, *, errors: ErrorOptions = "raise", **labels_kwargs
4937     ) -> T_Dataset:
4938         """Drop index labels from this dataset.
4939 
4940         Parameters
4941         ----------
4942         labels : mapping of hashable to Any
4943             Index labels to drop
4944         errors : {"raise", "ignore"}, default: "raise"
4945             If 'raise', raises a ValueError error if
4946             any of the index labels passed are not
4947             in the dataset. If 'ignore', any given labels that are in the
4948             dataset are dropped and no error is raised.
4949         **labels_kwargs : {dim: label, ...}, optional
4950             The keyword arguments form of ``dim`` and ``labels``
4951 
4952         Returns
4953         -------
4954         dropped : Dataset
4955 
4956         Examples
4957         --------
4958         >>> data = np.arange(6).reshape(2, 3)
4959         >>> labels = ["a", "b", "c"]
4960         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4961         >>> ds
4962         <xarray.Dataset>
4963         Dimensions:  (x: 2, y: 3)
4964         Coordinates:
4965           * y        (y) <U1 'a' 'b' 'c'
4966         Dimensions without coordinates: x
4967         Data variables:
4968             A        (x, y) int64 0 1 2 3 4 5
4969         >>> ds.drop_sel(y=["a", "c"])
4970         <xarray.Dataset>
4971         Dimensions:  (x: 2, y: 1)
4972         Coordinates:
4973           * y        (y) <U1 'b'
4974         Dimensions without coordinates: x
4975         Data variables:
4976             A        (x, y) int64 1 4
4977         >>> ds.drop_sel(y="b")
4978         <xarray.Dataset>
4979         Dimensions:  (x: 2, y: 2)
4980         Coordinates:
4981           * y        (y) <U1 'a' 'c'
4982         Dimensions without coordinates: x
4983         Data variables:
4984             A        (x, y) int64 0 2 3 5
4985         """
4986         if errors not in ["raise", "ignore"]:
4987             raise ValueError('errors must be either "raise" or "ignore"')
4988 
4989         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop_sel")
4990 
4991         ds = self
4992         for dim, labels_for_dim in labels.items():
4993             # Don't cast to set, as it would harm performance when labels
4994             # is a large numpy array
4995             if utils.is_scalar(labels_for_dim):
4996                 labels_for_dim = [labels_for_dim]
4997             labels_for_dim = np.asarray(labels_for_dim)
4998             try:
4999                 index = self.get_index(dim)
5000             except KeyError:
5001                 raise ValueError(f"dimension {dim!r} does not have coordinate labels")
5002             new_index = index.drop(labels_for_dim, errors=errors)
5003             ds = ds.loc[{dim: new_index}]
5004         return ds
5005 
5006     def drop_isel(self: T_Dataset, indexers=None, **indexers_kwargs) -> T_Dataset:
5007         """Drop index positions from this Dataset.
5008 
5009         Parameters
5010         ----------
5011         indexers : mapping of hashable to Any
5012             Index locations to drop
5013         **indexers_kwargs : {dim: position, ...}, optional
5014             The keyword arguments form of ``dim`` and ``positions``
5015 
5016         Returns
5017         -------
5018         dropped : Dataset
5019 
5020         Raises
5021         ------
5022         IndexError
5023 
5024         Examples
5025         --------
5026         >>> data = np.arange(6).reshape(2, 3)
5027         >>> labels = ["a", "b", "c"]
5028         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
5029         >>> ds
5030         <xarray.Dataset>
5031         Dimensions:  (x: 2, y: 3)
5032         Coordinates:
5033           * y        (y) <U1 'a' 'b' 'c'
5034         Dimensions without coordinates: x
5035         Data variables:
5036             A        (x, y) int64 0 1 2 3 4 5
5037         >>> ds.drop_isel(y=[0, 2])
5038         <xarray.Dataset>
5039         Dimensions:  (x: 2, y: 1)
5040         Coordinates:
5041           * y        (y) <U1 'b'
5042         Dimensions without coordinates: x
5043         Data variables:
5044             A        (x, y) int64 1 4
5045         >>> ds.drop_isel(y=1)
5046         <xarray.Dataset>
5047         Dimensions:  (x: 2, y: 2)
5048         Coordinates:
5049           * y        (y) <U1 'a' 'c'
5050         Dimensions without coordinates: x
5051         Data variables:
5052             A        (x, y) int64 0 2 3 5
5053         """
5054 
5055         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "drop_isel")
5056 
5057         ds = self
5058         dimension_index = {}
5059         for dim, pos_for_dim in indexers.items():
5060             # Don't cast to set, as it would harm performance when labels
5061             # is a large numpy array
5062             if utils.is_scalar(pos_for_dim):
5063                 pos_for_dim = [pos_for_dim]
5064             pos_for_dim = np.asarray(pos_for_dim)
5065             index = self.get_index(dim)
5066             new_index = index.delete(pos_for_dim)
5067             dimension_index[dim] = new_index
5068         ds = ds.loc[dimension_index]
5069         return ds
5070 
5071     def drop_dims(
5072         self: T_Dataset,
5073         drop_dims: Hashable | Iterable[Hashable],
5074         *,
5075         errors: ErrorOptions = "raise",
5076     ) -> T_Dataset:
5077         """Drop dimensions and associated variables from this dataset.
5078 
5079         Parameters
5080         ----------
5081         drop_dims : hashable or iterable of hashable
5082             Dimension or dimensions to drop.
5083         errors : {"raise", "ignore"}, default: "raise"
5084             If 'raise', raises a ValueError error if any of the
5085             dimensions passed are not in the dataset. If 'ignore', any given
5086             dimensions that are in the dataset are dropped and no error is raised.
5087 
5088         Returns
5089         -------
5090         obj : Dataset
5091             The dataset without the given dimensions (or any variables
5092             containing those dimensions).
5093         """
5094         if errors not in ["raise", "ignore"]:
5095             raise ValueError('errors must be either "raise" or "ignore"')
5096 
5097         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
5098             drop_dims = {drop_dims}
5099         else:
5100             drop_dims = set(drop_dims)
5101 
5102         if errors == "raise":
5103             missing_dims = drop_dims - set(self.dims)
5104             if missing_dims:
5105                 raise ValueError(
5106                     f"Dataset does not contain the dimensions: {missing_dims}"
5107                 )
5108 
5109         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
5110         return self.drop_vars(drop_vars)
5111 
5112     def transpose(
5113         self: T_Dataset,
5114         *dims: Hashable,
5115         missing_dims: ErrorOptionsWithWarn = "raise",
5116     ) -> T_Dataset:
5117         """Return a new Dataset object with all array dimensions transposed.
5118 
5119         Although the order of dimensions on each array will change, the dataset
5120         dimensions themselves will remain in fixed (sorted) order.
5121 
5122         Parameters
5123         ----------
5124         *dims : hashable, optional
5125             By default, reverse the dimensions on each array. Otherwise,
5126             reorder the dimensions to this order.
5127         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
5128             What to do if dimensions that should be selected from are not present in the
5129             Dataset:
5130             - "raise": raise an exception
5131             - "warn": raise a warning, and ignore the missing dimensions
5132             - "ignore": ignore the missing dimensions
5133 
5134         Returns
5135         -------
5136         transposed : Dataset
5137             Each array in the dataset (including) coordinates will be
5138             transposed to the given order.
5139 
5140         Notes
5141         -----
5142         This operation returns a view of each array's data. It is
5143         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
5144         -- the data will be fully loaded into memory.
5145 
5146         See Also
5147         --------
5148         numpy.transpose
5149         DataArray.transpose
5150         """
5151         # Use infix_dims to check once for missing dimensions
5152         if len(dims) != 0:
5153             _ = list(infix_dims(dims, self.dims, missing_dims))
5154 
5155         ds = self.copy()
5156         for name, var in self._variables.items():
5157             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
5158             ds._variables[name] = var.transpose(*var_dims)
5159         return ds
5160 
5161     def dropna(
5162         self: T_Dataset,
5163         dim: Hashable,
5164         how: Literal["any", "all"] = "any",
5165         thresh: int | None = None,
5166         subset: Iterable[Hashable] | None = None,
5167     ) -> T_Dataset:
5168         """Returns a new dataset with dropped labels for missing values along
5169         the provided dimension.
5170 
5171         Parameters
5172         ----------
5173         dim : hashable
5174             Dimension along which to drop missing values. Dropping along
5175             multiple dimensions simultaneously is not yet supported.
5176         how : {"any", "all"}, default: "any"
5177             - any : if any NA values are present, drop that label
5178             - all : if all values are NA, drop that label
5179 
5180         thresh : int or None, optional
5181             If supplied, require this many non-NA values.
5182         subset : iterable of hashable or None, optional
5183             Which variables to check for missing values. By default, all
5184             variables in the dataset are checked.
5185 
5186         Returns
5187         -------
5188         Dataset
5189         """
5190         # TODO: consider supporting multiple dimensions? Or not, given that
5191         # there are some ugly edge cases, e.g., pandas's dropna differs
5192         # depending on the order of the supplied axes.
5193 
5194         if dim not in self.dims:
5195             raise ValueError(f"{dim} must be a single dataset dimension")
5196 
5197         if subset is None:
5198             subset = iter(self.data_vars)
5199 
5200         count = np.zeros(self.dims[dim], dtype=np.int64)
5201         size = np.int_(0)  # for type checking
5202 
5203         for k in subset:
5204             array = self._variables[k]
5205             if dim in array.dims:
5206                 dims = [d for d in array.dims if d != dim]
5207                 count += np.asarray(array.count(dims))  # type: ignore[attr-defined]
5208                 size += math.prod([self.dims[d] for d in dims])
5209 
5210         if thresh is not None:
5211             mask = count >= thresh
5212         elif how == "any":
5213             mask = count == size
5214         elif how == "all":
5215             mask = count > 0
5216         elif how is not None:
5217             raise ValueError(f"invalid how option: {how}")
5218         else:
5219             raise TypeError("must specify how or thresh")
5220 
5221         return self.isel({dim: mask})
5222 
5223     def fillna(self: T_Dataset, value: Any) -> T_Dataset:
5224         """Fill missing values in this object.
5225 
5226         This operation follows the normal broadcasting and alignment rules that
5227         xarray uses for binary arithmetic, except the result is aligned to this
5228         object (``join='left'``) instead of aligned to the intersection of
5229         index coordinates (``join='inner'``).
5230 
5231         Parameters
5232         ----------
5233         value : scalar, ndarray, DataArray, dict or Dataset
5234             Used to fill all matching missing values in this dataset's data
5235             variables. Scalars, ndarrays or DataArrays arguments are used to
5236             fill all data with aligned coordinates (for DataArrays).
5237             Dictionaries or datasets match data variables and then align
5238             coordinates if necessary.
5239 
5240         Returns
5241         -------
5242         Dataset
5243 
5244         Examples
5245         --------
5246         >>> ds = xr.Dataset(
5247         ...     {
5248         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
5249         ...         "B": ("x", [3, 4, np.nan, 1]),
5250         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
5251         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
5252         ...     },
5253         ...     coords={"x": [0, 1, 2, 3]},
5254         ... )
5255         >>> ds
5256         <xarray.Dataset>
5257         Dimensions:  (x: 4)
5258         Coordinates:
5259           * x        (x) int64 0 1 2 3
5260         Data variables:
5261             A        (x) float64 nan 2.0 nan 0.0
5262             B        (x) float64 3.0 4.0 nan 1.0
5263             C        (x) float64 nan nan nan 5.0
5264             D        (x) float64 nan 3.0 nan 4.0
5265 
5266         Replace all `NaN` values with 0s.
5267 
5268         >>> ds.fillna(0)
5269         <xarray.Dataset>
5270         Dimensions:  (x: 4)
5271         Coordinates:
5272           * x        (x) int64 0 1 2 3
5273         Data variables:
5274             A        (x) float64 0.0 2.0 0.0 0.0
5275             B        (x) float64 3.0 4.0 0.0 1.0
5276             C        (x) float64 0.0 0.0 0.0 5.0
5277             D        (x) float64 0.0 3.0 0.0 4.0
5278 
5279         Replace all `NaN` elements in column ‘A’, ‘B’, ‘C’, and ‘D’, with 0, 1, 2, and 3 respectively.
5280 
5281         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
5282         >>> ds.fillna(value=values)
5283         <xarray.Dataset>
5284         Dimensions:  (x: 4)
5285         Coordinates:
5286           * x        (x) int64 0 1 2 3
5287         Data variables:
5288             A        (x) float64 0.0 2.0 0.0 0.0
5289             B        (x) float64 3.0 4.0 1.0 1.0
5290             C        (x) float64 2.0 2.0 2.0 5.0
5291             D        (x) float64 3.0 3.0 3.0 4.0
5292         """
5293         if utils.is_dict_like(value):
5294             value_keys = getattr(value, "data_vars", value).keys()
5295             if not set(value_keys) <= set(self.data_vars.keys()):
5296                 raise ValueError(
5297                     "all variables in the argument to `fillna` "
5298                     "must be contained in the original dataset"
5299                 )
5300         out = ops.fillna(self, value)
5301         return out
5302 
5303     def interpolate_na(
5304         self: T_Dataset,
5305         dim: Hashable | None = None,
5306         method: InterpOptions = "linear",
5307         limit: int = None,
5308         use_coordinate: bool | Hashable = True,
5309         max_gap: (
5310             int | float | str | pd.Timedelta | np.timedelta64 | datetime.timedelta
5311         ) = None,
5312         **kwargs: Any,
5313     ) -> T_Dataset:
5314         """Fill in NaNs by interpolating according to different methods.
5315 
5316         Parameters
5317         ----------
5318         dim : Hashable or None, optional
5319             Specifies the dimension along which to interpolate.
5320         method : {"linear", "nearest", "zero", "slinear", "quadratic", "cubic", "polynomial", \
5321             "barycentric", "krog", "pchip", "spline", "akima"}, default: "linear"
5322             String indicating which method to use for interpolation:
5323 
5324             - 'linear': linear interpolation. Additional keyword
5325               arguments are passed to :py:func:`numpy.interp`
5326             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
5327               are passed to :py:func:`scipy.interpolate.interp1d`. If
5328               ``method='polynomial'``, the ``order`` keyword argument must also be
5329               provided.
5330             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
5331               respective :py:class:`scipy.interpolate` classes.
5332 
5333         use_coordinate : bool or Hashable, default: True
5334             Specifies which index to use as the x values in the interpolation
5335             formulated as `y = f(x)`. If False, values are treated as if
5336             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
5337             used. If ``use_coordinate`` is a string, it specifies the name of a
5338             coordinate variariable to use as the index.
5339         limit : int, default: None
5340             Maximum number of consecutive NaNs to fill. Must be greater than 0
5341             or None for no limit. This filling is done regardless of the size of
5342             the gap in the data. To only interpolate over gaps less than a given length,
5343             see ``max_gap``.
5344         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
5345             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
5346             Use None for no limit. When interpolating along a datetime64 dimension
5347             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
5348 
5349             - a string that is valid input for pandas.to_timedelta
5350             - a :py:class:`numpy.timedelta64` object
5351             - a :py:class:`pandas.Timedelta` object
5352             - a :py:class:`datetime.timedelta` object
5353 
5354             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
5355             dimensions has not been implemented yet. Gap length is defined as the difference
5356             between coordinate values at the first data point after a gap and the last value
5357             before a gap. For gaps at the beginning (end), gap length is defined as the difference
5358             between coordinate values at the first (last) valid data point and the first (last) NaN.
5359             For example, consider::
5360 
5361                 <xarray.DataArray (x: 9)>
5362                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
5363                 Coordinates:
5364                   * x        (x) int64 0 1 2 3 4 5 6 7 8
5365 
5366             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
5367         **kwargs : dict, optional
5368             parameters passed verbatim to the underlying interpolation function
5369 
5370         Returns
5371         -------
5372         interpolated: Dataset
5373             Filled in Dataset.
5374 
5375         See Also
5376         --------
5377         numpy.interp
5378         scipy.interpolate
5379 
5380         Examples
5381         --------
5382         >>> ds = xr.Dataset(
5383         ...     {
5384         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
5385         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
5386         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
5387         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
5388         ...     },
5389         ...     coords={"x": [0, 1, 2, 3, 4]},
5390         ... )
5391         >>> ds
5392         <xarray.Dataset>
5393         Dimensions:  (x: 5)
5394         Coordinates:
5395           * x        (x) int64 0 1 2 3 4
5396         Data variables:
5397             A        (x) float64 nan 2.0 3.0 nan 0.0
5398             B        (x) float64 3.0 4.0 nan 1.0 7.0
5399             C        (x) float64 nan nan nan 5.0 0.0
5400             D        (x) float64 nan 3.0 nan -1.0 4.0
5401 
5402         >>> ds.interpolate_na(dim="x", method="linear")
5403         <xarray.Dataset>
5404         Dimensions:  (x: 5)
5405         Coordinates:
5406           * x        (x) int64 0 1 2 3 4
5407         Data variables:
5408             A        (x) float64 nan 2.0 3.0 1.5 0.0
5409             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5410             C        (x) float64 nan nan nan 5.0 0.0
5411             D        (x) float64 nan 3.0 1.0 -1.0 4.0
5412 
5413         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
5414         <xarray.Dataset>
5415         Dimensions:  (x: 5)
5416         Coordinates:
5417           * x        (x) int64 0 1 2 3 4
5418         Data variables:
5419             A        (x) float64 1.0 2.0 3.0 1.5 0.0
5420             B        (x) float64 3.0 4.0 2.5 1.0 7.0
5421             C        (x) float64 20.0 15.0 10.0 5.0 0.0
5422             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
5423         """
5424         from .missing import _apply_over_vars_with_dim, interp_na
5425 
5426         new = _apply_over_vars_with_dim(
5427             interp_na,
5428             self,
5429             dim=dim,
5430             method=method,
5431             limit=limit,
5432             use_coordinate=use_coordinate,
5433             max_gap=max_gap,
5434             **kwargs,
5435         )
5436         return new
5437 
5438     def ffill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5439         """Fill NaN values by propagating values forward
5440 
5441         *Requires bottleneck.*
5442 
5443         Parameters
5444         ----------
5445         dim : Hashable
5446             Specifies the dimension along which to propagate values when
5447             filling.
5448         limit : int or None, optional
5449             The maximum number of consecutive NaN values to forward fill. In
5450             other words, if there is a gap with more than this number of
5451             consecutive NaNs, it will only be partially filled. Must be greater
5452             than 0 or None for no limit. Must be None or greater than or equal
5453             to axis length if filling along chunked axes (dimensions).
5454 
5455         Returns
5456         -------
5457         Dataset
5458         """
5459         from .missing import _apply_over_vars_with_dim, ffill
5460 
5461         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
5462         return new
5463 
5464     def bfill(self: T_Dataset, dim: Hashable, limit: int | None = None) -> T_Dataset:
5465         """Fill NaN values by propagating values backward
5466 
5467         *Requires bottleneck.*
5468 
5469         Parameters
5470         ----------
5471         dim : Hashable
5472             Specifies the dimension along which to propagate values when
5473             filling.
5474         limit : int or None, optional
5475             The maximum number of consecutive NaN values to backward fill. In
5476             other words, if there is a gap with more than this number of
5477             consecutive NaNs, it will only be partially filled. Must be greater
5478             than 0 or None for no limit. Must be None or greater than or equal
5479             to axis length if filling along chunked axes (dimensions).
5480 
5481         Returns
5482         -------
5483         Dataset
5484         """
5485         from .missing import _apply_over_vars_with_dim, bfill
5486 
5487         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
5488         return new
5489 
5490     def combine_first(self: T_Dataset, other: T_Dataset) -> T_Dataset:
5491         """Combine two Datasets, default to data_vars of self.
5492 
5493         The new coordinates follow the normal broadcasting and alignment rules
5494         of ``join='outer'``.  Vacant cells in the expanded coordinates are
5495         filled with np.nan.
5496 
5497         Parameters
5498         ----------
5499         other : Dataset
5500             Used to fill all matching missing values in this array.
5501 
5502         Returns
5503         -------
5504         Dataset
5505         """
5506         out = ops.fillna(self, other, join="outer", dataset_join="outer")
5507         return out
5508 
5509     def reduce(
5510         self: T_Dataset,
5511         func: Callable,
5512         dim: Hashable | Iterable[Hashable] = None,
5513         *,
5514         keep_attrs: bool | None = None,
5515         keepdims: bool = False,
5516         numeric_only: bool = False,
5517         **kwargs: Any,
5518     ) -> T_Dataset:
5519         """Reduce this dataset by applying `func` along some dimension(s).
5520 
5521         Parameters
5522         ----------
5523         func : callable
5524             Function which can be called in the form
5525             `f(x, axis=axis, **kwargs)` to return the result of reducing an
5526             np.ndarray over an integer valued axis.
5527         dim : str or sequence of str, optional
5528             Dimension(s) over which to apply `func`.  By default `func` is
5529             applied over all dimensions.
5530         keep_attrs : bool or None, optional
5531             If True, the dataset's attributes (`attrs`) will be copied from
5532             the original object to the new one.  If False (default), the new
5533             object will be returned without attributes.
5534         keepdims : bool, default: False
5535             If True, the dimensions which are reduced are left in the result
5536             as dimensions of size one. Coordinates that use these dimensions
5537             are removed.
5538         numeric_only : bool, default: False
5539             If True, only apply ``func`` to variables with a numeric dtype.
5540         **kwargs : Any
5541             Additional keyword arguments passed on to ``func``.
5542 
5543         Returns
5544         -------
5545         reduced : Dataset
5546             Dataset with this object's DataArrays replaced with new DataArrays
5547             of summarized data and the indicated dimension(s) removed.
5548         """
5549         if kwargs.get("axis", None) is not None:
5550             raise ValueError(
5551                 "passing 'axis' to Dataset reduce methods is ambiguous."
5552                 " Please use 'dim' instead."
5553             )
5554 
5555         if dim is None or dim is ...:
5556             dims = set(self.dims)
5557         elif isinstance(dim, str) or not isinstance(dim, Iterable):
5558             dims = {dim}
5559         else:
5560             dims = set(dim)
5561 
5562         missing_dimensions = [d for d in dims if d not in self.dims]
5563         if missing_dimensions:
5564             raise ValueError(
5565                 f"Dataset does not contain the dimensions: {missing_dimensions}"
5566             )
5567 
5568         if keep_attrs is None:
5569             keep_attrs = _get_keep_attrs(default=False)
5570 
5571         variables: dict[Hashable, Variable] = {}
5572         for name, var in self._variables.items():
5573             reduce_dims = [d for d in var.dims if d in dims]
5574             if name in self.coords:
5575                 if not reduce_dims:
5576                     variables[name] = var
5577             else:
5578                 if (
5579                     # Some reduction functions (e.g. std, var) need to run on variables
5580                     # that don't have the reduce dims: PR5393
5581                     not reduce_dims
5582                     or not numeric_only
5583                     or np.issubdtype(var.dtype, np.number)
5584                     or (var.dtype == np.bool_)
5585                 ):
5586                     reduce_maybe_single: Hashable | None | list[Hashable]
5587                     if len(reduce_dims) == 1:
5588                         # unpack dimensions for the benefit of functions
5589                         # like np.argmin which can't handle tuple arguments
5590                         (reduce_maybe_single,) = reduce_dims
5591                     elif len(reduce_dims) == var.ndim:
5592                         # prefer to aggregate over axis=None rather than
5593                         # axis=(0, 1) if they will be equivalent, because
5594                         # the former is often more efficient
5595                         reduce_maybe_single = None
5596                     else:
5597                         reduce_maybe_single = reduce_dims
5598                     variables[name] = var.reduce(
5599                         func,
5600                         dim=reduce_maybe_single,
5601                         keep_attrs=keep_attrs,
5602                         keepdims=keepdims,
5603                         **kwargs,
5604                     )
5605 
5606         coord_names = {k for k in self.coords if k in variables}
5607         indexes = {k: v for k, v in self._indexes.items() if k in variables}
5608         attrs = self.attrs if keep_attrs else None
5609         return self._replace_with_new_dims(
5610             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5611         )
5612 
5613     def map(
5614         self: T_Dataset,
5615         func: Callable,
5616         keep_attrs: bool | None = None,
5617         args: Iterable[Any] = (),
5618         **kwargs: Any,
5619     ) -> T_Dataset:
5620         """Apply a function to each data variable in this dataset
5621 
5622         Parameters
5623         ----------
5624         func : callable
5625             Function which can be called in the form `func(x, *args, **kwargs)`
5626             to transform each DataArray `x` in this dataset into another
5627             DataArray.
5628         keep_attrs : bool or None, optional
5629             If True, both the dataset's and variables' attributes (`attrs`) will be
5630             copied from the original objects to the new ones. If False, the new dataset
5631             and variables will be returned without copying the attributes.
5632         args : iterable, optional
5633             Positional arguments passed on to `func`.
5634         **kwargs : Any
5635             Keyword arguments passed on to `func`.
5636 
5637         Returns
5638         -------
5639         applied : Dataset
5640             Resulting dataset from applying ``func`` to each data variable.
5641 
5642         Examples
5643         --------
5644         >>> da = xr.DataArray(np.random.randn(2, 3))
5645         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
5646         >>> ds
5647         <xarray.Dataset>
5648         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5649         Dimensions without coordinates: dim_0, dim_1, x
5650         Data variables:
5651             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
5652             bar      (x) int64 -1 2
5653         >>> ds.map(np.fabs)
5654         <xarray.Dataset>
5655         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
5656         Dimensions without coordinates: dim_0, dim_1, x
5657         Data variables:
5658             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
5659             bar      (x) float64 1.0 2.0
5660         """
5661         if keep_attrs is None:
5662             keep_attrs = _get_keep_attrs(default=False)
5663         variables = {
5664             k: maybe_wrap_array(v, func(v, *args, **kwargs))
5665             for k, v in self.data_vars.items()
5666         }
5667         if keep_attrs:
5668             for k, v in variables.items():
5669                 v._copy_attrs_from(self.data_vars[k])
5670         attrs = self.attrs if keep_attrs else None
5671         return type(self)(variables, attrs=attrs)
5672 
5673     def apply(
5674         self: T_Dataset,
5675         func: Callable,
5676         keep_attrs: bool | None = None,
5677         args: Iterable[Any] = (),
5678         **kwargs: Any,
5679     ) -> T_Dataset:
5680         """
5681         Backward compatible implementation of ``map``
5682 
5683         See Also
5684         --------
5685         Dataset.map
5686         """
5687         warnings.warn(
5688             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
5689             PendingDeprecationWarning,
5690             stacklevel=2,
5691         )
5692         return self.map(func, keep_attrs, args, **kwargs)
5693 
5694     def assign(
5695         self: T_Dataset,
5696         variables: Mapping[Any, Any] | None = None,
5697         **variables_kwargs: Any,
5698     ) -> T_Dataset:
5699         """Assign new data variables to a Dataset, returning a new object
5700         with all the original variables in addition to the new ones.
5701 
5702         Parameters
5703         ----------
5704         variables : mapping of hashable to Any
5705             Mapping from variables names to the new values. If the new values
5706             are callable, they are computed on the Dataset and assigned to new
5707             data variables. If the values are not callable, (e.g. a DataArray,
5708             scalar, or array), they are simply assigned.
5709         **variables_kwargs
5710             The keyword arguments form of ``variables``.
5711             One of variables or variables_kwargs must be provided.
5712 
5713         Returns
5714         -------
5715         ds : Dataset
5716             A new Dataset with the new variables in addition to all the
5717             existing variables.
5718 
5719         Notes
5720         -----
5721         Since ``kwargs`` is a dictionary, the order of your arguments may not
5722         be preserved, and so the order of the new variables is not well
5723         defined. Assigning multiple variables within the same ``assign`` is
5724         possible, but you cannot reference other variables created within the
5725         same ``assign`` call.
5726 
5727         See Also
5728         --------
5729         pandas.DataFrame.assign
5730 
5731         Examples
5732         --------
5733         >>> x = xr.Dataset(
5734         ...     {
5735         ...         "temperature_c": (
5736         ...             ("lat", "lon"),
5737         ...             20 * np.random.rand(4).reshape(2, 2),
5738         ...         ),
5739         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
5740         ...     },
5741         ...     coords={"lat": [10, 20], "lon": [150, 160]},
5742         ... )
5743         >>> x
5744         <xarray.Dataset>
5745         Dimensions:        (lat: 2, lon: 2)
5746         Coordinates:
5747           * lat            (lat) int64 10 20
5748           * lon            (lon) int64 150 160
5749         Data variables:
5750             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
5751             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
5752 
5753         Where the value is a callable, evaluated on dataset:
5754 
5755         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
5756         <xarray.Dataset>
5757         Dimensions:        (lat: 2, lon: 2)
5758         Coordinates:
5759           * lat            (lat) int64 10 20
5760           * lon            (lon) int64 150 160
5761         Data variables:
5762             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
5763             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
5764             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
5765 
5766         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
5767 
5768         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
5769         <xarray.Dataset>
5770         Dimensions:        (lat: 2, lon: 2)
5771         Coordinates:
5772           * lat            (lat) int64 10 20
5773           * lon            (lon) int64 150 160
5774         Data variables:
5775             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
5776             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
5777             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
5778 
5779         """
5780         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
5781         data = self.copy()
5782         # do all calculations first...
5783         results: CoercibleMapping = data._calc_assign_results(variables)
5784         data.coords._maybe_drop_multiindex_coords(set(results.keys()))
5785         # ... and then assign
5786         data.update(results)
5787         return data
5788 
5789     def to_array(
5790         self, dim: Hashable = "variable", name: Hashable | None = None
5791     ) -> DataArray:
5792         """Convert this dataset into an xarray.DataArray
5793 
5794         The data variables of this dataset will be broadcast against each other
5795         and stacked along the first axis of the new array. All coordinates of
5796         this dataset will remain coordinates.
5797 
5798         Parameters
5799         ----------
5800         dim : Hashable, default: "variable"
5801             Name of the new dimension.
5802         name : Hashable or None, optional
5803             Name of the new data array.
5804 
5805         Returns
5806         -------
5807         array : xarray.DataArray
5808         """
5809         from .dataarray import DataArray
5810 
5811         data_vars = [self.variables[k] for k in self.data_vars]
5812         broadcast_vars = broadcast_variables(*data_vars)
5813         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
5814 
5815         dims = (dim,) + broadcast_vars[0].dims
5816         variable = Variable(dims, data, self.attrs, fastpath=True)
5817 
5818         coords = {k: v.variable for k, v in self.coords.items()}
5819         indexes = filter_indexes_from_coords(self._indexes, set(coords))
5820         new_dim_index = PandasIndex(list(self.data_vars), dim)
5821         indexes[dim] = new_dim_index
5822         coords.update(new_dim_index.create_variables())
5823 
5824         return DataArray._construct_direct(variable, coords, name, indexes)
5825 
5826     def _normalize_dim_order(
5827         self, dim_order: Sequence[Hashable] | None = None
5828     ) -> dict[Hashable, int]:
5829         """
5830         Check the validity of the provided dimensions if any and return the mapping
5831         between dimension name and their size.
5832 
5833         Parameters
5834         ----------
5835         dim_order: Sequence of Hashable or None, optional
5836             Dimension order to validate (default to the alphabetical order if None).
5837 
5838         Returns
5839         -------
5840         result : dict[Hashable, int]
5841             Validated dimensions mapping.
5842 
5843         """
5844         if dim_order is None:
5845             dim_order = list(self.dims)
5846         elif set(dim_order) != set(self.dims):
5847             raise ValueError(
5848                 "dim_order {} does not match the set of dimensions of this "
5849                 "Dataset: {}".format(dim_order, list(self.dims))
5850             )
5851 
5852         ordered_dims = {k: self.dims[k] for k in dim_order}
5853 
5854         return ordered_dims
5855 
5856     def to_pandas(self) -> pd.Series | pd.DataFrame:
5857         """Convert this dataset into a pandas object without changing the number of dimensions.
5858 
5859         The type of the returned object depends on the number of Dataset
5860         dimensions:
5861 
5862         * 0D -> `pandas.Series`
5863         * 1D -> `pandas.DataFrame`
5864 
5865         Only works for Datasets with 1 or fewer dimensions.
5866         """
5867         if len(self.dims) == 0:
5868             return pd.Series({k: v.item() for k, v in self.items()})
5869         if len(self.dims) == 1:
5870             return self.to_dataframe()
5871         raise ValueError(
5872             "cannot convert Datasets with %s dimensions into "
5873             "pandas objects without changing the number of dimensions. "
5874             "Please use Dataset.to_dataframe() instead." % len(self.dims)
5875         )
5876 
5877     def _to_dataframe(self, ordered_dims: Mapping[Any, int]):
5878         columns = [k for k in self.variables if k not in self.dims]
5879         data = [
5880             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
5881             for k in columns
5882         ]
5883         index = self.coords.to_index([*ordered_dims])
5884         return pd.DataFrame(dict(zip(columns, data)), index=index)
5885 
5886     def to_dataframe(self, dim_order: Sequence[Hashable] | None = None) -> pd.DataFrame:
5887         """Convert this dataset into a pandas.DataFrame.
5888 
5889         Non-index variables in this dataset form the columns of the
5890         DataFrame. The DataFrame is indexed by the Cartesian product of
5891         this dataset's indices.
5892 
5893         Parameters
5894         ----------
5895         dim_order: Sequence of Hashable or None, optional
5896             Hierarchical dimension order for the resulting dataframe. All
5897             arrays are transposed to this order and then written out as flat
5898             vectors in contiguous order, so the last dimension in this list
5899             will be contiguous in the resulting DataFrame. This has a major
5900             influence on which operations are efficient on the resulting
5901             dataframe.
5902 
5903             If provided, must include all dimensions of this dataset. By
5904             default, dimensions are sorted alphabetically.
5905 
5906         Returns
5907         -------
5908         result : DataFrame
5909             Dataset as a pandas DataFrame.
5910 
5911         """
5912 
5913         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
5914 
5915         return self._to_dataframe(ordered_dims=ordered_dims)
5916 
5917     def _set_sparse_data_from_dataframe(
5918         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
5919     ) -> None:
5920         from sparse import COO
5921 
5922         if isinstance(idx, pd.MultiIndex):
5923             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
5924             is_sorted = idx.is_monotonic_increasing
5925             shape = tuple(lev.size for lev in idx.levels)
5926         else:
5927             coords = np.arange(idx.size).reshape(1, -1)
5928             is_sorted = True
5929             shape = (idx.size,)
5930 
5931         for name, values in arrays:
5932             # In virtually all real use cases, the sparse array will now have
5933             # missing values and needs a fill_value. For consistency, don't
5934             # special case the rare exceptions (e.g., dtype=int without a
5935             # MultiIndex).
5936             dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
5937             values = np.asarray(values, dtype=dtype)
5938 
5939             data = COO(
5940                 coords,
5941                 values,
5942                 shape,
5943                 has_duplicates=False,
5944                 sorted=is_sorted,
5945                 fill_value=fill_value,
5946             )
5947             self[name] = (dims, data)
5948 
5949     def _set_numpy_data_from_dataframe(
5950         self, idx: pd.Index, arrays: list[tuple[Hashable, np.ndarray]], dims: tuple
5951     ) -> None:
5952         if not isinstance(idx, pd.MultiIndex):
5953             for name, values in arrays:
5954                 self[name] = (dims, values)
5955             return
5956 
5957         # NB: similar, more general logic, now exists in
5958         # variable.unstack_once; we could consider combining them at some
5959         # point.
5960 
5961         shape = tuple(lev.size for lev in idx.levels)
5962         indexer = tuple(idx.codes)
5963 
5964         # We already verified that the MultiIndex has all unique values, so
5965         # there are missing values if and only if the size of output arrays is
5966         # larger that the index.
5967         missing_values = math.prod(shape) > idx.shape[0]
5968 
5969         for name, values in arrays:
5970             # NumPy indexing is much faster than using DataFrame.reindex() to
5971             # fill in missing values:
5972             # https://stackoverflow.com/a/35049899/809705
5973             if missing_values:
5974                 dtype, fill_value = xrdtypes.maybe_promote(values.dtype)
5975                 data = np.full(shape, fill_value, dtype)
5976             else:
5977                 # If there are no missing values, keep the existing dtype
5978                 # instead of promoting to support NA, e.g., keep integer
5979                 # columns as integers.
5980                 # TODO: consider removing this special case, which doesn't
5981                 # exist for sparse=True.
5982                 data = np.zeros(shape, values.dtype)
5983             data[indexer] = values
5984             self[name] = (dims, data)
5985 
5986     @classmethod
5987     def from_dataframe(
5988         cls: type[T_Dataset], dataframe: pd.DataFrame, sparse: bool = False
5989     ) -> T_Dataset:
5990         """Convert a pandas.DataFrame into an xarray.Dataset
5991 
5992         Each column will be converted into an independent variable in the
5993         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
5994         into a tensor product of one-dimensional indices (filling in missing
5995         values with NaN). This method will produce a Dataset very similar to
5996         that on which the 'to_dataframe' method was called, except with
5997         possibly redundant dimensions (since all dataset variables will have
5998         the same dimensionality)
5999 
6000         Parameters
6001         ----------
6002         dataframe : DataFrame
6003             DataFrame from which to copy data and indices.
6004         sparse : bool, default: False
6005             If true, create a sparse arrays instead of dense numpy arrays. This
6006             can potentially save a large amount of memory if the DataFrame has
6007             a MultiIndex. Requires the sparse package (sparse.pydata.org).
6008 
6009         Returns
6010         -------
6011         New Dataset.
6012 
6013         See Also
6014         --------
6015         xarray.DataArray.from_series
6016         pandas.DataFrame.to_xarray
6017         """
6018         # TODO: Add an option to remove dimensions along which the variables
6019         # are constant, to enable consistent serialization to/from a dataframe,
6020         # even if some variables have different dimensionality.
6021 
6022         if not dataframe.columns.is_unique:
6023             raise ValueError("cannot convert DataFrame with non-unique columns")
6024 
6025         idx = remove_unused_levels_categories(dataframe.index)
6026 
6027         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
6028             raise ValueError(
6029                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
6030             )
6031 
6032         # Cast to a NumPy array first, in case the Series is a pandas Extension
6033         # array (which doesn't have a valid NumPy dtype)
6034         # TODO: allow users to control how this casting happens, e.g., by
6035         # forwarding arguments to pandas.Series.to_numpy?
6036         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
6037 
6038         indexes: dict[Hashable, Index] = {}
6039         index_vars: dict[Hashable, Variable] = {}
6040 
6041         if isinstance(idx, pd.MultiIndex):
6042             dims = tuple(
6043                 name if name is not None else "level_%i" % n
6044                 for n, name in enumerate(idx.names)
6045             )
6046             for dim, lev in zip(dims, idx.levels):
6047                 xr_idx = PandasIndex(lev, dim)
6048                 indexes[dim] = xr_idx
6049                 index_vars.update(xr_idx.create_variables())
6050         else:
6051             index_name = idx.name if idx.name is not None else "index"
6052             dims = (index_name,)
6053             xr_idx = PandasIndex(idx, index_name)
6054             indexes[index_name] = xr_idx
6055             index_vars.update(xr_idx.create_variables())
6056 
6057         obj = cls._construct_direct(index_vars, set(index_vars), indexes=indexes)
6058 
6059         if sparse:
6060             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
6061         else:
6062             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
6063         return obj
6064 
6065     def to_dask_dataframe(
6066         self, dim_order: Sequence[Hashable] | None = None, set_index: bool = False
6067     ) -> DaskDataFrame:
6068         """
6069         Convert this dataset into a dask.dataframe.DataFrame.
6070 
6071         The dimensions, coordinates and data variables in this dataset form
6072         the columns of the DataFrame.
6073 
6074         Parameters
6075         ----------
6076         dim_order : list, optional
6077             Hierarchical dimension order for the resulting dataframe. All
6078             arrays are transposed to this order and then written out as flat
6079             vectors in contiguous order, so the last dimension in this list
6080             will be contiguous in the resulting DataFrame. This has a major
6081             influence on which operations are efficient on the resulting dask
6082             dataframe.
6083 
6084             If provided, must include all dimensions of this dataset. By
6085             default, dimensions are sorted alphabetically.
6086         set_index : bool, default: False
6087             If set_index=True, the dask DataFrame is indexed by this dataset's
6088             coordinate. Since dask DataFrames do not support multi-indexes,
6089             set_index only works if the dataset only contains one dimension.
6090 
6091         Returns
6092         -------
6093         dask.dataframe.DataFrame
6094         """
6095 
6096         import dask.array as da
6097         import dask.dataframe as dd
6098 
6099         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
6100 
6101         columns = list(ordered_dims)
6102         columns.extend(k for k in self.coords if k not in self.dims)
6103         columns.extend(self.data_vars)
6104 
6105         series_list = []
6106         for name in columns:
6107             try:
6108                 var = self.variables[name]
6109             except KeyError:
6110                 # dimension without a matching coordinate
6111                 size = self.dims[name]
6112                 data = da.arange(size, chunks=size, dtype=np.int64)
6113                 var = Variable((name,), data)
6114 
6115             # IndexVariable objects have a dummy .chunk() method
6116             if isinstance(var, IndexVariable):
6117                 var = var.to_base_variable()
6118 
6119             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
6120             series = dd.from_array(dask_array.reshape(-1), columns=[name])
6121             series_list.append(series)
6122 
6123         df = dd.concat(series_list, axis=1)
6124 
6125         if set_index:
6126             dim_order = [*ordered_dims]
6127 
6128             if len(dim_order) == 1:
6129                 (dim,) = dim_order
6130                 df = df.set_index(dim)
6131             else:
6132                 # triggers an error about multi-indexes, even if only one
6133                 # dimension is passed
6134                 df = df.set_index(dim_order)
6135 
6136         return df
6137 
6138     def to_dict(self, data: bool = True, encoding: bool = False) -> dict[str, Any]:
6139         """
6140         Convert this dataset to a dictionary following xarray naming
6141         conventions.
6142 
6143         Converts all variables and attributes to native Python objects
6144         Useful for converting to json. To avoid datetime incompatibility
6145         use decode_times=False kwarg in xarrray.open_dataset.
6146 
6147         Parameters
6148         ----------
6149         data : bool, default: True
6150             Whether to include the actual data in the dictionary. When set to
6151             False, returns just the schema.
6152         encoding : bool, default: False
6153             Whether to include the Dataset's encoding in the dictionary.
6154 
6155         Returns
6156         -------
6157         d : dict
6158             Dict with keys: "coords", "attrs", "dims", "data_vars" and optionally
6159             "encoding".
6160 
6161         See Also
6162         --------
6163         Dataset.from_dict
6164         DataArray.to_dict
6165         """
6166         d: dict = {
6167             "coords": {},
6168             "attrs": decode_numpy_dict_values(self.attrs),
6169             "dims": dict(self.dims),
6170             "data_vars": {},
6171         }
6172         for k in self.coords:
6173             d["coords"].update(
6174                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6175             )
6176         for k in self.data_vars:
6177             d["data_vars"].update(
6178                 {k: self[k].variable.to_dict(data=data, encoding=encoding)}
6179             )
6180         if encoding:
6181             d["encoding"] = dict(self.encoding)
6182         return d
6183 
6184     @classmethod
6185     def from_dict(cls: type[T_Dataset], d: Mapping[Any, Any]) -> T_Dataset:
6186         """Convert a dictionary into an xarray.Dataset.
6187 
6188         Parameters
6189         ----------
6190         d : dict-like
6191             Mapping with a minimum structure of
6192                 ``{"var_0": {"dims": [..], "data": [..]}, \
6193                             ...}``
6194 
6195         Returns
6196         -------
6197         obj : Dataset
6198 
6199         See also
6200         --------
6201         Dataset.to_dict
6202         DataArray.from_dict
6203 
6204         Examples
6205         --------
6206         >>> d = {
6207         ...     "t": {"dims": ("t"), "data": [0, 1, 2]},
6208         ...     "a": {"dims": ("t"), "data": ["a", "b", "c"]},
6209         ...     "b": {"dims": ("t"), "data": [10, 20, 30]},
6210         ... }
6211         >>> ds = xr.Dataset.from_dict(d)
6212         >>> ds
6213         <xarray.Dataset>
6214         Dimensions:  (t: 3)
6215         Coordinates:
6216           * t        (t) int64 0 1 2
6217         Data variables:
6218             a        (t) <U1 'a' 'b' 'c'
6219             b        (t) int64 10 20 30
6220 
6221         >>> d = {
6222         ...     "coords": {
6223         ...         "t": {"dims": "t", "data": [0, 1, 2], "attrs": {"units": "s"}}
6224         ...     },
6225         ...     "attrs": {"title": "air temperature"},
6226         ...     "dims": "t",
6227         ...     "data_vars": {
6228         ...         "a": {"dims": "t", "data": [10, 20, 30]},
6229         ...         "b": {"dims": "t", "data": ["a", "b", "c"]},
6230         ...     },
6231         ... }
6232         >>> ds = xr.Dataset.from_dict(d)
6233         >>> ds
6234         <xarray.Dataset>
6235         Dimensions:  (t: 3)
6236         Coordinates:
6237           * t        (t) int64 0 1 2
6238         Data variables:
6239             a        (t) int64 10 20 30
6240             b        (t) <U1 'a' 'b' 'c'
6241         Attributes:
6242             title:    air temperature
6243 
6244         """
6245 
6246         variables: Iterable[tuple[Hashable, Any]]
6247         if not {"coords", "data_vars"}.issubset(set(d)):
6248             variables = d.items()
6249         else:
6250             import itertools
6251 
6252             variables = itertools.chain(
6253                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
6254             )
6255         try:
6256             variable_dict = {
6257                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
6258             }
6259         except KeyError as e:
6260             raise ValueError(
6261                 "cannot convert dict without the key "
6262                 "'{dims_data}'".format(dims_data=str(e.args[0]))
6263             )
6264         obj = cls(variable_dict)
6265 
6266         # what if coords aren't dims?
6267         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
6268         obj = obj.set_coords(coords)
6269 
6270         obj.attrs.update(d.get("attrs", {}))
6271         obj.encoding.update(d.get("encoding", {}))
6272 
6273         return obj
6274 
6275     def _unary_op(self: T_Dataset, f, *args, **kwargs) -> T_Dataset:
6276         variables = {}
6277         keep_attrs = kwargs.pop("keep_attrs", None)
6278         if keep_attrs is None:
6279             keep_attrs = _get_keep_attrs(default=True)
6280         for k, v in self._variables.items():
6281             if k in self._coord_names:
6282                 variables[k] = v
6283             else:
6284                 variables[k] = f(v, *args, **kwargs)
6285                 if keep_attrs:
6286                     variables[k].attrs = v._attrs
6287         attrs = self._attrs if keep_attrs else None
6288         return self._replace_with_new_dims(variables, attrs=attrs)
6289 
6290     def _binary_op(self, other, f, reflexive=False, join=None) -> Dataset:
6291         from .dataarray import DataArray
6292         from .groupby import GroupBy
6293 
6294         if isinstance(other, GroupBy):
6295             return NotImplemented
6296         align_type = OPTIONS["arithmetic_join"] if join is None else join
6297         if isinstance(other, (DataArray, Dataset)):
6298             self, other = align(self, other, join=align_type, copy=False)  # type: ignore[assignment]
6299         g = f if not reflexive else lambda x, y: f(y, x)
6300         ds = self._calculate_binary_op(g, other, join=align_type)
6301         return ds
6302 
6303     def _inplace_binary_op(self: T_Dataset, other, f) -> T_Dataset:
6304         from .dataarray import DataArray
6305         from .groupby import GroupBy
6306 
6307         if isinstance(other, GroupBy):
6308             raise TypeError(
6309                 "in-place operations between a Dataset and "
6310                 "a grouped object are not permitted"
6311             )
6312         # we don't actually modify arrays in-place with in-place Dataset
6313         # arithmetic -- this lets us automatically align things
6314         if isinstance(other, (DataArray, Dataset)):
6315             other = other.reindex_like(self, copy=False)
6316         g = ops.inplace_to_noninplace_op(f)
6317         ds = self._calculate_binary_op(g, other, inplace=True)
6318         self._replace_with_new_dims(
6319             ds._variables,
6320             ds._coord_names,
6321             attrs=ds._attrs,
6322             indexes=ds._indexes,
6323             inplace=True,
6324         )
6325         return self
6326 
6327     def _calculate_binary_op(
6328         self, f, other, join="inner", inplace: bool = False
6329     ) -> Dataset:
6330         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
6331             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
6332                 raise ValueError(
6333                     "datasets must have the same data variables "
6334                     f"for in-place arithmetic operations: {list(lhs_data_vars)}, {list(rhs_data_vars)}"
6335                 )
6336 
6337             dest_vars = {}
6338 
6339             for k in lhs_data_vars:
6340                 if k in rhs_data_vars:
6341                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
6342                 elif join in ["left", "outer"]:
6343                     dest_vars[k] = f(lhs_vars[k], np.nan)
6344             for k in rhs_data_vars:
6345                 if k not in dest_vars and join in ["right", "outer"]:
6346                     dest_vars[k] = f(rhs_vars[k], np.nan)
6347             return dest_vars
6348 
6349         if utils.is_dict_like(other) and not isinstance(other, Dataset):
6350             # can't use our shortcut of doing the binary operation with
6351             # Variable objects, so apply over our data vars instead.
6352             new_data_vars = apply_over_both(
6353                 self.data_vars, other, self.data_vars, other
6354             )
6355             return type(self)(new_data_vars)
6356 
6357         other_coords: Coordinates | None = getattr(other, "coords", None)
6358         ds = self.coords.merge(other_coords)
6359 
6360         if isinstance(other, Dataset):
6361             new_vars = apply_over_both(
6362                 self.data_vars, other.data_vars, self.variables, other.variables
6363             )
6364         else:
6365             other_variable = getattr(other, "variable", other)
6366             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
6367         ds._variables.update(new_vars)
6368         ds._dims = calculate_dimensions(ds._variables)
6369         return ds
6370 
6371     def _copy_attrs_from(self, other):
6372         self.attrs = other.attrs
6373         for v in other.variables:
6374             if v in self.variables:
6375                 self.variables[v].attrs = other.variables[v].attrs
6376 
6377     def diff(
6378         self: T_Dataset,
6379         dim: Hashable,
6380         n: int = 1,
6381         label: Literal["upper", "lower"] = "upper",
6382     ) -> T_Dataset:
6383         """Calculate the n-th order discrete difference along given axis.
6384 
6385         Parameters
6386         ----------
6387         dim : Hashable
6388             Dimension over which to calculate the finite difference.
6389         n : int, default: 1
6390             The number of times values are differenced.
6391         label : {"upper", "lower"}, default: "upper"
6392             The new coordinate in dimension ``dim`` will have the
6393             values of either the minuend's or subtrahend's coordinate
6394             for values 'upper' and 'lower', respectively.
6395 
6396         Returns
6397         -------
6398         difference : Dataset
6399             The n-th order finite difference of this object.
6400 
6401         Notes
6402         -----
6403         `n` matches numpy's behavior and is different from pandas' first argument named
6404         `periods`.
6405 
6406         Examples
6407         --------
6408         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
6409         >>> ds.diff("x")
6410         <xarray.Dataset>
6411         Dimensions:  (x: 3)
6412         Dimensions without coordinates: x
6413         Data variables:
6414             foo      (x) int64 0 1 0
6415         >>> ds.diff("x", 2)
6416         <xarray.Dataset>
6417         Dimensions:  (x: 2)
6418         Dimensions without coordinates: x
6419         Data variables:
6420             foo      (x) int64 1 -1
6421 
6422         See Also
6423         --------
6424         Dataset.differentiate
6425         """
6426         if n == 0:
6427             return self
6428         if n < 0:
6429             raise ValueError(f"order `n` must be non-negative but got {n}")
6430 
6431         # prepare slices
6432         slice_start = {dim: slice(None, -1)}
6433         slice_end = {dim: slice(1, None)}
6434 
6435         # prepare new coordinate
6436         if label == "upper":
6437             slice_new = slice_end
6438         elif label == "lower":
6439             slice_new = slice_start
6440         else:
6441             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
6442 
6443         indexes, index_vars = isel_indexes(self.xindexes, slice_new)
6444         variables = {}
6445 
6446         for name, var in self.variables.items():
6447             if name in index_vars:
6448                 variables[name] = index_vars[name]
6449             elif dim in var.dims:
6450                 if name in self.data_vars:
6451                     variables[name] = var.isel(slice_end) - var.isel(slice_start)
6452                 else:
6453                     variables[name] = var.isel(slice_new)
6454             else:
6455                 variables[name] = var
6456 
6457         difference = self._replace_with_new_dims(variables, indexes=indexes)
6458 
6459         if n > 1:
6460             return difference.diff(dim, n - 1)
6461         else:
6462             return difference
6463 
6464     def shift(
6465         self: T_Dataset,
6466         shifts: Mapping[Any, int] | None = None,
6467         fill_value: Any = xrdtypes.NA,
6468         **shifts_kwargs: int,
6469     ) -> T_Dataset:
6470 
6471         """Shift this dataset by an offset along one or more dimensions.
6472 
6473         Only data variables are moved; coordinates stay in place. This is
6474         consistent with the behavior of ``shift`` in pandas.
6475 
6476         Values shifted from beyond array bounds will appear at one end of
6477         each dimension, which are filled according to `fill_value`. For periodic
6478         offsets instead see `roll`.
6479 
6480         Parameters
6481         ----------
6482         shifts : mapping of hashable to int
6483             Integer offset to shift along each of the given dimensions.
6484             Positive offsets shift to the right; negative offsets shift to the
6485             left.
6486         fill_value : scalar or dict-like, optional
6487             Value to use for newly missing values. If a dict-like, maps
6488             variable names (including coordinates) to fill values.
6489         **shifts_kwargs
6490             The keyword arguments form of ``shifts``.
6491             One of shifts or shifts_kwargs must be provided.
6492 
6493         Returns
6494         -------
6495         shifted : Dataset
6496             Dataset with the same coordinates and attributes but shifted data
6497             variables.
6498 
6499         See Also
6500         --------
6501         roll
6502 
6503         Examples
6504         --------
6505         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
6506         >>> ds.shift(x=2)
6507         <xarray.Dataset>
6508         Dimensions:  (x: 5)
6509         Dimensions without coordinates: x
6510         Data variables:
6511             foo      (x) object nan nan 'a' 'b' 'c'
6512         """
6513         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
6514         invalid = [k for k in shifts if k not in self.dims]
6515         if invalid:
6516             raise ValueError(f"dimensions {invalid!r} do not exist")
6517 
6518         variables = {}
6519         for name, var in self.variables.items():
6520             if name in self.data_vars:
6521                 fill_value_ = (
6522                     fill_value.get(name, xrdtypes.NA)
6523                     if isinstance(fill_value, dict)
6524                     else fill_value
6525                 )
6526 
6527                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
6528                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
6529             else:
6530                 variables[name] = var
6531 
6532         return self._replace(variables)
6533 
6534     def roll(
6535         self: T_Dataset,
6536         shifts: Mapping[Any, int] | None = None,
6537         roll_coords: bool = False,
6538         **shifts_kwargs: int,
6539     ) -> T_Dataset:
6540         """Roll this dataset by an offset along one or more dimensions.
6541 
6542         Unlike shift, roll treats the given dimensions as periodic, so will not
6543         create any missing values to be filled.
6544 
6545         Also unlike shift, roll may rotate all variables, including coordinates
6546         if specified. The direction of rotation is consistent with
6547         :py:func:`numpy.roll`.
6548 
6549         Parameters
6550         ----------
6551         shifts : mapping of hashable to int, optional
6552             A dict with keys matching dimensions and values given
6553             by integers to rotate each of the given dimensions. Positive
6554             offsets roll to the right; negative offsets roll to the left.
6555         roll_coords : bool, default: False
6556             Indicates whether to roll the coordinates by the offset too.
6557         **shifts_kwargs : {dim: offset, ...}, optional
6558             The keyword arguments form of ``shifts``.
6559             One of shifts or shifts_kwargs must be provided.
6560 
6561         Returns
6562         -------
6563         rolled : Dataset
6564             Dataset with the same attributes but rolled data and coordinates.
6565 
6566         See Also
6567         --------
6568         shift
6569 
6570         Examples
6571         --------
6572         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))}, coords={"x": np.arange(5)})
6573         >>> ds.roll(x=2)
6574         <xarray.Dataset>
6575         Dimensions:  (x: 5)
6576         Coordinates:
6577           * x        (x) int64 0 1 2 3 4
6578         Data variables:
6579             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6580 
6581         >>> ds.roll(x=2, roll_coords=True)
6582         <xarray.Dataset>
6583         Dimensions:  (x: 5)
6584         Coordinates:
6585           * x        (x) int64 3 4 0 1 2
6586         Data variables:
6587             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
6588 
6589         """
6590         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
6591         invalid = [k for k in shifts if k not in self.dims]
6592         if invalid:
6593             raise ValueError(f"dimensions {invalid!r} do not exist")
6594 
6595         unrolled_vars: tuple[Hashable, ...]
6596 
6597         if roll_coords:
6598             indexes, index_vars = roll_indexes(self.xindexes, shifts)
6599             unrolled_vars = ()
6600         else:
6601             indexes = dict(self._indexes)
6602             index_vars = dict(self.xindexes.variables)
6603             unrolled_vars = tuple(self.coords)
6604 
6605         variables = {}
6606         for k, var in self.variables.items():
6607             if k in index_vars:
6608                 variables[k] = index_vars[k]
6609             elif k not in unrolled_vars:
6610                 variables[k] = var.roll(
6611                     shifts={k: s for k, s in shifts.items() if k in var.dims}
6612                 )
6613             else:
6614                 variables[k] = var
6615 
6616         return self._replace(variables, indexes=indexes)
6617 
6618     def sortby(
6619         self: T_Dataset,
6620         variables: Hashable | DataArray | list[Hashable | DataArray],
6621         ascending: bool = True,
6622     ) -> T_Dataset:
6623         """
6624         Sort object by labels or values (along an axis).
6625 
6626         Sorts the dataset, either along specified dimensions,
6627         or according to values of 1-D dataarrays that share dimension
6628         with calling object.
6629 
6630         If the input variables are dataarrays, then the dataarrays are aligned
6631         (via left-join) to the calling object prior to sorting by cell values.
6632         NaNs are sorted to the end, following Numpy convention.
6633 
6634         If multiple sorts along the same dimension is
6635         given, numpy's lexsort is performed along that dimension:
6636         https://numpy.org/doc/stable/reference/generated/numpy.lexsort.html
6637         and the FIRST key in the sequence is used as the primary sort key,
6638         followed by the 2nd key, etc.
6639 
6640         Parameters
6641         ----------
6642         variables : Hashable, DataArray, or list of hashable or DataArray
6643             1D DataArray objects or name(s) of 1D variable(s) in
6644             coords/data_vars whose values are used to sort the dataset.
6645         ascending : bool, default: True
6646             Whether to sort by ascending or descending order.
6647 
6648         Returns
6649         -------
6650         sorted : Dataset
6651             A new dataset where all the specified dims are sorted by dim
6652             labels.
6653 
6654         See Also
6655         --------
6656         DataArray.sortby
6657         numpy.sort
6658         pandas.sort_values
6659         pandas.sort_index
6660 
6661         Examples
6662         --------
6663         >>> ds = xr.Dataset(
6664         ...     {
6665         ...         "A": (("x", "y"), [[1, 2], [3, 4]]),
6666         ...         "B": (("x", "y"), [[5, 6], [7, 8]]),
6667         ...     },
6668         ...     coords={"x": ["b", "a"], "y": [1, 0]},
6669         ... )
6670         >>> ds.sortby("x")
6671         <xarray.Dataset>
6672         Dimensions:  (x: 2, y: 2)
6673         Coordinates:
6674           * x        (x) <U1 'a' 'b'
6675           * y        (y) int64 1 0
6676         Data variables:
6677             A        (x, y) int64 3 4 1 2
6678             B        (x, y) int64 7 8 5 6
6679         """
6680         from .dataarray import DataArray
6681 
6682         if not isinstance(variables, list):
6683             variables = [variables]
6684         else:
6685             variables = variables
6686         arrays = [v if isinstance(v, DataArray) else self[v] for v in variables]
6687         aligned_vars = align(self, *arrays, join="left")  # type: ignore[type-var]
6688         aligned_self: T_Dataset = aligned_vars[0]  # type: ignore[assignment]
6689         aligned_other_vars: tuple[DataArray, ...] = aligned_vars[1:]  # type: ignore[assignment]
6690         vars_by_dim = defaultdict(list)
6691         for data_array in aligned_other_vars:
6692             if data_array.ndim != 1:
6693                 raise ValueError("Input DataArray is not 1-D.")
6694             (key,) = data_array.dims
6695             vars_by_dim[key].append(data_array)
6696 
6697         indices = {}
6698         for key, arrays in vars_by_dim.items():
6699             order = np.lexsort(tuple(reversed(arrays)))
6700             indices[key] = order if ascending else order[::-1]
6701         return aligned_self.isel(indices)
6702 
6703     def quantile(
6704         self: T_Dataset,
6705         q: ArrayLike,
6706         dim: str | Iterable[Hashable] | None = None,
6707         method: QUANTILE_METHODS = "linear",
6708         numeric_only: bool = False,
6709         keep_attrs: bool = None,
6710         skipna: bool = None,
6711         interpolation: QUANTILE_METHODS = None,
6712     ) -> T_Dataset:
6713         """Compute the qth quantile of the data along the specified dimension.
6714 
6715         Returns the qth quantiles(s) of the array elements for each variable
6716         in the Dataset.
6717 
6718         Parameters
6719         ----------
6720         q : float or array-like of float
6721             Quantile to compute, which must be between 0 and 1 inclusive.
6722         dim : str or Iterable of Hashable, optional
6723             Dimension(s) over which to apply quantile.
6724         method : str, default: "linear"
6725             This optional parameter specifies the interpolation method to use when the
6726             desired quantile lies between two data points. The options sorted by their R
6727             type as summarized in the H&F paper [1]_ are:
6728 
6729                 1. "inverted_cdf" (*)
6730                 2. "averaged_inverted_cdf" (*)
6731                 3. "closest_observation" (*)
6732                 4. "interpolated_inverted_cdf" (*)
6733                 5. "hazen" (*)
6734                 6. "weibull" (*)
6735                 7. "linear"  (default)
6736                 8. "median_unbiased" (*)
6737                 9. "normal_unbiased" (*)
6738 
6739             The first three methods are discontiuous.  The following discontinuous
6740             variations of the default "linear" (7.) option are also available:
6741 
6742                 * "lower"
6743                 * "higher"
6744                 * "midpoint"
6745                 * "nearest"
6746 
6747             See :py:func:`numpy.quantile` or [1]_ for details. The "method" argument
6748             was previously called "interpolation", renamed in accordance with numpy
6749             version 1.22.0.
6750 
6751             (*) These methods require numpy version 1.22 or newer.
6752 
6753         keep_attrs : bool, optional
6754             If True, the dataset's attributes (`attrs`) will be copied from
6755             the original object to the new one.  If False (default), the new
6756             object will be returned without attributes.
6757         numeric_only : bool, optional
6758             If True, only apply ``func`` to variables with a numeric dtype.
6759         skipna : bool, optional
6760             If True, skip missing values (as marked by NaN). By default, only
6761             skips missing values for float dtypes; other dtypes either do not
6762             have a sentinel missing value (int) or skipna=True has not been
6763             implemented (object, datetime64 or timedelta64).
6764 
6765         Returns
6766         -------
6767         quantiles : Dataset
6768             If `q` is a single quantile, then the result is a scalar for each
6769             variable in data_vars. If multiple percentiles are given, first
6770             axis of the result corresponds to the quantile and a quantile
6771             dimension is added to the return Dataset. The other dimensions are
6772             the dimensions that remain after the reduction of the array.
6773 
6774         See Also
6775         --------
6776         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
6777 
6778         Examples
6779         --------
6780         >>> ds = xr.Dataset(
6781         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
6782         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
6783         ... )
6784         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
6785         <xarray.Dataset>
6786         Dimensions:   ()
6787         Coordinates:
6788             quantile  float64 0.0
6789         Data variables:
6790             a         float64 0.7
6791         >>> ds.quantile(0, dim="x")
6792         <xarray.Dataset>
6793         Dimensions:   (y: 4)
6794         Coordinates:
6795           * y         (y) float64 1.0 1.5 2.0 2.5
6796             quantile  float64 0.0
6797         Data variables:
6798             a         (y) float64 0.7 4.2 2.6 1.5
6799         >>> ds.quantile([0, 0.5, 1])
6800         <xarray.Dataset>
6801         Dimensions:   (quantile: 3)
6802         Coordinates:
6803           * quantile  (quantile) float64 0.0 0.5 1.0
6804         Data variables:
6805             a         (quantile) float64 0.7 3.4 9.4
6806         >>> ds.quantile([0, 0.5, 1], dim="x")
6807         <xarray.Dataset>
6808         Dimensions:   (quantile: 3, y: 4)
6809         Coordinates:
6810           * y         (y) float64 1.0 1.5 2.0 2.5
6811           * quantile  (quantile) float64 0.0 0.5 1.0
6812         Data variables:
6813             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
6814 
6815         References
6816         ----------
6817         .. [1] R. J. Hyndman and Y. Fan,
6818            "Sample quantiles in statistical packages,"
6819            The American Statistician, 50(4), pp. 361-365, 1996
6820         """
6821 
6822         # interpolation renamed to method in version 0.21.0
6823         # check here and in variable to avoid repeated warnings
6824         if interpolation is not None:
6825             warnings.warn(
6826                 "The `interpolation` argument to quantile was renamed to `method`.",
6827                 FutureWarning,
6828             )
6829 
6830             if method != "linear":
6831                 raise TypeError("Cannot pass interpolation and method keywords!")
6832 
6833             method = interpolation
6834 
6835         dims: set[Hashable]
6836         if isinstance(dim, str):
6837             dims = {dim}
6838         elif dim is None or dim is ...:
6839             dims = set(self.dims)
6840         else:
6841             dims = set(dim)
6842 
6843         _assert_empty(
6844             tuple(d for d in dims if d not in self.dims),
6845             "Dataset does not contain the dimensions: %s",
6846         )
6847 
6848         q = np.asarray(q, dtype=np.float64)
6849 
6850         variables = {}
6851         for name, var in self.variables.items():
6852             reduce_dims = [d for d in var.dims if d in dims]
6853             if reduce_dims or not var.dims:
6854                 if name not in self.coords:
6855                     if (
6856                         not numeric_only
6857                         or np.issubdtype(var.dtype, np.number)
6858                         or var.dtype == np.bool_
6859                     ):
6860                         variables[name] = var.quantile(
6861                             q,
6862                             dim=reduce_dims,
6863                             method=method,
6864                             keep_attrs=keep_attrs,
6865                             skipna=skipna,
6866                         )
6867 
6868             else:
6869                 variables[name] = var
6870 
6871         # construct the new dataset
6872         coord_names = {k for k in self.coords if k in variables}
6873         indexes = {k: v for k, v in self._indexes.items() if k in variables}
6874         if keep_attrs is None:
6875             keep_attrs = _get_keep_attrs(default=False)
6876         attrs = self.attrs if keep_attrs else None
6877         new = self._replace_with_new_dims(
6878             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
6879         )
6880         return new.assign_coords(quantile=q)
6881 
6882     def rank(
6883         self: T_Dataset,
6884         dim: Hashable,
6885         pct: bool = False,
6886         keep_attrs: bool | None = None,
6887     ) -> T_Dataset:
6888         """Ranks the data.
6889 
6890         Equal values are assigned a rank that is the average of the ranks that
6891         would have been otherwise assigned to all of the values within
6892         that set.
6893         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
6894 
6895         NaNs in the input array are returned as NaNs.
6896 
6897         The `bottleneck` library is required.
6898 
6899         Parameters
6900         ----------
6901         dim : Hashable
6902             Dimension over which to compute rank.
6903         pct : bool, default: False
6904             If True, compute percentage ranks, otherwise compute integer ranks.
6905         keep_attrs : bool or None, optional
6906             If True, the dataset's attributes (`attrs`) will be copied from
6907             the original object to the new one.  If False, the new
6908             object will be returned without attributes.
6909 
6910         Returns
6911         -------
6912         ranked : Dataset
6913             Variables that do not depend on `dim` are dropped.
6914         """
6915         if not OPTIONS["use_bottleneck"]:
6916             raise RuntimeError(
6917                 "rank requires bottleneck to be enabled."
6918                 " Call `xr.set_options(use_bottleneck=True)` to enable it."
6919             )
6920 
6921         if dim not in self.dims:
6922             raise ValueError(f"Dataset does not contain the dimension: {dim}")
6923 
6924         variables = {}
6925         for name, var in self.variables.items():
6926             if name in self.data_vars:
6927                 if dim in var.dims:
6928                     variables[name] = var.rank(dim, pct=pct)
6929             else:
6930                 variables[name] = var
6931 
6932         coord_names = set(self.coords)
6933         if keep_attrs is None:
6934             keep_attrs = _get_keep_attrs(default=False)
6935         attrs = self.attrs if keep_attrs else None
6936         return self._replace(variables, coord_names, attrs=attrs)
6937 
6938     def differentiate(
6939         self: T_Dataset,
6940         coord: Hashable,
6941         edge_order: Literal[1, 2] = 1,
6942         datetime_unit: DatetimeUnitOptions | None = None,
6943     ) -> T_Dataset:
6944         """ Differentiate with the second order accurate central
6945         differences.
6946 
6947         .. note::
6948             This feature is limited to simple cartesian geometry, i.e. coord
6949             must be one dimensional.
6950 
6951         Parameters
6952         ----------
6953         coord : Hashable
6954             The coordinate to be used to compute the gradient.
6955         edge_order : {1, 2}, default: 1
6956             N-th order accurate differences at the boundaries.
6957         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
6958             "us", "ns", "ps", "fs", "as", None}, default: None
6959             Unit to compute gradient. Only valid for datetime coordinate.
6960 
6961         Returns
6962         -------
6963         differentiated: Dataset
6964 
6965         See also
6966         --------
6967         numpy.gradient: corresponding numpy function
6968         """
6969         from .variable import Variable
6970 
6971         if coord not in self.variables and coord not in self.dims:
6972             raise ValueError(f"Coordinate {coord} does not exist.")
6973 
6974         coord_var = self[coord].variable
6975         if coord_var.ndim != 1:
6976             raise ValueError(
6977                 "Coordinate {} must be 1 dimensional but is {}"
6978                 " dimensional".format(coord, coord_var.ndim)
6979             )
6980 
6981         dim = coord_var.dims[0]
6982         if _contains_datetime_like_objects(coord_var):
6983             if coord_var.dtype.kind in "mM" and datetime_unit is None:
6984                 datetime_unit = cast(
6985                     "DatetimeUnitOptions", np.datetime_data(coord_var.dtype)[0]
6986                 )
6987             elif datetime_unit is None:
6988                 datetime_unit = "s"  # Default to seconds for cftime objects
6989             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
6990 
6991         variables = {}
6992         for k, v in self.variables.items():
6993             if k in self.data_vars and dim in v.dims and k not in self.coords:
6994                 if _contains_datetime_like_objects(v):
6995                     v = v._to_numeric(datetime_unit=datetime_unit)
6996                 grad = duck_array_ops.gradient(
6997                     v.data,
6998                     coord_var.data,
6999                     edge_order=edge_order,
7000                     axis=v.get_axis_num(dim),
7001                 )
7002                 variables[k] = Variable(v.dims, grad)
7003             else:
7004                 variables[k] = v
7005         return self._replace(variables)
7006 
7007     def integrate(
7008         self: T_Dataset,
7009         coord: Hashable | Sequence[Hashable],
7010         datetime_unit: DatetimeUnitOptions = None,
7011     ) -> T_Dataset:
7012         """Integrate along the given coordinate using the trapezoidal rule.
7013 
7014         .. note::
7015             This feature is limited to simple cartesian geometry, i.e. coord
7016             must be one dimensional.
7017 
7018         Parameters
7019         ----------
7020         coord : hashable, or sequence of hashable
7021             Coordinate(s) used for the integration.
7022         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7023                         'ps', 'fs', 'as', None}, optional
7024             Specify the unit if datetime coordinate is used.
7025 
7026         Returns
7027         -------
7028         integrated : Dataset
7029 
7030         See also
7031         --------
7032         DataArray.integrate
7033         numpy.trapz : corresponding numpy function
7034 
7035         Examples
7036         --------
7037         >>> ds = xr.Dataset(
7038         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7039         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7040         ... )
7041         >>> ds
7042         <xarray.Dataset>
7043         Dimensions:  (x: 4)
7044         Coordinates:
7045           * x        (x) int64 0 1 2 3
7046             y        (x) int64 1 7 3 5
7047         Data variables:
7048             a        (x) int64 5 5 6 6
7049             b        (x) int64 1 2 1 0
7050         >>> ds.integrate("x")
7051         <xarray.Dataset>
7052         Dimensions:  ()
7053         Data variables:
7054             a        float64 16.5
7055             b        float64 3.5
7056         >>> ds.integrate("y")
7057         <xarray.Dataset>
7058         Dimensions:  ()
7059         Data variables:
7060             a        float64 20.0
7061             b        float64 4.0
7062         """
7063         if not isinstance(coord, (list, tuple)):
7064             coord = (coord,)
7065         result = self
7066         for c in coord:
7067             result = result._integrate_one(c, datetime_unit=datetime_unit)
7068         return result
7069 
7070     def _integrate_one(self, coord, datetime_unit=None, cumulative=False):
7071         from .variable import Variable
7072 
7073         if coord not in self.variables and coord not in self.dims:
7074             raise ValueError(f"Coordinate {coord} does not exist.")
7075 
7076         coord_var = self[coord].variable
7077         if coord_var.ndim != 1:
7078             raise ValueError(
7079                 "Coordinate {} must be 1 dimensional but is {}"
7080                 " dimensional".format(coord, coord_var.ndim)
7081             )
7082 
7083         dim = coord_var.dims[0]
7084         if _contains_datetime_like_objects(coord_var):
7085             if coord_var.dtype.kind in "mM" and datetime_unit is None:
7086                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
7087             elif datetime_unit is None:
7088                 datetime_unit = "s"  # Default to seconds for cftime objects
7089             coord_var = coord_var._replace(
7090                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
7091             )
7092 
7093         variables = {}
7094         coord_names = set()
7095         for k, v in self.variables.items():
7096             if k in self.coords:
7097                 if dim not in v.dims or cumulative:
7098                     variables[k] = v
7099                     coord_names.add(k)
7100             else:
7101                 if k in self.data_vars and dim in v.dims:
7102                     if _contains_datetime_like_objects(v):
7103                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
7104                     if cumulative:
7105                         integ = duck_array_ops.cumulative_trapezoid(
7106                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7107                         )
7108                         v_dims = v.dims
7109                     else:
7110                         integ = duck_array_ops.trapz(
7111                             v.data, coord_var.data, axis=v.get_axis_num(dim)
7112                         )
7113                         v_dims = list(v.dims)
7114                         v_dims.remove(dim)
7115                     variables[k] = Variable(v_dims, integ)
7116                 else:
7117                     variables[k] = v
7118         indexes = {k: v for k, v in self._indexes.items() if k in variables}
7119         return self._replace_with_new_dims(
7120             variables, coord_names=coord_names, indexes=indexes
7121         )
7122 
7123     def cumulative_integrate(
7124         self: T_Dataset,
7125         coord: Hashable | Sequence[Hashable],
7126         datetime_unit: DatetimeUnitOptions = None,
7127     ) -> T_Dataset:
7128         """Integrate along the given coordinate using the trapezoidal rule.
7129 
7130         .. note::
7131             This feature is limited to simple cartesian geometry, i.e. coord
7132             must be one dimensional.
7133 
7134             The first entry of the cumulative integral of each variable is always 0, in
7135             order to keep the length of the dimension unchanged between input and
7136             output.
7137 
7138         Parameters
7139         ----------
7140         coord : hashable, or sequence of hashable
7141             Coordinate(s) used for the integration.
7142         datetime_unit : {'Y', 'M', 'W', 'D', 'h', 'm', 's', 'ms', 'us', 'ns', \
7143                         'ps', 'fs', 'as', None}, optional
7144             Specify the unit if datetime coordinate is used.
7145 
7146         Returns
7147         -------
7148         integrated : Dataset
7149 
7150         See also
7151         --------
7152         DataArray.cumulative_integrate
7153         scipy.integrate.cumulative_trapezoid : corresponding scipy function
7154 
7155         Examples
7156         --------
7157         >>> ds = xr.Dataset(
7158         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
7159         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
7160         ... )
7161         >>> ds
7162         <xarray.Dataset>
7163         Dimensions:  (x: 4)
7164         Coordinates:
7165           * x        (x) int64 0 1 2 3
7166             y        (x) int64 1 7 3 5
7167         Data variables:
7168             a        (x) int64 5 5 6 6
7169             b        (x) int64 1 2 1 0
7170         >>> ds.cumulative_integrate("x")
7171         <xarray.Dataset>
7172         Dimensions:  (x: 4)
7173         Coordinates:
7174           * x        (x) int64 0 1 2 3
7175             y        (x) int64 1 7 3 5
7176         Data variables:
7177             a        (x) float64 0.0 5.0 10.5 16.5
7178             b        (x) float64 0.0 1.5 3.0 3.5
7179         >>> ds.cumulative_integrate("y")
7180         <xarray.Dataset>
7181         Dimensions:  (x: 4)
7182         Coordinates:
7183           * x        (x) int64 0 1 2 3
7184             y        (x) int64 1 7 3 5
7185         Data variables:
7186             a        (x) float64 0.0 30.0 8.0 20.0
7187             b        (x) float64 0.0 9.0 3.0 4.0
7188         """
7189         if not isinstance(coord, (list, tuple)):
7190             coord = (coord,)
7191         result = self
7192         for c in coord:
7193             result = result._integrate_one(
7194                 c, datetime_unit=datetime_unit, cumulative=True
7195             )
7196         return result
7197 
7198     @property
7199     def real(self: T_Dataset) -> T_Dataset:
7200         return self.map(lambda x: x.real, keep_attrs=True)
7201 
7202     @property
7203     def imag(self: T_Dataset) -> T_Dataset:
7204         return self.map(lambda x: x.imag, keep_attrs=True)
7205 
7206     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
7207 
7208     def filter_by_attrs(self: T_Dataset, **kwargs) -> T_Dataset:
7209         """Returns a ``Dataset`` with variables that match specific conditions.
7210 
7211         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
7212         containing only the variables for which all the filter tests pass.
7213         These tests are either ``key=value`` for which the attribute ``key``
7214         has the exact value ``value`` or the callable passed into
7215         ``key=callable`` returns True. The callable will be passed a single
7216         value, either the value of the attribute ``key`` or ``None`` if the
7217         DataArray does not have an attribute with the name ``key``.
7218 
7219         Parameters
7220         ----------
7221         **kwargs
7222             key : str
7223                 Attribute name.
7224             value : callable or obj
7225                 If value is a callable, it should return a boolean in the form
7226                 of bool = func(attr) where attr is da.attrs[key].
7227                 Otherwise, value will be compared to the each
7228                 DataArray's attrs[key].
7229 
7230         Returns
7231         -------
7232         new : Dataset
7233             New dataset with variables filtered by attribute.
7234 
7235         Examples
7236         --------
7237         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
7238         >>> precip = 10 * np.random.rand(2, 2, 3)
7239         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
7240         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
7241         >>> dims = ["x", "y", "time"]
7242         >>> temp_attr = dict(standard_name="air_potential_temperature")
7243         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
7244 
7245         >>> ds = xr.Dataset(
7246         ...     dict(
7247         ...         temperature=(dims, temp, temp_attr),
7248         ...         precipitation=(dims, precip, precip_attr),
7249         ...     ),
7250         ...     coords=dict(
7251         ...         lon=(["x", "y"], lon),
7252         ...         lat=(["x", "y"], lat),
7253         ...         time=pd.date_range("2014-09-06", periods=3),
7254         ...         reference_time=pd.Timestamp("2014-09-05"),
7255         ...     ),
7256         ... )
7257 
7258         Get variables matching a specific standard_name:
7259 
7260         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
7261         <xarray.Dataset>
7262         Dimensions:         (x: 2, y: 2, time: 3)
7263         Coordinates:
7264             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7265             lat             (x, y) float64 42.25 42.21 42.63 42.59
7266           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7267             reference_time  datetime64[ns] 2014-09-05
7268         Dimensions without coordinates: x, y
7269         Data variables:
7270             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7271 
7272         Get all variables that have a standard_name attribute:
7273 
7274         >>> standard_name = lambda v: v is not None
7275         >>> ds.filter_by_attrs(standard_name=standard_name)
7276         <xarray.Dataset>
7277         Dimensions:         (x: 2, y: 2, time: 3)
7278         Coordinates:
7279             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
7280             lat             (x, y) float64 42.25 42.21 42.63 42.59
7281           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
7282             reference_time  datetime64[ns] 2014-09-05
7283         Dimensions without coordinates: x, y
7284         Data variables:
7285             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
7286             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
7287 
7288         """
7289         selection = []
7290         for var_name, variable in self.variables.items():
7291             has_value_flag = False
7292             for attr_name, pattern in kwargs.items():
7293                 attr_value = variable.attrs.get(attr_name)
7294                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
7295                     has_value_flag = True
7296                 else:
7297                     has_value_flag = False
7298                     break
7299             if has_value_flag is True:
7300                 selection.append(var_name)
7301         return self[selection]
7302 
7303     def unify_chunks(self: T_Dataset) -> T_Dataset:
7304         """Unify chunk size along all chunked dimensions of this Dataset.
7305 
7306         Returns
7307         -------
7308         Dataset with consistent chunk sizes for all dask-array variables
7309 
7310         See Also
7311         --------
7312         dask.array.core.unify_chunks
7313         """
7314 
7315         return unify_chunks(self)[0]
7316 
7317     def map_blocks(
7318         self,
7319         func: Callable[..., T_Xarray],
7320         args: Sequence[Any] = (),
7321         kwargs: Mapping[str, Any] | None = None,
7322         template: DataArray | Dataset | None = None,
7323     ) -> T_Xarray:
7324         """
7325         Apply a function to each block of this Dataset.
7326 
7327         .. warning::
7328             This method is experimental and its signature may change.
7329 
7330         Parameters
7331         ----------
7332         func : callable
7333             User-provided function that accepts a Dataset as its first
7334             parameter. The function will receive a subset or 'block' of this Dataset (see below),
7335             corresponding to one chunk along each chunked dimension. ``func`` will be
7336             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
7337 
7338             This function must return either a single DataArray or a single Dataset.
7339 
7340             This function cannot add a new chunked dimension.
7341         args : sequence
7342             Passed to func after unpacking and subsetting any xarray objects by blocks.
7343             xarray objects in args must be aligned with obj, otherwise an error is raised.
7344         kwargs : Mapping or None
7345             Passed verbatim to func after unpacking. xarray objects, if any, will not be
7346             subset to blocks. Passing dask collections in kwargs is not allowed.
7347         template : DataArray, Dataset or None, optional
7348             xarray object representing the final result after compute is called. If not provided,
7349             the function will be first run on mocked-up data, that looks like this object but
7350             has sizes 0, to determine properties of the returned object such as dtype,
7351             variable names, attributes, new dimensions and new indexes (if any).
7352             ``template`` must be provided if the function changes the size of existing dimensions.
7353             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
7354             ``attrs`` set by ``func`` will be ignored.
7355 
7356         Returns
7357         -------
7358         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
7359         function.
7360 
7361         Notes
7362         -----
7363         This function is designed for when ``func`` needs to manipulate a whole xarray object
7364         subset to each block. Each block is loaded into memory. In the more common case where
7365         ``func`` can work on numpy arrays, it is recommended to use ``apply_ufunc``.
7366 
7367         If none of the variables in this object is backed by dask arrays, calling this function is
7368         equivalent to calling ``func(obj, *args, **kwargs)``.
7369 
7370         See Also
7371         --------
7372         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks
7373         xarray.DataArray.map_blocks
7374 
7375         Examples
7376         --------
7377         Calculate an anomaly from climatology using ``.groupby()``. Using
7378         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
7379         its indices, and its methods like ``.groupby()``.
7380 
7381         >>> def calculate_anomaly(da, groupby_type="time.month"):
7382         ...     gb = da.groupby(groupby_type)
7383         ...     clim = gb.mean(dim="time")
7384         ...     return gb - clim
7385         ...
7386         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
7387         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
7388         >>> np.random.seed(123)
7389         >>> array = xr.DataArray(
7390         ...     np.random.rand(len(time)),
7391         ...     dims=["time"],
7392         ...     coords={"time": time, "month": month},
7393         ... ).chunk()
7394         >>> ds = xr.Dataset({"a": array})
7395         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
7396         <xarray.Dataset>
7397         Dimensions:  (time: 24)
7398         Coordinates:
7399           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7400             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
7401         Data variables:
7402             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
7403 
7404         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
7405         to the function being applied in ``xr.map_blocks()``:
7406 
7407         >>> ds.map_blocks(
7408         ...     calculate_anomaly,
7409         ...     kwargs={"groupby_type": "time.year"},
7410         ...     template=ds,
7411         ... )
7412         <xarray.Dataset>
7413         Dimensions:  (time: 24)
7414         Coordinates:
7415           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
7416             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
7417         Data variables:
7418             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
7419         """
7420         from .parallel import map_blocks
7421 
7422         return map_blocks(func, self, args, kwargs, template)
7423 
7424     def polyfit(
7425         self: T_Dataset,
7426         dim: Hashable,
7427         deg: int,
7428         skipna: bool | None = None,
7429         rcond: float | None = None,
7430         w: Hashable | Any = None,
7431         full: bool = False,
7432         cov: bool | Literal["unscaled"] = False,
7433     ) -> T_Dataset:
7434         """
7435         Least squares polynomial fit.
7436 
7437         This replicates the behaviour of `numpy.polyfit` but differs by skipping
7438         invalid values when `skipna = True`.
7439 
7440         Parameters
7441         ----------
7442         dim : hashable
7443             Coordinate along which to fit the polynomials.
7444         deg : int
7445             Degree of the fitting polynomial.
7446         skipna : bool or None, optional
7447             If True, removes all invalid values before fitting each 1D slices of the array.
7448             Default is True if data is stored in a dask.array or if there is any
7449             invalid values, False otherwise.
7450         rcond : float or None, optional
7451             Relative condition number to the fit.
7452         w : hashable or Any, optional
7453             Weights to apply to the y-coordinate of the sample points.
7454             Can be an array-like object or the name of a coordinate in the dataset.
7455         full : bool, default: False
7456             Whether to return the residuals, matrix rank and singular values in addition
7457             to the coefficients.
7458         cov : bool or "unscaled", default: False
7459             Whether to return to the covariance matrix in addition to the coefficients.
7460             The matrix is not scaled if `cov='unscaled'`.
7461 
7462         Returns
7463         -------
7464         polyfit_results : Dataset
7465             A single dataset which contains (for each "var" in the input dataset):
7466 
7467             [var]_polyfit_coefficients
7468                 The coefficients of the best fit for each variable in this dataset.
7469             [var]_polyfit_residuals
7470                 The residuals of the least-square computation for each variable (only included if `full=True`)
7471                 When the matrix rank is deficient, np.nan is returned.
7472             [dim]_matrix_rank
7473                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7474                 The rank is computed ignoring the NaN values that might be skipped.
7475             [dim]_singular_values
7476                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
7477             [var]_polyfit_covariance
7478                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
7479 
7480         Warns
7481         -----
7482         RankWarning
7483             The rank of the coefficient matrix in the least-squares fit is deficient.
7484             The warning is not raised with in-memory (not dask) data and `full=True`.
7485 
7486         See Also
7487         --------
7488         numpy.polyfit
7489         numpy.polyval
7490         xarray.polyval
7491         """
7492         from .dataarray import DataArray
7493 
7494         variables = {}
7495         skipna_da = skipna
7496 
7497         x = get_clean_interp_index(self, dim, strict=False)
7498         xname = f"{self[dim].name}_"
7499         order = int(deg) + 1
7500         lhs = np.vander(x, order)
7501 
7502         if rcond is None:
7503             rcond = (
7504                 x.shape[0] * np.core.finfo(x.dtype).eps  # type: ignore[attr-defined]
7505             )
7506 
7507         # Weights:
7508         if w is not None:
7509             if isinstance(w, Hashable):
7510                 w = self.coords[w]
7511             w = np.asarray(w)
7512             if w.ndim != 1:
7513                 raise TypeError("Expected a 1-d array for weights.")
7514             if w.shape[0] != lhs.shape[0]:
7515                 raise TypeError(f"Expected w and {dim} to have the same length")
7516             lhs *= w[:, np.newaxis]
7517 
7518         # Scaling
7519         scale = np.sqrt((lhs * lhs).sum(axis=0))
7520         lhs /= scale
7521 
7522         degree_dim = utils.get_temp_dimname(self.dims, "degree")
7523 
7524         rank = np.linalg.matrix_rank(lhs)
7525 
7526         if full:
7527             rank = DataArray(rank, name=xname + "matrix_rank")
7528             variables[rank.name] = rank
7529             _sing = np.linalg.svd(lhs, compute_uv=False)
7530             sing = DataArray(
7531                 _sing,
7532                 dims=(degree_dim,),
7533                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
7534                 name=xname + "singular_values",
7535             )
7536             variables[sing.name] = sing
7537 
7538         for name, da in self.data_vars.items():
7539             if dim not in da.dims:
7540                 continue
7541 
7542             if is_duck_dask_array(da.data) and (
7543                 rank != order or full or skipna is None
7544             ):
7545                 # Current algorithm with dask and skipna=False neither supports
7546                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
7547                 skipna_da = True
7548             elif skipna is None:
7549                 skipna_da = bool(np.any(da.isnull()))
7550 
7551             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
7552             stacked_coords: dict[Hashable, DataArray] = {}
7553             if dims_to_stack:
7554                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
7555                 rhs = da.transpose(dim, *dims_to_stack).stack(
7556                     {stacked_dim: dims_to_stack}
7557                 )
7558                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
7559                 scale_da = scale[:, np.newaxis]
7560             else:
7561                 rhs = da
7562                 scale_da = scale
7563 
7564             if w is not None:
7565                 rhs *= w[:, np.newaxis]
7566 
7567             with warnings.catch_warnings():
7568                 if full:  # Copy np.polyfit behavior
7569                     warnings.simplefilter("ignore", np.RankWarning)
7570                 else:  # Raise only once per variable
7571                     warnings.simplefilter("once", np.RankWarning)
7572 
7573                 coeffs, residuals = duck_array_ops.least_squares(
7574                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
7575                 )
7576 
7577             if isinstance(name, str):
7578                 name = f"{name}_"
7579             else:
7580                 # Thus a ReprObject => polyfit was called on a DataArray
7581                 name = ""
7582 
7583             coeffs = DataArray(
7584                 coeffs / scale_da,
7585                 dims=[degree_dim] + list(stacked_coords.keys()),
7586                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
7587                 name=name + "polyfit_coefficients",
7588             )
7589             if dims_to_stack:
7590                 coeffs = coeffs.unstack(stacked_dim)
7591             variables[coeffs.name] = coeffs
7592 
7593             if full or (cov is True):
7594                 residuals = DataArray(
7595                     residuals if dims_to_stack else residuals.squeeze(),
7596                     dims=list(stacked_coords.keys()),
7597                     coords=stacked_coords,
7598                     name=name + "polyfit_residuals",
7599                 )
7600                 if dims_to_stack:
7601                     residuals = residuals.unstack(stacked_dim)
7602                 variables[residuals.name] = residuals
7603 
7604             if cov:
7605                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
7606                 Vbase /= np.outer(scale, scale)
7607                 if cov == "unscaled":
7608                     fac = 1
7609                 else:
7610                     if x.shape[0] <= order:
7611                         raise ValueError(
7612                             "The number of data points must exceed order to scale the covariance matrix."
7613                         )
7614                     fac = residuals / (x.shape[0] - order)
7615                 covariance = DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
7616                 variables[name + "polyfit_covariance"] = covariance
7617 
7618         return type(self)(data_vars=variables, attrs=self.attrs.copy())
7619 
7620     def pad(
7621         self: T_Dataset,
7622         pad_width: Mapping[Any, int | tuple[int, int]] = None,
7623         mode: PadModeOptions = "constant",
7624         stat_length: int
7625         | tuple[int, int]
7626         | Mapping[Any, tuple[int, int]]
7627         | None = None,
7628         constant_values: (
7629             float | tuple[float, float] | Mapping[Any, tuple[float, float]] | None
7630         ) = None,
7631         end_values: int | tuple[int, int] | Mapping[Any, tuple[int, int]] | None = None,
7632         reflect_type: PadReflectOptions = None,
7633         **pad_width_kwargs: Any,
7634     ) -> T_Dataset:
7635         """Pad this dataset along one or more dimensions.
7636 
7637         .. warning::
7638             This function is experimental and its behaviour is likely to change
7639             especially regarding padding of dimension coordinates (or IndexVariables).
7640 
7641         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
7642         coordinates will be padded with the same mode, otherwise coordinates
7643         are padded using the "constant" mode with fill_value dtypes.NA.
7644 
7645         Parameters
7646         ----------
7647         pad_width : mapping of hashable to tuple of int
7648             Mapping with the form of {dim: (pad_before, pad_after)}
7649             describing the number of values padded along each dimension.
7650             {dim: pad} is a shortcut for pad_before = pad_after = pad
7651         mode : {"constant", "edge", "linear_ramp", "maximum", "mean", "median", \
7652             "minimum", "reflect", "symmetric", "wrap"}, default: "constant"
7653             How to pad the DataArray (taken from numpy docs):
7654 
7655             - "constant": Pads with a constant value.
7656             - "edge": Pads with the edge values of array.
7657             - "linear_ramp": Pads with the linear ramp between end_value and the
7658               array edge value.
7659             - "maximum": Pads with the maximum value of all or part of the
7660               vector along each axis.
7661             - "mean": Pads with the mean value of all or part of the
7662               vector along each axis.
7663             - "median": Pads with the median value of all or part of the
7664               vector along each axis.
7665             - "minimum": Pads with the minimum value of all or part of the
7666               vector along each axis.
7667             - "reflect": Pads with the reflection of the vector mirrored on
7668               the first and last values of the vector along each axis.
7669             - "symmetric": Pads with the reflection of the vector mirrored
7670               along the edge of the array.
7671             - "wrap": Pads with the wrap of the vector along the axis.
7672               The first values are used to pad the end and the
7673               end values are used to pad the beginning.
7674 
7675         stat_length : int, tuple or mapping of hashable to tuple, default: None
7676             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
7677             values at edge of each axis used to calculate the statistic value.
7678             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
7679             statistic lengths along each dimension.
7680             ((before, after),) yields same before and after statistic lengths
7681             for each dimension.
7682             (stat_length,) or int is a shortcut for before = after = statistic
7683             length for all axes.
7684             Default is ``None``, to use the entire axis.
7685         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
7686             Used in 'constant'.  The values to set the padded values for each
7687             axis.
7688             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7689             pad constants along each dimension.
7690             ``((before, after),)`` yields same before and after constants for each
7691             dimension.
7692             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7693             all dimensions.
7694             Default is 0.
7695         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
7696             Used in 'linear_ramp'.  The values used for the ending value of the
7697             linear_ramp and that will form the edge of the padded array.
7698             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
7699             end values along each dimension.
7700             ``((before, after),)`` yields same before and after end values for each
7701             axis.
7702             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
7703             all axes.
7704             Default is 0.
7705         reflect_type : {"even", "odd", None}, optional
7706             Used in "reflect", and "symmetric".  The "even" style is the
7707             default with an unaltered reflection around the edge value.  For
7708             the "odd" style, the extended part of the array is created by
7709             subtracting the reflected values from two times the edge value.
7710         **pad_width_kwargs
7711             The keyword arguments form of ``pad_width``.
7712             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
7713 
7714         Returns
7715         -------
7716         padded : Dataset
7717             Dataset with the padded coordinates and data.
7718 
7719         See Also
7720         --------
7721         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
7722 
7723         Notes
7724         -----
7725         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
7726         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
7727         specify ``constant_values=np.nan``
7728 
7729         Padding coordinates will drop their corresponding index (if any) and will reset default
7730         indexes for dimension coordinates.
7731 
7732         Examples
7733         --------
7734         >>> ds = xr.Dataset({"foo": ("x", range(5))})
7735         >>> ds.pad(x=(1, 2))
7736         <xarray.Dataset>
7737         Dimensions:  (x: 8)
7738         Dimensions without coordinates: x
7739         Data variables:
7740             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
7741         """
7742         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
7743 
7744         if mode in ("edge", "reflect", "symmetric", "wrap"):
7745             coord_pad_mode = mode
7746             coord_pad_options = {
7747                 "stat_length": stat_length,
7748                 "constant_values": constant_values,
7749                 "end_values": end_values,
7750                 "reflect_type": reflect_type,
7751             }
7752         else:
7753             coord_pad_mode = "constant"
7754             coord_pad_options = {}
7755 
7756         variables = {}
7757 
7758         # keep indexes that won't be affected by pad and drop all other indexes
7759         xindexes = self.xindexes
7760         pad_dims = set(pad_width)
7761         indexes = {}
7762         for k, idx in xindexes.items():
7763             if not pad_dims.intersection(xindexes.get_all_dims(k)):
7764                 indexes[k] = idx
7765 
7766         for name, var in self.variables.items():
7767             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
7768             if not var_pad_width:
7769                 variables[name] = var
7770             elif name in self.data_vars:
7771                 variables[name] = var.pad(
7772                     pad_width=var_pad_width,
7773                     mode=mode,
7774                     stat_length=stat_length,
7775                     constant_values=constant_values,
7776                     end_values=end_values,
7777                     reflect_type=reflect_type,
7778                 )
7779             else:
7780                 variables[name] = var.pad(
7781                     pad_width=var_pad_width,
7782                     mode=coord_pad_mode,
7783                     **coord_pad_options,  # type: ignore[arg-type]
7784                 )
7785                 # reset default index of dimension coordinates
7786                 if (name,) == var.dims:
7787                     dim_var = {name: variables[name]}
7788                     index = PandasIndex.from_variables(dim_var)
7789                     index_vars = index.create_variables(dim_var)
7790                     indexes[name] = index
7791                     variables[name] = index_vars[name]
7792 
7793         return self._replace_with_new_dims(variables, indexes=indexes)
7794 
7795     def idxmin(
7796         self: T_Dataset,
7797         dim: Hashable | None = None,
7798         skipna: bool | None = None,
7799         fill_value: Any = xrdtypes.NA,
7800         keep_attrs: bool | None = None,
7801     ) -> T_Dataset:
7802         """Return the coordinate label of the minimum value along a dimension.
7803 
7804         Returns a new `Dataset` named after the dimension with the values of
7805         the coordinate labels along that dimension corresponding to minimum
7806         values along that dimension.
7807 
7808         In comparison to :py:meth:`~Dataset.argmin`, this returns the
7809         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
7810 
7811         Parameters
7812         ----------
7813         dim : Hashable, optional
7814             Dimension over which to apply `idxmin`.  This is optional for 1D
7815             variables, but required for variables with 2 or more dimensions.
7816         skipna : bool or None, optional
7817             If True, skip missing values (as marked by NaN). By default, only
7818             skips missing values for ``float``, ``complex``, and ``object``
7819             dtypes; other dtypes either do not have a sentinel missing value
7820             (``int``) or ``skipna=True`` has not been implemented
7821             (``datetime64`` or ``timedelta64``).
7822         fill_value : Any, default: NaN
7823             Value to be filled in case all of the values along a dimension are
7824             null.  By default this is NaN.  The fill value and result are
7825             automatically converted to a compatible dtype if possible.
7826             Ignored if ``skipna`` is False.
7827         keep_attrs : bool or None, optional
7828             If True, the attributes (``attrs``) will be copied from the
7829             original object to the new one. If False, the new object
7830             will be returned without attributes.
7831 
7832         Returns
7833         -------
7834         reduced : Dataset
7835             New `Dataset` object with `idxmin` applied to its data and the
7836             indicated dimension removed.
7837 
7838         See Also
7839         --------
7840         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
7841 
7842         Examples
7843         --------
7844         >>> array1 = xr.DataArray(
7845         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
7846         ... )
7847         >>> array2 = xr.DataArray(
7848         ...     [
7849         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
7850         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
7851         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
7852         ...     ],
7853         ...     dims=["y", "x"],
7854         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
7855         ... )
7856         >>> ds = xr.Dataset({"int": array1, "float": array2})
7857         >>> ds.min(dim="x")
7858         <xarray.Dataset>
7859         Dimensions:  (y: 3)
7860         Coordinates:
7861           * y        (y) int64 -1 0 1
7862         Data variables:
7863             int      int64 -2
7864             float    (y) float64 -2.0 -4.0 1.0
7865         >>> ds.argmin(dim="x")
7866         <xarray.Dataset>
7867         Dimensions:  (y: 3)
7868         Coordinates:
7869           * y        (y) int64 -1 0 1
7870         Data variables:
7871             int      int64 4
7872             float    (y) int64 4 0 2
7873         >>> ds.idxmin(dim="x")
7874         <xarray.Dataset>
7875         Dimensions:  (y: 3)
7876         Coordinates:
7877           * y        (y) int64 -1 0 1
7878         Data variables:
7879             int      <U1 'e'
7880             float    (y) object 'e' 'a' 'c'
7881         """
7882         return self.map(
7883             methodcaller(
7884                 "idxmin",
7885                 dim=dim,
7886                 skipna=skipna,
7887                 fill_value=fill_value,
7888                 keep_attrs=keep_attrs,
7889             )
7890         )
7891 
7892     def idxmax(
7893         self: T_Dataset,
7894         dim: Hashable | None = None,
7895         skipna: bool | None = None,
7896         fill_value: Any = xrdtypes.NA,
7897         keep_attrs: bool | None = None,
7898     ) -> T_Dataset:
7899         """Return the coordinate label of the maximum value along a dimension.
7900 
7901         Returns a new `Dataset` named after the dimension with the values of
7902         the coordinate labels along that dimension corresponding to maximum
7903         values along that dimension.
7904 
7905         In comparison to :py:meth:`~Dataset.argmax`, this returns the
7906         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
7907 
7908         Parameters
7909         ----------
7910         dim : str, optional
7911             Dimension over which to apply `idxmax`.  This is optional for 1D
7912             variables, but required for variables with 2 or more dimensions.
7913         skipna : bool or None, optional
7914             If True, skip missing values (as marked by NaN). By default, only
7915             skips missing values for ``float``, ``complex``, and ``object``
7916             dtypes; other dtypes either do not have a sentinel missing value
7917             (``int``) or ``skipna=True`` has not been implemented
7918             (``datetime64`` or ``timedelta64``).
7919         fill_value : Any, default: NaN
7920             Value to be filled in case all of the values along a dimension are
7921             null.  By default this is NaN.  The fill value and result are
7922             automatically converted to a compatible dtype if possible.
7923             Ignored if ``skipna`` is False.
7924         keep_attrs : bool or None, optional
7925             If True, the attributes (``attrs``) will be copied from the
7926             original object to the new one. If False, the new object
7927             will be returned without attributes.
7928 
7929         Returns
7930         -------
7931         reduced : Dataset
7932             New `Dataset` object with `idxmax` applied to its data and the
7933             indicated dimension removed.
7934 
7935         See Also
7936         --------
7937         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
7938 
7939         Examples
7940         --------
7941         >>> array1 = xr.DataArray(
7942         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
7943         ... )
7944         >>> array2 = xr.DataArray(
7945         ...     [
7946         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
7947         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
7948         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
7949         ...     ],
7950         ...     dims=["y", "x"],
7951         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
7952         ... )
7953         >>> ds = xr.Dataset({"int": array1, "float": array2})
7954         >>> ds.max(dim="x")
7955         <xarray.Dataset>
7956         Dimensions:  (y: 3)
7957         Coordinates:
7958           * y        (y) int64 -1 0 1
7959         Data variables:
7960             int      int64 2
7961             float    (y) float64 2.0 2.0 1.0
7962         >>> ds.argmax(dim="x")
7963         <xarray.Dataset>
7964         Dimensions:  (y: 3)
7965         Coordinates:
7966           * y        (y) int64 -1 0 1
7967         Data variables:
7968             int      int64 1
7969             float    (y) int64 0 2 2
7970         >>> ds.idxmax(dim="x")
7971         <xarray.Dataset>
7972         Dimensions:  (y: 3)
7973         Coordinates:
7974           * y        (y) int64 -1 0 1
7975         Data variables:
7976             int      <U1 'b'
7977             float    (y) object 'a' 'c' 'c'
7978         """
7979         return self.map(
7980             methodcaller(
7981                 "idxmax",
7982                 dim=dim,
7983                 skipna=skipna,
7984                 fill_value=fill_value,
7985                 keep_attrs=keep_attrs,
7986             )
7987         )
7988 
7989     def argmin(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
7990         """Indices of the minima of the member variables.
7991 
7992         If there are multiple minima, the indices of the first one found will be
7993         returned.
7994 
7995         Parameters
7996         ----------
7997         dim : Hashable, optional
7998             The dimension over which to find the minimum. By default, finds minimum over
7999             all dimensions - for now returning an int for backward compatibility, but
8000             this is deprecated, in future will be an error, since DataArray.argmin will
8001             return a dict with indices for all dimensions, which does not make sense for
8002             a Dataset.
8003         keep_attrs : bool, optional
8004             If True, the attributes (`attrs`) will be copied from the original
8005             object to the new one.  If False (default), the new object will be
8006             returned without attributes.
8007         skipna : bool, optional
8008             If True, skip missing values (as marked by NaN). By default, only
8009             skips missing values for float dtypes; other dtypes either do not
8010             have a sentinel missing value (int) or skipna=True has not been
8011             implemented (object, datetime64 or timedelta64).
8012 
8013         Returns
8014         -------
8015         result : Dataset
8016 
8017         See Also
8018         --------
8019         DataArray.argmin
8020         """
8021         if dim is None:
8022             warnings.warn(
8023                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8024                 "dim changes to return a dict of indices of each dimension, for "
8025                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8026                 "since we don't return a dict of Datasets.",
8027                 DeprecationWarning,
8028                 stacklevel=2,
8029             )
8030         if (
8031             dim is None
8032             or (not isinstance(dim, Sequence) and dim is not ...)
8033             or isinstance(dim, str)
8034         ):
8035             # Return int index if single dimension is passed, and is not part of a
8036             # sequence
8037             argmin_func = getattr(duck_array_ops, "argmin")
8038             return self.reduce(argmin_func, dim=dim, **kwargs)
8039         else:
8040             raise ValueError(
8041                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8042                 "dicts cannot be contained in a Dataset, so cannot call "
8043                 "Dataset.argmin() with a sequence or ... for dim"
8044             )
8045 
8046     def argmax(self: T_Dataset, dim: Hashable | None = None, **kwargs) -> T_Dataset:
8047         """Indices of the maxima of the member variables.
8048 
8049         If there are multiple maxima, the indices of the first one found will be
8050         returned.
8051 
8052         Parameters
8053         ----------
8054         dim : str, optional
8055             The dimension over which to find the maximum. By default, finds maximum over
8056             all dimensions - for now returning an int for backward compatibility, but
8057             this is deprecated, in future will be an error, since DataArray.argmax will
8058             return a dict with indices for all dimensions, which does not make sense for
8059             a Dataset.
8060         keep_attrs : bool, optional
8061             If True, the attributes (`attrs`) will be copied from the original
8062             object to the new one.  If False (default), the new object will be
8063             returned without attributes.
8064         skipna : bool, optional
8065             If True, skip missing values (as marked by NaN). By default, only
8066             skips missing values for float dtypes; other dtypes either do not
8067             have a sentinel missing value (int) or skipna=True has not been
8068             implemented (object, datetime64 or timedelta64).
8069 
8070         Returns
8071         -------
8072         result : Dataset
8073 
8074         See Also
8075         --------
8076         DataArray.argmax
8077 
8078         """
8079         if dim is None:
8080             warnings.warn(
8081                 "Once the behaviour of DataArray.argmin() and Variable.argmin() without "
8082                 "dim changes to return a dict of indices of each dimension, for "
8083                 "consistency it will be an error to call Dataset.argmin() with no argument,"
8084                 "since we don't return a dict of Datasets.",
8085                 DeprecationWarning,
8086                 stacklevel=2,
8087             )
8088         if (
8089             dim is None
8090             or (not isinstance(dim, Sequence) and dim is not ...)
8091             or isinstance(dim, str)
8092         ):
8093             # Return int index if single dimension is passed, and is not part of a
8094             # sequence
8095             argmax_func = getattr(duck_array_ops, "argmax")
8096             return self.reduce(argmax_func, dim=dim, **kwargs)
8097         else:
8098             raise ValueError(
8099                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
8100                 "dicts cannot be contained in a Dataset, so cannot call "
8101                 "Dataset.argmin() with a sequence or ... for dim"
8102             )
8103 
8104     def query(
8105         self: T_Dataset,
8106         queries: Mapping[Any, Any] | None = None,
8107         parser: QueryParserOptions = "pandas",
8108         engine: QueryEngineOptions = None,
8109         missing_dims: ErrorOptionsWithWarn = "raise",
8110         **queries_kwargs: Any,
8111     ) -> T_Dataset:
8112         """Return a new dataset with each array indexed along the specified
8113         dimension(s), where the indexers are given as strings containing
8114         Python expressions to be evaluated against the data variables in the
8115         dataset.
8116 
8117         Parameters
8118         ----------
8119         queries : dict-like, optional
8120             A dict-like with keys matching dimensions and values given by strings
8121             containing Python expressions to be evaluated against the data variables
8122             in the dataset. The expressions will be evaluated using the pandas
8123             eval() function, and can contain any valid Python expressions but cannot
8124             contain any Python statements.
8125         parser : {"pandas", "python"}, default: "pandas"
8126             The parser to use to construct the syntax tree from the expression.
8127             The default of 'pandas' parses code slightly different than standard
8128             Python. Alternatively, you can parse an expression using the 'python'
8129             parser to retain strict Python semantics.
8130         engine : {"python", "numexpr", None}, default: None
8131             The engine used to evaluate the expression. Supported engines are:
8132 
8133             - None: tries to use numexpr, falls back to python
8134             - "numexpr": evaluates expressions using numexpr
8135             - "python": performs operations as if you had eval’d in top level python
8136 
8137         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
8138             What to do if dimensions that should be selected from are not present in the
8139             Dataset:
8140 
8141             - "raise": raise an exception
8142             - "warn": raise a warning, and ignore the missing dimensions
8143             - "ignore": ignore the missing dimensions
8144 
8145         **queries_kwargs : {dim: query, ...}, optional
8146             The keyword arguments form of ``queries``.
8147             One of queries or queries_kwargs must be provided.
8148 
8149         Returns
8150         -------
8151         obj : Dataset
8152             A new Dataset with the same contents as this dataset, except each
8153             array and dimension is indexed by the results of the appropriate
8154             queries.
8155 
8156         See Also
8157         --------
8158         Dataset.isel
8159         pandas.eval
8160 
8161         Examples
8162         --------
8163         >>> a = np.arange(0, 5, 1)
8164         >>> b = np.linspace(0, 1, 5)
8165         >>> ds = xr.Dataset({"a": ("x", a), "b": ("x", b)})
8166         >>> ds
8167         <xarray.Dataset>
8168         Dimensions:  (x: 5)
8169         Dimensions without coordinates: x
8170         Data variables:
8171             a        (x) int64 0 1 2 3 4
8172             b        (x) float64 0.0 0.25 0.5 0.75 1.0
8173         >>> ds.query(x="a > 2")
8174         <xarray.Dataset>
8175         Dimensions:  (x: 2)
8176         Dimensions without coordinates: x
8177         Data variables:
8178             a        (x) int64 3 4
8179             b        (x) float64 0.75 1.0
8180         """
8181 
8182         # allow queries to be given either as a dict or as kwargs
8183         queries = either_dict_or_kwargs(queries, queries_kwargs, "query")
8184 
8185         # check queries
8186         for dim, expr in queries.items():
8187             if not isinstance(expr, str):
8188                 msg = f"expr for dim {dim} must be a string to be evaluated, {type(expr)} given"
8189                 raise ValueError(msg)
8190 
8191         # evaluate the queries to create the indexers
8192         indexers = {
8193             dim: pd.eval(expr, resolvers=[self], parser=parser, engine=engine)
8194             for dim, expr in queries.items()
8195         }
8196 
8197         # apply the selection
8198         return self.isel(indexers, missing_dims=missing_dims)
8199 
8200     def curvefit(
8201         self: T_Dataset,
8202         coords: str | DataArray | Iterable[str | DataArray],
8203         func: Callable[..., Any],
8204         reduce_dims: Hashable | Iterable[Hashable] | None = None,
8205         skipna: bool = True,
8206         p0: dict[str, Any] | None = None,
8207         bounds: dict[str, Any] | None = None,
8208         param_names: Sequence[str] | None = None,
8209         kwargs: dict[str, Any] | None = None,
8210     ) -> T_Dataset:
8211         """
8212         Curve fitting optimization for arbitrary functions.
8213 
8214         Wraps `scipy.optimize.curve_fit` with `apply_ufunc`.
8215 
8216         Parameters
8217         ----------
8218         coords : hashable, DataArray, or sequence of hashable or DataArray
8219             Independent coordinate(s) over which to perform the curve fitting. Must share
8220             at least one dimension with the calling object. When fitting multi-dimensional
8221             functions, supply `coords` as a sequence in the same order as arguments in
8222             `func`. To fit along existing dimensions of the calling object, `coords` can
8223             also be specified as a str or sequence of strs.
8224         func : callable
8225             User specified function in the form `f(x, *params)` which returns a numpy
8226             array of length `len(x)`. `params` are the fittable parameters which are optimized
8227             by scipy curve_fit. `x` can also be specified as a sequence containing multiple
8228             coordinates, e.g. `f((x0, x1), *params)`.
8229         reduce_dims : hashable or sequence of hashable
8230             Additional dimension(s) over which to aggregate while fitting. For example,
8231             calling `ds.curvefit(coords='time', reduce_dims=['lat', 'lon'], ...)` will
8232             aggregate all lat and lon points and fit the specified function along the
8233             time dimension.
8234         skipna : bool, default: True
8235             Whether to skip missing values when fitting. Default is True.
8236         p0 : dict-like, optional
8237             Optional dictionary of parameter names to initial guesses passed to the
8238             `curve_fit` `p0` arg. If none or only some parameters are passed, the rest will
8239             be assigned initial values following the default scipy behavior.
8240         bounds : dict-like, optional
8241             Optional dictionary of parameter names to bounding values passed to the
8242             `curve_fit` `bounds` arg. If none or only some parameters are passed, the rest
8243             will be unbounded following the default scipy behavior.
8244         param_names : sequence of hashable, optional
8245             Sequence of names for the fittable parameters of `func`. If not supplied,
8246             this will be automatically determined by arguments of `func`. `param_names`
8247             should be manually supplied when fitting a function that takes a variable
8248             number of parameters.
8249         **kwargs : optional
8250             Additional keyword arguments to passed to scipy curve_fit.
8251 
8252         Returns
8253         -------
8254         curvefit_results : Dataset
8255             A single dataset which contains:
8256 
8257             [var]_curvefit_coefficients
8258                 The coefficients of the best fit.
8259             [var]_curvefit_covariance
8260                 The covariance matrix of the coefficient estimates.
8261 
8262         See Also
8263         --------
8264         Dataset.polyfit
8265         scipy.optimize.curve_fit
8266         """
8267         from scipy.optimize import curve_fit
8268 
8269         from .alignment import broadcast
8270         from .computation import apply_ufunc
8271         from .dataarray import _THIS_ARRAY, DataArray
8272 
8273         if p0 is None:
8274             p0 = {}
8275         if bounds is None:
8276             bounds = {}
8277         if kwargs is None:
8278             kwargs = {}
8279 
8280         if not reduce_dims:
8281             reduce_dims_ = []
8282         elif isinstance(reduce_dims, str) or not isinstance(reduce_dims, Iterable):
8283             reduce_dims_ = [reduce_dims]
8284         else:
8285             reduce_dims_ = list(reduce_dims)
8286 
8287         if (
8288             isinstance(coords, str)
8289             or isinstance(coords, DataArray)
8290             or not isinstance(coords, Iterable)
8291         ):
8292             coords = [coords]
8293         coords_ = [self[coord] if isinstance(coord, str) else coord for coord in coords]
8294 
8295         # Determine whether any coords are dims on self
8296         for coord in coords_:
8297             reduce_dims_ += [c for c in self.dims if coord.equals(self[c])]
8298         reduce_dims_ = list(set(reduce_dims_))
8299         preserved_dims = list(set(self.dims) - set(reduce_dims_))
8300         if not reduce_dims_:
8301             raise ValueError(
8302                 "No arguments to `coords` were identified as a dimension on the calling "
8303                 "object, and no dims were supplied to `reduce_dims`. This would result "
8304                 "in fitting on scalar data."
8305             )
8306 
8307         # Broadcast all coords with each other
8308         coords_ = broadcast(*coords_)
8309         coords_ = [
8310             coord.broadcast_like(self, exclude=preserved_dims) for coord in coords_
8311         ]
8312 
8313         params, func_args = _get_func_args(func, param_names)
8314         param_defaults, bounds_defaults = _initialize_curvefit_params(
8315             params, p0, bounds, func_args
8316         )
8317         n_params = len(params)
8318         kwargs.setdefault("p0", [param_defaults[p] for p in params])
8319         kwargs.setdefault(
8320             "bounds",
8321             [
8322                 [bounds_defaults[p][0] for p in params],
8323                 [bounds_defaults[p][1] for p in params],
8324             ],
8325         )
8326 
8327         def _wrapper(Y, *coords_, **kwargs):
8328             # Wrap curve_fit with raveled coordinates and pointwise NaN handling
8329             x = np.vstack([c.ravel() for c in coords_])
8330             y = Y.ravel()
8331             if skipna:
8332                 mask = np.all([np.any(~np.isnan(x), axis=0), ~np.isnan(y)], axis=0)
8333                 x = x[:, mask]
8334                 y = y[mask]
8335                 if not len(y):
8336                     popt = np.full([n_params], np.nan)
8337                     pcov = np.full([n_params, n_params], np.nan)
8338                     return popt, pcov
8339             x = np.squeeze(x)
8340             popt, pcov = curve_fit(func, x, y, **kwargs)
8341             return popt, pcov
8342 
8343         result = type(self)()
8344         for name, da in self.data_vars.items():
8345             if name is _THIS_ARRAY:
8346                 name = ""
8347             else:
8348                 name = f"{str(name)}_"
8349 
8350             popt, pcov = apply_ufunc(
8351                 _wrapper,
8352                 da,
8353                 *coords_,
8354                 vectorize=True,
8355                 dask="parallelized",
8356                 input_core_dims=[reduce_dims_ for d in range(len(coords_) + 1)],
8357                 output_core_dims=[["param"], ["cov_i", "cov_j"]],
8358                 dask_gufunc_kwargs={
8359                     "output_sizes": {
8360                         "param": n_params,
8361                         "cov_i": n_params,
8362                         "cov_j": n_params,
8363                     },
8364                 },
8365                 output_dtypes=(np.float64, np.float64),
8366                 exclude_dims=set(reduce_dims_),
8367                 kwargs=kwargs,
8368             )
8369             result[name + "curvefit_coefficients"] = popt
8370             result[name + "curvefit_covariance"] = pcov
8371 
8372         result = result.assign_coords(
8373             {"param": params, "cov_i": params, "cov_j": params}
8374         )
8375         result.attrs = self.attrs.copy()
8376 
8377         return result
8378 
8379     def drop_duplicates(
8380         self: T_Dataset,
8381         dim: Hashable | Iterable[Hashable],
8382         keep: Literal["first", "last", False] = "first",
8383     ) -> T_Dataset:
8384         """Returns a new Dataset with duplicate dimension values removed.
8385 
8386         Parameters
8387         ----------
8388         dim : dimension label or labels
8389             Pass `...` to drop duplicates along all dimensions.
8390         keep : {"first", "last", False}, default: "first"
8391             Determines which duplicates (if any) to keep.
8392             - ``"first"`` : Drop duplicates except for the first occurrence.
8393             - ``"last"`` : Drop duplicates except for the last occurrence.
8394             - False : Drop all duplicates.
8395 
8396         Returns
8397         -------
8398         Dataset
8399 
8400         See Also
8401         --------
8402         DataArray.drop_duplicates
8403         """
8404         if isinstance(dim, str):
8405             dims: Iterable = (dim,)
8406         elif dim is ...:
8407             dims = self.dims
8408         elif not isinstance(dim, Iterable):
8409             dims = [dim]
8410         else:
8411             dims = dim
8412 
8413         missing_dims = set(dims) - set(self.dims)
8414         if missing_dims:
8415             raise ValueError(f"'{missing_dims}' not found in dimensions")
8416 
8417         indexes = {dim: ~self.get_index(dim).duplicated(keep=keep) for dim in dims}
8418         return self.isel(indexes)
8419 
8420     def convert_calendar(
8421         self: T_Dataset,
8422         calendar: CFCalendar,
8423         dim: Hashable = "time",
8424         align_on: Literal["date", "year", None] = None,
8425         missing: Any | None = None,
8426         use_cftime: bool | None = None,
8427     ) -> T_Dataset:
8428         """Convert the Dataset to another calendar.
8429 
8430         Only converts the individual timestamps, does not modify any data except
8431         in dropping invalid/surplus dates or inserting missing dates.
8432 
8433         If the source and target calendars are either no_leap, all_leap or a
8434         standard type, only the type of the time array is modified.
8435         When converting to a leap year from a non-leap year, the 29th of February
8436         is removed from the array. In the other direction the 29th of February
8437         will be missing in the output, unless `missing` is specified,
8438         in which case that value is inserted.
8439 
8440         For conversions involving `360_day` calendars, see Notes.
8441 
8442         This method is safe to use with sub-daily data as it doesn't touch the
8443         time part of the timestamps.
8444 
8445         Parameters
8446         ---------
8447         calendar : str
8448             The target calendar name.
8449         dim : Hashable, default: "time"
8450             Name of the time coordinate.
8451         align_on : {None, 'date', 'year'}, optional
8452             Must be specified when either source or target is a `360_day` calendar,
8453             ignored otherwise. See Notes.
8454         missing : Any or None, optional
8455             By default, i.e. if the value is None, this method will simply attempt
8456             to convert the dates in the source calendar to the same dates in the
8457             target calendar, and drop any of those that are not possible to
8458             represent.  If a value is provided, a new time coordinate will be
8459             created in the target calendar with the same frequency as the original
8460             time coordinate; for any dates that are not present in the source, the
8461             data will be filled with this value.  Note that using this mode requires
8462             that the source data have an inferable frequency; for more information
8463             see :py:func:`xarray.infer_freq`.  For certain frequency, source, and
8464             target calendar combinations, this could result in many missing values, see notes.
8465         use_cftime : bool or None, optional
8466             Whether to use cftime objects in the output, only used if `calendar`
8467             is one of {"proleptic_gregorian", "gregorian" or "standard"}.
8468             If True, the new time axis uses cftime objects.
8469             If None (default), it uses :py:class:`numpy.datetime64` values if the
8470             date range permits it, and :py:class:`cftime.datetime` objects if not.
8471             If False, it uses :py:class:`numpy.datetime64`  or fails.
8472 
8473         Returns
8474         -------
8475         Dataset
8476             Copy of the dataarray with the time coordinate converted to the
8477             target calendar. If 'missing' was None (default), invalid dates in
8478             the new calendar are dropped, but missing dates are not inserted.
8479             If `missing` was given, the new data is reindexed to have a time axis
8480             with the same frequency as the source, but in the new calendar; any
8481             missing datapoints are filled with `missing`.
8482 
8483         Notes
8484         -----
8485         Passing a value to `missing` is only usable if the source's time coordinate as an
8486         inferable frequencies (see :py:func:`~xarray.infer_freq`) and is only appropriate
8487         if the target coordinate, generated from this frequency, has dates equivalent to the
8488         source. It is usually **not** appropriate to use this mode with:
8489 
8490         - Period-end frequencies : 'A', 'Y', 'Q' or 'M', in opposition to 'AS' 'YS', 'QS' and 'MS'
8491         - Sub-monthly frequencies that do not divide a day evenly : 'W', 'nD' where `N != 1`
8492             or 'mH' where 24 % m != 0).
8493 
8494         If one of the source or target calendars is `"360_day"`, `align_on` must
8495         be specified and two options are offered.
8496 
8497         - "year"
8498             The dates are translated according to their relative position in the year,
8499             ignoring their original month and day information, meaning that the
8500             missing/surplus days are added/removed at regular intervals.
8501 
8502             From a `360_day` to a standard calendar, the output will be missing the
8503             following dates (day of year in parentheses):
8504 
8505             To a leap year:
8506                 January 31st (31), March 31st (91), June 1st (153), July 31st (213),
8507                 September 31st (275) and November 30th (335).
8508             To a non-leap year:
8509                 February 6th (36), April 19th (109), July 2nd (183),
8510                 September 12th (255), November 25th (329).
8511 
8512             From a standard calendar to a `"360_day"`, the following dates in the
8513             source array will be dropped:
8514 
8515             From a leap year:
8516                 January 31st (31), April 1st (92), June 1st (153), August 1st (214),
8517                 September 31st (275), December 1st (336)
8518             From a non-leap year:
8519                 February 6th (37), April 20th (110), July 2nd (183),
8520                 September 13th (256), November 25th (329)
8521 
8522             This option is best used on daily and subdaily data.
8523 
8524         - "date"
8525             The month/day information is conserved and invalid dates are dropped
8526             from the output. This means that when converting from a `"360_day"` to a
8527             standard calendar, all 31st (Jan, March, May, July, August, October and
8528             December) will be missing as there is no equivalent dates in the
8529             `"360_day"` calendar and the 29th (on non-leap years) and 30th of February
8530             will be dropped as there are no equivalent dates in a standard calendar.
8531 
8532             This option is best used with data on a frequency coarser than daily.
8533         """
8534         return convert_calendar(
8535             self,
8536             calendar,
8537             dim=dim,
8538             align_on=align_on,
8539             missing=missing,
8540             use_cftime=use_cftime,
8541         )
8542 
8543     def interp_calendar(
8544         self: T_Dataset,
8545         target: pd.DatetimeIndex | CFTimeIndex | DataArray,
8546         dim: Hashable = "time",
8547     ) -> T_Dataset:
8548         """Interpolates the Dataset to another calendar based on decimal year measure.
8549 
8550         Each timestamp in `source` and `target` are first converted to their decimal
8551         year equivalent then `source` is interpolated on the target coordinate.
8552         The decimal year of a timestamp is its year plus its sub-year component
8553         converted to the fraction of its year. For example "2000-03-01 12:00" is
8554         2000.1653 in a standard calendar or 2000.16301 in a `"noleap"` calendar.
8555 
8556         This method should only be used when the time (HH:MM:SS) information of
8557         time coordinate is not important.
8558 
8559         Parameters
8560         ----------
8561         target: DataArray or DatetimeIndex or CFTimeIndex
8562             The target time coordinate of a valid dtype
8563             (np.datetime64 or cftime objects)
8564         dim : Hashable, default: "time"
8565             The time coordinate name.
8566 
8567         Return
8568         ------
8569         DataArray
8570             The source interpolated on the decimal years of target,
8571         """
8572         return interp_calendar(self, target, dim=dim)
8573 
8574     def groupby(
8575         self,
8576         group: Hashable | DataArray | IndexVariable,
8577         squeeze: bool = True,
8578         restore_coord_dims: bool = False,
8579     ) -> DatasetGroupBy:
8580         """Returns a DatasetGroupBy object for performing grouped operations.
8581 
8582         Parameters
8583         ----------
8584         group : Hashable, DataArray or IndexVariable
8585             Array whose unique values should be used to group this array. If a
8586             string, must be the name of a variable contained in this dataset.
8587         squeeze : bool, default: True
8588             If "group" is a dimension of any arrays in this dataset, `squeeze`
8589             controls whether the subarrays have a dimension of length 1 along
8590             that dimension or if the dimension is squeezed out.
8591         restore_coord_dims : bool, default: False
8592             If True, also restore the dimension order of multi-dimensional
8593             coordinates.
8594 
8595         Returns
8596         -------
8597         grouped : DatasetGroupBy
8598             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8599             iterated over in the form of `(unique_value, grouped_array)` pairs.
8600 
8601         See Also
8602         --------
8603         Dataset.groupby_bins
8604         DataArray.groupby
8605         core.groupby.DatasetGroupBy
8606         pandas.DataFrame.groupby
8607         """
8608         from .groupby import DatasetGroupBy
8609 
8610         # While we don't generally check the type of every arg, passing
8611         # multiple dimensions as multiple arguments is common enough, and the
8612         # consequences hidden enough (strings evaluate as true) to warrant
8613         # checking here.
8614         # A future version could make squeeze kwarg only, but would face
8615         # backward-compat issues.
8616         if not isinstance(squeeze, bool):
8617             raise TypeError(
8618                 f"`squeeze` must be True or False, but {squeeze} was supplied"
8619             )
8620 
8621         return DatasetGroupBy(
8622             self, group, squeeze=squeeze, restore_coord_dims=restore_coord_dims
8623         )
8624 
8625     def groupby_bins(
8626         self,
8627         group: Hashable | DataArray | IndexVariable,
8628         bins: ArrayLike,
8629         right: bool = True,
8630         labels: ArrayLike | None = None,
8631         precision: int = 3,
8632         include_lowest: bool = False,
8633         squeeze: bool = True,
8634         restore_coord_dims: bool = False,
8635     ) -> DatasetGroupBy:
8636         """Returns a DatasetGroupBy object for performing grouped operations.
8637 
8638         Rather than using all unique values of `group`, the values are discretized
8639         first by applying `pandas.cut` [1]_ to `group`.
8640 
8641         Parameters
8642         ----------
8643         group : Hashable, DataArray or IndexVariable
8644             Array whose binned values should be used to group this array. If a
8645             string, must be the name of a variable contained in this dataset.
8646         bins : int or array-like
8647             If bins is an int, it defines the number of equal-width bins in the
8648             range of x. However, in this case, the range of x is extended by .1%
8649             on each side to include the min or max values of x. If bins is a
8650             sequence it defines the bin edges allowing for non-uniform bin
8651             width. No extension of the range of x is done in this case.
8652         right : bool, default: True
8653             Indicates whether the bins include the rightmost edge or not. If
8654             right == True (the default), then the bins [1,2,3,4] indicate
8655             (1,2], (2,3], (3,4].
8656         labels : array-like or bool, default: None
8657             Used as labels for the resulting bins. Must be of the same length as
8658             the resulting bins. If False, string bin labels are assigned by
8659             `pandas.cut`.
8660         precision : int, default: 3
8661             The precision at which to store and display the bins labels.
8662         include_lowest : bool, default: False
8663             Whether the first interval should be left-inclusive or not.
8664         squeeze : bool, default: True
8665             If "group" is a dimension of any arrays in this dataset, `squeeze`
8666             controls whether the subarrays have a dimension of length 1 along
8667             that dimension or if the dimension is squeezed out.
8668         restore_coord_dims : bool, default: False
8669             If True, also restore the dimension order of multi-dimensional
8670             coordinates.
8671 
8672         Returns
8673         -------
8674         grouped : DatasetGroupBy
8675             A `DatasetGroupBy` object patterned after `pandas.GroupBy` that can be
8676             iterated over in the form of `(unique_value, grouped_array)` pairs.
8677             The name of the group has the added suffix `_bins` in order to
8678             distinguish it from the original variable.
8679 
8680         See Also
8681         --------
8682         Dataset.groupby
8683         DataArray.groupby_bins
8684         core.groupby.DatasetGroupBy
8685         pandas.DataFrame.groupby
8686 
8687         References
8688         ----------
8689         .. [1] http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html
8690         """
8691         from .groupby import DatasetGroupBy
8692 
8693         return DatasetGroupBy(
8694             self,
8695             group,
8696             squeeze=squeeze,
8697             bins=bins,
8698             restore_coord_dims=restore_coord_dims,
8699             cut_kwargs={
8700                 "right": right,
8701                 "labels": labels,
8702                 "precision": precision,
8703                 "include_lowest": include_lowest,
8704             },
8705         )
8706 
8707     def weighted(self, weights: DataArray) -> DatasetWeighted:
8708         """
8709         Weighted Dataset operations.
8710 
8711         Parameters
8712         ----------
8713         weights : DataArray
8714             An array of weights associated with the values in this Dataset.
8715             Each value in the data contributes to the reduction operation
8716             according to its associated weight.
8717 
8718         Notes
8719         -----
8720         ``weights`` must be a DataArray and cannot contain missing values.
8721         Missing values can be replaced by ``weights.fillna(0)``.
8722 
8723         Returns
8724         -------
8725         core.weighted.DatasetWeighted
8726 
8727         See Also
8728         --------
8729         DataArray.weighted
8730         """
8731         from .weighted import DatasetWeighted
8732 
8733         return DatasetWeighted(self, weights)
8734 
8735     def rolling(
8736         self,
8737         dim: Mapping[Any, int] | None = None,
8738         min_periods: int | None = None,
8739         center: bool | Mapping[Any, bool] = False,
8740         **window_kwargs: int,
8741     ) -> DatasetRolling:
8742         """
8743         Rolling window object for Datasets.
8744 
8745         Parameters
8746         ----------
8747         dim : dict, optional
8748             Mapping from the dimension name to create the rolling iterator
8749             along (e.g. `time`) to its moving window size.
8750         min_periods : int or None, default: None
8751             Minimum number of observations in window required to have a value
8752             (otherwise result is NA). The default, None, is equivalent to
8753             setting min_periods equal to the size of the window.
8754         center : bool or Mapping to int, default: False
8755             Set the labels at the center of the window.
8756         **window_kwargs : optional
8757             The keyword arguments form of ``dim``.
8758             One of dim or window_kwargs must be provided.
8759 
8760         Returns
8761         -------
8762         core.rolling.DatasetRolling
8763 
8764         See Also
8765         --------
8766         core.rolling.DatasetRolling
8767         DataArray.rolling
8768         """
8769         from .rolling import DatasetRolling
8770 
8771         dim = either_dict_or_kwargs(dim, window_kwargs, "rolling")
8772         return DatasetRolling(self, dim, min_periods=min_periods, center=center)
8773 
8774     def coarsen(
8775         self,
8776         dim: Mapping[Any, int] | None = None,
8777         boundary: CoarsenBoundaryOptions = "exact",
8778         side: SideOptions | Mapping[Any, SideOptions] = "left",
8779         coord_func: str | Callable | Mapping[Any, str | Callable] = "mean",
8780         **window_kwargs: int,
8781     ) -> DatasetCoarsen:
8782         """
8783         Coarsen object for Datasets.
8784 
8785         Parameters
8786         ----------
8787         dim : mapping of hashable to int, optional
8788             Mapping from the dimension name to the window size.
8789         boundary : {"exact", "trim", "pad"}, default: "exact"
8790             If 'exact', a ValueError will be raised if dimension size is not a
8791             multiple of the window size. If 'trim', the excess entries are
8792             dropped. If 'pad', NA will be padded.
8793         side : {"left", "right"} or mapping of str to {"left", "right"}, default: "left"
8794         coord_func : str or mapping of hashable to str, default: "mean"
8795             function (name) that is applied to the coordinates,
8796             or a mapping from coordinate name to function (name).
8797 
8798         Returns
8799         -------
8800         core.rolling.DatasetCoarsen
8801 
8802         See Also
8803         --------
8804         core.rolling.DatasetCoarsen
8805         DataArray.coarsen
8806         """
8807         from .rolling import DatasetCoarsen
8808 
8809         dim = either_dict_or_kwargs(dim, window_kwargs, "coarsen")
8810         return DatasetCoarsen(
8811             self,
8812             dim,
8813             boundary=boundary,
8814             side=side,
8815             coord_func=coord_func,
8816         )
8817 
8818     def resample(
8819         self,
8820         indexer: Mapping[Any, str] | None = None,
8821         skipna: bool | None = None,
8822         closed: SideOptions | None = None,
8823         label: SideOptions | None = None,
8824         base: int = 0,
8825         keep_attrs: bool | None = None,
8826         loffset: datetime.timedelta | str | None = None,
8827         restore_coord_dims: bool | None = None,
8828         **indexer_kwargs: str,
8829     ) -> DatasetResample:
8830         """Returns a Resample object for performing resampling operations.
8831 
8832         Handles both downsampling and upsampling. The resampled
8833         dimension must be a datetime-like coordinate. If any intervals
8834         contain no values from the original object, they will be given
8835         the value ``NaN``.
8836 
8837         Parameters
8838         ----------
8839         indexer : Mapping of Hashable to str, optional
8840             Mapping from the dimension name to resample frequency [1]_. The
8841             dimension must be datetime-like.
8842         skipna : bool, optional
8843             Whether to skip missing values when aggregating in downsampling.
8844         closed : {"left", "right"}, optional
8845             Side of each interval to treat as closed.
8846         label : {"left", "right"}, optional
8847             Side of each interval to use for labeling.
8848         base : int, default = 0
8849             For frequencies that evenly subdivide 1 day, the "origin" of the
8850             aggregated intervals. For example, for "24H" frequency, base could
8851             range from 0 through 23.
8852         loffset : timedelta or str, optional
8853             Offset used to adjust the resampled time labels. Some pandas date
8854             offset strings are supported.
8855         restore_coord_dims : bool, optional
8856             If True, also restore the dimension order of multi-dimensional
8857             coordinates.
8858         **indexer_kwargs : str
8859             The keyword arguments form of ``indexer``.
8860             One of indexer or indexer_kwargs must be provided.
8861 
8862         Returns
8863         -------
8864         resampled : core.resample.DataArrayResample
8865             This object resampled.
8866 
8867         See Also
8868         --------
8869         DataArray.resample
8870         pandas.Series.resample
8871         pandas.DataFrame.resample
8872 
8873         References
8874         ----------
8875         .. [1] http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases
8876         """
8877         from .resample import DatasetResample
8878 
8879         return self._resample(
8880             resample_cls=DatasetResample,
8881             indexer=indexer,
8882             skipna=skipna,
8883             closed=closed,
8884             label=label,
8885             base=base,
8886             keep_attrs=keep_attrs,
8887             loffset=loffset,
8888             restore_coord_dims=restore_coord_dims,
8889             **indexer_kwargs,
8890         )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 04:16:29.175 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 04:16:29.176 | INFO     | Search-only: skipping patch selection
2025-10-18 04:16:29.247 | INFO     | Task pydata__xarray-6999 completed successfully.
2025-10-18 04:16:29.248 | INFO     | No patch generated. You can try running ACR again.
