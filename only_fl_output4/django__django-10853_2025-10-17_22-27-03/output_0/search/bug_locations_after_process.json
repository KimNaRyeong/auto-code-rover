[
    {
        "rel_file_path": "django/db/backends/sqlite3/base.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.0/django/db/backends/sqlite3/base.py",
        "start": 1,
        "end": 560,
        "class_name": null,
        "method_name": null,
        "code": "1 \"\"\"\n2 SQLite backend for the sqlite3 module in the standard library.\n3 \"\"\"\n4 import datetime\n5 import decimal\n6 import functools\n7 import math\n8 import operator\n9 import re\n10 import statistics\n11 import warnings\n12 from itertools import chain\n13 from sqlite3 import dbapi2 as Database\n14 \n15 import pytz\n16 \n17 from django.core.exceptions import ImproperlyConfigured\n18 from django.db import utils\n19 from django.db.backends import utils as backend_utils\n20 from django.db.backends.base.base import BaseDatabaseWrapper\n21 from django.utils import timezone\n22 from django.utils.dateparse import parse_datetime, parse_time\n23 from django.utils.duration import duration_microseconds\n24 \n25 from .client import DatabaseClient                          # isort:skip\n26 from .creation import DatabaseCreation                      # isort:skip\n27 from .features import DatabaseFeatures                      # isort:skip\n28 from .introspection import DatabaseIntrospection            # isort:skip\n29 from .operations import DatabaseOperations                  # isort:skip\n30 from .schema import DatabaseSchemaEditor                    # isort:skip\n31 \n32 \n33 def decoder(conv_func):\n34     \"\"\"\n35     Convert bytestrings from Python's sqlite3 interface to a regular string.\n36     \"\"\"\n37     return lambda s: conv_func(s.decode())\n38 \n39 \n40 def none_guard(func):\n41     \"\"\"\n42     Decorator that returns None if any of the arguments to the decorated\n43     function are None. Many SQL functions return NULL if any of their arguments\n44     are NULL. This decorator simplifies the implementation of this for the\n45     custom functions registered below.\n46     \"\"\"\n47     @functools.wraps(func)\n48     def wrapper(*args, **kwargs):\n49         return None if None in args else func(*args, **kwargs)\n50     return wrapper\n51 \n52 \n53 def list_aggregate(function):\n54     \"\"\"\n55     Return an aggregate class that accumulates values in a list and applies\n56     the provided function to the data.\n57     \"\"\"\n58     return type('ListAggregate', (list,), {'finalize': function, 'step': list.append})\n59 \n60 \n61 def check_sqlite_version():\n62     if Database.sqlite_version_info < (3, 8, 3):\n63         raise ImproperlyConfigured('SQLite 3.8.3 or later is required (found %s).' % Database.sqlite_version)\n64 \n65 \n66 check_sqlite_version()\n67 \n68 Database.register_converter(\"bool\", b'1'.__eq__)\n69 Database.register_converter(\"time\", decoder(parse_time))\n70 Database.register_converter(\"datetime\", decoder(parse_datetime))\n71 Database.register_converter(\"timestamp\", decoder(parse_datetime))\n72 Database.register_converter(\"TIMESTAMP\", decoder(parse_datetime))\n73 \n74 Database.register_adapter(decimal.Decimal, str)\n75 \n76 \n77 class DatabaseWrapper(BaseDatabaseWrapper):\n78     vendor = 'sqlite'\n79     display_name = 'SQLite'\n80     # SQLite doesn't actually support most of these types, but it \"does the right\n81     # thing\" given more verbose field definitions, so leave them as is so that\n82     # schema inspection is more useful.\n83     data_types = {\n84         'AutoField': 'integer',\n85         'BigAutoField': 'integer',\n86         'BinaryField': 'BLOB',\n87         'BooleanField': 'bool',\n88         'CharField': 'varchar(%(max_length)s)',\n89         'DateField': 'date',\n90         'DateTimeField': 'datetime',\n91         'DecimalField': 'decimal',\n92         'DurationField': 'bigint',\n93         'FileField': 'varchar(%(max_length)s)',\n94         'FilePathField': 'varchar(%(max_length)s)',\n95         'FloatField': 'real',\n96         'IntegerField': 'integer',\n97         'BigIntegerField': 'bigint',\n98         'IPAddressField': 'char(15)',\n99         'GenericIPAddressField': 'char(39)',\n100         'NullBooleanField': 'bool',\n101         'OneToOneField': 'integer',\n102         'PositiveIntegerField': 'integer unsigned',\n103         'PositiveSmallIntegerField': 'smallint unsigned',\n104         'SlugField': 'varchar(%(max_length)s)',\n105         'SmallIntegerField': 'smallint',\n106         'TextField': 'text',\n107         'TimeField': 'time',\n108         'UUIDField': 'char(32)',\n109     }\n110     data_type_check_constraints = {\n111         'PositiveIntegerField': '\"%(column)s\" >= 0',\n112         'PositiveSmallIntegerField': '\"%(column)s\" >= 0',\n113     }\n114     data_types_suffix = {\n115         'AutoField': 'AUTOINCREMENT',\n116         'BigAutoField': 'AUTOINCREMENT',\n117     }\n118     # SQLite requires LIKE statements to include an ESCAPE clause if the value\n119     # being escaped has a percent or underscore in it.\n120     # See https://www.sqlite.org/lang_expr.html for an explanation.\n121     operators = {\n122         'exact': '= %s',\n123         'iexact': \"LIKE %s ESCAPE '\\\\'\",\n124         'contains': \"LIKE %s ESCAPE '\\\\'\",\n125         'icontains': \"LIKE %s ESCAPE '\\\\'\",\n126         'regex': 'REGEXP %s',\n127         'iregex': \"REGEXP '(?i)' || %s\",\n128         'gt': '> %s',\n129         'gte': '>= %s',\n130         'lt': '< %s',\n131         'lte': '<= %s',\n132         'startswith': \"LIKE %s ESCAPE '\\\\'\",\n133         'endswith': \"LIKE %s ESCAPE '\\\\'\",\n134         'istartswith': \"LIKE %s ESCAPE '\\\\'\",\n135         'iendswith': \"LIKE %s ESCAPE '\\\\'\",\n136     }\n137 \n138     # The patterns below are used to generate SQL pattern lookup clauses when\n139     # the right-hand side of the lookup isn't a raw string (it might be an expression\n140     # or the result of a bilateral transformation).\n141     # In those cases, special characters for LIKE operators (e.g. \\, *, _) should be\n142     # escaped on database side.\n143     #\n144     # Note: we use str.format() here for readability as '%' is used as a wildcard for\n145     # the LIKE operator.\n146     pattern_esc = r\"REPLACE(REPLACE(REPLACE({}, '\\', '\\\\'), '%%', '\\%%'), '_', '\\_')\"\n147     pattern_ops = {\n148         'contains': r\"LIKE '%%' || {} || '%%' ESCAPE '\\'\",\n149         'icontains': r\"LIKE '%%' || UPPER({}) || '%%' ESCAPE '\\'\",\n150         'startswith': r\"LIKE {} || '%%' ESCAPE '\\'\",\n151         'istartswith': r\"LIKE UPPER({}) || '%%' ESCAPE '\\'\",\n152         'endswith': r\"LIKE '%%' || {} ESCAPE '\\'\",\n153         'iendswith': r\"LIKE '%%' || UPPER({}) ESCAPE '\\'\",\n154     }\n155 \n156     Database = Database\n157     SchemaEditorClass = DatabaseSchemaEditor\n158     # Classes instantiated in __init__().\n159     client_class = DatabaseClient\n160     creation_class = DatabaseCreation\n161     features_class = DatabaseFeatures\n162     introspection_class = DatabaseIntrospection\n163     ops_class = DatabaseOperations\n164 \n165     def get_connection_params(self):\n166         settings_dict = self.settings_dict\n167         if not settings_dict['NAME']:\n168             raise ImproperlyConfigured(\n169                 \"settings.DATABASES is improperly configured. \"\n170                 \"Please supply the NAME value.\")\n171         kwargs = {\n172             'database': settings_dict['NAME'],\n173             'detect_types': Database.PARSE_DECLTYPES | Database.PARSE_COLNAMES,\n174             **settings_dict['OPTIONS'],\n175         }\n176         # Always allow the underlying SQLite connection to be shareable\n177         # between multiple threads. The safe-guarding will be handled at a\n178         # higher level by the `BaseDatabaseWrapper.allow_thread_sharing`\n179         # property. This is necessary as the shareability is disabled by\n180         # default in pysqlite and it cannot be changed once a connection is\n181         # opened.\n182         if 'check_same_thread' in kwargs and kwargs['check_same_thread']:\n183             warnings.warn(\n184                 'The `check_same_thread` option was provided and set to '\n185                 'True. It will be overridden with False. Use the '\n186                 '`DatabaseWrapper.allow_thread_sharing` property instead '\n187                 'for controlling thread shareability.',\n188                 RuntimeWarning\n189             )\n190         kwargs.update({'check_same_thread': False, 'uri': True})\n191         return kwargs\n192 \n193     def get_new_connection(self, conn_params):\n194         conn = Database.connect(**conn_params)\n195         conn.create_function(\"django_date_extract\", 2, _sqlite_datetime_extract)\n196         conn.create_function(\"django_date_trunc\", 2, _sqlite_date_trunc)\n197         conn.create_function(\"django_datetime_cast_date\", 2, _sqlite_datetime_cast_date)\n198         conn.create_function(\"django_datetime_cast_time\", 2, _sqlite_datetime_cast_time)\n199         conn.create_function(\"django_datetime_extract\", 3, _sqlite_datetime_extract)\n200         conn.create_function(\"django_datetime_trunc\", 3, _sqlite_datetime_trunc)\n201         conn.create_function(\"django_time_extract\", 2, _sqlite_time_extract)\n202         conn.create_function(\"django_time_trunc\", 2, _sqlite_time_trunc)\n203         conn.create_function(\"django_time_diff\", 2, _sqlite_time_diff)\n204         conn.create_function(\"django_timestamp_diff\", 2, _sqlite_timestamp_diff)\n205         conn.create_function(\"django_format_dtdelta\", 3, _sqlite_format_dtdelta)\n206         conn.create_function('regexp', 2, _sqlite_regexp)\n207         conn.create_function('ACOS', 1, none_guard(math.acos))\n208         conn.create_function('ASIN', 1, none_guard(math.asin))\n209         conn.create_function('ATAN', 1, none_guard(math.atan))\n210         conn.create_function('ATAN2', 2, none_guard(math.atan2))\n211         conn.create_function('CEILING', 1, none_guard(math.ceil))\n212         conn.create_function('COS', 1, none_guard(math.cos))\n213         conn.create_function('COT', 1, none_guard(lambda x: 1 / math.tan(x)))\n214         conn.create_function('DEGREES', 1, none_guard(math.degrees))\n215         conn.create_function('EXP', 1, none_guard(math.exp))\n216         conn.create_function('FLOOR', 1, none_guard(math.floor))\n217         conn.create_function('LN', 1, none_guard(math.log))\n218         conn.create_function('LOG', 2, none_guard(lambda x, y: math.log(y, x)))\n219         conn.create_function('LPAD', 3, _sqlite_lpad)\n220         conn.create_function('MOD', 2, none_guard(math.fmod))\n221         conn.create_function('PI', 0, lambda: math.pi)\n222         conn.create_function('POWER', 2, none_guard(operator.pow))\n223         conn.create_function('RADIANS', 1, none_guard(math.radians))\n224         conn.create_function('REPEAT', 2, none_guard(operator.mul))\n225         conn.create_function('REVERSE', 1, none_guard(lambda x: x[::-1]))\n226         conn.create_function('RPAD', 3, _sqlite_rpad)\n227         conn.create_function('SIN', 1, none_guard(math.sin))\n228         conn.create_function('SQRT', 1, none_guard(math.sqrt))\n229         conn.create_function('TAN', 1, none_guard(math.tan))\n230         conn.create_aggregate('STDDEV_POP', 1, list_aggregate(statistics.pstdev))\n231         conn.create_aggregate('STDDEV_SAMP', 1, list_aggregate(statistics.stdev))\n232         conn.create_aggregate('VAR_POP', 1, list_aggregate(statistics.pvariance))\n233         conn.create_aggregate('VAR_SAMP', 1, list_aggregate(statistics.variance))\n234         conn.execute('PRAGMA foreign_keys = ON')\n235         return conn\n236 \n237     def init_connection_state(self):\n238         pass\n239 \n240     def create_cursor(self, name=None):\n241         return self.connection.cursor(factory=SQLiteCursorWrapper)\n242 \n243     def close(self):\n244         self.validate_thread_sharing()\n245         # If database is in memory, closing the connection destroys the\n246         # database. To prevent accidental data loss, ignore close requests on\n247         # an in-memory db.\n248         if not self.is_in_memory_db():\n249             BaseDatabaseWrapper.close(self)\n250 \n251     def _savepoint_allowed(self):\n252         # When 'isolation_level' is not None, sqlite3 commits before each\n253         # savepoint; it's a bug. When it is None, savepoints don't make sense\n254         # because autocommit is enabled. The only exception is inside 'atomic'\n255         # blocks. To work around that bug, on SQLite, 'atomic' starts a\n256         # transaction explicitly rather than simply disable autocommit.\n257         return self.in_atomic_block\n258 \n259     def _set_autocommit(self, autocommit):\n260         if autocommit:\n261             level = None\n262         else:\n263             # sqlite3's internal default is ''. It's different from None.\n264             # See Modules/_sqlite/connection.c.\n265             level = ''\n266         # 'isolation_level' is a misleading API.\n267         # SQLite always runs at the SERIALIZABLE isolation level.\n268         with self.wrap_database_errors:\n269             self.connection.isolation_level = level\n270 \n271     def disable_constraint_checking(self):\n272         with self.cursor() as cursor:\n273             cursor.execute('PRAGMA foreign_keys = OFF')\n274             # Foreign key constraints cannot be turned off while in a multi-\n275             # statement transaction. Fetch the current state of the pragma\n276             # to determine if constraints are effectively disabled.\n277             enabled = cursor.execute('PRAGMA foreign_keys').fetchone()[0]\n278         return not bool(enabled)\n279 \n280     def enable_constraint_checking(self):\n281         self.cursor().execute('PRAGMA foreign_keys = ON')\n282 \n283     def check_constraints(self, table_names=None):\n284         \"\"\"\n285         Check each table name in `table_names` for rows with invalid foreign\n286         key references. This method is intended to be used in conjunction with\n287         `disable_constraint_checking()` and `enable_constraint_checking()`, to\n288         determine if rows with invalid references were entered while constraint\n289         checks were off.\n290         \"\"\"\n291         if self.features.supports_pragma_foreign_key_check:\n292             with self.cursor() as cursor:\n293                 if table_names is None:\n294                     violations = self.cursor().execute('PRAGMA foreign_key_check').fetchall()\n295                 else:\n296                     violations = chain.from_iterable(\n297                         cursor.execute('PRAGMA foreign_key_check(%s)' % table_name).fetchall()\n298                         for table_name in table_names\n299                     )\n300                 # See https://www.sqlite.org/pragma.html#pragma_foreign_key_check\n301                 for table_name, rowid, referenced_table_name, foreign_key_index in violations:\n302                     foreign_key = cursor.execute(\n303                         'PRAGMA foreign_key_list(%s)' % table_name\n304                     ).fetchall()[foreign_key_index]\n305                     column_name, referenced_column_name = foreign_key[3:5]\n306                     primary_key_column_name = self.introspection.get_primary_key_column(cursor, table_name)\n307                     primary_key_value, bad_value = cursor.execute(\n308                         'SELECT %s, %s FROM %s WHERE rowid = %%s' % (\n309                             primary_key_column_name, column_name, table_name\n310                         ),\n311                         (rowid,),\n312                     ).fetchone()\n313                     raise utils.IntegrityError(\n314                         \"The row in table '%s' with primary key '%s' has an \"\n315                         \"invalid foreign key: %s.%s contains a value '%s' that \"\n316                         \"does not have a corresponding value in %s.%s.\" % (\n317                             table_name, primary_key_value, table_name, column_name,\n318                             bad_value, referenced_table_name, referenced_column_name\n319                         )\n320                     )\n321         else:\n322             with self.cursor() as cursor:\n323                 if table_names is None:\n324                     table_names = self.introspection.table_names(cursor)\n325                 for table_name in table_names:\n326                     primary_key_column_name = self.introspection.get_primary_key_column(cursor, table_name)\n327                     if not primary_key_column_name:\n328                         continue\n329                     key_columns = self.introspection.get_key_columns(cursor, table_name)\n330                     for column_name, referenced_table_name, referenced_column_name in key_columns:\n331                         cursor.execute(\n332                             \"\"\"\n333                             SELECT REFERRING.`%s`, REFERRING.`%s` FROM `%s` as REFERRING\n334                             LEFT JOIN `%s` as REFERRED\n335                             ON (REFERRING.`%s` = REFERRED.`%s`)\n336                             WHERE REFERRING.`%s` IS NOT NULL AND REFERRED.`%s` IS NULL\n337                             \"\"\"\n338                             % (\n339                                 primary_key_column_name, column_name, table_name,\n340                                 referenced_table_name, column_name, referenced_column_name,\n341                                 column_name, referenced_column_name,\n342                             )\n343                         )\n344                         for bad_row in cursor.fetchall():\n345                             raise utils.IntegrityError(\n346                                 \"The row in table '%s' with primary key '%s' has an \"\n347                                 \"invalid foreign key: %s.%s contains a value '%s' that \"\n348                                 \"does not have a corresponding value in %s.%s.\" % (\n349                                     table_name, bad_row[0], table_name, column_name,\n350                                     bad_row[1], referenced_table_name, referenced_column_name,\n351                                 )\n352                             )\n353 \n354     def is_usable(self):\n355         return True\n356 \n357     def _start_transaction_under_autocommit(self):\n358         \"\"\"\n359         Start a transaction explicitly in autocommit mode.\n360 \n361         Staying in autocommit mode works around a bug of sqlite3 that breaks\n362         savepoints when autocommit is disabled.\n363         \"\"\"\n364         self.cursor().execute(\"BEGIN\")\n365 \n366     def is_in_memory_db(self):\n367         return self.creation.is_in_memory_db(self.settings_dict['NAME'])\n368 \n369 \n370 FORMAT_QMARK_REGEX = re.compile(r'(?<!%)%s')\n371 \n372 \n373 class SQLiteCursorWrapper(Database.Cursor):\n374     \"\"\"\n375     Django uses \"format\" style placeholders, but pysqlite2 uses \"qmark\" style.\n376     This fixes it -- but note that if you want to use a literal \"%s\" in a query,\n377     you'll need to use \"%%s\".\n378     \"\"\"\n379     def execute(self, query, params=None):\n380         if params is None:\n381             return Database.Cursor.execute(self, query)\n382         query = self.convert_query(query)\n383         return Database.Cursor.execute(self, query, params)\n384 \n385     def executemany(self, query, param_list):\n386         query = self.convert_query(query)\n387         return Database.Cursor.executemany(self, query, param_list)\n388 \n389     def convert_query(self, query):\n390         return FORMAT_QMARK_REGEX.sub('?', query).replace('%%', '%')\n391 \n392 \n393 def _sqlite_datetime_parse(dt, tzname=None):\n394     if dt is None:\n395         return None\n396     try:\n397         dt = backend_utils.typecast_timestamp(dt)\n398     except (TypeError, ValueError):\n399         return None\n400     if tzname is not None:\n401         dt = timezone.localtime(dt, pytz.timezone(tzname))\n402     return dt\n403 \n404 \n405 def _sqlite_date_trunc(lookup_type, dt):\n406     dt = _sqlite_datetime_parse(dt)\n407     if dt is None:\n408         return None\n409     if lookup_type == 'year':\n410         return \"%i-01-01\" % dt.year\n411     elif lookup_type == 'quarter':\n412         month_in_quarter = dt.month - (dt.month - 1) % 3\n413         return '%i-%02i-01' % (dt.year, month_in_quarter)\n414     elif lookup_type == 'month':\n415         return \"%i-%02i-01\" % (dt.year, dt.month)\n416     elif lookup_type == 'week':\n417         dt = dt - datetime.timedelta(days=dt.weekday())\n418         return \"%i-%02i-%02i\" % (dt.year, dt.month, dt.day)\n419     elif lookup_type == 'day':\n420         return \"%i-%02i-%02i\" % (dt.year, dt.month, dt.day)\n421 \n422 \n423 def _sqlite_time_trunc(lookup_type, dt):\n424     if dt is None:\n425         return None\n426     try:\n427         dt = backend_utils.typecast_time(dt)\n428     except (ValueError, TypeError):\n429         return None\n430     if lookup_type == 'hour':\n431         return \"%02i:00:00\" % dt.hour\n432     elif lookup_type == 'minute':\n433         return \"%02i:%02i:00\" % (dt.hour, dt.minute)\n434     elif lookup_type == 'second':\n435         return \"%02i:%02i:%02i\" % (dt.hour, dt.minute, dt.second)\n436 \n437 \n438 def _sqlite_datetime_cast_date(dt, tzname):\n439     dt = _sqlite_datetime_parse(dt, tzname)\n440     if dt is None:\n441         return None\n442     return dt.date().isoformat()\n443 \n444 \n445 def _sqlite_datetime_cast_time(dt, tzname):\n446     dt = _sqlite_datetime_parse(dt, tzname)\n447     if dt is None:\n448         return None\n449     return dt.time().isoformat()\n450 \n451 \n452 def _sqlite_datetime_extract(lookup_type, dt, tzname=None):\n453     dt = _sqlite_datetime_parse(dt, tzname)\n454     if dt is None:\n455         return None\n456     if lookup_type == 'week_day':\n457         return (dt.isoweekday() % 7) + 1\n458     elif lookup_type == 'week':\n459         return dt.isocalendar()[1]\n460     elif lookup_type == 'quarter':\n461         return math.ceil(dt.month / 3)\n462     elif lookup_type == 'iso_year':\n463         return dt.isocalendar()[0]\n464     else:\n465         return getattr(dt, lookup_type)\n466 \n467 \n468 def _sqlite_datetime_trunc(lookup_type, dt, tzname):\n469     dt = _sqlite_datetime_parse(dt, tzname)\n470     if dt is None:\n471         return None\n472     if lookup_type == 'year':\n473         return \"%i-01-01 00:00:00\" % dt.year\n474     elif lookup_type == 'quarter':\n475         month_in_quarter = dt.month - (dt.month - 1) % 3\n476         return '%i-%02i-01 00:00:00' % (dt.year, month_in_quarter)\n477     elif lookup_type == 'month':\n478         return \"%i-%02i-01 00:00:00\" % (dt.year, dt.month)\n479     elif lookup_type == 'week':\n480         dt = dt - datetime.timedelta(days=dt.weekday())\n481         return \"%i-%02i-%02i 00:00:00\" % (dt.year, dt.month, dt.day)\n482     elif lookup_type == 'day':\n483         return \"%i-%02i-%02i 00:00:00\" % (dt.year, dt.month, dt.day)\n484     elif lookup_type == 'hour':\n485         return \"%i-%02i-%02i %02i:00:00\" % (dt.year, dt.month, dt.day, dt.hour)\n486     elif lookup_type == 'minute':\n487         return \"%i-%02i-%02i %02i:%02i:00\" % (dt.year, dt.month, dt.day, dt.hour, dt.minute)\n488     elif lookup_type == 'second':\n489         return \"%i-%02i-%02i %02i:%02i:%02i\" % (dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second)\n490 \n491 \n492 def _sqlite_time_extract(lookup_type, dt):\n493     if dt is None:\n494         return None\n495     try:\n496         dt = backend_utils.typecast_time(dt)\n497     except (ValueError, TypeError):\n498         return None\n499     return getattr(dt, lookup_type)\n500 \n501 \n502 @none_guard\n503 def _sqlite_format_dtdelta(conn, lhs, rhs):\n504     \"\"\"\n505     LHS and RHS can be either:\n506     - An integer number of microseconds\n507     - A string representing a datetime\n508     \"\"\"\n509     try:\n510         real_lhs = datetime.timedelta(0, 0, lhs) if isinstance(lhs, int) else backend_utils.typecast_timestamp(lhs)\n511         real_rhs = datetime.timedelta(0, 0, rhs) if isinstance(rhs, int) else backend_utils.typecast_timestamp(rhs)\n512         if conn.strip() == '+':\n513             out = real_lhs + real_rhs\n514         else:\n515             out = real_lhs - real_rhs\n516     except (ValueError, TypeError):\n517         return None\n518     # typecast_timestamp returns a date or a datetime without timezone.\n519     # It will be formatted as \"%Y-%m-%d\" or \"%Y-%m-%d %H:%M:%S[.%f]\"\n520     return str(out)\n521 \n522 \n523 @none_guard\n524 def _sqlite_time_diff(lhs, rhs):\n525     left = backend_utils.typecast_time(lhs)\n526     right = backend_utils.typecast_time(rhs)\n527     return (\n528         (left.hour * 60 * 60 * 1000000) +\n529         (left.minute * 60 * 1000000) +\n530         (left.second * 1000000) +\n531         (left.microsecond) -\n532         (right.hour * 60 * 60 * 1000000) -\n533         (right.minute * 60 * 1000000) -\n534         (right.second * 1000000) -\n535         (right.microsecond)\n536     )\n537 \n538 \n539 @none_guard\n540 def _sqlite_timestamp_diff(lhs, rhs):\n541     left = backend_utils.typecast_timestamp(lhs)\n542     right = backend_utils.typecast_timestamp(rhs)\n543     return duration_microseconds(left - right)\n544 \n545 \n546 @none_guard\n547 def _sqlite_regexp(re_pattern, re_string):\n548     return bool(re.search(re_pattern, str(re_string)))\n549 \n550 \n551 @none_guard\n552 def _sqlite_lpad(text, length, fill_text):\n553     if len(text) >= length:\n554         return text[:length]\n555     return (fill_text * length)[:length - len(text)] + text\n556 \n557 \n558 @none_guard\n559 def _sqlite_rpad(text, length, fill_text):\n560     return (text + fill_text * length)[:length]\n",
        "intended_behavior": "To resolve the issue, a mechanism should be introduced within the SQLite backend, particularly in the initial setup or query execution pathways that checks the SQLite version in use. If the version is older than 3.25.0, the backend should mark window functions as unsupported. This can be done by setting a feature flag specifically for window functions within the `features` property of the backend class. Before attempting to execute a query involving window functions, the Django ORM should check this feature flag. If unsupported, a descriptive error should be raised informing the developer that their current SQLite version does not support window functions."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.0/django/db/models/query.py",
        "start": 1011,
        "end": 1055,
        "class_name": "QuerySet",
        "method_name": "annotate",
        "code": "1011     def annotate(self, *args, **kwargs):\n1012         \"\"\"\n1013         Return a query set in which the returned objects have been annotated\n1014         with extra data or aggregations.\n1015         \"\"\"\n1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n1017         annotations = {}\n1018         for arg in args:\n1019             # The default_alias property may raise a TypeError.\n1020             try:\n1021                 if arg.default_alias in kwargs:\n1022                     raise ValueError(\"The named annotation '%s' conflicts with the \"\n1023                                      \"default name for another annotation.\"\n1024                                      % arg.default_alias)\n1025             except TypeError:\n1026                 raise TypeError(\"Complex annotations require an alias\")\n1027             annotations[arg.default_alias] = arg\n1028         annotations.update(kwargs)\n1029 \n1030         clone = self._chain()\n1031         names = self._fields\n1032         if names is None:\n1033             names = set(chain.from_iterable(\n1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n1035                 for field in self.model._meta.get_fields()\n1036             ))\n1037 \n1038         for alias, annotation in annotations.items():\n1039             if alias in names:\n1040                 raise ValueError(\"The annotation '%s' conflicts with a field on \"\n1041                                  \"the model.\" % alias)\n1042             if isinstance(annotation, FilteredRelation):\n1043                 clone.query.add_filtered_relation(annotation, alias)\n1044             else:\n1045                 clone.query.add_annotation(annotation, alias, is_summary=False)\n1046 \n1047         for alias, annotation in clone.query.annotations.items():\n1048             if alias in annotations and annotation.contains_aggregate:\n1049                 if clone._fields is None:\n1050                     clone.query.group_by = True\n1051                 else:\n1052                     clone.query.set_group_by()\n1053                 break\n1054 \n1055         return clone\n",
        "intended_behavior": "In the `annotate` method, when annotations that involve window functions are being added to a queryset, the ORM should verify if the underlying database engine (in contexts where SQLite is used) supports window functions. This could be achieved by consulting a feature flag (as described previously) indicating the availability of window functions. If the feature is unsupported, this method should raise a Django-specific exception (e.g., `FeatureNotSupportedError`) with a clear message indicating the absence of window function support in the SQLite version being used. This check would help developers understand the limitations of their current environment, rather than facing a low-level `OperationalError` from the SQLite engine."
    },
    {
        "rel_file_path": "django/db/models/query.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/django__django/setup_django__django__3.0/django/db/models/query.py",
        "start": 185,
        "end": 1301,
        "class_name": "QuerySet",
        "method_name": null,
        "code": "185 class QuerySet:\n186     \"\"\"Represent a lazy database lookup for a set of objects.\"\"\"\n187 \n188     def __init__(self, model=None, query=None, using=None, hints=None):\n189         self.model = model\n190         self._db = using\n191         self._hints = hints or {}\n192         self.query = query or sql.Query(self.model)\n193         self._result_cache = None\n194         self._sticky_filter = False\n195         self._for_write = False\n196         self._prefetch_related_lookups = ()\n197         self._prefetch_done = False\n198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}\n199         self._iterable_class = ModelIterable\n200         self._fields = None\n201 \n202     def as_manager(cls):\n203         # Address the circular dependency between `Queryset` and `Manager`.\n204         from django.db.models.manager import Manager\n205         manager = Manager.from_queryset(cls)()\n206         manager._built_with_as_manager = True\n207         return manager\n208     as_manager.queryset_only = True\n209     as_manager = classmethod(as_manager)\n210 \n211     ########################\n212     # PYTHON MAGIC METHODS #\n213     ########################\n214 \n215     def __deepcopy__(self, memo):\n216         \"\"\"Don't populate the QuerySet's cache.\"\"\"\n217         obj = self.__class__()\n218         for k, v in self.__dict__.items():\n219             if k == '_result_cache':\n220                 obj.__dict__[k] = None\n221             else:\n222                 obj.__dict__[k] = copy.deepcopy(v, memo)\n223         return obj\n224 \n225     def __getstate__(self):\n226         # Force the cache to be fully populated.\n227         self._fetch_all()\n228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}\n229 \n230     def __setstate__(self, state):\n231         msg = None\n232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)\n233         if pickled_version:\n234             current_version = get_version()\n235             if current_version != pickled_version:\n236                 msg = (\n237                     \"Pickled queryset instance's Django version %s does not \"\n238                     \"match the current version %s.\" % (pickled_version, current_version)\n239                 )\n240         else:\n241             msg = \"Pickled queryset instance's Django version is not specified.\"\n242 \n243         if msg:\n244             warnings.warn(msg, RuntimeWarning, stacklevel=2)\n245 \n246         self.__dict__.update(state)\n247 \n248     def __repr__(self):\n249         data = list(self[:REPR_OUTPUT_SIZE + 1])\n250         if len(data) > REPR_OUTPUT_SIZE:\n251             data[-1] = \"\u2026(remaining elements truncated)\u2026\"\n252         return '<%s %r>' % (self.__class__.__name__, data)\n253 \n254     def __len__(self):\n255         self._fetch_all()\n256         return len(self._result_cache)\n257 \n258     def __iter__(self):\n259         \"\"\"\n260         The queryset iterator protocol uses three nested iterators in the\n261         default case:\n262             1. sql.compiler.execute_sql()\n263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)\n264                  using cursor.fetchmany(). This part is responsible for\n265                  doing some column masking, and returning the rows in chunks.\n266             2. sql.compiler.results_iter()\n267                - Returns one row at time. At this point the rows are still just\n268                  tuples. In some cases the return values are converted to\n269                  Python values at this location.\n270             3. self.iterator()\n271                - Responsible for turning the rows into model objects.\n272         \"\"\"\n273         self._fetch_all()\n274         return iter(self._result_cache)\n275 \n276     def __bool__(self):\n277         self._fetch_all()\n278         return bool(self._result_cache)\n279 \n280     def __getitem__(self, k):\n281         \"\"\"Retrieve an item or slice from the set of results.\"\"\"\n282         if not isinstance(k, (int, slice)):\n283             raise TypeError\n284         assert ((not isinstance(k, slice) and (k >= 0)) or\n285                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and\n286                  (k.stop is None or k.stop >= 0))), \\\n287             \"Negative indexing is not supported.\"\n288 \n289         if self._result_cache is not None:\n290             return self._result_cache[k]\n291 \n292         if isinstance(k, slice):\n293             qs = self._chain()\n294             if k.start is not None:\n295                 start = int(k.start)\n296             else:\n297                 start = None\n298             if k.stop is not None:\n299                 stop = int(k.stop)\n300             else:\n301                 stop = None\n302             qs.query.set_limits(start, stop)\n303             return list(qs)[::k.step] if k.step else qs\n304 \n305         qs = self._chain()\n306         qs.query.set_limits(k, k + 1)\n307         qs._fetch_all()\n308         return qs._result_cache[0]\n309 \n310     def __and__(self, other):\n311         self._merge_sanity_check(other)\n312         if isinstance(other, EmptyQuerySet):\n313             return other\n314         if isinstance(self, EmptyQuerySet):\n315             return self\n316         combined = self._chain()\n317         combined._merge_known_related_objects(other)\n318         combined.query.combine(other.query, sql.AND)\n319         return combined\n320 \n321     def __or__(self, other):\n322         self._merge_sanity_check(other)\n323         if isinstance(self, EmptyQuerySet):\n324             return other\n325         if isinstance(other, EmptyQuerySet):\n326             return self\n327         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))\n328         combined = query._chain()\n329         combined._merge_known_related_objects(other)\n330         if not other.query.can_filter():\n331             other = other.model._base_manager.filter(pk__in=other.values('pk'))\n332         combined.query.combine(other.query, sql.OR)\n333         return combined\n334 \n335     ####################################\n336     # METHODS THAT DO DATABASE QUERIES #\n337     ####################################\n338 \n339     def _iterator(self, use_chunked_fetch, chunk_size):\n340         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)\n341 \n342     def iterator(self, chunk_size=2000):\n343         \"\"\"\n344         An iterator over the results from applying this QuerySet to the\n345         database.\n346         \"\"\"\n347         if chunk_size <= 0:\n348             raise ValueError('Chunk size must be strictly positive.')\n349         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')\n350         return self._iterator(use_chunked_fetch, chunk_size)\n351 \n352     def aggregate(self, *args, **kwargs):\n353         \"\"\"\n354         Return a dictionary containing the calculations (aggregation)\n355         over the current queryset.\n356 \n357         If args is present the expression is passed as a kwarg using\n358         the Aggregate object's default alias.\n359         \"\"\"\n360         if self.query.distinct_fields:\n361             raise NotImplementedError(\"aggregate() + distinct(fields) not implemented.\")\n362         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')\n363         for arg in args:\n364             # The default_alias property raises TypeError if default_alias\n365             # can't be set automatically or AttributeError if it isn't an\n366             # attribute.\n367             try:\n368                 arg.default_alias\n369             except (AttributeError, TypeError):\n370                 raise TypeError(\"Complex aggregates require an alias\")\n371             kwargs[arg.default_alias] = arg\n372 \n373         query = self.query.chain()\n374         for (alias, aggregate_expr) in kwargs.items():\n375             query.add_annotation(aggregate_expr, alias, is_summary=True)\n376             if not query.annotations[alias].contains_aggregate:\n377                 raise TypeError(\"%s is not an aggregate expression\" % alias)\n378         return query.get_aggregation(self.db, kwargs)\n379 \n380     def count(self):\n381         \"\"\"\n382         Perform a SELECT COUNT() and return the number of records as an\n383         integer.\n384 \n385         If the QuerySet is already fully cached, return the length of the\n386         cached results set to avoid multiple SELECT COUNT(*) calls.\n387         \"\"\"\n388         if self._result_cache is not None:\n389             return len(self._result_cache)\n390 \n391         return self.query.get_count(using=self.db)\n392 \n393     def get(self, *args, **kwargs):\n394         \"\"\"\n395         Perform the query and return a single object matching the given\n396         keyword arguments.\n397         \"\"\"\n398         clone = self.filter(*args, **kwargs)\n399         if self.query.can_filter() and not self.query.distinct_fields:\n400             clone = clone.order_by()\n401         num = len(clone)\n402         if num == 1:\n403             return clone._result_cache[0]\n404         if not num:\n405             raise self.model.DoesNotExist(\n406                 \"%s matching query does not exist.\" %\n407                 self.model._meta.object_name\n408             )\n409         raise self.model.MultipleObjectsReturned(\n410             \"get() returned more than one %s -- it returned %s!\" %\n411             (self.model._meta.object_name, num)\n412         )\n413 \n414     def create(self, **kwargs):\n415         \"\"\"\n416         Create a new object with the given kwargs, saving it to the database\n417         and returning the created object.\n418         \"\"\"\n419         obj = self.model(**kwargs)\n420         self._for_write = True\n421         obj.save(force_insert=True, using=self.db)\n422         return obj\n423 \n424     def _populate_pk_values(self, objs):\n425         for obj in objs:\n426             if obj.pk is None:\n427                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)\n428 \n429     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):\n430         \"\"\"\n431         Insert each of the instances into the database. Do *not* call\n432         save() on each of the instances, do not send any pre/post_save\n433         signals, and do not set the primary key attribute if it is an\n434         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).\n435         Multi-table models are not supported.\n436         \"\"\"\n437         # When you bulk insert you don't get the primary keys back (if it's an\n438         # autoincrement, except if can_return_rows_from_bulk_insert=True), so\n439         # you can't insert into the child tables which references this. There\n440         # are two workarounds:\n441         # 1) This could be implemented if you didn't have an autoincrement pk\n442         # 2) You could do it by doing O(n) normal inserts into the parent\n443         #    tables to get the primary keys back and then doing a single bulk\n444         #    insert into the childmost table.\n445         # We currently set the primary keys on the objects when using\n446         # PostgreSQL via the RETURNING ID clause. It should be possible for\n447         # Oracle as well, but the semantics for extracting the primary keys is\n448         # trickier so it's not done yet.\n449         assert batch_size is None or batch_size > 0\n450         # Check that the parents share the same concrete model with the our\n451         # model to detect the inheritance pattern ConcreteGrandParent ->\n452         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy\n453         # would not identify that case as involving multiple tables.\n454         for parent in self.model._meta.get_parent_list():\n455             if parent._meta.concrete_model is not self.model._meta.concrete_model:\n456                 raise ValueError(\"Can't bulk create a multi-table inherited model\")\n457         if not objs:\n458             return objs\n459         self._for_write = True\n460         connection = connections[self.db]\n461         fields = self.model._meta.concrete_fields\n462         objs = list(objs)\n463         self._populate_pk_values(objs)\n464         with transaction.atomic(using=self.db, savepoint=False):\n465             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)\n466             if objs_with_pk:\n467                 self._batched_insert(objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)\n468                 for obj_with_pk in objs_with_pk:\n469                     obj_with_pk._state.adding = False\n470                     obj_with_pk._state.db = self.db\n471             if objs_without_pk:\n472                 fields = [f for f in fields if not isinstance(f, AutoField)]\n473                 ids = self._batched_insert(objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts)\n474                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:\n475                     assert len(ids) == len(objs_without_pk)\n476                 for obj_without_pk, pk in zip(objs_without_pk, ids):\n477                     obj_without_pk.pk = pk\n478                     obj_without_pk._state.adding = False\n479                     obj_without_pk._state.db = self.db\n480 \n481         return objs\n482 \n483     def bulk_update(self, objs, fields, batch_size=None):\n484         \"\"\"\n485         Update the given fields in each of the given objects in the database.\n486         \"\"\"\n487         if batch_size is not None and batch_size < 0:\n488             raise ValueError('Batch size must be a positive integer.')\n489         if not fields:\n490             raise ValueError('Field names must be given to bulk_update().')\n491         objs = tuple(objs)\n492         if any(obj.pk is None for obj in objs):\n493             raise ValueError('All bulk_update() objects must have a primary key set.')\n494         fields = [self.model._meta.get_field(name) for name in fields]\n495         if any(not f.concrete or f.many_to_many for f in fields):\n496             raise ValueError('bulk_update() can only be used with concrete fields.')\n497         if any(f.primary_key for f in fields):\n498             raise ValueError('bulk_update() cannot be used with primary key fields.')\n499         if not objs:\n500             return\n501         # PK is used twice in the resulting update query, once in the filter\n502         # and once in the WHEN. Each field will also have one CAST.\n503         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)\n504         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size\n505         requires_casting = connections[self.db].features.requires_casted_case_in_updates\n506         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))\n507         updates = []\n508         for batch_objs in batches:\n509             update_kwargs = {}\n510             for field in fields:\n511                 when_statements = []\n512                 for obj in batch_objs:\n513                     attr = getattr(obj, field.attname)\n514                     if not isinstance(attr, Expression):\n515                         attr = Value(attr, output_field=field)\n516                     when_statements.append(When(pk=obj.pk, then=attr))\n517                 case_statement = Case(*when_statements, output_field=field)\n518                 if requires_casting:\n519                     case_statement = Cast(case_statement, output_field=field)\n520                 update_kwargs[field.attname] = case_statement\n521             updates.append(([obj.pk for obj in batch_objs], update_kwargs))\n522         with transaction.atomic(using=self.db, savepoint=False):\n523             for pks, update_kwargs in updates:\n524                 self.filter(pk__in=pks).update(**update_kwargs)\n525     bulk_update.alters_data = True\n526 \n527     def get_or_create(self, defaults=None, **kwargs):\n528         \"\"\"\n529         Look up an object with the given kwargs, creating one if necessary.\n530         Return a tuple of (object, created), where created is a boolean\n531         specifying whether an object was created.\n532         \"\"\"\n533         # The get() needs to be targeted at the write database in order\n534         # to avoid potential transaction consistency problems.\n535         self._for_write = True\n536         try:\n537             return self.get(**kwargs), False\n538         except self.model.DoesNotExist:\n539             params = self._extract_model_params(defaults, **kwargs)\n540             return self._create_object_from_params(kwargs, params)\n541 \n542     def update_or_create(self, defaults=None, **kwargs):\n543         \"\"\"\n544         Look up an object with the given kwargs, updating one with defaults\n545         if it exists, otherwise create a new one.\n546         Return a tuple (object, created), where created is a boolean\n547         specifying whether an object was created.\n548         \"\"\"\n549         defaults = defaults or {}\n550         self._for_write = True\n551         with transaction.atomic(using=self.db):\n552             try:\n553                 obj = self.select_for_update().get(**kwargs)\n554             except self.model.DoesNotExist:\n555                 params = self._extract_model_params(defaults, **kwargs)\n556                 # Lock the row so that a concurrent update is blocked until\n557                 # after update_or_create() has performed its save.\n558                 obj, created = self._create_object_from_params(kwargs, params, lock=True)\n559                 if created:\n560                     return obj, created\n561             for k, v in defaults.items():\n562                 setattr(obj, k, v() if callable(v) else v)\n563             obj.save(using=self.db)\n564         return obj, False\n565 \n566     def _create_object_from_params(self, lookup, params, lock=False):\n567         \"\"\"\n568         Try to create an object using passed params. Used by get_or_create()\n569         and update_or_create().\n570         \"\"\"\n571         try:\n572             with transaction.atomic(using=self.db):\n573                 params = {k: v() if callable(v) else v for k, v in params.items()}\n574                 obj = self.create(**params)\n575             return obj, True\n576         except IntegrityError as e:\n577             try:\n578                 qs = self.select_for_update() if lock else self\n579                 return qs.get(**lookup), False\n580             except self.model.DoesNotExist:\n581                 pass\n582             raise e\n583 \n584     def _extract_model_params(self, defaults, **kwargs):\n585         \"\"\"\n586         Prepare `params` for creating a model instance based on the given\n587         kwargs; for use by get_or_create() and update_or_create().\n588         \"\"\"\n589         defaults = defaults or {}\n590         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}\n591         params.update(defaults)\n592         property_names = self.model._meta._property_names\n593         invalid_params = []\n594         for param in params:\n595             try:\n596                 self.model._meta.get_field(param)\n597             except exceptions.FieldDoesNotExist:\n598                 # It's okay to use a model's property if it has a setter.\n599                 if not (param in property_names and getattr(self.model, param).fset):\n600                     invalid_params.append(param)\n601         if invalid_params:\n602             raise exceptions.FieldError(\n603                 \"Invalid field name(s) for model %s: '%s'.\" % (\n604                     self.model._meta.object_name,\n605                     \"', '\".join(sorted(invalid_params)),\n606                 ))\n607         return params\n608 \n609     def _earliest(self, *fields):\n610         \"\"\"\n611         Return the earliest object according to fields (if given) or by the\n612         model's Meta.get_latest_by.\n613         \"\"\"\n614         if fields:\n615             order_by = fields\n616         else:\n617             order_by = getattr(self.model._meta, 'get_latest_by')\n618             if order_by and not isinstance(order_by, (tuple, list)):\n619                 order_by = (order_by,)\n620         if order_by is None:\n621             raise ValueError(\n622                 \"earliest() and latest() require either fields as positional \"\n623                 \"arguments or 'get_latest_by' in the model's Meta.\"\n624             )\n625 \n626         assert self.query.can_filter(), \\\n627             \"Cannot change a query once a slice has been taken.\"\n628         obj = self._chain()\n629         obj.query.set_limits(high=1)\n630         obj.query.clear_ordering(force_empty=True)\n631         obj.query.add_ordering(*order_by)\n632         return obj.get()\n633 \n634     def earliest(self, *fields):\n635         return self._earliest(*fields)\n636 \n637     def latest(self, *fields):\n638         return self.reverse()._earliest(*fields)\n639 \n640     def first(self):\n641         \"\"\"Return the first object of a query or None if no match is found.\"\"\"\n642         for obj in (self if self.ordered else self.order_by('pk'))[:1]:\n643             return obj\n644 \n645     def last(self):\n646         \"\"\"Return the last object of a query or None if no match is found.\"\"\"\n647         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:\n648             return obj\n649 \n650     def in_bulk(self, id_list=None, *, field_name='pk'):\n651         \"\"\"\n652         Return a dictionary mapping each of the given IDs to the object with\n653         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.\n654         \"\"\"\n655         assert self.query.can_filter(), \\\n656             \"Cannot use 'limit' or 'offset' with in_bulk\"\n657         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:\n658             raise ValueError(\"in_bulk()'s field_name must be a unique field but %r isn't.\" % field_name)\n659         if id_list is not None:\n660             if not id_list:\n661                 return {}\n662             filter_key = '{}__in'.format(field_name)\n663             batch_size = connections[self.db].features.max_query_params\n664             id_list = tuple(id_list)\n665             # If the database has a limit on the number of query parameters\n666             # (e.g. SQLite), retrieve objects in batches if necessary.\n667             if batch_size and batch_size < len(id_list):\n668                 qs = ()\n669                 for offset in range(0, len(id_list), batch_size):\n670                     batch = id_list[offset:offset + batch_size]\n671                     qs += tuple(self.filter(**{filter_key: batch}).order_by())\n672             else:\n673                 qs = self.filter(**{filter_key: id_list}).order_by()\n674         else:\n675             qs = self._chain()\n676         return {getattr(obj, field_name): obj for obj in qs}\n677 \n678     def delete(self):\n679         \"\"\"Delete the records in the current QuerySet.\"\"\"\n680         assert self.query.can_filter(), \\\n681             \"Cannot use 'limit' or 'offset' with delete.\"\n682 \n683         if self._fields is not None:\n684             raise TypeError(\"Cannot call delete() after .values() or .values_list()\")\n685 \n686         del_query = self._chain()\n687 \n688         # The delete is actually 2 queries - one to find related objects,\n689         # and one to delete. Make sure that the discovery of related\n690         # objects is performed on the same database as the deletion.\n691         del_query._for_write = True\n692 \n693         # Disable non-supported fields.\n694         del_query.query.select_for_update = False\n695         del_query.query.select_related = False\n696         del_query.query.clear_ordering(force_empty=True)\n697 \n698         collector = Collector(using=del_query.db)\n699         collector.collect(del_query)\n700         deleted, _rows_count = collector.delete()\n701 \n702         # Clear the result cache, in case this QuerySet gets reused.\n703         self._result_cache = None\n704         return deleted, _rows_count\n705 \n706     delete.alters_data = True\n707     delete.queryset_only = True\n708 \n709     def _raw_delete(self, using):\n710         \"\"\"\n711         Delete objects found from the given queryset in single direct SQL\n712         query. No signals are sent and there is no protection for cascades.\n713         \"\"\"\n714         return sql.DeleteQuery(self.model).delete_qs(self, using)\n715     _raw_delete.alters_data = True\n716 \n717     def update(self, **kwargs):\n718         \"\"\"\n719         Update all elements in the current QuerySet, setting all the given\n720         fields to the appropriate values.\n721         \"\"\"\n722         assert self.query.can_filter(), \\\n723             \"Cannot update a query once a slice has been taken.\"\n724         self._for_write = True\n725         query = self.query.chain(sql.UpdateQuery)\n726         query.add_update_values(kwargs)\n727         # Clear any annotations so that they won't be present in subqueries.\n728         query.annotations = {}\n729         with transaction.mark_for_rollback_on_error(using=self.db):\n730             rows = query.get_compiler(self.db).execute_sql(CURSOR)\n731         self._result_cache = None\n732         return rows\n733     update.alters_data = True\n734 \n735     def _update(self, values):\n736         \"\"\"\n737         A version of update() that accepts field objects instead of field names.\n738         Used primarily for model saving and not intended for use by general\n739         code (it requires too much poking around at model internals to be\n740         useful at that level).\n741         \"\"\"\n742         assert self.query.can_filter(), \\\n743             \"Cannot update a query once a slice has been taken.\"\n744         query = self.query.chain(sql.UpdateQuery)\n745         query.add_update_fields(values)\n746         # Clear any annotations so that they won't be present in subqueries.\n747         query.annotations = {}\n748         self._result_cache = None\n749         return query.get_compiler(self.db).execute_sql(CURSOR)\n750     _update.alters_data = True\n751     _update.queryset_only = False\n752 \n753     def exists(self):\n754         if self._result_cache is None:\n755             return self.query.has_results(using=self.db)\n756         return bool(self._result_cache)\n757 \n758     def _prefetch_related_objects(self):\n759         # This method can only be called once the result cache has been filled.\n760         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)\n761         self._prefetch_done = True\n762 \n763     def explain(self, *, format=None, **options):\n764         return self.query.explain(using=self.db, format=format, **options)\n765 \n766     ##################################################\n767     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #\n768     ##################################################\n769 \n770     def raw(self, raw_query, params=None, translations=None, using=None):\n771         if using is None:\n772             using = self.db\n773         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)\n774         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]\n775         return qs\n776 \n777     def _values(self, *fields, **expressions):\n778         clone = self._chain()\n779         if expressions:\n780             clone = clone.annotate(**expressions)\n781         clone._fields = fields\n782         clone.query.set_values(fields)\n783         return clone\n784 \n785     def values(self, *fields, **expressions):\n786         fields += tuple(expressions)\n787         clone = self._values(*fields, **expressions)\n788         clone._iterable_class = ValuesIterable\n789         return clone\n790 \n791     def values_list(self, *fields, flat=False, named=False):\n792         if flat and named:\n793             raise TypeError(\"'flat' and 'named' can't be used together.\")\n794         if flat and len(fields) > 1:\n795             raise TypeError(\"'flat' is not valid when values_list is called with more than one field.\")\n796 \n797         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}\n798         _fields = []\n799         expressions = {}\n800         counter = 1\n801         for field in fields:\n802             if hasattr(field, 'resolve_expression'):\n803                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())\n804                 while True:\n805                     field_id = field_id_prefix + str(counter)\n806                     counter += 1\n807                     if field_id not in field_names:\n808                         break\n809                 expressions[field_id] = field\n810                 _fields.append(field_id)\n811             else:\n812                 _fields.append(field)\n813 \n814         clone = self._values(*_fields, **expressions)\n815         clone._iterable_class = (\n816             NamedValuesListIterable if named\n817             else FlatValuesListIterable if flat\n818             else ValuesListIterable\n819         )\n820         return clone\n821 \n822     def dates(self, field_name, kind, order='ASC'):\n823         \"\"\"\n824         Return a list of date objects representing all available dates for\n825         the given field_name, scoped to 'kind'.\n826         \"\"\"\n827         assert kind in ('year', 'month', 'week', 'day'), \\\n828             \"'kind' must be one of 'year', 'month', 'week', or 'day'.\"\n829         assert order in ('ASC', 'DESC'), \\\n830             \"'order' must be either 'ASC' or 'DESC'.\"\n831         return self.annotate(\n832             datefield=Trunc(field_name, kind, output_field=DateField()),\n833             plain_field=F(field_name)\n834         ).values_list(\n835             'datefield', flat=True\n836         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')\n837 \n838     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):\n839         \"\"\"\n840         Return a list of datetime objects representing all available\n841         datetimes for the given field_name, scoped to 'kind'.\n842         \"\"\"\n843         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \\\n844             \"'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'.\"\n845         assert order in ('ASC', 'DESC'), \\\n846             \"'order' must be either 'ASC' or 'DESC'.\"\n847         if settings.USE_TZ:\n848             if tzinfo is None:\n849                 tzinfo = timezone.get_current_timezone()\n850         else:\n851             tzinfo = None\n852         return self.annotate(\n853             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),\n854             plain_field=F(field_name)\n855         ).values_list(\n856             'datetimefield', flat=True\n857         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')\n858 \n859     def none(self):\n860         \"\"\"Return an empty QuerySet.\"\"\"\n861         clone = self._chain()\n862         clone.query.set_empty()\n863         return clone\n864 \n865     ##################################################################\n866     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #\n867     ##################################################################\n868 \n869     def all(self):\n870         \"\"\"\n871         Return a new QuerySet that is a copy of the current one. This allows a\n872         QuerySet to proxy for a model manager in some cases.\n873         \"\"\"\n874         return self._chain()\n875 \n876     def filter(self, *args, **kwargs):\n877         \"\"\"\n878         Return a new QuerySet instance with the args ANDed to the existing\n879         set.\n880         \"\"\"\n881         return self._filter_or_exclude(False, *args, **kwargs)\n882 \n883     def exclude(self, *args, **kwargs):\n884         \"\"\"\n885         Return a new QuerySet instance with NOT (args) ANDed to the existing\n886         set.\n887         \"\"\"\n888         return self._filter_or_exclude(True, *args, **kwargs)\n889 \n890     def _filter_or_exclude(self, negate, *args, **kwargs):\n891         if args or kwargs:\n892             assert self.query.can_filter(), \\\n893                 \"Cannot filter a query once a slice has been taken.\"\n894 \n895         clone = self._chain()\n896         if negate:\n897             clone.query.add_q(~Q(*args, **kwargs))\n898         else:\n899             clone.query.add_q(Q(*args, **kwargs))\n900         return clone\n901 \n902     def complex_filter(self, filter_obj):\n903         \"\"\"\n904         Return a new QuerySet instance with filter_obj added to the filters.\n905 \n906         filter_obj can be a Q object or a dictionary of keyword lookup\n907         arguments.\n908 \n909         This exists to support framework features such as 'limit_choices_to',\n910         and usually it will be more natural to use other methods.\n911         \"\"\"\n912         if isinstance(filter_obj, Q):\n913             clone = self._chain()\n914             clone.query.add_q(filter_obj)\n915             return clone\n916         else:\n917             return self._filter_or_exclude(None, **filter_obj)\n918 \n919     def _combinator_query(self, combinator, *other_qs, all=False):\n920         # Clone the query to inherit the select list and everything\n921         clone = self._chain()\n922         # Clear limits and ordering so they can be reapplied\n923         clone.query.clear_ordering(True)\n924         clone.query.clear_limits()\n925         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)\n926         clone.query.combinator = combinator\n927         clone.query.combinator_all = all\n928         return clone\n929 \n930     def union(self, *other_qs, all=False):\n931         # If the query is an EmptyQuerySet, combine all nonempty querysets.\n932         if isinstance(self, EmptyQuerySet):\n933             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]\n934             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self\n935         return self._combinator_query('union', *other_qs, all=all)\n936 \n937     def intersection(self, *other_qs):\n938         # If any query is an EmptyQuerySet, return it.\n939         if isinstance(self, EmptyQuerySet):\n940             return self\n941         for other in other_qs:\n942             if isinstance(other, EmptyQuerySet):\n943                 return other\n944         return self._combinator_query('intersection', *other_qs)\n945 \n946     def difference(self, *other_qs):\n947         # If the query is an EmptyQuerySet, return it.\n948         if isinstance(self, EmptyQuerySet):\n949             return self\n950         return self._combinator_query('difference', *other_qs)\n951 \n952     def select_for_update(self, nowait=False, skip_locked=False, of=()):\n953         \"\"\"\n954         Return a new QuerySet instance that will select objects with a\n955         FOR UPDATE lock.\n956         \"\"\"\n957         if nowait and skip_locked:\n958             raise ValueError('The nowait option cannot be used with skip_locked.')\n959         obj = self._chain()\n960         obj._for_write = True\n961         obj.query.select_for_update = True\n962         obj.query.select_for_update_nowait = nowait\n963         obj.query.select_for_update_skip_locked = skip_locked\n964         obj.query.select_for_update_of = of\n965         return obj\n966 \n967     def select_related(self, *fields):\n968         \"\"\"\n969         Return a new QuerySet instance that will select related objects.\n970 \n971         If fields are specified, they must be ForeignKey fields and only those\n972         related objects are included in the selection.\n973 \n974         If select_related(None) is called, clear the list.\n975         \"\"\"\n976 \n977         if self._fields is not None:\n978             raise TypeError(\"Cannot call select_related() after .values() or .values_list()\")\n979 \n980         obj = self._chain()\n981         if fields == (None,):\n982             obj.query.select_related = False\n983         elif fields:\n984             obj.query.add_select_related(fields)\n985         else:\n986             obj.query.select_related = True\n987         return obj\n988 \n989     def prefetch_related(self, *lookups):\n990         \"\"\"\n991         Return a new QuerySet instance that will prefetch the specified\n992         Many-To-One and Many-To-Many related objects when the QuerySet is\n993         evaluated.\n994 \n995         When prefetch_related() is called more than once, append to the list of\n996         prefetch lookups. If prefetch_related(None) is called, clear the list.\n997         \"\"\"\n998         clone = self._chain()\n999         if lookups == (None,):\n1000             clone._prefetch_related_lookups = ()\n1001         else:\n1002             for lookup in lookups:\n1003                 if isinstance(lookup, Prefetch):\n1004                     lookup = lookup.prefetch_to\n1005                 lookup = lookup.split(LOOKUP_SEP, 1)[0]\n1006                 if lookup in self.query._filtered_relations:\n1007                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')\n1008             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups\n1009         return clone\n1010 \n1011     def annotate(self, *args, **kwargs):\n1012         \"\"\"\n1013         Return a query set in which the returned objects have been annotated\n1014         with extra data or aggregations.\n1015         \"\"\"\n1016         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')\n1017         annotations = {}\n1018         for arg in args:\n1019             # The default_alias property may raise a TypeError.\n1020             try:\n1021                 if arg.default_alias in kwargs:\n1022                     raise ValueError(\"The named annotation '%s' conflicts with the \"\n1023                                      \"default name for another annotation.\"\n1024                                      % arg.default_alias)\n1025             except TypeError:\n1026                 raise TypeError(\"Complex annotations require an alias\")\n1027             annotations[arg.default_alias] = arg\n1028         annotations.update(kwargs)\n1029 \n1030         clone = self._chain()\n1031         names = self._fields\n1032         if names is None:\n1033             names = set(chain.from_iterable(\n1034                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)\n1035                 for field in self.model._meta.get_fields()\n1036             ))\n1037 \n1038         for alias, annotation in annotations.items():\n1039             if alias in names:\n1040                 raise ValueError(\"The annotation '%s' conflicts with a field on \"\n1041                                  \"the model.\" % alias)\n1042             if isinstance(annotation, FilteredRelation):\n1043                 clone.query.add_filtered_relation(annotation, alias)\n1044             else:\n1045                 clone.query.add_annotation(annotation, alias, is_summary=False)\n1046 \n1047         for alias, annotation in clone.query.annotations.items():\n1048             if alias in annotations and annotation.contains_aggregate:\n1049                 if clone._fields is None:\n1050                     clone.query.group_by = True\n1051                 else:\n1052                     clone.query.set_group_by()\n1053                 break\n1054 \n1055         return clone\n1056 \n1057     def order_by(self, *field_names):\n1058         \"\"\"Return a new QuerySet instance with the ordering changed.\"\"\"\n1059         assert self.query.can_filter(), \\\n1060             \"Cannot reorder a query once a slice has been taken.\"\n1061         obj = self._chain()\n1062         obj.query.clear_ordering(force_empty=False)\n1063         obj.query.add_ordering(*field_names)\n1064         return obj\n1065 \n1066     def distinct(self, *field_names):\n1067         \"\"\"\n1068         Return a new QuerySet instance that will select only distinct results.\n1069         \"\"\"\n1070         assert self.query.can_filter(), \\\n1071             \"Cannot create distinct fields once a slice has been taken.\"\n1072         obj = self._chain()\n1073         obj.query.add_distinct_fields(*field_names)\n1074         return obj\n1075 \n1076     def extra(self, select=None, where=None, params=None, tables=None,\n1077               order_by=None, select_params=None):\n1078         \"\"\"Add extra SQL fragments to the query.\"\"\"\n1079         assert self.query.can_filter(), \\\n1080             \"Cannot change a query once a slice has been taken\"\n1081         clone = self._chain()\n1082         clone.query.add_extra(select, select_params, where, params, tables, order_by)\n1083         return clone\n1084 \n1085     def reverse(self):\n1086         \"\"\"Reverse the ordering of the QuerySet.\"\"\"\n1087         if not self.query.can_filter():\n1088             raise TypeError('Cannot reverse a query once a slice has been taken.')\n1089         clone = self._chain()\n1090         clone.query.standard_ordering = not clone.query.standard_ordering\n1091         return clone\n1092 \n1093     def defer(self, *fields):\n1094         \"\"\"\n1095         Defer the loading of data for certain fields until they are accessed.\n1096         Add the set of deferred fields to any existing set of deferred fields.\n1097         The only exception to this is if None is passed in as the only\n1098         parameter, in which case removal all deferrals.\n1099         \"\"\"\n1100         if self._fields is not None:\n1101             raise TypeError(\"Cannot call defer() after .values() or .values_list()\")\n1102         clone = self._chain()\n1103         if fields == (None,):\n1104             clone.query.clear_deferred_loading()\n1105         else:\n1106             clone.query.add_deferred_loading(fields)\n1107         return clone\n1108 \n1109     def only(self, *fields):\n1110         \"\"\"\n1111         Essentially, the opposite of defer(). Only the fields passed into this\n1112         method and that are not already specified as deferred are loaded\n1113         immediately when the queryset is evaluated.\n1114         \"\"\"\n1115         if self._fields is not None:\n1116             raise TypeError(\"Cannot call only() after .values() or .values_list()\")\n1117         if fields == (None,):\n1118             # Can only pass None to defer(), not only(), as the rest option.\n1119             # That won't stop people trying to do this, so let's be explicit.\n1120             raise TypeError(\"Cannot pass None as an argument to only().\")\n1121         for field in fields:\n1122             field = field.split(LOOKUP_SEP, 1)[0]\n1123             if field in self.query._filtered_relations:\n1124                 raise ValueError('only() is not supported with FilteredRelation.')\n1125         clone = self._chain()\n1126         clone.query.add_immediate_loading(fields)\n1127         return clone\n1128 \n1129     def using(self, alias):\n1130         \"\"\"Select which database this QuerySet should execute against.\"\"\"\n1131         clone = self._chain()\n1132         clone._db = alias\n1133         return clone\n1134 \n1135     ###################################\n1136     # PUBLIC INTROSPECTION ATTRIBUTES #\n1137     ###################################\n1138 \n1139     @property\n1140     def ordered(self):\n1141         \"\"\"\n1142         Return True if the QuerySet is ordered -- i.e. has an order_by()\n1143         clause or a default ordering on the model (or is empty).\n1144         \"\"\"\n1145         if isinstance(self, EmptyQuerySet):\n1146             return True\n1147         if self.query.extra_order_by or self.query.order_by:\n1148             return True\n1149         elif self.query.default_ordering and self.query.get_meta().ordering:\n1150             return True\n1151         else:\n1152             return False\n1153 \n1154     @property\n1155     def db(self):\n1156         \"\"\"Return the database used if this query is executed now.\"\"\"\n1157         if self._for_write:\n1158             return self._db or router.db_for_write(self.model, **self._hints)\n1159         return self._db or router.db_for_read(self.model, **self._hints)\n1160 \n1161     ###################\n1162     # PRIVATE METHODS #\n1163     ###################\n1164 \n1165     def _insert(self, objs, fields, return_id=False, raw=False, using=None, ignore_conflicts=False):\n1166         \"\"\"\n1167         Insert a new record for the given model. This provides an interface to\n1168         the InsertQuery class and is how Model.save() is implemented.\n1169         \"\"\"\n1170         self._for_write = True\n1171         if using is None:\n1172             using = self.db\n1173         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)\n1174         query.insert_values(fields, objs, raw=raw)\n1175         return query.get_compiler(using=using).execute_sql(return_id)\n1176     _insert.alters_data = True\n1177     _insert.queryset_only = False\n1178 \n1179     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):\n1180         \"\"\"\n1181         Helper method for bulk_create() to insert objs one batch at a time.\n1182         \"\"\"\n1183         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:\n1184             raise NotSupportedError('This database backend does not support ignoring conflicts.')\n1185         ops = connections[self.db].ops\n1186         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))\n1187         inserted_ids = []\n1188         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert\n1189         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:\n1190             if bulk_return and not ignore_conflicts:\n1191                 inserted_id = self._insert(\n1192                     item, fields=fields, using=self.db, return_id=True,\n1193                     ignore_conflicts=ignore_conflicts,\n1194                 )\n1195                 if isinstance(inserted_id, list):\n1196                     inserted_ids.extend(inserted_id)\n1197                 else:\n1198                     inserted_ids.append(inserted_id)\n1199             else:\n1200                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)\n1201         return inserted_ids\n1202 \n1203     def _chain(self, **kwargs):\n1204         \"\"\"\n1205         Return a copy of the current QuerySet that's ready for another\n1206         operation.\n1207         \"\"\"\n1208         obj = self._clone()\n1209         if obj._sticky_filter:\n1210             obj.query.filter_is_sticky = True\n1211             obj._sticky_filter = False\n1212         obj.__dict__.update(kwargs)\n1213         return obj\n1214 \n1215     def _clone(self):\n1216         \"\"\"\n1217         Return a copy of the current QuerySet. A lightweight alternative\n1218         to deepcopy().\n1219         \"\"\"\n1220         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)\n1221         c._sticky_filter = self._sticky_filter\n1222         c._for_write = self._for_write\n1223         c._prefetch_related_lookups = self._prefetch_related_lookups[:]\n1224         c._known_related_objects = self._known_related_objects\n1225         c._iterable_class = self._iterable_class\n1226         c._fields = self._fields\n1227         return c\n1228 \n1229     def _fetch_all(self):\n1230         if self._result_cache is None:\n1231             self._result_cache = list(self._iterable_class(self))\n1232         if self._prefetch_related_lookups and not self._prefetch_done:\n1233             self._prefetch_related_objects()\n1234 \n1235     def _next_is_sticky(self):\n1236         \"\"\"\n1237         Indicate that the next filter call and the one following that should\n1238         be treated as a single filter. This is only important when it comes to\n1239         determining when to reuse tables for many-to-many filters. Required so\n1240         that we can filter naturally on the results of related managers.\n1241 \n1242         This doesn't return a clone of the current QuerySet (it returns\n1243         \"self\"). The method is only used internally and should be immediately\n1244         followed by a filter() that does create a clone.\n1245         \"\"\"\n1246         self._sticky_filter = True\n1247         return self\n1248 \n1249     def _merge_sanity_check(self, other):\n1250         \"\"\"Check that two QuerySet classes may be merged.\"\"\"\n1251         if self._fields is not None and (\n1252                 set(self.query.values_select) != set(other.query.values_select) or\n1253                 set(self.query.extra_select) != set(other.query.extra_select) or\n1254                 set(self.query.annotation_select) != set(other.query.annotation_select)):\n1255             raise TypeError(\n1256                 \"Merging '%s' classes must involve the same values in each case.\"\n1257                 % self.__class__.__name__\n1258             )\n1259 \n1260     def _merge_known_related_objects(self, other):\n1261         \"\"\"\n1262         Keep track of all known related objects from either QuerySet instance.\n1263         \"\"\"\n1264         for field, objects in other._known_related_objects.items():\n1265             self._known_related_objects.setdefault(field, {}).update(objects)\n1266 \n1267     def resolve_expression(self, *args, **kwargs):\n1268         if self._fields and len(self._fields) > 1:\n1269             # values() queryset can only be used as nested queries\n1270             # if they are set up to select only a single field.\n1271             raise TypeError('Cannot use multi-field values as a filter value.')\n1272         query = self.query.resolve_expression(*args, **kwargs)\n1273         query._db = self._db\n1274         return query\n1275     resolve_expression.queryset_only = True\n1276 \n1277     def _add_hints(self, **hints):\n1278         \"\"\"\n1279         Update hinting information for use by routers. Add new key/values or\n1280         overwrite existing key/values.\n1281         \"\"\"\n1282         self._hints.update(hints)\n1283 \n1284     def _has_filters(self):\n1285         \"\"\"\n1286         Check if this QuerySet has any filtering going on. This isn't\n1287         equivalent with checking if all objects are present in results, for\n1288         example, qs[1:]._has_filters() -> False.\n1289         \"\"\"\n1290         return self.query.has_filters()\n1291 \n1292     @staticmethod\n1293     def _validate_values_are_expressions(values, method_name):\n1294         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))\n1295         if invalid_args:\n1296             raise TypeError(\n1297                 'QuerySet.%s() received non-expression(s): %s.' % (\n1298                     method_name,\n1299                     ', '.join(invalid_args),\n1300                 )\n1301             )\n",
        "intended_behavior": "This class provides additional context to the issue."
    }
]